# CLIP

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip)

## 概述

CLIP 模型是由 Alec Radford、Jong Wook Kim、Chris Hallacy、Aditya Ramesh、Gabriel Goh、Sandhini Agarwal、Girish Sastry、Amanda Askell、Pamela Mishkin、Jack Clark、Gretchen Krueger、Ilya Sutskever 在[从自然语言监督中学习可转移的视觉模型](https://arxiv.org/abs/2103.00020)中提出的。CLIP（对比语言-图像预训练）是一个在各种（图像，文本）对上训练的神经网络。它可以用自然语言指导来预测最相关的文本片段，给定一个图像，而不直接为任务进行优化，类似于 GPT-2 和 3 的零-shot 能力。

论文的摘要如下：

*最先进的计算机视觉系统被训练来预测一组固定的预定对象类别。这种受限的监督形式限制了它们的普遍性和可用性，因为需要额外的标记数据来指定任何其他视觉概念。直接从关于图像的原始文本中学习是一个有前途的替代方案，它利用了更广泛的监督来源。我们证明了预测哪个标题与哪个图像相匹配的简单预训练任务是一种有效且可扩展的方式，可以从互联网收集的 4 亿（图像，文本）对数据集上从头开始学习 SOTA 图像表示。预训练后，自然语言用于引用学习的视觉概念（或描述新的概念），从而实现模型对下游任务的零-shot 转移。我们通过在超过 30 个不同的现有计算机视觉数据集上进行基准测试来研究这种方法的性能，涵盖了 OCR、视频中的动作识别、地理定位以及许多类型的细粒度对象分类等任务。该模型对大多数任务进行了非平凡的转移，并且通常与完全监督的基线具有竞争力，而无需进行任何特定数据集的训练。例如，我们在 ImageNet 零-shot 上匹配了原始 ResNet-50 的准确率，而无需使用其训练的 128 万个训练示例中的任何一个。我们在此 https URL 上发布我们的代码和预训练模型权重。*

这个模型是由[valhalla](https://huggingface.co/valhalla)贡献的。原始代码可以在[这里](https://github.com/openai/CLIP)找到。

## 使用提示和示例

CLIP 是一个多模态视觉和语言模型。它可用于图像文本相似性和零-shot 图像分类。CLIP 使用类似 ViT 的 transformer 获取视觉特征，并使用因果语言模型获取文本特征。然后将文本和视觉特征投影到具有相同维度的潜在空间。然后使用投影图像和文本特征之间的点积作为相似分数。

为了将图像输入 Transformer 编码器，每个图像被分割成一系列固定大小且不重叠的补丁，然后进行线性嵌入。添加一个[CLS]标记作为整个图像的表示。作者还添加了绝对位置嵌入，并将结果向量序列馈送到标准 Transformer 编码器。CLIPImageProcessor 可用于调整（或重新缩放）和规范化图像以供模型使用。

使用 CLIPTokenizer 对文本进行编码。CLIPProcessor 将 CLIPImageProcessor 和 CLIPTokenizer 包装成单个实例，用于同时对文本进行编码和准备图像。以下示例展示了如何使用 CLIPProcessor 和 CLIPModel 获取图像文本相似度分数。

```py
>>> from PIL import Image
>>> import requests

>>> from transformers import CLIPProcessor, CLIPModel

>>> model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
>>> processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)

>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
```

## 资源

官方 Hugging Face 和社区（🌎标志）资源列表，帮助您开始使用 CLIP。

+   [使用遥感（卫星）图像和标题微调 CLIP](https://huggingface.co/blog/fine-tune-clip-rsicd)，一篇关于如何使用[RSICD 数据集](https://github.com/201528014227051/RSICD_optimal)微调 CLIP 并比较由于数据增强而导致的性能变化的博客文章。

+   这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text)展示了如何使用预训练的视觉和文本编码器训练类似 CLIP 的视觉-文本双编码器模型，使用[COCO 数据集](https://cocodataset.org/#home)。

​ 图像到文本

+   使用预训练的 CLIP 进行推理，使用波束搜索进行图像字幕生成的[笔记本](https://colab.research.google.com/drive/1tuoAC5F4sC7qid56Z0ap-stR3rwdk0ZV?usp=sharing)。🌎

**图像检索**

+   使用预训练的 CLIP 进行图像检索并计算 MRR（平均倒数排名）分数的[笔记本](https://colab.research.google.com/drive/1bLVwVKpAndpEDHqjzxVPr_9nGrSbuOQd?usp=sharing)。🌎

+   关于图像检索和显示相似度分数的[笔记本](https://colab.research.google.com/github/deep-diver/image_search_with_natural_language/blob/main/notebooks/Image_Search_CLIP.ipynb)。🌎

+   使用 Multilingual CLIP 将图像和文本映射到相同向量空间的[笔记本](https://colab.research.google.com/drive/1xO-wC_m_GNzgjIBQ4a4znvQkvDoZJvH4?usp=sharing)。🌎

+   关于如何在[Unsplash](https://unsplash.com)和[TMBD](https://www.themoviedb.org/)数据集上运行语义图像搜索的 CLIP 的[笔记本](https://colab.research.google.com/github/vivien000/clip-demo/blob/master/clip.ipynb#scrollTo=uzdFhRGqiWkR)。🌎

**可解释性**

+   关于如何可视化输入标记和图像段之间相似性的[笔记本](https://colab.research.google.com/github/hila-chefer/Transformer-MM-Explainability/blob/main/CLIP_explainability.ipynb)。🌎

如果您有兴趣提交资源以包含在此处，请随时提交拉取请求，我们将进行审核。资源应该尽可能展示新内容，而不是重复现有资源。

## CLIPConfig

### `class transformers.CLIPConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/configuration_clip.py#L266)

```py
( text_config = None vision_config = None projection_dim = 512 logit_scale_init_value = 2.6592 **kwargs )
```

参数

+   `text_config` (`dict`, *可选*) — 用于初始化 CLIPTextConfig 的配置选项字典。

+   `vision_config` (`dict`, *可选*) — 用于初始化 CLIPVisionConfig 的配置选项字典。

+   `projection_dim` (`int`, *可选*, 默认为 512) — 文本和视觉投影层的维度。

+   `logit_scale_init_value` (`float`, *可选*, 默认为 2.6592) — *logit_scale*参数的初始值。默认值根据原始 CLIP 实现使用。

+   `kwargs` (*可选*) — 关键字参数字典。

CLIPConfig 是用于存储 CLIPModel 配置的类。它用于根据指定的参数实例化一个 CLIP 模型，定义文本模型和视觉模型配置。使用默认值实例化配置将产生与 CLIP [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) 架构类似的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import CLIPConfig, CLIPModel

>>> # Initializing a CLIPConfig with openai/clip-vit-base-patch32 style configuration
>>> configuration = CLIPConfig()

>>> # Initializing a CLIPModel (with random weights) from the openai/clip-vit-base-patch32 style configuration
>>> model = CLIPModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config

>>> # We can also initialize a CLIPConfig from a CLIPTextConfig and a CLIPVisionConfig
>>> from transformers import CLIPTextConfig, CLIPVisionConfig

>>> # Initializing a CLIPText and CLIPVision configuration
>>> config_text = CLIPTextConfig()
>>> config_vision = CLIPVisionConfig()

>>> config = CLIPConfig.from_text_vision_configs(config_text, config_vision)
```

#### `from_text_vision_configs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/configuration_clip.py#L402)

```py
( text_config: CLIPTextConfig vision_config: CLIPVisionConfig **kwargs ) → export const metadata = 'undefined';CLIPConfig
```

返回

CLIPConfig

配置对象的一个实例

从 clip 文本模型配置和 clip 视觉模型配置实例化一个 CLIPConfig（或派生类）。

## CLIPTextConfig

### `class transformers.CLIPTextConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/configuration_clip.py#L39)

```py
( vocab_size = 49408 hidden_size = 512 intermediate_size = 2048 projection_dim = 512 num_hidden_layers = 12 num_attention_heads = 8 max_position_embeddings = 77 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 pad_token_id = 1 bos_token_id = 49406 eos_token_id = 49407 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 49408) — CLIP 文本模型的词汇量。定义了在调用 CLIPModel 时可以表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, defaults to 512) — 编码器层和池化层的维度。

+   `intermediate_size` (`int`, *optional*, defaults to 2048) — Transformer 编码器中“中间”（即前馈）层的维度。

+   `projection_dim` (`int`, *optional*, defaults to 512) — 文本和视觉投影层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer 编码器中的隐藏层数。

+   `num_attention_heads` (`int`, *optional*, defaults to 8) — Transformer 编码器中每个注意力层的注意力头数。

+   `max_position_embeddings` (`int`, *optional*, defaults to 77) — 该模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512 或 1024 或 2048）。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"quick_gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"` `"quick_gelu"`。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — 层归一化层使用的 epsilon。

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的 dropout 比率。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `initializer_factor` (`float`, *optional*, defaults to 1.0) — 用于初始化所有权重矩阵的因子（应保持为 1，用于内部初始化测试）。

+   `pad_token_id` (`int`, *optional*, defaults to 1) — 填充标记 id。

+   `bos_token_id` (`int`, *optional*, defaults to 49406) — 流开始标记 id。

+   `eos_token_id` (`int`, *optional*, defaults to 49407) — 流结束标记 id。

这是用于存储 CLIPTextModel 配置的配置类。根据指定的参数实例化一个 CLIP 文本编码器，定义模型架构。使用默认值实例化配置将产生类似于 CLIP [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32)架构的文本编码器的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import CLIPTextConfig, CLIPTextModel

>>> # Initializing a CLIPTextConfig with openai/clip-vit-base-patch32 style configuration
>>> configuration = CLIPTextConfig()

>>> # Initializing a CLIPTextModel (with random weights) from the openai/clip-vit-base-patch32 style configuration
>>> model = CLIPTextModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## CLIPVisionConfig

### `class transformers.CLIPVisionConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/configuration_clip.py#L157)

```py
( hidden_size = 768 intermediate_size = 3072 projection_dim = 512 num_hidden_layers = 12 num_attention_heads = 12 num_channels = 3 image_size = 224 patch_size = 32 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 **kwargs )
```

参数

+   `hidden_size` (`int`, *可选*, 默认为 768) — 编码器层和池化层的维度。

+   `intermediate_size` (`int`, *可选*, 默认为 3072) — Transformer 编码器中“中间”（即前馈）层的维度。

+   `projection_dim` (`int`, *可选*, 默认为 512) — 文本和视觉投影层的维度。

+   `num_hidden_layers` (`int`, *可选*, 默认为 12) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *可选*, 默认为 12) — Transformer 编码器中每个注意力层的注意力头数。

+   `num_channels` (`int`, *可选*, 默认为 3) — 输入通道的数量。

+   `image_size` (`int`, *可选*, 默认为 224) — 每个图像的大小（分辨率）。

+   `patch_size` (`int`, *可选*, 默认为 32) — 每个补丁的大小（分辨率）。

+   `hidden_act` (`str`或`function`, *可选*, 默认为`"quick_gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`以及`"quick_gelu"`。

+   `layer_norm_eps` (`float`, *可选*, 默认为 1e-05) — 层归一化层使用的 epsilon。

+   `attention_dropout` (`float`, *可选*, 默认为 0.0) — 注意力概率的 dropout 比率。

+   `initializer_range` (`float`, *可选*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `initializer_factor` (`float`, *可选*, 默认为 1.0) — 用于初始化所有权重矩阵的因子（应保持为 1，用于内部初始化测试）。

这是用于存储 CLIPVisionModel 配置的配置类。根据指定的参数实例化一个 CLIP 视觉编码器，定义模型架构。使用默认值实例化配置将产生类似于 CLIP [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32)架构的视觉编码器的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import CLIPVisionConfig, CLIPVisionModel

>>> # Initializing a CLIPVisionConfig with openai/clip-vit-base-patch32 style configuration
>>> configuration = CLIPVisionConfig()

>>> # Initializing a CLIPVisionModel (with random weights) from the openai/clip-vit-base-patch32 style configuration
>>> model = CLIPVisionModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## CLIPTokenizer

### `class transformers.CLIPTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip.py#L272)

```py
( vocab_file merges_file errors = 'replace' unk_token = '<|endoftext|>' bos_token = '<|startoftext|>' eos_token = '<|endoftext|>' pad_token = '<|endoftext|>' **kwargs )
```

参数

+   `vocab_file` (`str`) — 词汇表文件的路径。

+   `merges_file` (`str`) — 合并文件的路径。

+   `errors`（`str`，*可选*，默认为`"replace"`）— 解码字节为 UTF-8 时要遵循的范例。有关更多信息，请参阅[bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)。

+   `unk_token`（`str`，*optional*，默认为``"<|endoftext|>"``）--未知令牌。词汇表中没有的令牌无法转换为 ID，而是设置为该令牌。

+   `bos_token`（`str`，*optional*，默认为``"<|startoftext|>"``）--序列标记的开头。

+   `eos_token`（`str`，*optional*，默认为``"<|endoftext|>"``）--序列结束标记。

+   `pad_token`（`str`，*optional*，默认为``"<|endoftext|>"``）--用于填充的令牌，例如，在批处理不同长度的序列时。

构建一个 CLIP 分词器。基于字节级字节对编码。

此分词器继承自 PreTrainedTokenizer，其中包含大部分主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip.py#L359)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）— 将添加特殊令牌的 ID 列表。

+   `token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个 ID 列表。

返回

`List[int]`

具有适当特殊令牌的输入 ID 列表。

通过连接和添加特殊令牌，从序列或序列对构建用于序列分类任务的模型输入。CLIP 序列具有以下格式：

+   单个序列：`<|startoftext|> X <|endoftext|>`

序列对不是预期的使用情况，但它们将在没有分隔符的情况下处理。

#### `get_special_tokens_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip.py#L386)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）— ID 列表。

+   `token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个 ID 列表。

+   `already_has_special_tokens`（`bool`，*可选*，默认为`False`）— 令牌列表是否已经为模型格式化了特殊令牌。

返回

`List[int]`

一个整数列表，范围为[0, 1]：1 表示特殊令牌，0 表示序列令牌。

从没有添加特殊令牌的令牌列表中检索序列 id。当使用 tokenizer 的`prepare_for_model`方法添加特殊令牌时，将调用此方法。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip.py#L414)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）— ID 列表。

+   `token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个 ID 列表。

返回

`List[int]`

零列表。

从传递的两个序列创建一个掩码。CLIP 不使用令牌类型 id，因此返回一个零列表。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip.py#L509)

```py
( save_directory: str filename_prefix: Optional = None )
```

## CLIPTokenizerFast

### `class transformers.CLIPTokenizerFast`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip_fast.py#L50)

```py
( vocab_file = None merges_file = None tokenizer_file = None unk_token = '<|endoftext|>' bos_token = '<|startoftext|>' eos_token = '<|endoftext|>' pad_token = '<|endoftext|>' **kwargs )
```

参数

+   `vocab_file`（`str`，*可选*）— 词汇文件的路径。

+   `merges_file`（`str`，*可选*）— 合并文件的路径。

+   `tokenizer_file`（`str`，*可选*）— 要使用的分词器文件的路径，而不是词汇文件。

+   `unk_token`（`str`，*optional*，默认为``"<|endoftext|>"``）--未知令牌。词汇表中没有的令牌无法转换为 ID，而是设置为该令牌。

+   `bos_token`（`str`，*optional*，默认为``"<|startoftext|>"``）--序列标记的开头。

+   `eos_token`（`str`，*optional*，默认为``"<|endoftext|>"``）--序列结束标记。

+   `pad_token`（`str`，*optional*，默认为``"<|endoftext|>"``）--用于填充的令牌，例如，在批处理不同长度的序列时。

构建一个“快速”CLIP 分词器（由 HuggingFace 的*tokenizers*库支持）。基于字节级字节对编码。

此分词器继承自 PreTrainedTokenizerFast，其中包含大部分主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip_fast.py#L127)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）— 将添加特殊令牌的 ID 列表。

+   `token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个 ID 列表。

返回

`List[int]`

具有适当特殊令牌的输入 ID 列表。

通过连接和添加特殊标记，从序列或序列对构建用于序列分类任务的模型输入。CLIP 序列的格式如下：

+   单个序列：`<|startoftext|> X <|endoftext|>`

序列对不是预期的用例，但它们将被处理而无需分隔符。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip_fast.py#L154)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID 列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个 ID 列表。

返回

`List[int]`

零的列表。

从传递的两个序列创建一个掩码。CLIP 不使用标记类型 ID，因此返回一个零列表。

## CLIPImageProcessor

### `class transformers.CLIPImageProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/image_processing_clip.py#L50)

```py
( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BICUBIC: 3> do_center_crop: bool = True crop_size: Dict = None do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_convert_rgb: bool = True **kwargs )
```

参数

+   `do_resize` (`bool`, *可选*, 默认为 `True`) — 是否将图像的（高度，宽度）尺寸调整为指定的`size`。可以被`preprocess`方法中的`do_resize`覆盖。

+   `size` (`Dict[str, int]` *可选*, 默认为 `{"shortest_edge" -- 224}`): 调整大小后的图像大小。图像的最短边被调整为 size[“shortest_edge”]，最长边被调整以保持输入的纵横比。可以被`preprocess`方法中的`size`覆盖。

+   `resample` (`PILImageResampling`, *可选*, 默认为 `Resampling.BICUBIC`) — 如果调整图像大小，则使用的重采样滤波器。可以被`preprocess`方法中的`resample`覆盖。

+   `do_center_crop` (`bool`, *可选*, 默认为 `True`) — 是否将图像居中裁剪到指定的`crop_size`。可以被`preprocess`方法中的`do_center_crop`覆盖。

+   `crop_size` (`Dict[str, int]` *可选*, 默认为 224) — 应用`center_crop`后输出图像的大小。可以被`preprocess`方法中的`crop_size`覆盖。

+   `do_rescale` (`bool`, *可选*, 默认为 `True`) — 是否按指定比例`rescale_factor`重新缩放图像。可以被`preprocess`方法中的`do_rescale`覆盖。

+   `rescale_factor` (`int` 或 `float`, *可选*, 默认为 `1/255`) — 如果重新缩放图像，则使用的比例因子。可以被`preprocess`方法中的`rescale_factor`覆盖。

+   `do_normalize` (`bool`, *可选*, 默认为 `True`) — 是否对图像进行规范化。可以被`preprocess`方法中的`do_normalize`覆盖。

+   `image_mean` (`float` 或 `List[float]`, *可选*, 默认为 `[0.48145466, 0.4578275, 0.40821073]`) — 如果规范化图像，则使用的均值。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被`preprocess`方法中的`image_mean`参数覆盖。

+   `image_std` (`float` 或 `List[float]`, *可选*, 默认为 `[0.26862954, 0.26130258, 0.27577711]`) — 如果规范化图像，则使用的标准差。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被`preprocess`方法中的`image_std`参数覆盖。

+   `do_convert_rgb` (`bool`, *可选*, 默认为 `True`) — 是否将图像转换为 RGB。

构建一个 CLIP 图像处理器。

#### `preprocess`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/image_processing_clip.py#L177)

```py
( images: Union do_resize: bool = None size: Dict = None resample: Resampling = None do_center_crop: bool = None crop_size: int = None do_rescale: bool = None rescale_factor: float = None do_normalize: bool = None image_mean: Union = None image_std: Union = None do_convert_rgb: bool = None return_tensors: Union = None data_format: Optional = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要预处理的图像。期望单个或批量图像，像素值范围从 0 到 255。如果传入像素值在 0 到 1 之间的图像，请设置`do_rescale=False`。

+   `do_resize` (`bool`, *可选*, 默认为 `self.do_resize`) — 是否调整图像大小。

+   `size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — 调整大小后的图像尺寸。图像的最短边被调整为 size[“shortest_edge”]，最长边被调整以保持输入的长宽比。

+   `resample` (`int`, *optional*, defaults to `self.resample`) — 如果调整图像大小，则使用的重采样滤波器。可以是枚举`PILImageResampling`之一。仅在`do_resize`设置为`True`时有效。

+   `do_center_crop` (`bool`, *optional*, defaults to `self.do_center_crop`) — 是否对图像进行中心裁剪。

+   `crop_size` (`Dict[str, int]`, *optional*, defaults to `self.crop_size`) — 中心裁剪的尺寸。仅在`do_center_crop`设置为`True`时有效。

+   `do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — 是否重新缩放图像。

+   `rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) — 如果`do_rescale`设置为`True`，则用于重新缩放图像的缩放因子。

+   `do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — 是否对图像进行归一化。

+   `image_mean` (`float`或`List[float]`, *optional*, defaults to `self.image_mean`) — 用于归一化的图像均值。仅在`do_normalize`设置为`True`时有效。

+   `image_std` (`float`或`List[float]`, *optional*, defaults to `self.image_std`) — 用于归一化的图像标准差。仅在`do_normalize`设置为`True`时有效。

+   `do_convert_rgb` (`bool`, *optional*, defaults to `self.do_convert_rgb`) — 是否将图像转换为 RGB。

+   `return_tensors` (`str`或`TensorType`, *optional*) — 要返回的张量类型。可以是以下之一：

    +   未设置：返回一个`np.ndarray`列表。

    +   `TensorType.TENSORFLOW`或`'tf'`：返回一个`tf.Tensor`类型的批次。

    +   `TensorType.PYTORCH`或`'pt'`：返回一个`torch.Tensor`类型的批次。

    +   `TensorType.NUMPY`或`'np'`：返回一个`np.ndarray`类型的批次。

    +   `TensorType.JAX`或`'jax'`：返回一个`jax.numpy.ndarray`类型的批次。

+   `data_format` (`ChannelDimension`或`str`, *optional*, defaults to `ChannelDimension.FIRST`) — 输出图像的通道维度格式。可以是以下之一：

    +   `"channels_first"`或`ChannelDimension.FIRST`：图像以（通道数，高度，宽度）格式。

    +   `"channels_last"`或`ChannelDimension.LAST`：图像以（高度，宽度，通道数）格式。

    +   未设置：使用输入图像的通道维度格式。

+   `input_data_format` (`ChannelDimension`或`str`, *optional*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"`或`ChannelDimension.FIRST`：图像以（通道数，高度，宽度）格式。

    +   `"channels_last"`或`ChannelDimension.LAST`：图像以（高度，宽度，通道数）格式。

    +   `"none"`或`ChannelDimension.NONE`：图像以（高度，宽度）格式。

预处理一张图像或一批图像。

## CLIPFeatureExtractor

### `class transformers.CLIPFeatureExtractor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/feature_extraction_clip.py#L26)

```py
( *args **kwargs )
```

## CLIPProcessor

### `class transformers.CLIPProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/processing_clip.py#L25)

```py
( image_processor = None tokenizer = None **kwargs )
```

参数

+   `image_processor` (CLIPImageProcessor, *optional*) — 图像处理器是必需的输入。

+   `tokenizer` (CLIPTokenizerFast, *optional*) — 分词器是必需的输入。

构建一个 CLIP 处理器，将 CLIP 图像处理器和 CLIP 分词器封装成一个处理器。

CLIPProcessor 提供了 CLIPImageProcessor 和 CLIPTokenizerFast 的所有功能。查看`__call__()`和 decode()以获取更多信息。

#### `批量解码`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/processing_clip.py#L114)

```py
( *args **kwargs )
```

此方法将其所有参数转发给 CLIPTokenizerFast 的 batch_decode()。有关更多信息，请参阅此方法的文档字符串。

#### `解码`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/processing_clip.py#L121)

```py
( *args **kwargs )
```

此方法将其所有参数转发给 CLIPTokenizerFast 的 decode()。有关更多信息，请参阅此方法的文档字符串。

Pytorch 隐藏 Pytorch 内容

## CLIPModel

### `class transformers.CLIPModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L925)

```py
( config: CLIPConfig )
```

参数

+   `config`（CLIPConfig）— 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

此模型继承自 PreTrainedModel。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是 PyTorch 的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `前向`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1057)

```py
( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None position_ids: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.clip.modeling_clip.CLIPOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 输入序列标记在词汇表中的索引。默认情况下，如果提供填充，将忽略填充。

    可以使用 AutoTokenizer 获取索引。查看 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()获取详细信息。

    什么是输入 ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`范围内：

    +   1 表示`未被掩盖`的标记，

    +   0 表示`被掩盖`的标记。

    什么是注意力掩码？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    什么是位置 ID？

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) — 像素值。默认情况下会忽略填充。可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 CLIPImageProcessor.`call`()。

+   `return_loss` (`bool`, *可选*) — 是否返回对比损失。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回一个 ModelOutput 而不是一个普通元组。

返回

`transformers.models.clip.modeling_clip.CLIPOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.clip.modeling_clip.CLIPOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包含根据配置(`<class 'transformers.models.clip.configuration_clip.CLIPConfig'>`)和输入的不同元素。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *可选*, 当`return_loss`为`True`时返回) — 图像-文本相似度的对比损失。

+   `logits_per_image:(torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`) — `image_embeds`和`text_embeds`之间的缩放点积分数。这代表图像-文本相似度分数。

+   `logits_per_text:(torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`) — `text_embeds`和`image_embeds`之间的缩放点积分数。这代表文本-图像相似度分数。

+   `text_embeds(torch.FloatTensor` of shape `(batch_size, output_dim`) — 通过将投影层应用于 CLIPTextModel 的池化输出获得的文本嵌入。

+   `image_embeds(torch.FloatTensor` of shape `(batch_size, output_dim`) — 通过将投影层应用于 CLIPVisionModel 的池化输出获得的图像嵌入。

+   `text_model_output(BaseModelOutputWithPooling):` CLIPTextModel 的输出。

+   `vision_model_output(BaseModelOutputWithPooling):` CLIPVisionModel 的输出。

CLIPModel 的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例:

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, CLIPModel

>>> model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
>>> processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(
...     text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True
... )

>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
```

#### `get_text_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L961)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。默认情况下会忽略填充。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   对于未被`masked`的标记为 1，

    +   对于被`masked`的标记为 0。

    什么是注意力掩码？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置 ID？

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个 ModelOutput 而不是一个普通元组。

返回

text_features（形状为`(batch_size, output_dim)`的`torch.FloatTensor`）

通过将投影层应用于 CLIPTextModel 的池化输出获得的文本嵌入。

CLIPModel 的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, CLIPModel

>>> model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
>>> tokenizer = AutoTokenizer.from_pretrained("openai/clip-vit-base-patch32")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
>>> text_features = model.get_text_features(**inputs)
```

#### `get_image_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1008)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。默认情况下将忽略填充。如果提供，可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参见 CLIPImageProcessor.`call`()。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个 ModelOutput 而不是一个普通元组。

返回

image_features（形状为`(batch_size, output_dim)`的`torch.FloatTensor`）

通过将投影层应用于 CLIPVisionModel 的池化输出获得的图像嵌入。

CLIPModel 的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, CLIPModel

>>> model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
>>> processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> image_features = model.get_image_features(**inputs)
```

## CLIPTextModel

### `class transformers.CLIPTextModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L747)

```py
( config: CLIPTextConfig )
```

参数

+   `config`（CLIPConfig）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained() 方法以加载模型权重。

CLIP 中的文本模型没有任何头部或顶部的投影。该模型继承自 PreTrainedModel。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 的子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L768)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`）— 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode() 和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask`（形状为 `(batch_size, sequence_length)` 的 `torch.Tensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。掩码值选择在 `[0, 1]` 中：

    +   对于未被掩码的标记为 1，

    +   对于被掩码的标记为 0。

    什么是注意力掩码？

+   `position_ids`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]` 中选择。

    什么是位置 ID？

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回 ModelOutput 而不是普通元组。

返回

transformers.modeling_outputs.BaseModelOutputWithPooling 或 `tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.BaseModelOutputWithPooling 或 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时），包括根据配置（`<class 'transformers.models.clip.configuration_clip.CLIPTextConfig'>`）和输入的不同元素。

+   `last_hidden_state`（形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor`）— 模型最后一层的隐藏状态序列。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）- 经过用于辅助预训练任务的层进一步处理后的序列的第一个标记（分类标记）的最后一层隐藏状态。例如，对于 BERT 系列模型，这将返回经过线性层和 tanh 激活函数处理后的分类标记。线性层的权重是从预训练期间的下一个句子预测（分类）目标中训练的。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层，则为嵌入的输出+每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在自注意力头中用于计算加权平均值的注意力 softmax 后的注意力权重。

CLIPTextModel 的前向方法覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例而不是这个，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, CLIPTextModel

>>> model = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32")
>>> tokenizer = AutoTokenizer.from_pretrained("openai/clip-vit-base-patch32")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states
```

## CLIPTextModelWithProjection

### `class transformers.CLIPTextModelWithProjection`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1151)

```py
( config: CLIPTextConfig )
```

参数

+   `config`（CLIPConfig）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

在顶部具有投影层（在池化输出的顶部的线性层）的 CLIP 文本模型。

该模型继承自 PreTrainedModel。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型还是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1178)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.clip.modeling_clip.CLIPTextModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask`（`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`范围内。

    +   1 表示`未被掩码`的标记，

    +   0 表示`被掩码`的标记。

    什么是注意力掩码？

+   `position_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选在范围`[0, config.max_position_embeddings - 1]`内。

    什么是位置 ID？

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。查看返回张量中的`attentions`以获取更多细节。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。查看返回张量中的`hidden_states`以获取更多细节。

+   `return_dict`（`bool`，*可选*）— 是否返回一个 ModelOutput 而不是一个普通元组。

返回

`transformers.models.clip.modeling_clip.CLIPTextModelOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.clip.modeling_clip.CLIPTextModelOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或者`config.return_dict=False`）包含各种元素，取决于配置（`<class 'transformers.models.clip.configuration_clip.CLIPTextConfig'>`）和输入。

+   `text_embeds`（`torch.FloatTensor`，形状为`(batch_size, output_dim)`，*可选*，在使用`with_projection=True`初始化模型时返回）— 通过将投影层应用于 pooler_output 获得的文本嵌入。

+   `last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`）— 模型最后一层的隐藏状态序列。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或者`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或者`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

CLIPTextModelWithProjection 的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, CLIPTextModelWithProjection

>>> model = CLIPTextModelWithProjection.from_pretrained("openai/clip-vit-base-patch32")
>>> tokenizer = AutoTokenizer.from_pretrained("openai/clip-vit-base-patch32")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")

>>> outputs = model(**inputs)
>>> text_embeds = outputs.text_embeds
```

## CLIPVisionModelWithProjection

### `class transformers.CLIPVisionModelWithProjection`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1232)

```py
( config: CLIPVisionConfig )
```

参数

+   `config`（CLIPConfig）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法加载模型权重。

在顶部带有投影层的 CLIP Vision 模型（在池化输出的顶部有一个线性层）。

这个模型继承自 PreTrainedModel。查看超类文档，了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1255)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.clip.modeling_clip.CLIPVisionModelOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。如果提供，可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 CLIPImageProcessor.`call`()。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回一个 ModelOutput 而不是一个普通的元组。

返回

`transformers.models.clip.modeling_clip.CLIPVisionModelOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.clip.modeling_clip.CLIPVisionModelOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（`<class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'>`）和输入。

+   `image_embeds` (`torch.FloatTensor`，形状为`(batch_size, output_dim)` *可选*，当模型初始化时带有`with_projection=True`时返回) — 通过将投影层应用于 pooler_output 获得的图像嵌入。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每个层的输出）。

    每个层输出的模型的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

CLIPVisionModelWithProjection 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在之后调用`Module`实例，而不是在这里调用，因为前者会处理运行前处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, CLIPVisionModelWithProjection

>>> model = CLIPVisionModelWithProjection.from_pretrained("openai/clip-vit-base-patch32")
>>> processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> outputs = model(**inputs)
>>> image_embeds = outputs.image_embeds
```

## CLIPVisionModel

### `class transformers.CLIPVisionModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L866)

```py
( config: CLIPVisionConfig )
```

参数

+   `config` (CLIPConfig) — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

CLIP 中的视觉模型，没有顶部的头部或投影。此模型继承自 PreTrainedModel。查看超类文档以了解库为其所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L884)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 CLIPImageProcessor.`call`()。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请查看返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回 ModelOutput 而不是普通元组。

返回

transformers.modeling_outputs.BaseModelOutputWithPooling 或`torch.FloatTensor`元组

一个 transformers.modeling_outputs.BaseModelOutputWithPooling 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'>`）和输入的各种元素。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`) — 经过用于辅助预训练任务的层进一步处理后的序列第一个标记（分类标记）的最后一层隐藏状态。例如，对于 BERT 系列模型，这返回经过线性层和 tanh 激活函数处理后的分类标记。线性层的权重是从下一个句子预测（分类）目标在预训练期间训练的。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个 + 每个层的输出的一个）。

    模型在每个层的输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

CLIPVisionModel 的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是此函数，因为前者会负责运行预处理和后处理步骤，而后者会默默忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, CLIPVisionModel

>>> model = CLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")
>>> processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled CLS states
```

TensorFlow 隐藏 TensorFlow 内容

## TFCLIPModel

### `class transformers.TFCLIPModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1307)

```py
( config: CLIPConfig *inputs **kwargs )
```

参数

+   `config`（CLIPConfig） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

此模型继承自 TFPreTrainedModel。查看超类文档以获取库为其所有模型实现的通用方法（例如下载或保存，调整输入嵌入，修剪头等）。

此模型还是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)的子类。将其用作常规 TF 2.0 Keras 模型，并参考 TF 2.0 文档以获取与一般用法和行为相关的所有内容。

`transformers`中的 TensorFlow 模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于 PyTorch 模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

支持第二种格式的原因是 Keras 方法在将输入传递给模型和层时更喜欢这种格式。由于有此支持，当使用`model.fit()`等方法时，应该“只需工作” - 只需以`model.fit()`支持的任何格式传递输入和标签！但是，如果要在 Keras 方法之外使用第二种格式，例如在使用 Keras`Functional` API 创建自己的层或模型时，有三种可能性可用于收集第一个位置参数中的所有输入张量：

+   仅包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`

+   一个长度可变的列表，其中包含按照文档字符串中给定的顺序的一个或多个输入张量：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个字典，其中包含与文档字符串中给定的输入名称相关联的一个或多个输入张量：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，在使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像对待任何其他 Python 函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1398)

```py
( input_ids: TFModelInputType | None = None pixel_values: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None return_loss: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';transformers.models.clip.modeling_tf_clip.TFCLIPOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例的形状必须为`(batch_size, sequence_length)`） — 输入序列标记在词汇表中的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.`call`()和 PreTrainedTokenizer.encode()。

    什么是输入 ID？

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`np.ndarray`、`tf.Tensor`、`List[tf.Tensor]`、`Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例必须具有相同形状）— 像素值。可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 CLIPImageProcessor.`call`()。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`np.ndarray`或`tf.Tensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：

    +   对于未被遮罩的标记，

    +   对于被遮罩的标记为 0。

    什么是注意力掩码？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`np.ndarray`或`tf.Tensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置 ID？

+   `return_loss`（`bool`，*可选*）— 是否返回对比损失。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。此参数仅可在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅可在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict`（`bool`，*可选*）— 是否返回 ModelOutput 而不是普通元组。此参数仅可在急切模式下使用，在图模式下该值将始终设置为 True。

+   `training`（`bool`，*可选*，默认为`False`）— 是否在训练模式下使用模型（一些模块如 dropout 模块在训练和评估之间有不同的行为）。

返回

`transformers.models.clip.modeling_tf_clip.TFCLIPOutput`或`tuple(tf.Tensor)`

一个`transformers.models.clip.modeling_tf_clip.TFCLIPOutput`或一个`tf.Tensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.clip.configuration_clip.CLIPConfig'>`）和输入的不同元素。

+   `loss`（形状为`(1,)`的`tf.Tensor`，*可选*，当`return_loss`为`True`时返回）— 图像-文本相似性的对比损失。

+   `logits_per_image:(形状为`(image_batch_size, text_batch_size)`的`tf.Tensor`) — `image_embeds`和`text_embeds`之间的缩放点积分数。这代表图像-文本相似性分数。

+   `logits_per_text:(形状为`(text_batch_size, image_batch_size)`的`tf.Tensor`) — `text_embeds`和`image_embeds`之间的缩放点积分数。这代表文本-图像相似性分数。

+   `text_embeds(tf.Tensor`的形状为`(batch_size, output_dim)`）— 通过将投影层应用于 TFCLIPTextModel 的汇聚输出获得的文本嵌入。

+   `image_embeds(tf.Tensor` of shape `(batch_size, output_dim`) — 通过将池化输出应用于 TFCLIPVisionModel 的投影层获得的图像嵌入。

+   `text_model_output(~modeling_tf_utils.TFBaseModelOutputWithPooling):` TFCLIPTextModel 的输出。

+   `vision_model_output(~modeling_tf_utils.TFBaseModelOutputWithPooling):` TFCLIPVisionModel 的输出。

TFCLIPModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> import tensorflow as tf
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, TFCLIPModel

>>> model = TFCLIPModel.from_pretrained("openai/clip-vit-base-patch32")
>>> processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(
...     text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="tf", padding=True
... )

>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities
```

#### `get_text_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1316)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';text_features (tf.Tensor of shape (batch_size, output_dim)
```

参数

+   `input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]` 或 `Dict[str, np.ndarray]`，每个示例的形状必须为`(batch_size, sequence_length)`) — 词汇表中输入序列令牌的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.`call`()和 PreTrainedTokenizer.encode()。

    什么是输入 ID？

+   `attention_mask` (`np.ndarray` 或 `tf.Tensor` of shape `(batch_size, sequence_length)`, *可选*) — 用于避免在填充令牌索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   对于`未被掩码`的令牌为 1，

    +   对于`被掩码`的令牌为 0。

    什么是注意力掩码？

+   `position_ids` (`np.ndarray` 或 `tf.Tensor` of shape `(batch_size, sequence_length)`, *可选*) — 每个输入序列令牌的位置在位置嵌入中的索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    什么是位置 ID？

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict` (`bool`, *可选*) — 是否返回 ModelOutput 而不是普通元组。此参数仅在急切模式下使用，在图模式下该值将始终设置为 True。

+   `training` (`bool`, *可选*, 默认为`False“) — 是否在训练模式下使用模型（一些模块如 dropout 模块在训练和评估之间有不同的行为）。

返回

text_features (`tf.Tensor` of shape `(batch_size, output_dim`)

通过将池化输出应用于 TFCLIPTextModel 的投影层获得的文本嵌入。

TFCLIPModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, TFCLIPModel

>>> model = TFCLIPModel.from_pretrained("openai/clip-vit-base-patch32")
>>> tokenizer = AutoTokenizer.from_pretrained("openai/clip-vit-base-patch32")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="tf")
>>> text_features = model.get_text_features(**inputs)
```

#### `get_image_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1356)

```py
( pixel_values: TFModelInputType | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';image_features (tf.Tensor of shape (batch_size, output_dim)
```

参数

+   `pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]` 或 `Dict[str, np.ndarray]`，每个示例的形状必须为 `(batch_size, num_channels, height, width)`) — 像素值。可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 CLIPImageProcessor.`call`()。output_attentions (`bool`，*可选*): 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的 `attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的 `hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict` (`bool`，*可选*) — 是否返回 ModelOutput 而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为 True。

+   `training` (`bool`，*可选*，默认为 `False“) — 是否在训练模式下使用模型（一些模块如 dropout 模块在训练和评估之间有不同的行为）。

返回

image_features (`tf.Tensor`，形状为 `(batch_size, output_dim`)

通过将投影层应用于 TFCLIPVisionModel 的汇总输出获得的图像嵌入。

TFCLIPModel 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, TFCLIPModel

>>> model = TFCLIPModel.from_pretrained("openai/clip-vit-base-patch32")
>>> processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="tf")

>>> image_features = model.get_image_features(**inputs)
```

## TFCLIPTextModel

### `class transformers.TFCLIPTextModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1185)

```py
( config: CLIPTextConfig *inputs **kwargs )
```

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1193)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling or tuple(tf.Tensor)
```

参数

+   `input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]` 或 `Dict[str, np.ndarray]`，每个示例的形状必须为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.`call`()和 PreTrainedTokenizer.encode()。

    什么是输入 ID？

+   `attention_mask` (`np.ndarray` 或 `tf.Tensor`，形状为 `(batch_size, sequence_length)`，*可选*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]` 之间：

    +   1 代表未被屏蔽的标记，

    +   0 代表被屏蔽的标记。

    什么是注意力掩码？

+   `position_ids` (`np.ndarray`或形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置 ID？

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict` (`bool`, *可选*) — 是否返回 ModelOutput 而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为 True。

+   `training` (`bool`, *可选*, 默认为`False“) — 是否在训练模式下使用模型（一些模块如 dropout 模块在训练和评估之间有不同的行为）。

返回

transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling 或`tf.Tensor`元组

一个 transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling 或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（`<class 'transformers.models.clip.configuration_clip.CLIPTextConfig'>`）和输入。

+   `last_hidden_state` (`tf.Tensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`tf.Tensor`，形状为`(batch_size, hidden_size)`) — 序列第一个标记（分类标记）的最后一层隐藏状态，经过线性层和 Tanh 激活函数进一步处理。线性层的权重在预训练期间从下一个句子预测（分类）目标中训练。

    这个输出通常*不是*输入的语义内容的好摘要，您通常最好对整个输入序列的隐藏状态进行平均或池化。

+   `hidden_states` (`tuple(tf.Tensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(tf.Tensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    自注意力头中的注意力 softmax 之后的注意力权重，用于计算加权平均值。

TFCLIPTextModel 的前向方法，覆盖`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, TFCLIPTextModel

>>> model = TFCLIPTextModel.from_pretrained("openai/clip-vit-base-patch32")
>>> tokenizer = AutoTokenizer.from_pretrained("openai/clip-vit-base-patch32")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="tf")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states
```

## TFCLIPVisionModel

### `class transformers.TFCLIPVisionModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1245)

```py
( config: CLIPVisionConfig *inputs **kwargs )
```

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1254)

```py
( pixel_values: TFModelInputType | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling or tuple(tf.Tensor)
```

参数

+   `pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` `Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例必须具有形状`(batch_size, num_channels, height, width)`) — 像素值。可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 CLIPImageProcessor.`call`()。output_attentions (`bool`, *optional*): 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。此参数可以在急切模式下使用，在图模式下该值将始终设置为 True。

+   `training` (`bool`, *optional*, 默认为`False`) — 是否在训练模式下使用模型（一些模块如 dropout 模块在训练和评估之间有不同的行为）。

返回

transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling 或 `tuple(tf.Tensor)`

一个 transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling 或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含各种元素，取决于配置（`<class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'>`）和输入。

+   `last_hidden_state` (`tf.Tensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`tf.Tensor`，形状为`(batch_size, hidden_size)`) — 序列第一个标记（分类标记）的最后一层隐藏状态，进一步由线性层和 Tanh 激活函数处理。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

    这个输出通常*不是*输入语义内容的好摘要，通常最好对整个输入序列的隐藏状态进行平均或池化。

+   `hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每个层的输出的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个层一个）。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

TFCLIPVisionModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, TFCLIPVisionModel

>>> model = TFCLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")
>>> processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="tf")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled CLS states
```

JAX 隐藏 JAX 内容

## FlaxCLIPModel

### `class transformers.FlaxCLIPModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L1262)

```py
( config: CLIPConfig input_shape: Optional = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

参数

+   `config` (CLIPConfig) — 模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

+   `dtype` (`jax.numpy.dtype`, *可选*, 默认为 `jax.numpy.float32`) — 计算的数据类型。可以是`jax.numpy.float32`、`jax.numpy.float16`（在 GPU 上）和`jax.numpy.bfloat16`（在 TPU 上）之一。

    这可以用于在 GPU 或 TPU 上启用混合精度训练或半精度推断。如果指定了`dtype`，则所有计算将使用给定的`dtype`执行。

    `请注意，这仅指定计算的数据类型，不会影响模型参数的数据类型。`

    如果要更改模型参数的数据类型，请参阅 to_fp16()和 to_bf16()。

此模型继承自 FlaxPreTrainedModel。查看超类文档以了解库为其所有模型实现的通用方法（如从 PyTorch 模型下载、保存和转换权重）。

此模型也是[flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)的子类。将其用作常规的 Flax linen 模块，并参考 Flax 文档以了解所有与一般用法和行为相关的事项。

最后，此模型支持 JAX 的固有功能，例如：

+   [即时编译（JIT）](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [向量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L820)

```py
( input_ids pixel_values attention_mask = None position_ids = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下，如果提供填充，则将忽略填充。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   1 代表`未被掩盖`的标记，

    +   0 代表`被掩盖`的标记。

    什么是注意力掩码？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`，*可选*）- 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置 ID？

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`numpy.ndarray`）- 像素值。默认情况下将忽略填充。可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参见 CLIPImageProcessor.`call`()。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回 ModelOutput 而不是普通元组。

返回

`transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput`或`tuple(torch.FloatTensor)`

`transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，取决于配置（`<class 'transformers.models.clip.configuration_clip.CLIPConfig'>`）和输入。

+   `logits_per_image:(jnp.ndarray`，形状为`(image_batch_size, text_batch_size)`）- `image_embeds`和`text_embeds`之间的缩放点积分数。这代表图像-文本相似度分数。

+   `logits_per_text:(jnp.ndarray`，形状为`(text_batch_size, image_batch_size)`）- `text_embeds`和`image_embeds`之间的缩放点积分数。这代表文本-图像相似度分数。

+   `text_embeds(jnp.ndarray`，形状为`(batch_size, output_dim)`）- 通过将投影层应用于 FlaxCLIPTextModel 的汇聚输出获得的文本嵌入。

+   `image_embeds(jnp.ndarray`，形状为`(batch_size, output_dim)`）- 通过将投影层应用于 FlaxCLIPVisionModel 的汇聚输出获得的图像嵌入。

+   `text_model_output(FlaxBaseModelOutputWithPooling):` FlaxCLIPTextModel 的输出。

+   `vision_model_output(FlaxBaseModelOutputWithPooling):` FlaxCLIPVisionModel 的输出。

`FlaxCLIPPreTrainedModel`的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import jax
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, FlaxCLIPModel

>>> model = FlaxCLIPModel.from_pretrained("openai/clip-vit-base-patch32")
>>> processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(
...     text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="np", padding=True
... )

>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = jax.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities
```

#### `get_text_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L865)

```py
( input_ids attention_mask = None position_ids = None params: dict = None dropout_rng: PRNGKey = None train = False ) → export const metadata = 'undefined';text_features (jnp.ndarray of shape (batch_size, output_dim)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`）- 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

返回

文本特征（`jnp.ndarray`，形状为`(batch_size, output_dim)`）

通过将 FlaxCLIPTextModel 的池化输出应用于投影层获得的文本嵌入。

示例：

```py
>>> from transformers import AutoTokenizer, FlaxCLIPModel

>>> model = FlaxCLIPModel.from_pretrained("openai/clip-vit-base-patch32")
>>> tokenizer = AutoTokenizer.from_pretrained("openai/clip-vit-base-patch32")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="np")
>>> text_features = model.get_text_features(**inputs)
```

#### `get_image_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L932)

```py
( pixel_values params: dict = None dropout_rng: PRNGKey = None train = False ) → export const metadata = 'undefined';image_features (jnp.ndarray of shape (batch_size, output_dim)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`numpy.ndarray`）— 像素值。默认情况下将忽略填充。可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 CLIPImageProcessor.`call`()。

返回

图像特征（`jnp.ndarray`，形状为`(batch_size, output_dim)`）

通过将 FlaxCLIPVisionModel 的池化输出应用于投影层获得的图像嵌入

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, FlaxCLIPModel

>>> model = FlaxCLIPModel.from_pretrained("openai/clip-vit-base-patch32")
>>> processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="np")

>>> image_features = model.get_image_features(**inputs)
```

## FlaxCLIPTextModel

### `class transformers.FlaxCLIPTextModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L1012)

```py
( config: CLIPTextConfig input_shape = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L665)

```py
( input_ids attention_mask = None position_ids = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`）— 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`，*可选*）— 避免在填充标记索引上执行注意力的掩码。选择的掩码值为`[0, 1]`：

    +   对于`未被掩盖`的标记为 1，

    +   0 表示`被掩盖`的标记。

    什么是注意力掩码？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置 ID？

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回 ModelOutput 而不是普通元组。

返回

transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling 或 `tuple(torch.FloatTensor)`

一个 transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling 或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.clip.configuration_clip.CLIPTextConfig'>`）和输入不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`）— 模型最后一层的隐藏状态序列输出。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`jnp.ndarray`）— 序列第一个标记（分类标记）的最后一层隐藏状态，经过线性层和 Tanh 激活函数进一步处理。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states`（`tuple(jnp.ndarray)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入输出，一个用于每一层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(jnp.ndarray)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

`FlaxCLIPTextPreTrainedModel`的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, FlaxCLIPTextModel

>>> model = FlaxCLIPTextModel.from_pretrained("openai/clip-vit-base-patch32")
>>> tokenizer = AutoTokenizer.from_pretrained("openai/clip-vit-base-patch32")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="np")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooler_output = outputs.pooler_output  # pooled (EOS token) states
```

## FlaxCLIPTextModelWithProjection

### `class transformers.FlaxCLIPTextModelWithProjection`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L1083)

```py
( config: CLIPTextConfig input_shape = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L665)

```py
( input_ids attention_mask = None position_ids = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.clip.modeling_flax_clip.FlaxCLIPTextModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`）— 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。选择在`[0, 1]`范围内的掩码值：

    +   1 表示`未被掩码`的标记，

    +   0 表示`被掩码`的标记。

    什么是注意力掩码？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    什么是位置 ID？

+   `output_attentions`（*可选*，`bool`）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states`（*可选*，`bool`）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回 ModelOutput 而不是普通元组。

返回

`transformers.models.clip.modeling_flax_clip.FlaxCLIPTextModelOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.clip.modeling_flax_clip.FlaxCLIPTextModelOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置(`<class 'transformers.models.clip.configuration_clip.CLIPTextConfig'>`)和输入的各种元素。

+   `text_embeds` (`形状为(batch_size, output_dim)的 jnp.ndarray`) — 通过将投影层应用于 FlaxCLIPTextModel 的池化输出获得的文本嵌入。

+   `last_hidden_state` (`形状为(batch_size, sequence_length, hidden_size)的 jnp.ndarray`) — 模型最后一层的隐藏状态序列。

+   `hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出 + 一个用于每层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    注意力权重在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

`FlaxCLIPTextPreTrainedModel`的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, FlaxCLIPTextModelWithProjection

>>> model = FlaxCLIPTextModelWithProjection.from_pretrained("openai/clip-vit-base-patch32")
>>> tokenizer = AutoTokenizer.from_pretrained("openai/clip-vit-base-patch32")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="np")

>>> outputs = model(**inputs)
>>> text_embeds = outputs.text_embeds
```

## FlaxCLIPVisionModel

### `class transformers.FlaxCLIPVisionModel`

[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L1137)

```py
( config: CLIPVisionConfig input_shape: Optional = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

#### `__call__`

[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L745)

```py
( pixel_values params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`形状为(batch_size, num_channels, height, width)的 numpy.ndarray`) — 像素值。默认情况下将忽略填充。可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 CLIPImageProcessor.`call`()。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回 ModelOutput 而不是普通元组。

返回

transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling 或 `tuple(torch.FloatTensor)`

一个 transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包括根据配置（`<class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'>`）和输入的不同元素。

+   `last_hidden_state`（形状为 `(batch_size, sequence_length, hidden_size)` 的 `jnp.ndarray`）- 模型最后一层的隐藏状态序列。

+   `pooler_output`（形状为 `(batch_size, hidden_size)` 的 `jnp.ndarray`）- 序列第一个标记（分类标记）的最后一层隐藏状态，经过线性层和 Tanh 激活函数进一步处理。线性层权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states`（`tuple(jnp.ndarray)`，*可选*，当传递 `output_hidden_states=True` 或当 `config.output_hidden_states=True` 时返回）- 形状为 `(batch_size, sequence_length, hidden_size)` 的 `jnp.ndarray` 元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(jnp.ndarray)`，*可选*，当传递 `output_attentions=True` 或当 `config.output_attentions=True` 时返回）- 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `jnp.ndarray` 元组（每层一个）。

    注意力权重在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

`FlaxCLIPVisionPreTrainedModel` 的前向方法，覆盖了 `__call__` 特殊方法。

尽管前向传递的方法需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, FlaxCLIPVisionModel

>>> model = FlaxCLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")
>>> processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="np")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooler_output = outputs.pooler_output  # pooled CLS states
```
