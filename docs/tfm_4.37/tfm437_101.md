# BERTology

> 原文：[`huggingface.co/docs/transformers/v4.37.2/en/bertology`](https://huggingface.co/docs/transformers/v4.37.2/en/bertology)

有一个不断增长的研究领域，关注调查大规模变压器（如 BERT）的内部工作（有些人称之为“BERTology”）。 这个领域的一些很好的例子是：

+   BERT 重新发现了经典的 NLP 流程 作者：Ian Tenney，Dipanjan Das，Ellie Pavlick：[`arxiv.org/abs/1905.05950`](https://arxiv.org/abs/1905.05950)

+   十六个头真的比一个好吗？ 作者：Paul Michel，Omer Levy，Graham Neubig：[`arxiv.org/abs/1905.10650`](https://arxiv.org/abs/1905.10650)

+   BERT 看什么？ 作者：Kevin Clark，Urvashi Khandelwal，Omer Levy，Christopher D. Manning：[`arxiv.org/abs/1906.04341`](https://arxiv.org/abs/1906.04341)

+   CAT 探测：一种基于度量的方法，解释预训练模型如何关注代码结构：[`arxiv.org/abs/2210.04633`](https://arxiv.org/abs/2210.04633)

为了帮助这个新领域发展，我们在 BERT/GPT/GPT-2 模型中添加了一些额外功能，以帮助人们访问内部表示，主要是从 Paul Michel 的伟大工作中改编的（[`arxiv.org/abs/1905.10650`](https://arxiv.org/abs/1905.10650)）：

+   访问 BERT/GPT/GPT-2 的所有隐藏状态，

+   访问 BERT/GPT/GPT-2 每个头的所有注意权重，

+   检索头输出值和梯度，以便计算头重要性分数并修剪头，如[`arxiv.org/abs/1905.10650`](https://arxiv.org/abs/1905.10650)中所解释的。

为了帮助您理解和使用这些功能，我们添加了一个特定的示例脚本：[bertology.py](https://github.com/huggingface/transformers/tree/main/examples/research_projects/bertology/run_bertology.py)，同时提取在 GLUE 上预训练的模型的信息并修剪。
