# MegatronGPT2

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/megatron_gpt2`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/megatron_gpt2)

## 概述

MegatronGPT2 模型是由 Mohammad Shoeybi、Mostofa Patwary、Raul Puri、Patrick LeGresley、Jared Casper 和 Bryan Catanzaro 在[使用模型并行训练多十亿参数语言模型的 Megatron-LM](https://arxiv.org/abs/1909.08053)中提出的。

论文摘要如下：

*最近在语言建模方面的工作表明，训练大型 Transformer 模型可以推动自然语言处理应用的最新技术。然而，非常大的模型可能由于内存限制而难以训练。在这项工作中，我们提出了训练非常大的 Transformer 模型的技术，并实现了一种简单高效的层内模型并行方法，使得可以训练具有数十亿参数的 Transformer 模型。我们的方法不需要新的编译器或库更改，与管道模型并行性正交且互补，并且可以通过在原生 PyTorch 中插入几个通信操作来完全实现。我们通过使用 512 个 GPU 收敛基于 Transformer 的模型，达到了 83 亿参数。与维持 39 TeraFLOPs 的强单 GPU 基线相比，我们在整个应用程序中保持了 15.1 PetaFLOPs，其扩展效率为 76%，这是峰值 FLOPs 的 30%。为了证明大型语言模型可以进一步推动最新技术（SOTA），我们训练了一个 83 亿参数的 Transformer 语言模型，类似于 GPT-2，以及一个 39 亿参数的类似 BERT 的模型。我们展示了在 BERT 类似模型中对层归一化的放置要特别注意，这对于随着模型规模增长而实现性能提升至关重要。使用 GPT-2 模型，我们在 WikiText103（10.8，与 15.8 的 SOTA 困惑度相比）和 LAMBADA（66.5%，与 63.2%的 SOTA 准确率相比）数据集上取得了 SOTA 结果。我们的 BERT 模型在 RACE 数据集上取得了 SOTA 结果（90.9%，与 89.4%的 SOTA 准确率相比）。*

这个模型是由[jdemouth](https://huggingface.co/jdemouth)贡献的。原始代码可以在[这里](https://github.com/NVIDIA/Megatron-LM)找到。该存储库包含了 Megatron 语言模型的多 GPU 和多节点实现。特别是，它包含了使用“张量并行”和“管道并行”技术的混合模型并行方法。

## 使用提示

我们提供了预训练的[GPT2-345M](https://ngc.nvidia.com/catalog/models/nvidia:megatron_lm_345m)检查点，用于评估或微调下游任务。

要访问这些检查点，首先[注册](https://ngc.nvidia.com/signup)并设置 NVIDIA GPU 云（NGC）注册表 CLI。有关下载模型的更多文档，请参阅[NGC 文档](https://docs.nvidia.com/dgx/ngc-registry-cli-user-guide/index.html#topic_6_4_1)。

或者，您可以直接下载检查点：

```py
wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O
megatron_gpt2_345m_v0_0.zip
```

一旦您从 NVIDIA GPU 云（NGC）获得了检查点，您需要将其转换为 Hugging Face Transformers GPT2 实现可以轻松加载的格式。

以下命令允许您进行转换。我们假设文件夹`models/megatron_gpt2`包含`megatron_gpt2_345m_v0_0.zip`，并且该命令是从该文件夹运行的：

```py
python3 $PATH_TO_TRANSFORMERS/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py megatron_gpt2_345m_v0_0.zip
```

MegatronGPT2 架构与 OpenAI GPT-2 相同。有关配置类和其参数的信息，请参考 GPT-2 文档。
