# UL2

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/ul2`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/ul2)

## 概述

T5 模型在[Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler 撰写的《统一语言学习范式》](https://arxiv.org/pdf/2205.05131v1.pdf)中被提出。

论文的摘要如下：

*现有的预训练模型通常针对特定类别的问题。迄今为止，关于正确的架构和预训练设置应该是什么，似乎仍然没有共识。本文提出了一个统一的框架，用于预训练模型在数据集和设置上具有普遍有效性。我们首先通过解开具有预训练目标的架构原型来开始——这两个概念通常被混淆。接下来，我们提出了自监督在 NLP 中的泛化和统一视角，并展示了不同的预训练目标如何相互转换以及如何在不同目标之间插值可以是有效的。然后，我们提出了去噪器混合（MoD），这是一个将不同的预训练范式结合在一起的预训练目标。此外，我们引入了一种模式切换的概念，其中下游微调与特定的预训练方案相关联。我们进行了大量的消融实验，比较了多个预训练目标，并发现我们的方法通过在多个不同设置中优于 T5 和/或类似 GPT 的模型来推动帕累托前沿。最后，通过将我们的模型扩展到 20B 参数，我们在 50 个建立良好的监督 NLP 任务上实现了 SOTA 性能，涵盖了语言生成（自动化和人工评估）、语言理解、文本分类、问题回答、常识推理、长文本推理、结构化知识基础和信息检索。我们的模型还在上下文学习方面取得了强大的结果，在零样本 SuperGLUE 上优于 175B 的 GPT-3，并在一次性摘要上将 T5-XXL 的性能提高了三倍。*

此模型由[DanielHesslow](https://huggingface.co/Seledorn)贡献。原始代码可以在[这里](https://github.com/google-research/google-research/tree/master/ul2)找到。

## 使用提示

+   UL2 是一个编码器-解码器模型，预先在一系列去噪函数的混合上进行了预训练，并在一系列下游任务上进行了微调。

+   UL2 与 T5v1.1 具有相同的架构，但使用了门控 SiLU 激活函数，而不是门控 GELU。

+   作者发布了一个架构的检查点，可以在[这里](https://huggingface.co/google/ul2)看到

由于 UL2 与 T5v1.1 具有相同的架构，请参考 T5 的文档页面获取 API 参考、提示、代码示例和笔记本。
