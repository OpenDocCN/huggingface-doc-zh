# DeBERTa

> åŸæ–‡ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/deberta`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/deberta)

## æ¦‚è¿°

DeBERTa æ¨¡å‹æ˜¯ç”±ä½•é¹ç¨‹ã€åˆ˜æ™“ä¸œã€é«˜å»ºå³°ã€é™ˆä¼ŸæŸ±åœ¨[DeBERTa: å…·æœ‰è§£è€¦æ³¨æ„åŠ›çš„è§£ç å¢å¼º BERT](https://arxiv.org/abs/2006.03654)ä¸­æå‡ºçš„ï¼ŒåŸºäº 2018 å¹´å‘å¸ƒçš„ Google çš„ BERT æ¨¡å‹å’Œ 2019 å¹´å‘å¸ƒçš„ Facebook çš„ RoBERTa æ¨¡å‹ã€‚

å®ƒåœ¨ RoBERTa çš„åŸºç¡€ä¸Šä½¿ç”¨è§£è€¦æ³¨æ„åŠ›å’Œå¢å¼ºçš„æ©ç è§£ç å™¨è®­ç»ƒï¼Œä½¿ç”¨ RoBERTa ä¸€åŠçš„æ•°æ®ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

æœ€è¿‘ï¼Œåœ¨é¢„è®­ç»ƒç¥ç»è¯­è¨€æ¨¡å‹æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¤§å¤§æé«˜äº†è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹æ¶æ„ DeBERTaï¼ˆå…·æœ‰è§£è€¦æ³¨æ„åŠ›çš„è§£ç å¢å¼º BERTï¼‰ï¼Œåˆ©ç”¨ä¸¤ç§æ–°æŠ€æœ¯æ”¹è¿›äº† BERT å’Œ RoBERTa æ¨¡å‹ã€‚ç¬¬ä¸€ç§æ˜¯è§£è€¦æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶ä¸­æ¯ä¸ªå•è¯ä½¿ç”¨ä¸¤ä¸ªå‘é‡è¡¨ç¤ºï¼Œåˆ†åˆ«ç¼–ç å…¶å†…å®¹å’Œä½ç½®ï¼Œå•è¯ä¹‹é—´çš„æ³¨æ„åŠ›æƒé‡æ˜¯é€šè¿‡è§£è€¦çŸ©é˜µåœ¨å®ƒä»¬çš„å†…å®¹å’Œç›¸å¯¹ä½ç½®ä¸Šè®¡ç®—çš„ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨å¢å¼ºçš„æ©ç è§£ç å™¨æ¥æ›¿æ¢è¾“å‡º softmax å±‚ï¼Œä»¥é¢„æµ‹æ¨¡å‹é¢„è®­ç»ƒçš„æ©ç æ ‡è®°ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¿™ä¸¤ç§æŠ€æœ¯æ˜¾è‘—æé«˜äº†æ¨¡å‹é¢„è®­ç»ƒçš„æ•ˆç‡å’Œä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚ä¸ RoBERTa-Large ç›¸æ¯”ï¼ŒDeBERTa æ¨¡å‹åœ¨ä¸€åŠè®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒï¼Œå¯¹å„ç§ NLP ä»»åŠ¡è¡¨ç°å‡ºæ›´å¥½çš„ä¸€è‡´æ€§ï¼ŒMNLI æé«˜äº†+0.9%ï¼ˆ90.2% vs. 91.1%ï¼‰ï¼ŒSQuAD v2.0 æé«˜äº†+2.3%ï¼ˆ88.4% vs. 90.7%ï¼‰ï¼ŒRACE æé«˜äº†+3.6%ï¼ˆ83.2% vs. 86.8%ï¼‰ã€‚DeBERTa çš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†åœ¨[`github.com/microsoft/DeBERTa`](https://github.com/microsoft/DeBERTa)ä¸Šå…¬å¼€æä¾›ã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯ç”±[DeBERTa](https://huggingface.co/DeBERTa)è´¡çŒ®çš„ã€‚è¿™ä¸ªæ¨¡å‹ TF 2.0 å®ç°æ˜¯ç”±[kamalkraj](https://huggingface.co/kamalkraj)è´¡çŒ®çš„ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/microsoft/DeBERTa)æ‰¾åˆ°ã€‚

## èµ„æº

åˆ—å‡ºäº†å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºçš„æ¸…å•ï¼Œä»¥å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ DeBERTaã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ª Pull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºæœ€å¥½å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

æ–‡æœ¬åˆ†ç±»

+   ä¸€ç¯‡å…³äºå¦‚ä½•ä½¿ç”¨ DeepSpeed åŠ é€Ÿå¤§å‹æ¨¡å‹è®­ç»ƒçš„åšå®¢æ–‡ç« ï¼Œä½¿ç”¨ DeBERTaã€‚

+   ä¸€ç¯‡å…³äºå¦‚ä½•ä½¿ç”¨æœºå™¨å­¦ä¹ æå‡å®¢æˆ·æœåŠ¡çš„åšå®¢æ–‡ç« ï¼Œä½¿ç”¨ DeBERTaã€‚

+   DebertaForSequenceClassification ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)æ”¯æŒã€‚

+   TFDebertaForSequenceClassification ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)æ”¯æŒã€‚

+   æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—

æ ‡è®°åˆ†ç±»

+   DebertaForTokenClassification å¯ä»¥é€šè¿‡è¿™ä¸ª [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb) æ”¯æŒã€‚

+   TFDebertaForTokenClassification å¯ä»¥é€šè¿‡è¿™ä¸ª [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb) æ”¯æŒã€‚

+   ğŸ¤— Hugging Face è¯¾ç¨‹çš„ [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) ç« èŠ‚ã€‚

+   [Byte-Pair Encoding tokenization](https://huggingface.co/course/chapter6/5?fw=pt) ç« èŠ‚ã€‚

+   Token classification ä»»åŠ¡æŒ‡å—

å¡«å……-é®è”½

+   DebertaForMaskedLM å¯ä»¥é€šè¿‡è¿™ä¸ª [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb) æ”¯æŒã€‚

+   TFDebertaForMaskedLM å¯ä»¥é€šè¿‡è¿™ä¸ª [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb) æ”¯æŒã€‚

+   ğŸ¤— Hugging Face è¯¾ç¨‹çš„ [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) ç« èŠ‚ã€‚

+   é®è”½è¯­è¨€å»ºæ¨¡ä»»åŠ¡æŒ‡å—

é—®ç­”

+   DebertaForQuestionAnswering å¯ä»¥é€šè¿‡è¿™ä¸ª [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb) æ”¯æŒã€‚

+   TFDebertaForQuestionAnswering å¯ä»¥é€šè¿‡è¿™ä¸ª [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb) æ”¯æŒã€‚

+   ğŸ¤— Hugging Face è¯¾ç¨‹çš„ [é—®ç­”](https://huggingface.co/course/chapter7/7?fw=pt) ç« èŠ‚ã€‚

+   é—®ç­”ä»»åŠ¡æŒ‡å—

## DebertaConfig

### `class transformers.DebertaConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/configuration_deberta.py#L40)

```py
( vocab_size = 50265 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 0 initializer_range = 0.02 layer_norm_eps = 1e-07 relative_attention = False max_relative_positions = -1 pad_token_id = 0 position_biased_input = True pos_att_type = None pooler_dropout = 0 pooler_hidden_act = 'gelu' **kwargs )
```

å‚æ•°

+   `vocab_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 30522) â€” DeBERTa æ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨ DebertaModel æˆ– TFDebertaModel æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒ token æ•°é‡ã€‚

+   `hidden_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚

+   `num_attention_heads` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 12) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `intermediate_size` (`int`, *optional*, defaults to 3072) â€” Transformer ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"silu"`ã€`"gelu"`ã€`"tanh"`ã€`"gelu_fast"`ã€`"mish"`ã€`"linear"`ã€`"sigmoid"`å’Œ`"gelu_new"`ã€‚

+   `hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ dropout æ¦‚ç‡ã€‚

+   `attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ dropout æ¯”ç‡ã€‚

+   `max_position_embeddings` (`int`, *optional*, defaults to 512) â€” æ­¤æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸å°†å…¶è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚ï¼Œ512 æˆ– 1024 æˆ– 2048ï¼‰ã€‚

+   `type_vocab_size` (`int`, *optional*, defaults to 2) â€” åœ¨è°ƒç”¨ DebertaModel æˆ– TFDebertaModel æ—¶ä¼ é€’çš„`token_type_ids`çš„è¯æ±‡è¡¨å¤§å°ã€‚

+   `initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `relative_attention` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦ä½¿ç”¨ç›¸å¯¹ä½ç½®ç¼–ç ã€‚

+   `max_relative_positions` (`int`, *optional*, defaults to 1) â€” ç›¸å¯¹ä½ç½®èŒƒå›´`[-max_position_embeddings, max_position_embeddings]`ã€‚ä½¿ç”¨ä¸`max_position_embeddings`ç›¸åŒçš„å€¼ã€‚

+   `pad_token_id` (`int`, *optional*, defaults to 0) â€” ç”¨äºå¡«å…… input_ids çš„å€¼ã€‚

+   `position_biased_input` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦å°†ç»å¯¹ä½ç½®åµŒå…¥æ·»åŠ åˆ°å†…å®¹åµŒå…¥ä¸­ã€‚

+   `pos_att_type` (`List[str]`, *optional*) â€” ç›¸å¯¹ä½ç½®æ³¨æ„åŠ›çš„ç±»å‹ï¼Œå¯ä»¥æ˜¯`["p2c", "c2p"]`çš„ç»„åˆï¼Œä¾‹å¦‚`["p2c"]`ã€`["p2c", "c2p"]`ã€‚

+   `layer_norm_eps` (`float`, optional, defaults to 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ DebertaModel æˆ– TFDebertaModel é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª DeBERTa æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº DeBERTa [microsoft/deberta-base](https://huggingface.co/microsoft/deberta-base)æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import DebertaConfig, DebertaModel

>>> # Initializing a DeBERTa microsoft/deberta-base style configuration
>>> configuration = DebertaConfig()

>>> # Initializing a model (with random weights) from the microsoft/deberta-base style configuration
>>> model = DebertaModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## DebertaTokenizer

### `class transformers.DebertaTokenizer`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/tokenization_deberta.py#L109)

```py
( vocab_file merges_file errors = 'replace' bos_token = '[CLS]' eos_token = '[SEP]' sep_token = '[SEP]' cls_token = '[CLS]' unk_token = '[UNK]' pad_token = '[PAD]' mask_token = '[MASK]' add_prefix_space = False add_bos_token = False **kwargs )
```

å‚æ•°

+   `vocab_file` (`str`) â€” è¯æ±‡è¡¨æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `merges_file` (`str`) â€” åˆå¹¶æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `errors` (`str`, *optional*, defaults to `"replace"`) â€” è§£ç å­—èŠ‚ä¸º UTF-8 æ—¶è¦éµå¾ªçš„èŒƒä¾‹ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)ã€‚

+   `bos_token` (`str`, *optional*, defaults to `"[CLS]"`) â€” åºåˆ—å¼€å§‹æ ‡è®°ã€‚

+   `eos_token` (`str`, *optional*, defaults to `"[SEP]"`) â€” åºåˆ—ç»“æŸæ ‡è®°ã€‚

+   `sep_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"[SEP]"`) â€” ç”¨äºä»å¤šä¸ªåºåˆ—æ„å»ºåºåˆ—æ—¶ä½¿ç”¨çš„åˆ†éš”ç¬¦æ ‡è®°ï¼Œä¾‹å¦‚ç”¨äºåºåˆ—åˆ†ç±»çš„ä¸¤ä¸ªåºåˆ—æˆ–ç”¨äºæ–‡æœ¬å’Œé—®é¢˜çš„é—®é¢˜å›ç­”ã€‚å®ƒä¹Ÿç”¨ä½œä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºçš„åºåˆ—çš„æœ€åä¸€ä¸ªæ ‡è®°ã€‚

+   `cls_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"[CLS]"`) â€” ç”¨äºåºåˆ—åˆ†ç±»æ—¶ä½¿ç”¨çš„åˆ†ç±»å™¨æ ‡è®°ï¼ˆå¯¹æ•´ä¸ªåºåˆ—è¿›è¡Œåˆ†ç±»ï¼Œè€Œä¸æ˜¯å¯¹æ¯ä¸ªæ ‡è®°è¿›è¡Œåˆ†ç±»ï¼‰ã€‚å½“ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºåºåˆ—æ—¶ï¼Œå®ƒæ˜¯åºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ã€‚

+   `unk_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"[UNK]"`) â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­æ²¡æœ‰çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸º IDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚

+   `pad_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"[PAD]"`) â€” ç”¨äºå¡«å……çš„æ ‡è®°ï¼Œä¾‹å¦‚åœ¨æ‰¹å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—æ—¶ä½¿ç”¨ã€‚

+   `mask_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"[MASK]"`) â€” ç”¨äºå±è”½å€¼çš„æ ‡è®°ã€‚åœ¨ä½¿ç”¨æ©ç è¯­è¨€å»ºæ¨¡è®­ç»ƒæ­¤æ¨¡å‹æ—¶ä½¿ç”¨çš„æ ‡è®°ã€‚è¿™æ˜¯æ¨¡å‹å°†å°è¯•é¢„æµ‹çš„æ ‡è®°ã€‚

+   `add_prefix_space` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨è¾“å…¥å‰æ·»åŠ ä¸€ä¸ªåˆå§‹ç©ºæ ¼ã€‚è¿™å…è®¸å°†å‰å¯¼å•è¯è§†ä¸ºä»»ä½•å…¶ä»–å•è¯ã€‚ï¼ˆDeberta åˆ†è¯å™¨é€šè¿‡å‰é¢çš„ç©ºæ ¼æ£€æµ‹å•è¯çš„å¼€å¤´ï¼‰ã€‚

+   `add_bos_token`ï¼ˆ`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`False`ï¼‰--æ˜¯å¦å‘è¾“å…¥ä¸­æ·»åŠ é¦–å­—æ¯<|endoftext|>ã€‚è¿™æ ·å°±å¯ä»¥åƒå¯¹å¾…å…¶ä»–å•è¯ä¸€æ ·å¯¹å¾…å‰å¯¼è¯ã€‚

æ„å»ºä¸€ä¸ª DeBERTa åˆ†è¯å™¨ã€‚åŸºäºå­—èŠ‚çº§å­—èŠ‚å¯¹ç¼–ç ã€‚

è¿™ä¸ªåˆ†è¯å™¨å·²ç»è®­ç»ƒè¿‡ï¼Œå°†ç©ºæ ¼è§†ä¸ºæ ‡è®°çš„ä¸€éƒ¨åˆ†ï¼ˆæœ‰ç‚¹åƒ sentencepieceï¼‰ï¼Œæ‰€ä»¥ä¸€ä¸ªå•è¯ä¼š

åœ¨å¥å­å¼€å¤´ï¼ˆæ— ç©ºæ ¼ï¼‰æˆ–ä¸åœ¨å¥å­å¼€å¤´æ—¶ï¼Œå°†è¢«ç¼–ç ä¸ºä¸åŒçš„æ–¹å¼ï¼š

```py
>>> from transformers import DebertaTokenizer

>>> tokenizer = DebertaTokenizer.from_pretrained("microsoft/deberta-base")
>>> tokenizer("Hello world")["input_ids"]
[1, 31414, 232, 2]

>>> tokenizer(" Hello world")["input_ids"]
[1, 20920, 232, 2]
```

æ‚¨å¯ä»¥é€šè¿‡åœ¨å®ä¾‹åŒ–æ­¤åˆ†è¯å™¨æ—¶æˆ–åœ¨å¯¹æŸäº›æ–‡æœ¬è°ƒç”¨å®ƒæ—¶ä¼ é€’ `add_prefix_space=True` æ¥é¿å…è¿™ç§è¡Œä¸ºï¼Œä½†ç”±äºæ¨¡å‹ä¸æ˜¯ä»¥è¿™ç§æ–¹å¼è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

å½“ä¸ `is_split_into_words=True` ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œæ­¤åˆ†è¯å™¨å°†åœ¨æ¯ä¸ªå•è¯ä¹‹å‰æ·»åŠ ä¸€ä¸ªç©ºæ ¼ï¼ˆç”šè‡³ç¬¬ä¸€ä¸ªå•è¯ï¼‰ã€‚

è¿™ä¸ªåˆ†è¯å™¨ç»§æ‰¿è‡ª PreTrainedTokenizerï¼Œå…¶ä¸­åŒ…å«å¤§éƒ¨åˆ†ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒè¿™ä¸ªè¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

`build_inputs_with_special_tokens`å‡½æ•°

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/tokenization_deberta.py#L288)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0` (`List[int]`) â€” å°†æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„ ID åˆ—è¡¨ã€‚

+   `token_ids_1` (`List[int]`, *å¯é€‰*) â€” ç¬¬äºŒä¸ªåºåˆ—çš„ ID åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰ã€‚

è¿”å›

`List[int]`

å¸¦æœ‰é€‚å½“ç‰¹æ®Šæ ‡è®°çš„ input IDs åˆ—è¡¨ã€‚

é€šè¿‡è¿æ¥å’Œæ·»åŠ ç‰¹æ®Šæ ‡è®°ä»åºåˆ—æˆ–åºåˆ—å¯¹æ„å»ºç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹è¾“å…¥ã€‚DeBERTa åºåˆ—çš„æ ¼å¼å¦‚ä¸‹ï¼š

+   å•ä¸ªåºåˆ—ï¼š[CLS] X [SEP]

+   åºåˆ—å¯¹ï¼š[CLS] A [SEP] B [SEP]

`get_special_tokens_mask`å‡½æ•°

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/tokenization_deberta.py#L313)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0` (`List[int]`) â€” ID åˆ—è¡¨ã€‚

+   `token_ids_1` (`List[int]`, *å¯é€‰*) â€” ç¬¬äºŒä¸ªåºåˆ—å¯¹çš„ ID åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰ã€‚

+   `already_has_special_tokens` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ ‡è®°åˆ—è¡¨æ˜¯å¦å·²ç»æ ¼å¼åŒ–ä¸ºæ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°ã€‚

è¿”å›

`List[int]`

ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼ŒèŒƒå›´ä¸º[0, 1]ï¼š1 è¡¨ç¤ºç‰¹æ®Šæ ‡è®°ï¼Œ0 è¡¨ç¤ºåºåˆ—æ ‡è®°ã€‚

ä»æ²¡æœ‰æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„æ ‡è®°åˆ—è¡¨ä¸­æ£€ç´¢åºåˆ— IDã€‚å½“ä½¿ç”¨åˆ†è¯å™¨çš„ `prepare_for_model` æˆ– `encode_plus` æ–¹æ³•æ·»åŠ ç‰¹æ®Šæ ‡è®°æ—¶ï¼Œå°†è°ƒç”¨æ­¤æ–¹æ³•ã€‚

`create_token_type_ids_from_sequences`å‡½æ•°

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/tokenization_deberta.py#L340)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0` (`List[int]`) â€” ID åˆ—è¡¨ã€‚

+   `token_ids_1` (`List[int]`, *optional*) â€” åºåˆ—å¯¹çš„å¯é€‰ç¬¬äºŒä¸ª ID åˆ—è¡¨ã€‚

è¿”å›

`List[int]`

æ ¹æ®ç»™å®šåºåˆ—çš„ token ç±»å‹ ID åˆ—è¡¨ã€‚

ä»ä¼ é€’çš„ä¸¤ä¸ªåºåˆ—åˆ›å»ºä¸€ä¸ªç”¨äºåºåˆ—å¯¹åˆ†ç±»ä»»åŠ¡çš„æ©ç ã€‚DeBERTa

åºåˆ—å¯¹æ©ç çš„æ ¼å¼å¦‚ä¸‹ï¼š

```py
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |
```

å¦‚æœ `token_ids_1` ä¸º `None`ï¼Œåˆ™æ­¤æ–¹æ³•ä»…è¿”å›æ©ç çš„ç¬¬ä¸€éƒ¨åˆ†ï¼ˆ0ï¼‰ã€‚

#### `save_vocabulary`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/tokenization_deberta.py#L399)

```py
( save_directory: str filename_prefix: Optional = None )
```

## DebertaTokenizerFast

### `class transformers.DebertaTokenizerFast`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/tokenization_deberta_fast.py#L70)

```py
( vocab_file = None merges_file = None tokenizer_file = None errors = 'replace' bos_token = '[CLS]' eos_token = '[SEP]' sep_token = '[SEP]' cls_token = '[CLS]' unk_token = '[UNK]' pad_token = '[PAD]' mask_token = '[MASK]' add_prefix_space = False **kwargs )
```

å‚æ•°

+   `vocab_file` (`str`, *optional*) â€” è¯æ±‡æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `merges_file` (`str`, *optional*) â€” åˆå¹¶æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `tokenizer_file` (`str`, *optional*) â€” è¦ä½¿ç”¨çš„åˆ†è¯å™¨æ–‡ä»¶çš„è·¯å¾„ï¼Œè€Œä¸æ˜¯è¯æ±‡æ–‡ä»¶ã€‚

+   `errors` (`str`, *optional*, defaults to `"replace"`) â€” è§£ç å­—èŠ‚ä¸º UTF-8 æ—¶è¦éµå¾ªçš„èŒƒä¾‹ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)ã€‚

+   `bos_token` (`str`, *optional*, defaults to `"[CLS]"`) â€” åºåˆ—å¼€å§‹æ ‡è®°ã€‚

+   `eos_token` (`str`, *optional*, defaults to `"[SEP]"`) â€” åºåˆ—ç»“æŸæ ‡è®°ã€‚

+   `sep_token` (`str`, *optional*, defaults to `"[SEP]"`) â€” åˆ†éš”ç¬¦æ ‡è®°ï¼Œåœ¨ä»å¤šä¸ªåºåˆ—æ„å»ºåºåˆ—æ—¶ä½¿ç”¨ï¼Œä¾‹å¦‚ç”¨äºåºåˆ—åˆ†ç±»çš„ä¸¤ä¸ªåºåˆ—æˆ–ç”¨äºæ–‡æœ¬å’Œé—®é¢˜çš„é—®é¢˜å›ç­”ã€‚å®ƒè¿˜ç”¨ä½œä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºçš„åºåˆ—çš„æœ€åä¸€ä¸ªæ ‡è®°ã€‚

+   `cls_token` (`str`, *optional*, defaults to `"[CLS]"`) â€” åˆ†ç±»å™¨æ ‡è®°ï¼Œç”¨äºè¿›è¡Œåºåˆ—åˆ†ç±»ï¼ˆå¯¹æ•´ä¸ªåºåˆ—è¿›è¡Œåˆ†ç±»ï¼Œè€Œä¸æ˜¯æ¯ä¸ªæ ‡è®°è¿›è¡Œåˆ†ç±»ï¼‰ã€‚åœ¨ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ„å»ºæ—¶ï¼Œå®ƒæ˜¯åºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ã€‚

+   `unk_token` (`str`, *optional*, defaults to `"[UNK]"`) â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸º IDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚

+   `pad_token` (`str`, *optional*, defaults to `"[PAD]"`) â€” ç”¨äºå¡«å……çš„æ ‡è®°ï¼Œä¾‹å¦‚åœ¨æ‰¹å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—æ—¶ä½¿ç”¨ã€‚

+   `mask_token` (`str`, *optional*, defaults to `"[MASK]"`) â€” ç”¨äºå±è”½å€¼çš„æ ‡è®°ã€‚åœ¨ä½¿ç”¨æ©ç è¯­è¨€å»ºæ¨¡è®­ç»ƒæ­¤æ¨¡å‹æ—¶ä½¿ç”¨çš„æ ‡è®°ã€‚è¿™æ˜¯æ¨¡å‹å°†å°è¯•é¢„æµ‹çš„æ ‡è®°ã€‚

+   `add_prefix_space` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦å°†åˆå§‹ç©ºæ ¼æ·»åŠ åˆ°è¾“å…¥ã€‚è¿™å…è®¸å°†å‰å¯¼å•è¯è§†ä¸ºä»»ä½•å…¶ä»–å•è¯ã€‚ï¼ˆDeberta åˆ†è¯å™¨é€šè¿‡å‰é¢çš„ç©ºæ ¼æ£€æµ‹å•è¯çš„å¼€å¤´ï¼‰ã€‚

æ„å»ºä¸€ä¸ªâ€œå¿«é€Ÿâ€ DeBERTa åˆ†è¯å™¨ï¼ˆç”± HuggingFace çš„ *tokenizers* åº“æ”¯æŒï¼‰ã€‚åŸºäºå­—èŠ‚çº§å­—èŠ‚å¯¹ç¼–ç ã€‚

æ­¤åˆ†è¯å™¨ç»è¿‡è®­ç»ƒï¼Œå°†ç©ºæ ¼è§†ä¸ºæ ‡è®°çš„ä¸€éƒ¨åˆ†ï¼ˆæœ‰ç‚¹åƒ sentencepieceï¼‰ï¼Œå› æ­¤ä¸€ä¸ªå•è¯å°†

åœ¨å¥å­å¼€å¤´ï¼ˆæ— ç©ºæ ¼ï¼‰æˆ–ä¸åœ¨å¥å­å¼€å¤´æ—¶ï¼Œå°†è¢«ç¼–ç ä¸ºä¸åŒçš„æ–¹å¼ï¼š

```py
>>> from transformers import DebertaTokenizerFast

>>> tokenizer = DebertaTokenizerFast.from_pretrained("microsoft/deberta-base")
>>> tokenizer("Hello world")["input_ids"]
[1, 31414, 232, 2]

>>> tokenizer(" Hello world")["input_ids"]
[1, 20920, 232, 2]
```

æ‚¨å¯ä»¥é€šè¿‡åœ¨å®ä¾‹åŒ–æ­¤åˆ†è¯å™¨æ—¶ä¼ é€’ `add_prefix_space=True` æ¥é¿å…è¯¥è¡Œä¸ºï¼Œä½†ç”±äºæ¨¡å‹ä¸æ˜¯ä»¥è¿™ç§æ–¹å¼è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œå› æ­¤å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

å½“ä¸ `is_split_into_words=True` ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œéœ€è¦ä½¿ç”¨ `add_prefix_space=True` å®ä¾‹åŒ–æ­¤åˆ†è¯å™¨ã€‚

æ­¤åˆ†è¯å™¨ç»§æ‰¿è‡ª PreTrainedTokenizerFastï¼Œå…¶ä¸­åŒ…å«å¤§éƒ¨åˆ†ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

#### `build_inputs_with_special_tokens`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/tokenization_deberta_fast.py#L207)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0` (`List[int]`) â€” è¦æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„ ID åˆ—è¡¨ã€‚

+   `token_ids_1` (`List[int]`, *optional*) â€” å¯é€‰çš„ç¬¬äºŒä¸ªåºåˆ—å¯¹åº”çš„ ID åˆ—è¡¨ã€‚

è¿”å›

`List[int]`

å…·æœ‰é€‚å½“ç‰¹æ®Šæ ‡è®°çš„ input IDs åˆ—è¡¨ã€‚

é€šè¿‡è¿æ¥å’Œæ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œä»åºåˆ—æˆ–åºåˆ—å¯¹æ„å»ºæ¨¡å‹è¾“å…¥ï¼Œç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡ã€‚DeBERTa åºåˆ—çš„æ ¼å¼å¦‚ä¸‹ï¼š

+   å•ä¸ªåºåˆ—ï¼š[CLS] X [SEP]

+   åºåˆ—å¯¹ï¼š[CLS] A [SEP] B [SEP]

#### `create_token_type_ids_from_sequences`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/tokenization_deberta_fast.py#L232)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0` (`List[int]`) â€” ID åˆ—è¡¨ã€‚

+   `token_ids_1` (`List[int]`, *optional*) â€” å¯é€‰çš„ç¬¬äºŒä¸ªåºåˆ—å¯¹åº”çš„ ID åˆ—è¡¨ã€‚

è¿”å›

`List[int]`

æ ¹æ®ç»™å®šåºåˆ—çš„ token type IDs åˆ—è¡¨ã€‚

ä»ä¼ é€’çš„ä¸¤ä¸ªåºåˆ—åˆ›å»ºä¸€ä¸ªç”¨äºåºåˆ—å¯¹åˆ†ç±»ä»»åŠ¡çš„æ©ç ã€‚ä¸€ä¸ª DeBERTa

åºåˆ—å¯¹æ©ç çš„æ ¼å¼å¦‚ä¸‹ï¼š

```py
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |
```

å¦‚æœ`token_ids_1`ä¸º`None`ï¼Œæ­¤æ–¹æ³•åªè¿”å›æ©ç çš„ç¬¬ä¸€éƒ¨åˆ†ï¼ˆ0sï¼‰ã€‚

PytorchHide Pytorch å†…å®¹

## DebertaModel

### `class transformers.DebertaModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_deberta.py#L898)

```py
( config )
```

å‚æ•°

+   `config` (DebertaConfig) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„ DeBERTa æ¨¡å‹å˜å‹å™¨ï¼Œè¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚DeBERTa æ¨¡å‹æ˜¯ç”± Pengcheng Heï¼ŒXiaodong Liuï¼ŒJianfeng Gaoï¼ŒWeizhu Chen åœ¨[DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)ä¸­æå‡ºçš„ï¼Œå®ƒå»ºç«‹åœ¨ BERT/RoBERTa ä¹‹ä¸Šï¼Œå…·æœ‰ä¸¤ä¸ªæ”¹è¿›ï¼Œå³è§£è€¦çš„æ³¨æ„åŠ›å’Œå¢å¼ºçš„æ©ç è§£ç å™¨ã€‚é€šè¿‡è¿™ä¸¤ä¸ªæ”¹è¿›ï¼Œå®ƒåœ¨ 80GB çš„é¢„è®­ç»ƒæ•°æ®ä¸Šèƒœè¿‡ BERT/RoBERTa çš„å¤§å¤šæ•°ä»»åŠ¡ã€‚

æ­¤æ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ä¿¡æ¯ã€‚

`forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_deberta.py#L926)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼‰ â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯ input IDs?

+   `attention_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤º`æœªè¢«æ©ç `çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤º`è¢«æ©ç `çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*) â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯ token type IDs?

+   `position_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰çš„*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰çš„*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¼ é€’ä¸€ä¸ªåµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†*input_ids*ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `output_attentions` (`bool`, *å¯é€‰çš„*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰çš„*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *å¯é€‰çš„*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

è¿”å›

transformers.modeling_outputs.BaseModelOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.BaseModelOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆDebertaConfigï¼‰å’Œè¾“å…¥ã€‚

+   `last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€çš„åºåˆ—ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰çš„*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨è‡ªæ³¨æ„åŠ›å¤´ä¸­ç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼çš„æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ã€‚

DebertaModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤ä¹‹åè°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, DebertaModel
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-base")
>>> model = DebertaModel.from_pretrained("microsoft/deberta-base")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## DebertaPreTrainedModel

### `class transformers.DebertaPreTrainedModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_deberta.py#L812)

```py
( config: PretrainedConfig *inputs **kwargs )
```

ä¸€ä¸ªå¤„ç†æƒé‡åˆå§‹åŒ–å’Œä¸‹è½½å’ŒåŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„ç®€å•æ¥å£çš„æŠ½è±¡ç±»ã€‚

## DebertaForMaskedLM

### `class transformers.DebertaForMaskedLM`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_deberta.py#L1013)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆDebertaConfigï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

åœ¨é¡¶éƒ¨å¸¦æœ‰`è¯­è¨€å»ºæ¨¡`å¤´çš„ DeBERTa æ¨¡å‹ã€‚DeBERTa æ¨¡å‹ç”±ä½•é¹ç¨‹ã€åˆ˜æ™“ä¸œã€é«˜å»ºå³°ã€é™ˆä¼Ÿé“¸åœ¨[DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)ä¸­æå‡ºã€‚å®ƒåœ¨ BERT/RoBERTa çš„åŸºç¡€ä¸Šè¿›è¡Œäº†ä¸¤é¡¹æ”¹è¿›ï¼Œå³è§£è€¦æ³¨æ„åŠ›å’Œå¢å¼ºæ©ç è§£ç å™¨ã€‚é€šè¿‡è¿™ä¸¤é¡¹æ”¹è¿›ï¼Œå®ƒåœ¨ 80GB é¢„è®­ç»ƒæ•°æ®ä¸Šä¼˜äº BERT/RoBERTa çš„å¤§å¤šæ•°ä»»åŠ¡ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_deberta.py#L1032)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.MaskedLMOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ï¼š

    +   1 ä»£è¡¨`not masked`çš„æ ‡è®°ã€‚

    +   0 ä»£è¡¨`masked`çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•é€‰æ‹©åœ¨`[0, 1]`ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.max_position_embeddings - 1]`ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `inputs_embeds`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†*input_ids*ç´¢å¼•è½¬æ¢ä¸ºå…³è”å‘é‡ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *å¯é€‰çš„*) â€” ç”¨äºè®¡ç®—æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[-100, 0, ..., config.vocab_size]`å†…ï¼ˆå‚è§`input_ids`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰æ ‡ç­¾åœ¨`[0, ..., config.vocab_size]`ä¸­çš„æ ‡è®°ã€‚

è¿”å›

transformers.modeling_outputs.MaskedLMOutput æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.MaskedLMOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆDebertaConfigï¼‰å’Œè¾“å…¥ã€‚

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *å¯é€‰çš„*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›) â€” æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰æŸå¤±ã€‚

+   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

DebertaForMaskedLM çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤ä¹‹åè°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, DebertaForMaskedLM
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("lsanochkin/deberta-large-feedback")
>>> model = DebertaForMaskedLM.from_pretrained("lsanochkin/deberta-large-feedback")

>>> inputs = tokenizer("The capital of France is [MASK].", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # retrieve index of [MASK]
>>> mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

>>> predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)
>>> tokenizer.decode(predicted_token_id)
' Paris'

>>> labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]
>>> # mask labels of non-[MASK] tokens
>>> labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

>>> outputs = model(**inputs, labels=labels)
>>> round(outputs.loss.item(), 2)
0.54
```

## DebertaForSequenceClassification

### `class transformers.DebertaForSequenceClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_deberta.py#L1144)

```py
( config )
```

å‚æ•°

+   `config` (DebertaConfig) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

DeBERTa æ¨¡å‹å˜æ¢å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰åºåˆ—åˆ†ç±»/å›å½’å¤´ï¼ˆæ± åŒ–è¾“å‡ºé¡¶éƒ¨çš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äº GLUE ä»»åŠ¡ã€‚

DeBERTa æ¨¡å‹æ˜¯ç”± Pengcheng Heã€Xiaodong Liuã€Jianfeng Gaoã€Weizhu Chen åœ¨[DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)ä¸­æå‡ºçš„ã€‚å®ƒæ˜¯åœ¨ BERT/RoBERTa çš„åŸºç¡€ä¸Šè¿›è¡Œäº†ä¸¤é¡¹æ”¹è¿›ï¼Œå³è§£ç¼ æ³¨æ„åŠ›å’Œå¢å¼ºæ©ç è§£ç å™¨ã€‚é€šè¿‡è¿™ä¸¤é¡¹æ”¹è¿›ï¼Œå®ƒåœ¨ 80GB é¢„è®­ç»ƒæ•°æ®ä¸Šçš„å¤§å¤šæ•°ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äº BERT/RoBERTaã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_deberta.py#L1176)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`èŒƒå›´å†…ï¼š

    +   å¯¹äºæœªè¢«â€œmaskedâ€æ‰çš„æ ‡è®°ä¸º 1ï¼Œ

    +   å¯¹äºè¢«`masked`æ‰çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`èŒƒå›´å†…é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.max_position_embeddings - 1]`ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†*input_ids*ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

transformers.modeling_outputs.SequenceClassifierOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.SequenceClassifierOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆDebertaConfigï¼‰å’Œè¾“å…¥ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” åˆ†ç±»ï¼ˆæˆ–å›å½’ï¼Œå¦‚æœ config.num_labels==1ï¼‰æŸå¤±ã€‚

+   `logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€” åˆ†ç±»ï¼ˆå¦‚æœ config.num_labels==1 åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚çš„è¾“å‡ºï¼Œåˆ™ä¸ºåµŒå…¥å±‚çš„è¾“å‡º+æ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨è‡ªæ³¨æ„åŠ›å¤´ä¸­ä½¿ç”¨çš„æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼ã€‚

DebertaForSequenceClassification çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

å•æ ‡ç­¾åˆ†ç±»ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from transformers import AutoTokenizer, DebertaForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-base")
>>> model = DebertaForSequenceClassification.from_pretrained("microsoft/deberta-base")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_id = logits.argmax().item()

>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = DebertaForSequenceClassification.from_pretrained("microsoft/deberta-base", num_labels=num_labels)

>>> labels = torch.tensor([1])
>>> loss = model(**inputs, labels=labels).loss
```

å¤šæ ‡ç­¾åˆ†ç±»ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from transformers import AutoTokenizer, DebertaForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-base")
>>> model = DebertaForSequenceClassification.from_pretrained("microsoft/deberta-base", problem_type="multi_label_classification")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]

>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = DebertaForSequenceClassification.from_pretrained(
...     "microsoft/deberta-base", num_labels=num_labels, problem_type="multi_label_classification"
... )

>>> labels = torch.sum(
...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1
... ).to(torch.float)
>>> loss = model(**inputs, labels=labels).loss
```

## DebertaForTokenClassification

### `class transformers.DebertaForTokenClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_deberta.py#L1262)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆDebertaConfigï¼‰ â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

åœ¨ DeBERTa æ¨¡å‹çš„é¡¶éƒ¨å¸¦æœ‰ä¸€ä¸ªæ ‡è®°åˆ†ç±»å¤´éƒ¨ï¼ˆéšè—çŠ¶æ€è¾“å‡ºçš„çº¿æ€§å±‚ï¼‰çš„ DeBERTa æ¨¡å‹ï¼Œä¾‹å¦‚ç”¨äºå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ã€‚

DeBERTa æ¨¡å‹æ˜¯ç”± Pengcheng Heã€Xiaodong Liuã€Jianfeng Gaoã€Weizhu Chen åœ¨[DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)ä¸­æå‡ºçš„ã€‚å®ƒåœ¨ BERT/RoBERTa çš„åŸºç¡€ä¸Šè¿›è¡Œäº†ä¸¤é¡¹æ”¹è¿›ï¼Œå³è§£è€¦çš„æ³¨æ„åŠ›å’Œå¢å¼ºçš„æ©ç è§£ç å™¨ã€‚é€šè¿‡è¿™ä¸¤é¡¹æ”¹è¿›ï¼Œå®ƒåœ¨ 80GB çš„é¢„è®­ç»ƒæ•°æ®ä¸Šä¼˜äº BERT/RoBERTa çš„å¤§å¤šæ•°ä»»åŠ¡ã€‚

è¯¥æ¨¡å‹ä¹Ÿæ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_deberta.py#L1281)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰ â€” è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *å¯é€‰*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º 1ï¼Œ

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º 0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†*input_ids*ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—æ ‡è®°åˆ†ç±»æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚

è¿”å›

transformers.modeling_outputs.TokenClassifierOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.TokenClassifierOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆDebertaConfigï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” åˆ†ç±»æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.num_labels)`çš„`torch.FloatTensor`ï¼‰â€” åˆ†ç±»åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰ä¸€ä¸ªåµŒå…¥å±‚ï¼Œ+ ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨è‡ªæ³¨æ„åŠ›å¤´ä¸­ç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼çš„æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ã€‚

DebertaForTokenClassification çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, DebertaForTokenClassification
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-base")
>>> model = DebertaForTokenClassification.from_pretrained("microsoft/deberta-base")

>>> inputs = tokenizer(
...     "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="pt"
... )

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_token_class_ids = logits.argmax(-1)

>>> # Note that tokens are classified rather then input words which means that
>>> # there might be more predicted token classes than words.
>>> # Multiple token classes might account for the same word
>>> predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]

>>> labels = predicted_token_class_ids
>>> loss = model(**inputs, labels=labels).loss
```

## DebertaForQuestionAnswering

### `class transformers.DebertaForQuestionAnswering`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_deberta.py#L1335)

```py
( config )
```

å‚æ•°

+   `config` (DebertaConfig) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

DeBERTa æ¨¡å‹åœ¨é¡¶éƒ¨å…·æœ‰ç”¨äºæå–å¼é—®ç­”ä»»åŠ¡ï¼ˆå¦‚ SQuADï¼‰çš„è·¨åº¦åˆ†ç±»å¤´ï¼ˆåœ¨éšè—çŠ¶æ€è¾“å‡ºçš„çº¿æ€§å±‚ä¸Šè®¡ç®—`span start logits`å’Œ`span end logits`ï¼‰ã€‚

DeBERTa æ¨¡å‹ç”± Pengcheng Heã€Xiaodong Liuã€Jianfeng Gaoã€Weizhu Chen åœ¨[DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)ä¸­æå‡ºã€‚å®ƒåœ¨ BERT/RoBERTa çš„åŸºç¡€ä¸Šè¿›è¡Œäº†ä¸¤é¡¹æ”¹è¿›ï¼Œå³è§£è€¦æ³¨æ„åŠ›å’Œå¢å¼ºæ©ç è§£ç å™¨ã€‚é€šè¿‡è¿™ä¸¤é¡¹æ”¹è¿›ï¼Œå®ƒåœ¨ 80GB é¢„è®­ç»ƒæ•°æ®ä¸Šçš„å¤§å¤šæ•°ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äº BERT/RoBERTaã€‚

è¯¥æ¨¡å‹ä¹Ÿæ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_deberta.py#L1353)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.QuestionAnsweringModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©åœ¨`[0, 1]`èŒƒå›´å†…çš„æ©ç å€¼ï¼š

    +   1 è¡¨ç¤ºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢«`masked`çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒï¼Œä»¥ä¾¿å°†*input_ids*ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

+   `start_positions`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè®¡ç®—æ ‡è®°è·¨åº¦çš„èµ·å§‹ä½ç½®ï¼ˆç´¢å¼•ï¼‰çš„æ ‡ç­¾ã€‚ä½ç½®è¢«å¤¹ç´§åˆ°åºåˆ—çš„é•¿åº¦ï¼ˆ`sequence_length`ï¼‰ã€‚åºåˆ—å¤–çš„ä½ç½®ä¸è®¡å…¥æŸå¤±è®¡ç®—ã€‚

+   `end_positions`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè®¡ç®—æ ‡è®°è·¨åº¦çš„ç»“æŸä½ç½®ï¼ˆç´¢å¼•ï¼‰çš„æ ‡ç­¾ã€‚ä½ç½®è¢«å¤¹ç´§åˆ°åºåˆ—çš„é•¿åº¦ï¼ˆ`sequence_length`ï¼‰ã€‚åºåˆ—å¤–çš„ä½ç½®ä¸è®¡å…¥æŸå¤±è®¡ç®—ã€‚

è¿”å›

transformers.modeling_outputs.QuestionAnsweringModelOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.QuestionAnsweringModelOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆDebertaConfigï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰- æ€»è·¨åº¦æå–æŸå¤±æ˜¯èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„äº¤å‰ç†µä¹‹å’Œã€‚

+   `start_logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰- è·¨åº¦å¼€å§‹å¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `end_logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰- è·¨åº¦ç»“æŸå¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹æ¯å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

DebertaForQuestionAnswering çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, DebertaForQuestionAnswering
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("Palak/microsoft_deberta-large_squad")
>>> model = DebertaForQuestionAnswering.from_pretrained("Palak/microsoft_deberta-large_squad")

>>> question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

>>> inputs = tokenizer(question, text, return_tensors="pt")
>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> answer_start_index = outputs.start_logits.argmax()
>>> answer_end_index = outputs.end_logits.argmax()

>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
>>> tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)
' a nice puppet'

>>> # target is "nice puppet"
>>> target_start_index = torch.tensor([12])
>>> target_end_index = torch.tensor([14])

>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
>>> loss = outputs.loss
>>> round(loss.item(), 2)
0.14
```

TensorFlow éšè— TensorFlow å†…å®¹

## TFDebertaModel

### `class transformers.TFDebertaModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_tf_deberta.py#L1236)

```py
( config: DebertaConfig *inputs **kwargs )
```

å‚æ•°

+   `config`ï¼ˆDebertaConfigï¼‰- æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸ DeBERTa æ¨¡å‹å˜å‹å™¨è¾“å‡ºæ²¡æœ‰ç‰¹å®šå¤´éƒ¨çš„åŸå§‹éšè—çŠ¶æ€ã€‚DeBERTa æ¨¡å‹æ˜¯ç”± Pengcheng Heã€Xiaodong Liuã€Jianfeng Gaoã€Weizhu Chen åœ¨ [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) ä¸­æå‡ºçš„ã€‚å®ƒåœ¨ BERT/RoBERTa çš„åŸºç¡€ä¸Šè¿›è¡Œäº†ä¸¤é¡¹æ”¹è¿›ï¼Œå³è§£ç¼ æ³¨æ„åŠ›å’Œå¢å¼ºæ©ç è§£ç å™¨ã€‚é€šè¿‡è¿™ä¸¤é¡¹æ”¹è¿›ï¼Œå®ƒåœ¨ä½¿ç”¨ 80GB é¢„è®­ç»ƒæ•°æ®çš„å¤§å¤šæ•°ä»»åŠ¡ä¸Šä¼˜äº BERT/RoBERTaã€‚

è¯¥æ¨¡å‹ä¹Ÿæ˜¯ [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

`transformers` ä¸­çš„ TensorFlow æ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äº PyTorch æ¨¡å‹ï¼‰ï¼Œæˆ–

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ Keras æ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºè¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨ `model.fit()` ç­‰æ–¹æ³•æ—¶ï¼Œæ‚¨åº”è¯¥å¯ä»¥â€œè½»æ¾ä½¿ç”¨â€ - åªéœ€ä»¥ `model.fit()` æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’è¾“å…¥å’Œæ ‡ç­¾å³å¯ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Keras æ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œæ¯”å¦‚åœ¨ä½¿ç”¨ Keras `Functional` API åˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†æ‰€æœ‰è¾“å…¥å¼ é‡æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ï¼š

+   ä¸€ä¸ªä»…åŒ…å« `input_ids` çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`

+   ä¸€ä¸ªé•¿åº¦å¯å˜çš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«æŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºçš„ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼š`model([input_ids, attention_mask])` æˆ– `model([input_ids, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨ [å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨ä¸éœ€è¦æ‹…å¿ƒè¿™äº›é—®é¢˜ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå°†è¾“å…¥ä¼ é€’ç»™ä»»ä½•å…¶ä»– Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

`call`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_tf_deberta.py#L1246)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]` æˆ– `Dict[str, np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º `(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode() å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`np.ndarray` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   1 ç”¨äºæœªè¢« `masked` çš„æ ‡è®°ï¼Œ

    +   0 ç”¨äºè¢« `masked` çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids` (`np.ndarray` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” åˆ†æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº *å¥å­ A* æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº *å¥å­ B* æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids` (`np.ndarray` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´ `[0, config.max_position_embeddings - 1]` ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `inputs_embeds` (`np.ndarray` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†*input_ids*ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[`~utils.ModelOutput`]è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

è¿”å›å€¼

transformers.modeling_tf_outputs.TFBaseModelOutput æˆ– `tuple(tf.Tensor)`

ä¸€ä¸ª transformers.modeling_tf_outputs.TFBaseModelOutput æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆDebertaConfigï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `last_hidden_state` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `hidden_states` (`tuple(tf.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

TFDebertaModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹è€Œä¸æ˜¯è¿™ä¸ªå‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, TFDebertaModel
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("kamalkraj/deberta-base")
>>> model = TFDebertaModel.from_pretrained("kamalkraj/deberta-base")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
>>> outputs = model(inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## TFDebertaPreTrainedModel

### `class transformers.TFDebertaPreTrainedModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_tf_deberta.py#L1140)

```py
( *args **kwargs )
```

ä¸€ä¸ªå¤„ç†æƒé‡åˆå§‹åŒ–å’Œä¸‹è½½å’ŒåŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„ç®€å•æ¥å£çš„æŠ½è±¡ç±»ã€‚

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/keras/src/engine/training.py#L592)

```py
( inputs training = None mask = None )
```

å¯¹æ–°è¾“å…¥è°ƒç”¨æ¨¡å‹å¹¶å°†è¾“å‡ºä½œä¸ºå¼ é‡è¿”å›ã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`call()`åªæ˜¯é‡æ–°åº”ç”¨å›¾ä¸­çš„æ‰€æœ‰æ“ä½œåˆ°æ–°çš„è¾“å…¥ä¸Šï¼ˆä¾‹å¦‚ï¼Œä»æä¾›çš„è¾“å…¥æ„å»ºä¸€ä¸ªæ–°çš„è®¡ç®—å›¾ï¼‰ã€‚

æ³¨æ„ï¼šä¸åº”ç›´æ¥è°ƒç”¨æ­¤æ–¹æ³•ã€‚åªæœ‰åœ¨å­ç±»åŒ–`tf.keras.Model`æ—¶æ‰åº”è¯¥è¢«è¦†ç›–ã€‚è¦åœ¨è¾“å…¥ä¸Šè°ƒç”¨æ¨¡å‹ï¼Œå§‹ç»ˆä½¿ç”¨`__call__()`æ–¹æ³•ï¼Œå³`model(inputs)`ï¼Œå®ƒä¾èµ–äºåº•å±‚çš„`call()`æ–¹æ³•ã€‚

## TFDebertaForMaskedLM

### `class transformers.TFDebertaForMaskedLM`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_tf_deberta.py#L1288)

```py
( config: DebertaConfig *inputs **kwargs )
```

å‚æ•°

+   `config`ï¼ˆDebertaConfigï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

DeBERTa æ¨¡å‹åœ¨é¡¶éƒ¨å¸¦æœ‰ä¸€ä¸ª`è¯­è¨€å»ºæ¨¡`å¤´ã€‚DeBERTa æ¨¡å‹æ˜¯ç”± Pengcheng Heã€Xiaodong Liuã€Jianfeng Gaoã€Weizhu Chen åœ¨[DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)ä¸­æå‡ºçš„ã€‚å®ƒæ˜¯åœ¨ BERT/RoBERTa çš„åŸºç¡€ä¸Šè¿›è¡Œäº†ä¸¤é¡¹æ”¹è¿›ï¼Œå³è§£è€¦æ³¨æ„åŠ›å’Œå¢å¼ºæ©ç è§£ç å™¨ã€‚é€šè¿‡è¿™ä¸¤é¡¹æ”¹è¿›ï¼Œå®ƒåœ¨ 80GB é¢„è®­ç»ƒæ•°æ®ä¸Šçš„å¤§å¤šæ•°ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äº BERT/RoBERTaã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

`transformers`ä¸­çš„ TensorFlow æ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äº PyTorch æ¨¡å‹ï¼‰ï¼Œæˆ–

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ï¼ŒKeras æ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºè¿™ç§æ”¯æŒï¼Œåœ¨ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯ä»¥â€œæ­£å¸¸å·¥ä½œâ€ - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’è¾“å…¥å’Œæ ‡ç­¾ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Keras æ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œä¾‹å¦‚åœ¨ä½¿ç”¨ Keras `Functional` API åˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ç”¨äºåœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­æ”¶é›†æ‰€æœ‰è¾“å…¥å¼ é‡ï¼š

+   ä¸€ä¸ªåªåŒ…å«`input_ids`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`

+   ä¸€ä¸ªé•¿åº¦ä¸å®šçš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šé¡ºåºçš„è¾“å…¥å¼ é‡ï¼š`model([input_ids, attention_mask])`æˆ–`model([input_ids, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šè¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒè¿™äº›é—®é¢˜ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…ä»»ä½•å…¶ä»– Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_tf_deberta.py#L1305)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFMaskedLMOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `input_ids`ï¼ˆ`np.ndarray`ï¼Œ`tf.Tensor`ï¼Œ`List[tf.Tensor]` ``Dict[str, tf.Tensor]`æˆ–`Dict[str, np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º`(batch_size, sequence_length)`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š

    +   å¯¹äº`æœªè¢«æ©ç `çš„æ ‡è®°ä¸º 1ï¼Œ

    +   å¯¹äº`è¢«æ©ç `çš„æ ‡è®°ä¸º 0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids` (`np.ndarray` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” æ®µæ ‡è®°ç´¢å¼•ï¼Œç”¨äºæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº *å¥å­ A* æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº *å¥å­ B* æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids` (`np.ndarray` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´ `[0, config.max_position_embeddings - 1]` ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `inputs_embeds` (`np.ndarray` æˆ– `tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’ `input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒï¼Œä»¥ä¾¿å°† *input_ids* ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚

+   `output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„ `attentions`ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª [`~utils.ModelOutputâ€œ] è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

+   `æ ‡ç­¾` (`tf.Tensor` æˆ– `np.ndarray`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨ `[-100, 0, ..., config.vocab_size]` å†…ï¼ˆå‚è§ `input_ids` æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º `-100` çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰æ ‡ç­¾åœ¨ `[0, ..., config.vocab_size]` å†…çš„æ ‡è®°ã€‚

è¿”å›

transformers.modeling_tf_outputs.TFMaskedLMOutput æˆ– `tuple(tf.Tensor)`

ä¸€ä¸ª transformers.modeling_tf_outputs.TFMaskedLMOutput æˆ–ä¸€ä¸ª `tf.Tensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ– `config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆDebertaConfigï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(n,)`ï¼Œ*å¯é€‰*ï¼Œå…¶ä¸­ n æ˜¯éæ©ç æ ‡ç­¾çš„æ•°é‡ï¼Œå½“æä¾› `labels` æ—¶è¿”å›ï¼‰ â€” æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰æŸå¤±ã€‚

+   `logits` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states` (`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ– `config.output_hidden_states=True` æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `tf.Tensor` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_attentions=True` æˆ– `config.output_attentions=True` æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `tf.Tensor` å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

TFDebertaForMaskedLM çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, TFDebertaForMaskedLM
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("kamalkraj/deberta-base")
>>> model = TFDebertaForMaskedLM.from_pretrained("kamalkraj/deberta-base")

>>> inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
>>> logits = model(**inputs).logits

>>> # retrieve index of [MASK]
>>> mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])
>>> selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)

>>> predicted_token_id = tf.math.argmax(selected_logits, axis=-1)
```

```py
>>> labels = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]
>>> # mask labels of non-[MASK] tokens
>>> labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

>>> outputs = model(**inputs, labels=labels)
```

## TFDebertaForSequenceClassification

### `class transformers.TFDebertaForSequenceClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_tf_deberta.py#L1369)

```py
( config: DebertaConfig *inputs **kwargs )
```

å‚æ•°

+   `config`ï¼ˆDebertaConfigï¼‰- å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

DeBERTa æ¨¡å‹å˜å‹å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰åºåˆ—åˆ†ç±»/å›å½’å¤´ï¼ˆæ± åŒ–è¾“å‡ºçš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äº GLUE ä»»åŠ¡ã€‚

DeBERTa æ¨¡å‹æ˜¯ç”± Pengcheng Heï¼ŒXiaodong Liuï¼ŒJianfeng Gaoï¼ŒWeizhu Chen åœ¨[DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)ä¸­æå‡ºçš„ã€‚å®ƒåœ¨ BERT/RoBERTa çš„åŸºç¡€ä¸Šè¿›è¡Œäº†ä¸¤é¡¹æ”¹è¿›ï¼Œå³è§£ç¼ æ³¨æ„åŠ›å’Œå¢å¼ºæ©ç è§£ç å™¨ã€‚é€šè¿‡è¿™ä¸¤é¡¹æ”¹è¿›ï¼Œå®ƒåœ¨ 80GB é¢„è®­ç»ƒæ•°æ®ä¸Šçš„å¤§å¤šæ•°ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äº BERT/RoBERTaã€‚

è¯¥æ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

åœ¨`transformers`ä¸­ï¼ŒTensorFlow æ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§è¾“å…¥æ ¼å¼ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äº PyTorch æ¨¡å‹ï¼‰ï¼Œæˆ–

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ï¼Œå½“å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶ï¼ŒKeras æ–¹æ³•æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºè¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯ä»¥æ­£å¸¸å·¥ä½œ - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’è¾“å…¥å’Œæ ‡ç­¾å³å¯ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Keras æ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œä¾‹å¦‚åœ¨ä½¿ç”¨ Keras`Functional` API åˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ç”¨äºæ”¶é›†ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­çš„æ‰€æœ‰è¾“å…¥å¼ é‡ï¼š

+   ä¸€ä¸ªä»…åŒ…å«`input_ids`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`

+   ä¸€ä¸ªé•¿åº¦ä¸å®šçš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼ŒæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºï¼š`model([input_ids, attention_mask])`æˆ–`model([input_ids, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒè¿™äº›é—®é¢˜ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…ä»»ä½•å…¶ä»– Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_tf_deberta.py#L1395)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSequenceClassifierOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `input_ids`ï¼ˆ`np.ndarray`ï¼Œ`tf.Tensor`ï¼Œ`List[tf.Tensor]`ï¼Œ`Dict[str, tf.Tensor]`æˆ–`Dict[str, np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º`(batch_size, sequence_length)`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`np.ndarray` æˆ– `tf.Tensor` of shape `(batch_size, sequence_length)`, *å¯é€‰*) â€” ç”¨äºé¿å…åœ¨å¡«å……ä»¤ç‰Œç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0, 1]`ï¼š

    +   1 è¡¨ç¤ºæœªè¢«æ©ç›–çš„ä»¤ç‰Œï¼Œ

    +   0 è¡¨ç¤ºè¢«`masked`çš„ä»¤ç‰Œã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids` (`np.ndarray` æˆ– `tf.Tensor` of shape `(batch_size, sequence_length)`, *å¯é€‰*) â€” æ®µä»¤ç‰Œç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*ä»¤ç‰Œï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*ä»¤ç‰Œã€‚

    ä»€ä¹ˆæ˜¯ä»¤ç‰Œç±»å‹ IDï¼Ÿ

+   `position_ids` (`np.ndarray` æˆ– `tf.Tensor` of shape `(batch_size, sequence_length)`, *å¯é€‰*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—ä»¤ç‰Œåœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `inputs_embeds` (`np.ndarray` æˆ– `tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *å¯é€‰*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒæ¥å°†*input_ids*ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚

+   `output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›[`~utils.ModelOutput`]è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels` (`tf.Tensor` æˆ– `np.ndarray` of shape `(batch_size,)`, *å¯é€‰*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

transformers.modeling_tf_outputs.TFSequenceClassifierOutput æˆ– `tuple(tf.Tensor)`

ä¸€ä¸ª transformers.modeling_tf_outputs.TFSequenceClassifierOutput æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œè¿™å–å†³äºé…ç½®ï¼ˆDebertaConfigï¼‰å’Œè¾“å…¥ã€‚

+   `loss` (`tf.Tensor` of shape `(batch_size, )`, *å¯é€‰*, å½“æä¾›`labels`æ—¶è¿”å›) â€” åˆ†ç±»ï¼ˆå¦‚æœ config.num_labels==1 åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) â€” åˆ†ç±»ï¼ˆå¦‚æœ config.num_labels==1 åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(tf.Tensor)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¯å±‚æ¨¡å‹çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(tf.Tensor)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨è‡ªæ³¨æ„åŠ›å¤´ä¸­ç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼çš„æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ã€‚

TFDebertaForSequenceClassification çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, TFDebertaForSequenceClassification
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("kamalkraj/deberta-base")
>>> model = TFDebertaForSequenceClassification.from_pretrained("kamalkraj/deberta-base")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")

>>> logits = model(**inputs).logits

>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])
```

```py
>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = TFDebertaForSequenceClassification.from_pretrained("kamalkraj/deberta-base", num_labels=num_labels)

>>> labels = tf.constant(1)
>>> loss = model(**inputs, labels=labels).loss
```

## TFDebertaForTokenClassification

### `class transformers.TFDebertaForTokenClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_tf_deberta.py#L1468)

```py
( config: DebertaConfig *inputs **kwargs )
```

å‚æ•°

+   `config`ï¼ˆDebertaConfigï¼‰-æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

DeBERTa æ¨¡å‹åœ¨é¡¶éƒ¨æœ‰ä¸€ä¸ªæ ‡è®°åˆ†ç±»å¤´éƒ¨ï¼ˆéšè—çŠ¶æ€è¾“å‡ºçš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ã€‚

DeBERTa æ¨¡å‹æ˜¯ç”± Pengcheng Heã€Xiaodong Liuã€Jianfeng Gaoã€Weizhu Chen åœ¨[DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)ä¸­æå‡ºçš„ã€‚å®ƒåœ¨ BERT/RoBERTa çš„åŸºç¡€ä¸Šè¿›è¡Œäº†ä¸¤é¡¹æ”¹è¿›ï¼Œå³è§£ç¼ æ³¨æ„åŠ›å’Œå¢å¼ºæ©ç è§£ç å™¨ã€‚é€šè¿‡è¿™ä¸¤é¡¹æ”¹è¿›ï¼Œå®ƒåœ¨ 80GB é¢„è®­ç»ƒæ•°æ®ä¸Šçš„å¤§å¤šæ•°ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äº BERT/RoBERTaã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

TensorFlow æ¨¡å‹å’Œå±‚åœ¨`transformers`ä¸­æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äº PyTorch æ¨¡å‹ï¼‰ï¼Œæˆ–

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ï¼ŒKeras æ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºæœ‰äº†è¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯ä»¥â€œæ­£å¸¸å·¥ä½œâ€ - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’è¾“å…¥å’Œæ ‡ç­¾å³å¯ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Keras æ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œæ¯”å¦‚åœ¨ä½¿ç”¨ Keras `Functional` API åˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­çš„æ‰€æœ‰è¾“å…¥å¼ é‡ï¼š

+   åªæœ‰ä¸€ä¸ªåŒ…å«`input_ids`çš„å¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`

+   ä¸€ä¸ªé•¿åº¦ä¸å®šçš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼ŒæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºï¼š`model([input_ids, attention_mask])`æˆ–`model([input_ids, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨ä¸éœ€è¦æ‹…å¿ƒè¿™äº›é—®é¢˜ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå°†è¾“å…¥ä¼ é€’ç»™ä»»ä½•å…¶ä»– Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_tf_deberta.py#L1488)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFTokenClassifierOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `input_ids`ï¼ˆ`np.ndarray`ã€`tf.Tensor`ã€`List[tf.Tensor]`ã€`Dict[str, tf.Tensor]`æˆ–`Dict[str, np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º`(batch_size, sequence_length)`ï¼‰-è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯ input IDs?

+   `attention_mask` (`np.ndarray`æˆ–å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`ï¼Œ*optional*) â€” é¿å…åœ¨å¡«å…… token ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºæœªè¢«`masked`çš„ tokenï¼Œ

    +   0 è¡¨ç¤ºè¢«`masked`çš„ tokenã€‚

    ä»€ä¹ˆæ˜¯ attention masks?

+   `token_type_ids` (`np.ndarray`æˆ–å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`ï¼Œ*optional*) â€” æŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†çš„æ®µ token ç´¢å¼•ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*sentence A* tokenï¼Œ

    +   1 å¯¹åº”äº*sentence B* tokenã€‚

    ä»€ä¹ˆæ˜¯ token type IDs?

+   `position_ids` (`np.ndarray`æˆ–å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`ï¼Œ*optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ— token çš„ä½ç½®åµŒå…¥çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ position IDs?

+   `inputs_embeds` (`np.ndarray`æˆ–å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`ï¼Œ*optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†*input_ids*ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›[`~utils.ModelOutput`]è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels` (`tf.Tensor`æˆ–å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`np.ndarray`ï¼Œ*optional*) â€” ç”¨äºè®¡ç®— token åˆ†ç±»æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`ä¸­ã€‚

è¿”å›

transformers.modeling_tf_outputs.TFTokenClassifierOutput æˆ–`tuple(tf.Tensor)`

transformers.modeling_tf_outputs.TFTokenClassifierOutput æˆ–åŒ…å«å„ç§å…ƒç´ çš„`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰å–å†³äºé…ç½®ï¼ˆDebertaConfigï¼‰å’Œè¾“å…¥ã€‚

+   `loss` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(n,)`ï¼Œ*optional*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›) â€” åˆ†ç±»æŸå¤±ã€‚

+   `logits` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.num_labels)`) â€” SoftMax ä¹‹å‰çš„åˆ†ç±»åˆ†æ•°ã€‚

+   `hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¯ä¸ªå±‚çš„æ¨¡å‹çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨è‡ªæ³¨æ„åŠ›å¤´ä¸­ç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼çš„æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ã€‚

TFDebertaForTokenClassification çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, TFDebertaForTokenClassification
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("kamalkraj/deberta-base")
>>> model = TFDebertaForTokenClassification.from_pretrained("kamalkraj/deberta-base")

>>> inputs = tokenizer(
...     "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="tf"
... )

>>> logits = model(**inputs).logits
>>> predicted_token_class_ids = tf.math.argmax(logits, axis=-1)

>>> # Note that tokens are classified rather then input words which means that
>>> # there might be more predicted token classes than words.
>>> # Multiple token classes might account for the same word
>>> predicted_tokens_classes = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]
```

```py
>>> labels = predicted_token_class_ids
>>> loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)
```

## TFDebertaForQuestionAnswering

### `class transformers.TFDebertaForQuestionAnswering`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_tf_deberta.py#L1551)

```py
( config: DebertaConfig *inputs **kwargs )
```

å‚æ•°

+   `config`ï¼ˆDebertaConfigï¼‰â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

DeBERTa æ¨¡å‹åœ¨é¡¶éƒ¨æœ‰ä¸€ä¸ªç”¨äºæå–å¼é—®ç­”ä»»åŠ¡ï¼ˆå¦‚ SQuADï¼‰çš„è·¨åº¦åˆ†ç±»å¤´ï¼ˆåœ¨éšè—çŠ¶æ€è¾“å‡ºçš„çº¿æ€§å±‚ä¸Šè®¡ç®—`span start logits`å’Œ`span end logits`ï¼‰ã€‚

DeBERTa æ¨¡å‹æ˜¯ç”± Pengcheng Heã€Xiaodong Liuã€Jianfeng Gaoã€Weizhu Chen åœ¨[DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)ä¸­æå‡ºçš„ã€‚å®ƒåœ¨ BERT/RoBERTa çš„åŸºç¡€ä¸Šè¿›è¡Œäº†ä¸¤é¡¹æ”¹è¿›ï¼Œå³è§£ç¼ æ³¨æ„åŠ›å’Œå¢å¼ºæ©ç è§£ç å™¨ã€‚é€šè¿‡è¿™ä¸¤é¡¹æ”¹è¿›ï¼Œå®ƒåœ¨ 80GB é¢„è®­ç»ƒæ•°æ®ä¸Šçš„å¤§å¤šæ•°ä»»åŠ¡ä¸­ä¼˜äº BERT/RoBERTaã€‚

è¯¥æ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰ä¿¡æ¯ã€‚

`transformers`ä¸­çš„ TensorFlow æ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äº PyTorch æ¨¡å‹ï¼‰ï¼Œæˆ–

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸çš„ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ Keras æ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºæœ‰è¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥â€œåªéœ€å·¥ä½œâ€ - åªéœ€ä¼ é€’æ‚¨çš„è¾“å…¥å’Œæ ‡ç­¾ä»¥ä»»ä½•`model.fit()`æ”¯æŒçš„æ ¼å¼ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Keras æ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œä¾‹å¦‚åœ¨ä½¿ç”¨ Keras`Functional` API åˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†æ‰€æœ‰è¾“å…¥å¼ é‡åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ï¼š

+   åªæœ‰ä¸€ä¸ªåŒ…å«`input_ids`çš„å¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`

+   ä¸€ä¸ªé•¿åº¦å¯å˜çš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºçš„è¾“å…¥å¼ é‡ï¼š`model([input_ids, attention_mask])`æˆ–`model([input_ids, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒè¿™äº›é—®é¢˜ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…ä»»ä½•å…¶ä»– Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deberta/modeling_tf_deberta.py#L1570)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None start_positions: np.ndarray | tf.Tensor | None = None end_positions: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `input_ids`ï¼ˆ`np.ndarray`ï¼Œ`tf.Tensor`ï¼Œ`List[tf.Tensor]`ï¼Œ`Dict[str, tf.Tensor]`æˆ–`Dict[str, np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º`(batch_size, sequence_length)`ï¼‰â€” è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤º`æœªè¢«æ©ç›–`çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤º`è¢«æ©ç›–`çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*å¥å­ A*çš„æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº*å¥å­ B*çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    ä»€ä¹ˆæ˜¯ä½ç½® IDï¼Ÿ

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†*input_ids*ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›[`~utils.ModelOutputâ€œ]è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `start_positions`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`tf.Tensor`æˆ–`np.ndarray`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè®¡ç®—æ ‡è®°åˆ†ç±»æŸå¤±çš„æ ‡è®°è·¨åº¦èµ·å§‹ä½ç½®çš„æ ‡ç­¾ã€‚ä½ç½®è¢«å¤¹ç´§åˆ°åºåˆ—çš„é•¿åº¦ï¼ˆ`sequence_length`ï¼‰ã€‚åºåˆ—å¤–çš„ä½ç½®ä¸ä¼šè®¡å…¥æŸå¤±è®¡ç®—ã€‚

+   `end_positions`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`tf.Tensor`æˆ–`np.ndarray`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè®¡ç®—æ ‡è®°è·¨åº¦ç»“æŸä½ç½®çš„ä½ç½®ï¼ˆç´¢å¼•ï¼‰çš„æ ‡ç­¾ã€‚ä½ç½®è¢«å¤¹ç´§åˆ°åºåˆ—çš„é•¿åº¦ï¼ˆ`sequence_length`ï¼‰ã€‚åºåˆ—å¤–çš„ä½ç½®ä¸ä¼šè®¡å…¥æŸå¤±è®¡ç®—ã€‚

è¿”å›

transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput æˆ–`tuple(tf.Tensor)`

transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput æˆ–`tf.Tensor`çš„å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆDebertaConfigï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`start_positions`å’Œ`end_positions`æ—¶è¿”å›ï¼‰- æ€»è·¨åº¦æå–æŸå¤±æ˜¯èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„äº¤å‰ç†µä¹‹å’Œã€‚

+   `start_logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`ï¼‰- è·¨åº¦èµ·å§‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `end_logits` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`) â€” è·¨åº¦ç»“æŸå¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(tf.Tensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å› â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(tf.Tensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å› â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

TFDebertaForQuestionAnswering çš„å‰å‘æ–¹æ³•é‡å†™äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, TFDebertaForQuestionAnswering
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("kamalkraj/deberta-base")
>>> model = TFDebertaForQuestionAnswering.from_pretrained("kamalkraj/deberta-base")

>>> question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

>>> inputs = tokenizer(question, text, return_tensors="tf")
>>> outputs = model(**inputs)

>>> answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])
>>> answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])

>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
```

```py
>>> # target is "nice puppet"
>>> target_start_index = tf.constant([14])
>>> target_end_index = tf.constant([15])

>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
>>> loss = tf.math.reduce_mean(outputs.loss)
```
