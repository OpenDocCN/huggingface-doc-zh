# 性能和可伸缩性

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/performance`](https://huggingface.co/docs/transformers/v4.37.2/en/performance)

训练大型 Transformer 模型并将其部署到生产环境中会带来各种挑战。

在训练过程中，模型可能需要比可用 GPU 内存更多的 GPU 内存，或者表现出训练速度较慢。在部署阶段，模型可能难以处理生产环境中所需的吞吐量。

本文档旨在帮助您克服这些挑战，并找到适合您用例的最佳设置。指南分为训练和推理部分，因为每个部分都有不同的挑战和解决方案。在每个部分中，您将找到针对不同硬件配置的单独指南，例如单个 GPU 与多个 GPU 进行训练，或 CPU 与 GPU 进行推理。

将本文档作为您进一步导航到与您的情况匹配的方法的起点。

## 训练

训练大型 Transformer 模型高效地需要像 GPU 或 TPU 这样的加速器。最常见的情况是你只有一块 GPU。您可以应用的方法来提高单个 GPU 上的训练效率也适用于其他设置，如多个 GPU。然而，也有一些特定于多 GPU 或 CPU 训练的技术。我们在单独的部分中进行介绍。

+   在单个 GPU 上进行高效训练的方法和工具：从这里开始学习可以帮助优化 GPU 内存利用率、加快训练速度或两者兼具的常见方法。

+   多 GPU 训练部分：探索本节，了解适用于多 GPU 设置的进一步优化方法，如数据、张量和管道并行。

+   CPU 训练部分：了解在 CPU 上进行混合精度训练。

+   在多个 CPU 上进行高效训练：了解分布式 CPU 训练。

+   使用 TensorFlow 在 TPU 上训练：如果您是 TPU 的新手，请参考本节，了解在 TPU 上训练和使用 XLA 的主观介绍。

+   用于训练的自定义硬件：在构建自己的深度学习装置时找到技巧和窍门。

+   使用 Trainer API 进行超参数搜索

## 推理

在生产环境中使用大型模型进行高效推理可能与训练它们一样具有挑战性。在接下来的部分中，我们将介绍在 CPU 和单/多 GPU 设置上运行推理的步骤。

+   在单个 CPU 上进行推理

+   在单个 GPU 上进行推理

+   多 GPU 推理

+   XLA 集成用于 TensorFlow 模型

## 训练和推理

在这里，您将找到适用于训练模型或使用模型进行推理的技术、提示和技巧。

+   实例化一个大模型

+   性能问题的故障排除

## 贡献

这份文档远未完整，还有很多需要添加的内容，所以如果您有补充或更正，请不要犹豫，打开一个 PR，或者如果您不确定，请开始一个 Issue，我们可以在那里讨论细节。

在提出 A 优于 B 的贡献时，请尽量包含可重现的基准测试和/或指向该信息来源的链接（除非信息直接来自您）。
