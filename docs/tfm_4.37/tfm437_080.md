# GPU æ¨ç†

> åŸæ–‡ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/perf_infer_gpu_one`](https://huggingface.co/docs/transformers/v4.37.2/en/perf_infer_gpu_one)

ä¸ CPU ä¸åŒï¼ŒGPU æ˜¯æœºå™¨å­¦ä¹ çš„æ ‡å‡†ç¡¬ä»¶é€‰æ‹©ï¼Œå› ä¸ºå®ƒä»¬é’ˆå¯¹å†…å­˜å¸¦å®½å’Œå¹¶è¡Œæ€§è¿›è¡Œäº†ä¼˜åŒ–ã€‚ä¸ºäº†è·Ÿä¸Šç°ä»£æ¨¡å‹çš„æ›´å¤§å°ºå¯¸æˆ–åœ¨ç°æœ‰å’Œè¾ƒæ—§çš„ç¡¬ä»¶ä¸Šè¿è¡Œè¿™äº›å¤§å‹æ¨¡å‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å‡ ç§ä¼˜åŒ–æ–¹æ³•æ¥åŠ é€Ÿ GPU æ¨ç†ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ FlashAttention-2ï¼ˆä¸€ç§æ›´èŠ‚çœå†…å­˜çš„æ³¨æ„åŠ›æœºåˆ¶ï¼‰ã€BetterTransformerï¼ˆPyTorch æœ¬åœ°å¿«é€Ÿæ‰§è¡Œè·¯å¾„ï¼‰å’Œ bitsandbytes å°†æ¨¡å‹é‡åŒ–ä¸ºè¾ƒä½ç²¾åº¦ã€‚æœ€åï¼Œå­¦ä¹ å¦‚ä½•ä½¿ç”¨ğŸ¤— Optimum åœ¨ Nvidia å’Œ AMD GPU ä¸ŠåŠ é€Ÿæ¨ç†ã€‚

è¿™é‡Œæè¿°çš„å¤§å¤šæ•°ä¼˜åŒ–ä¹Ÿé€‚ç”¨äºå¤š GPU è®¾ç½®ï¼

## FlashAttention-2

FlashAttention-2 æ˜¯å®éªŒæ€§çš„ï¼Œæœªæ¥ç‰ˆæœ¬å¯èƒ½ä¼šå‘ç”Ÿè¾ƒå¤§å˜åŒ–ã€‚

[FlashAttention-2](https://huggingface.co/papers/2205.14135)æ˜¯æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶çš„æ›´å¿«ã€æ›´é«˜æ•ˆçš„å®ç°ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ˜¾è‘—åŠ é€Ÿæ¨ç†ï¼š

1.  æ­¤å¤–ï¼Œå¯ä»¥é€šè¿‡åœ¨åºåˆ—é•¿åº¦ä¸Šå¹¶è¡ŒåŒ–æ³¨æ„åŠ›è®¡ç®—æ¥ä¼˜åŒ–

1.  å°†å·¥ä½œåˆ†åŒºåœ¨ GPU çº¿ç¨‹ä¹‹é—´ï¼Œä»¥å‡å°‘å®ƒä»¬ä¹‹é—´çš„é€šä¿¡å’Œå…±äº«å†…å­˜è¯»/å†™

ç›®å‰æ”¯æŒä»¥ä¸‹æ¶æ„çš„ FlashAttention-2ï¼š

+   [Bark](https://huggingface.co/docs/transformers/model_doc/bark#transformers.BarkModel)

+   [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)

+   [DistilBert](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel)

+   [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)

+   [GPTNeo](https://huggingface.co/docs/transformers/model_doc/gpt_neo#transformers.GPTNeoModel)

+   [GPTNeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox#transformers.GPTNeoXModel)

+   [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)

+   [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)

+   [Llava](https://huggingface.co/docs/transformers/model_doc/llava)

+   [VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)

+   [MBart](https://huggingface.co/docs/transformers/model_doc/mbart#transformers.MBartModel)

+   [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)

+   [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)

+   [OPT](https://huggingface.co/docs/transformers/model_doc/opt#transformers.OPTModel)

+   [Phi](https://huggingface.co/docs/transformers/model_doc/phi#transformers.PhiModel)

+   [Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model)

+   [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)

æ‚¨å¯ä»¥é€šè¿‡æ‰“å¼€ GitHub Issue æˆ– Pull Request æ¥è¯·æ±‚ä¸ºå¦ä¸€ä¸ªæ¨¡å‹æ·»åŠ  FlashAttention-2 æ”¯æŒã€‚

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£… FlashAttention-2ã€‚

NVIDIAAMD

```py
pip install flash-attn --no-build-isolation
```

æˆ‘ä»¬å¼ºçƒˆå»ºè®®å‚è€ƒè¯¦ç»†çš„[å®‰è£…è¯´æ˜](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features)ä»¥äº†è§£æ›´å¤šæ”¯æŒçš„ç¡¬ä»¶å’Œæ•°æ®ç±»å‹ï¼

è¦å¯ç”¨ FlashAttention-2ï¼Œè¯·å°†å‚æ•°`attn_implementation="flash_attention_2"`ä¼ é€’ç»™ from_pretrained()ï¼š

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    torch_dtype=torch.bfloat16, 
    attn_implementation="flash_attention_2",
)
```

åªæœ‰å½“æ¨¡å‹çš„ dtype ä¸º`fp16`æˆ–`bf16`æ—¶ï¼Œæ‰èƒ½ä½¿ç”¨ FlashAttention-2ã€‚åœ¨ä½¿ç”¨ FlashAttention-2 ä¹‹å‰ï¼Œè¯·ç¡®ä¿å°†æ¨¡å‹è½¬æ¢ä¸ºé€‚å½“çš„ dtype å¹¶åŠ è½½åˆ°æ”¯æŒçš„è®¾å¤‡ä¸Šã€‚

æ‚¨è¿˜å¯ä»¥è®¾ç½®`use_flash_attention_2=True`æ¥å¯ç”¨ FlashAttention-2ï¼Œä½†å·²è¢«å¼ƒç”¨ï¼Œæ¨èä½¿ç”¨`attn_implementation="flash_attention_2"`ã€‚

FlashAttention-2 å¯ä»¥ä¸å…¶ä»–ä¼˜åŒ–æŠ€æœ¯ï¼ˆå¦‚é‡åŒ–ï¼‰ç»“åˆï¼Œä»¥è¿›ä¸€æ­¥åŠ é€Ÿæ¨ç†ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥å°† FlashAttention-2 ä¸ 8 ä½æˆ– 4 ä½é‡åŒ–ç»“åˆä½¿ç”¨ï¼š

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

# load in 8bit
model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    load_in_8bit=True,
    attn_implementation="flash_attention_2",
)

# load in 4bit
model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    load_in_4bit=True,
    attn_implementation="flash_attention_2",
)
```

### é¢„æœŸçš„åŠ é€Ÿ

æ‚¨å¯ä»¥ä»æ¨ç†ä¸­è·å¾—ç›¸å½“å¤§çš„åŠ é€Ÿï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…·æœ‰é•¿åºåˆ—çš„è¾“å…¥ã€‚ä½†æ˜¯ï¼Œç”±äº FlashAttention-2 ä¸æ”¯æŒä½¿ç”¨å¡«å……ä»¤ç‰Œè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œå› æ­¤åœ¨åºåˆ—åŒ…å«å¡«å……ä»¤ç‰Œæ—¶ï¼Œæ‚¨å¿…é¡»æ‰‹åŠ¨å¡«å……/å–æ¶ˆå¡«å……æ³¨æ„åŠ›åˆ†æ•°ä»¥è¿›è¡Œæ‰¹é‡æ¨ç†ã€‚è¿™ä¼šå¯¼è‡´ä½¿ç”¨å¡«å……ä»¤ç‰Œè¿›è¡Œæ‰¹é‡ç”Ÿæˆæ—¶å‡ºç°æ˜¾ç€å‡é€Ÿã€‚

ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œåœ¨è®­ç»ƒæœŸé—´åº”è¯¥ä½¿ç”¨ä¸å¸¦å¡«å……ä»¤ç‰Œçš„ FlashAttention-2ï¼ˆé€šè¿‡æ‰“åŒ…æ•°æ®é›†æˆ–[è¿æ¥åºåˆ—](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L516)ç›´åˆ°è¾¾åˆ°æœ€å¤§åºåˆ—é•¿åº¦ï¼‰ã€‚

å¯¹äºåœ¨[tiiuae/falcon-7b](https://hf.co/tiiuae/falcon-7b)ä¸Šè¿›è¡Œå•æ¬¡å‰å‘ä¼ é€’ï¼Œåºåˆ—é•¿åº¦ä¸º 4096ï¼Œå„ç§æ‰¹é‡å¤§å°ä¸”æ²¡æœ‰å¡«å……ä»¤ç‰Œï¼Œé¢„æœŸçš„åŠ é€Ÿæ˜¯ï¼š

![](img/463d3f3c66f2489865a258a5082f46f7.png)

å¯¹äºåœ¨[meta-llama/Llama-7b-hf](https://hf.co/meta-llama/Llama-7b-hf)ä¸Šè¿›è¡Œå•æ¬¡å‰å‘ä¼ é€’ï¼Œåºåˆ—é•¿åº¦ä¸º 4096ï¼Œå„ç§æ‰¹é‡å¤§å°ä¸”æ²¡æœ‰å¡«å……ä»¤ç‰Œï¼Œé¢„æœŸçš„åŠ é€Ÿæ˜¯ï¼š

![](img/7ae0be2e7a0a9e0f4d275f8884f4d7d3.png)

å¯¹äºå…·æœ‰å¡«å……ä»¤ç‰Œçš„åºåˆ—ï¼ˆä½¿ç”¨å¡«å……ä»¤ç‰Œç”Ÿæˆï¼‰ï¼Œæ‚¨éœ€è¦å–æ¶ˆå¡«å……/å¡«å……è¾“å…¥åºåˆ—ä»¥æ­£ç¡®è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ã€‚å¯¹äºç›¸å¯¹è¾ƒå°çš„åºåˆ—é•¿åº¦ï¼Œå•æ¬¡å‰å‘ä¼ é€’ä¼šäº§ç”Ÿé¢å¤–å¼€é”€ï¼Œå¯¼è‡´è½»å¾®åŠ é€Ÿï¼ˆåœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œè¾“å…¥çš„ 30%å¡«å……æœ‰å¡«å……ä»¤ç‰Œï¼‰ï¼š

![](img/44a86fa9a8504c27decb2bf7133621cb.png)

ä½†æ˜¯å¯¹äºæ›´å¤§çš„åºåˆ—é•¿åº¦ï¼Œæ‚¨å¯ä»¥æœŸæœ›è·å¾—æ›´å¤šçš„åŠ é€Ÿæ•ˆç›Šï¼š

FlashAttention æ›´å…·å†…å­˜æ•ˆç‡ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥åœ¨æ›´å¤§çš„åºåˆ—é•¿åº¦ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè€Œä¸ä¼šé‡åˆ°å†…å­˜ä¸è¶³çš„é—®é¢˜ã€‚å¯¹äºæ›´å¤§çš„åºåˆ—é•¿åº¦ï¼Œæ‚¨å¯ä»¥å°†å†…å­˜ä½¿ç”¨é‡é™ä½å¤šè¾¾ 20 å€ã€‚æŸ¥çœ‹[flash-attention](https://github.com/Dao-AILab/flash-attention)å­˜å‚¨åº“ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

![](img/36c674b28857433397883375e3a3644d.png)

## PyTorch ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›

PyTorch çš„[`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)ï¼ˆSDPAï¼‰ä¹Ÿå¯ä»¥åœ¨åº•å±‚è°ƒç”¨ FlashAttention å’Œå†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›æ ¸ã€‚å½“å¯ç”¨å®ç°æ—¶ï¼ŒSDPA æ”¯æŒç›®å‰æ­£åœ¨ Transformers ä¸­æœ¬åœ°æ·»åŠ ï¼Œå¹¶ä¸”åœ¨`torch>=2.1.1`æ—¶é»˜è®¤ç”¨äº`torch`ã€‚

ç›®å‰ï¼ŒTransformers æ”¯æŒä»¥ä¸‹æ¶æ„çš„ SDPA æ¨ç†å’Œè®­ç»ƒï¼š

+   [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)

+   [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)

+   [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)

+   [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)

+   [Idefics](https://huggingface.co/docs/transformers/model_doc/idefics#transformers.IdeficsModel)

+   [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)

+   [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)

+   [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)

+   [Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model)

FlashAttention åªèƒ½ç”¨äºå…·æœ‰`fp16`æˆ–`bf16` torch ç±»å‹çš„æ¨¡å‹ï¼Œå› æ­¤è¯·ç¡®ä¿é¦–å…ˆå°†æ‚¨çš„æ¨¡å‹è½¬æ¢ä¸ºé€‚å½“çš„ç±»å‹ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼ŒSDPA é€‰æ‹©æœ€é«˜æ•ˆçš„å¯ç”¨å†…æ ¸ï¼Œä½†æ‚¨å¯ä»¥ä½¿ç”¨[`torch.backends.cuda.sdp_kernel`](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel)ä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¥æ£€æŸ¥åœ¨ç»™å®šè®¾ç½®ï¼ˆç¡¬ä»¶ã€é—®é¢˜å¤§å°ï¼‰ä¸­æ˜¯å¦æœ‰å¯ç”¨çš„åç«¯ï¼š

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", torch_dtype=torch.float16).to("cuda")
# convert the model to BetterTransformer
model.to_bettertransformer()

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

å¦‚æœæ‚¨çœ‹åˆ°ä¸‹é¢çš„å›æº¯ä¸­æœ‰é”™è¯¯ï¼Œè¯·å°è¯•ä½¿ç”¨ PyTorch çš„å¤œé—´ç‰ˆæœ¬ï¼Œè¿™å¯èƒ½å¯¹ FlashAttention æœ‰æ›´å¹¿æ³›çš„è¦†ç›–èŒƒå›´ï¼š

```py
RuntimeError: No available kernel. Aborting execution.

# install PyTorch nightly
pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118
```

## BetterTransformer

ä¸€äº› BetterTransformer åŠŸèƒ½æ­£åœ¨è¢«ä¸Šæ¸¸åˆ° Transformersï¼Œæ”¯æŒæœ¬æœº`torch.nn.scaled_dot_product_attention`ã€‚BetterTransformer ä»ç„¶æ¯” Transformers SDPA é›†æˆå…·æœ‰æ›´å¹¿æ³›çš„è¦†ç›–èŒƒå›´ï¼Œä½†æ‚¨å¯ä»¥æœŸæœ›è¶Šæ¥è¶Šå¤šçš„æ¶æ„åœ¨ Transformers ä¸­æœ¬åœ°æ”¯æŒ SDPAã€‚

æŸ¥çœ‹æˆ‘ä»¬åœ¨[PyTorch 2.0 ä¸­ä½¿ç”¨ BetterTransformer å’Œç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„å¼€ç®±å³ç”¨åŠ é€Ÿå’Œå†…å­˜èŠ‚çœ](https://pytorch.org/blog/out-of-the-box-acceleration/)ä¸­çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨[BetterTransformer](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)åšå®¢æ–‡ç« ä¸­äº†è§£æ›´å¤šå…³äºå¿«é€Ÿæ‰§è¡Œçš„ä¿¡æ¯ã€‚

BetterTransformer é€šè¿‡å…¶å¿«é€Ÿè·¯å¾„ï¼ˆTransformer å‡½æ•°çš„æœ¬æœº PyTorch ä¸“ç”¨å®ç°ï¼‰æ‰§è¡ŒåŠ é€Ÿæ¨æ–­ã€‚å¿«é€Ÿè·¯å¾„æ‰§è¡Œä¸­çš„ä¸¤ä¸ªä¼˜åŒ–æ˜¯ï¼š

1.  èåˆï¼Œå°†å¤šä¸ªè¿ç»­æ“ä½œç»„åˆæˆä¸€ä¸ªå•ä¸€çš„â€œå†…æ ¸â€ï¼Œä»¥å‡å°‘è®¡ç®—æ­¥éª¤çš„æ•°é‡

1.  è·³è¿‡å¡«å……ä»¤ç‰Œçš„å›ºæœ‰ç¨€ç–æ€§ï¼Œä»¥é¿å…ä½¿ç”¨åµŒå¥—å¼ é‡è¿›è¡Œä¸å¿…è¦çš„è®¡ç®—

BetterTransformer è¿˜å°†æ‰€æœ‰æ³¨æ„åŠ›æ“ä½œè½¬æ¢ä¸ºæ›´èŠ‚çœå†…å­˜çš„[scaled dot product attention (SDPA)](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)ï¼Œå¹¶åœ¨åº•å±‚è°ƒç”¨ä¼˜åŒ–çš„å†…æ ¸ï¼Œå¦‚[FlashAttention](https://huggingface.co/papers/2205.14135)ã€‚

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…ğŸ¤— Optimum [ï¼ˆå·²å®‰è£…ï¼‰](https://huggingface.co/docs/optimum/installation)ã€‚

ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ PreTrainedModel.to_bettertransformer()æ–¹æ³•å¯ç”¨ BetterTransformerï¼š

```py
model = model.to_bettertransformer()
```

æ‚¨å¯ä»¥ä½¿ç”¨ reverse_bettertransformer()æ–¹æ³•è¿”å›åŸå§‹çš„ Transformers æ¨¡å‹ã€‚åœ¨ä¿å­˜æ¨¡å‹ä¹‹å‰ï¼Œåº”è¯¥ä½¿ç”¨è¿™ä¸ªæ–¹æ³•æ¥ä½¿ç”¨è§„èŒƒçš„ Transformers å»ºæ¨¡ï¼š

```py
model = model.reverse_bettertransformer()
model.save_pretrained("saved_model")
```

## bitsandbytes

bitsandbytes æ˜¯ä¸€ä¸ªåŒ…å«å¯¹ 4 ä½å’Œ 8 ä½é‡åŒ–æ”¯æŒçš„é‡åŒ–åº“ã€‚ä¸å…¶åŸç”Ÿå…¨ç²¾åº¦ç‰ˆæœ¬ç›¸æ¯”ï¼Œé‡åŒ–å¯ä»¥å‡å°æ¨¡å‹å¤§å°ï¼Œä½¿å…¶æ›´å®¹æ˜“é€‚åº”å†…å­˜æœ‰é™çš„ GPUã€‚

ç¡®ä¿æ‚¨å·²å®‰è£… bitsandbytes å’ŒğŸ¤— Accelerateï¼š

```py
# these versions support 8-bit and 4-bit
pip install bitsandbytes>=0.39.0 accelerate>=0.20.0

# install Transformers
pip install transformers
```

### 4 ä½

è¦åœ¨ 4 ä½æ¨¡å‹ä¸­è¿›è¡Œæ¨æ–­ï¼Œä½¿ç”¨`load_in_4bit`å‚æ•°ã€‚`device_map`å‚æ•°æ˜¯å¯é€‰çš„ï¼Œä½†æˆ‘ä»¬å»ºè®®å°†å…¶è®¾ç½®ä¸º`"auto"`ï¼Œä»¥ä¾¿ğŸ¤— Accelerate æ ¹æ®ç¯å¢ƒä¸­çš„å¯ç”¨èµ„æºè‡ªåŠ¨é«˜æ•ˆåœ°åˆ†é…æ¨¡å‹ã€‚

```py
from transformers import AutoModelForCausalLM

model_name = "bigscience/bloom-2b5"
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)
```

è¦åœ¨å¤šä¸ª GPU ä¸ŠåŠ è½½ 4 ä½æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼Œæ‚¨å¯ä»¥æ§åˆ¶è¦ä¸ºæ¯ä¸ª GPU åˆ†é…å¤šå°‘ GPU RAMã€‚ä¾‹å¦‚ï¼Œå°† 600MB çš„å†…å­˜åˆ†é…ç»™ç¬¬ä¸€ä¸ª GPUï¼Œå°† 1GB çš„å†…å­˜åˆ†é…ç»™ç¬¬äºŒä¸ª GPUï¼š

```py
max_memory_mapping = {0: "600MB", 1: "1GB"}
model_name = "bigscience/bloom-3b"
model_4bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_4bit=True, max_memory=max_memory_mapping
)
```

### 8 ä½

å¦‚æœæ‚¨å¯¹ 8 ä½é‡åŒ–çš„æ¦‚å¿µæ„Ÿå…´è¶£å¹¶æƒ³äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·é˜…è¯»[Hugging Face Transformersã€Accelerate å’Œ bitsandbytes ä½¿ç”¨è§„æ¨¡åŒ–å˜å‹å™¨è¿›è¡Œ 8 ä½çŸ©é˜µä¹˜æ³•çš„åˆæ­¥ä»‹ç»](https://huggingface.co/blog/hf-bitsandbytes-integration)åšå®¢æ–‡ç« ã€‚

è¦åœ¨ 8 ä½æ¨¡å‹ä¸­è¿›è¡Œæ¨æ–­ï¼Œä½¿ç”¨`load_in_8bit`å‚æ•°ã€‚`device_map`å‚æ•°æ˜¯å¯é€‰çš„ï¼Œä½†æˆ‘ä»¬å»ºè®®å°†å…¶è®¾ç½®ä¸º`"auto"`ï¼Œä»¥ä¾¿ğŸ¤— Accelerate æ ¹æ®ç¯å¢ƒä¸­çš„å¯ç”¨èµ„æºè‡ªåŠ¨é«˜æ•ˆåœ°åˆ†é…æ¨¡å‹ï¼š

```py
from transformers import AutoModelForCausalLM

model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)
```

å¦‚æœæ‚¨è¦åŠ è½½ 8 ä½æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œåº”è¯¥ä½¿ç”¨ generate()æ–¹æ³•ï¼Œè€Œä¸æ˜¯æœªç»ä¼˜åŒ–çš„ Pipeline å‡½æ•°ï¼Œåè€…å¯¹ 8 ä½æ¨¡å‹ä¸é€‚ç”¨ä¸”é€Ÿåº¦è¾ƒæ…¢ã€‚ä¸€äº›é‡‡æ ·ç­–ç•¥ï¼Œå¦‚æ ¸é‡‡æ ·ï¼Œä¹Ÿä¸å— Pipeline æ”¯æŒã€‚æ‚¨è¿˜åº”è¯¥å°†æ‰€æœ‰è¾“å…¥æ”¾åœ¨ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡ä¸Šï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "bigscience/bloom-2b5"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)

prompt = "Hello, my llama is cute"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
```

è¦åœ¨å¤šä¸ª GPU ä¸ŠåŠ è½½ 4 ä½æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼Œæ‚¨å¯ä»¥æ§åˆ¶è¦ä¸ºæ¯ä¸ª GPU åˆ†é…å¤šå°‘ GPU RAMã€‚ä¾‹å¦‚ï¼Œè¦å°† 1GB å†…å­˜åˆ†é…ç»™ç¬¬ä¸€ä¸ª GPUï¼Œå°† 2GB å†…å­˜åˆ†é…ç»™ç¬¬äºŒä¸ª GPUï¼š

```py
max_memory_mapping = {0: "1GB", 1: "2GB"}
model_name = "bigscience/bloom-3b"
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_8bit=True, max_memory=max_memory_mapping
)
```

éšæ„å°è¯•åœ¨ Google Colab çš„å…è´¹ GPU ä¸Šè¿è¡Œä¸€ä¸ªæ‹¥æœ‰ 110 äº¿å‚æ•°çš„[T5 æ¨¡å‹](https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing)æˆ– 30 äº¿å‚æ•°çš„[BLOOM æ¨¡å‹](https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing)è¿›è¡Œæ¨æ–­ï¼

## ğŸ¤— Optimum

äº†è§£æœ‰å…³åœ¨[NVIDIA GPU ä¸Šè¿›è¡ŒåŠ é€Ÿæ¨æ–­](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#accelerated-inference-on-nvidia-gpus)å’Œ[AMD GPU ä¸Šè¿›è¡ŒåŠ é€Ÿæ¨æ–­](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/amdgpu#accelerated-inference-on-amd-gpus)çš„æŒ‡å—ä¸­ä½¿ç”¨ ORT çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚æœ¬èŠ‚ä»…æä¾›ç®€è¦ä¸”ç®€å•çš„ç¤ºä¾‹ã€‚

ONNX Runtimeï¼ˆORTï¼‰æ˜¯ä¸€ä¸ªæ¨¡å‹åŠ é€Ÿå™¨ï¼Œæ”¯æŒåœ¨ Nvidia GPU å’Œä½¿ç”¨[ROCm](https://www.amd.com/en/products/software/rocm.html)å †æ ˆçš„ AMD GPU ä¸Šè¿›è¡ŒåŠ é€Ÿæ¨æ–­ã€‚ORT ä½¿ç”¨ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚å°†å¸¸è§æ“ä½œèåˆä¸ºå•ä¸ªèŠ‚ç‚¹å’Œå¸¸é‡æŠ˜å ï¼Œä»¥å‡å°‘æ‰§è¡Œçš„è®¡ç®—é‡å¹¶åŠ å¿«æ¨æ–­é€Ÿåº¦ã€‚ORT è¿˜å°†è®¡ç®—å¯†é›†å‹æ“ä½œæ”¾åœ¨ GPU ä¸Šï¼Œå…¶ä½™æ“ä½œæ”¾åœ¨ CPU ä¸Šï¼Œæ™ºèƒ½åœ°åœ¨ä¸¤ä¸ªè®¾å¤‡ä¹‹é—´åˆ†é…å·¥ä½œè´Ÿè½½ã€‚

ORT å—ğŸ¤— Optimum æ”¯æŒï¼Œå¯ä»¥åœ¨ğŸ¤— Transformers ä¸­ä½¿ç”¨ã€‚æ‚¨éœ€è¦ä½¿ç”¨ä¸€ä¸ª[ORTModel](https://huggingface.co/docs/optimum/v1.16.2/en/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel)æ¥è§£å†³æ‚¨çš„ä»»åŠ¡ï¼Œå¹¶æŒ‡å®š`provider`å‚æ•°ï¼Œå¯ä»¥è®¾ç½®ä¸º[`CUDAExecutionProvider`](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#cudaexecutionprovider)ã€[`ROCMExecutionProvider`](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/amdgpu)æˆ–[`TensorrtExecutionProvider`](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#tensorrtexecutionprovider)ã€‚å¦‚æœè¦åŠ è½½å°šæœªå¯¼å‡ºä¸º ONNX çš„æ¨¡å‹ï¼Œå¯ä»¥è®¾ç½®`export=True`å°†æ‚¨çš„æ¨¡å‹å³æ—¶è½¬æ¢ä¸º ONNX æ ¼å¼ï¼š

```py
from optimum.onnxruntime import ORTModelForSequenceClassification

ort_model = ORTModelForSequenceClassification.from_pretrained(
  "distilbert-base-uncased-finetuned-sst-2-english",
  export=True,
  provider="CUDAExecutionProvider",
)
```

ç°åœ¨æ‚¨å¯ä»¥è‡ªç”±åœ°ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼š

```py
from optimum.pipelines import pipeline
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

pipeline = pipeline(task="text-classification", model=ort_model, tokenizer=tokenizer, device="cuda:0")
result = pipeline("Both the music and visual were astounding, not to mention the actors performance.")
```

## ç»“åˆä¼˜åŒ–

é€šå¸¸å¯ä»¥ç»“åˆä¸Šè¿°æè¿°çš„å¤šç§ä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥è·å¾—æœ€ä½³çš„æ¨æ–­æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥åŠ è½½ä¸€ä¸ª 4 ä½æ¨¡å‹ï¼Œç„¶åå¯ç”¨å¸¦æœ‰ FlashAttention çš„ BetterTransformerï¼š

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# load model in 4-bit
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", quantization_config=quantization_config)

# enable BetterTransformer
model = model.to_bettertransformer()

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

# enable FlashAttention
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```
