# IA3

> 原始文本：[`huggingface.co/docs/peft/package_reference/ia3`](https://huggingface.co/docs/peft/package_reference/ia3)

通过抑制和放大内部激活的注入适配器，或[IA3](https://hf.co/papers/2205.05638)，是一种方法，通过添加三个学习向量来重新调整自注意力和编码器-解码器注意力层的键和值，以及位置逐层前馈网络的中间激活。

论文摘要如下：

*少样本上下文学习（ICL）使预训练语言模型能够在没有任何基于梯度的训练的情况下执行以前未见过的任务，只需将少量训练示例作为输入的一部分。ICL 会产生大量的计算、内存和存储成本，因为它涉及每次进行预测时都要处理所有训练示例。参数高效微调（PEFT）（例如适配器模块、提示微调、稀疏更新方法等）提供了一种替代范式，其中训练一小组参数以使模型能够执行新任务。在本文中，我们严格比较了少样本 ICL 和 PEFT，并证明后者提供了更好的准确性以及显著更低的计算成本。在此过程中，我们介绍了一种名为（IA）³的新 PEFT 方法，通过学习向量扩展激活，实现更强的性能，同时只引入了相对较少的新参数。我们还提出了一种基于 T0 模型的简单配方，称为 T-Few，可以应用于新任务，而无需特定于任务的微调或修改。我们通过将其应用于 RAFT 基准测试中完全未见过的任务，验证了 T-Few 的有效性，首次实现了超人类性能，并超过了现有技术水平 6 个百分点。我们在实验中使用的所有代码都是公开可用的*。

## IA3Config

### `class peft.IA3Config`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/ia3/config.py#L22)

```py
( peft_type: Union = None auto_mapping: Optional = None base_model_name_or_path: Optional = None revision: Optional = None task_type: Union = None inference_mode: bool = False target_modules: Union = None feedforward_modules: Union = None fan_in_fan_out: bool = False modules_to_save: Optional = None init_ia3_weights: bool = True )
```

参数

+   `target_modules` (`Optional[Union[List[str], str]]`) — 要应用适配器的模块的名称。如果指定了这个参数，只有指定名称的模块将被替换。当传递一个字符串时，将执行正则匹配。当传递一个字符串列表时，将执行精确匹配或检查模块的名称是否以任何传递的字符串结尾。如果指定为'all-linear'，则选择所有线性/Conv1D 模块，不包括输出层。如果未指定，将根据模型架构选择模块。如果架构未知，将引发错误 — 在这种情况下，您应该手动指定目标模块。

+   `feedforward_modules` (`Optional[Union[List[str], str]]`) — 要作为前馈模块处理的模块的名称，就像原始论文中一样。这些模块将对输入进行（IA）³向量乘法，而不是对输出进行乘法。`feedforward_modules`必须是`target_modules`中存在的名称或名称子集。

+   `fan_in_fan_out` (`bool`) — 如果要替换的层存储类似于（fan_in，fan_out）的权重，请将此设置为 True。例如，gpt-2 使用存储类似于（fan_in，fan_out）的`Conv1D`，因此应将其设置为`True`。

+   `modules_to_save` (`Optional[List[str]]`) — 除了（IA）³层之外要设置为可训练并保存在最终检查点中的模块列表。

+   `init_ia3_weights` (`bool`) — 是否初始化（IA）³层中的向量，默认为`True`。不建议将此设置为`False`。

这是一个配置类，用于存储 IA3Model 的配置。

## IA3Model

### `class peft.IA3Model`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/ia3/model.py#L38)

```py
( model config adapter_name ) → export const metadata = 'undefined';torch.nn.Module
```

参数

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) — 要适配的模型。

+   `config` (IA3Config) — (IA)³ 模型的配置。

+   `adapter_name` (`str`) — 适配器的名称，默认为`"default"`。

返回

`torch.nn.Module`

(IA)³ 模型。

从预训练的 transformers 模型创建一个通过抑制和放大内部激活来注入适配器的(IA)³ 模型。该方法在[`arxiv.org/abs/2205.05638`](https://arxiv.org/abs/2205.05638)中有详细描述。

示例:

```py
>>> from transformers import AutoModelForSeq2SeqLM, ia3Config
>>> from peft import IA3Model, IA3Config

>>> config = IA3Config(
...     peft_type="IA3",
...     task_type="SEQ_2_SEQ_LM",
...     target_modules=["k", "v", "w0"],
...     feedforward_modules=["w0"],
... )

>>> model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")
>>> ia3_model = IA3Model(config, model)
```

**属性**:

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) — 要适配的模型。

+   `peft_config` (`ia3Config`): (IA)³ 模型的配置。

#### `delete_adapter`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/ia3/model.py#L368)

```py
( adapter_name: str )
```

参数

+   `adapter_name` (str) — 要删除的适配器的名称。

删除现有的适配器。

#### `disable_adapter_layers`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/ia3/model.py#L252)

```py
( )
```

禁用所有适配器。

当禁用所有适配器时，模型输出对应于基础模型的输出。

#### `enable_adapter_layers`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/ia3/model.py#L245)

```py
( )
```

启用所有适配器。

如果之前禁用了所有适配器并希望重新启用它们，请调用此方法。

#### `merge_and_unload`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/ia3/model.py#L334)

```py
( safe_merge: bool = False adapter_names: Optional[list[str]] = None )
```

参数

+   `safe_merge` (`bool`) — 是否激活安全合并检查以检查适配器权重中是否存在任何潜在的 NaN

+   `adapter_names` (`List[str]`, *可选*) — 应合并的适配器名称列表。如果为 None，则将合并所有活动适配器。默认为`None`。

此方法将 IA³层合并到基础模型中。如果有人想要将基础模型用作独立模型，则需要这样做。

示例:

```py
>>> from transformers import AutoModelForCausalLM
>>> from peft import PeftModel

>>> base_model = AutoModelForCausalLM.from_pretrained("tiiuae/falcon-40b")
>>> peft_model_id = "smangrul/falcon-40B-int4-peft-lora-sfttrainer-sample"
>>> model = PeftModel.from_pretrained(base_model, peft_model_id)
>>> merged_model = model.merge_and_unload()
```

#### `set_adapter`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/ia3/model.py#L259)

```py
( adapter_name: str | list[str] )
```

参数

+   `adapter_name` (`str`或`list[str]`) — 要激活的适配器的名称。

设置活动适配器。

此外，此函数将设置指定的适配器为可训练的（即，requires_grad=True）。如果不需要这样做，请使用以下代码。

```py
>>> for name, param in model_peft.named_parameters():
...     if ...:  # some check on name (ex. if 'lora' in name)
...         param.requires_grad = False
```

#### `卸载`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/ia3/model.py#L361)

```py
( )
```

通过删除所有 IA³模块而获得基础模型。这将还原原始基础模型。
