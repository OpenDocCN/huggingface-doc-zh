# IA3

> 原文链接：[`huggingface.co/docs/peft/conceptual_guides/ia3`](https://huggingface.co/docs/peft/conceptual_guides/ia3)

这个概念指南简要概述了[IA3](https://arxiv.org/abs/2205.05638)，这是一种旨在改进 LoRA 的参数高效微调技术。

为了使微调更加高效，IA3（通过抑制和放大内部激活注入的适配器）使用学习向量重新缩放内部激活。这些学习向量被注入到典型基于 Transformer 的架构中的注意力和前馈模块中。这些学习向量是微调期间唯一可训练的参数，因此原始权重保持冻结。处理学习向量（而不是像 LoRA 那样学习权重矩阵的低秩更新）可以使可训练参数的数量大大减少。

与 LoRA 类似，IA3 具有许多相同的优势：

+   IA3 通过大幅减少可训练参数使微调更加高效。（对于 T0，IA3 模型只有大约 0.01%的可训练参数，而即使 LoRA 也有> 0.1%）

+   原始预训练权重保持冻结，这意味着您可以基于它们构建多个轻量级和便携的 IA3 模型，用于各种下游任务。

+   使用 IA3 进行微调的模型性能与完全微调的模型性能相当。

+   IA3 不会增加任何推理延迟，因为适配器权重可以与基础模型合并。

原则上，IA3 可以应用于神经网络中的任何权重矩阵子集，以减少可训练参数的数量。根据作者的实现，IA3 权重被添加到 Transformer 模型的关键、值和前馈层中。具体来说，对于 Transformer 模型，IA3 权重被添加到关键和值层的输出，以及每个 Transformer 块中第二个前馈层的输入。

根据注入 IA3 参数的目标层，可以根据权重矩阵的大小确定可训练参数的数量。

## PEFT 中常见的 IA3 参数

与 PEFT 支持的其他方法一样，要使用 IA3 微调模型，您需要：

1.  实例化一个基础模型。

1.  创建一个配置（`IA3Config`），在其中定义 IA3 特定的参数。

1.  使用`get_peft_model()`包装基础模型以获得可训练的`PeftModel`。

1.  像通常训练基础模型一样训练`PeftModel`。

`IA3Config`允许您通过以下参数控制如何将 IA3 应用于基础模型：

+   `target_modules`：应用 IA3 向量的模块（例如，注意力块）。

+   `feedforward_modules`：在`target_modules`中作为前馈层处理的模块列表。虽然学习向量与注意力块的输出激活相乘，但这些向量与经典前馈层的输入相乘。请注意，`feedforward_modules`必须是`target_modules`的子集。

+   `modules_to_save`：除 IA3 层之外要设置为可训练并保存在最终检查点中的模块列表。这些通常包括为微调任务随机初始化的模型自定义头部。

## 示例用法

对于序列分类任务，可以如下初始化 Llama 模型的 IA3 配置：

```py
peft_config = IA3Config(
    task_type=TaskType.SEQ_CLS, target_modules=["k_proj", "v_proj", "down_proj"], feedforward_modules=["down_proj"]
)
```
