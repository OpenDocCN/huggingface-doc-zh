# 完全分片数据并行

> 原始文本：[`huggingface.co/docs/peft/accelerate/fsdp`](https://huggingface.co/docs/peft/accelerate/fsdp)

[完全分片数据并行](https://pytorch.org/docs/stable/fsdp.html)（FSDP）是为了分布式训练大型预训练模型，最多可达 1T 参数。FSDP 通过在数据并行进程之间分片模型参数、梯度和优化器状态来实现这一点，还可以将分片模型参数卸载到 CPU。FSDP 提供的内存效率使您可以将训练扩展到更大的批次或模型大小。

目前，FSDP 并不会减少 GPU 内存的使用量，而具有 CPU 卸载的 FSDP 在训练期间实际上会消耗 1.65 倍的 GPU 内存。您可以跟踪这个 PyTorch [问题](https://github.com/pytorch/pytorch/issues/91165) 获取任何更新。

FSDP 在🤗 Accelerate 中受到支持，您可以与🤗 PEFT 一起使用。本指南将帮助您学习如何使用我们的 FSDP[训练脚本](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py)。您将配置脚本以训练一个大型模型用于条件生成。

## 配置

首先运行以下命令来[创建一个 FSDP 配置文件](https://huggingface.co/docs/accelerate/main/en/usage_guides/fsdp)与🤗 Accelerate 一起。使用`--config_file`标志将配置文件保存到特定位置，否则它将保存为🤗 Accelerate 缓存中的`default_config.yaml`文件。

配置文件用于在启动训练脚本时设置默认选项。

```py
accelerate config --config_file fsdp_config.yaml
```

您将被问及有关您的设置的几个问题，并配置以下参数。在本示例中，请确保完全分片模型参数、梯度、优化器状态，利用 CPU 进行卸载，并根据 Transformer 层类名包装模型层。

```py
`Sharding Strategy`: [1] FULL_SHARD (shards optimizer states, gradients and parameters), [2] SHARD_GRAD_OP (shards optimizer states and gradients), [3] NO_SHARD
`Offload Params`: Decides Whether to offload parameters and gradients to CPU
`Auto Wrap Policy`: [1] TRANSFORMER_BASED_WRAP, [2] SIZE_BASED_WRAP, [3] NO_WRAP 
`Transformer Layer Class to Wrap`: When using `TRANSFORMER_BASED_WRAP`, user specifies comma-separated string of transformer layer class names (case-sensitive) to wrap ,e.g, 
`BertLayer`, `GPTJBlock`, `T5Block`, `BertLayer,BertEmbeddings,BertSelfOutput`...
`Min Num Params`: minimum number of parameters when using `SIZE_BASED_WRAP`
`Backward Prefetch`: [1] BACKWARD_PRE, [2] BACKWARD_POST, [3] NO_PREFETCH
`State Dict Type`: [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT  
```

例如，您的 FSDP 配置文件可能如下所示：

```py
command_file: null
commands: null
compute_environment: LOCAL_MACHINE
deepspeed_config: {}
distributed_type: FSDP
downcast_bf16: 'no'
dynamo_backend: 'NO'
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch_policy: BACKWARD_PRE
  fsdp_offload_params: true
  fsdp_sharding_strategy: 1
  fsdp_state_dict_type: FULL_STATE_DICT
  fsdp_transformer_layer_cls_to_wrap: T5Block
gpu_ids: null
machine_rank: 0
main_process_ip: null
main_process_port: null
main_training_function: main
megatron_lm_config: {}
mixed_precision: 'no'
num_machines: 1
num_processes: 2
rdzv_backend: static
same_network: true
tpu_name: null
tpu_zone: null
use_cpu: false
```

## 重要部分

让我们深入了解训练脚本的工作原理。

[`main()`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py#L14) 函数从初始化一个[Accelerator](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)类开始，该类处理分布式训练的所有内容，例如自动检测您的训练环境。

💡 随意更改`main`函数中的模型和数据集。如果您的数据集格式与脚本中的不同，您可能还需要编写自己的预处理函数。

脚本还创建了一个与您正在使用的🤗 PEFT 方法相对应的配置。对于 LoRA，您将使用 LoraConfig 来指定任务类型，以及其他一些重要参数，如低秩矩阵的维度、矩阵缩放因子和 LoRA 层的丢失概率。如果您想使用不同的🤗 PEFT 方法，请将`LoraConfig`替换为适当的类。

接下来，脚本使用 get_peft_model()函数将基础模型和`peft_config`包装起来，以创建一个 PeftModel。

```py
 def main():
+    accelerator = Accelerator()
     model_name_or_path = "t5-base"
     base_path = "temp/data/FinancialPhraseBank-v1.0"
+    peft_config = LoraConfig(
         task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
     )
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
+   model = get_peft_model(model, peft_config)
```

在整个脚本中，您将看到[main_process_first](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.main_process_first)和[wait_for_everyone](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.wait_for_everyone)函数，这些函数有助于控制和同步进程的执行时间。

准备好数据集，并加载所有必要的训练组件后，脚本会检查您是否在使用`fsdp_plugin`。PyTorch 提供了两种在 FSDP 中包装模型层的方法，自动或手动。最简单的方法是允许 FSDP 自动递归包装模型层，而无需更改任何其他代码。您可以选择根据层名称或大小（参数数量）来包装模型层。在 FSDP 配置文件中，它使用`TRANSFORMER_BASED_WRAP`选项来包装`T5Block`层。

```py
if getattr(accelerator.state, "fsdp_plugin", None) is not None:
    accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)
```

接下来，使用🤗 Accelerate 的[prepare](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)函数来准备模型、数据集、优化器和调度器进行训练。

```py
model, train_dataloader, eval_dataloader, optimizer, lr_scheduler = accelerator.prepare(
    model, train_dataloader, eval_dataloader, optimizer, lr_scheduler
)
```

从这里开始，脚本的其余部分处理训练循环、评估，并将您的模型分享到 Hub。

## 训练

运行以下命令启动训练脚本。之前，您将配置文件保存为`fsdp_config.yaml`，因此您需要通过`--config_file`参数传递路径给启动器，就像这样：

```py
accelerate launch --config_file fsdp_config.yaml examples/peft_lora_seq2seq_accelerate_fsdp.py
```

一旦训练完成，脚本将返回准确性并将预测与标签进行比较。
