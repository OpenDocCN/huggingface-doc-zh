# P-tuning

> 原始文本：[`huggingface.co/docs/peft/package_reference/p_tuning`](https://huggingface.co/docs/peft/package_reference/p_tuning)

[P-tuning](https://hf.co/papers/2103.10385)将可训练的提示嵌入添加到输入中，由提示编码器优化以找到更好的提示，消除了手动设计提示的需要。提示标记可以添加到输入序列的任何位置，P-tuning 还引入了用于提高性能的锚定标记。

来自论文的摘要是：

*尽管传统微调的 GPT 在自然语言理解（NLU）方面表现不佳，但我们表明，使用一种新颖的方法 P-tuning，即使用可训练的连续提示嵌入，GPT 在 NLU 任务上可以优于或与类似大小的 BERT 相媲美。在知识探测（LAMA）基准测试中，最佳 GPT 在测试时没有提供任何额外文本的情况下，恢复了 64\%（P@1）的世界知识，这大大提高了之前最佳结果超过 20 个百分点。在 SuperGlue 基准测试中，GPT 在监督学习中实现了与类似大小的 BERT 相媲美甚至更好的性能。重要的是，我们发现 P-tuning 还改善了 BERT 在少样本和监督设置中的性能，同时大大减少了提示工程的需求。因此，P-tuning 在少样本 SuperGlue 基准测试中胜过了最先进的方法。*。

## PromptEncoderConfig

### `class peft.PromptEncoderConfig`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/p_tuning/config.py#L28)

```py
( peft_type: Union = None auto_mapping: Optional = None base_model_name_or_path: Optional = None revision: Optional = None task_type: Union = None inference_mode: bool = False num_virtual_tokens: int = None token_dim: int = None num_transformer_submodules: Optional = None num_attention_heads: Optional = None num_layers: Optional = None encoder_reparameterization_type: Union = <PromptEncoderReparameterizationType.MLP: 'MLP'> encoder_hidden_size: int = None encoder_num_layers: int = 2 encoder_dropout: float = 0.0 )
```

参数

+   `encoder_reparameterization_type`（Union[`PromptEncoderReparameterizationType`，`str`]）— 要使用的重新参数化类型。

+   `encoder_hidden_size`（`int`）— 提示编码器的隐藏大小。

+   `encoder_num_layers`（`int`）— 提示编码器的层数。

+   `encoder_dropout`（`float`）— 提示编码器的丢失概率。

这是一个配置类，用于存储 PromptEncoder 的配置。

## PromptEncoder

### `class peft.PromptEncoder`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/p_tuning/model.py#L24)

```py
( config )
```

参数

+   `config`（PromptEncoderConfig）— 提示编码器的配置。

用于生成 P-tuning 的虚拟标记嵌入的提示编码器网络。

示例：

```py
>>> from peft import PromptEncoder, PromptEncoderConfig

>>> config = PromptEncoderConfig(
...     peft_type="P_TUNING",
...     task_type="SEQ_2_SEQ_LM",
...     num_virtual_tokens=20,
...     token_dim=768,
...     num_transformer_submodules=1,
...     num_attention_heads=12,
...     num_layers=12,
...     encoder_reparameterization_type="MLP",
...     encoder_hidden_size=768,
... )

>>> prompt_encoder = PromptEncoder(config)
```

**属性**：

+   `embedding`（`torch.nn.Embedding`）— 提示编码器的嵌入层。

+   `mlp_head`（`torch.nn.Sequential`）— 如果`inference_mode=False`，则为提示编码器的 MLP 头。

+   `lstm_head`（`torch.nn.LSTM`）— 如果`inference_mode=False`和`encoder_reparameterization_type="LSTM"`，则为提示编码器的 LSTM 头。

+   `token_dim`（`int`）— 基础变换器模型的隐藏嵌入维度。

+   `input_size`（`int`）— 提示编码器的输入大小。

+   `output_size`（`int`）— 提示编码器的输出大小。

+   `hidden_size`（`int`）— 提示编码器的隐藏大小。

+   `total_virtual_tokens`（`int`）：提示编码器的虚拟标记总数。

+   `encoder_type`（Union[`PromptEncoderReparameterizationType`，`str`]）：提示编码器的编码器类型。

输入形状：（`batch_size`，`total_virtual_tokens`）

输出形状：（`batch_size`，`total_virtual_tokens`，`token_dim`）
