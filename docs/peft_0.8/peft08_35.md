# AdaLoRA

> [原文链接](https://huggingface.co/docs/peft/package_reference/adalora)

[AdaLoRA](https://hf.co/papers/2303.10512) 是一种用于优化可分配给权重矩阵和层的可训练参数数量的方法，与 LoRA 不同，LoRA 将参数均匀分配到所有模块中。对于重要的权重矩阵和层分配更多的参数预算，而对于不太重要的部分分配较少的参数。

该论文的摘要是：

*在下游任务上对大型预训练语言模型进行微调已经成为自然语言处理中的重要范式。然而，常见做法是微调预训练模型中的所有参数，当存在大量下游任务时，这种做法变得不切实际。因此，提出了许多微调方法，以一种参数高效的方式学习预训练权重的增量更新，例如低秩增量。这些方法通常会将增量更新的预算均匀分配到所有预训练权重矩阵中，忽视了不同权重参数的重要性差异。因此，微调性能是次优的。为了弥补这一差距，我们提出了 AdaLoRA，根据它们的重要性分数自适应地在权重矩阵之间分配参数预算。特别地，AdaLoRA 将增量更新参数化为奇异值分解的形式。这种新颖的方法使我们能够有效地修剪不重要更新的奇异值，从而减少它们的参数预算，但避免了繁重的精确 SVD 计算。我们对自然语言处理、问答和自然语言生成中的几个预训练模型进行了广泛实验，以验证 AdaLoRA 的有效性。结果表明，AdaLoRA 在基线上表现出显著的改进，特别是在低预算设置下。我们的代码公开可用于 [`github.com/QingruZhang/AdaLoRA`](https://github.com/QingruZhang/AdaLoRA)*。

## AdaLoraConfig

### `class peft.AdaLoraConfig`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adalora/config.py#L22)

```py
( peft_type: Union = None auto_mapping: Optional = None base_model_name_or_path: Optional = None revision: Optional = None task_type: Union = None inference_mode: bool = False r: int = 8 target_modules: Optional[Union[list[str], str]] = None lora_alpha: int = 8 lora_dropout: float = 0.0 fan_in_fan_out: bool = False bias: Literal['none', 'all', 'lora_only'] = 'none' use_rslora: bool = False modules_to_save: Optional[list[str]] = None init_lora_weights: bool | Literal['gaussian', 'loftq'] = True layers_to_transform: Optional[Union[list[int], int]] = None layers_pattern: Optional[Union[list[str], str]] = None rank_pattern: Optional = None alpha_pattern: Optional[dict] = <factory> megatron_config: Optional[dict] = None megatron_core: Optional[str] = 'megatron.core' loftq_config: Union[LoftQConfig, dict] = <factory> target_r: int = 8 init_r: int = 12 tinit: int = 0 tfinal: int = 0 deltaT: int = 1 beta1: float = 0.85 beta2: float = 0.85 orth_reg_weight: float = 0.5 total_step: Optional = None )
```

参数

+   `target_r` (`int`) — 增量矩阵的目标平均秩。

+   `init_r` (`int`) — 每个增量矩阵的初始秩。

+   `tinit` (`int`) — 初始微调热身步骤。

+   `tfinal` (`int`) — 最终微调步骤。

+   `deltaT` (`int`) — 两个预算分配之间的时间间隔。

+   `beta1` (`float`) — 用于敏感性平滑的 EMA 超参数。

+   `beta2` (`float`) — 用于不确定性量化的 EMA 超参数。

+   `orth_reg_weight` (`float`) — 正交正则化的系数。

+   `total_step` (`int`) — 在训练之前应指定的总训练步骤。

+   `rank_pattern` (`list`) — 由 RankAllocator 为每个权重矩阵分配的秩。

这是用于存储 `~peft.AdaLora` 配置的配置类。

## AdaLoraModel

### `class peft.AdaLoraModel`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adalora/model.py#L35)

```py
( model config adapter_name ) → export const metadata = 'undefined';torch.nn.Module
```

参数

+   `model` ([*transformers.PreTrainedModel*]) — 要适应的模型。

+   `config` ([*AdaLoraConfig*]) — AdaLora 模型的配置。

+   `adapter_name` (*str*) — 适配器的名称，默认为 *“default”*。

返回

*torch.nn.Module*

AdaLora 模型。

从预训练的 transformers 模型创建 AdaLoRA (自适应 LoRA) 模型。论文：[`openreview.net/forum?id=lq62uWRJjiY`](https://openreview.net/forum?id=lq62uWRJjiY)

示例:

```py
>>> from transformers import AutoModelForSeq2SeqLM, LoraConfig >>> from peft import AdaLoraModel, AdaLoraConfig
>>> config = AdaLoraConfig(
peft_type="ADALORA", task_type="SEQ_2_SEQ_LM", r=8, lora_alpha=32, target_modules=["q", "v"],
lora_dropout=0.01,
)
>>> model = AutoModelForSeq2SeqLM.from_pretrained("t5-base") >>> model = AdaLoraModel(model, config, "default")
```

**属性**：

+   `model` ([*transformers.PreTrainedModel*]) — 要适应的模型。

+   `peft_config` ([*AdaLoraConfig*])：AdaLora 模型的配置。

#### `update_and_allocate`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adalora/model.py#L306)

```py
( global_step )
```

参数

+   `global_step` (`int`) — 当前训练步骤，用于计算 adalora 预算。

该方法更新 Adalora 预算和掩码。

在`loss.backward()`之后和`zero_grad()`之前的每个训练步骤中应该调用此方法。

`tinit`，`tfinal`和`deltaT`在该方法中处理。

示例：

```py
>>> loss = model(**input).loss
>>> loss.backward()
>>> optimizer.step()
>>> model.base_model.update_and_allocate(i_step)
>>> optimizer.zero_grad()
```
