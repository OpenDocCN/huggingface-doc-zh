# 量化

> 原始文本：[`huggingface.co/docs/peft/developer_guides/quantization`](https://huggingface.co/docs/peft/developer_guides/quantization)

量化使用更少的位表示数据，这是一种减少内存使用和加速推理的有用技术，特别是对于大型语言模型（LLMs）。有几种量化模型的方法，包括：

+   使用[AWQ](https://hf.co/papers/2306.00978)算法优化哪些模型权重被量化

+   使用[GPTQ](https://hf.co/papers/2210.17323)算法独立量化每行权重矩阵

+   使用[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)库将 8 位和 4 位精度独立量化每行权重矩阵

然而，一旦模型被量化，通常不会进一步为下游任务进行训练，因为由于权重和激活的精度降低，训练可能不稳定。但由于 PEFT 方法只添加*额外*的可训练参数，这使您可以在顶部使用 PEFT 适配器训练一个量化模型！将量化与 PEFT 结合起来可以是在单个 GPU 上训练甚至最大模型的一个好策略。例如，[QLoRA](https://hf.co/papers/2305.14314)是一种将模型量化为 4 位，然后使用 LoRA 进行训练的方法。这种方法允许您在单个 48GB GPU 上微调一个 65B 参数模型！

在本指南中，您将看到如何将模型量化为 4 位并使用 LoRA 进行训练。

## 量化模型

[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)是一个具有 Transformers 集成的量化库。通过此集成，您可以将模型量化为 8 位或 4 位，并通过配置[BitsAndBytesConfig](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)类启用许多其他选项。例如，您可以：

+   设置`load_in_4bit=True`在加载时将模型量化为 4 位

+   设置`bnb_4bit_quant_type="nf4"`以使用从正态分布初始化的特殊 4 位数据类型的权重

+   设置`bnb_4bit_use_double_quant=True`以使用嵌套量化方案量化已经量化的权重

+   设置`bnb_4bit_compute_dtype=torch.bfloat16`以使用 bfloat16 进行更快的计算。

```py
import torch
from transformers import BitsAndBytesConfig

config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
)
```

将`config`传递给[from_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained)方法。

```py
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1", quantization_config=config)
```

接下来，您应该调用 prepare_model_for_kbit_training()函数对量化模型进行预处理以进行训练。

```py
from peft import prepare_model_for_kbit_training

model = prepare_model_for_kbit_training(model)
```

现在量化模型已经准备好了，让我们设置一个配置。

## LoraConfig

使用以下参数创建 LoraConfig（或选择您自己的参数）：

```py
from peft import LoraConfig

config = LoraConfig(
    r=16,
    lora_alpha=8,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05
    bias="none",
    task_type="CAUSAL_LM"
)
```

然后使用 get_peft_model()函数从量化模型和配置创建 PeftModel。

```py
from peft import get_peft_model

model = get_peft_model(model, config)
```

您已经准备好使用您喜欢的任何训练方法进行训练了！

### LoftQ 初始化

[LoftQ](https://hf.co/papers/2310.08659)初始化 LoRA 权重，使量化误差最小化，并且在训练量化模型时可以提高性能。要开始，请创建一个`LoftQConfig`并设置`loftq_bits=4`以进行 4 位量化。

LoftQ 初始化不需要在[from_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained)方法中使用`load_in_4bits`参数对基础模型进行量化！在 Initialization options 部分了解更多关于 LoftQ 初始化的信息。

注意：您只能在 GPU 上执行 LoftQ 初始化。

```py
from transformers import AutoModelForCausalLM
from peft import LoftQConfig, LoraConfig, get_peft_model

model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1", device_map="auto")
loftq_config = LoftQConfig(loftq_bits=4)
```

现在将`loftq_config`传递给 LoraConfig 以启用 LoftQ 初始化，并创建一个用于训练的 PeftModel。

```py
lora_config = LoraConfig(
    init_lora_weights="loftq",
    loftq_config=loftq_config,
    r=16,
    lora_alpha=8,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
```

### QLoRA 风格的训练

QLoRA 在 transformer 架构的所有线性层中添加了可训练权重。由于这些线性层的属性名称在不同架构中可能会有所不同，将`target_modules`设置为`"all-linear"`以将 LoRA 添加到所有线性层中：

```py
config = LoraConfig(target_modules="all-linear", ...)
```

## 下一步

如果您对学习更多关于量化感兴趣，以下内容可能会有所帮助：

+   了解更多关于 QLoRA 的细节，并查看其在[使用 bitsandbytes、4 位量化和 QLoRA 使 LLMs 更易访问](https://huggingface.co/blog/4bit-transformers-bitsandbytes)博客文章中的一些基准测试。

+   在 Transformers 的[量化](https://hf.co/docs/transformers/main/quantization)指南中阅读更多关于不同量化方案的内容。
