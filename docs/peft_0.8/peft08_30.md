# æ¨¡å‹

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/peft/package_reference/peft_model`](https://huggingface.co/docs/peft/package_reference/peft_model)

PeftModel æ˜¯æŒ‡å®šåŸºç¡€å˜æ¢å™¨æ¨¡å‹å’Œé…ç½®ä»¥åº”ç”¨ PEFT æ–¹æ³•çš„åŸºç¡€æ¨¡å‹ç±»ã€‚åŸºç¡€`PeftModel`åŒ…å«ä» Hub åŠ è½½å’Œä¿å­˜æ¨¡å‹çš„æ–¹æ³•ã€‚

## PeftModel

### `class peft.PeftModel`

[<æ¥æº>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L88)

```py
( model: PreTrainedModel peft_config: PeftConfig adapter_name: str = 'default' )
```

å‚æ•°

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) â€” ç”¨äº Peft çš„åŸºç¡€å˜æ¢å™¨æ¨¡å‹ã€‚

+   `peft_config` (PeftConfig) â€” Peft æ¨¡å‹çš„é…ç½®ã€‚

+   `adapter_name` (`str`, *å¯é€‰*) â€” é€‚é…å™¨çš„åç§°ï¼Œé»˜è®¤ä¸º`"default"`ã€‚

åŒ…å«å„ç§ Peft æ–¹æ³•çš„åŸºç¡€æ¨¡å‹ã€‚

**å±æ€§**ï¼š

+   `base_model` (`torch.nn.Module`) â€” ç”¨äº Peft çš„åŸºç¡€å˜æ¢å™¨æ¨¡å‹ã€‚

+   `peft_config` (PeftConfig) â€” Peft æ¨¡å‹çš„é…ç½®ã€‚

+   `modules_to_save` (`list` of `str`) â€” ä¿å­˜æ¨¡å‹æ—¶è¦ä¿å­˜çš„å­æ¨¡å—åç§°çš„åˆ—è¡¨ã€‚

+   `prompt_encoder` (PromptEncoder) â€” å¦‚æœä½¿ç”¨ PromptLearningConfigï¼Œåˆ™ç”¨äº Peft çš„æç¤ºç¼–ç å™¨ã€‚

+   `prompt_tokens` (`torch.Tensor`) â€” å¦‚æœä½¿ç”¨ PromptLearningConfigï¼Œåˆ™ç”¨äº Peft çš„è™šæ‹Ÿæç¤ºæ ‡è®°ã€‚

+   `transformer_backbone_name` (`str`) â€” å¦‚æœä½¿ç”¨ PromptLearningConfigï¼Œåˆ™æ˜¯åŸºç¡€æ¨¡å‹ä¸­å˜æ¢å™¨ä¸»å¹²çš„åç§°ã€‚

+   `word_embeddings` (`torch.nn.Embedding`) â€” å¦‚æœä½¿ç”¨ PromptLearningConfigï¼Œåˆ™æ˜¯åŸºç¡€æ¨¡å‹ä¸­å˜æ¢å™¨ä¸»å¹²çš„è¯åµŒå…¥ã€‚

#### `add_adapter`

[<æ¥æº>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L587)

```py
( adapter_name: str peft_config: PeftConfig )
```

å‚æ•°

+   `adapter_name` (`str`) â€” è¦æ·»åŠ çš„é€‚é…å™¨çš„åç§°ã€‚

+   `peft_config` (PeftConfig) â€” è¦æ·»åŠ çš„é€‚é…å™¨çš„é…ç½®ã€‚

æ ¹æ®ä¼ é€’çš„é…ç½®å‘æ¨¡å‹æ·»åŠ é€‚é…å™¨ã€‚

æ–°é€‚é…å™¨çš„åç§°åº”è¯¥æ˜¯å”¯ä¸€çš„ã€‚

æ–°é€‚é…å™¨ä¸ä¼šè‡ªåŠ¨è®¾ç½®ä¸ºæ´»åŠ¨é€‚é…å™¨ã€‚ä½¿ç”¨ PeftModel.set_adapter()æ¥è®¾ç½®æ´»åŠ¨é€‚é…å™¨ã€‚

#### `create_or_update_model_card`

[<æ¥æº>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L777)

```py
( output_dir: str )
```

æ›´æ–°æˆ–åˆ›å»ºæ¨¡å‹å¡ä»¥åŒ…å«æœ‰å…³ peft çš„ä¿¡æ¯ï¼š

1.  æ·»åŠ `peft`åº“æ ‡ç­¾

1.  æ·»åŠ  peft ç‰ˆæœ¬

1.  æ·»åŠ åŸºç¡€æ¨¡å‹ä¿¡æ¯

1.  å¦‚æœä½¿ç”¨äº†é‡åŒ–ä¿¡æ¯ï¼Œåˆ™æ·»åŠ é‡åŒ–ä¿¡æ¯

#### `disable_adapter`

[<æ¥æº>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L547)

```py
( )
```

ç¦ç”¨é€‚é…å™¨æ¨¡å—çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚ä½¿ç”¨æ­¤åŠŸèƒ½åœ¨åŸºç¡€æ¨¡å‹ä¸Šè¿è¡Œæ¨ç†ã€‚

ç¤ºä¾‹ï¼š

```py
>>> with model.disable_adapter():
...     model(inputs)
```

#### `forward`

[<æ¥æº>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L533)

```py
( *args: Any **kwargs: Any )
```

æ¨¡å‹çš„å‰å‘ä¼ é€’ã€‚

#### `from_pretrained`

[<æ¥æº>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L283)

```py
( model: torch.nn.Module model_id: Union[str, os.PathLike] adapter_name: str = 'default' is_trainable: bool = False config: Optional[PeftConfig] = None **kwargs: Any )
```

å‚æ•°

+   `model` (`torch.nn.Module`) â€” è¦é€‚é…çš„æ¨¡å‹ã€‚å¯¹äºğŸ¤—å˜æ¢å™¨æ¨¡å‹ï¼Œæ¨¡å‹åº”è¯¥ä½¿ç”¨[from_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)è¿›è¡Œåˆå§‹åŒ–ã€‚

+   `model_id` (`str`æˆ–`os.PathLike`) â€” è¦ä½¿ç”¨çš„ PEFT é…ç½®çš„åç§°ã€‚å¯ä»¥æ˜¯ï¼š

    +   ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ŒPEFT é…ç½®çš„ `model id`ï¼Œæ‰˜ç®¡åœ¨ Hugging Face Hub ä¸Šçš„æ¨¡å‹å­˜å‚¨åº“ä¸­ã€‚

    +   åŒ…å«ä½¿ç”¨ `save_pretrained` æ–¹æ³•ä¿å­˜çš„ PEFT é…ç½®æ–‡ä»¶çš„ç›®å½•è·¯å¾„ï¼ˆ`./my_peft_config_directory/`ï¼‰ã€‚

+   `adapter_name`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `"default"`ï¼‰ â€” è¦åŠ è½½çš„é€‚é…å™¨çš„åç§°ã€‚è¿™å¯¹äºåŠ è½½å¤šä¸ªé€‚é…å™¨å¾ˆæœ‰ç”¨ã€‚

+   `is_trainable`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`ï¼‰ â€” é€‚é…å™¨æ˜¯å¦åº”è¯¥æ˜¯å¯è®­ç»ƒçš„ã€‚å¦‚æœä¸º `False`ï¼Œé€‚é…å™¨å°†è¢«å†»ç»“ï¼Œåªèƒ½ç”¨äºæ¨æ–­ã€‚

+   `config`ï¼ˆPeftConfigï¼Œ*å¯é€‰*ï¼‰ â€” è¦ä½¿ç”¨çš„é…ç½®å¯¹è±¡ï¼Œè€Œä¸æ˜¯è‡ªåŠ¨åŠ è½½çš„é…ç½®ã€‚æ­¤é…ç½®å¯¹è±¡ä¸ `model_id` å’Œ `kwargs` äº’æ–¥ã€‚åœ¨è°ƒç”¨ `from_pretrained` ä¹‹å‰å·²åŠ è½½é…ç½®æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨ã€‚kwargs â€”ï¼ˆ*å¯é€‰*ï¼‰ï¼šä¼ é€’ç»™ç‰¹å®š PEFT é…ç½®ç±»çš„å…¶ä»–å…³é”®å­—å‚æ•°ã€‚

ä»é¢„è®­ç»ƒæ¨¡å‹å’ŒåŠ è½½çš„ PEFT æƒé‡å®ä¾‹åŒ–ä¸€ä¸ª PEFT æ¨¡å‹ã€‚

æ³¨æ„ï¼Œä¼ é€’çš„ `model` å¯èƒ½ä¼šè¢«å°±åœ°ä¿®æ”¹ã€‚

#### `get_base_model`

[<æ¥æº>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L577)

```py
( )
```

è¿”å›åŸºç¡€æ¨¡å‹ã€‚

#### `get_nb_trainable_parameters`

[<æ¥æº>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L492)

```py
( )
```

è¿”å›æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡å’Œæ‰€æœ‰å‚æ•°çš„æ•°é‡ã€‚

#### `get_prompt`

[<æ¥æº>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L446)

```py
( batch_size: int task_ids: Optional[torch.Tensor] = None )
```

è¿”å›ç”¨äº Peft çš„è™šæ‹Ÿæç¤ºã€‚ä»…åœ¨ä½¿ç”¨æç¤ºå­¦ä¹ æ–¹æ³•æ—¶é€‚ç”¨ã€‚

#### `get_prompt_embedding_to_save`

[<æ¥æº>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L427)

```py
( adapter_name: str )
```

åœ¨ä¿å­˜æ¨¡å‹æ—¶è¿”å›æç¤ºåµŒå…¥ã€‚ä»…åœ¨ä½¿ç”¨æç¤ºå­¦ä¹ æ–¹æ³•æ—¶é€‚ç”¨ã€‚

#### `load_adapter`

[<æ¥æº>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L652)

```py
( model_id: str adapter_name: str is_trainable: bool = False **kwargs: Any )
```

å‚æ•°

+   `adapter_name`ï¼ˆ`str`ï¼‰ â€” è¦æ·»åŠ çš„é€‚é…å™¨çš„åç§°ã€‚

+   `peft_config`ï¼ˆPeftConfigï¼‰ â€” è¦æ·»åŠ çš„é€‚é…å™¨çš„é…ç½®ã€‚

+   `is_trainable`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`ï¼‰ â€” é€‚é…å™¨æ˜¯å¦åº”è¯¥æ˜¯å¯è®­ç»ƒçš„ã€‚å¦‚æœä¸º `False`ï¼Œé€‚é…å™¨å°†è¢«å†»ç»“ï¼Œåªèƒ½ç”¨äºæ¨æ–­ã€‚kwargs â€”ï¼ˆ*å¯é€‰*ï¼‰ï¼šä¿®æ”¹åŠ è½½é€‚é…å™¨æ–¹å¼çš„å…¶ä»–å‚æ•°ï¼Œä¾‹å¦‚ Hugging Face Hub çš„ä»¤ç‰Œã€‚

å°†è®­ç»ƒå¥½çš„é€‚é…å™¨åŠ è½½åˆ°æ¨¡å‹ä¸­ã€‚

æ–°é€‚é…å™¨çš„åç§°åº”è¯¥æ˜¯å”¯ä¸€çš„ã€‚

æ–°é€‚é…å™¨ä¸ä¼šè‡ªåŠ¨è®¾ç½®ä¸ºæ´»åŠ¨é€‚é…å™¨ã€‚ä½¿ç”¨ PeftModel.set_adapter() æ¥è®¾ç½®æ´»åŠ¨é€‚é…å™¨ã€‚

#### `print_trainable_parameters`

[<æ¥æº>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L516)

```py
( )
```

æ‰“å°æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚

#### `save_pretrained`

[<æ¥æº>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L161)

```py
( save_directory: str safe_serialization: bool = True selected_adapters: Optional[list[str]] = None save_embedding_layers: Union[str, bool] = 'auto' is_main_process: bool = True **kwargs: Any )
```

å‚æ•°

+   `save_directory`ï¼ˆ`str`ï¼‰ â€” é€‚é…å™¨æ¨¡å‹å’Œé…ç½®æ–‡ä»¶å°†ä¿å­˜åœ¨å…¶ä¸­çš„ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨å°†åˆ›å»ºï¼‰ã€‚

+   `safe_serialization`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦ä»¥ safetensors æ ¼å¼ä¿å­˜é€‚é…å™¨æ–‡ä»¶ï¼Œé»˜è®¤ä¸º `True`ã€‚

+   `selected_adapters`ï¼ˆ`List[str]`ï¼Œ*å¯é€‰*ï¼‰ â€” è¦ä¿å­˜çš„é€‚é…å™¨åˆ—è¡¨ã€‚å¦‚æœä¸º `None`ï¼Œå°†é»˜è®¤ä¸ºæ‰€æœ‰é€‚é…å™¨ã€‚

+   `save_embedding_layers`ï¼ˆ`Union[bool, str]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `"auto"`ï¼‰ â€” å¦‚æœä¸º `True`ï¼Œé™¤äº†é€‚é…å™¨æƒé‡å¤–è¿˜ä¿å­˜åµŒå…¥å±‚ã€‚å¦‚æœä¸º `auto`ï¼Œåˆ™åœ¨å¯ç”¨æ—¶æ£€æŸ¥é…ç½®çš„ `target_modules` ä¸­çš„å¸¸è§åµŒå…¥å±‚ `peft.utils.other.EMBEDDING_LAYER_NAMES`ï¼Œå¹¶è‡ªåŠ¨è®¾ç½®å¸ƒå°”æ ‡å¿—ã€‚è¿™ä»…é€‚ç”¨äº ğŸ¤— transformers æ¨¡å‹ã€‚

+   `is_main_process` (`bool`, *å¯é€‰*) â€” è°ƒç”¨æ­¤å‡½æ•°çš„è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ã€‚é»˜è®¤ä¸º `True`ã€‚å¦‚æœä¸æ˜¯ä¸»è¿›ç¨‹ï¼Œåˆ™ä¸ä¼šä¿å­˜æ£€æŸ¥ç‚¹ï¼Œè¿™å¯¹äºå¤šè®¾å¤‡è®¾ç½®ï¼ˆä¾‹å¦‚ DDPï¼‰å¾ˆé‡è¦ã€‚

+   `kwargs`ï¼ˆé¢å¤–çš„å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰ â€” ä¼ é€’ç»™ `push_to_hub` æ–¹æ³•çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚

æ­¤å‡½æ•°å°†é€‚é…å™¨æ¨¡å‹å’Œé€‚é…å™¨é…ç½®æ–‡ä»¶ä¿å­˜åˆ°ä¸€ä¸ªç›®å½•ä¸­ï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨ PeftModel.from_pretrained() ç±»æ–¹æ³•é‡æ–°åŠ è½½ï¼Œå¹¶ä¸”ä¹Ÿå¯ä»¥è¢« `PeftModel.push_to_hub()` æ–¹æ³•ä½¿ç”¨ã€‚

#### `set_adapter`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L743)

```py
( adapter_name: str )
```

å‚æ•°

+   `adapter_name` (`str`) â€” è¦è®¾ç½®ä¸ºæ´»åŠ¨çš„é€‚é…å™¨çš„åç§°ã€‚å¿…é¡»å…ˆåŠ è½½é€‚é…å™¨ã€‚

è®¾ç½®æ´»åŠ¨é€‚é…å™¨ã€‚

ä¸€æ¬¡åªèƒ½æœ‰ä¸€ä¸ªé€‚é…å™¨å¤„äºæ´»åŠ¨çŠ¶æ€ã€‚

æ­¤å¤–ï¼Œæ­¤å‡½æ•°å°†è®¾ç½®æŒ‡å®šçš„é€‚é…å™¨ä¸ºå¯è®­ç»ƒçš„ï¼ˆå³ï¼Œrequires_grad=Trueï¼‰ã€‚å¦‚æœä¸éœ€è¦æ­¤åŠŸèƒ½ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹ä»£ç ã€‚

```py
>>> for name, param in model_peft.named_parameters():
...     if ...:  # some check on name (ex. if 'lora' in name)
...         param.requires_grad = False
```

## PeftModelForSequenceClassification

ç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡çš„ `PeftModel`ã€‚

### `class peft.PeftModelForSequenceClassification`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L830)

```py
( model: torch.nn.Module peft_config: PeftConfig adapter_name: str = 'default' )
```

å‚æ•°

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) â€” åŸºç¡€å˜æ¢å™¨æ¨¡å‹ã€‚

+   `peft_config` (PeftConfig) â€” Peft é…ç½®ã€‚

ç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡çš„ Peft æ¨¡å‹ã€‚

**å±æ€§**:

+   `config` ([PretrainedConfig](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)) â€” åŸºç¡€æ¨¡å‹çš„é…ç½®å¯¹è±¡ã€‚

+   `cls_layer_name` (`str`) â€” åˆ†ç±»å±‚çš„åç§°ã€‚

ç¤ºä¾‹:

```py
>>> from transformers import AutoModelForSequenceClassification
>>> from peft import PeftModelForSequenceClassification, get_peft_config

>>> config = {
...     "peft_type": "PREFIX_TUNING",
...     "task_type": "SEQ_CLS",
...     "inference_mode": False,
...     "num_virtual_tokens": 20,
...     "token_dim": 768,
...     "num_transformer_submodules": 1,
...     "num_attention_heads": 12,
...     "num_layers": 12,
...     "encoder_hidden_size": 768,
...     "prefix_projection": False,
...     "postprocess_past_key_value_function": None,
... }

>>> peft_config = get_peft_config(config)
>>> model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")
>>> peft_model = PeftModelForSequenceClassification(model, peft_config)
>>> peft_model.print_trainable_parameters()
trainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117
```

## PeftModelForTokenClassification

ç”¨äºæ ‡è®°åˆ†ç±»ä»»åŠ¡çš„ `PeftModel`ã€‚

### `class peft.PeftModelForTokenClassification`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L1460)

```py
( model: torch.nn.Module peft_config: PeftConfig = None adapter_name: str = 'default' )
```

å‚æ•°

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) â€” åŸºç¡€å˜æ¢å™¨æ¨¡å‹ã€‚

+   `peft_config` (PeftConfig) â€” Peft é…ç½®ã€‚

ç”¨äºæ ‡è®°åˆ†ç±»ä»»åŠ¡çš„ Peft æ¨¡å‹ã€‚

**å±æ€§**:

+   `config` ([PretrainedConfig](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)) â€” åŸºç¡€æ¨¡å‹çš„é…ç½®å¯¹è±¡ã€‚

+   `cls_layer_name` (`str`) â€” åˆ†ç±»å±‚çš„åç§°ã€‚

ç¤ºä¾‹:

```py
>>> from transformers import AutoModelForSequenceClassification
>>> from peft import PeftModelForTokenClassification, get_peft_config

>>> config = {
...     "peft_type": "PREFIX_TUNING",
...     "task_type": "TOKEN_CLS",
...     "inference_mode": False,
...     "num_virtual_tokens": 20,
...     "token_dim": 768,
...     "num_transformer_submodules": 1,
...     "num_attention_heads": 12,
...     "num_layers": 12,
...     "encoder_hidden_size": 768,
...     "prefix_projection": False,
...     "postprocess_past_key_value_function": None,
... }

>>> peft_config = get_peft_config(config)
>>> model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")
>>> peft_model = PeftModelForTokenClassification(model, peft_config)
>>> peft_model.print_trainable_parameters()
trainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117
```

## PeftModelForCausalLM

ç”¨äºå› æœè¯­è¨€å»ºæ¨¡çš„ `PeftModel`ã€‚

### `class peft.PeftModelForCausalLM`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L1021)

```py
( model: torch.nn.Module peft_config: PeftConfig adapter_name: str = 'default' )
```

å‚æ•°

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) â€” åŸºç¡€å˜æ¢å™¨æ¨¡å‹ã€‚

+   `peft_config` (PeftConfig) â€” Peft é…ç½®ã€‚

ç”¨äºå› æœè¯­è¨€å»ºæ¨¡çš„ Peft æ¨¡å‹ã€‚

ç¤ºä¾‹:

```py
>>> from transformers import AutoModelForCausalLM
>>> from peft import PeftModelForCausalLM, get_peft_config

>>> config = {
...     "peft_type": "PREFIX_TUNING",
...     "task_type": "CAUSAL_LM",
...     "inference_mode": False,
...     "num_virtual_tokens": 20,
...     "token_dim": 1280,
...     "num_transformer_submodules": 1,
...     "num_attention_heads": 20,
...     "num_layers": 36,
...     "encoder_hidden_size": 1280,
...     "prefix_projection": False,
...     "postprocess_past_key_value_function": None,
... }

>>> peft_config = get_peft_config(config)
>>> model = AutoModelForCausalLM.from_pretrained("gpt2-large")
>>> peft_model = PeftModelForCausalLM(model, peft_config)
>>> peft_model.print_trainable_parameters()
trainable params: 1843200 || all params: 775873280 || trainable%: 0.23756456724479544
```

## PeftModelForSeq2SeqLM

ç”¨äºåºåˆ—åˆ°åºåˆ—è¯­è¨€å»ºæ¨¡çš„ `PeftModel`ã€‚

### `class peft.PeftModelForSeq2SeqLM`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L1211)

```py
( model: torch.nn.Module peft_config: PeftConfig adapter_name: str = 'default' )
```

å‚æ•°

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) â€” åŸºç¡€å˜æ¢å™¨æ¨¡å‹ã€‚

+   `peft_config` (PeftConfig) â€” Peft é…ç½®ã€‚

ç”¨äºåºåˆ—åˆ°åºåˆ—è¯­è¨€å»ºæ¨¡çš„ Peft æ¨¡å‹ã€‚

ç¤ºä¾‹:

```py
>>> from transformers import AutoModelForSeq2SeqLM
>>> from peft import PeftModelForSeq2SeqLM, get_peft_config

>>> config = {
...     "peft_type": "LORA",
...     "task_type": "SEQ_2_SEQ_LM",
...     "inference_mode": False,
...     "r": 8,
...     "target_modules": ["q", "v"],
...     "lora_alpha": 32,
...     "lora_dropout": 0.1,
...     "fan_in_fan_out": False,
...     "enable_lora": None,
...     "bias": "none",
... }

>>> peft_config = get_peft_config(config)
>>> model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")
>>> peft_model = PeftModelForSeq2SeqLM(model, peft_config)
>>> peft_model.print_trainable_parameters()
trainable params: 884736 || all params: 223843584 || trainable%: 0.3952474242013566
```

## PeftModelForQuestionAnswering

ç”¨äºé—®ç­”çš„ `PeftModel`ã€‚

### `class peft.PeftModelForQuestionAnswering`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L1635)

```py
( model: torch.nn.Module peft_config: PeftConfig adapter_name: str = 'default' )
```

å‚æ•°

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) â€” åŸºç¡€å˜æ¢å™¨æ¨¡å‹ã€‚

+   `peft_config` (PeftConfig) â€” Peft é…ç½®ã€‚

ç”¨äºæŠ½å–å¼é—®ç­”çš„ Peft æ¨¡å‹ã€‚

**å±æ€§**ï¼š

+   `config` ([PretrainedConfig](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)) â€” åŸºç¡€æ¨¡å‹çš„é…ç½®å¯¹è±¡ã€‚

+   `cls_layer_name` (`str`) â€” åˆ†ç±»å±‚çš„åç§°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoModelForQuestionAnswering
>>> from peft import PeftModelForQuestionAnswering, get_peft_config

>>> config = {
...     "peft_type": "LORA",
...     "task_type": "QUESTION_ANS",
...     "inference_mode": False,
...     "r": 16,
...     "target_modules": ["query", "value"],
...     "lora_alpha": 32,
...     "lora_dropout": 0.05,
...     "fan_in_fan_out": False,
...     "bias": "none",
... }

>>> peft_config = get_peft_config(config)
>>> model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")
>>> peft_model = PeftModelForQuestionAnswering(model, peft_config)
>>> peft_model.print_trainable_parameters()
trainable params: 592900 || all params: 108312580 || trainable%: 0.5473971721475013
```

## PeftModelForFeatureExtraction

ç”¨äºä»å˜æ¢å™¨æ¨¡å‹ä¸­æå–ç‰¹å¾/åµŒå…¥çš„`PeftModel`ã€‚

### `class peft.PeftModelForFeatureExtraction`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L1830)

```py
( model: torch.nn.Module peft_config: PeftConfig adapter_name: str = 'default' )
```

å‚æ•°

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) â€” åŸºç¡€å˜æ¢å™¨æ¨¡å‹ã€‚

+   `peft_config` (PeftConfig) â€” Peft é…ç½®ã€‚

ç”¨äºä»å˜æ¢å™¨æ¨¡å‹ä¸­æå–ç‰¹å¾/åµŒå…¥çš„ Peft æ¨¡å‹ã€‚

**å±æ€§**ï¼š

+   `config` ([PretrainedConfig](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)) â€” åŸºç¡€æ¨¡å‹çš„é…ç½®å¯¹è±¡ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoModel
>>> from peft import PeftModelForFeatureExtraction, get_peft_config

>>> config = {
...     "peft_type": "LORA",
...     "task_type": "FEATURE_EXTRACTION",
...     "inference_mode": False,
...     "r": 16,
...     "target_modules": ["query", "value"],
...     "lora_alpha": 32,
...     "lora_dropout": 0.05,
...     "fan_in_fan_out": False,
...     "bias": "none",
... }
>>> peft_config = get_peft_config(config)
>>> model = AutoModel.from_pretrained("bert-base-cased")
>>> peft_model = PeftModelForFeatureExtraction(model, peft_config)
>>> peft_model.print_trainable_parameters()
```

## PeftMixedModel

ç”¨äºæ··åˆä¸åŒé€‚é…å™¨ç±»å‹ï¼ˆä¾‹å¦‚ LoRA å’Œ LoHaï¼‰çš„`PeftModel`ã€‚

### `class peft.PeftMixedModel`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L83)

```py
( model: nn.Module peft_config: PeftConfig adapter_name: str = 'default' )
```

å‚æ•°

+   `model` (`torch.nn.Module`) â€” è¦è°ƒæ•´çš„æ¨¡å‹ã€‚

+   `config` (`PeftConfig`) â€” è¦è°ƒæ•´çš„æ¨¡å‹çš„é…ç½®ã€‚é€‚é…å™¨ç±»å‹å¿…é¡»å…¼å®¹ã€‚

+   `adapter_name` (`str`, `å¯é€‰`, é»˜è®¤ä¸º`"default"`) â€” ç¬¬ä¸€ä¸ªé€‚é…å™¨çš„åç§°ã€‚

ç”¨äºåŠ è½½ä¸åŒç±»å‹é€‚é…å™¨è¿›è¡Œæ¨æ–­çš„ PeftMixedModelã€‚

æ­¤ç±»ä¸æ”¯æŒåŠ è½½/ä¿å­˜ï¼Œé€šå¸¸ä¸åº”ç›´æ¥åˆå§‹åŒ–ã€‚è€Œæ˜¯ä½¿ç”¨`get_peft_model`å¹¶ä½¿ç”¨å‚æ•°`mixed=True`ã€‚

é˜…è¯»[Mixed adapter types](https://huggingface.co/docs/peft/en/developer_guides/mixed_models)æŒ‡å—ï¼Œäº†è§£æ›´å¤šå…³äºä½¿ç”¨ä¸åŒé€‚é…å™¨ç±»å‹çš„ä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from peft import get_peft_model

>>> base_model = ...  # load the base model, e.g. from transformers
>>> peft_model = PeftMixedModel.from_pretrained(base_model, path_to_adapter1, "adapter1").eval()
>>> peft_model.load_adapter(path_to_adapter2, "adapter2")
>>> peft_model.set_adapter(["adapter1", "adapter2"])  # activate both adapters
>>> peft_model(data)  # forward pass using both adapters
```

#### `disable_adapter`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L203)

```py
( )
```

ç¦ç”¨é€‚é…å™¨æ¨¡å—ã€‚

#### `forward`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L191)

```py
( *args: Any **kwargs: Any )
```

æ¨¡å‹çš„å‰å‘ä¼ é€’ã€‚

#### `from_pretrained`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L329)

```py
( model: nn.Module model_id: str | os.PathLike adapter_name: str = 'default' is_trainable: bool = False config: Optional[PeftConfig] = None **kwargs: Any )
```

å‚æ•°

+   `model` (`nn.Module`) â€” è¦é€‚åº”çš„æ¨¡å‹ã€‚

+   `model_id` (`str`æˆ–`os.PathLike`) â€” è¦ä½¿ç”¨çš„ PEFT é…ç½®çš„åç§°ã€‚å¯ä»¥æ˜¯ï¼š

    +   ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ŒPEFT é…ç½®çš„`æ¨¡å‹ id`ï¼Œæ‰˜ç®¡åœ¨ Hugging Face Hub ä¸Šçš„æ¨¡å‹å­˜å‚¨åº“ä¸­ã€‚

    +   åŒ…å«ä½¿ç”¨`save_pretrained`æ–¹æ³•ä¿å­˜çš„ PEFT é…ç½®æ–‡ä»¶çš„ç›®å½•è·¯å¾„ï¼ˆ`./my_peft_config_directory/`ï¼‰ã€‚

+   `adapter_name` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`"default"`) â€” è¦åŠ è½½çš„é€‚é…å™¨çš„åç§°ã€‚è¿™å¯¹äºåŠ è½½å¤šä¸ªé€‚é…å™¨å¾ˆæœ‰ç”¨ã€‚

+   `is_trainable` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” é€‚é…å™¨æ˜¯å¦å¯è®­ç»ƒã€‚å¦‚æœä¸º`False`ï¼Œé€‚é…å™¨å°†è¢«å†»ç»“å¹¶ç”¨äºæ¨æ–­ã€‚

+   `config` (PeftConfig, *å¯é€‰*) â€” è¦ä½¿ç”¨çš„é…ç½®å¯¹è±¡ï¼Œè€Œä¸æ˜¯è‡ªåŠ¨åŠ è½½çš„é…ç½®ã€‚æ­¤é…ç½®å¯¹è±¡ä¸`model_id`å’Œ`kwargs`äº’æ–¥ã€‚åœ¨è°ƒç”¨`from_pretrained`ä¹‹å‰å·²åŠ è½½é…ç½®æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨ã€‚kwargs â€” (`å¯é€‰`): ä¼ é€’ç»™ç‰¹å®š PEFT é…ç½®ç±»çš„å…¶ä»–å…³é”®å­—å‚æ•°ã€‚

ä»é¢„è®­ç»ƒæ¨¡å‹å’ŒåŠ è½½çš„ PEFT æƒé‡å®ä¾‹åŒ–ä¸€ä¸ª PEFT æ··åˆæ¨¡å‹ã€‚

è¯·æ³¨æ„ï¼Œä¼ é€’çš„ `model` å¯èƒ½ä¼šè¢«åŸåœ°ä¿®æ”¹ã€‚

#### `generate`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L197)

```py
( *args: Any **kwargs: Any )
```

ç”Ÿæˆè¾“å‡ºã€‚

#### `get_nb_trainable_parameters`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L146)

```py
( )
```

è¿”å›æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡å’Œæ‰€æœ‰å‚æ•°çš„æ•°é‡ã€‚

#### `merge_and_unload`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L283)

```py
( *args: Any **kwargs: Any )
```

å‚æ•°

+   `progressbar` (`bool`) â€” æ˜¯å¦æ˜¾ç¤ºæŒ‡ç¤ºå¸è½½å’Œåˆå¹¶è¿‡ç¨‹çš„è¿›åº¦æ¡

+   `safe_merge` (`bool`) â€” æ˜¯å¦æ¿€æ´»å®‰å…¨åˆå¹¶æ£€æŸ¥ä»¥æ£€æŸ¥é€‚é…å™¨æƒé‡ä¸­æ˜¯å¦å­˜åœ¨æ½œåœ¨çš„ NaN

+   `adapter_names` (`List[str]`, *optional*) â€” åº”åˆå¹¶çš„é€‚é…å™¨åç§°åˆ—è¡¨ã€‚å¦‚æœä¸º Noneï¼Œåˆ™å°†åˆå¹¶æ‰€æœ‰æ´»åŠ¨é€‚é…å™¨ã€‚é»˜è®¤ä¸º `None`ã€‚

æ­¤æ–¹æ³•å°†é€‚é…å™¨å±‚åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­ã€‚å¦‚æœæœ‰äººæƒ³è¦å°†åŸºç¡€æ¨¡å‹ç”¨ä½œç‹¬ç«‹æ¨¡å‹ï¼Œåˆ™éœ€è¦è¿™æ ·åšã€‚

#### `print_trainable_parameters`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L171)

```py
( )
```

æ‰“å°æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚

#### `set_adapter`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L237)

```py
( adapter_name: Union[str, list[str]] )
```

å‚æ•°

+   `adapter_name` (`str` æˆ– `List[str]`) â€” è¦æ¿€æ´»çš„é€‚é…å™¨çš„åç§°ã€‚

è®¾ç½®æ¨¡å‹çš„æ´»åŠ¨é€‚é…å™¨ã€‚

è¯·æ³¨æ„ï¼Œåœ¨å‰å‘ä¼ é€’æœŸé—´åº”ç”¨é€‚é…å™¨çš„é¡ºåºå¯èƒ½ä¸å°†é€‚é…å™¨ä¼ é€’ç»™æ­¤å‡½æ•°çš„é¡ºåºä¸åŒã€‚ç›¸åï¼Œåœ¨å‰å‘ä¼ é€’æœŸé—´çš„é¡ºåºç”±é€‚é…å™¨åŠ è½½åˆ°æ¨¡å‹ä¸­çš„é¡ºåºç¡®å®šã€‚æ´»åŠ¨é€‚é…å™¨ä»…ç¡®å®šåœ¨å‰å‘ä¼ é€’æœŸé—´å“ªäº›é€‚é…å™¨å¤„äºæ´»åŠ¨çŠ¶æ€ï¼Œä½†ä¸ç¡®å®šå®ƒä»¬è¢«åº”ç”¨çš„é¡ºåºã€‚

æ­¤å¤–ï¼Œæ­¤å‡½æ•°å°†æŒ‡å®šçš„é€‚é…å™¨è®¾ç½®ä¸ºå¯è®­ç»ƒï¼ˆå³ï¼Œrequires_grad=Trueï¼‰ã€‚å¦‚æœä¸éœ€è¦è¿™æ ·åšï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹ä»£ç ã€‚

```py
>>> for name, param in model_peft.named_parameters():
...     if ...:  # some check on name (ex. if 'lora' in name)
...         param.requires_grad = False
```

#### `unload`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L300)

```py
( *args: Any **kwargs: Any )
```

é€šè¿‡åˆ é™¤æ‰€æœ‰é€‚é…å™¨æ¨¡å—è€Œä¸è¿›è¡Œåˆå¹¶æ¥è·å–åŸºç¡€æ¨¡å‹ã€‚è¿™å°†è¿˜åŸåŸå§‹åŸºç¡€æ¨¡å‹ã€‚

## å®ç”¨å·¥å…·

#### `peft.cast_mixed_precision_params`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/utils/other.py#L523)

```py
( model dtype )
```

å‚æ•°

+   `model` (`torch.nn.Module`) â€” è¦è½¬æ¢ä¸å¯è®­ç»ƒå‚æ•°çš„æ¨¡å‹ã€‚

+   `dtype` (`torch.dtype`) â€” è¦å°†ä¸å¯è®­ç»ƒå‚æ•°è½¬æ¢ä¸ºçš„ dtypeã€‚`dtype` å¯ä»¥æ˜¯ `torch.float16` æˆ–

å°†æ¨¡å‹çš„æ‰€æœ‰ä¸å¯è®­ç»ƒå‚æ•°è½¬æ¢ä¸ºç»™å®šçš„ `dtype`ã€‚`dtype` å¯ä»¥æ˜¯ `torch.float16` æˆ– `torch.bfloat16`ï¼Œæ ¹æ®æ‚¨æ­£åœ¨æ‰§è¡Œçš„æ··åˆç²¾åº¦è®­ç»ƒè€Œå®šã€‚å¯è®­ç»ƒå‚æ•°è½¬æ¢ä¸ºå…¨ç²¾åº¦ã€‚è¿™æ—¨åœ¨é€šè¿‡ä½¿ç”¨åŠç²¾åº¦ dtype å‡å°‘ä½¿ç”¨ PEFT æ–¹æ³•æ—¶çš„ GPU å†…å­˜ä½¿ç”¨é‡ã€‚å°†å¯è®­ç»ƒå‚æ•°ä¿ç•™ä¸ºå…¨ç²¾åº¦å¯åœ¨ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒæ—¶ä¿æŒè®­ç»ƒç¨³å®šæ€§ã€‚

`torch.bfloat16` æ ¹æ®æ‚¨æ­£åœ¨æ‰§è¡Œçš„æ··åˆç²¾åº¦è®­ç»ƒã€‚

#### `peft.get_peft_model`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mapping.py#L106)

```py
( model: PreTrainedModel peft_config: PeftConfig adapter_name: str = 'default' mixed: bool = False )
```

å‚æ•°

+   `model` ([transformers.PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) â€” Model to be wrapped.

+   `peft_config` (PeftConfig) â€” åŒ…å« Peft æ¨¡å‹å‚æ•°çš„é…ç½®å¯¹è±¡ã€‚

+   `adapter_name` (`str`, `optional`, é»˜è®¤ä¸º `"default"`) â€” è¦æ³¨å…¥çš„é€‚é…å™¨çš„åç§°ï¼Œå¦‚æœæœªæä¾›ï¼Œåˆ™ä½¿ç”¨é»˜è®¤é€‚é…å™¨åç§°ï¼ˆ`"default"`ï¼‰ã€‚

+   `mixed` (`bool`, `optional`, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦å…è®¸æ··åˆä¸åŒï¼ˆå…¼å®¹çš„ï¼‰é€‚é…å™¨ç±»å‹ã€‚

ä»æ¨¡å‹å’Œé…ç½®è¿”å›ä¸€ä¸ª Peft æ¨¡å‹å¯¹è±¡ã€‚

#### `peft.inject_adapter_in_model`

[<æ¥æº>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mapping.py#L139)

```py
( peft_config: PeftConfig model: torch.nn.Module adapter_name: str = 'default' )
```

å‚æ•°

+   `peft_config`ï¼ˆ`PeftConfig`ï¼‰- åŒ…å« Peft æ¨¡å‹å‚æ•°çš„é…ç½®å¯¹è±¡ã€‚

+   `model`ï¼ˆ`torch.nn.Module`ï¼‰- è¦æ³¨å…¥é€‚é…å™¨çš„è¾“å…¥æ¨¡å‹ã€‚

+   `adapter_name`ï¼ˆ`str`ï¼Œ`å¯é€‰`ï¼Œé»˜è®¤ä¸º`"default"`ï¼‰- è¦æ³¨å…¥çš„é€‚é…å™¨çš„åç§°ï¼Œå¦‚æœæœªæä¾›ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„é€‚é…å™¨åç§°ï¼ˆ`"default"`ï¼‰ã€‚

ä¸€ä¸ªç®€å•çš„ APIï¼Œç”¨äºåœ¨æ¨¡å‹ä¸­åˆ›å»ºå¹¶å°±åœ°æ³¨å…¥é€‚é…å™¨ã€‚ç›®å‰ï¼Œè¯¥ API ä¸æ”¯æŒæç¤ºå­¦ä¹ æ–¹æ³•å’Œé€‚åº”æç¤ºã€‚è¯·ç¡®ä¿åœ¨`peft_config`å¯¹è±¡ä¸­è®¾ç½®äº†æ­£ç¡®çš„`target_names`ã€‚API åœ¨åº•å±‚è°ƒç”¨`get_peft_model`ï¼Œä½†ä»…é™äºéæç¤ºå­¦ä¹ æ–¹æ³•ã€‚

#### `peft.get_peft_model_state_dict`

[<æ¥æº>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/utils/save_and_load.py#L46)

```py
( model state_dict = None adapter_name = 'default' unwrap_compiled = False save_embedding_layers = 'auto' )
```

å‚æ•°

+   `model`ï¼ˆPeftModelï¼‰- Peft æ¨¡å‹ã€‚åœ¨ä½¿ç”¨ torch.nn.DistributedDataParallelã€DeepSpeed æˆ– FSDP æ—¶ï¼Œæ¨¡å‹åº”ä¸ºåŸºç¡€æ¨¡å‹/å–æ¶ˆåŒ…è£…æ¨¡å‹ï¼ˆå³ model.moduleï¼‰ã€‚

+   `state_dict`ï¼ˆ`dict`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰- æ¨¡å‹çš„çŠ¶æ€å­—å…¸ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†ä½¿ç”¨ä¼ é€’æ¨¡å‹çš„çŠ¶æ€å­—å…¸ã€‚

+   `adapter_name`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"default"`ï¼‰- åº”è¿”å›å…¶çŠ¶æ€å­—å…¸çš„é€‚é…å™¨çš„åç§°ã€‚

+   `unwrap_compiled`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰- æ˜¯å¦åœ¨ä½¿ç”¨ torch.compile æ—¶å–æ¶ˆåŒ…è£…æ¨¡å‹ã€‚

+   `save_embedding_layers`ï¼ˆ`Union[bool, str]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`auto`ï¼‰- å¦‚æœä¸º`True`ï¼Œåˆ™ä¿å­˜åµŒå…¥å±‚ä»¥åŠé€‚é…å™¨æƒé‡ã€‚å¦‚æœä¸º`auto`ï¼Œåˆ™åœ¨é…ç½®çš„`target_modules`ä¸­æ£€æŸ¥å¸¸è§çš„åµŒå…¥å±‚`peft.utils.other.EMBEDDING_LAYER_NAMES`ï¼Œå¹¶æ ¹æ®å…¶è®¾ç½®å¸ƒå°”æ ‡å¿—ã€‚è¿™ä»…é€‚ç”¨äºğŸ¤— transformers æ¨¡å‹ã€‚

è·å– Peft æ¨¡å‹çš„çŠ¶æ€å­—å…¸ã€‚

#### `peft.prepare_model_for_kbit_training`

[<æ¥æº>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/utils/other.py#L77)

```py
( model use_gradient_checkpointing = True gradient_checkpointing_kwargs = None )
```

å‚æ•°

+   `model`ï¼ˆ`transformers.PreTrainedModel`ï¼‰- ä»`transformers`åŠ è½½çš„æ¨¡å‹

+   `use_gradient_checkpointing`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰- å¦‚æœä¸º Trueï¼Œåˆ™ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜ï¼Œä½†ä¼šå¯¼è‡´åå‘ä¼ æ’­é€Ÿåº¦å˜æ…¢ã€‚

+   `gradient_checkpointing_kwargs`ï¼ˆ`dict`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰- ä¼ é€’ç»™æ¢¯åº¦æ£€æŸ¥ç‚¹å‡½æ•°çš„å…³é”®å­—å‚æ•°ï¼Œè¯·å‚è€ƒ`torch.utils.checkpoint.checkpoint`çš„æ–‡æ¡£ï¼Œäº†è§£å¯ä»¥ä¼ é€’ç»™è¯¥æ–¹æ³•çš„å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚è¯·æ³¨æ„ï¼Œè¿™ä»…é€‚ç”¨äºæœ€æ–°çš„ transformers ç‰ˆæœ¬ï¼ˆ> 4.34.1ï¼‰ã€‚

è¯·æ³¨æ„ï¼Œæ­¤æ–¹æ³•ä»…é€‚ç”¨äº`transformers`æ¨¡å‹ã€‚

æ­¤æ–¹æ³•åŒ…è£…äº†å‡†å¤‡æ¨¡å‹åœ¨è¿è¡Œè®­ç»ƒä¹‹å‰çš„æ•´ä¸ªåè®®ã€‚è¿™åŒ…æ‹¬ï¼š1- å°† layernorm è½¬æ¢ä¸º fp32 2- ä½¿è¾“å‡ºåµŒå…¥å±‚éœ€è¦æ¢¯åº¦ 3- å°† lm å¤´çš„å‡çº§è½¬æ¢ä¸º fp32
