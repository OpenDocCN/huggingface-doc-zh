# è‡ªå®šä¹‰æ¨¡å‹

> åŸæ–‡é“¾æ¥ï¼š[`huggingface.co/docs/peft/developer_guides/custom_models`](https://huggingface.co/docs/peft/developer_guides/custom_models)

ä¸€äº›å¾®è°ƒæŠ€æœ¯ï¼Œå¦‚æç¤ºå¾®è°ƒï¼Œæ˜¯ç‰¹å®šäºè¯­è¨€æ¨¡å‹çš„ã€‚è¿™æ„å‘³ç€åœ¨ğŸ¤— PEFT ä¸­ï¼Œå‡å®šæ­£åœ¨ä½¿ç”¨ğŸ¤— Transformers æ¨¡å‹ã€‚ç„¶è€Œï¼Œå…¶ä»–å¾®è°ƒæŠ€æœ¯ - å¦‚ LoRA - ä¸é™äºç‰¹å®šçš„æ¨¡å‹ç±»å‹ã€‚

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ° LoRA å¦‚ä½•åº”ç”¨äºå¤šå±‚æ„ŸçŸ¥å™¨ï¼Œæ¥è‡ª[timm](https://huggingface.co/docs/timm/index)åº“çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹ï¼Œæˆ–è€…ä¸€ä¸ªæ–°çš„ğŸ¤— Transformers æ¶æ„ã€‚

## å¤šå±‚æ„ŸçŸ¥å™¨

å‡è®¾æˆ‘ä»¬æƒ³è¦ç”¨ LoRA å¾®è°ƒä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ã€‚è¿™æ˜¯å®šä¹‰ï¼š

```py
from torch import nn

class MLP(nn.Module):
    def __init__(self, num_units_hidden=2000):
        super().__init__()
        self.seq = nn.Sequential(
            nn.Linear(20, num_units_hidden),
            nn.ReLU(),
            nn.Linear(num_units_hidden, num_units_hidden),
            nn.ReLU(),
            nn.Linear(num_units_hidden, 2),
            nn.LogSoftmax(dim=-1),
        )

    def forward(self, X):
        return self.seq(X)
```

è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œæœ‰ä¸€ä¸ªè¾“å…¥å±‚ï¼Œä¸€ä¸ªéšè—å±‚å’Œä¸€ä¸ªè¾“å‡ºå±‚ã€‚

å¯¹äºè¿™ä¸ªç©å…·ç¤ºä¾‹ï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä¸€ä¸ªéå¸¸å¤§çš„éšè—å•å…ƒæ•°ï¼Œä»¥çªå‡º PEFT çš„æ•ˆç‡æå‡ï¼Œä½†è¿™äº›æå‡ä¸æ›´ç°å®çš„ç¤ºä¾‹ä¸€è‡´ã€‚

è¿™ä¸ªæ¨¡å‹ä¸­æœ‰ä¸€äº›çº¿æ€§å±‚å¯ä»¥ç”¨ LoRA è°ƒæ•´ã€‚å½“ä½¿ç”¨å¸¸è§çš„ğŸ¤— Transformers æ¨¡å‹æ—¶ï¼ŒPEFT ä¼šçŸ¥é“è¦å°† LoRA åº”ç”¨äºå“ªäº›å±‚ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½œä¸ºç”¨æˆ·éœ€è¦é€‰æ‹©å±‚ã€‚è¦ç¡®å®šè¦è°ƒæ•´çš„å±‚çš„åç§°ï¼š

```py
print([(n, type(m)) for n, m in MLP().named_modules()])
```

è¿™åº”è¯¥æ‰“å°ï¼š

```py
[('', __main__.MLP),
 ('seq', torch.nn.modules.container.Sequential),
 ('seq.0', torch.nn.modules.linear.Linear),
 ('seq.1', torch.nn.modules.activation.ReLU),
 ('seq.2', torch.nn.modules.linear.Linear),
 ('seq.3', torch.nn.modules.activation.ReLU),
 ('seq.4', torch.nn.modules.linear.Linear),
 ('seq.5', torch.nn.modules.activation.LogSoftmax)]
```

å‡è®¾æˆ‘ä»¬æƒ³å°† LoRA åº”ç”¨äºè¾“å…¥å±‚å’Œéšè—å±‚ï¼Œå®ƒä»¬åˆ†åˆ«æ˜¯`'seq.0'`å’Œ`'seq.2'`ã€‚æ­¤å¤–ï¼Œå‡è®¾æˆ‘ä»¬æƒ³è¦æ›´æ–°è¾“å‡ºå±‚è€Œä¸ä½¿ç”¨ LoRAï¼Œé‚£å°†æ˜¯`'seq.4'`ã€‚ç›¸åº”çš„é…ç½®å°†æ˜¯ï¼š

```py
from peft import LoraConfig

config = LoraConfig(
    target_modules=["seq.0", "seq.2"],
    modules_to_save=["seq.4"],
)
```

æœ‰äº†è¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæˆ‘ä»¬çš„ PEFT æ¨¡å‹å¹¶æ£€æŸ¥è®­ç»ƒå‚æ•°çš„æ¯”ä¾‹ï¼š

```py
from peft import get_peft_model

model = MLP()
peft_model = get_peft_model(model, config)
peft_model.print_trainable_parameters()
# prints trainable params: 56,164 || all params: 4,100,164 || trainable%: 1.369798866581922
```

æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½•æˆ‘ä»¬å–œæ¬¢çš„è®­ç»ƒæ¡†æ¶ï¼Œæˆ–è€…ç¼–å†™æˆ‘ä»¬è‡ªå·±çš„æ‹Ÿåˆå¾ªç¯ï¼Œæ¥è®­ç»ƒ`peft_model`ã€‚

è¦æŸ¥çœ‹å®Œæ•´ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹[æ­¤ç¬”è®°æœ¬](https://github.com/huggingface/peft/blob/main/examples/multilayer_perceptron/multilayer_perceptron_lora.ipynb)ã€‚

## timm æ¨¡å‹

[timm](https://huggingface.co/docs/timm/index)åº“åŒ…å«å¤§é‡é¢„è®­ç»ƒçš„è®¡ç®—æœºè§†è§‰æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹ä¹Ÿå¯ä»¥ç”¨ PEFT è¿›è¡Œå¾®è°ƒã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™åœ¨å®è·µä¸­æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

é¦–å…ˆç¡®ä¿ timm å·²å®‰è£…åœ¨ Python ç¯å¢ƒä¸­ï¼š

```py
python -m pip install -U timm
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¸ºå›¾åƒåˆ†ç±»ä»»åŠ¡åŠ è½½ä¸€ä¸ª timm æ¨¡å‹ï¼š

```py
import timm

num_classes = ...
model_id = "timm/poolformer_m36.sail_in1k"
model = timm.create_model(model_id, pretrained=True, num_classes=num_classes)
```

å†æ¬¡ï¼Œæˆ‘ä»¬éœ€è¦å†³å®šè¦å°† LoRA åº”ç”¨äºå“ªäº›å±‚ã€‚ç”±äº LoRA æ”¯æŒ 2D å·ç§¯å±‚ï¼Œå¹¶ä¸”è¿™äº›å±‚æ˜¯è¯¥æ¨¡å‹çš„ä¸»è¦æ„å»ºæ¨¡å—ï¼Œæˆ‘ä»¬åº”è¯¥å°† LoRA åº”ç”¨äº 2D å·ç§¯å±‚ã€‚ä¸ºäº†ç¡®å®šè¿™äº›å±‚çš„åç§°ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ‰€æœ‰çš„å±‚åç§°ï¼š

```py
print([(n, type(m)) for n, m in model.named_modules()])
```

è¿™å°†æ‰“å°ä¸€ä¸ªéå¸¸é•¿çš„åˆ—è¡¨ï¼Œæˆ‘ä»¬åªæ˜¾ç¤ºå‰å‡ ä¸ªï¼š

```py
[('', timm.models.metaformer.MetaFormer),
 ('stem', timm.models.metaformer.Stem),
 ('stem.conv', torch.nn.modules.conv.Conv2d),
 ('stem.norm', torch.nn.modules.linear.Identity),
 ('stages', torch.nn.modules.container.Sequential),
 ('stages.0', timm.models.metaformer.MetaFormerStage),
 ('stages.0.downsample', torch.nn.modules.linear.Identity),
 ('stages.0.blocks', torch.nn.modules.container.Sequential),
 ('stages.0.blocks.0', timm.models.metaformer.MetaFormerBlock),
 ('stages.0.blocks.0.norm1', timm.layers.norm.GroupNorm1),
 ('stages.0.blocks.0.token_mixer', timm.models.metaformer.Pooling),
 ('stages.0.blocks.0.token_mixer.pool', torch.nn.modules.pooling.AvgPool2d),
 ('stages.0.blocks.0.drop_path1', torch.nn.modules.linear.Identity),
 ('stages.0.blocks.0.layer_scale1', timm.models.metaformer.Scale),
 ('stages.0.blocks.0.res_scale1', torch.nn.modules.linear.Identity),
 ('stages.0.blocks.0.norm2', timm.layers.norm.GroupNorm1),
 ('stages.0.blocks.0.mlp', timm.layers.mlp.Mlp),
 ('stages.0.blocks.0.mlp.fc1', torch.nn.modules.conv.Conv2d),
 ('stages.0.blocks.0.mlp.act', torch.nn.modules.activation.GELU),
 ('stages.0.blocks.0.mlp.drop1', torch.nn.modules.dropout.Dropout),
 ('stages.0.blocks.0.mlp.norm', torch.nn.modules.linear.Identity),
 ('stages.0.blocks.0.mlp.fc2', torch.nn.modules.conv.Conv2d),
 ('stages.0.blocks.0.mlp.drop2', torch.nn.modules.dropout.Dropout),
 ('stages.0.blocks.0.drop_path2', torch.nn.modules.linear.Identity),
 ('stages.0.blocks.0.layer_scale2', timm.models.metaformer.Scale),
 ('stages.0.blocks.0.res_scale2', torch.nn.modules.linear.Identity),
 ('stages.0.blocks.1', timm.models.metaformer.MetaFormerBlock),
 ('stages.0.blocks.1.norm1', timm.layers.norm.GroupNorm1),
 ('stages.0.blocks.1.token_mixer', timm.models.metaformer.Pooling),
 ('stages.0.blocks.1.token_mixer.pool', torch.nn.modules.pooling.AvgPool2d),
 ...
 ('head.global_pool.flatten', torch.nn.modules.linear.Identity),
 ('head.norm', timm.layers.norm.LayerNorm2d),
 ('head.flatten', torch.nn.modules.flatten.Flatten),
 ('head.drop', torch.nn.modules.linear.Identity),
 ('head.fc', torch.nn.modules.linear.Linear)]
 ]
```

ä»”ç»†æ£€æŸ¥åï¼Œæˆ‘ä»¬çœ‹åˆ° 2D å·ç§¯å±‚çš„åç§°å¦‚`"stages.0.blocks.0.mlp.fc1"`å’Œ`"stages.0.blocks.0.mlp.fc2"`ã€‚æˆ‘ä»¬å¦‚ä½•åŒ¹é…è¿™äº›å±‚çš„åç§°ï¼Ÿæ‚¨å¯ä»¥ç¼–å†™ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼æ¥åŒ¹é…å±‚çš„åç§°ã€‚å¯¹äºæˆ‘ä»¬çš„æƒ…å†µï¼Œæ­£åˆ™è¡¨è¾¾å¼`r".*\.mlp\.fc\d"`åº”è¯¥å¯ä»¥èƒœä»»ã€‚

æ­¤å¤–ï¼Œä¸ç¬¬ä¸€ä¸ªç¤ºä¾‹ä¸€æ ·ï¼Œæˆ‘ä»¬åº”è¯¥ç¡®ä¿è¾“å‡ºå±‚ï¼Œå³åˆ†ç±»å¤´ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ä¹Ÿè¢«æ›´æ–°ã€‚æŸ¥çœ‹ä¸Šé¢æ‰“å°åˆ—è¡¨çš„æœ«å°¾ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒçš„åç§°æ˜¯`'head.fc'`ã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œè¿™æ˜¯æˆ‘ä»¬çš„ LoRA é…ç½®ï¼š

```py
config = LoraConfig(target_modules=r".*\.mlp\.fc\d", modules_to_save=["head.fc"])
```

ç„¶åæˆ‘ä»¬åªéœ€è¦é€šè¿‡å°†åŸºæœ¬æ¨¡å‹å’Œé…ç½®ä¼ é€’ç»™`get_peft_model`æ¥åˆ›å»º PEFT æ¨¡å‹ï¼š

```py
peft_model = get_peft_model(model, config)
peft_model.print_trainable_parameters()
# prints trainable params: 1,064,454 || all params: 56,467,974 || trainable%: 1.88505789139876
```

è¿™å‘æˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬åªéœ€è¦è®­ç»ƒä¸åˆ°æ‰€æœ‰å‚æ•°çš„ 2ï¼…ï¼Œè¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„æ•ˆç‡æå‡ã€‚

è¦æŸ¥çœ‹å®Œæ•´ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹[æ­¤ç¬”è®°æœ¬](https://github.com/huggingface/peft/blob/main/examples/image_classification/image_classification_timm_peft_lora.ipynb)ã€‚

## æ–°çš„ transformers æ¶æ„

å½“å‘å¸ƒæ–°çš„çƒ­é—¨ transformers æ¶æ„æ—¶ï¼Œæˆ‘ä»¬ä¼šå°½å¿«å°†å®ƒä»¬æ·»åŠ åˆ° PEFT ä¸­ã€‚å¦‚æœæ‚¨é‡åˆ°ä¸€ä¸ªä¸æ”¯æŒçš„ transformers æ¨¡å‹ï¼Œä¸ç”¨æ‹…å¿ƒï¼Œåªè¦é…ç½®æ­£ç¡®ï¼Œå®ƒå¾ˆå¯èƒ½ä»ç„¶å¯ä»¥å·¥ä½œã€‚å…·ä½“æ¥è¯´ï¼Œæ‚¨å¿…é¡»è¯†åˆ«åº”è¯¥é€‚åº”çš„å±‚ï¼Œå¹¶åœ¨åˆå§‹åŒ–ç›¸åº”çš„é…ç½®ç±»æ—¶æ­£ç¡®è®¾ç½®å®ƒä»¬ï¼Œä¾‹å¦‚`LoraConfig`ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸®åŠ©çš„æç¤ºã€‚

é¦–å…ˆï¼Œæ£€æŸ¥ç°æœ‰æ¨¡å‹ä»¥è·å–çµæ„Ÿæ˜¯ä¸ªå¥½ä¸»æ„ã€‚æ‚¨å¯ä»¥åœ¨ PEFT å­˜å‚¨åº“çš„[constants.py](https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py)ä¸­æ‰¾åˆ°å®ƒä»¬ã€‚é€šå¸¸ï¼Œæ‚¨ä¼šå‘ç°ä¸€ä¸ªç±»ä¼¼çš„æ¶æ„ä½¿ç”¨ç›¸åŒçš„åç§°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ–°æ¨¡å‹æ¶æ„æ˜¯â€œmistralâ€æ¨¡å‹çš„å˜ä½“ï¼Œå¹¶ä¸”æ‚¨æƒ³åº”ç”¨ LoRAï¼Œæ‚¨å¯ä»¥çœ‹åˆ°`TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING`ä¸­â€œmistralâ€æ¡ç›®åŒ…å«`["q_proj", "v_proj"]`ã€‚è¿™å‘Šè¯‰æ‚¨å¯¹äºâ€œmistralâ€æ¨¡å‹ï¼ŒLoRA çš„`target_modules`åº”è¯¥æ˜¯`["q_proj", "v_proj"]`ï¼š

```py
from peft import LoraConfig, get_peft_model

my_mistral_model = ...
config = LoraConfig(
    target_modules=["q_proj", "v_proj"],
    ...,  # other LoRA arguments
)
peft_model = get_peft_model(my_mistral_model, config)
```

å¦‚æœè¿™æ²¡æœ‰å¸®åŠ©ï¼Œè¯·ä½¿ç”¨`named_modules`æ–¹æ³•æ£€æŸ¥æ¨¡å‹æ¶æ„ä¸­çš„ç°æœ‰æ¨¡å—ï¼Œå¹¶å°è¯•è¯†åˆ«æ³¨æ„åŠ›å±‚ï¼Œç‰¹åˆ«æ˜¯å…³é”®ã€æŸ¥è¯¢å’Œå€¼å±‚ã€‚è¿™äº›é€šå¸¸ä¼šæœ‰åç§°ï¼Œå¦‚`c_attn`ã€`query`ã€`q_proj`ç­‰ã€‚å…³é”®å±‚å¹¶ä¸æ€»æ˜¯é€‚åº”çš„ï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œæ‚¨åº”è¯¥æ£€æŸ¥åŒ…æ‹¬å®ƒæ˜¯å¦ä¼šå¯¼è‡´æ›´å¥½çš„æ€§èƒ½ã€‚

æ­¤å¤–ï¼Œçº¿æ€§å±‚æ˜¯å¸¸è§çš„é€‚åº”ç›®æ ‡ï¼ˆä¾‹å¦‚ï¼Œåœ¨[QLoRA è®ºæ–‡](https://arxiv.org/abs/2305.14314)ä¸­ï¼Œä½œè€…å»ºè®®ä¹Ÿå¯¹å…¶è¿›è¡Œé€‚åº”ï¼‰ã€‚å®ƒä»¬çš„åç§°é€šå¸¸ä¼šåŒ…å«å­—ç¬¦ä¸²`fc`æˆ–`dense`ã€‚

å¦‚æœæ‚¨æƒ³å°†æ–°æ¨¡å‹æ·»åŠ åˆ° PEFTï¼Œè¯·åœ¨[constants.py](https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py)ä¸­åˆ›å»ºä¸€ä¸ªæ¡ç›®ï¼Œå¹¶åœ¨[å­˜å‚¨åº“](https://github.com/huggingface/peft/pulls)ä¸Šæ‰“å¼€ä¸€ä¸ªæ‹‰å–è¯·æ±‚ã€‚ä¸è¦å¿˜è®°æ›´æ–°[README](https://github.com/huggingface/peft#models-support-matrix)ã€‚

## éªŒè¯å‚æ•°å’Œå±‚

æ‚¨å¯ä»¥é€šè¿‡å‡ ç§æ–¹å¼éªŒè¯æ˜¯å¦å·²æ­£ç¡®å°† PEFT æ–¹æ³•åº”ç”¨äºæ‚¨çš„æ¨¡å‹ã€‚

+   ä½¿ç”¨ print_trainable_parameters()æ–¹æ³•æ£€æŸ¥å¯è®­ç»ƒå‚æ•°çš„æ¯”ä¾‹ã€‚å¦‚æœæ­¤æ•°å­—ä½äºæˆ–é«˜äºé¢„æœŸï¼Œè¯·é€šè¿‡æ‰“å°æ¨¡å‹æ¥æ£€æŸ¥æ¨¡å‹çš„`repr`ã€‚è¿™å°†æ˜¾ç¤ºæ¨¡å‹ä¸­æ‰€æœ‰å±‚ç±»å‹çš„åç§°ã€‚ç¡®ä¿åªæœ‰é¢„æœŸçš„ç›®æ ‡å±‚è¢«é€‚é…å±‚æ›¿æ¢ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ LoRA åº”ç”¨äº`nn.Linear`å±‚ï¼Œåˆ™åº”åªçœ‹åˆ°ä½¿ç”¨`lora.Linear`å±‚ã€‚

```py
peft_model.print_trainable_parameters()
```

+   æ‚¨å¯ä»¥æŸ¥çœ‹é€‚åº”çš„å±‚çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨`targeted_module_names`å±æ€§åˆ—å‡ºæ¯ä¸ªå·²é€‚åº”æ¨¡å—çš„åç§°ã€‚

```py
print(peft_model.targeted_module_names)
```
