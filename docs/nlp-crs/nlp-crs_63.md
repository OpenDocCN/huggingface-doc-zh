# æå–æ–‡æœ¬æ‘˜è¦

> åŸæ–‡ï¼š[`huggingface.co/learn/nlp-course/zh-CN/chapter7/5?fw=pt`](https://huggingface.co/learn/nlp-course/zh-CN/chapter7/5?fw=pt)

                Pytorch TensorFlow![Open In Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section5_pt.ipynb) ![Open In Studio Lab](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section5_pt.ipynb)

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ Transformer æ¨¡å‹å°†é•¿æ–‡æ¡£å‹ç¼©ä¸ºæ‘˜è¦ï¼Œè¿™é¡¹ä»»åŠ¡ç§°ä¸ºæ–‡æœ¬æ‘˜è¦.è¿™æ˜¯æœ€å…·æŒ‘æˆ˜æ€§çš„ NLP ä»»åŠ¡ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒéœ€è¦ä¸€ç³»åˆ—èƒ½åŠ›ï¼Œä¾‹å¦‚ç†è§£é•¿ç¯‡æ–‡ç« å’Œç”Ÿæˆèƒ½å¤Ÿæ•æ‰æ–‡æ¡£ä¸­ä¸»è¦ä¸»é¢˜çš„è¿è´¯æ–‡æœ¬ã€‚ä½†æ˜¯ï¼Œå¦‚æœåšå¾—å¥½ï¼Œæ–‡æœ¬æ‘˜è¦æ˜¯ä¸€ç§å¼ºå¤§çš„å·¥å…·ï¼Œå¯ä»¥å‡è½»é¢†åŸŸä¸“å®¶è¯¦ç»†é˜…è¯»é•¿æ–‡æ¡£çš„è´Ÿæ‹…ï¼Œä»è€ŒåŠ å¿«å„ç§ä¸šåŠ¡æµç¨‹ã€‚

[`www.youtube-nocookie.com/embed/yHnr5Dk2zCI`](https://www.youtube-nocookie.com/embed/yHnr5Dk2zCI)

å°½ç®¡åœ¨[Hugging Face Hub](https://huggingface.co/models?pipeline_tag=summarization=downloads)ä¸Šå·²ç»å­˜åœ¨å„ç§å¾®è°ƒæ¨¡å‹ç”¨äºæ–‡æœ¬æ‘˜è¦ï¼Œå‡ ä¹æ‰€æœ‰è¿™äº›éƒ½åªé€‚ç”¨äºè‹±æ–‡æ–‡æ¡£ã€‚å› æ­¤ï¼Œä¸ºäº†åœ¨æœ¬èŠ‚ä¸­æ·»åŠ ä¸€äº›å˜åŒ–ï¼Œæˆ‘ä»¬å°†ä¸ºè‹±è¯­å’Œè¥¿ç­ç‰™è¯­è®­ç»ƒä¸€ä¸ªåŒè¯­æ¨¡å‹ã€‚åœ¨æœ¬èŠ‚ç»“æŸæ—¶ï¼Œæ‚¨å°†æœ‰ä¸€ä¸ªå¯ä»¥æ€»ç»“å®¢æˆ·è¯„è®ºçš„[æ¨¡å‹](https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es)ã€‚

[`course-demos-mt5-small-finetuned-amazon-en-es.hf.space`](https://course-demos-mt5-small-finetuned-amazon-en-es.hf.space)

å¦‚ä¸‹æ‰€ç¤ºï¼šæ­£å¦‚æˆ‘ä»¬å°†çœ‹åˆ°çš„ï¼Œè¿™äº›æ‘˜è¦å¾ˆç®€æ´ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ä»å®¢æˆ·åœ¨äº§å“è¯„è®ºä¸­æä¾›çš„æ ‡é¢˜ä¸­å­¦åˆ°çš„ã€‚è®©æˆ‘ä»¬é¦–å…ˆä¸ºè¿™é¡¹ä»»åŠ¡å‡†å¤‡ä¸€ä¸ªåˆé€‚çš„åŒè¯­è¯­æ–™åº“ã€‚

## å‡†å¤‡å¤šè¯­è¨€è¯­æ–™åº“

æˆ‘ä»¬å°†ä½¿ç”¨[å¤šè¯­è¨€äºšé©¬é€Šè¯„è®ºè¯­æ–™åº“](https://huggingface.co/datasets/amazon_reviews_multi)åˆ›å»ºæˆ‘ä»¬çš„åŒè¯­æ‘˜è¦å™¨ã€‚è¯¥è¯­æ–™åº“ç”±å…­ç§è¯­è¨€çš„äºšé©¬é€Šäº§å“è¯„è®ºç»„æˆï¼Œé€šå¸¸ç”¨äºå¯¹å¤šè¯­è¨€åˆ†ç±»å™¨è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚ç„¶è€Œï¼Œç”±äºæ¯æ¡è¯„è®ºéƒ½é™„æœ‰ä¸€ä¸ªç®€çŸ­çš„æ ‡é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ ‡é¢˜ä½œä¸ºæˆ‘ä»¬æ¨¡å‹å­¦ä¹ çš„ç›®æ ‡æ‘˜è¦ï¼é¦–å…ˆï¼Œè®©æˆ‘ä»¬ä» Hugging Face Hub ä¸‹è½½è‹±è¯­å’Œè¥¿ç­ç‰™è¯­å­é›†ï¼š

```py
from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset
```

```py
DatasetDict({
    train: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
})
```

å¦‚æ‚¨æ‰€è§ï¼Œå¯¹äºæ¯ç§è¯­è¨€ï¼Œéƒ½æœ‰ 200,000 æ¡è¯„è®º **train** æ‹†åˆ†ï¼Œæ¯ä¸ªè¯„è®ºæœ‰ 5,000 æ¡è¯„è®º **validation** å’Œ **test** åˆ†è£‚ã€‚æˆ‘ä»¬æ„Ÿå…´è¶£çš„è¯„è®ºä¿¡æ¯åŒ…å«åœ¨ **review_body** å’Œ **review_title** åˆ—ã€‚è®©æˆ‘ä»¬é€šè¿‡åˆ›å»ºä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥æŸ¥çœ‹ä¸€äº›ç¤ºä¾‹ï¼Œè¯¥å‡½æ•°ä½¿ç”¨æˆ‘ä»¬åœ¨ç¬¬äº”ç« å­¦åˆ°è¿‡ï¼š

```py
def show_samples(dataset, num_samples=3, seed=42):
    sample = dataset["train"].shuffle(seed=seed).select(range(num_samples))
    for example in sample:
        print(f"\n'>> Title: {example['review_title']}'")
        print(f"'>> Review: {example['review_body']}'")

show_samples(english_dataset)
```

```py
'>> Title: Worked in front position, not rear'
'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'

'>> Title: meh'
'>> Review: Does itâ€™s job and itâ€™s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'

'>> Title: Can\'t beat these for the money'
'>> Review: Bought this for handling miscellaneous aircraft parts and hanger "stuff" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\'t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\'s heavy duty enough to hold metal parts, but being made of plastic it\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\'t beat it. Best one of these I\'ve bought to date-- and I\'ve been using some version of these for over forty years.'
```

âœï¸ **è¯•è¯•çœ‹ï¼** æ›´æ”¹ `Dataset.shuffle()` å‘½ä»¤ä¸­çš„éšæœºç§å­ä»¥æ¢ç´¢è¯­æ–™åº“ä¸­çš„å…¶ä»–è¯„è®ºã€‚ å¦‚æœæ‚¨æ˜¯è¯´è¥¿ç­ç‰™è¯­çš„äººï¼Œè¯·æŸ¥çœ‹ `spanish_dataset` ä¸­çš„ä¸€äº›è¯„è®ºï¼Œçœ‹çœ‹æ ‡é¢˜æ˜¯å¦ä¹Ÿåƒåˆç†çš„æ‘˜è¦ã€‚

æ­¤ç¤ºä¾‹æ˜¾ç¤ºäº†äººä»¬é€šå¸¸åœ¨ç½‘ä¸Šæ‰¾åˆ°çš„è¯„è®ºçš„å¤šæ ·æ€§ï¼Œä»æ­£é¢åˆ°è´Ÿé¢ï¼ˆä»¥åŠä»‹äºä¸¤è€…ä¹‹é—´çš„æ‰€æœ‰å†…å®¹ï¼ï¼‰ã€‚å°½ç®¡æ ‡é¢˜ä¸ºâ€œmehâ€çš„ç¤ºä¾‹ä¿¡æ¯é‡ä¸å¤§ï¼Œä½†å…¶ä»–æ ‡é¢˜çœ‹èµ·æ¥åƒæ˜¯å¯¹è¯„è®ºæœ¬èº«çš„ä½“é¢æ€»ç»“ã€‚åœ¨å•ä¸ª GPU ä¸Šè®­ç»ƒæ‰€æœ‰ 400,000 æ¡è¯„è®ºçš„æ‘˜è¦æ¨¡å‹å°†èŠ±è´¹å¤ªé•¿æ—¶é—´ï¼Œå› æ­¤æˆ‘ä»¬å°†ä¸“æ³¨äºä¸ºå•ä¸ªäº§å“é¢†åŸŸç”Ÿæˆæ‘˜è¦ã€‚ä¸ºäº†äº†è§£æˆ‘ä»¬å¯ä»¥é€‰æ‹©å“ªäº›åŸŸï¼Œè®©æˆ‘ä»¬å°† **english_dataset** è½¬æ¢åˆ° **pandas.DataFrame** å¹¶è®¡ç®—æ¯ä¸ªäº§å“ç±»åˆ«çš„è¯„è®ºæ•°é‡ï¼š

```py
english_dataset.set_format("pandas")
english_df = english_dataset["train"][:]
# Show counts for top 20 products
english_df["product_category"].value_counts()[:20]
```

```py
home                      17679
apparel                   15951
wireless                  15717
other                     13418
beauty                    12091
drugstore                 11730
kitchen                   10382
toy                        8745
sports                     8277
automotive                 7506
lawn_and_garden            7327
home_improvement           7136
pet_products               7082
digital_ebook_purchase     6749
pc                         6401
electronics                6186
office_product             5521
shoes                      5197
grocery                    4730
book                       3756
Name: product_category, dtype: int64
```

è‹±è¯­æ•°æ®é›†ä¸­æœ€å—æ¬¢è¿çš„äº§å“æ˜¯å®¶å±…ç”¨å“ã€æœè£…å’Œæ— çº¿ç”µå­äº§å“ã€‚ä¸è¿‡ï¼Œä¸ºäº†åšæŒäºšé©¬é€Šçš„ä¸»é¢˜ï¼Œè®©æˆ‘ä»¬ä¸“æ³¨äºæ€»ç»“ä¹¦ç±çš„è¯„è®ºâ€”â€”æ¯•ç«Ÿï¼Œè¿™æ˜¯äºšé©¬é€Šè¿™å®¶å…¬å¸æˆç«‹çš„åŸºç¡€ï¼æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªç¬¦åˆè¦æ±‚çš„äº§å“ç±»åˆ«ï¼ˆ **book** å’Œ **digital_ebook_purchase** )ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä¸ºè¿™äº›äº§å“è¿‡æ»¤ä¸¤ç§è¯­è¨€çš„æ•°æ®é›†ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬äº”ç« å­¦åˆ°çš„ï¼Œ è¿™ **Dataset.filter()** å‡½æ•°å…è®¸æˆ‘ä»¬éå¸¸æœ‰æ•ˆåœ°å¯¹æ•°æ®é›†è¿›è¡Œåˆ‡ç‰‡ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥æ‰§è¡Œæ­¤æ“ä½œï¼š

```py
def filter_books(example):
    return (
        example["product_category"] == "book"
        or example["product_category"] == "digital_ebook_purchase"
    )
```

ç°åœ¨ï¼Œå½“æˆ‘ä»¬å°†æ­¤å‡½æ•°åº”ç”¨äº **english_dataset** å’Œ **spanish_dataset** ï¼Œç»“æœå°†åªåŒ…å«æ¶‰åŠä¹¦ç±ç±»åˆ«çš„é‚£äº›è¡Œã€‚åœ¨åº”ç”¨è¿‡æ»¤å™¨ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å°†**english_dataset**çš„æ ¼å¼ä» **pandas** åˆ‡æ¢å›åˆ° **arrow** ï¼š

```py
english_dataset.reset_format()
```

ç„¶åæˆ‘ä»¬å¯ä»¥åº”ç”¨è¿‡æ»¤å™¨åŠŸèƒ½ï¼Œä½œä¸ºå¥å…¨æ€§æ£€æŸ¥ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥è¯„è®ºæ ·æœ¬ï¼Œçœ‹çœ‹å®ƒä»¬æ˜¯å¦ç¡®å®ä¸ä¹¦ç±æœ‰å…³ï¼š

```py
spanish_books = spanish_dataset.filter(filter_books)
english_books = english_dataset.filter(filter_books)
show_samples(english_books)
```

```py
'>> Title: I\'m dissapointed.'
'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\'m dissapointed.'

'>> Title: Good art, good price, poor design'
'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'

'>> Title: Helpful'
'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'
```

å¥½çš„ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¯„è®ºå¹¶ä¸æ˜¯ä¸¥æ ¼æ„ä¹‰ä¸Šçš„ä¹¦ç±ï¼Œå¯èƒ½æ˜¯æŒ‡æ—¥å†å’Œ OneNote ç­‰ç”µå­åº”ç”¨ç¨‹åºç­‰å†…å®¹ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè¯¥é¢†åŸŸä¼¼ä¹é€‚åˆè®­ç»ƒæ‘˜è¦æ¨¡å‹ã€‚åœ¨æˆ‘ä»¬æŸ¥çœ‹é€‚åˆæ­¤ä»»åŠ¡çš„å„ç§æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜æœ‰æœ€åä¸€ç‚¹æ•°æ®å‡†å¤‡è¦åšï¼šå°†è‹±è¯­å’Œè¥¿ç­ç‰™è¯­è¯„è®ºåˆå¹¶ä¸ºä¸€ä¸ª **DatasetDict** ç›®çš„ã€‚ ğŸ¤— Datasets æä¾›äº†ä¸€ä¸ªæ–¹ä¾¿çš„ **concatenate_datasets()** å‡½æ•°ï¼ˆé¡¾åæ€ä¹‰ï¼‰åˆå¹¶ **Dataset** å¯¹è±¡ã€‚å› æ­¤ï¼Œä¸ºäº†åˆ›å»ºæˆ‘ä»¬çš„åŒè¯­æ•°æ®é›†ï¼Œæˆ‘ä»¬å°†éå†æ¯ä¸ªæ‹†åˆ†ï¼Œè¿æ¥è¯¥æ‹†åˆ†çš„æ•°æ®é›†ï¼Œå¹¶æ‰“ä¹±ç»“æœä»¥ç¡®ä¿æˆ‘ä»¬çš„æ¨¡å‹ä¸ä¼šè¿‡åº¦æ‹Ÿåˆå•ä¸€è¯­è¨€ï¼š

```py
from datasets import concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

for split in english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=42)

# Peek at a few examples
show_samples(books_dataset)
```

```py
'>> Title: Easy to follow!!!!'
'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'

'>> Title: PARCIALMENTE DAÃ‘ADO'
'>> Review: Me llegÃ³ el dÃ­a que tocaba, junto a otros libros que pedÃ­, pero la caja llegÃ³ en mal estado lo cual daÃ±Ã³ las esquinas de los libros porque venÃ­an sin protecciÃ³n (forro).'

'>> Title: no lo he podido descargar'
'>> Review: igual que el anterior'
```

è¿™å½“ç„¶çœ‹èµ·æ¥åƒæ˜¯è‹±è¯­å’Œè¥¿ç­ç‰™è¯­è¯„è®ºçš„æ··åˆï¼ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªè®­ç»ƒè¯­æ–™åº“ï¼Œæœ€åè¦æ£€æŸ¥çš„ä¸€ä»¶äº‹æ˜¯è¯„è®ºä¸­å•è¯çš„åˆ†å¸ƒåŠå…¶æ ‡é¢˜ã€‚è¿™å¯¹äºæ‘˜è¦ä»»åŠ¡å°¤å…¶é‡è¦ï¼Œå…¶ä¸­æ•°æ®ä¸­çš„ç®€çŸ­å‚è€ƒæ‘˜è¦ä¼šä½¿æ¨¡å‹åå‘äºä»…åœ¨ç”Ÿæˆçš„æ‘˜è¦ä¸­è¾“å‡ºä¸€ä¸¤ä¸ªå•è¯ã€‚ä¸‹é¢çš„å›¾æ˜¾ç¤ºäº†å•è¯åˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœ‰äº›æ ‡é¢˜ä¸¥é‡åå‘äº 1-2 ä¸ªå•è¯ï¼š

![Word count distributions for the review titles and texts.](img/4c336a3ddf7ed03d13fd26950df52cf9.png) ![Word count distributions for the review titles and texts.](img/5ad8673270e8c77f177b224347921ac7.png)

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†è¿‡æ»¤æ‰æ ‡é¢˜éå¸¸çŸ­çš„ç¤ºä¾‹ï¼Œä»¥ä¾¿æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ç”Ÿæˆæ›´æœ‰è¶£çš„æ‘˜è¦ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨å¤„ç†è‹±æ–‡å’Œè¥¿ç­ç‰™æ–‡æ–‡æœ¬ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç²—ç•¥çš„å¯å‘å¼æ–¹æ³•åœ¨ç©ºç™½å¤„æ‹†åˆ†æ ‡é¢˜ï¼Œç„¶åä½¿ç”¨æˆ‘ä»¬å¯ä¿¡èµ–çš„ **Dataset.filter()** æ–¹æ³•å¦‚ä¸‹ï¼š

```py
books_dataset = books_dataset.filter(lambda x: len(x["review_title"].split()) > 2)
```

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æˆ‘ä»¬çš„è¯­æ–™åº“ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹ä¸€äº›å¯ä»¥å¯¹å…¶è¿›è¡Œå¾®è°ƒçš„å¯èƒ½çš„ Transformer æ¨¡å‹ï¼

## æ–‡æœ¬æ‘˜è¦æ¨¡å‹

å¦‚æœä½ ä»”ç»†æƒ³æƒ³ï¼Œæ–‡æœ¬æ‘˜è¦æ˜¯ä¸€ç§ç±»ä¼¼äºæœºå™¨ç¿»è¯‘çš„ä»»åŠ¡ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªåƒè¯„è®ºè¿™æ ·çš„æ–‡æœ¬æ­£æ–‡ï¼Œæˆ‘ä»¬å¸Œæœ›å°†å…¶â€œç¿»è¯‘â€æˆä¸€ä¸ªè¾ƒçŸ­çš„ç‰ˆæœ¬ï¼Œä»¥æ•æ‰è¾“å…¥çš„æ˜¾ç€ç‰¹å¾ã€‚å› æ­¤ï¼Œå¤§å¤šæ•°ç”¨äºæ–‡æœ¬æ‘˜è¦çš„ Transformer æ¨¡å‹é‡‡ç”¨äº†æˆ‘ä»¬åœ¨ç¬¬ä¸€ç« é‡åˆ°çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚å°½ç®¡æœ‰ä¸€äº›ä¾‹å¤–ï¼Œä¾‹å¦‚ GPT ç³»åˆ—æ¨¡å‹ï¼Œå®ƒä»¬åœ¨ few-shotï¼ˆå°‘é‡å¾®è°ƒï¼‰ä¹‹åä¹Ÿå¯ä»¥æå–æ‘˜è¦ã€‚ä¸‹è¡¨åˆ—å‡ºäº†ä¸€äº›æµè¡Œçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯ä»¥å¯¹å…¶è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œæ±‡æ€»ã€‚

| Transformer æ¨¡å‹ | æè¿° | å¤šç§è¯­è¨€? |
| :-: | --- | :-: |
| [GPT-2](https://huggingface.co/gpt2-xl) | è™½ç„¶è®­ç»ƒä¸ºè‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œä½†æ‚¨å¯ä»¥é€šè¿‡åœ¨è¾“å…¥æ–‡æœ¬æœ«å°¾é™„åŠ â€œTL;DRâ€æ¥ä½¿ GPT-2 ç”Ÿæˆæ‘˜è¦ã€‚ | âŒ |
| [PEGASUS](https://huggingface.co/google/pegasus-large) | åœ¨é¢„è®­ç»ƒæ˜¯çš„ç›®æ ‡æ˜¯æ¥é¢„æµ‹å¤šå¥å­æ–‡æœ¬ä¸­çš„å±è”½å¥å­ã€‚ è¿™ä¸ªé¢„è®­ç»ƒç›®æ ‡æ¯”æ™®é€šè¯­è¨€å»ºæ¨¡æ›´æ¥è¿‘æ–‡æœ¬æ‘˜è¦ï¼Œå¹¶ä¸”åœ¨æµè¡Œçš„åŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†å¾ˆé«˜ã€‚ | âŒ |
| [T5](https://huggingface.co/t5-base) | é€šç”¨çš„ Transformer æ¶æ„ï¼Œåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬çš„æ¡†æ¶ä¸­åˆ¶å®šæ‰€æœ‰ä»»åŠ¡ï¼› ä¾‹å¦‚ï¼Œæ¨¡å‹æ–‡æœ¬æ‘˜è¦çš„è¾“å…¥æ ¼å¼æ˜¯`summarize: ARTICLE`ã€‚ | âŒ |
| [mT5](https://huggingface.co/google/mt5-base) | T5 çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œåœ¨å¤šè¯­è¨€ Common Crawl è¯­æ–™åº“ (mC4) ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›– 101 ç§è¯­è¨€ã€‚ | âœ… |
| [BART](https://huggingface.co/facebook/bart-base) | ä¸€ç§æ–°é¢–çš„ Transformer æ¶æ„ï¼Œå…¶ä¸­åŒ…å«ç»è¿‡è®­ç»ƒçš„ç¼–ç å™¨å’Œè§£ç å™¨å †æ ˆï¼Œä»¥é‡å»ºè¢«ç ´åçš„è¾“å…¥ï¼Œç»“åˆäº† BERT å’Œ GPT-2 çš„é¢„è®­ç»ƒæ–¹æ¡ˆã€‚ | âŒ |
| [mBART-50](https://huggingface.co/facebook/mbart-large-50) | BART çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œé¢„è®­ç»ƒäº† 50 ç§è¯­è¨€ã€‚ | âœ… |

ä»æ­¤è¡¨ä¸­å¯ä»¥çœ‹å‡ºï¼Œå¤§å¤šæ•°ç”¨äºæ‘˜è¦çš„ Transformer æ¨¡å‹ï¼ˆä»¥åŠå¤§å¤šæ•° NLP ä»»åŠ¡ï¼‰éƒ½æ˜¯å•è¯­çš„ã€‚å¦‚æœæ‚¨çš„ä»»åŠ¡æ˜¯ä½¿ç”¨â€œæœ‰å¤§é‡è¯­æ–™åº“â€çš„è¯­è¨€ï¼ˆå¦‚è‹±è¯­æˆ–å¾·è¯­ï¼‰ï¼Œè¿™å¾ˆå¥½ï¼Œä½†å¯¹äºä¸–ç•Œå„åœ°æ­£åœ¨ä½¿ç”¨çš„æ•°åƒç§å…¶ä»–è¯­è¨€ï¼Œåˆ™ä¸ç„¶ã€‚å¹¸è¿çš„æ˜¯ï¼Œæœ‰ä¸€ç±»å¤šè¯­è¨€ Transformer æ¨¡å‹ï¼Œå¦‚ mT5 å’Œ mBARTï¼Œå¯ä»¥è§£å†³é—®é¢˜ã€‚è¿™äº›æ¨¡å‹æ˜¯ä½¿ç”¨è¯­è¨€å»ºæ¨¡è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œä½†æœ‰ä¸€ç‚¹ä¸åŒï¼šå®ƒä»¬ä¸æ˜¯åœ¨ä¸€ç§è¯­è¨€çš„è¯­æ–™åº“ä¸Šè®­ç»ƒï¼Œè€Œæ˜¯åŒæ—¶åœ¨ 50 å¤šç§è¯­è¨€çš„æ–‡æœ¬ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼

æˆ‘ä»¬å°†ä½¿ç”¨ mT5ï¼Œè¿™æ˜¯ä¸€ç§åŸºäº T5 çš„æœ‰è¶£æ¶æ„ï¼Œåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬æ¡†æ¶ä¸­è¿›è¡Œäº†é¢„è®­ç»ƒã€‚åœ¨ T5 ä¸­ï¼Œæ¯ä¸ª NLP ä»»åŠ¡éƒ½æ˜¯æ ¹æ®æç¤ºå‰ç¼€æ¥åˆ¶å®šçš„ï¼Œä¾‹å¦‚ **summarize:** è¿™ä½¿æ¨¡å‹ä½¿ç”Ÿæˆçš„æ–‡æœ¬é€‚åº”æç¤ºã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¿™è®© T5 å˜å¾—éå¸¸é€šç”¨ï¼Œå› ä¸ºä½ å¯ä»¥ç”¨ä¸€ä¸ªæ¨¡å‹è§£å†³å¾ˆå¤šä»»åŠ¡ï¼

![Different tasks performed by the T5 architecture.](img/19e5917b8990000666f27dc3b84ed471.png) ![Different tasks performed by the T5 architecture.](img/d01eef5637189e725a5554c44db13f7e.png)

mT5 ä¸ä½¿ç”¨å‰ç¼€ï¼Œä½†å…·æœ‰ T5 çš„å¤§éƒ¨åˆ†åŠŸèƒ½ï¼Œå¹¶ä¸”å…·æœ‰å¤šè¯­è¨€çš„ä¼˜åŠ¿ã€‚ç°åœ¨æˆ‘ä»¬å·²ç»é€‰æ‹©äº†ä¸€ä¸ªæ¨¡å‹ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹å‡†å¤‡æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ã€‚

âœï¸ **è¯•è¯•çœ‹ï¼** å®Œæˆæœ¬èŠ‚åï¼Œé€šè¿‡ä½¿ç”¨ç›¸åŒçš„æŠ€æœ¯å¯¹ mBART è¿›è¡Œå¾®è°ƒï¼Œçœ‹çœ‹ mT5 ä¸ mBART ç›¸æ¯”æœ‰å¤šå¥½ã€‚ å¯¹äºå¥–åŠ±ç§¯åˆ†ï¼Œæ‚¨è¿˜å¯ä»¥å°è¯•ä»…åœ¨è‹±æ–‡è¯„è®ºä¸Šå¾®è°ƒ T5ã€‚ ç”±äº T5 éœ€è¦ä¸€ä¸ªç‰¹æ®Šçš„å‰ç¼€æç¤ºï¼Œå› æ­¤æ‚¨éœ€è¦åœ¨ä¸‹é¢çš„é¢„å¤„ç†æ­¥éª¤ä¸­å°†â€œsummarize:â€æ·»åŠ åˆ°è¾“å…¥ç¤ºä¾‹ä¸­ã€‚

## é¢„å¤„ç†æ•°æ®

[`www.youtube-nocookie.com/embed/1m7BerpSq8A`](https://www.youtube-nocookie.com/embed/1m7BerpSq8A)

æˆ‘ä»¬çš„ä¸‹ä¸€ä¸ªä»»åŠ¡æ˜¯å¯¹æˆ‘ä»¬çš„è¯„è®ºåŠå…¶æ ‡é¢˜è¿›è¡Œæ ‡è®°å’Œç¼–ç ã€‚åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬é¦–å…ˆåŠ è½½ä¸é¢„è®­ç»ƒæ¨¡å‹æ£€æŸ¥ç‚¹ç›¸å…³çš„æ ‡è®°å™¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ **mt5-small** ä½œä¸ºæˆ‘ä»¬çš„æ£€æŸ¥ç‚¹ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥åœ¨åˆç†çš„æ—¶é—´å†…å¾®è°ƒæ¨¡å‹ï¼š

```py
from transformers import AutoTokenizer

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

ğŸ’¡åœ¨ NLP é¡¹ç›®çš„æ—©æœŸé˜¶æ®µï¼Œä¸€ä¸ªå¥½çš„åšæ³•æ˜¯åœ¨å°æ ·æœ¬æ•°æ®ä¸Šè®­ç»ƒä¸€ç±»â€œå°â€æ¨¡å‹ã€‚è¿™ä½¿æ‚¨å¯ä»¥æ›´å¿«åœ°è°ƒè¯•å’Œè¿­ä»£ç«¯åˆ°ç«¯å·¥ä½œæµã€‚ä¸€æ—¦æ‚¨å¯¹ç»“æœå……æ»¡ä¿¡å¿ƒï¼Œæ‚¨å§‹ç»ˆå¯ä»¥é€šè¿‡ç®€å•åœ°æ›´æ”¹æ¨¡å‹æ£€æŸ¥ç‚¹æ¥åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼

è®©æˆ‘ä»¬åœ¨ä¸€ä¸ªå°ä¾‹å­ä¸Šæµ‹è¯• mT5 æ ‡è®°å™¨ï¼š

```py
inputs = tokenizer("I loved reading the Hunger Games!")
inputs
```

```py
{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

åœ¨è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬åœ¨ç¬¬ä¸‰ç« ç¬¬ä¸€æ¬¡å¾®è°ƒå®éªŒä¸­é‡åˆ°çš„ç†Ÿæ‚‰çš„ **input_ids** å’Œ **attention_mask** .è®©æˆ‘ä»¬ç”¨åˆ†è¯å™¨è§£ç è¿™äº›è¾“å…¥ ID ï¼Œå¯ä»¥**convert_ids_to_tokens()** å‡½æ•°æ¥æŸ¥çœ‹æˆ‘ä»¬æ­£åœ¨å¤„ç†ä»€ä¹ˆæ ·çš„æ ‡è®°å™¨ï¼š

```py
tokenizer.convert_ids_to_tokens(inputs.input_ids)
```

```py
['â–I', 'â–', 'loved', 'â–reading', 'â–the', 'â–Hung', 'er', 'â–Games', '</s>']
```

ç‰¹æ®Šçš„ Unicode å­—ç¬¦ `â–` å’Œåºåˆ—ç»“æŸæ ‡è®° `</s>` è¡¨æ˜æˆ‘ä»¬æ­£åœ¨å¤„ç† SentencePiece åˆ†è¯å™¨ï¼Œå®ƒåŸºäºåœ¨ç¬¬å…­ç« ä¸­è®¨è®ºçš„ Unigram åˆ†è¯ç®—æ³•. Unigram å¯¹å¤šè¯­è¨€è¯­æ–™åº“ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå…è®¸ SentencePiece ä¸çŸ¥é“é‡éŸ³ã€æ ‡ç‚¹ç¬¦å·ä»¥åŠè®¸å¤šè¯­è¨€ï¼ˆå¦‚æ—¥è¯­ï¼‰æ²¡æœ‰ç©ºæ ¼å­—ç¬¦ã€‚

ä¸ºäº†æ ‡è®°æˆ‘ä»¬çš„è¯­æ–™åº“ï¼Œæˆ‘ä»¬å¿…é¡»å¤„ç†ä¸æ‘˜è¦ç›¸å…³çš„å¾®å¦™ä¹‹å¤„ï¼šå› ä¸ºæˆ‘ä»¬çš„æ ‡ç­¾ä¹Ÿæ˜¯æ–‡æœ¬ï¼Œæ‰€ä»¥å®ƒä»¬å¯èƒ½è¶…è¿‡æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡å¤§å°ã€‚ è¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦å¯¹è¯„è®ºåŠå…¶æ ‡é¢˜è¿›è¡Œæˆªæ–­ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ä¸ä¼šå°†è¿‡é•¿çš„è¾“å…¥ä¼ é€’ç»™æˆ‘ä»¬çš„æ¨¡å‹ã€‚ ğŸ¤— Transformers ä¸­çš„åˆ†è¯å™¨æä¾›äº†ä¸€ä¸ªç»å¦™çš„ `text_target` å‚æ•°ï¼Œå…è®¸æ‚¨å°†æ ‡ç­¾ä¸è¾“å…¥å¹¶è¡Œåˆ†è¯ã€‚ ä»¥ä¸‹æ˜¯å¦‚ä½•ä¸º mT5 å¤„ç†è¾“å…¥å’Œç›®æ ‡çš„ç¤ºä¾‹ï¼š

```py
max_input_length = 512
max_target_length = 30

def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["review_body"],
        max_length=max_input_length,
        truncation=True,
    )
    labels = tokenizer(
        examples["review_title"], max_length=max_target_length, truncation=True
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

è®©æˆ‘ä»¬é€šè¿‡è¿™æ®µä»£ç æ¥äº†è§£å‘ç”Ÿäº†ä»€ä¹ˆã€‚æˆ‘ä»¬åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯å®šä¹‰å€¼ **max_input_length** å’Œ **max_target_length** ï¼Œå®ƒä¸ºæˆ‘ä»¬çš„è¯„è®ºå’Œæ ‡é¢˜çš„é•¿åº¦è®¾ç½®äº†ä¸Šé™ã€‚ç”±äºè¯„è®ºæ­£æ–‡é€šå¸¸æ¯”æ ‡é¢˜å¤§å¾—å¤šï¼Œæˆ‘ä»¬ç›¸åº”åœ°è°ƒæ•´äº†è¿™äº›å€¼ã€‚

æœ‰äº† `preprocess_function()`ï¼Œæˆ‘ä»¬åœ¨æ•´ä¸ªè¯¾ç¨‹ä¸­å¹¿æ³›ä½¿ç”¨çš„æ–¹ä¾¿çš„ `Dataset.map()` å‡½æ•°æ¥æ ‡è®°æ•´ä¸ªè¯­æ–™åº“æ˜¯ä¸€ä»¶ç®€å•çš„äº‹æƒ…ï¼š

```py
tokenized_datasets = books_dataset.map(preprocess_function, batched=True)
```

æ—¢ç„¶è¯­æ–™åº“å·²ç»é¢„å¤„ç†å®Œæ¯•ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ä¸€äº›å¸¸ç”¨çš„æ‘˜è¦æŒ‡æ ‡ã€‚æ­£å¦‚æˆ‘ä»¬å°†çœ‹åˆ°çš„ï¼Œåœ¨è¡¡é‡æœºå™¨ç”Ÿæˆçš„æ–‡æœ¬çš„è´¨é‡æ–¹é¢æ²¡æœ‰çµä¸¹å¦™è¯ã€‚

ğŸ’¡ ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°æˆ‘ä»¬åœ¨ä¸Šé¢çš„ `Dataset.map()` å‡½æ•°ä¸­ä½¿ç”¨äº† `batched=True`ã€‚ è¿™ä¼šä»¥ 1,000 ä¸ªï¼ˆé»˜è®¤ï¼‰ä¸ºå•ä½å¯¹ç¤ºä¾‹è¿›è¡Œç¼–ç ï¼Œå¹¶å…è®¸æ‚¨åˆ©ç”¨ ğŸ¤— Transformers ä¸­å¿«é€Ÿæ ‡è®°å™¨çš„å¤šçº¿ç¨‹åŠŸèƒ½ã€‚ åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œå°è¯•ä½¿ç”¨ `batched=True` æ¥åŠ é€Ÿæ‚¨çš„é¢„å¤„ç†ï¼

## æ–‡æœ¬æ‘˜è¦çš„æŒ‡æ ‡

[`www.youtube-nocookie.com/embed/TMshhnrEXlg`](https://www.youtube-nocookie.com/embed/TMshhnrEXlg)

ä¸æˆ‘ä»¬åœ¨æœ¬è¯¾ç¨‹ä¸­æ¶µç›–çš„å¤§å¤šæ•°å…¶ä»–ä»»åŠ¡ç›¸æ¯”ï¼Œè¡¡é‡æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æ‘˜è¦æˆ–ç¿»è¯‘ï¼‰çš„æ€§èƒ½å¹¶ä¸é‚£ä¹ˆç®€å•ã€‚ä¾‹å¦‚ï¼Œå¯¹äºâ€œæˆ‘å–œæ¬¢é˜…è¯»é¥¥é¥¿æ¸¸æˆâ€è¿™æ ·çš„è¯„è®ºï¼Œæœ‰å¤šä¸ªæœ‰æ•ˆæ‘˜è¦ï¼Œä¾‹å¦‚â€œæˆ‘å–œæ¬¢é¥¥é¥¿æ¸¸æˆâ€æˆ–â€œé¥¥é¥¿æ¸¸æˆæ˜¯ä¸€æœ¬å¥½ä¹¦â€ã€‚æ˜¾ç„¶ï¼Œåœ¨ç”Ÿæˆçš„æ‘˜è¦å’Œæ ‡ç­¾ä¹‹é—´åº”ç”¨æŸç§ç²¾ç¡®åŒ¹é…å¹¶ä¸æ˜¯ä¸€ä¸ªå¥½çš„è§£å†³æ–¹æ¡ˆâ€”â€”å³ä½¿æ˜¯äººç±»åœ¨è¿™æ ·çš„æŒ‡æ ‡ä¸‹ä¹Ÿä¼šè¡¨ç°ä¸ä½³ï¼Œå› ä¸ºæˆ‘ä»¬éƒ½æœ‰è‡ªå·±çš„å†™ä½œé£æ ¼ã€‚

æ€»è€Œè¨€ä¹‹ï¼Œæœ€å¸¸ç”¨çš„æŒ‡æ ‡ä¹‹ä¸€æ˜¯[ROUGE åˆ†æ•°](https://en.wikipedia.org/wiki/ROUGE_(metric))ï¼ˆRecall-Oriented Understudy for Gisting Evaluation çš„ç¼©å†™ï¼‰ã€‚è¯¥æŒ‡æ ‡èƒŒåçš„åŸºæœ¬æ€æƒ³æ˜¯å°†ç”Ÿæˆçš„æ‘˜è¦ä¸ä¸€ç»„é€šå¸¸ç”±äººç±»åˆ›å»ºçš„å‚è€ƒæ‘˜è¦è¿›è¡Œæ¯”è¾ƒã€‚ä¸ºäº†æ›´ç²¾ç¡®ï¼Œå‡è®¾æˆ‘ä»¬è¦æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªæ‘˜è¦ï¼š

```py
generated_summary = "I absolutely loved reading the Hunger Games"
reference_summary = "I loved reading the Hunger Games"
```

æ¯”è¾ƒå®ƒä»¬çš„ä¸€ç§æ–¹æ³•æ˜¯è®¡ç®—é‡å å•è¯çš„æ•°é‡ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ä¸º 6ã€‚ä½†æ˜¯ï¼Œè¿™æœ‰ç‚¹ç²—ç³™ï¼Œå› æ­¤ ROUGE æ˜¯åŸºäºè®¡ç®—è®¡ç®—é‡å çš„ *precision* å’Œ *recall* åˆ†æ•°ã€‚ã€‚

ğŸ™‹ å¦‚æœè¿™æ˜¯æ‚¨ç¬¬ä¸€æ¬¡å¬è¯´ç²¾ç¡®ç‡å’Œå¬å›ç‡ï¼Œè¯·ä¸è¦æ‹…å¿ƒâ€”â€”æˆ‘ä»¬å°†ä¸€èµ·é€šè¿‡ä¸€äº›æ˜ç¡®çš„ç¤ºä¾‹æ¥è¯´æ˜ä¸€åˆ‡ã€‚ è¿™äº›æŒ‡æ ‡é€šå¸¸åœ¨åˆ†ç±»ä»»åŠ¡ä¸­é‡åˆ°ï¼Œå› æ­¤å¦‚æœæ‚¨æƒ³äº†è§£åœ¨è¯¥ä¸Šä¸‹æ–‡ä¸­å¦‚ä½•å®šä¹‰ç²¾ç¡®åº¦å’Œå¬å›ç‡ï¼Œæˆ‘ä»¬å»ºè®®æŸ¥çœ‹ scikit-learn æŒ‡å— /auto_examples/model_selection/plot_precision_recall.htmlï¼‰ã€‚

å¯¹äº ROUGEï¼Œrecall è¡¡é‡ç”Ÿæˆçš„å‚è€ƒæ‘˜è¦åŒ…å«äº†å¤šå°‘å‚è€ƒæ‘˜è¦ã€‚å¦‚æœæˆ‘ä»¬åªæ˜¯æ¯”è¾ƒå•è¯ï¼Œrecall å¯ä»¥æ ¹æ®ä»¥ä¸‹å…¬å¼è®¡ç®—ï¼š <math display="block"><semantics><mrow><mrow><mi mathvariant="normal">R</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">l</mi></mrow><mo>=</mo><mfrac><mrow><mi mathvariant="normal">N</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">v</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">g</mi><mi mathvariant="normal">w</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi></mrow><mrow><mi mathvariant="normal">T</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">w</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">y</mi></mrow></mfrac></mrow> <annotation encoding="application/x-tex">\mathrm{Recall} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, reference\, summary}}</annotation></semantics></math> Recall=TotalnumberofwordsinreferencesummaryNumberofoverlappingwordsâ€‹

å¯¹äºæˆ‘ä»¬ä¸Šé¢çš„ç®€å•ä¾‹å­ï¼Œè¿™ä¸ªå…¬å¼ç»™å‡ºäº† 6/6 = 1 çš„å®Œç¾å¬å›ç‡ï¼›å³ï¼Œå‚è€ƒæ‘˜è¦ä¸­çš„æ‰€æœ‰å•è¯éƒ½å·²ç”±æ¨¡å‹ç”Ÿæˆã€‚è¿™å¬èµ·æ¥å¯èƒ½å¾ˆæ£’ï¼Œä½†æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæˆ‘ä»¬ç”Ÿæˆçš„æ‘˜è¦æ˜¯â€œæˆ‘çœŸçš„å¾ˆå–œæ¬¢æ•´æ™šé˜…è¯»é¥¥é¥¿æ¸¸æˆâ€ã€‚è¿™ä¹Ÿå°†æœ‰å®Œç¾çš„ recallï¼Œä½†å¯ä»¥è¯´æ˜¯ä¸€ä¸ªæ›´ç³Ÿç³•çš„æ€»ç»“ï¼Œå› ä¸ºå®ƒå¾ˆå†—é•¿ã€‚ä¸ºäº†å¤„ç†è¿™äº›åœºæ™¯ï¼Œæˆ‘ä»¬è¿˜è®¡ç®—äº† pecisionï¼Œå®ƒåœ¨ ROUGE ä¸Šä¸‹æ–‡ä¸­è¡¡é‡ç”Ÿæˆçš„æ‘˜è¦ä¸­æœ‰å¤šå°‘æ˜¯ç›¸å…³çš„ï¼š <math display="block"><semantics><mrow><mrow><mi mathvariant="normal">P</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mrow><mo>=</mo><mfrac><mrow><mi mathvariant="normal">N</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">v</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">g</mi><mi mathvariant="normal">w</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi></mrow><mrow><mi mathvariant="normal">T</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">w</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">g</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">y</mi></mrow></mfrac></mrow> <annotation encoding="application/x-tex">\mathrm{Precision} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, generated\, summary}}</annotation></semantics></math> Precision=TotalnumberofwordsingeneratedsummaryNumberofoverlappingwordsâ€‹

å°†æ­¤åº”ç”¨åˆ°æˆ‘ä»¬çš„è¯¦ç»†æ‘˜è¦ä¸­ä¼šå¾—åˆ° 6/10 = 0.6 çš„ç²¾åº¦ï¼Œè¿™æ¯”æˆ‘ä»¬è¾ƒçŸ­çš„æ‘˜è¦è·å¾—çš„ 6/7 = 0.86 çš„ç²¾åº¦è¦å·®å¾—å¤šã€‚åœ¨å®è·µä¸­ï¼Œé€šå¸¸è®¡ç®—ç²¾åº¦å’Œå¬å›ç‡ï¼Œç„¶åæŠ¥å‘Š F1-scoreï¼ˆç²¾åº¦å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡å€¼ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥åœ¨ ğŸ¤— Datasets ä¸­é€šè¿‡å®‰è£… **rouge_score** åŒ…æ¥è®¡ç®—ä»–ä»¬ï¼š

```py
!pip install rouge_score
```

ç„¶åæŒ‰å¦‚ä¸‹æ–¹å¼åŠ è½½ ROUGE æŒ‡æ ‡ï¼š

```py
import evaluate

rouge_score = evaluate.load("rouge")
```

ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ **rouge_score.compute()** ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰æŒ‡æ ‡çš„å‡½æ•°ï¼š

```py
scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores
```

```py
{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),
 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}
```

å“‡ï¼Œé‚£ä¸ªè¾“å‡ºä¸­æœ‰å¾ˆå¤šä¿¡æ¯â€”â€”è¿™éƒ½æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿé¦–å…ˆï¼ŒğŸ¤— Datasets å®é™…ä¸Šè®¡ç®—äº†ç²¾åº¦ã€å¬å›ç‡å’Œ F1 åˆ†æ•°çš„ç½®ä¿¡åŒºé—´ï¼›è¿™äº›æ˜¯ä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°çš„ **low** , **mid** ï¼Œ å’Œ **high** å±æ€§ã€‚æ­¤å¤–ï¼ŒğŸ¤— Datasets åœ¨æ¯”è¾ƒç”Ÿæˆæ‘˜è¦å’Œå‚è€ƒæ‘˜è¦æ—¶ï¼Œä¼šæ ¹æ®ä¸åŒç±»å‹çš„æ–‡æœ¬ç²’åº¦è®¡ç®—å„ç§ ROUGE åˆ†æ•°ã€‚è¿™ **rouge1** å˜ä½“æ˜¯ä¸€å…ƒç»„çš„é‡å â€”â€”è¿™åªæ˜¯è¡¨è¾¾å•è¯é‡å çš„ä¸€ç§å¥‡ç‰¹æ–¹å¼ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬ä¸Šé¢è®¨è®ºçš„åº¦é‡æ ‡å‡†ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬è¾“å‡º **mid** çš„æ•°å€¼ï¼š

```py
scores["rouge1"].mid
```

```py
Score(precision=0.86, recall=1.0, fmeasure=0.92)
```

å¤ªå¥½äº†ï¼Œå‡†ç¡®ç‡å’Œå¬å›ç‡åŒ¹é…äº†ï¼é‚£ä¹ˆå…¶ä»–çš„ ROUGE åˆ†æ•°å‘¢ï¼Ÿ **rouge2** æµ‹é‡äºŒå…ƒç»„ä¹‹é—´çš„é‡å ï¼ˆæƒ³æƒ³å•è¯å¯¹çš„é‡å ï¼‰ï¼Œè€Œ **rougeL** å’Œ **rougeLsum** é€šè¿‡åœ¨ç”Ÿæˆçš„å’Œå‚è€ƒæ‘˜è¦ä¸­æŸ¥æ‰¾æœ€é•¿çš„å…¬å…±å­ä¸²æ¥æµ‹é‡æœ€é•¿çš„å•è¯åŒ¹é…åºåˆ—ã€‚ä¸­çš„â€œæ€»å’Œâ€ **rougeLsum** æŒ‡çš„æ˜¯è¿™ä¸ªæŒ‡æ ‡æ˜¯åœ¨æ•´ä¸ªæ‘˜è¦ä¸Šè®¡ç®—çš„ï¼Œè€Œ **rougeL** è®¡ç®—ä¸ºå•ä¸ªå¥å­çš„å¹³å‡å€¼ã€‚

âœï¸ **è¯•è¯•çœ‹ï¼** åˆ›å»ºæ‚¨è‡ªå·±çš„ç”Ÿæˆå’Œå‚è€ƒæ‘˜è¦ç¤ºä¾‹ï¼Œå¹¶æŸ¥çœ‹ç”Ÿæˆçš„ ROUGE åˆ†æ•°æ˜¯å¦ä¸åŸºäºç²¾ç¡®åº¦å’Œå¬å›ç‡å…¬å¼çš„æ‰‹åŠ¨è®¡ç®—ä¸€è‡´ã€‚ å¯¹äºé™„åŠ åˆ†ï¼Œå°†æ–‡æœ¬æ‹†åˆ†ä¸ºäºŒå…ƒç»„å¹¶æ¯”è¾ƒâ€œrouge2â€æŒ‡æ ‡çš„ç²¾åº¦å’Œå¬å›ç‡ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨è¿™äº› ROUGE åˆ†æ•°æ¥è·Ÿè¸ªæˆ‘ä»¬æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†åœ¨æ­¤ä¹‹å‰ï¼Œè®©æˆ‘ä»¬åšæ¯ä¸ªä¼˜ç§€çš„ NLP ä»ä¸šè€…éƒ½åº”è¯¥åšçš„äº‹æƒ…ï¼šåˆ›å»ºä¸€ä¸ªå¼ºå¤§è€Œç®€å•çš„ baselineï¼

### åˆ›å»ºå¼ºå¤§çš„ baseline

æ–‡æœ¬æ‘˜è¦çš„ä¸€ä¸ªå¸¸è§åŸºçº¿æ˜¯ç®€å•åœ°å–ä¸€ç¯‡æ–‡ç« çš„å‰ä¸‰ä¸ªå¥å­ï¼Œé€šå¸¸ç§°ä¸º *lead-3* åŸºçº¿ã€‚ æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¥å·(è‹±æ–‡ä½¿ç”¨.)æ¥è·Ÿè¸ªå¥å­è¾¹ç•Œï¼Œä½†è¿™åœ¨â€U.S.â€ or â€œU.N.â€ä¹‹ç±»çš„é¦–å­—æ¯ç¼©ç•¥è¯ä¸Šä¼šå¤±è´¥ã€‚æ‰€ä»¥æˆ‘ä»¬å°†ä½¿ç”¨ `nltk` åº“ï¼Œå®ƒåŒ…å«ä¸€ä¸ªæ›´å¥½çš„ç®—æ³•æ¥å¤„ç†è¿™äº›æƒ…å†µã€‚ æ‚¨å¯ä»¥ä½¿ç”¨ `pip` å®‰è£…è½¯ä»¶åŒ…ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
!pip install nltk
```

ç„¶åä¸‹è½½æ ‡ç‚¹è§„åˆ™ï¼š

```py
import nltk

nltk.download("punkt")
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä» `nltk` å¯¼å…¥å¥å­æ ‡è®°å™¨å¹¶åˆ›å»ºä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥æå–è¯„è®ºä¸­çš„å‰ä¸‰ä¸ªå¥å­ã€‚ æ–‡æœ¬æ‘˜è¦çš„çº¦å®šæ˜¯ç”¨æ¢è¡Œç¬¦åˆ†éš”æ¯ä¸ªæ‘˜è¦ï¼Œå› æ­¤æˆ‘ä»¬ä¹Ÿå°†å…¶åŒ…å«åœ¨å†…å¹¶åœ¨è®­ç»ƒç¤ºä¾‹ä¸Šå¯¹å…¶è¿›è¡Œæµ‹è¯•ï¼š

```py
from nltk.tokenize import sent_tokenize

def three_sentence_summary(text):
    return "\n".join(sent_tokenize(text)[:3])

print(three_sentence_summary(books_dataset["train"][1]["review_body"]))
```

```py
'I grew up reading Koontz, and years ago, I stopped,convinced i had "outgrown" him.'
'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.'
'She found Strangers.'
```

è¿™ä¼¼ä¹æœ‰æ•ˆï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ç°åœ¨å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œä»æ•°æ®é›†ä¸­æå–è¿™äº›â€œæ‘˜è¦â€å¹¶è®¡ç®— baseline çš„ ROUGE åˆ†æ•°ï¼š

```py
def evaluate_baseline(dataset, metric):
    summaries = [three_sentence_summary(text) for text in dataset["review_body"]]
    return metric.compute(predictions=summaries, references=dataset["review_title"])
```

ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªå‡½æ•°æ¥è®¡ç®—éªŒè¯é›†ä¸Šçš„ ROUGE åˆ†æ•°ï¼Œå¹¶ä½¿ç”¨ Pandas å¯¹å®ƒä»¬è¿›è¡Œä¸€äº›ç¾åŒ–ï¼š

```py
import pandas as pd

score = evaluate_baseline(books_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict
```

```py
{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°`rouge2`çš„åˆ†æ•°æ˜æ˜¾ä½äºå…¶ä»–ï¼› è¿™å¯èƒ½åæ˜ äº†è¿™æ ·ä¸€ä¸ªäº‹å®ï¼Œå³è¯„è®ºæ ‡é¢˜é€šå¸¸å¾ˆç®€æ´ï¼Œå› æ­¤ lead-3 baseline è¿‡äºå†—é•¿ã€‚ ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªå¾ˆå¥½çš„åŸºå‡†ï¼Œè®©æˆ‘ä»¬å°†æ³¨æ„åŠ›è½¬å‘å¾®è°ƒ mT5ï¼

## ä½¿ç”¨ Trainer API å¾®è°ƒ mT5

å¾®è°ƒæ¨¡å‹ä»¥è¿›è¡Œæå–æ‘˜è¦ä¸æˆ‘ä»¬åœ¨æœ¬ç« ä¸­ä»‹ç»çš„å…¶ä»–ä»»åŠ¡éå¸¸ç›¸ä¼¼ã€‚ æˆ‘ä»¬éœ€è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯ä»`mt5-small`æ£€æŸ¥ç‚¹åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ã€‚ ç”±äºæ‘˜è¦æå–æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ AutoModelForSeq2SeqLM ç±»åŠ è½½æ¨¡å‹ï¼Œè¯¥ç±»ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶ç¼“å­˜æƒé‡ï¼š

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

ğŸ’¡ If youâ€™re wondering why you donâ€™t see any warnings about fine-tuning the model on a downstream task, thatâ€™s because for sequence-to-sequence tasks we keep all the weights of the network. Compare this to our text classification model in Chapter 3, where the head of the pretrained model was replaced with a randomly initialized network. ğŸ’¡ å¦‚æœæ‚¨æƒ³çŸ¥é“ä¸ºä»€ä¹ˆåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­æ²¡æœ‰çœ‹åˆ°ä»»ä½•å…³äºå¾®è°ƒæ¨¡å‹çš„è­¦å‘Šï¼Œé‚£æ˜¯å› ä¸ºå¯¹äºåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬ä¿ç•™äº†ç½‘ç»œçš„æ‰€æœ‰æƒé‡ã€‚ä¸æˆ‘ä»¬åœ¨[ç¬¬ä¸‰ç« ] (/course/chapter3)ä¸­çš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œæ–‡æœ¬åˆ†ç±»æ¨¡å‹é¢„è®­ç»ƒæ¨¡å‹çš„å¤´éƒ¨è¢«éšæœºåˆå§‹åŒ–çš„ç½‘ç»œæ›¿æ¢ã€‚

æˆ‘ä»¬éœ€è¦åšçš„ä¸‹ä¸€ä»¶äº‹æ˜¯ç™»å½• Hugging Face Hubã€‚å¦‚æœæ‚¨åœ¨ notebook ä¸­è¿è¡Œæ­¤ä»£ç ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å®ç”¨ç¨‹åºå‡½æ•°æ‰§è¡Œæ­¤æ“ä½œï¼š

```py
from huggingface_hub import notebook_login

notebook_login()
```

è¿™å°†æ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥æ‚¨çš„å‡­æ®ã€‚æˆ–è€…ï¼Œæ‚¨å¯ä»¥åœ¨ç»ˆç«¯ä¸­è¿è¡Œæ­¤å‘½ä»¤å¹¶åœ¨é‚£é‡Œç™»å½•ï¼š

```py
huggingface-cli login
```

æˆ‘ä»¬éœ€è¦ç”Ÿæˆæ‘˜è¦ä»¥ä¾¿åœ¨è®­ç»ƒæœŸé—´è®¡ç®— ROUGE åˆ†æ•°ã€‚å¹¸è¿çš„æ˜¯ï¼ŒğŸ¤— Transformers æä¾›äº†ä¸“ç”¨çš„ `Seq2SeqTrainingArguments` å’Œ `Seq2SeqTrainer` ç±»ï¼Œå¯ä»¥è‡ªåŠ¨ä¸ºæˆ‘ä»¬å®Œæˆè¿™é¡¹å·¥ä½œï¼ ä¸ºäº†äº†è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œè®©æˆ‘ä»¬é¦–å…ˆä¸ºæˆ‘ä»¬çš„å®éªŒå®šä¹‰è¶…å‚æ•°å’Œå…¶ä»–å‚æ•°ï¼š

```py
from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_train_epochs = 8
# Show the training loss with every epoch
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

args = Seq2SeqTrainingArguments(
    output_dir=f"{model_name}-finetuned-amazon-en-es",
    evaluation_strategy="epoch",
    learning_rate=5.6e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=num_train_epochs,
    predict_with_generate=True,
    logging_steps=logging_steps,
    push_to_hub=True,
)
```

åœ¨è¿™é‡Œï¼Œ **predict_with_generate** å‚æ•°å·²è®¾ç½®ä¸º True è¡¨æ˜æˆ‘ä»¬åº”è¯¥åœ¨è¯„ä¼°æœŸé—´ç”Ÿæˆæ‘˜è¦ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªæ—¶æœŸçš„ ROUGE åˆ†æ•°ã€‚æ­£å¦‚åœ¨ç¬¬ä¸€ç« æ‰€è®¨è®ºçš„ï¼Œè§£ç å™¨é€šè¿‡é€ä¸ªé¢„æµ‹ä»¤ç‰Œæ¥æ‰§è¡Œæ¨ç†ï¼Œè¿™æ˜¯ç”±æ¨¡å‹çš„ **generate()** æ–¹æ³•å®ç°çš„ã€‚è®¾ç½® **predict_with_generate=True** å‘Šè¯‰ **Seq2SeqTrainer** ä½¿ç”¨è¯¥æ–¹æ³•è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬è¿˜è°ƒæ•´äº†ä¸€äº›é»˜è®¤çš„è¶…å‚æ•°ï¼Œä¾‹å¦‚å­¦ä¹ ç‡ã€epoch æ•°å’Œæƒé‡è¡°å‡ï¼Œå¹¶ä¸”æˆ‘ä»¬è®¾ç½®äº† **save_total_limit** è®­ç»ƒæœŸé—´æœ€å¤šåªä¿å­˜ 3 ä¸ªæ£€æŸ¥ç‚¹çš„é€‰é¡¹â€”â€”è¿™æ˜¯å› ä¸ºå³ä½¿æ˜¯ mT5 çš„â€œsmallâ€ç‰ˆæœ¬ä¹Ÿä½¿ç”¨å¤§çº¦ 1 GB çš„ç¡¬ç›˜ç©ºé—´ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡é™åˆ¶æˆ‘ä»¬ä¿å­˜çš„å‰¯æœ¬æ•°é‡æ¥èŠ‚çœä¸€ç‚¹ç©ºé—´ã€‚

`push_to_hub=True` å‚æ•°å°†å…è®¸æˆ‘ä»¬åœ¨è®­ç»ƒåå°†æ¨¡å‹æ¨é€åˆ° Hubï¼› æ‚¨å°†åœ¨`output_dir`å®šä¹‰çš„ä½ç½®ä¸­çš„ç”¨æˆ·é…ç½®æ–‡ä»¶ä¸‹æ‰¾åˆ°å­˜å‚¨åº“ã€‚ è¯·æ³¨æ„ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“çš„åç§°ï¼ˆç‰¹åˆ«æ˜¯å½“æ‚¨æƒ³è¦æ¨é€åˆ°ç»„ç»‡æ—¶ï¼Œæ‚¨å¿…é¡»ä½¿ç”¨æ­¤å‚æ•°ï¼‰ã€‚ ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬æ·»åŠ äº†`hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"` åˆ° `Seq2SeqTrainingArguments`ã€‚

æˆ‘ä»¬éœ€è¦åšçš„ä¸‹ä¸€ä»¶äº‹æ˜¯ä¸ºè®­ç»ƒå™¨æä¾›ä¸€ä¸ªâ€œcompute_metrics()â€å‡½æ•°ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒæœŸé—´è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ã€‚ æ€»ç»“èµ·æ¥ï¼Œè¿™æ¯”ç®€å•åœ°åœ¨æ¨¡å‹çš„é¢„æµ‹ä¸Šè°ƒç”¨ `rouge_score.compute()` æ›´å¤æ‚ä¸€äº›ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦åœ¨è®¡ç®— ROUGE åˆ†æ•°ä¹‹å‰å°†è¾“å‡ºå’Œæ ‡ç­¾è§£ç ä¸ºæ–‡æœ¬ã€‚ ä¸‹é¢çš„å‡½æ•°æ­£æ˜¯è¿™æ ·åšçš„ï¼Œå¹¶ä¸”è¿˜åˆ©ç”¨ `nltk` ä¸­çš„ `sent_tokenize()` å‡½æ•°æ¥ç”¨æ¢è¡Œç¬¦åˆ†éš”æ‘˜è¦è¯­å¥ï¼š

```py
import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # Decode generated summaries into text
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # Replace -100 in the labels as we can't decode them
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # Decode reference summaries into text
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # ROUGE expects a newline after each sentence
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    # Compute ROUGE scores
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # Extract the median scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæˆ‘ä»¬çš„åºåˆ—åˆ°åºåˆ—ä»»åŠ¡å®šä¹‰ä¸€ä¸ªæ•°æ®æ•´ç†å™¨ã€‚ç”±äº mT5 æ˜¯ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨ Transformer æ¨¡å‹ï¼Œå‡†å¤‡æˆ‘ä»¬çš„æ‰¹æ¬¡çš„ä¸€ä¸ªå¾®å¦™ä¹‹å¤„æ˜¯ï¼Œåœ¨è§£ç è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å°†æ ‡ç­¾å‘å³ç§»åŠ¨ä¸€ä¸ªã€‚ è¿™æ˜¯ä¸ºäº†ç¡®ä¿è§£ç å™¨åªçœ‹åˆ°ä¹‹å‰çš„çœŸå®çš„æ ‡ç­¾ï¼Œè€Œä¸æ˜¯å½“å‰æˆ–æœªæ¥çš„æ ‡ç­¾ï¼Œè¿™å¯¹äºæ¨¡å‹æ¥è¯´å¾ˆå®¹æ˜“è®°å¿†ã€‚ è¿™ç±»ä¼¼äºåœ¨ å› æœè¯­è¨€å»ºæ¨¡ ç­‰ä»»åŠ¡ä¸­å¦‚ä½•å°†æ©è”½çš„è‡ªæˆ‘æ³¨æ„åº”ç”¨äºè¾“å…¥ã€‚

å¹¸è¿çš„æ˜¯ï¼ŒğŸ¤— Transformers æä¾›äº†ä¸€ä¸ª `DataCollatorForSeq2Seq` æ•´ç†å™¨ï¼Œå®ƒå°†ä¸ºæˆ‘ä»¬åŠ¨æ€å¡«å……è¾“å…¥å’Œæ ‡ç­¾ã€‚ è¦å®ä¾‹åŒ–è¿™ä¸ªæ”¶é›†å™¨ï¼Œæˆ‘ä»¬åªéœ€è¦æä¾› `tokenizer` å’Œ `model`ï¼š

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªæ•´ç†å™¨åœ¨è¾“å…¥ä¸€å°æ‰¹ç¤ºä¾‹æ—¶ä¼šäº§ç”Ÿä»€ä¹ˆã€‚ é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åˆ é™¤å¸¦æœ‰å­—ç¬¦ä¸²çš„åˆ—ï¼Œå› ä¸ºæ•´ç†å™¨ä¸çŸ¥é“å¦‚ä½•å¡«å……è¿™äº›å…ƒç´ ï¼š

```py
tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset["train"].column_names
)
```

ç”±äº collator éœ€è¦ä¸€ä¸ª `dict` çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ª `dict` ä»£è¡¨æ•°æ®é›†ä¸­çš„ä¸€ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬è¿˜éœ€è¦åœ¨å°†æ•°æ®ä¼ é€’ç»™ data collator ä¹‹å‰å°†æ•°æ®æ•´ç†æˆé¢„æœŸçš„æ ¼å¼ï¼š

```py
features = [tokenized_datasets["train"][i] for i in range(2)]
data_collator(features)
```

```py
{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,
            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,
           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,
            260,      1,      0,      0,      0,      0,      0,      0],
        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,
          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,
           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,
           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],
        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],
        [    0,   259, 27531, 13483,   259,  7505]])}
```

è¿™é‡Œè¦æ³¨æ„çš„ä¸»è¦æ˜¯ç¬¬ä¸€ä¸ªä¾‹å­æ¯”ç¬¬äºŒä¸ªä¾‹å­è¦é•¿ï¼Œæ‰€ä»¥ç¬¬äºŒä¸ªä¾‹å­çš„ `input_ids` å’Œ `attention_mask` å·²ç»åœ¨å³ä¾§å¡«å……äº†ä¸€ä¸ª `[PAD]` æ ‡è®°ï¼ˆå…¶ ID æ˜¯ `0`ï¼‰ã€‚ ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° `labels` å·²ç”¨ `-100` å¡«å……ï¼Œä»¥ç¡®ä¿å¡«å……æ ‡è®°è¢«æŸå¤±å‡½æ•°å¿½ç•¥ã€‚ æœ€åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€ä¸ªæ–°çš„ `decoder_input_ids`ï¼Œå®ƒé€šè¿‡åœ¨ç¬¬ä¸€ä¸ªæ¡ç›®ä¸­æ’å…¥ `[PAD]` æ ‡è®°å°†æ ‡ç­¾å‘å³ç§»åŠ¨ã€‚

æˆ‘ä»¬ç»ˆäºæ‹¥æœ‰äº†è®­ç»ƒæ‰€éœ€çš„æ‰€æœ‰çš„å‰æœŸå‡†å¤‡ï¼æˆ‘ä»¬ç°åœ¨åªéœ€è¦ä½¿ç”¨æ ‡å‡†å‚æ•°å®ä¾‹åŒ–è®­ç»ƒå™¨ï¼š

```py
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

å¹¶å¯åŠ¨æˆ‘ä»¬çš„è®­ç»ƒï¼š

```py
trainer.train()
```

åœ¨è®­ç»ƒæœŸé—´ï¼Œæ‚¨åº”è¯¥ä¼šçœ‹åˆ°è®­ç»ƒæŸå¤±å‡å°‘å¹¶ä¸” ROUGE åˆ†æ•°éšç€æ¯ä¸ª epoch å¢åŠ ã€‚è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥é€šè¿‡è¿è¡Œ**Trainer.evaluate()** æŸ¥çœ‹æœ€ç»ˆçš„ ROUGE åˆ†æ•° ï¼š

```py
trainer.evaluate()
```

```py
{'eval_loss': 3.028524398803711,
 'eval_rouge1': 16.9728,
 'eval_rouge2': 8.2969,
 'eval_rougeL': 16.8366,
 'eval_rougeLsum': 16.851,
 'eval_gen_len': 10.1597,
 'eval_runtime': 6.1054,
 'eval_samples_per_second': 38.982,
 'eval_steps_per_second': 4.914}
```

ä»åˆ†æ•°ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è½»æ¾è¶…è¿‡äº†æˆ‘ä»¬çš„ lead-3 baselineâ€”â€”å¾ˆå¥½ï¼æœ€åè¦åšçš„æ˜¯å°†æ¨¡å‹æƒé‡æ¨é€åˆ° Hubï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
trainer.push_to_hub(commit_message="Training complete", tags="summarization")
```

```py
'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0'
```

è¿™ä¼šå°†æ£€æŸ¥ç‚¹å’Œé…ç½®æ–‡ä»¶ä¿å­˜åˆ° **output_dir** , åœ¨å°†æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ åˆ°é›†çº¿å™¨ä¹‹å‰ã€‚é€šè¿‡æŒ‡å®š **tags** å‚æ•°ï¼Œæˆ‘ä»¬è¿˜ç¡®ä¿é›†çº¿å™¨ä¸Šçš„å°éƒ¨ä»¶å°†æ˜¯ä¸€ä¸ªç”¨äºæ±‡æ€»ç®¡é“çš„å°éƒ¨ä»¶ï¼Œè€Œä¸æ˜¯ä¸ mT5 æ¶æ„å…³è”çš„é»˜è®¤æ–‡æœ¬ç”Ÿæˆå°éƒ¨ä»¶ï¼ˆæœ‰å…³æ¨¡å‹æ ‡ç­¾çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[ğŸ¤— Hub æ–‡æ¡£](https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined)ï¼‰ã€‚è¾“å‡ºæ¥è‡ª **trainer.push_to_hub()** æ˜¯ Git æäº¤å“ˆå¸Œçš„ URLï¼Œå› æ­¤æ‚¨å¯ä»¥è½»æ¾æŸ¥çœ‹å¯¹æ¨¡å‹å­˜å‚¨åº“æ‰€åšçš„æ›´æ”¹ï¼

åœ¨ç»“æŸæœ¬èŠ‚ä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å¦‚ä½•ä½¿ç”¨ ğŸ¤— Accelerate æä¾›çš„åº•å±‚ API å¯¹ mT5 è¿›è¡Œå¾®è°ƒã€‚

## ä½¿ç”¨ ğŸ¤— Accelerate å¾®è°ƒ mT5

ä½¿ç”¨ ğŸ¤— Accelerate å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹ä¸æˆ‘ä»¬åœ¨ Chapter 3 ä¸­é‡åˆ°çš„æ–‡æœ¬åˆ†ç±»ç¤ºä¾‹éå¸¸ç›¸ä¼¼ã€‚ ä¸»è¦åŒºåˆ«åœ¨äºéœ€è¦åœ¨è®­ç»ƒæœŸé—´æ˜¾å¼ç”Ÿæˆæ‘˜è¦å¹¶å®šä¹‰æˆ‘ä»¬å¦‚ä½•è®¡ç®— ROUGE åˆ†æ•°ï¼ˆå›æƒ³ä¸€ä¸‹ï¼Œ`Seq2SeqTrainer` ä¸ºæˆ‘ä»¬ç”Ÿæˆäº†æ‘˜è¦ï¼‰ã€‚ è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬å¦‚ä½•åœ¨ ğŸ¤— Accelerate ä¸­å®ç°è¿™ä¸¤ä¸ªè¦æ±‚ï¼

### ä¸ºè®­ç»ƒåšå¥½ä¸€åˆ‡å‡†å¤‡

The first thing we need to do is create a `DataLoader` for each of our splits. Since the PyTorch dataloaders expect batches of tensors, we need to set the format to `"torch"` in our datasets: æˆ‘ä»¬éœ€è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯ä¸ºæ¯ä¸ªæ•°æ®é›†çš„æ¯ä¸€ä¸ªæ‹†åˆ†åˆ›å»ºä¸€ä¸ª`DataLoader`ã€‚ ç”±äº PyTorch æ•°æ®åŠ è½½å™¨éœ€è¦æˆæ‰¹çš„å¼ é‡ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ•°æ®é›†ä¸­å°†æ ¼å¼è®¾ç½®ä¸º`torch`ï¼š

```py
tokenized_datasets.set_format("torch")
```

ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰äº†ä»…ç”±å¼ é‡ç»„æˆçš„æ•°æ®é›†ï¼Œæ¥ä¸‹æ¥è¦åšçš„æ˜¯å†æ¬¡å®ä¾‹åŒ–`DataCollatorForSeq2Seq`ã€‚ ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æä¾›æ¨¡å‹å¾®è°ƒå‰çš„ç‰ˆæœ¬ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä»ç¼“å­˜ä¸­å†æ¬¡åŠ è½½å®ƒï¼š

```py
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

ç„¶åæˆ‘ä»¬å¯ä»¥å®ä¾‹åŒ–æ•°æ®æ•´ç†å™¨å¹¶ä½¿ç”¨å®ƒæ¥å®šä¹‰æˆ‘ä»¬çš„æ•°æ®åŠ è½½å™¨ï¼š

```py
from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=batch_size
)
```

æ¥ä¸‹æ¥è¦åšçš„æ˜¯å®šä¹‰æˆ‘ä»¬æƒ³è¦ä½¿ç”¨çš„ä¼˜åŒ–å™¨ã€‚ä¸æˆ‘ä»¬çš„å…¶ä»–ç¤ºä¾‹ä¸€æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ **AdamW** ï¼Œè¿™é€‚ç”¨äºå¤§å¤šæ•°é—®é¢˜ï¼š

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

æœ€åï¼Œæˆ‘ä»¬å°†æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œæ•°æ®åŠ è½½å™¨æä¾›ç»™ **accelerator.prepare()** æ–¹æ³•ï¼š

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

ğŸš¨å¦‚æœæ‚¨åœ¨ TPU ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåˆ™éœ€è¦å°†ä¸Šè¿°æ‰€æœ‰ä»£ç ç§»åŠ¨åˆ°ä¸“é—¨çš„è®­ç»ƒå‡½æ•°ä¸­ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ç¬¬ä¸‰ç« ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æˆ‘ä»¬ç´¢è¦ç”¨çš„å¯¹è±¡ï¼Œè¿˜æœ‰ä¸‰ä»¶äº‹è¦åšï¼š

*   å®šä¹‰å­¦ä¹ ç‡è°ƒåº¦è®¡åˆ’ã€‚
*   å®ç°ä¸€ä¸ªåŠŸèƒ½æ¥å¯¹æ‘˜è¦è¿›è¡Œåç»­å¤„ç†ä»¥è¿›è¡Œè¯„ä¼°ã€‚
*   åœ¨ Hub ä¸Šåˆ›å»ºä¸€ä¸ªå­˜å‚¨åº“ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹æ¨é€åˆ°è¯¥å­˜å‚¨åº“ã€‚

å¯¹äºå­¦ä¹ ç‡è°ƒåº¦ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‰å‡ èŠ‚ä¸­çš„æ ‡å‡†çº¿æ€§è¡°å‡ï¼š

```py
from transformers import get_scheduler

num_train_epochs = 10
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

å¯¹äºåç»­å¤„ç†ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‡½æ•°ï¼Œå°†ç”Ÿæˆçš„æ‘˜è¦æ‹†åˆ†ä¸ºç”±æ¢è¡Œç¬¦åˆ†éš”çš„å¥å­ã€‚ è¿™æ˜¯ ROUGE æŒ‡æ ‡æ‰€æœŸæœ›çš„æ ¼å¼ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ç‰‡æ®µæ¥å®ç°ï¼š

```py
def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    # ROUGE expects a newline after each sentence
    preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]
    labels = ["\n".join(nltk.sent_tokenize(label)) for label in labels]

    return preds, labels
```

å¦‚æœä½ è¿˜è®°å¾—æˆ‘ä»¬æ˜¯å¦‚ä½•å®šä¹‰ `Seq2SeqTrainer` çš„ `compute_metrics()` å‡½æ•°çš„ï¼Œè¿™å¯¹ä½ æ¥è¯´åº”è¯¥å¾ˆç†Ÿæ‚‰ã€‚

æœ€åï¼Œæˆ‘ä»¬éœ€è¦åœ¨ Hugging Face Hub ä¸Šåˆ›å»ºä¸€ä¸ªæ¨¡å‹å­˜å‚¨åº“ã€‚ ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ğŸ¤— Hub åº“çš„ get_full_repo_nameã€‚ æˆ‘ä»¬åªéœ€è¦ä¸ºæˆ‘ä»¬çš„å­˜å‚¨åº“å®šä¹‰ä¸€ä¸ªåç§°ï¼Œè¯¥åº“æœ‰ä¸€ä¸ªéå¸¸å¥½ç”¨çš„å‡½æ•°å¯ä»¥å°†å­˜å‚¨åº“ ID ä¸ç”¨æˆ·é…ç½®æ–‡ä»¶ç»“åˆèµ·æ¥ï¼š

```py
from huggingface_hub import get_full_repo_name

model_name = "test-bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```py
'lewtun/mt5-finetuned-amazon-en-es-accelerate'
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªå­˜å‚¨åº“åç§°å°†æœ¬åœ°ç‰ˆæœ¬å…‹éš†åˆ°æˆ‘ä»¬çš„ç»“æœç›®å½•ä¸­ï¼Œè¯¥ç›®å½•å°†å­˜å‚¨è®­ç»ƒçš„æ¨¡å‹ï¼š

```py
from huggingface_hub import Repository

output_dir = "results-mt5-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

è¿™å°†å…è®¸æˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´é€šè¿‡è°ƒç”¨ `repo.push_to_hub()` æ–¹æ³•å°†æ¨¡å‹æ¨é€åˆ° Hubï¼ ç°åœ¨è®©æˆ‘ä»¬é€šè¿‡å†™å‡ºå®Œæ•´çš„è®­ç»ƒå¾ªç¯æ¥ç»“æŸæˆ‘ä»¬çš„åˆ†æã€‚

### è®­ç»ƒå¾ªç¯

æ–‡æœ¬æ‘˜è¦çš„è®­ç»ƒå¾ªç¯ä¸æˆ‘ä»¬é‡åˆ°çš„å…¶ä»– ğŸ¤— Accelerate ç¤ºä¾‹éå¸¸ç›¸ä¼¼ï¼Œå¤§è‡´åˆ†ä¸ºå››ä¸ªä¸»è¦æ­¥éª¤ï¼šè¿™

1.  é€šè¿‡åœ¨æ¯ä¸ª epoch è¿­ä»£ `train_dataloader` ä¸­çš„æ‰€æœ‰ç¤ºä¾‹æ¥è®­ç»ƒæ¨¡å‹ã€‚
2.  åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ç”Ÿæˆæ¨¡å‹æ‘˜è¦ï¼Œé¦–å…ˆç”Ÿæˆæ ‡è®°ï¼Œç„¶åå°†å®ƒä»¬ï¼ˆå’Œå‚è€ƒæ‘˜è¦ï¼‰è§£ç ä¸ºæ–‡æœ¬ã€‚
3.  ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ç›¸åŒæŠ€æœ¯è®¡ç®— ROUGE åˆ†æ•°ã€‚
4.  ä¿å­˜æ£€æŸ¥ç‚¹å¹¶å°†æ‰€æœ‰å†…å®¹æ¨é€åˆ° Hubã€‚ åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¾èµ– `Repository` å¯¹è±¡çš„å·§å¦™çš„ `blocking=False` å‚æ•°ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥åœ¨æ¯ä¸ª epoch å¼‚æ­¥åœ°ä¸Šä¼ æ£€æŸ¥ç‚¹ã€‚ è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿç»§ç»­è®­ç»ƒï¼Œè€Œä¸å¿…ç­‰å¾…ä¸ GB å¤§å°çš„æ¨¡å‹æ…¢å‘¼å‘¼çš„ä¸Šä¼ ï¼

è¿™äº›æ­¥éª¤å¯ä»¥åœ¨ä»¥ä¸‹ä»£ç å—ä¸­çœ‹åˆ°ï¼š

```py
from tqdm.auto import tqdm
import torch
import numpy as np

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
            )
            labels = batch["labels"]

            # If we did not pad to max length, we need to pad the labels too
            labels = accelerator.pad_across_processes(
                batch["labels"], dim=1, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            # Replace -100 in the labels as we can't decode them
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            if isinstance(generated_tokens, tuple):
                generated_tokens = generated_tokens[0]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=True
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    # Compute metrics
    result = rouge_score.compute()
    # Extract the median ROUGE scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    result = {k: round(v, 4) for k, v in result.items()}
    print(f"Epoch {epoch}:", result)

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```py
Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}
Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}
Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}
Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}
Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}
Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}
Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}
Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}
Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}
Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}
```

å°±æ˜¯è¿™æ ·ï¼ è¿è¡Œæ­¤ç¨‹åºåï¼Œæ‚¨å°†è·å¾—ä¸æˆ‘ä»¬ä½¿ç”¨â€œTrainerâ€è·å¾—çš„æ¨¡å‹å’Œç»“æœéå¸¸ç›¸ä¼¼çš„æ¨¡å‹å’Œç»“æœã€‚

## ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹

å°†æ¨¡å‹æ¨é€åˆ° Hub åï¼Œæ‚¨å¯ä»¥é€šè¿‡æ¨ç†å°éƒ¨ä»¶æˆ–â€œç®¡é“â€å¯¹è±¡æ¥ä½¿ç”¨å®ƒï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
from transformers import pipeline

hub_model_id = "huggingface-course/mt5-small-finetuned-amazon-en-es"
summarizer = pipeline("summarization", model=hub_model_id)
```

æˆ‘ä»¬å¯ä»¥å°†æµ‹è¯•é›†ä¸­çš„ä¸€äº›ç¤ºä¾‹ï¼ˆæ¨¡å‹è¿˜æ²¡æœ‰çœ‹åˆ°ï¼‰æä¾›ç»™æˆ‘ä»¬çš„ç®¡é“ï¼Œä»¥äº†è§£ç”Ÿæˆæ‘˜è¦çš„è´¨é‡ã€‚ é¦–å…ˆè®©æˆ‘ä»¬å®ç°ä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥ä¸€èµ·æ˜¾ç¤ºè¯„è®ºã€æ ‡é¢˜å’Œç”Ÿæˆçš„æ‘˜è¦ï¼š

```py
def print_summary(idx):
    review = books_dataset["test"][idx]["review_body"]
    title = books_dataset["test"][idx]["review_title"]
    summary = summarizer(books_dataset["test"][idx]["review_body"])[0]["summary_text"]
    print(f"'>>> Review: {review}'")
    print(f"\n'>>> Title: {title}'")
    print(f"\n'>>> Summary: {summary}'")
```

è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æˆ‘ä»¬å¾—åˆ°çš„ä¸€ä¸ªè‹±æ–‡ä¾‹å­ï¼š

```py
print_summary(100)
```

```py
'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesnâ€™t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. Itâ€™s also really expensive for what it is.'

'>>> Title: Not impressed at all... buy something else'

'>>> Summary: Nothing special at all about this product'
```

è¿™è¿˜ä¸é”™ï¼ æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®é™…ä¸Šå·²ç»èƒ½å¤Ÿé€šè¿‡å¢åŠ éƒ¨åˆ†æ–°è¯æ¥æ‰§è¡ŒæŠ½è±¡æ‘˜è¦ã€‚ ä¹Ÿè®¸æˆ‘ä»¬æ¨¡å‹æœ€é…·çš„æ–¹é¢æ˜¯å®ƒæ˜¯åŒè¯­çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿˜å¯ä»¥ç”Ÿæˆè¥¿ç­ç‰™è¯­è¯„è®ºçš„æ‘˜è¦ï¼š

```py
print_summary(0)
```

```py
'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada'

'>>> Title: Buena literatura para adolescentes'

'>>> Summary: Muy facil de leer'
```

æ‘˜è¦ç¿»è¯‘æˆäº†è‹±æ–‡çš„â€œéå¸¸å®¹æ˜“é˜…è¯»â€ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒæ˜¯ç›´æ¥ä»è¯„è®ºä¸­æå–çš„ã€‚ è¿™æ˜¾ç¤ºäº† mT5 æ¨¡å‹çš„å¤šåŠŸèƒ½æ€§ï¼Œå¹¶è®©æ‚¨ä½“éªŒäº†å¤„ç†å¤šè¯­è¨€è¯­æ–™åº“çš„æ„Ÿè§‰ï¼

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æŠŠæ³¨æ„åŠ›è½¬å‘ç¨å¾®å¤æ‚çš„ä»»åŠ¡ï¼šä»å¤´å¼€å§‹è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚