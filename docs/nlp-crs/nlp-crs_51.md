# æ ‡å‡†åŒ–å’Œé¢„æ ‡è®°åŒ–

> åŸæ–‡ï¼š[`huggingface.co/learn/nlp-course/zh-CN/chapter6/4?fw=pt`](https://huggingface.co/learn/nlp-course/zh-CN/chapter6/4?fw=pt)

                ![Ask a Question](https://discuss.huggingface.co/t/chapter-6-questions) ![Open In Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section4.ipynb) ![Open In Studio Lab](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section4.ipynb)

åœ¨æˆ‘ä»¬æ›´æ·±å…¥åœ°ç ”ç©¶ä¸ Transformer æ¨¡å‹ï¼ˆå­—èŠ‚å¯¹ç¼–ç  [BPE]ã€WordPiece å’Œ Unigramï¼‰ä¸€èµ·ä½¿ç”¨çš„ä¸‰ç§æœ€å¸¸è§çš„å­è¯æ ‡è®°åŒ–ç®—æ³•ä¹‹å‰ï¼Œæˆ‘ä»¬å°†é¦–å…ˆçœ‹ä¸€ä¸‹æ¯ä¸ªæ ‡è®°å™¨åº”ç”¨äºæ–‡æœ¬çš„é¢„å¤„ç†ã€‚ä»¥ä¸‹æ˜¯æ ‡è®°åŒ–ç®¡é“ä¸­æ­¥éª¤çš„é«˜çº§æ¦‚è¿°ï¼š

![The tokenization pipeline.](img/648d9f2ea149d92346d8e5b52684a09c.png) ![The tokenization pipeline.](img/84d45b2e06b08900ef9e1ab86334bfe0.png)

åœ¨å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå­æ ‡è®°ä¹‹å‰ï¼ˆæ ¹æ®å…¶æ¨¡å‹ï¼‰ï¼Œåˆ†è¯å™¨æ‰§è¡Œä¸¤ä¸ªæ­¥éª¤ï¼š *normalization* å’Œ *pre-tokenization*.

## æ­£å¸¸åŒ–

[`www.youtube-nocookie.com/embed/4IIC2jI9CaU`](https://www.youtube-nocookie.com/embed/4IIC2jI9CaU)

æ ‡å‡†åŒ–æ­¥éª¤æ¶‰åŠä¸€äº›å¸¸è§„æ¸…ç†ï¼Œä¾‹å¦‚åˆ é™¤ä¸å¿…è¦çš„ç©ºæ ¼ã€å°å†™å’Œ/æˆ–åˆ é™¤é‡éŸ³ç¬¦å·ã€‚å¦‚æœä½ ç†Ÿæ‚‰[Unicode normalization](http://www.unicode.org/reports/tr15/)ï¼ˆä¾‹å¦‚ NFC æˆ– NFKCï¼‰ï¼Œè¿™ä¹Ÿæ˜¯ tokenizer å¯èƒ½åº”ç”¨çš„ä¸œè¥¿ã€‚

ğŸ¤—Transformers **tokenizer** æœ‰ä¸€ä¸ªå±æ€§å«åš **backend_tokenizer** å®ƒæä¾›äº†å¯¹ ğŸ¤— Tokenizers åº“ä¸­åº•å±‚æ ‡è®°å™¨çš„è®¿é—®ï¼š

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(type(tokenizer.backend_tokenizer))
```

```py
<class 'tokenizers.Tokenizer'>
```

**normalizer** çš„å±æ€§ **tokenizer** å¯¹è±¡æœ‰ä¸€ä¸ª **normalize_str()** æˆ‘ä»¬å¯ä»¥ç”¨æ¥æŸ¥çœ‹æ ‡å‡†åŒ–æ˜¯å¦‚ä½•æ‰§è¡Œçš„æ–¹æ³•ï¼š

```py
print(tokenizer.backend_tokenizer.normalizer.normalize_str("HÃ©llÃ² hÃ´w are Ã¼?"))
```

```py
'hello how are u?'
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå› ä¸ºæˆ‘ä»¬é€‰æ‹©äº† **bert-base-uncased** æ£€æŸ¥ç‚¹ï¼Œæ ‡å‡†åŒ–åº”ç”¨å°å†™å¹¶åˆ é™¤é‡éŸ³ã€‚

âœï¸ **è¯•è¯•çœ‹!** ä»æ£€æŸ¥ç‚¹åŠ è½½æ ‡è®°å™¨å¹¶å°†ç›¸åŒçš„ç¤ºä¾‹ä¼ é€’ç»™å®ƒã€‚æ‚¨å¯ä»¥çœ‹åˆ°åˆ†è¯å™¨çš„å¸¦å£³å’Œæ— å£³ç‰ˆæœ¬ä¹‹é—´çš„ä¸»è¦åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ

## é¢„æ ‡è®°åŒ–

[`www.youtube-nocookie.com/embed/grlLV8AIXug`](https://www.youtube-nocookie.com/embed/grlLV8AIXug)

æ­£å¦‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­çœ‹åˆ°çš„ï¼Œåˆ†è¯å™¨ä¸èƒ½å•ç‹¬åœ¨åŸå§‹æ–‡æœ¬ä¸Šè¿›è¡Œè®­ç»ƒã€‚ç›¸åï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå°å®ä½“ï¼Œä¾‹å¦‚å•è¯ã€‚è¿™å°±æ˜¯é¢„æ ‡è®°åŒ–æ­¥éª¤çš„ç”¨æ­¦ä¹‹åœ°ã€‚ æ­£å¦‚æˆ‘ä»¬åœ¨ Chapter 2, åŸºäºå•è¯çš„æ ‡è®°å™¨å¯ä»¥ç®€å•åœ°å°†åŸå§‹æ–‡æœ¬æ‹†åˆ†ä¸ºç©ºç™½å’Œæ ‡ç‚¹ç¬¦å·çš„å•è¯ã€‚è¿™äº›è¯å°†æ˜¯åˆ†è¯å™¨åœ¨è®­ç»ƒæœŸé—´å¯ä»¥å­¦ä¹ çš„å­æ ‡è®°çš„è¾¹ç•Œã€‚

è¦æŸ¥çœ‹å¿«é€Ÿåˆ†è¯å™¨å¦‚ä½•æ‰§è¡Œé¢„åˆ†è¯ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ **pre_tokenize_str()** çš„æ–¹æ³• **pre_tokenizer** çš„å±æ€§ **tokenizer** ç›®çš„ï¼š

```py
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```py
[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]
```

è¯·æ³¨æ„åˆ†è¯å™¨å¦‚ä½•å·²ç»è·Ÿè¸ªåç§»é‡ï¼Œè¿™å°±æ˜¯å®ƒå¦‚ä½•ä¸ºæˆ‘ä»¬æä¾›ä¸Šä¸€èŠ‚ä¸­ä½¿ç”¨çš„åç§»é‡æ˜ å°„ã€‚è¿™é‡Œåˆ†è¯å™¨å¿½ç•¥äº†è¿™ä¸¤ä¸ªç©ºæ ¼ï¼Œåªç”¨ä¸€ä¸ªæ›¿æ¢å®ƒä»¬ï¼Œä½†åç§»é‡åœ¨ **are** å’Œ **you** è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ã€‚

ç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ BERT åˆ†è¯å™¨ï¼Œé¢„åˆ†è¯æ¶‰åŠå¯¹ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·è¿›è¡Œæ‹†åˆ†ã€‚å¯¹äºè¿™ä¸€æ­¥ï¼Œå…¶ä»–æ ‡è®°å™¨å¯ä»¥æœ‰ä¸åŒçš„è§„åˆ™ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨ GPT-2 æ ‡è®°å™¨ï¼š

```py
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

å®ƒä¹Ÿä¼šåœ¨ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·ä¸Šæ‹†åˆ†ï¼Œä½†å®ƒä¼šä¿ç•™ç©ºæ ¼å¹¶å°†å®ƒä»¬æ›¿æ¢ä¸º **Ä ** ç¬¦å·ï¼Œå¦‚æœæˆ‘ä»¬è§£ç ä»¤ç‰Œï¼Œåˆ™ä½¿å…¶èƒ½å¤Ÿæ¢å¤åŸå§‹ç©ºæ ¼ï¼š

```py
[('Hello', (0, 5)), (',', (5, 6)), ('Ä how', (6, 10)), ('Ä are', (10, 14)), ('Ä ', (14, 15)), ('Ä you', (15, 19)),
 ('?', (19, 20))]
```

å¦è¯·æ³¨æ„ï¼Œä¸ BERT åˆ†è¯å™¨ä¸åŒï¼Œæ­¤åˆ†è¯å™¨ä¸ä¼šå¿½ç•¥åŒç©ºæ ¼

æœ€åä¸€ä¸ªä¾‹å­ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹åŸºäº SentencePiece ç®—æ³•çš„ T5 åˆ†è¯å™¨ï¼š

```py
tokenizer = AutoTokenizer.from_pretrained("t5-small")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```py
[('â–Hello,', (0, 6)), ('â–how', (7, 10)), ('â–are', (11, 14)), ('â–you?', (16, 20))]
```

ä¸ GPT-2 æ ‡è®°å™¨ä¸€æ ·ï¼Œè¿™ä¸ªæ ‡è®°å™¨ä¿ç•™ç©ºæ ¼å¹¶ç”¨ç‰¹å®šæ ‡è®°æ›¿æ¢å®ƒä»¬ï¼ˆ **_** )ï¼Œä½† T5 åˆ†è¯å™¨åªåœ¨ç©ºæ ¼ä¸Šæ‹†åˆ†ï¼Œè€Œä¸æ˜¯æ ‡ç‚¹ç¬¦å·ã€‚è¿˜è¦æ³¨æ„ï¼Œå®ƒé»˜è®¤åœ¨å¥å­çš„å¼€å¤´æ·»åŠ äº†ä¸€ä¸ªç©ºæ ¼ï¼ˆä¹‹å‰ **Hello** ) å¹¶å¿½ç•¥äº†ä¹‹é—´çš„åŒç©ºæ ¼ **are** å’Œ **you** .

ç°åœ¨æˆ‘ä»¬å·²ç»äº†è§£äº†ä¸€äº›ä¸åŒçš„æ ‡è®°å™¨å¦‚ä½•å¤„ç†æ–‡æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹æ¢ç´¢åº•å±‚ç®—æ³•æœ¬èº«ã€‚æˆ‘ä»¬é¦–å…ˆå¿«é€Ÿæµè§ˆä¸€ä¸‹å¹¿æ³›é€‚ç”¨çš„ SentencePieceï¼›ç„¶åï¼Œåœ¨æ¥ä¸‹æ¥çš„ä¸‰ä¸ªéƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶ç”¨äºå­è¯æ ‡è®°åŒ–çš„ä¸‰ç§ä¸»è¦ç®—æ³•æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

## å¥å­

[SentencePiece](https://github.com/google/sentencepiece) æ˜¯ä¸€ç§ç”¨äºæ–‡æœ¬é¢„å¤„ç†çš„æ ‡è®°åŒ–ç®—æ³•ï¼Œæ‚¨å¯ä»¥å°†å…¶ä¸æˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„ä¸‰ä¸ªéƒ¨åˆ†ä¸­çœ‹åˆ°çš„ä»»ä½•æ¨¡å‹ä¸€èµ·ä½¿ç”¨ã€‚å®ƒå°†æ–‡æœ¬è§†ä¸º Unicode å­—ç¬¦åºåˆ—ï¼Œå¹¶ç”¨ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ç©ºæ ¼ï¼Œ **â–** .ä¸ Unigram ç®—æ³•ç»“åˆä½¿ç”¨ï¼ˆå‚è§ section 7), å®ƒç”šè‡³ä¸éœ€è¦é¢„æ ‡è®°åŒ–æ­¥éª¤ï¼Œè¿™å¯¹äºä¸ä½¿ç”¨ç©ºæ ¼å­—ç¬¦çš„è¯­è¨€ï¼ˆå¦‚ä¸­æ–‡æˆ–æ—¥è¯­ï¼‰éå¸¸æœ‰ç”¨ã€‚

SentencePiece çš„å¦ä¸€ä¸ªä¸»è¦ç‰¹ç‚¹æ˜¯å¯é€†æ ‡è®°åŒ–ï¼šç”±äºæ²¡æœ‰å¯¹ç©ºæ ¼è¿›è¡Œç‰¹æ®Šå¤„ç†ï¼Œå› æ­¤åªéœ€é€šè¿‡å°†å®ƒä»¬è¿æ¥èµ·æ¥å¹¶æ›¿æ¢ **_** s å¸¦ç©ºæ ¼â€”â€”è¿™ä¼šå¯¼è‡´æ ‡å‡†åŒ–çš„æ–‡æœ¬ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼ŒBERT åˆ†è¯å™¨åˆ é™¤äº†é‡å¤çš„ç©ºæ ¼ï¼Œå› æ­¤å®ƒçš„åˆ†è¯æ˜¯ä¸å¯é€†çš„ã€‚

## ç®—æ³•æ¦‚è¿°

åœ¨ä¸‹é¢çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥ç ”ç©¶ä¸‰ç§ä¸»è¦çš„å­è¯æ ‡è®°åŒ–ç®—æ³•ï¼šBPEï¼ˆç”± GPT-2 å’Œå…¶ä»–äººä½¿ç”¨ï¼‰ã€WordPieceï¼ˆä¾‹å¦‚ç”± BERT ä½¿ç”¨ï¼‰å’Œ Unigramï¼ˆç”± T5 å’Œå…¶ä»–äººä½¿ç”¨ï¼‰ã€‚åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰ï¼Œè¿™é‡Œæ˜¯å®ƒä»¬å„è‡ªå·¥ä½œåŸç†çš„å¿«é€Ÿæ¦‚è¿°ã€‚å¦‚æœæ‚¨è¿˜æ²¡æœ‰ç†è§£ï¼Œè¯·åœ¨é˜…è¯»ä¸‹ä¸€èŠ‚åç«‹å³å›åˆ°æ­¤è¡¨ã€‚

| Model | BPE | WordPiece | Unigram |
| :-: | :-: | :-: | :-: |
| Training | Starts from a small vocabulary and learns rules to merge tokens | Starts from a small vocabulary and learns rules to merge tokens | Starts from a large vocabulary and learns rules to remove tokens |
| Training step | Merges the tokens corresponding to the most common pair | Merges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent | Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus |
| Learns | Merge rules and a vocabulary | Just a vocabulary | A vocabulary with a score for each token |
| Encoding | Splits a word into characters and applies the merges learned during training | Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word | Finds the most likely split into tokens, using the scores learned during training |

ç°åœ¨è®©æˆ‘ä»¬æ·±å…¥äº†è§£ BPEï¼