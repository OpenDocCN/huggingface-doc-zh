# å­—èŠ‚å¯¹ç¼–ç æ ‡è®°åŒ–

> åŸæ–‡ï¼š[`huggingface.co/learn/nlp-course/zh-CN/chapter6/5?fw=pt`](https://huggingface.co/learn/nlp-course/zh-CN/chapter6/5?fw=pt)

                ![Ask a Question](https://discuss.huggingface.co/t/chapter-6-questions) ![Open In Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section5.ipynb) ![Open In Studio Lab](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section5.ipynb)

å­—èŠ‚å¯¹ç¼–ç (BPE)æœ€åˆè¢«å¼€å‘ä¸ºä¸€ç§å‹ç¼©æ–‡æœ¬çš„ç®—æ³•,ç„¶ååœ¨é¢„è®­ç»ƒ GPT æ¨¡å‹æ—¶è¢« OpenAI ç”¨äºæ ‡è®°åŒ–ã€‚è®¸å¤š Transformer æ¨¡å‹éƒ½ä½¿ç”¨å®ƒ,åŒ…æ‹¬ GPTã€GPT-2ã€RoBERTaã€BART å’Œ DeBERTaã€‚

[`www.youtube-nocookie.com/embed/HEikzVL-lZU`](https://www.youtube-nocookie.com/embed/HEikzVL-lZU)

ğŸ’¡ æœ¬èŠ‚æ·±å…¥ä»‹ç»äº† BPE,ç”šè‡³å±•ç¤ºäº†ä¸€ä¸ªå®Œæ•´çš„å®ç°ã€‚å¦‚æœä½ åªæƒ³å¤§è‡´äº†è§£æ ‡è®°åŒ–ç®—æ³•,å¯ä»¥è·³åˆ°æœ€åã€‚

## è®­ç»ƒç®—æ³•

BPE è®­ç»ƒé¦–å…ˆè®¡ç®—è¯­æ–™åº“ä¸­ä½¿ç”¨çš„å”¯ä¸€å•è¯é›†(åœ¨å®Œæˆæ ‡å‡†åŒ–å’Œé¢„æ ‡è®°åŒ–æ­¥éª¤ä¹‹å),ç„¶åé€šè¿‡è·å–ç”¨äºç¼–å†™è¿™äº›å•è¯çš„æ‰€æœ‰ç¬¦å·æ¥æ„å»ºè¯æ±‡è¡¨ã€‚ä¸€ä¸ªéå¸¸ç®€å•çš„ä¾‹å­,å‡è®¾æˆ‘ä»¬çš„è¯­æ–™åº“ä½¿ç”¨äº†è¿™äº”ä¸ªè¯:

```py
"hug", "pug", "pun", "bun", "hugs"
```

åŸºç¡€è¯æ±‡å°†æ˜¯ `["b", "g", "h", "n", "p", "s", "u"]`ã€‚å¯¹äºå®é™…æƒ…å†µ,åŸºæœ¬è¯æ±‡è¡¨å°†åŒ…å«æ‰€æœ‰ ASCII å­—ç¬¦,è‡³å°‘,å¯èƒ½è¿˜åŒ…å«ä¸€äº› Unicode å­—ç¬¦ã€‚å¦‚æœæ‚¨æ­£åœ¨æ ‡è®°çš„ç¤ºä¾‹ä½¿ç”¨ä¸åœ¨è®­ç»ƒè¯­æ–™åº“ä¸­çš„å­—ç¬¦,åˆ™è¯¥å­—ç¬¦å°†è½¬æ¢ä¸ºæœªçŸ¥æ ‡è®°ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè®¸å¤š NLP æ¨¡å‹åœ¨åˆ†æå¸¦æœ‰è¡¨æƒ…ç¬¦å·çš„å†…å®¹æ–¹é¢éå¸¸ç³Ÿç³•çš„åŸå› ä¹‹ä¸€ã€‚

TGPT-2 å’Œ RoBERTa æ ‡è®°å™¨(éå¸¸ç›¸ä¼¼)æœ‰ä¸€ä¸ªèªæ˜çš„æ–¹æ³•æ¥å¤„ç†è¿™ä¸ªé—®é¢˜: ä»–ä»¬ä¸æŠŠå•è¯çœ‹æˆæ˜¯ç”¨ Unicode å­—ç¬¦å†™çš„ï¼Œè€Œæ˜¯ç”¨å­—èŠ‚å†™çš„ã€‚è¿™æ ·,åŸºæœ¬è¯æ±‡è¡¨çš„å¤§å°å¾ˆå°(256),ä½†ä½ èƒ½æƒ³åˆ°çš„æ¯ä¸ªå­—ç¬¦ä»å°†è¢«åŒ…å«åœ¨å†…,è€Œä¸ä¼šæœ€ç»ˆè½¬æ¢ä¸ºæœªçŸ¥æ ‡è®°ã€‚è¿™ä¸ªæŠ€å·§è¢«ç§°ä¸º *å­—èŠ‚çº§ BPE*ã€‚

è·å¾—è¿™ä¸ªåŸºæœ¬è¯æ±‡å,æˆ‘ä»¬æ·»åŠ æ–°çš„æ ‡è®°,ç›´åˆ°é€šè¿‡å­¦ä¹ *åˆå¹¶*è¾¾åˆ°æ‰€éœ€çš„è¯æ±‡é‡,è¿™æ˜¯å°†ç°æœ‰è¯æ±‡è¡¨çš„ä¸¤ä¸ªå…ƒç´ åˆå¹¶ä¸ºä¸€ä¸ªæ–°å…ƒç´ çš„è§„åˆ™ã€‚å› æ­¤,åœ¨å¼€å§‹æ—¶,è¿™äº›åˆå¹¶å°†åˆ›å»ºå…·æœ‰ä¸¤ä¸ªå­—ç¬¦çš„æ ‡è®°,ç„¶å,éšç€è®­ç»ƒçš„è¿›è¡Œ,ä¼šåˆ›å»ºæ›´é•¿çš„å­è¯ã€‚

åœ¨åˆ†è¯å™¨è®­ç»ƒæœŸé—´çš„ä»»ä½•ä¸€æ­¥,BPE ç®—æ³•éƒ½ä¼šæœç´¢æœ€å¸¸è§çš„ç°æœ‰æ ‡è®°å¯¹ (â€œå¯¹â€,è¿™é‡Œæˆ‘ä»¬æŒ‡çš„æ˜¯å•è¯ä¸­çš„ä¸¤ä¸ªè¿ç»­æ ‡è®°)ã€‚æœ€é¢‘ç¹çš„ä¸€å¯¹å°†è¢«åˆå¹¶,æˆ‘ä»¬å†²æ´—å¹¶é‡å¤ä¸‹ä¸€æ­¥ã€‚

å›åˆ°æˆ‘ä»¬ä¹‹å‰çš„ä¾‹å­,è®©æˆ‘ä»¬å‡è®¾å•è¯å…·æœ‰ä»¥ä¸‹é¢‘ç‡:

```py
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

æ„å‘³ç€ `"hug"` åœ¨è¯­æ–™åº“ä¸­å‡ºç°äº† 10 æ¬¡, `"pug"` 5 æ¬¡, `"pun"` 12 æ¬¡, `"bun"` 4 æ¬¡, ä»¥åŠ `"hugs"` 5 æ¬¡ã€‚æˆ‘ä»¬é€šè¿‡å°†æ¯ä¸ªå•è¯æ‹†åˆ†ä¸ºå­—ç¬¦(å½¢æˆæˆ‘ä»¬åˆå§‹è¯æ±‡è¡¨çš„å­—ç¬¦)æ¥å¼€å§‹è®­ç»ƒ,è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å°†æ¯ä¸ªå•è¯è§†ä¸ºä¸€ä¸ªæ ‡è®°åˆ—è¡¨:

```py
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

ç„¶åæˆ‘ä»¬çœ‹æˆå¯¹ã€‚è¿™å¯¹ `("h", "u")` å‡ºç°åœ¨å•è¯ `"hug"` å’Œ `"hugs"`ä¸­,æ‰€ä»¥è¯­æ–™åº“ä¸­æ€»å…±æœ‰ 15 æ¬¡ã€‚ä¸è¿‡,è¿™å¹¶ä¸æ˜¯æœ€é¢‘ç¹çš„ä¸€å¯¹:è¿™ä¸ªè£èª‰å±äº `("u", "g")`,å®ƒå‡ºç°åœ¨ `"hug"`, `"pug"`, ä»¥åŠ `"hugs"`ä¸­,åœ¨è¯æ±‡è¡¨ä¸­æ€»å…± 20 æ¬¡ã€‚

å› æ­¤,æ ‡è®°å™¨å­¦ä¹ çš„ç¬¬ä¸€ä¸ªåˆå¹¶è§„åˆ™æ˜¯ `("u", "g") -> "ug"`,æ„æ€å°±æ˜¯ `"ug"` å°†è¢«æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­,å¹¶ä¸”è¿™å¯¹åº”è¯¥åˆå¹¶åˆ°è¯­æ–™åº“çš„æ‰€æœ‰å•è¯ä¸­ã€‚åœ¨è¿™ä¸ªé˜¶æ®µç»“æŸæ—¶,è¯æ±‡è¡¨å’Œè¯­æ–™åº“çœ‹èµ·æ¥åƒè¿™æ ·:

```py
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

ç°åœ¨æˆ‘ä»¬æœ‰ä¸€äº›å¯¼è‡´æ ‡è®°é•¿äºä¸¤ä¸ªå­—ç¬¦çš„å¯¹: ä¾‹å¦‚ `("h", "ug")`, åœ¨è¯­æ–™åº“ä¸­å‡ºç° 15 æ¬¡ã€‚ç„¶è€Œ,è¿™ä¸ªé˜¶æ®µæœ€é¢‘ç¹çš„å¯¹æ˜¯ `("u", "n")`,åœ¨è¯­æ–™åº“ä¸­å‡ºç° 16 æ¬¡,æ‰€ä»¥å­¦åˆ°çš„ç¬¬äºŒä¸ªåˆå¹¶è§„åˆ™æ˜¯ `("u", "n") -> "un"`ã€‚å°†å…¶æ·»åŠ åˆ°è¯æ±‡è¡¨å¹¶åˆå¹¶æ‰€æœ‰ç°æœ‰çš„è¿™ä¸ªå¯¹,å°†å‡ºç°:

```py
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
```

ç°åœ¨æœ€é¢‘ç¹çš„ä¸€å¯¹æ˜¯ `("h", "ug")`,æ‰€ä»¥æˆ‘ä»¬å­¦ä¹ äº†åˆå¹¶è§„åˆ™ `("h", "ug") -> "hug"`,è¿™ç»™äº†æˆ‘ä»¬ç¬¬ä¸€ä¸ªä¸‰ä¸ªå­—æ¯çš„æ ‡è®°ã€‚åˆå¹¶å,è¯­æ–™åº“å¦‚ä¸‹æ‰€ç¤º:

```py
Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
Corpus: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

æˆ‘ä»¬ç»§ç»­è¿™æ ·åˆå¹¶,ç›´åˆ°è¾¾åˆ°æˆ‘ä»¬æ‰€éœ€çš„è¯æ±‡é‡ã€‚

âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†!**ä½ è®¤ä¸ºä¸‹ä¸€ä¸ªåˆå¹¶è§„åˆ™æ˜¯ä»€ä¹ˆï¼Ÿ

## æ ‡è®°åŒ–ç®—æ³•

æ ‡è®°åŒ–ç´§è·Ÿè®­ç»ƒè¿‡ç¨‹,ä»æŸç§æ„ä¹‰ä¸Šè¯´,é€šè¿‡åº”ç”¨ä»¥ä¸‹æ­¥éª¤å¯¹æ–°è¾“å…¥è¿›è¡Œæ ‡è®°:

1.  è§„èŒƒåŒ–
2.  é¢„æ ‡è®°åŒ–
3.  å°†å•è¯æ‹†åˆ†ä¸ºå•ä¸ªå­—ç¬¦
4.  å°†å­¦ä¹ çš„åˆå¹¶è§„åˆ™æŒ‰é¡ºåºåº”ç”¨äºè¿™äº›æ‹†åˆ†

è®©æˆ‘ä»¬ä»¥æˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨çš„ç¤ºä¾‹ä¸ºä¾‹,å­¦ä¹ ä¸‰ä¸ªåˆå¹¶è§„åˆ™:

```py
("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"
```

è¿™ä¸ªå•è¯ `"bug"` å°†è¢«æ ‡è®°ä¸º `["b", "ug"]`ã€‚ç„¶è€Œ `"mug"`,å°†è¢«æ ‡è®°ä¸º `["[UNK]", "ug"]`,å› ä¸ºå­—æ¯ `"m"` ä¸å†åŸºæœ¬è¯æ±‡è¡¨ä¸­ã€‚åŒæ ·,å•è¯`"thug"` ä¼šè¢«æ ‡è®°ä¸º `["[UNK]", "hug"]`: å­—æ¯ `"t"` ä¸åœ¨åŸºæœ¬è¯æ±‡è¡¨ä¸­,åº”ç”¨åˆå¹¶è§„åˆ™é¦–å…ˆå¯¼è‡´ `"u"` å’Œ `"g"` è¢«åˆå¹¶,ç„¶åæ˜¯ `"hu"` å’Œ `"g"` è¢«åˆå¹¶ã€‚

âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†!** ä½ è®¤ä¸ºè¿™ä¸ªè¯ `"unhug"` å°†å¦‚ä½•è¢«æ ‡è®°ï¼Ÿ

## å®ç° BPE

ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ BPE ç®—æ³•çš„å®ç°ã€‚è¿™ä¸ä¼šæ˜¯ä½ å¯ä»¥åœ¨å¤§å‹è¯­æ–™åº“ä¸Šå®é™…ä½¿ç”¨çš„ä¼˜åŒ–ç‰ˆæœ¬;æˆ‘ä»¬åªæ˜¯æƒ³å‘ä½ å±•ç¤ºä»£ç ,ä»¥ä¾¿ä½ å¯ä»¥æ›´å¥½åœ°ç†è§£ç®—æ³•

é¦–å…ˆæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¯­æ–™åº“,æ‰€ä»¥è®©æˆ‘ä»¬ç”¨å‡ å¥è¯åˆ›å»ºä¸€ä¸ªç®€å•çš„è¯­æ–™åº“:

```py
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

æ¥ä¸‹æ¥,æˆ‘ä»¬éœ€è¦å°†è¯¥è¯­æ–™åº“é¢„å…ˆæ ‡è®°ä¸ºå•è¯ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨å¤åˆ¶ BPE æ ‡è®°å™¨(å¦‚ GPT-2),æˆ‘ä»¬å°†ä½¿ç”¨ `gpt2` æ ‡è®°å™¨ä½œä¸ºé¢„æ ‡è®°åŒ–çš„æ ‡è®°å™¨:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

ç„¶åæˆ‘ä»¬åœ¨è¿›è¡Œé¢„æ ‡è®°åŒ–æ—¶è®¡ç®—è¯­æ–™åº“ä¸­æ¯ä¸ªå•è¯çš„é¢‘ç‡:

```py
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)
```

```py
defaultdict(int, {'This': 3, 'Ä is': 2, 'Ä the': 1, 'Ä Hugging': 1, 'Ä Face': 1, 'Ä Course': 1, '.': 4, 'Ä chapter': 1,
    'Ä about': 1, 'Ä tokenization': 1, 'Ä section': 1, 'Ä shows': 1, 'Ä several': 1, 'Ä tokenizer': 1, 'Ä algorithms': 1,
    'Hopefully': 1, ',': 1, 'Ä you': 1, 'Ä will': 1, 'Ä be': 1, 'Ä able': 1, 'Ä to': 1, 'Ä understand': 1, 'Ä how': 1,
    'Ä they': 1, 'Ä are': 1, 'Ä trained': 1, 'Ä and': 1, 'Ä generate': 1, 'Ä tokens': 1})
```

ä¸‹ä¸€æ­¥æ˜¯è®¡ç®—åŸºæœ¬è¯æ±‡,ç”±è¯­æ–™åº“ä¸­ä½¿ç”¨çš„æ‰€æœ‰å­—ç¬¦ç»„æˆ:

```py
alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)
```

```py
[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
  't', 'u', 'v', 'w', 'y', 'z', 'Ä ']
```

æˆ‘ä»¬è¿˜åœ¨è¯¥è¯æ±‡è¡¨çš„å¼€å¤´æ·»åŠ äº†æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Šæ ‡è®°ã€‚å¯¹äº GPT-2,å”¯ä¸€çš„ç‰¹æ®Šæ ‡è®°æ˜¯ `"<|endoftext|>"`:

```py
vocab = ["<|endoftext|>"] + alphabet.copy()
```

æˆ‘ä»¬ç°åœ¨éœ€è¦å°†æ¯ä¸ªå•è¯æ‹†åˆ†ä¸ºå•ç‹¬çš„å­—ç¬¦,ä»¥ä¾¿èƒ½å¤Ÿå¼€å§‹è®­ç»ƒ:

```py
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

ç°åœ¨æˆ‘ä»¬å·²å‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒ,è®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æ¯å¯¹çš„é¢‘ç‡ã€‚æˆ‘ä»¬éœ€è¦åœ¨è®­ç»ƒçš„æ¯ä¸ªæ­¥éª¤ä¸­ä½¿ç”¨å®ƒ:

```py
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs
```

è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸ªå­—å…¸åœ¨åˆå§‹æ‹†åˆ†åçš„ä¸€éƒ¨åˆ†:

```py
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break
```

```py
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('Ä ', 'i'): 2
('Ä ', 't'): 7
('t', 'h'): 3
```

ç°åœ¨, æ‰¾åˆ°æœ€é¢‘ç¹çš„å¯¹åªéœ€è¦ä¸€ä¸ªå¿«é€Ÿçš„å¾ªç¯:

```py
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq < freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)
```

```py
('Ä ', 't') 7
```

æ‰€ä»¥ç¬¬ä¸€ä¸ªè¦å­¦ä¹ çš„åˆå¹¶æ˜¯ `('Ä ', 't') -> 'Ä t'`, æˆ‘ä»¬æ·»åŠ  `'Ä t'` åˆ°è¯æ±‡è¡¨:

```py
merges = {("Ä ", "t"): "Ä t"}
vocab.append("Ä t")
```

è¦ç»§ç»­æ¥ä¸‹æ¥çš„æ­¥éª¤,æˆ‘ä»¬éœ€è¦åœ¨æˆ‘ä»¬çš„`åˆ†è¯`å­—å…¸ä¸­åº”ç”¨è¯¥åˆå¹¶ã€‚è®©æˆ‘ä»¬ä¸ºæ­¤ç¼–å†™å¦ä¸€ä¸ªå‡½æ•°:

```py
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

æˆ‘ä»¬å¯ä»¥çœ‹çœ‹ç¬¬ä¸€æ¬¡åˆå¹¶çš„ç»“æœ:

```py
splits = merge_pair("Ä ", "t", splits)
print(splits["Ä trained"])
```

```py
['Ä t', 'r', 'a', 'i', 'n', 'e', 'd']
```

ç°åœ¨æˆ‘ä»¬æœ‰äº†å¾ªç¯æ‰€éœ€çš„ä¸€åˆ‡,ç›´åˆ°æˆ‘ä»¬å­¦ä¼šäº†æˆ‘ä»¬æƒ³è¦çš„æ‰€æœ‰åˆå¹¶ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¯æ±‡é‡è¾¾åˆ° 50:

```py
vocab_size = 50

while len(vocab) < vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq < freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])
```

ç»“æœ,æˆ‘ä»¬å­¦ä¹ äº† 19 æ¡åˆå¹¶è§„åˆ™(åˆå§‹è¯æ±‡è¡¨çš„å¤§å° 31 â€” 30 å­—æ¯å­—ç¬¦,åŠ ä¸Šç‰¹æ®Šæ ‡è®°):

```py
print(merges)
```

```py
{('Ä ', 't'): 'Ä t', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ä ', 'a'): 'Ä a', ('Ä t', 'o'): 'Ä to', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ä to', 'k'): 'Ä tok',
 ('Ä tok', 'en'): 'Ä token', ('n', 'd'): 'nd', ('Ä ', 'is'): 'Ä is', ('Ä t', 'h'): 'Ä th', ('Ä th', 'e'): 'Ä the',
 ('i', 'n'): 'in', ('Ä a', 'b'): 'Ä ab', ('Ä token', 'i'): 'Ä tokeni'}
```

è¯æ±‡è¡¨ç”±ç‰¹æ®Šæ ‡è®°ã€åˆå§‹å­—æ¯å’Œæ‰€æœ‰åˆå¹¶ç»“æœç»„æˆ:

```py
print(vocab)
```

```py
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ', 'Ä t', 'is', 'er', 'Ä a', 'Ä to', 'en', 'Th', 'This', 'ou', 'se',
 'Ä tok', 'Ä token', 'nd', 'Ä is', 'Ä th', 'Ä the', 'in', 'Ä ab', 'Ä tokeni']
```

ğŸ’¡ åœ¨åŒä¸€è¯­æ–™åº“ä¸Šä½¿ç”¨ `train_new_from_iterator()` ä¸ä¼šäº§ç”Ÿå®Œå…¨ç›¸åŒçš„è¯æ±‡è¡¨ã€‚è¿™æ˜¯å› ä¸ºå½“æœ‰æœ€é¢‘ç¹å¯¹çš„é€‰æ‹©æ—¶,æˆ‘ä»¬é€‰æ‹©é‡åˆ°çš„ç¬¬ä¸€ä¸ª, è€Œ ğŸ¤— Tokenizers åº“æ ¹æ®å†…éƒ¨ ID é€‰æ‹©ç¬¬ä¸€ä¸ªã€‚

ä¸ºäº†å¯¹æ–°æ–‡æœ¬è¿›è¡Œåˆ†è¯,æˆ‘ä»¬å¯¹å…¶è¿›è¡Œé¢„åˆ†è¯ã€æ‹†åˆ†ï¼Œç„¶ååº”ç”¨å­¦åˆ°çš„æ‰€æœ‰åˆå¹¶è§„åˆ™:

```py
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```

æˆ‘ä»¬å¯ä»¥åœ¨ä»»ä½•ç”±å­—æ¯è¡¨ä¸­çš„å­—ç¬¦ç»„æˆçš„æ–‡æœ¬ä¸Šå°è¯•è¿™ä¸ª:

```py
tokenize("This is not a token.")
```

```py
['This', 'Ä is', 'Ä ', 'n', 'o', 't', 'Ä a', 'Ä token', '.']
```

âš ï¸ å¦‚æœå­˜åœ¨æœªçŸ¥å­—ç¬¦,æˆ‘ä»¬çš„å®ç°å°†æŠ›å‡ºé”™è¯¯,å› ä¸ºæˆ‘ä»¬æ²¡æœ‰åšä»»ä½•å¤„ç†å®ƒä»¬ã€‚GPT-2 å®é™…ä¸Šæ²¡æœ‰æœªçŸ¥æ ‡è®°(ä½¿ç”¨å­—èŠ‚çº§ BPE æ—¶ä¸å¯èƒ½å¾—åˆ°æœªçŸ¥å­—ç¬¦),ä½†è¿™å¯èƒ½å‘ç”Ÿåœ¨è¿™é‡Œ,å› ä¸ºæˆ‘ä»¬æ²¡æœ‰åœ¨åˆå§‹è¯æ±‡è¡¨ä¸­åŒ…å«æ‰€æœ‰å¯èƒ½çš„å­—èŠ‚ã€‚ BPE çš„è¿™æ–¹é¢è¶…å‡ºäº†æœ¬èŠ‚çš„èŒƒå›´,å› æ­¤æˆ‘ä»¬å¿½ç•¥äº†ç»†èŠ‚ã€‚

è¿™å°±æ˜¯ BPE ç®—æ³•ï¼æ¥ä¸‹æ¥,æˆ‘ä»¬å°†çœ‹çœ‹ WordPieceã€‚