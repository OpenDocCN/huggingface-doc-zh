# å‡å°‘å†…å­˜ä½¿ç”¨

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/diffusers/optimization/memory`](https://huggingface.co/docs/diffusers/optimization/memory)

ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„éšœç¢æ˜¯æ‰€éœ€çš„å¤§é‡å†…å­˜ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å‡ ç§å‡å°‘å†…å­˜çš„æŠ€æœ¯ï¼Œç”šè‡³å¯ä»¥åœ¨å…è´¹æˆ–æ¶ˆè´¹çº§ GPU ä¸Šè¿è¡Œä¸€äº›æœ€å¤§çš„æ¨¡å‹ã€‚æœ‰äº›æŠ€æœ¯ç”šè‡³å¯ä»¥ç»“åˆä½¿ç”¨ä»¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚

åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œä¼˜åŒ–å†…å­˜æˆ–é€Ÿåº¦ä¼šå¯¼è‡´å¦ä¸€ä¸ªæ–¹é¢çš„æ€§èƒ½æé«˜ï¼Œå› æ­¤æ‚¨åº”è¯¥å°½å¯èƒ½åŒæ—¶ä¼˜åŒ–ä¸¤è€…ã€‚æœ¬æŒ‡å—ä¾§é‡äºæœ€å°åŒ–å†…å­˜ä½¿ç”¨ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥äº†è§£æœ‰å…³å¦‚ä½•åŠ é€Ÿæ¨æ–­çš„æ›´å¤šä¿¡æ¯ã€‚

ä»¥ä¸‹ç»“æœæ˜¯ä»åœ¨ Nvidia Titan RTX ä¸Šä½¿ç”¨ 50 ä¸ª DDIM æ­¥éª¤ä»ä¸€ä¸ªåœ¨ç«æ˜Ÿä¸Šéª‘é©¬çš„å®‡èˆªå‘˜çš„ç…§ç‰‡ç”Ÿæˆå•ä¸ª 512x512 å›¾åƒè·å¾—çš„ï¼Œå±•ç¤ºäº†ç”±äºå‡å°‘å†…å­˜æ¶ˆè€—è€Œå¯ä»¥æœŸæœ›çš„åŠ é€Ÿã€‚

|  | å»¶è¿Ÿ | åŠ é€Ÿ |
| --- | --- | --- |
| åŸå§‹ | 9.50 ç§’ | x1 |
| fp16 | 3.61 ç§’ | x2.63 |
| é€šé“æœ€å | 3.30 ç§’ | x2.88 |
| è¿½è¸ªçš„ UNet | 3.21 ç§’ | x2.96 |
| å†…å­˜é«˜æ•ˆæ³¨æ„åŠ› | 2.63 ç§’ | x3.61 |

## åˆ‡ç‰‡çš„ VAE

åˆ‡ç‰‡çš„ VAE é€šè¿‡é€ä¸ªè§£ç æ½œåœ¨çš„æ‰¹é‡å›¾åƒæˆ–åŒ…å« 32 å¼ æˆ–æ›´å¤šå›¾åƒçš„æ‰¹æ¬¡æ¥è§£ç å¤§æ‰¹é‡å›¾åƒï¼Œä»è€Œä½¿æœ‰é™çš„ VRAM æˆ–æ‰¹æ¬¡å¤„ç†æ›´å®¹æ˜“ã€‚å¦‚æœæ‚¨å®‰è£…äº† xFormersï¼Œæ‚¨å¯èƒ½è¿˜æƒ³å°†å…¶ä¸ enable_xformers_memory_efficient_attention()ç»“åˆä½¿ç”¨ä»¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚

è¦ä½¿ç”¨åˆ‡ç‰‡çš„ VAEï¼Œè¯·åœ¨æ¨æ–­ä¹‹å‰åœ¨æ‚¨çš„ç®¡é“ä¸Šè°ƒç”¨ enable_vae_slicing()ï¼š

```py
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
pipe.enable_vae_slicing()
#pipe.enable_xformers_memory_efficient_attention()
images = pipe([prompt] * 32).images
```

åœ¨å¤šå›¾åƒæ‰¹æ¬¡ä¸Šè¿›è¡Œ VAE è§£ç å¯èƒ½ä¼šå¸¦æ¥è½»å¾®çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¸”å¯¹äºå•å›¾åƒæ‰¹æ¬¡ä¸åº”è¯¥æœ‰æ€§èƒ½å½±å“ã€‚

## å¹³é“ºçš„ VAE

å¹³é“ºçš„ VAE å¤„ç†è¿˜å¯ä»¥é€šè¿‡å°†å›¾åƒåˆ†æˆé‡å çš„ç“¦ç‰‡ï¼Œè§£ç ç“¦ç‰‡ï¼Œç„¶åå°†è¾“å‡ºæ··åˆåœ¨ä¸€èµ·æ¥ç»„æˆæœ€ç»ˆå›¾åƒï¼Œä»è€Œåœ¨æœ‰é™çš„ VRAM ä¸Šå¤„ç†å¤§å›¾åƒï¼ˆä¾‹å¦‚ï¼Œåœ¨ 8GB çš„ VRAM ä¸Šç”Ÿæˆ 4k å›¾åƒï¼‰ã€‚å¦‚æœæ‚¨å®‰è£…äº† xFormersï¼Œè¿˜åº”è¯¥ä½¿ç”¨ enable_xformers_memory_efficient_attention()æ¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚

è¦ä½¿ç”¨å¹³é“ºçš„ VAE å¤„ç†ï¼Œè¯·åœ¨æ¨æ–­ä¹‹å‰åœ¨æ‚¨çš„ç®¡é“ä¸Šè°ƒç”¨ enable_vae_tiling()ï¼š

```py
import torch
from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe = pipe.to("cuda")
prompt = "a beautiful landscape photograph"
pipe.enable_vae_tiling()
#pipe.enable_xformers_memory_efficient_attention()

image = pipe([prompt], width=3840, height=2224, num_inference_steps=20).images[0]
```

è¾“å‡ºå›¾åƒå­˜åœ¨ä¸€äº›ç“¦ç‰‡ä¹‹é—´çš„è‰²è°ƒå˜åŒ–ï¼Œå› ä¸ºç“¦ç‰‡æ˜¯åˆ†å¼€è§£ç çš„ï¼Œä½†æ‚¨ä¸åº”è¯¥çœ‹åˆ°ç“¦ç‰‡ä¹‹é—´çš„æ˜æ˜¾ç¼éš™ã€‚å¯¹äºå°ºå¯¸ä¸º 512x512 æˆ–æ›´å°çš„å›¾åƒï¼Œç“¦ç‰‡åŠŸèƒ½å·²å…³é—­ã€‚

## CPU å¸è½½

å°†æƒé‡å¸è½½åˆ° CPUï¼Œå¹¶ä»…åœ¨æ‰§è¡Œå‰å‘ä¼ é€’æ—¶åŠ è½½å®ƒä»¬åˆ° GPU ä¸Šä¹Ÿå¯ä»¥èŠ‚çœå†…å­˜ã€‚é€šå¸¸ï¼Œè¿™ç§æŠ€æœ¯å¯ä»¥å°†å†…å­˜æ¶ˆè€—å‡å°‘åˆ°ä¸åˆ° 3GBã€‚

è¦æ‰§è¡Œ CPU å¸è½½ï¼Œè¯·åœ¨æ¨æ–­ä¹‹å‰åœ¨æ‚¨çš„ç®¡é“ä¸Šè°ƒç”¨ enable_sequential_cpu_offload():

```py
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
)

prompt = "a photo of an astronaut riding a horse on mars"
pipe.enable_sequential_cpu_offload()
image = pipe(prompt).images[0]
```

CPU å¸è½½é€‚ç”¨äºå­æ¨¡å—è€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹ã€‚è¿™æ˜¯æœ€å°åŒ–å†…å­˜æ¶ˆè€—çš„æœ€ä½³æ–¹æ³•ï¼Œä½†ç”±äºæ‰©æ•£è¿‡ç¨‹çš„è¿­ä»£æ€§è´¨ï¼Œæ¨æ–­é€Ÿåº¦è¦æ…¢å¾—å¤šã€‚ç®¡é“çš„ UNet ç»„ä»¶è¿è¡Œå¤šæ¬¡ï¼ˆæœ€å¤š`num_inference_steps`æ¬¡ï¼‰ï¼›æ¯æ¬¡ï¼Œä¸åŒçš„ UNet å­æ¨¡å—æ ¹æ®éœ€è¦é¡ºåºåŠ è½½å’Œå¸è½½ï¼Œå¯¼è‡´å¤§é‡çš„å†…å­˜ä¼ è¾“ã€‚

å¦‚æœæ‚¨æƒ³è¦ä¼˜åŒ–é€Ÿåº¦ï¼Œè€ƒè™‘ä½¿ç”¨ model offloadingï¼Œå› ä¸ºå®ƒé€Ÿåº¦æ›´å¿«ã€‚æƒè¡¡æ˜¯æ‚¨çš„å†…å­˜èŠ‚çœé‡ä¸ä¼šé‚£ä¹ˆå¤§ã€‚

åœ¨ä½¿ç”¨ enable_sequential_cpu_offload()æ—¶ï¼Œä¸è¦äº‹å…ˆå°†ç®¡é“ç§»åŠ¨åˆ° CUDAï¼Œå¦åˆ™å†…å­˜æ¶ˆè€—çš„å¢ç›Šå°†åªæ˜¯å¾®ä¸è¶³é“çš„ï¼ˆæœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§æ­¤[é—®é¢˜](https://github.com/huggingface/diffusers/issues/1934)ï¼‰ã€‚

enable_sequential_cpu_offload()æ˜¯ä¸€ä¸ªæœ‰çŠ¶æ€çš„æ“ä½œï¼Œå®ƒåœ¨æ¨¡å‹ä¸Šå®‰è£…é’©å­ã€‚

## æ¨¡å‹å¸è½½

æ¨¡å‹å¸è½½éœ€è¦ğŸ¤— Accelerate ç‰ˆæœ¬ 0.17.0 æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚

Sequential CPU offloading ä¿ç•™äº†å¤§é‡å†…å­˜ï¼Œä½†ä½¿æ¨ç†é€Ÿåº¦å˜æ…¢ï¼Œå› ä¸ºå­æ¨¡å—åœ¨éœ€è¦æ—¶ç§»åŠ¨åˆ° GPUï¼Œå¹¶åœ¨æ–°æ¨¡å—è¿è¡Œæ—¶ç«‹å³è¿”å›åˆ° CPUã€‚

å®Œæ•´æ¨¡å‹å¸è½½æ˜¯ä¸€ç§å°†æ•´ä¸ªæ¨¡å‹ç§»åŠ¨åˆ° GPU çš„æ›¿ä»£æ–¹æ³•ï¼Œè€Œä¸æ˜¯å¤„ç†æ¯ä¸ªæ¨¡å‹çš„ç»„æˆ*å­æ¨¡å—*ã€‚ä¸å°†ç®¡é“ç§»åŠ¨åˆ°`cuda`ç›¸æ¯”ï¼Œå¯¹æ¨ç†æ—¶é—´å‡ ä¹æ²¡æœ‰å½±å“ï¼Œå¹¶ä¸”ä»ç„¶æä¾›ä¸€äº›å†…å­˜èŠ‚çœã€‚

åœ¨æ¨¡å‹å¸è½½æœŸé—´ï¼Œç®¡é“çš„ä¸»è¦ç»„ä»¶ä¹‹ä¸€ï¼ˆé€šå¸¸æ˜¯æ–‡æœ¬ç¼–ç å™¨ã€UNet å’Œ VAEï¼‰è¢«æ”¾ç½®åœ¨ GPU ä¸Šï¼Œè€Œå…¶ä»–ç»„ä»¶åˆ™åœ¨ CPU ä¸Šç­‰å¾…ã€‚åƒ UNet è¿™æ ·è¿è¡Œå¤šæ¬¡è¿­ä»£çš„ç»„ä»¶ä¼šä¸€ç›´ä¿ç•™åœ¨ GPU ä¸Šï¼Œç›´åˆ°ä¸å†éœ€è¦ä¸ºæ­¢ã€‚

é€šè¿‡åœ¨ç®¡é“ä¸Šè°ƒç”¨ enable_model_cpu_offload()å¯ç”¨æ¨¡å‹å¸è½½ï¼š

```py
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
)

prompt = "a photo of an astronaut riding a horse on mars"
pipe.enable_model_cpu_offload()
image = pipe(prompt).images[0]
```

ä¸ºäº†åœ¨è°ƒç”¨æ¨¡å‹åæ­£ç¡®å¸è½½æ¨¡å‹ï¼Œéœ€è¦è¿è¡Œæ•´ä¸ªç®¡é“ï¼Œå¹¶æŒ‰ç…§ç®¡é“çš„é¢„æœŸé¡ºåºè°ƒç”¨æ¨¡å‹ã€‚å¦‚æœåœ¨å®‰è£…é’©å­ååœ¨ç®¡é“ä¸Šä¸‹æ–‡ä¹‹å¤–é‡ç”¨æ¨¡å‹ï¼Œè¯·è°¨æ…è¡Œäº‹ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[Removing Hooks](https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.hooks.remove_hook_from_module)ã€‚

enable_model_cpu_offload() æ˜¯ä¸€ä¸ªæœ‰çŠ¶æ€çš„æ“ä½œï¼Œå®ƒåœ¨æ¨¡å‹ä¸Šå®‰è£…é’©å­ï¼Œå¹¶åœ¨ç®¡é“ä¸Šå®‰è£…çŠ¶æ€ã€‚

## é€šé“æœ€åçš„å†…å­˜æ ¼å¼

é€šé“æœ€åçš„å†…å­˜æ ¼å¼æ˜¯åœ¨å†…å­˜ä¸­å¯¹ NCHW å¼ é‡è¿›è¡Œæ’åºçš„å¦ä¸€ç§æ–¹å¼ï¼Œä»¥ä¿ç•™ç»´åº¦æ’åºã€‚é€šé“æœ€åçš„å¼ é‡æ˜¯æŒ‰ç…§é€šé“æˆä¸ºæœ€å¯†é›†ç»´åº¦çš„æ–¹å¼æ’åºçš„ï¼ˆä»¥åƒç´ ä¸ºå•ä½å­˜å‚¨å›¾åƒï¼‰ã€‚ç”±äºç›®å‰å¹¶éæ‰€æœ‰è¿ç®—ç¬¦éƒ½æ”¯æŒé€šé“æœ€åçš„æ ¼å¼ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½è¾ƒå·®ï¼Œä½†æ‚¨ä»åº”å°è¯•å¹¶æŸ¥çœ‹å®ƒæ˜¯å¦é€‚ç”¨äºæ‚¨çš„æ¨¡å‹ã€‚

ä¾‹å¦‚ï¼Œè¦å°†ç®¡é“çš„ UNet è®¾ç½®ä¸ºä½¿ç”¨é€šé“æœ€åçš„æ ¼å¼ï¼š

```py
print(pipe.unet.conv_out.state_dict()["weight"].stride())  # (2880, 9, 3, 1)
pipe.unet.to(memory_format=torch.channels_last)  # in-place operation
print(
    pipe.unet.conv_out.state_dict()["weight"].stride()
)  # (2880, 1, 960, 320) having a stride of 1 for the 2nd dimension proves that it works
```

## è·Ÿè¸ª

è·Ÿè¸ªé€šè¿‡æ¨¡å‹çš„ç¤ºä¾‹è¾“å…¥å¼ é‡ï¼Œå¹¶æ•è·åœ¨å…¶é€šè¿‡æ¨¡å‹çš„å±‚æ—¶æ‰§è¡Œçš„æ“ä½œã€‚è¿”å›çš„å¯æ‰§è¡Œæ–‡ä»¶æˆ–`ScriptFunction`ç»è¿‡å³æ—¶ç¼–è¯‘è¿›è¡Œäº†ä¼˜åŒ–ã€‚

è¦è·Ÿè¸ªä¸€ä¸ª UNetï¼š

```py
import time
import torch
from diffusers import StableDiffusionPipeline
import functools

# torch disable grad
torch.set_grad_enabled(False)

# set variables
n_experiments = 2
unet_runs_per_experiment = 50

# load inputs
def generate_inputs():
    sample = torch.randn((2, 4, 64, 64), device="cuda", dtype=torch.float16)
    timestep = torch.rand(1, device="cuda", dtype=torch.float16) * 999
    encoder_hidden_states = torch.randn((2, 77, 768), device="cuda", dtype=torch.float16)
    return sample, timestep, encoder_hidden_states

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
).to("cuda")
unet = pipe.unet
unet.eval()
unet.to(memory_format=torch.channels_last)  # use channels_last memory format
unet.forward = functools.partial(unet.forward, return_dict=False)  # set return_dict=False as default

# warmup
for _ in range(3):
    with torch.inference_mode():
        inputs = generate_inputs()
        orig_output = unet(*inputs)

# trace
print("tracing..")
unet_traced = torch.jit.trace(unet, inputs)
unet_traced.eval()
print("done tracing")

# warmup and optimize graph
for _ in range(5):
    with torch.inference_mode():
        inputs = generate_inputs()
        orig_output = unet_traced(*inputs)

# benchmarking
with torch.inference_mode():
    for _ in range(n_experiments):
        torch.cuda.synchronize()
        start_time = time.time()
        for _ in range(unet_runs_per_experiment):
            orig_output = unet_traced(*inputs)
        torch.cuda.synchronize()
        print(f"unet traced inference took {time.time() - start_time:.2f} seconds")
    for _ in range(n_experiments):
        torch.cuda.synchronize()
        start_time = time.time()
        for _ in range(unet_runs_per_experiment):
            orig_output = unet(*inputs)
        torch.cuda.synchronize()
        print(f"unet inference took {time.time() - start_time:.2f} seconds")

# save the model
unet_traced.save("unet_traced.pt")
```

ç”¨è·Ÿè¸ªçš„æ¨¡å‹æ›¿æ¢ç®¡é“çš„`unet`å±æ€§ï¼š

```py
from diffusers import StableDiffusionPipeline
import torch
from dataclasses import dataclass

@dataclass
class UNet2DConditionOutput:
    sample: torch.FloatTensor

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
).to("cuda")

# use jitted unet
unet_traced = torch.jit.load("unet_traced.pt")

# del pipe.unet
class TracedUNet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.in_channels = pipe.unet.config.in_channels
        self.device = pipe.unet.device

    def forward(self, latent_model_input, t, encoder_hidden_states):
        sample = unet_traced(latent_model_input, t, encoder_hidden_states)[0]
        return UNet2DConditionOutput(sample=sample)

pipe.unet = TracedUNet()

with torch.inference_mode():
    image = pipe([prompt] * 1, num_inference_steps=50).images[0]
```

## å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›

æœ€è¿‘å…³äºåœ¨æ³¨æ„åŠ›å—ä¸­ä¼˜åŒ–å¸¦å®½çš„å·¥ä½œäº§ç”Ÿäº†å·¨å¤§çš„åŠ é€Ÿå’Œ GPU å†…å­˜ä½¿ç”¨é‡çš„å‡å°‘ã€‚æœ€æ–°çš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ç±»å‹æ˜¯[Flash Attention](https://arxiv.org/abs/2205.14135)ï¼ˆæ‚¨å¯ä»¥åœ¨[HazyResearch/flash-attention](https://github.com/HazyResearch/flash-attention)æŸ¥çœ‹åŸå§‹ä»£ç ï¼‰ã€‚

å¦‚æœå·²å®‰è£… PyTorch >= 2.0ï¼Œåˆ™åœ¨å¯ç”¨`xformers`æ—¶ä¸åº”æœŸæœ›æ¨ç†é€Ÿåº¦æå‡ã€‚

ä½¿ç”¨ Flash Attentionï¼Œå®‰è£…ä»¥ä¸‹å†…å®¹ï¼š

+   PyTorch > 1.12

+   CUDA å¯ç”¨

+   xFormers

ç„¶ååœ¨ç®¡é“ä¸Šè°ƒç”¨ enable_xformers_memory_efficient_attention()ï¼š

```py
from diffusers import DiffusionPipeline
import torch

pipe = DiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
).to("cuda")

pipe.enable_xformers_memory_efficient_attention()

with torch.inference_mode():
    sample = pipe("a small cat")

# optional: You can disable it via
# pipe.disable_xformers_memory_efficient_attention()
```

ä½¿ç”¨`xformers`æ—¶çš„è¿­ä»£é€Ÿåº¦åº”è¯¥ä¸ PyTorch 2.0 çš„è¿­ä»£é€Ÿåº¦ç›¸åŒ¹é…ï¼Œå¦‚ torch2.0 ä¸­æ‰€è¿°ã€‚
