# 语义引导

> 原始文本：[`huggingface.co/docs/diffusers/api/pipelines/semantic_stable_diffusion`](https://huggingface.co/docs/diffusers/api/pipelines/semantic_stable_diffusion)

提出了用于扩散模型的语义引导，详见[SEGA: Instructing Text-to-Image Models using Semantic Guidance](https://huggingface.co/papers/2301.12247)，为图像生成提供了强大的语义控制。对文本提示进行微小更改通常会导致完全不同的输出图像。然而，使用 SEGA 可以实现对图像的各种更改，可以轻松直观地控制，同时保持原始图像构图的真实性。

论文摘要如下：

*最近，文本到图像扩散模型因其惊人的能力从仅文本生成高保真图像而受到了很多关注。然而，实现与用户意图一致的一次性生成几乎是不可能的，然而对输入提示进行微小更改通常会导致非常不同的图像。这使用户几乎没有语义控制。为了让用户控制，我们展示了如何与扩散过程互动，灵活地引导它沿着语义方向前进。这种语义引导（SEGA）适用于任何使用无分类器引导的生成架构。更重要的是，它允许进行微妙和广泛的编辑，改变构图和风格，以及优化整体艺术构思。我们展示了 SEGA 在潜在和基于像素的扩散模型（如 Stable Diffusion、Paella 和 DeepFloyd-IF）上的有效性，使用各种任务，从而为其多功能性、灵活性和改进现有方法提供了有力证据。*

确保查看调度器指南，以了解如何探索调度器速度和质量之间的权衡，并查看重用管道中的组件部分，以了解如何有效地将相同组件加载到多个管道中。

## SemanticStableDiffusionPipeline

### `class diffusers.SemanticStableDiffusionPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/semantic_stable_diffusion/pipeline_semantic_stable_diffusion.py#L21)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor requires_safety_checker: bool = True )
```

参数

+   `vae`（AutoencoderKL）。

+   `tokenizer`（[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer））— 用于对文本进行标记化的`CLIPTokenizer`。

+   `unet`（UNet2DConditionModel、LMSDiscreteScheduler 或 PNDMScheduler 之一。

+   `safety_checker`（`Q16SafetyChecker`）— 用于估计生成图像是否可能被视为冒犯或有害的分类模块。请参考[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) — 用于从生成的图像中提取特征的`CLIPImageProcessor`；作为`safety_checker`的输入。

使用稳定扩散和潜在编辑进行文本到图像生成的流水线。

该模型继承自 DiffusionPipeline，并构建在 StableDiffusionPipeline 之上。查看超类文档以获取所有流水线实现的通用方法（下载、保存、在特定设备上运行等）。

`__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/semantic_stable_diffusion/pipeline_semantic_stable_diffusion.py#L210)

```py
( prompt: Union height: Optional = None width: Optional = None num_inference_steps: int = 50 guidance_scale: float = 7.5 negative_prompt: Union = None num_images_per_prompt: int = 1 eta: float = 0.0 generator: Union = None latents: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 editing_prompt: Union = None editing_prompt_embeddings: Optional = None reverse_editing_direction: Union = False edit_guidance_scale: Union = 5 edit_warmup_steps: Union = 10 edit_cooldown_steps: Union = None edit_threshold: Union = 0.9 edit_momentum_scale: Optional = 0.1 edit_mom_beta: Optional = 0.4 edit_weights: Optional = None sem_guidance: Optional = None ) → export const metadata = 'undefined';~pipelines.semantic_stable_diffusion.SemanticStableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`) — 指导图像生成的提示或提示。

+   `height` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *可选*, 默认为 50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但推断速度会变慢。

+   `guidance_scale` (`float`, *可选*, 默认为 7.5) — 更高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 指导图像生成中不包含什么的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用引导时（`guidance_scale < 1`）忽略。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *可选*, 默认为 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta（η）。仅适用于 DDIMScheduler，在其他调度程序中被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成的嘈杂潜在变量，用作图像生成的输入。可以用来使用不同的提示调整相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成一个潜在变量张量。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回一个 StableDiffusionPipelineOutput 而不是一个普通的元组。

+   `callback` (`Callable`, *可选*) — 在推断过程中每`callback_steps`步调用的函数。该函数使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *可选*, 默认为 1) — 调用`callback`函数的频率。如果未指定，则在每一步调用回调函数。

+   `editing_prompt` (`str` 或 `List[str]`, *可选*) — 用于语义引导的提示或提示。通过设置`editing_prompt = None`来禁用语义引导。提示的引导方向应通过`reverse_editing_direction`指定。

+   `editing_prompt_embeddings`（`torch.Tensor`，*可选*）— 用于语义指导的预先计算的嵌入。应通过`reverse_editing_direction`指定嵌入的指导方向。

+   `reverse_editing_direction`（`bool`或`List[bool]`，*可选*，默认为`False`）— 是否应增加或减少`editing_prompt`中对应的提示。

+   `edit_guidance_scale`（`float`或`List[float]`，*可选*，默认为 5）— 语义指导的指导比例。如果以列表形式提供，值应与`editing_prompt`对应。

+   `edit_warmup_steps`（`float`或`List[float]`，*可选*，默认为 10）— 对于每个提示，不应用语义指导的扩散步骤数。在这些步骤中计算动量，并在所有预热期结束后应用。

+   `edit_cooldown_steps`（`float`或`List[float]`，*可选*，默认为`None`）— 在应用语义指导之后的扩散步骤数（对于每个提示）。

+   `edit_threshold`（`float`或`List[float]`，*可选*，默认为 0.9）— 语义指导的阈值。

+   `edit_momentum_scale`（`float`，*可选*，默认为 0.1）— 要添加到每个扩散步骤的语义指导的动量比例。如果设置为 0.0，则禁用动量。在预热期间（对于扩散步骤小于`sld_warmup_steps`）已经建立了动量。只有在所有预热期结束后才会将动量添加到潜在指导中。

+   `edit_mom_beta`（`float`，*可选*，默认为 0.4）— 定义语义指导动量如何建立。`edit_mom_beta`指示保留多少先前动量。在预热期间（对于扩散步骤小于`edit_warmup_steps`）已经建立了动量。

+   `edit_weights`（`List[float]`，*可选*，默认为`None`）— 指示每个单独概念应该如何影响整体指导。如果未提供权重，则所有概念均等应用。

+   `sem_guidance`（`List[torch.Tensor]`，*可选*）— 预生成的指导向量列表，用于在生成时应用。列表的长度必须与`num_inference_steps`相对应。

返回

`~pipelines.semantic_stable_diffusion.SemanticStableDiffusionPipelineOutput`或`tuple`

如果`return_dict`为`True`，则返回`~pipelines.semantic_stable_diffusion.SemanticStableDiffusionPipelineOutput`，否则返回一个`tuple`，其中第一个元素是生成图像的列表，第二个元素是一个`bool`列表，指示相应生成的图像是否包含“不适宜工作”（nsfw）内容。

用于生成的管道的调用函数。

示例：

```py
>>> import torch
>>> from diffusers import SemanticStableDiffusionPipeline

>>> pipe = SemanticStableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> out = pipe(
...     prompt="a photo of the face of a woman",
...     num_images_per_prompt=1,
...     guidance_scale=7,
...     editing_prompt=[
...         "smiling, smile",  # Concepts to apply
...         "glasses, wearing glasses",
...         "curls, wavy hair, curly hair",
...         "beard, full beard, mustache",
...     ],
...     reverse_editing_direction=[
...         False,
...         False,
...         False,
...         False,
...     ],  # Direction of guidance i.e. increase all concepts
...     edit_warmup_steps=[10, 10, 10, 10],  # Warmup period for each concept
...     edit_guidance_scale=[4, 5, 5, 5.4],  # Guidance scale for each concept
...     edit_threshold=[
...         0.99,
...         0.975,
...         0.925,
...         0.96,
...     ],  # Threshold for each concept. Threshold equals the percentile of the latent space that will be discarded. I.e. threshold=0.99 uses 1% of the latent dimensions
...     edit_momentum_scale=0.3,  # Momentum scale that will be added to the latent guidance
...     edit_mom_beta=0.6,  # Momentum beta
...     edit_weights=[1, 1, 1, 1, 1],  # Weights of the individual concepts against each other
... )
>>> image = out.images[0]
```

## 稳定扩散安全管道输出

### `class diffusers.pipelines.semantic_stable_diffusion.pipeline_output.SemanticStableDiffusionPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/semantic_stable_diffusion/pipeline_output.py#L10)

```py
( images: Union nsfw_content_detected: Optional )
```

参数

+   `images`（`List[PIL.Image.Image]`或`np.ndarray`）— 长度为`batch_size`的去噪 PIL 图像列表或形状为`(batch_size, height, width, num_channels)`的 NumPy 数组。

+   `nsfw_content_detected`（`List[bool]`）— 列表指示相应生成的图像是否包含“不适宜工作”（nsfw）内容，如果无法执行安全检查，则为`None`。

稳定扩散管道的输出类。
