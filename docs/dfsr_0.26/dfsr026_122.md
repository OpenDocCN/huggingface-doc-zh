# Transformer2D

> 原始文本：[`huggingface.co/docs/diffusers/api/models/transformer2d`](https://huggingface.co/docs/diffusers/api/models/transformer2d)

一种用于类似图像数据的 Transformer 模型，来自[CompVis](https://huggingface.co/CompVis)，基于 Dosovitskiy 等人介绍的[视觉 Transformer](https://huggingface.co/papers/2010.11929)。Transformer2DModel 接受离散（向量嵌入的类别）或连续（实际嵌入）输入。

当输入是**连续**时：

1.  将输入投影并重塑为`(batch_size, sequence_length, feature_dimension)`。

1.  以标准方式应用 Transformer 块。

1.  将图像重塑。

当输入是**离散**时：

假设输入类别之一是被屏蔽的潜在像素。无噪声图像的预测类别不包含对被屏蔽像素的预测，因为无噪声图像无法被屏蔽。

1.  将输入（潜在像素的类别）转换为嵌入并应用位置嵌入。

1.  以标准方式应用 Transformer 块。

1.  预测无噪声图像的类别。

## Transformer2DModel

### `class diffusers.Transformer2DModel`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/transformer_2d.py#L44)

```py
( num_attention_heads: int = 16 attention_head_dim: int = 88 in_channels: Optional = None out_channels: Optional = None num_layers: int = 1 dropout: float = 0.0 norm_num_groups: int = 32 cross_attention_dim: Optional = None attention_bias: bool = False sample_size: Optional = None num_vector_embeds: Optional = None patch_size: Optional = None activation_fn: str = 'geglu' num_embeds_ada_norm: Optional = None use_linear_projection: bool = False only_cross_attention: bool = False double_self_attention: bool = False upcast_attention: bool = False norm_type: str = 'layer_norm' norm_elementwise_affine: bool = True norm_eps: float = 1e-05 attention_type: str = 'default' caption_channels: int = None )
```

参数

+   `num_attention_heads`（`int`，*可选*，默认为 16） - 用于多头注意力的头数。

+   `attention_head_dim`（`int`，*可选*，默认为 88） - 每个头中的通道数。

+   `in_channels`（`int`，*可选*） - 输入和输出中的通道数（如果输入是**连续**，请指定）。

+   `num_layers`（`int`，*可选*，默认为 1） - 要使用的 Transformer 块的层数。

+   `dropout`（`float`，*可选*，默认为 0.0） - 要使用的丢弃概率。

+   `cross_attention_dim`（`int`，*可选*） - 要使用的`encoder_hidden_states`维度数。

+   `sample_size`（`int`，*可选*） - 潜在图像的宽度（如果输入是**离散**，请指定）。在训练期间固定，因为它用于学习一些位置嵌入。

+   `num_vector_embeds`（`int`，*可选*） - 潜在像素的向量嵌入的类别数（如果输入是**离散**，请指定）。包括被屏蔽的潜在像素的类别。

+   `activation_fn`（`str`，*可选*，默认为`"geglu"`） - 在前馈中使用的激活函数。

+   `num_embeds_ada_norm`（`int`，*可选*） - 训练期间使用的扩散步骤数。如果至少一个 norm_layers 是`AdaLayerNorm`，请传递。在训练期间固定，因为它用于学习添加到隐藏状态的嵌入的数量。

    在推断期间，您可以去噪最多但不超过`num_embeds_ada_norm`步。

+   `attention_bias`（`bool`，*可选*） - 配置`TransformerBlocks`注意力是否应包含偏置参数。

用于类似图像的数据的 2D Transformer 模型。

#### `forward`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/transformer_2d.py#L245)

```py
( hidden_states: Tensor encoder_hidden_states: Optional = None timestep: Optional = None added_cond_kwargs: Dict = None class_labels: Optional = None cross_attention_kwargs: Dict = None attention_mask: Optional = None encoder_attention_mask: Optional = None return_dict: bool = True )
```

参数

+   `hidden_states`（如果是离散，则为形状为`(batch size, num latent pixels)`的`torch.LongTensor`，如果是连续，则为形状为`(batch size, channel, height, width)`的`torch.FloatTensor`） - 输入`hidden_states`。

+   `encoder_hidden_states`（`torch.FloatTensor`，形状为`(batch size, sequence len, embed dims)`，*可选*） - 用于交叉注意力层的条件嵌入。如果未给出，则交叉注意力默认为自注意力。

+   `timestep`（`torch.LongTensor`，*可选*） - 用于指示去噪步骤。可选的时间步骤将作为`AdaLayerNorm`中的嵌入应用。

+   `class_labels`（`torch.LongTensor`，形状为`(batch size, num classes)`，*可选*） - 用于指示类别标签条件。可选的类别标签将作为`AdaLayerZeroNorm`中的嵌入应用。

+   `cross_attention_kwargs`（`Dict[str, Any]`，*可选*）- 如果指定，将作为 kwargs 字典传递给`AttentionProcessor`，如[diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中的`self.processor`中定义的那样。

+   `attention_mask`（`torch.Tensor`，*可选*）- 形状为`(batch, key_tokens)`的注意力掩码应用于`encoder_hidden_states`。如果为`1`，则保留掩码，否则如果为`0`，则丢弃。掩码将被转换为偏置，该偏置会向与“丢弃”标记对应的注意力分数添加大的负值。

+   `encoder_attention_mask`（`torch.Tensor`，*可选*）- 应用于`encoder_hidden_states`的交叉注意力掩码。支持两种格式：

    +   掩码（`batch, sequence_length`）True = 保留，False = 丢弃。

    +   偏置（`batch, 1, sequence_length`）0 = 保留，-10000 = 丢弃。

    如果`ndim == 2`：将被解释为一个掩码，然后转换为与上述格式一致的偏置。此偏置将添加到交叉注意力分数中。

+   `return_dict`（`bool`，*可选*，默认为`True`）- 是否返回 UNet2DConditionOutput 而不是普通元组。

Transformer2DModel 的前向方法。

## Transformer2DModelOutput

### `class diffusers.models.transformers.transformer_2d.Transformer2DModelOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/transformer_2d.py#L30)

```py
( sample: FloatTensor )
```

参数

+   `sample`（形状为`(batch_size, num_channels, height, width)`或`(batch size, num_vector_embeds - 1, num_latent_pixels)`的`torch.FloatTensor`）如果 Transformer2DModel 是离散的）- 在`encoder_hidden_states`输入条件下的隐藏状态输出。如果是离散的，则返回未加噪声的潜在像素的概率分布。

Transformer2DModel 的输出。
