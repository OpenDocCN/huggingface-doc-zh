# MusicLDM

> 原文链接：[`huggingface.co/docs/diffusers/api/pipelines/musicldm`](https://huggingface.co/docs/diffusers/api/pipelines/musicldm)

MusicLDM 是由 Ke Chen、Yusong Wu、Haohe Liu、Marianna Nezhurina、Taylor Berg-Kirkpatrick、Shlomo Dubnov 在[MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies](https://huggingface.co/papers/2308.01546)中提出的。MusicLDM 以文本提示作为输入，并预测相应的音乐样本。

受到[Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview)和[AudioLDM](https://huggingface.co/docs/diffusers/api/pipelines/audioldm)的启发，MusicLDM 是一个文本到音乐的*潜在扩散模型（LDM）*，从[CLAP](https://huggingface.co/docs/transformers/main/model_doc/clap)潜在中学习连续音频表示。

MusicLDM 在 466 小时的音乐数据语料库上进行训练。节拍同步数据增强策略应用于音乐样本，既在时间域中，也在潜在空间中。使用节拍同步数据增强策略鼓励模型在训练样本之间插值，但仍保持在训练数据的领域内。结果是生成的音乐更加多样化，同时仍然忠实于相应的风格。

该论文的摘要如下：

*扩散模型在跨模态生成任务中表现出有希望的结果，包括文本到图像和文本到音频的生成。然而，生成音乐作为一种特殊类型的音频，由于音乐数据的有限可用性以及与版权和抄袭相关的敏感问题，存在独特的挑战。在本文中，为了解决这些挑战，我们首先构建了一个最先进的文本到音乐模型 MusicLDM，该模型将 Stable Diffusion 和 AudioLDM 架构调整到音乐领域。我们通过在一组音乐数据样本上重新训练对比语言-音频预训练模型（CLAP）和 Hifi-GAN 声码器，作为 MusicLDM 的组成部分。然后，为了解决训练数据的限制并避免抄袭，我们利用一个节拍跟踪模型，并提出了两种不同的数据增强混合策略：节拍同步音频混合和节拍同步潜在混合，分别直接或通过潜在嵌入空间重新组合训练音频。这些混合策略鼓励模型在音乐训练样本之间插值，并在训练数据的凸包内生成新音乐，使生成的音乐更加多样化，同时仍然忠实于相应的风格。除了流行的评估指标外，我们设计了几个基于 CLAP 分数的新评估指标，以证明我们提出的 MusicLDM 和节拍同步混合策略提高了生成音乐的质量和新颖性，以及输入文本与生成音乐之间的对应关系。*

该管道由[sanchit-gandhi](https://huggingface.co/sanchit-gandhi)贡献。

## 提示

在构建提示时，请记住：

+   描述性的提示输入效果最好；使用形容词描述声音（例如，“高质量”或“清晰”），尽可能使提示具体（例如，“旋律技术与快节奏和合成器”比“技术”更好）。

+   使用*负面提示*可以显著提高生成音频的质量。尝试使用“低质量，平均质量”的负面提示。

在推理过程中：

+   生成的音频样本的质量可以通过`num_inference_steps`参数进行控制；更高的步骤会提供更高质量的音频，但推理速度会变慢。

+   可以一次生成多个波形：将`num_waveforms_per_prompt`设置为大于 1 的值以启用。将对生成的波形和提示文本进行自动评分，并相应地对音频进行排序。

+   生成的音频样本的*长度*可以通过变化`audio_length_in_s`参数来控制。

确保查看调度器指南以了解如何探索调度器速度和质量之间的权衡，并查看跨管道重用组件部分以了解如何有效地将相同组件加载到多个管道中。

## MusicLDMPipeline

### `class diffusers.MusicLDMPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L67)

```py
( vae: AutoencoderKL text_encoder: Union tokenizer: Union feature_extractor: Optional unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers vocoder: SpeechT5HifiGan )
```

参数

+   `vae` (AutoencoderKL) — 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。

+   `text_encoder` ([ClapModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel)) — 冻结的文本-音频嵌入模型（`ClapTextModel`），具体来说是[laion/clap-htsat-unfused](https://huggingface.co/laion/clap-htsat-unfused)变种。

+   `tokenizer` (`PreTrainedTokenizer`) — 用于对文本进行标记化的[RobertaTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)。

+   `feature_extractor` ([ClapFeatureExtractor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor)) — 从音频波形计算梅尔频谱图的特征提取器。

+   `unet` (UNet2DConditionModel) — 用于降噪编码音频潜在空间的`UNet2DConditionModel`。

+   `scheduler` (SchedulerMixin) — 与`unet`结合使用的调度器，用于降噪编码音频潜在空间。可以是 DDIMScheduler、LMSDiscreteScheduler 或 PNDMScheduler 之一。

+   `vocoder` ([SpeechT5HifiGan](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5HifiGan)) — 类`SpeechT5HifiGan`的声码器。

使用 MusicLDM 进行文本到音频生成的管道。

该模型继承自 DiffusionPipeline。查看超类文档以了解为所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L434)

```py
( prompt: Union = None audio_length_in_s: Optional = None num_inference_steps: int = 200 guidance_scale: float = 2.0 negative_prompt: Union = None num_waveforms_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None return_dict: bool = True callback: Optional = None callback_steps: Optional = 1 cross_attention_kwargs: Optional = None output_type: Optional = 'np' ) → export const metadata = 'undefined';AudioPipelineOutput or tuple
```

参数

+   `prompt` (`str` or `List[str]`, *可选*) — 指导音频生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。

+   `audio_length_in_s` (`int`, *可选*, 默认为 10.24) — 生成的音频样本的长度（秒）。

+   `num_inference_steps` (`int`, *可选*, 默认为 200) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的音频，但会降低推理速度。

+   `guidance_scale` (`float`, *可选*, 默认为 2.0) — 更高的引导比例值鼓励模型生成与文本`prompt`密切相关的音频，但会降低声音质量。当`guidance_scale > 1`时启用引导比例。

+   `negative_prompt` (`str` or `List[str]`, *可选*) — 指导音频生成中不包括的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。当不使用引导时（`guidance_scale < 1`）将被忽略。

+   `num_waveforms_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的波形数量。如果 `num_waveforms_per_prompt > 1`，文本编码模型是一个联合文本-音频模型（[ClapModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel)），并且分词器是一个 `[~transformers.ClapProcessor]`，则将在生成的输出和输入文本之间执行自动评分。此评分根据它们在联合文本-音频嵌入空间中与文本输入的余弦相似度对生成的波形进行排名。

+   `eta` (`float`, *可选*, 默认为 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta (η)。仅适用于 DDIMScheduler，在其他调度程序中将被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成的嘈杂潜在变量，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机 `generator` 进行采样生成潜在变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，文本嵌入将从 `prompt` 输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds` 将从 `negative_prompt` 输入参数生成。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回 AudioPipelineOutput 而不是普通元组。

+   `callback` (`Callable`, *可选*) — 在推断期间每 `callback_steps` 步调用一次的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *可选*, 默认为 1) — 调用 `callback` 函数的频率。如果未指定，将在每一步调用回调。

+   `cross_attention_kwargs` (`dict`, *可选*) — 如果指定，将传递给 `AttentionProcessor` 的 kwargs 字典，如[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的那样。

+   `output_type` (`str`, *可选*, 默认为 `"np"`) — 生成的音频的输出格式。选择 `"np"` 返回一个 NumPy `np.ndarray` 或 `"pt"` 返回一个 PyTorch `torch.Tensor` 对象。设置为 `"latent"` 以返回潜在扩散模型 (LDM) 输出。

返回

AudioPipelineOutput 或 `tuple`

如果 `return_dict` 为 `True`，将返回 AudioPipelineOutput，否则将返回一个 `tuple`，其中第一个元素是包含生成音频的列表。

用于生成的管道的调用函数。

示例:

```py
>>> from diffusers import MusicLDMPipeline
>>> import torch
>>> import scipy

>>> repo_id = "ucsd-reach/musicldm"
>>> pipe = MusicLDMPipeline.from_pretrained(repo_id, torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")

>>> prompt = "Techno music with a strong, upbeat tempo and high melodic riffs"
>>> audio = pipe(prompt, num_inference_steps=10, audio_length_in_s=5.0).audios[0]

>>> # save the audio sample as a .wav file
>>> scipy.io.wavfile.write("techno.wav", rate=16000, data=audio)
```

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L125)

```py
( )
```

禁用切片 VAE 解码。如果先前启用了 `enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `enable_model_cpu_offload`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L400)

```py
( gpu_id = 0 )
```

使用加速器将所有模型转移到 CPU，减少内存使用量，对性能影响较小。与`enable_sequential_cpu_offload`相比，此方法在调用其`forward`方法时一次将一个完整模型移动到 GPU，并且该模型保持在 GPU 中直到下一个模型运行。与`enable_sequential_cpu_offload`相比，内存节省较少，但由于`unet`的迭代执行，性能要好得多。

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/musicldm/pipeline_musicldm.py#L117)

```py
( )
```

启用切片 VAE 解码。当启用此选项时，VAE 将在几个步骤中将输入张量分割成片来计算解码。这对节省一些内存并允许更大的批量大小很有用。
