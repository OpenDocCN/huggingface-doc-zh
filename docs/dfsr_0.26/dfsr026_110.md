# 模型

> 原文链接：[`huggingface.co/docs/diffusers/api/models/overview`](https://huggingface.co/docs/diffusers/api/models/overview)

🤗 Diffusers 为流行的算法和模块提供预训练模型，以创建自定义扩散系统。模型的主要功能是去噪输入样本，其模型由分布<math><semantics><mrow><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_{\theta}(x_{t-1}|x_{t})</annotation></semantics></math>pθ​(xt−1​∣xt​)建模。

所有模型都是从基础 ModelMixin 类构建的，该类是一个[`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)，提供了保存和加载模型的基本功能，可以在本地和从 Hugging Face Hub 中加载。

## ModelMixin

### `class diffusers.ModelMixin`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_utils.py#L186)

```py
( )
```

所有模型的基类。

ModelMixin 负责存储模型配置，并提供了加载、下载和保存模型的方法。

+   `config_name` (`str`) — 在调用 save_pretrained()时保存模型的文件名。

#### `disable_gradient_checkpointing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_utils.py#L238)

```py
( )
```

为当前模型停用梯度检查点（在其他框架中可能被称为*激活检查点*或*检查点激活*）。

#### `disable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_utils.py#L299)

```py
( )
```

从[xFormers](https://facebookresearch.github.io/xformers/)禁用内存高效的注意力。

#### `enable_gradient_checkpointing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_utils.py#L229)

```py
( )
```

为当前模型激活梯度检查点（在其他框架中可能被称为*激活检查点*或*检查点激活*）。

#### `enable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_utils.py#L263)

```py
( attention_op: Optional = None )
```

参数

+   `attention_op` (`Callable`, *optional*) — 用作 xFormers 的[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)函数的`op`参数的默认`None`运算符的覆盖。

从[xFormers](https://facebookresearch.github.io/xformers/)启用内存高效的注意力。

启用此选项时，您应该观察到较低的 GPU 内存使用量，并且在推断期间可能会加速。不能保证在训练期间加速。

⚠️ 当内存高效的注意力和切片注意力都启用时，内存高效的注意力优先。

示例：

```py
>>> import torch
>>> from diffusers import UNet2DConditionModel
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> model = UNet2DConditionModel.from_pretrained(
...     "stabilityai/stable-diffusion-2-1", subfolder="unet", torch_dtype=torch.float16
... )
>>> model = model.to("cuda")
>>> model.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
```

#### `from_pretrained`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_utils.py#L393)

```py
( pretrained_model_name_or_path: Union **kwargs )
```

参数

+   `pretrained_model_name_or_path` (`str`或`os.PathLike`, *optional*) — 可以是：

    +   一个字符串，预训练模型在 Hub 上托管的*模型 id*（例如`google/ddpm-celebahq-256`）。

    +   一个*目录*路径（例如`./my_model_directory`），其中包含使用 save_pretrained()保存的模型权重。

+   `cache_dir` (`Union[str, os.PathLike]`, *optional*) — 如果不使用标准缓存，则下载的预训练模型配置将被缓存在其中的目录路径。

+   `torch_dtype` (`str` 或 `torch.dtype`, *可选*) — 覆盖默认的 `torch.dtype` 并使用另一种 dtype 加载模型。如果传递 `"auto"`，dtype 将自动从模型的权重中派生。

+   `force_download` (`bool`, *可选*, 默认为 `False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`, *可选*, 默认为 `False`) — 是否恢复下载模型权重和配置文件。如果设置为 `False`，任何未完全下载的文件将被删除。

+   `proxies` (`Dict[str, str]`, *可选*) — 一个按协议或端点使用的代理服务器字典，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。代理将在每个请求中使用。

+   `output_loading_info` (`bool`, *可选*, 默认为 `False`) — 是否还返回一个包含缺失键、意外键和错误消息的字典。

+   `local_files_only(bool,` *可选*, 默认为 `False`) — 是否仅加载本地模型权重和配置文件。如果设置为 `True`，则不会从 Hub 下载模型。

+   `token` (`str` 或 *bool*, *可选*) — 用作远程文件的 HTTP bearer 授权的令牌。如果为 `True`，则使用从 `diffusers-cli login` 生成的令牌（存储在 `~/.huggingface`）。

+   `revision` (`str`, *可选*, 默认为 `"main"`) — 要使用的特定模型版本。可以是分支名称、标签名称、提交 ID 或 Git 允许的任何标识符。

+   `from_flax` (`bool`, *可选*, 默认为 `False`) — 从 Flax 检查点保存文件加载模型权重。

+   `subfolder` (`str`, *可选*, 默认为 `""`) — Hub 或本地较大模型存储库中模型文件的子文件夹位置。

+   `mirror` (`str`, *可选*) — 镜像源以解决在中国下载模型时的可访问性问题。我们不保证源的及时性或安全性，您应参考镜像站点获取更多信息。

+   `device_map` (`str` 或 `Dict[str, Union[int, str, torch.device]]`, *可选*) — 一个指定每个子模块应该放在哪里的映射。不需要为每个参数/缓冲区名称定义；一旦给定模块名称在内部，它的每个子模块都将被发送到相同的设备。

    设置 `device_map="auto"` 以使 🤗 Accelerate 自动计算最优化的 `device_map`。有关每个选项的更多信息，请参阅[设计设备映射](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map)。

+   `max_memory` (`Dict`, *可选*) — 用于最大内存的设备标识符字典。如果未设置，将默认为每个 GPU 和可用 CPU RAM 的最大内存。

+   `offload_folder` (`str` 或 `os.PathLike`, *可选*) — 如果 `device_map` 包含值 `"disk"`，则是卸载权重的路径。

+   `offload_state_dict` (`bool`, *可选*) — 如果为 `True`，则临时将 CPU 状态字典转移到硬盘以避免 CPU RAM 不足，如果 CPU 状态字典的重量 + 检查点的最大分片的重量不适合。当存在一些磁盘卸载时，默认为 `True`。

+   `low_cpu_mem_usage` (`bool`, *可选*, 如果 torch 版本 >= 1.9.0 则默认为 `True`，否则为 `False`) — 加速模型加载，仅加载预训练权重而不初始化权重。在加载模型时，还尝试不使用超过 CPU 内存中的 1x 模型大小（包括峰值内存）。仅支持 PyTorch >= 1.9.0。如果您使用较旧版本的 PyTorch，将此参数设置为 `True` 将引发错误。

+   `variant` (`str`, *可选*) — 从指定的 `variant` 文件名加载权重，例如 `"fp16"` 或 `"ema"`。在加载 `from_flax` 时会被忽略。

+   `use_safetensors` (`bool`, *可选*, 默认为 `None`) — 如果设置为 `None`，则会下载 `safetensors` 权重（如果可用且已安装 `safetensors` 库）。如果设置为 `True`，则会强制从 `safetensors` 权重加载模型。如果设置为 `False`，则不会加载 `safetensors` 权重。

从预训练模型配置实例化预训练的 PyTorch 模型。

默认情况下，模型处于评估模式 - `model.eval()` - 并且关闭了 dropout 模块。要训练模型，请使用 `model.train()` 将其设置回训练模式。

要使用私有或[门控模型](https://huggingface.co/docs/hub/models-gated#gated-models)，请使用 `huggingface-cli login` 登录。您还可以激活特殊的[“离线模式”](https://huggingface.co/diffusers/installation.html#offline-mode)以在受防火墙保护的环境中使用此方法。

示例：

```py
from diffusers import UNet2DConditionModel

unet = UNet2DConditionModel.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="unet")
```

如果您收到下面的错误消息，则需要为下游任务微调权重：

```py
Some weights of UNet2DConditionModel were not initialized from the model checkpoint at runwayml/stable-diffusion-v1-5 and are newly initialized because the shapes did not match:
- conv_in.weight: found shape torch.Size([320, 4, 3, 3]) in the checkpoint and torch.Size([320, 9, 3, 3]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

#### `num_parameters`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_utils.py#L893)

```py
( only_trainable: bool = False exclude_embeddings: bool = False ) → export const metadata = 'undefined';int
```

参数

+   `only_trainable` (`bool`, *可选*, 默认为 `False`) — 是否仅返回可训练参数的数量。

+   `exclude_embeddings` (`bool`, *可选*, 默认为 `False`) — 是否仅返回非嵌入参数的数量。

返回

`int`

参数数量。

获取模块中（可训练或非嵌入）参数的数量。

示例：

```py
from diffusers import UNet2DConditionModel

model_id = "runwayml/stable-diffusion-v1-5"
unet = UNet2DConditionModel.from_pretrained(model_id, subfolder="unet")
unet.num_parameters(only_trainable=True)
859520964
```

#### `save_pretrained`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_utils.py#L305)

```py
( save_directory: Union is_main_process: bool = True save_function: Optional = None safe_serialization: bool = True variant: Optional = None push_to_hub: bool = False **kwargs )
```

参数

+   `save_directory` (`str` 或 `os.PathLike`) — 保存模型及其配置文件的目录。如果不存在，将会创建。

+   `is_main_process` (`bool`, *可选*, 默认为 `True`) — 调用此函数的进程是否为主进程。在分布式训练期间，当您需要在所有进程上调用此函数时，这很有用。在这种情况下，仅在主进程上设置 `is_main_process=True` 以避免竞争条件。

+   `save_function` (`Callable`) — 用于保存状态字典的函数。在分布式训练期间，当您需要用另一种方法替换 `torch.save` 时，这很有用。可以使用环境变量 `DIFFUSERS_SAVE_MODE` 进行配置。

+   `safe_serialization` (`bool`, *可选*, 默认为 `True`) — 是否使用 `safetensors` 或传统的 PyTorch 方法与 `pickle` 保存模型。

+   `variant` (`str`, *可选*) — 如果指定，权重将以 `pytorch_model.<variant>.bin` 格式保存。

+   `push_to_hub` (`bool`, *可选*, 默认为 `False`) — 是否在保存后将模型推送到 Hugging Face Hub。您可以使用 `repo_id` 指定要推送到的存储库（将默认为您的命名空间中的 `save_directory` 名称）。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 传递给 push_to_hub() 方法的额外关键字参数。

将模型及其配置文件保存到一个目录，以便可以使用 from_pretrained() 类方法重新加载它。

## FlaxModelMixin

### `class diffusers.FlaxModelMixin`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_flax_utils.py#L50)

```py
( )
```

所有 Flax 模型的基类。

FlaxModelMixin 负责存储模型配置，并提供加载、下载和保存模型的方法。

+   `config_name` (`str`) — 在调用 save_pretrained() 时保存模型的文件名。

#### `from_pretrained`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_flax_utils.py#L203)

```py
( pretrained_model_name_or_path: Union dtype: dtype = <class 'jax.numpy.float32'> *model_args **kwargs )
```

参数

+   `pretrained_model_name_or_path` (`str` 或 `os.PathLike`) — 可以是以下之一：

    +   一个字符串，预训练模型在 Hub 上托管的*模型 ID*（例如 `runwayml/stable-diffusion-v1-5`）。

    +   指向包含使用 save_pretrained()保存的模型权重的*目录*的路径（例如 `./my_model_directory`）。

+   `dtype` (`jax.numpy.dtype`, *optional*, 默认为 `jax.numpy.float32`) — 计算的数据类型。可以是 `jax.numpy.float32`、`jax.numpy.float16`（在 GPU 上）和 `jax.numpy.bfloat16`（在 TPU 上）之一。

    这可用于在 GPU 或 TPU 上启用混合精度训练或半精度推断。如果指定，所有计算将使用给定的 `dtype` 执行。

    这仅指定*计算*的数据类型，不影响模型参数的数据类型。

    如果要更改模型参数的数据类型，请参阅 to_fp16()和 to_bf16()。

+   `model_args`（位置参数序列，*optional*） — 所有剩余的位置参数都传递给基础模型的 `__init__` 方法。

+   `cache_dir` (`Union[str, os.PathLike]`, *optional*) — 下载的预训练模型配置缓存在其中的目录路径，如果未使用标准缓存。

+   `force_download` (`bool`, *optional*, 默认为 `False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`, *optional*, 默认为 `False`) — 是否恢复下载模型权重和配置文件。如果设置为 `False`，则删除任何未完全下载的文件。

+   `proxies` (`Dict[str, str]`, *optional*) — 用于每个请求的协议或端点的代理服务器字典，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。代理服务器在每个请求上使用。

+   `local_files_only(bool,` *optional*, 默认为 `False`) — 是否仅加载本地模型权重和配置文件。如果设置为 `True`，则模型不会从 Hub 下载。

+   `revision` (`str`, *optional*, 默认为 `"main"`) — 要使用的特定模型版本。可以是分支名称、标签名称、提交 ID 或 Git 允许的任何标识符。

+   `from_pt` (`bool`, *optional*, 默认为 `False`) — 从 PyTorch 检查点保存文件加载模型权重。

+   `kwargs`（剩余的关键字参数字典，*optional*） — 可用于更新配置对象（加载后）并启动模型（例如，`output_attentions=True`）。根据是否提供或自动加载了 `config`，行为不同。

    +   如果提供了 `config`，则 `kwargs` 直接传递给基础模型的 `__init__` 方法（我们假设配置的所有相关更新已经完成）。

    +   如果未提供配置，则首先将 `kwargs` 传递给配置类初始化函数 from_config()。与配置属性对应的 `kwargs` 的每个键都用提供的 `kwargs` 值覆盖该属性。不对应任何配置属性的剩余键将传递给基础模型的 `__init__` 函数。

从预训练模型配置实例化预训练的 Flax 模型。

示例：

```py
>>> from diffusers import FlaxUNet2DConditionModel

>>> # Download model and configuration from huggingface.co and cache.
>>> model, params = FlaxUNet2DConditionModel.from_pretrained("runwayml/stable-diffusion-v1-5")
>>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
>>> model, params = FlaxUNet2DConditionModel.from_pretrained("./test/saved_model/")
```

如果您收到下面的错误消息，则需要为您的下游任务微调权重：

```py
Some weights of UNet2DConditionModel were not initialized from the model checkpoint at runwayml/stable-diffusion-v1-5 and are newly initialized because the shapes did not match:
- conv_in.weight: found shape torch.Size([320, 4, 3, 3]) in the checkpoint and torch.Size([320, 9, 3, 3]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

#### `save_pretrained`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_flax_utils.py#L502)

```py
( save_directory: Union params: Union is_main_process: bool = True push_to_hub: bool = False **kwargs )
```

参数

+   `save_directory` (`str`或`os.PathLike`) — 要保存模型及其配置文件的目录。如果目录不存在，将会创建。

+   `params` (`Union[Dict, FrozenDict]`) — 模型参数的`PyTree`。

+   `is_main_process` (`bool`, *可选*, 默认为`True`) — 调用此函数的进程是否为主进程。在分布式训练中非常有用，当您需要在所有进程上调用此函数时。在这种情况下，仅在主进程上设置`is_main_process=True`以避免竞争条件。

+   `push_to_hub` (`bool`, *可选*, 默认为`False`) — 是否在保存后将模型推送到 Hugging Face 模型中心。您可以使用`repo_id`指定要推送到的存储库（默认为您的命名空间中的`save_directory`名称）。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 传递给 push_to_hub()方法的额外关键字参数。

将模型及其配置文件保存到一个目录中，以便可以使用 from_pretrained()类方法重新加载。

#### `to_bf16`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_flax_utils.py#L95)

```py
( params: Union mask: Any = None )
```

参数

+   `params` (`Union[Dict, FrozenDict]`) — 模型参数的`PyTree`。

+   `mask` (`Union[Dict, FrozenDict]`) — 一个与`params`树结构相同的`PyTree`。叶子节点应为布尔值。对于要转换的参数应为`True`，对于要跳过的参数应为`False`。

将浮点`params`转换为`jax.numpy.bfloat16`。这将返回一个新的`params`树，不会直接转换`params`。

这种方法可以在 TPU 上使用，显式地将模型参数转换为 bfloat16 精度，以进行完全的半精度训练或者保存权重为 bfloat16 以节省内存并提高速度。

示例:

```py
>>> from diffusers import FlaxUNet2DConditionModel

>>> # load model
>>> model, params = FlaxUNet2DConditionModel.from_pretrained("runwayml/stable-diffusion-v1-5")
>>> # By default, the model parameters will be in fp32 precision, to cast these to bfloat16 precision
>>> params = model.to_bf16(params)
>>> # If you don't want to cast certain parameters (for example layer norm bias and scale)
>>> # then pass the mask as follows
>>> from flax import traverse_util

>>> model, params = FlaxUNet2DConditionModel.from_pretrained("runwayml/stable-diffusion-v1-5")
>>> flat_params = traverse_util.flatten_dict(params)
>>> mask = {
...     path: (path[-2] != ("LayerNorm", "bias") and path[-2:] != ("LayerNorm", "scale"))
...     for path in flat_params
... }
>>> mask = traverse_util.unflatten_dict(mask)
>>> params = model.to_bf16(params, mask)
```

#### `to_fp16`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_flax_utils.py#L161)

```py
( params: Union mask: Any = None )
```

参数

+   `params` (`Union[Dict, FrozenDict]`) — 模型参数的`PyTree`。

+   `mask` (`Union[Dict, FrozenDict]`) — 一个与`params`树结构相同的`PyTree`。叶子节点应为布尔值。对于要转换的参数应为`True`，对于要跳过的参数应为`False`。

将浮点`params`转换为`jax.numpy.float16`。这将返回一个新的`params`树，不会直接转换`params`。

这种方法可以在 GPU 上使用，显式地将模型参数转换为 float16 精度，以进行完全的半精度训练或者保存权重为 float16 以节省内存并提高速度。

示例：

```py
>>> from diffusers import FlaxUNet2DConditionModel

>>> # load model
>>> model, params = FlaxUNet2DConditionModel.from_pretrained("runwayml/stable-diffusion-v1-5")
>>> # By default, the model params will be in fp32, to cast these to float16
>>> params = model.to_fp16(params)
>>> # If you want don't want to cast certain parameters (for example layer norm bias and scale)
>>> # then pass the mask as follows
>>> from flax import traverse_util

>>> model, params = FlaxUNet2DConditionModel.from_pretrained("runwayml/stable-diffusion-v1-5")
>>> flat_params = traverse_util.flatten_dict(params)
>>> mask = {
...     path: (path[-2] != ("LayerNorm", "bias") and path[-2:] != ("LayerNorm", "scale"))
...     for path in flat_params
... }
>>> mask = traverse_util.unflatten_dict(mask)
>>> params = model.to_fp16(params, mask)
```

#### `to_fp32`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_flax_utils.py#L134)

```py
( params: Union mask: Any = None )
```

参数

+   `params` (`Union[Dict, FrozenDict]`) — 模型参数的`PyTree`。

+   `mask` (`Union[Dict, FrozenDict]`) — 一个与`params`树结构相同的`PyTree`。叶子节点应为布尔值。对于要转换的参数应为`True`，对于要跳过的参数应为`False`。

将浮点`params`转换为`jax.numpy.float32`。这种方法可以用于显式地将模型参数转换为 fp32 精度。这将返回一个新的`params`树，不会直接转换`params`。

示例：

```py
>>> from diffusers import FlaxUNet2DConditionModel

>>> # Download model and configuration from huggingface.co
>>> model, params = FlaxUNet2DConditionModel.from_pretrained("runwayml/stable-diffusion-v1-5")
>>> # By default, the model params will be in fp32, to illustrate the use of this method,
>>> # we'll first cast to fp16 and back to fp32
>>> params = model.to_f16(params)
>>> # now cast back to fp32
>>> params = model.to_fp32(params)
```

## PushToHubMixin

### `class diffusers.utils.PushToHubMixin`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/utils/hub_utils.py#L351)

```py
( )
```

将模型、调度器或流水线推送到 Hugging Face Hub 的 Mixin。

`push_to_hub`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/utils/hub_utils.py#L380)

```py
( repo_id: str commit_message: Optional = None private: Optional = None token: Optional = None create_pr: bool = False safe_serialization: bool = True variant: Optional = None )
```

参数

+   `repo_id` (`str`) — 您要推送模型、调度器或流水线文件的存储库名称。在推送到组织时，应包含您的组织名称。`repo_id`也可以是本地目录的路径。

+   `commit_message` (`str`, *optional*) — 推送时要提交的消息。默认为`"Upload {object}"`。

+   `private` (`bool`, *optional*) — 是否应将创建的存储库设置为私有。

+   `token` (`str`, *optional*) — 用作远程文件的 HTTP bearer 授权的令牌。在运行`huggingface-cli login`时生成的令牌（存储在`~/.huggingface`中）。

+   `create_pr` (`bool`, *optional*, 默认为`False`) — 是否创建一个带有上传文件的 PR 或直接提交。

+   `safe_serialization` (`bool`, *optional*, 默认为`True`) — 是否将模型权重转换为`safetensors`格式。

+   `variant` (`str`, *optional*) — 如果指定，权重将以`pytorch_model.<variant>.bin`格式保存。

将模型、调度器或流水线文件上传到🤗 Hugging Face Hub。

示例：

```py
from diffusers import UNet2DConditionModel

unet = UNet2DConditionModel.from_pretrained("stabilityai/stable-diffusion-2", subfolder="unet")

# Push the `unet` to your namespace with the name "my-finetuned-unet".
unet.push_to_hub("my-finetuned-unet")

# Push the `unet` to an organization with the name "my-finetuned-unet".
unet.push_to_hub("your-org/my-finetuned-unet")
```
