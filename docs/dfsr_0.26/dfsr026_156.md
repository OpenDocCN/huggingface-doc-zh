# 自注意引导

> 原始文本：[`huggingface.co/docs/diffusers/api/pipelines/self_attention_guidance`](https://huggingface.co/docs/diffusers/api/pipelines/self_attention_guidance)

[使用自注意引导改进扩散模型的样本质量](https://huggingface.co/papers/2210.00939)是由洪素松等人撰写的。

该论文的摘要是：

*去噪扩散模型（DDMs）因其出色的生成质量和多样性而受到关注。这一成功很大程度上归因于使用基于类别或文本条件的扩散引导方法，例如分类器和无分类器引导。在本文中，我们提出了一个超越传统引导方法的更全面的视角。从这个广义的视角出发，我们引入了新颖的无条件和无训练策略，以增强生成图像的质量。作为一个简单的解决方案，模糊引导提高了中间样本的适用性，使其具有精细的信息和结构，从而使扩散模型能够生成具有适度引导规模的更高质量样本。在此基础上，自注意引导（SAG）使用扩散模型的中间自注意力图来增强其稳定性和效果。具体而言，SAG 仅在每次迭代时模糊扩散模型关注的区域，并相应地引导它们。我们的实验结果表明，我们的 SAG 提高了各种扩散模型的性能，包括 ADM、IDDPM、稳定扩散和 DiT。此外，将 SAG 与传统引导方法结合使用会进一步改善。*

您可以在[项目页面](https://ku-cvlab.github.io/Self-Attention-Guidance)、[原始代码库](https://github.com/KU-CVLAB/Self-Attention-Guidance)上找到有关自注意引导的更多信息，并在[演示](https://huggingface.co/spaces/susunghong/Self-Attention-Guidance)或[笔记本](https://colab.research.google.com/github/SusungHong/Self-Attention-Guidance/blob/main/SAG_Stable.ipynb)中尝试它。

请务必查看调度器指南以了解如何探索调度器速度和质量之间的权衡，并查看跨管道重用组件部分，以了解如何有效地将相同组件加载到多个管道中。

## StableDiffusionSAGPipeline

### `class diffusers.StableDiffusionSAGPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_sag/pipeline_stable_diffusion_sag.py#L101)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: Optional = None requires_safety_checker: bool = True )
```

参数

+   `vae`（AutoencoderKL）- 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示形式。

+   `text_encoder`（[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)）- 冻结的文本编码器（clip-vit-large-patch14）。

+   `tokenizer`（[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)）- 用于对文本进行标记化的`CLIPTokenizer`。

+   `unet`（UNet2DConditionModel）- 用于去噪编码图像潜在表示的`UNet2DConditionModel`。

+   `scheduler`（SchedulerMixin）- 用于与`unet`结合使用以去噪编码图像潜在表示的调度器。可以是 DDIMScheduler、LMSDiscreteScheduler 或 PNDMScheduler 之一。

+   `safety_checker` (`StableDiffusionSafetyChecker`) — 估计生成的图像是否可能被视为具有冒犯性或有害的分类模块。有关模型潜在危害的更多详细信息，请参考[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)。

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) — 用于从生成的图像中提取特征的 `CLIPImageProcessor`；作为 `safety_checker` 的输入。

使用 Stable Diffusion 进行文本到图像生成的流水线。

该模型继承自 DiffusionPipeline。查看超类文档以获取所有流水线实现的通用方法（下载、保存、在特定设备上运行等）。

该流水线还继承了以下加载方法：

+   load_textual_inversion() 用于加载文本反转嵌入

+   load_ip_adapter() 用于加载 IP 适配器

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_sag/pipeline_stable_diffusion_sag.py#L536)

```py
( prompt: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 50 guidance_scale: float = 7.5 sag_scale: float = 0.75 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: Optional = 1 cross_attention_kwargs: Optional = None clip_skip: Optional = None ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *optional*) — 指导图像生成的提示或提示。如果未定义，则需要传递 `prompt_embeds`。

+   `height` (`int`, *optional*, 默认为 `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *optional*, 默认为 `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *optional*, 默认为 50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *optional*, 默认为 7.5) — 更高的引导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用引导比例。

+   `sag_scale` (`float`, *optional*, 默认为 0.75) — 选择在 [0, 1.0] 之间以获得更好的质量。

+   `negative_prompt` (`str` 或 `List[str]`, *optional*) — 指导图像生成中不包含的提示或提示。如果未定义，则需要传递 `negative_prompt_embeds`。在不使用引导时被忽略 (`guidance_scale < 1`)。

+   `num_images_per_prompt` (`int`, *optional*, 默认为 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *optional*, 默认为 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta (η)。仅适用于 DDIMScheduler，在其他调度程序中将被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，则通过使用提供的随机 `generator` 进行采样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从 `prompt` 输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，将从 `negative_prompt` 输入参数生成 `negative_prompt_embeds`。ip_adapter_image — (`PipelineImageInput`, *optional*): 与 IP 适配器一起使用的可选图像输入。

+   `output_type` (`str`, *optional*, defaults to `"pil"`) — 生成图像的输出格式。选择 `PIL.Image` 或 `np.array` 之间的格式。

+   `return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回 StableDiffusionPipelineOutput 而不是普通元组。

+   `callback` (`Callable`, *optional*) — 在推断过程中每隔 `callback_steps` 步调用一次的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *optional*, defaults to 1) — `callback` 函数被调用的频率。如果未指定，则在每一步都会调用回调函数。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将作为关键字参数传递给 [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) 中定义的 `AttentionProcessor`。

+   `clip_skip` (`int`, *optional*) — 在计算提示嵌入时要跳过的 CLIP 层的数量。值为 1 表示将使用预最终层的输出来计算提示嵌入。

返回

StableDiffusionPipelineOutput 或 `tuple`

如果 `return_dict` 为 `True`，将返回 StableDiffusionPipelineOutput，否则将返回一个元组，其中第一个元素是生成的图像列表，第二个元素是一个包含相应生成图像是否包含“不适宜工作”（nsfw）内容的布尔值列表。

用于生成的管道的调用函数。

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionSAGPipeline

>>> pipe = StableDiffusionSAGPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> image = pipe(prompt, sag_scale=0.75).images[0]
```

#### `disable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_sag/pipeline_stable_diffusion_sag.py#L173)

```py
( )
```

禁用切片 VAE 解码。如果之前启用了 `enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `enable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_sag/pipeline_stable_diffusion_sag.py#L165)

```py
( )
```

启用切片 VAE 解码。启用此选项时，VAE 将将输入张量分割成片段，以便在多个步骤中计算解码。这对于节省一些内存并允许更大的批量大小很有用。

#### `encode_prompt`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_sag/pipeline_stable_diffusion_sag.py#L214)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` or `List[str]`, *optional*) — 要编码的提示 device — (`torch.device`): torch 设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用无分类器指导。 

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 不用来指导图像生成的提示。如果未定义，则必须传递 `negative_prompt_embeds`。当不使用指导时（即如果 `guidance_scale` 小于 `1` 时），将被忽略。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，则将从 `prompt` 输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。

+   `lora_scale` (`float`, *可选*) — 如果加载了 LoRA 层，则将应用于文本编码器的所有 LoRA 层的 LoRA 比例。

+   `clip_skip` (`int`, *可选*) — 从 CLIP 中跳过的层数，用于计算提示嵌入。值为 1 意味着将使用前一层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

## StableDiffusionOutput

### `class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L10)

```py
( images: Union nsfw_content_detected: Optional )
```

参数

+   `images` (`List[PIL.Image.Image]` 或 `np.ndarray`) — 长度为`batch_size`的去噪 PIL 图像列表或形状为`(batch_size, height, width, num_channels)`的 NumPy 数组。

+   `nsfw_content_detected` (`List[bool]`) — 列表指示相应生成的图像是否包含“不安全内容”（nsfw），如果无法执行安全检查，则为`None`。

稳定扩散管道的输出类。
