# GLIGEN（基于语言的图像生成）

> 原始文本：[`huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/gligen`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/gligen)

GLIGEN 模型是由[威斯康星大学麦迪逊分校、哥伦比亚大学和微软](https://github.com/gligen/GLIGEN)的研究人员和工程师创建的。StableDiffusionGLIGENPipeline 和 StableDiffusionGLIGENTextImagePipeline 可以生成基于基础输入的逼真图像。除了文本和边界框外，如果输入图像，StableDiffusionGLIGENTextImagePipeline 可以在由边界框定义的区域插入文本描述的对象。否则，它将生成由标题/提示描述的图像，并在由边界框定义的区域插入文本描述的对象。它在 COCO2014D 和 COCO2014CD 数据集上进行了训练，该模型使用冻结的 CLIP ViT-L/14 文本编码器来根据基础输入对自身进行条件化。

来自[论文](https://huggingface.co/papers/2301.07093)的摘要是：

*大规模文本到图像扩散模型取得了惊人的进展。然而，现状是仅使用文本输入，这可能会影响可控性。在这项工作中，我们提出 GLIGEN，即基于语言的图像生成，这是一种新颖的方法，它在现有预训练的文本到图像扩散模型的基础上构建并扩展功能，使它们也能够根据基础输入进行条件化。为了保留预训练模型的广泛概念知识，我们冻结了所有权重，并通过门控机制将基础信息注入到新的可训练层中。我们的模型实现了具有标题和边界框条件输入的开放世界基于基础的文本到图像生成，而且基础能力很好地推广到新颖的空间配置和概念。GLIGEN 在 COCO 和 LVIS 上的零样本性能远远超过现有的监督布局到图像基线。*

请务必查看稳定扩散[Tips](https://huggingface.co/docs/diffusers/en/api/pipelines/stable_diffusion/overview#tips)部分，了解如何探索调度程序速度和质量之间的权衡，以及如何高效地重用管道组件！

如果您想要使用任务的官方检查点之一，请探索[gligen](https://huggingface.co/gligen) Hub 组织！

StableDiffusionGLIGENPipeline 由[Nikhil Gajendrakumar](https://github.com/nikhil-masterful)贡献，而 StableDiffusionGLIGENTextImagePipeline 由[Nguyễn Công Tú Anh](https://github.com/tuanh123789)贡献。

## 稳定扩散 GLIGEN 管道

### `class diffusers.StableDiffusionGLIGENPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py#L102)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPFeatureExtractor requires_safety_checker: bool = True )
```

参数

+   `vae`（AutoencoderKL) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 用于对文本进行标记化的 `CLIPTokenizer`。

+   `unet` (UNet2DConditionModel) — 用于去噪编码图像潜在特征的 `UNet2DConditionModel`。

+   `scheduler` (SchedulerMixin) — 用于与 `unet` 结合使用以去噪编码图像潜在特征的调度器。可以是 DDIMScheduler、LMSDiscreteScheduler 或 PNDMScheduler 中的一个。

+   `safety_checker` (`StableDiffusionSafetyChecker`) — 用于估计生成图像是否可能被视为具有攻击性或有害的分类模块。请参考[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) — 用于从生成的图像中提取特征的 `CLIPImageProcessor`；作为输入传递给 `safety_checker`。

使用 Stable Diffusion 和 Grounded-Language-to-Image Generation (GLIGEN) 进行文本到图像生成的管道。

此模型继承自 DiffusionPipeline。检查超类文档以了解库为所有管道实现的通用方法（例如下载或保存，运行在特定设备上等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py#L551)

```py
( prompt: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 50 guidance_scale: float = 7.5 gligen_scheduled_sampling_beta: float = 0.3 gligen_phrases: List = None gligen_boxes: List = None gligen_inpaint_image: Optional = None negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None clip_skip: Optional = None ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 用于引导图像生成的提示或提示。如果未定义，则需要传递 `prompt_embeds`。

+   `height` (`int`, *可选*, 默认为 `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为 `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *可选*, 默认为 50) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *可选*, 默认为 7.5) — 更高的引导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用引导比例。

+   `gligen_phrases` (`List[str]`) — 用于指导每个由相应的 `gligen_boxes` 定义的区域中包含什么内容的短语。每个边界框只能有一个短语。

+   `gligen_boxes` (`List[List[float]]`) — 用于标识将填充由相应的 `gligen_phrases` 描述的内容的图像的矩形区域的边界框。每个矩形框被定义为包含 4 个元素 `[xmin, ymin, xmax, ymax]` 的 `List[float]`，其中每个值都在 [0,1] 之间。

+   `gligen_inpaint_image` (`PIL.Image.Image`, *可选*) — 如果提供了输入图像，则将其用由 `gligen_boxes` 和 `gligen_phrases` 描述的对象修复。否则，它将被视为在空白输入图像上的生成任务。

+   `gligen_scheduled_sampling_beta` (`float`，默认为 0.3) — 来自[GLIGEN: Open-Set Grounded Text-to-Image Generation](https://arxiv.org/pdf/2301.07093.pdf)的 Scheduled Sampling 因子。Scheduled Sampling 因子仅在推断过程中为了提高质量和可控性而变化。

+   `negative_prompt` (`str`或`List[str]`，*可选*) — 用于指导图像生成中不包含的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用指导时被忽略（`guidance_scale < 1`）。

+   `num_images_per_prompt` (`int`，*可选*，默认为 1) — 每个提示生成的图像数量。

+   `eta` (`float`，*可选*，默认为 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta (η)。仅适用于 DDIMScheduler，在其他调度器中被忽略。

+   `generator` (`torch.Generator`或`List[torch.Generator]`，*可选*) — 一个[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`，*可选*) — 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`，*可选*) — 预生成的文本嵌入。可用于轻松微调文本输入（提示加权）。如果未提供，则从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`，*可选*) — 预生成的负文本嵌入。可用于轻松微调文本输入（提示加权）。如果未提供，则从`negative_prompt`输入参数生成`negative_prompt_embeds`。

+   `output_type` (`str`，*可选*，默认为`"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间。

+   `return_dict` (`bool`，*可选*，默认为`True`) — 是否返回一个 StableDiffusionPipelineOutput 而不是一个普通元组。

+   `callback` (`Callable`，*可选*) — 在推断过程中每隔`callback_steps`步调用的函数。该函数使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`，*可选*，默认为 1) — 调用`callback`函数的频率。如果未指定，将在每一步调用回调。

+   `cross_attention_kwargs` (`dict`，*可选*) — 如果指定，将传递给`AttentionProcessor`的 kwargs 字典，如[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的那样。

+   `guidance_rescale` (`float`，*可选*，默认为 0.0) — 来自[Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf)的指导缩放因子。指导缩放因子应在使用零终端信噪比时修复过曝光问题。

+   `clip_skip` (`int`，*可选*) — 在计算提示嵌入时要跳过的 CLIP 层数。值为 1 意味着将使用预终层的输出来计算提示嵌入。

返回

StableDiffusionPipelineOutput 或`tuple`

如果`return_dict`为`True`，则返回 StableDiffusionPipelineOutput，否则返回一个`tuple`，其中第一个元素是生成的图像列表，第二个元素是一个`bool`列表，指示相应生成的图像是否包含“不适宜工作”（nsfw）内容。

用于生成的管道的调用函数。

示例:

```py
>>> import torch
>>> from diffusers import StableDiffusionGLIGENPipeline
>>> from diffusers.utils import load_image

>>> # Insert objects described by text at the region defined by bounding boxes
>>> pipe = StableDiffusionGLIGENPipeline.from_pretrained(
...     "masterful/gligen-1-4-inpainting-text-box", variant="fp16", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> input_image = load_image(
...     "https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/gligen/livingroom_modern.png"
... )
>>> prompt = "a birthday cake"
>>> boxes = [[0.2676, 0.6088, 0.4773, 0.7183]]
>>> phrases = ["a birthday cake"]

>>> images = pipe(
...     prompt=prompt,
...     gligen_phrases=phrases,
...     gligen_inpaint_image=input_image,
...     gligen_boxes=boxes,
...     gligen_scheduled_sampling_beta=1,
...     output_type="pil",
...     num_inference_steps=50,
... ).images

>>> images[0].save("./gligen-1-4-inpainting-text-box.jpg")

>>> # Generate an image described by the prompt and
>>> # insert objects described by text at the region defined by bounding boxes
>>> pipe = StableDiffusionGLIGENPipeline.from_pretrained(
...     "masterful/gligen-1-4-generation-text-box", variant="fp16", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> prompt = "a waterfall and a modern high speed train running through the tunnel in a beautiful forest with fall foliage"
>>> boxes = [[0.1387, 0.2051, 0.4277, 0.7090], [0.4980, 0.4355, 0.8516, 0.7266]]
>>> phrases = ["a waterfall", "a modern high speed train running through the tunnel"]

>>> images = pipe(
...     prompt=prompt,
...     gligen_phrases=phrases,
...     gligen_boxes=boxes,
...     gligen_scheduled_sampling_beta=1,
...     output_type="pil",
...     num_inference_steps=50,
... ).images

>>> images[0].save("./gligen-1-4-generation-text-box.jpg")
```

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py#L175)

```py
( )
```

启用切片 VAE 解码。启用此选项时，VAE 将在几个步骤中将输入张量分割为切片以进行解码。这对于节省一些内存并允许更大的批量大小非常有用。

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py#L182)

```py
( )
```

禁用切片 VAE 解码。如果先前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py#L189)

```py
( )
```

启用切片 VAE 解码。启用此选项时，VAE 将将输入张量分割为瓦片以在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

#### `disable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py#L197)

```py
( )
```

禁用切片 VAE 解码。如果先前启用了`enable_vae_tiling`，则此方法将返回到一步计算解码。

#### `enable_model_cpu_offload`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L1410)

```py
( gpu_id: Optional = None device: Union = 'cuda' )
```

参数

+   `gpu_id` (`int`, *可选*) — 用于推断的加速器的 ID。如果未指定，将默认为 0。

+   `device` (`torch.Device` 或 `str`, *可选*, 默认为“cuda”) — 用于推断的加速器的 PyTorch 设备类型。如果未指定，将默认为“cuda”。

使用加速器将所有模型转移到 CPU，减少内存使用量，对性能影响较小。与`enable_sequential_cpu_offload`相比，此方法在调用其`forward`方法时一次将一个完整模型移动到 GPU，并且模型保持在 GPU 中直到下一个模型运行。与`enable_sequential_cpu_offload`相比，内存节省较低，但由于`unet`的迭代执行，性能要好得多。

#### `prepare_latents`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py#L507)

```py
( batch_size num_channels_latents height width dtype device generator latents = None )
```

#### `enable_fuser`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py#L524)

```py
( enabled = True )
```

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py#L238)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示 device — (`torch.device`): torch 设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用分类器自由指导

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即，如果`guidance_scale`小于`1`，则忽略）。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成 negative_prompt_embeds。

+   `lora_scale` (`float`, *可选*) — 如果加载了 LoRA 层，则将应用于文本编码器的所有 LoRA 层的 LoRA 比例。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要从 CLIP 中跳过的层数。值为 1 意味着将使用预终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

## StableDiffusionGLIGENTextImagePipeline

### `class diffusers.StableDiffusionGLIGENTextImagePipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py#L148)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer processor: CLIPProcessor image_encoder: CLIPVisionModelWithProjection image_project: CLIPImageProjection unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPFeatureExtractor requires_safety_checker: bool = True )
```

参数

+   `vae` (AutoencoderKL) — 变分自动编码器（VAE）模型，用于对图像进行编码和解码，并从潜在表示中解码图像。

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 用于对文本进行标记化的`CLIPTokenizer`。

+   `processor` ([CLIPProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPProcessor)) — 用于处理参考图像的`CLIPProcessor`。

+   `image_encoder` ([CLIPVisionModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection)) — 冻结的图像编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `image_project` (`CLIPImageProjection`) — 用于将图像嵌入投影到短语嵌入空间的`CLIPImageProjection`。

+   `unet` (UNet2DConditionModel) — 用于去噪编码图像潜在的`UNet2DConditionModel`。

+   `scheduler` (SchedulerMixin) — 用于与`unet`结合使用以去噪编码图像潜在的调度器。可以是 DDIMScheduler、LMSDiscreteScheduler 或 PNDMScheduler 之一。

+   `safety_checker` (`StableDiffusionSafetyChecker`) — 估计生成的图像是否可能被认为是冒犯性或有害的分类模块。请参考[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) — 用于从生成的图像中提取特征的`CLIPImageProcessor`；作为`safety_checker`的输入。

使用稳定扩散和基于语言到图像生成（GLIGEN）的文本到图像生成管道。

该模型继承自 DiffusionPipeline。检查超类文档以获取库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py#L712)

```py
( prompt: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 50 guidance_scale: float = 7.5 gligen_scheduled_sampling_beta: float = 0.3 gligen_phrases: List = None gligen_images: List = None input_phrases_mask: Union = None input_images_mask: Union = None gligen_boxes: List = None gligen_inpaint_image: Optional = None negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None gligen_normalize_constant: float = 28.7 clip_skip: int = None ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *optional*) — 指导图像生成的提示或提示。如果未定义，则需要传递 `prompt_embeds`。

+   `height` (`int`, *optional*, 默认为 `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *optional*, 默认为 `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *optional*, 默认为 50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *optional*, 默认为 7.5) — 更高的指导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用指导比例。

+   `gligen_phrases` (`List[str]`) — 指导每个由相应的 `gligen_boxes` 定义的区域中包含什么内容的短语。每个边界框只应有一个短语。

+   `gligen_images` (`List[PIL.Image.Image]`) — 指导每个由相应的 `gligen_boxes` 定义的区域中包含什么内容的图像。每个边界框只应有一个图像

+   `input_phrases_mask` (`int` 或 `List[int]`) — 由相应的 `input_phrases_mask` 定义的预短语掩码输入

+   `input_images_mask` (`int` 或 `List[int]`) — 由相应的 `input_images_mask` 定义的预图像掩码输入

+   `gligen_boxes` (`List[List[float]]`) — 标识图像矩形区域的边界框，这些区域将填充由相应的 `gligen_phrases` 描述的内容。每个矩形框被定义为包含 4 个元素 `[xmin, ymin, xmax, ymax]` 的 `List[float]`，其中每个值在 [0,1] 之间。

+   `gligen_inpaint_image` (`PIL.Image.Image`, *optional*) — 如果提供了输入图像，则将其用由 `gligen_boxes` 和 `gligen_phrases` 描述的对象修补。否则，它被视为在空白输入图像上的生成任务。

+   `gligen_scheduled_sampling_beta` (`float`, 默认为 0.3) — 来自 [GLIGEN: 开放集文本到图像生成](https://arxiv.org/pdf/2301.07093.pdf) 的计划抽样因子。计划抽样因子仅用于推理期间的计划抽样，以提高质量和可控性。

+   `negative_prompt` (`str` 或 `List[str]`, *optional*) — 指导图像生成中不包含的提示或提示。如果未定义，则需要传递 `negative_prompt_embeds`。在不使用指导时（`guidance_scale < 1`）将被忽略。

+   `num_images_per_prompt` (`int`, *optional*, 默认为 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *optional*, 默认为 0.0) — 对应于 [DDIM](https://arxiv.org/abs/2010.02502) 论文中的参数 eta (η)。仅适用于 DDIMScheduler，在其他调度程序中将被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 用于使生成具有确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中抽样的预生成噪声潜变量，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，则通过使用提供的随机 `generator` 进行抽样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从 `prompt` 输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的一个。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回 StableDiffusionPipelineOutput 而不是普通的 tuple。

+   `callback` (`Callable`, *可选*) — 在推断期间每`callback_steps`步调用的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *可选*, 默认为 1) — 调用`callback`函数的频率。如果未指定，将在每一步调用回调。

+   `cross_attention_kwargs` (`dict`, *可选*) — 如果指定了此 kwargs 字典，则将其传递给`AttentionProcessor`，如[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的那样。

+   `gligen_normalize_constant` (`float`, *可选*, 默认为 28.7) — 图像嵌入的归一化值。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时，要从 CLIP 跳过的层数。值为 1 意味着将使用预终层的输出来计算提示嵌入。

返回

StableDiffusionPipelineOutput 或 `tuple`

如果`return_dict`为`True`，将返回 StableDiffusionPipelineOutput，否则将返回一个`tuple`，其中第一个元素是生成的图像列表，第二个元素是一个包含相应生成图像是否包含“不适宜工作”（nsfw）内容的`bool`列表。

用于生成的管道的调用函数。

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionGLIGENTextImagePipeline
>>> from diffusers.utils import load_image

>>> # Insert objects described by image at the region defined by bounding boxes
>>> pipe = StableDiffusionGLIGENTextImagePipeline.from_pretrained(
...     "anhnct/Gligen_Inpainting_Text_Image", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> input_image = load_image(
...     "https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/gligen/livingroom_modern.png"
... )
>>> prompt = "a backpack"
>>> boxes = [[0.2676, 0.4088, 0.4773, 0.7183]]
>>> phrases = None
>>> gligen_image = load_image(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/gligen/backpack.jpeg"
... )

>>> images = pipe(
...     prompt=prompt,
...     gligen_phrases=phrases,
...     gligen_inpaint_image=input_image,
...     gligen_boxes=boxes,
...     gligen_images=[gligen_image],
...     gligen_scheduled_sampling_beta=1,
...     output_type="pil",
...     num_inference_steps=50,
... ).images

>>> images[0].save("./gligen-inpainting-text-image-box.jpg")

>>> # Generate an image described by the prompt and
>>> # insert objects described by text and image at the region defined by bounding boxes
>>> pipe = StableDiffusionGLIGENTextImagePipeline.from_pretrained(
...     "anhnct/Gligen_Text_Image", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> prompt = "a flower sitting on the beach"
>>> boxes = [[0.0, 0.09, 0.53, 0.76]]
>>> phrases = ["flower"]
>>> gligen_image = load_image(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/gligen/pexels-pixabay-60597.jpg"
... )

>>> images = pipe(
...     prompt=prompt,
...     gligen_phrases=phrases,
...     gligen_images=[gligen_image],
...     gligen_boxes=boxes,
...     gligen_scheduled_sampling_beta=1,
...     output_type="pil",
...     num_inference_steps=50,
... ).images

>>> images[0].save("./gligen-generation-text-image-box.jpg")

>>> # Generate an image described by the prompt and
>>> # transfer style described by image at the region defined by bounding boxes
>>> pipe = StableDiffusionGLIGENTextImagePipeline.from_pretrained(
...     "anhnct/Gligen_Text_Image", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> prompt = "a dragon flying on the sky"
>>> boxes = [[0.4, 0.2, 1.0, 0.8], [0.0, 1.0, 0.0, 1.0]]  # Set `[0.0, 1.0, 0.0, 1.0]` for the style

>>> gligen_image = load_image(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png"
... )

>>> gligen_placeholder = load_image(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png"
... )

>>> images = pipe(
...     prompt=prompt,
...     gligen_phrases=[
...         "dragon",
...         "placeholder",
...     ],  # Can use any text instead of `placeholder` token, because we will use mask here
...     gligen_images=[
...         gligen_placeholder,
...         gligen_image,
...     ],  # Can use any image in gligen_placeholder, because we will use mask here
...     input_phrases_mask=[1, 0],  # Set 0 for the placeholder token
...     input_images_mask=[0, 1],  # Set 0 for the placeholder image
...     gligen_boxes=boxes,
...     gligen_scheduled_sampling_beta=1,
...     output_type="pil",
...     num_inference_steps=50,
... ).images

>>> images[0].save("./gligen-generation-text-image-box-style-transfer.jpg")
```

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py#L233)

```py
( )
```

启用切片 VAE 解码。启用此选项时，VAE 将在几个步骤中分割输入张量以计算解码。这对于节省一些内存并允许更大的批量大小很有用。

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py#L240)

```py
( )
```

禁用切片 VAE 解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到在一个步骤中计算解码。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py#L247)

```py
( )
```

启用平铺 VAE 解码。启用此选项时，VAE 将将输入张量分割成瓦片以在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像很有用。

#### `disable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py#L255)

```py
( )
```

禁用平铺 VAE 解码。如果之前启用了`enable_vae_tiling`，则此方法将返回到在一个步骤中计算解码。

#### `enable_model_cpu_offload`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L1410)

```py
( gpu_id: Optional = None device: Union = 'cuda' )
```

参数

+   `gpu_id`（`int`，*可选*）— 应在推理中使用的加速器的 ID。如果未指定，将默认为 0。

+   `device`（`torch.Device`或`str`，*可选*，默认为“cuda”）— 应在推理中使用的加速器的 PyTorch 设备类型。如果未指定，将默认为“cuda”。

使用加速器将所有模型转移到 CPU，减少内存使用量，对性能影响较小。与`enable_sequential_cpu_offload`相比，此方法在调用其`forward`方法时将一个完整的模型一次性移动到 GPU，并且模型保持在 GPU 中，直到下一个模型运行。与`enable_sequential_cpu_offload`相比，内存节省较少，但由于`unet`的迭代执行，性能要好得多。

#### `prepare_latents`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py#L531)

```py
( batch_size num_channels_latents height width dtype device generator latents = None )
```

#### `enable_fuser`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py#L548)

```py
( enabled = True )
```

#### `complete_mask`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py#L585)

```py
( has_mask max_objs device )
```

根据输入遮罩对应的值`0 或 1`，为每个短语和图像，遮罩与短语和图像对应的特征。

#### `crop`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py#L565)

```py
( im new_width new_height )
```

将输入图像裁剪到指定的尺寸。

#### `draw_inpaint_mask_from_boxes`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py#L553)

```py
( boxes size )
```

基于给定的框创建一个修复遮罩。此函数使用提供的框生成一个修复遮罩，以标记需要修复的区域。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py#L263)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）— 要编码的提示设备 —（`torch.device`）：torch 设备

+   `num_images_per_prompt`（`int`）— 每个提示应生成的图像数量

+   `do_classifier_free_guidance`（`bool`）— 是否使用无分类器指导

+   `negative_prompt`（`str`或`List[str]`，*可选*）— 不指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即，如果`guidance_scale`小于`1`，则忽略）。

+   `prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。

+   `negative_prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成负文本嵌入。

+   `lora_scale`（`float`，*可选*）— 如果加载了 LoRA 层，则将应用于文本编码器的所有 LoRA 层的 LoRA 比例。

+   `clip_skip`（`int`，*可选*）— 在计算提示嵌入时，要从 CLIP 中跳过的层数。值为 1 意味着将使用前一层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

#### `get_clip_feature`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py#L601)

```py
( input normalize_constant device is_image = False )
```

通过使用 CLIP 预训练模型获取图像和短语嵌入。通过投影，将图像嵌入转换为短语嵌入空间。

#### `get_cross_attention_kwargs_with_grounded`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py#L625)

```py
( hidden_size gligen_phrases gligen_images gligen_boxes input_phrases_mask input_images_mask repeat_batch normalize_constant max_objs device )
```

准备包含有关基础输入（框、蒙版、图像嵌入、短语嵌入）信息的交叉注意力 kwargs。

#### `get_cross_attention_kwargs_without_grounded`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py#L689)

```py
( hidden_size repeat_batch max_objs device )
```

准备不包含有关基础输入（框、蒙版、图像嵌入、短语嵌入）信息的交叉注意力 kwargs（全部为零张量）。

#### `target_size_center_crop`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py#L576)

```py
( im new_hw )
```

将图像裁剪并调整大小到目标尺寸，同时保持中心位置。

## StableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L10)

```py
( images: Union nsfw_content_detected: Optional )
```

参数

+   `images` (`List[PIL.Image.Image]` 或 `np.ndarray`) — 长度为 `batch_size` 的去噪 PIL 图像列表或形状为 `(batch_size, height, width, num_channels)` 的 NumPy 数组。

+   `nsfw_content_detected` (`List[bool]`) — 列表指示相应生成的图像是否包含“不安全内容”（nsfw），如果无法执行安全检查，则为 `None`。

稳定扩散管道的输出类。
