# 潜一致性模型

> 原始文本：[`huggingface.co/docs/diffusers/api/pipelines/latent_consistency_models`](https://huggingface.co/docs/diffusers/api/pipelines/latent_consistency_models)

潜一致性模型（LCMs）由 Simian Luo、Yiqin Tan、Longbo Huang、Jian Li 和 Hang Zhao 在[潜一致性模型：通过少步推断合成高分辨率图像](https://huggingface.co/papers/2310.04378)中提出。

该论文的摘要如下：

*潜扩散模型（LDMs）在合成高分辨率图像方面取得了显著的成果。然而，迭代采样过程计算密集且导致生成速度缓慢。受一致性模型（song 等人）的启发，我们提出了潜一致性模型（LCMs），在任何预训练的 LDM 上实现快速推断，步骤最少，包括稳定扩散（rombach 等人）。将引导的逆扩散过程视为解决潜空间中的增广概率流 ODE（PF-ODE），LCMs 旨在直接预测此类 ODE 的解，减轻了对大量迭代的需求，实现了快速、高保真度的采样。从预训练的无分类器引导扩散模型中高效提取，高质量的 768 x 768 2~4 步 LCM 仅需 32 个 A100 GPU 小时进行训练。此外，我们引入了潜一致性微调（LCF），这是一种专为在定制图像数据集上微调 LCMs 的新方法。对 LAION-5B-Aesthetics 数据集的评估表明，LCMs 在少步推断下实现了最先进的文本到图像生成性能。项目页面：[此处的 URL](https://latent-consistency-models.github.io/)。*

可以在[此处](https://huggingface.co/spaces/SimianLuo/Latent_Consistency_Model)找到[SimianLuo/LCM_Dreamshaper_v7](https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7)检查点的演示。

这些管道由[luosiallen](https://luosiallen.github.io/)、[nagolinc](https://github.com/nagolinc)和[dg845](https://github.com/dg845)贡献。

## LatentConsistencyModelPipeline

### `class diffusers.LatentConsistencyModelPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_text2img.py#L109)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: LCMScheduler safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: Optional = None requires_safety_checker: bool = True )
```

参数

+   `vae`（AutoencoderKL）- 变分自动编码器（VAE）模型，用于对图像进行编码和解码以及从潜在表示到图像的解码。

+   `text_encoder`（[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)）- 冻结的文本编码器（clip-vit-large-patch14）。

+   `tokenizer`（[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)）- 用于对文本进行标记化的`CLIPTokenizer`。

+   `unet`（UNet2DConditionModel）- 用于去噪编码图像潜变量的`UNet2DConditionModel`。

+   `scheduler`（SchedulerMixin）- 与`unet`结合使用以去噪编码图像潜变量的调度程序。目前仅支持 LCMScheduler。

+   `safety_checker`（`StableDiffusionSafetyChecker`）- 估计生成的图像是否可能被视为具有冒犯性或有害性的分类模块。有关模型潜在危害的更多详细信息，请参阅[模型卡片](https://huggingface.co/runwayml/stable-diffusion-v1-5)。

+   `feature_extractor`（[CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)）- 用于从生成的图像中提取特征的`CLIPImageProcessor`；作为输入用于`safety_checker`。

+   `requires_safety_checker` (`bool`, *optional*, 默认值为`True`) — 流水线是否需要安全检查器组件。

使用潜在一致性模型进行文本到图像生成的流水线。

该模型继承自 DiffusionPipeline。查看超类文档以获取所有流水线实现的通用方法（下载、保存、在特定设备上运行等）。

该流水线还继承了以下加载方法：

+   load_textual_inversion() 用于加载文本反演嵌入

+   load_lora_weights() 用于加载 LoRA 权重

+   save_lora_weights() 用于保存 LoRA 权重

+   from_single_file() 用于加载`.ckpt`文件

+   load_ip_adapter() 用于加载 IP 适配器

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_text2img.py#L597)

```py
( prompt: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 4 original_inference_steps: int = None timesteps: List = None guidance_scale: float = 8.5 num_images_per_prompt: Optional = 1 generator: Union = None latents: Optional = None prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *optional*) — 用于引导图像生成的提示。如果未定义，则需要传递`prompt_embeds`。

+   `height` (`int`, *optional*, 默认值为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *optional*, 默认值为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *optional*, 默认值为 50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `original_inference_steps` (`int`, *optional*) — 用于生成线性间隔时间步骤计划的原始推理步骤数，我们将从中绘制`num_inference_steps`等间隔时间步骤作为我们的最终时间步骤计划，遵循论文中的跳步方法（参见第 4.3 节）。如果未设置，将默认为调度器的`original_inference_steps`属性。

+   `timesteps` (`List[int]`, *optional*) — 用于降噪过程的自定义时间步骤。如果未定义，则使用原始 LCM 训练/蒸馏时间步骤计划上等间隔的`num_inference_steps`时间步骤。必须按降序排列。

+   `guidance_scale` (`float`, *optional*, 默认值为 7.5) — 较高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。请注意，原始的潜在一致性模型论文使用了不同的 CFG 公式，其中引导比例减少了 1（因此在论文公式中，当`guidance_scale > 0`时启用 CFG）。

+   `num_images_per_prompt` (`int`, *optional*, 默认值为 1) — 每个提示生成的图像数量。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中采样的预生成噪声潜在变量，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，将使用提供的随机`generator`生成潜在变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，文本嵌入将从`prompt`输入参数生成。ip_adapter_image — (`PipelineImageInput`, *optional*): 可选的图像输入，用于与 IP 适配器一起使用。

+   `output_type` (`str`, *optional*, 默认为`"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的一个。

+   `return_dict` (`bool`, *optional*, 默认为`True`) — 是否返回 StableDiffusionPipelineOutput 而不是普通的`tuple`。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将作为`AttentionProcessor`中的参数传递给[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)。

+   `clip_skip` (`int`, *optional*) — 在计算提示嵌入时要跳过的层数。值为 1 表示将使用预最终层的输出来计算提示嵌入。

+   `callback_on_step_end` (`Callable`, *optional*) — 在推断过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) — `callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在流水线类的`._callback_tensor_inputs`属性中列出的变量。

返回

StableDiffusionPipelineOutput 或`tuple`

如果`return_dict`为`True`，将返回 StableDiffusionPipelineOutput，否则将返回一个`tuple`，其中第一个元素是生成的图像列表，第二个元素是一个包含指示相应生成图像是否包含“不适宜工作”（nsfw）内容的`bool`列表。

用于生成的流水线的调用函数。

示例：

```py
>>> from diffusers import DiffusionPipeline
>>> import torch

>>> pipe = DiffusionPipeline.from_pretrained("SimianLuo/LCM_Dreamshaper_v7")
>>> # To save GPU memory, torch.float16 can be used, but it may compromise image quality.
>>> pipe.to(torch_device="cuda", torch_dtype=torch.float32)

>>> prompt = "Self-portrait oil painting, a beautiful cyborg with golden hair, 8k"

>>> # Can be set to 1~50 steps. LCM support fast inference even <= 4 steps. Recommend: 1~8 steps.
>>> num_inference_steps = 4
>>> images = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=8.0).images
>>> images[0].save("image.png")
```

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_text2img.py#L230)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1` (`float`) — 第 1 阶段的缩放因子，用于减弱跳跃特征的贡献。这样做是为了在增强去噪过程中减轻“过度平滑效应”。

+   `s2` (`float`) — 第 2 阶段的缩放因子，用于减弱跳跃特征的贡献。这样做是为了在增强去噪过程中减轻“过度平滑效应”。

+   `b1` (`float`) — 第 1 阶段的缩放因子，用于放大骨干特征的贡献。

+   `b2` (`float`) — 第 2 阶段的缩放因子，用于放大骨干特征的贡献。

启用 FreeU 机制，如[`arxiv.org/abs/2309.11497`](https://arxiv.org/abs/2309.11497)。

缩放因子后缀表示应用它们的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同流水线（如 Stable Diffusion v1、v2 和 Stable Diffusion XL）的值组合。

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_text2img.py#L253)

```py
( )
```

禁用了启用的 FreeU 机制。

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_text2img.py#L197)

```py
( )
```

启用切片 VAE 解码。当启用此选项时，VAE 将将输入张量分成片段以在多个步骤中计算解码。这对于节省一些内存并允许更大的批量大小很有用。

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_text2img.py#L205)

```py
( )
```

禁用切片的 VAE 解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_text2img.py#L213)

```py
( )
```

启用平铺的 VAE 解码。当启用此选项时，VAE 将将输入张量分成瓦片以在多个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像很有用。

#### `disable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_text2img.py#L222)

```py
( )
```

禁用平铺的 VAE 解码。如果之前启用了`enable_vae_tiling`，则此方法将返回到一步计算解码。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_text2img.py#L258)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示设备 — (`torch.device`): torch 设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用分类器自由指导

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用于指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即，如果`guidance_scale`小于`1`，则忽略）。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，*例如* 提示加权。如果未提供，文本嵌入将从 `prompt` 输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，*例如* 提示加权。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。

+   `lora_scale` (`float`, *可选*) — 如果加载了 LoRA 层，则将应用于文本编码器的所有 LoRA 层的 LoRA 比例。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要跳过的层数。值为 1 意味着将使用预最终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

#### `get_guidance_scale_embedding`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_text2img.py#L497)

```py
( w embedding_dim = 512 dtype = torch.float32 ) → export const metadata = 'undefined';torch.FloatTensor
```

参数

+   `timesteps` (`torch.Tensor`) — 在这些时间步生成嵌入向量

+   `embedding_dim` (`int`, *可选*, 默认为 512) — 要生成的嵌入的维度 dtype — 生成的嵌入的数据类型

返回

`torch.FloatTensor`

形状为 `(len(timesteps), embedding_dim)` 的嵌入向量

参见 [`github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298`](https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298)

## LatentConsistencyModelImg2ImgPipeline

### `class diffusers.LatentConsistencyModelImg2ImgPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_img2img.py#L131)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: LCMScheduler safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: Optional = None requires_safety_checker: bool = True )
```

参数

+   `vae` (AutoencoderKL) — 变分自动编码器（VAE）模型，用于对图像进行编码和解码，并转换为潜在表示。

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)) — 冻结的文本编码器（clip-vit-large-patch14）。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 用于对文本进行标记化的 `CLIPTokenizer`。

+   `unet` (UNet2DConditionModel) — 用于降噪编码图像潜在的 `UNet2DConditionModel`。

+   `scheduler` (SchedulerMixin) — 与 `unet` 结合使用的调度器，用于降噪编码图像潜在。目前仅支持 LCMScheduler。

+   `safety_checker` (`StableDiffusionSafetyChecker`) — 用于估计生成的图像是否可能被视为具有冒犯性或有害的分类模块。请参考[模型卡片](https://huggingface.co/runwayml/stable-diffusion-v1-5)以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) — 用于从生成的图像中提取特征的 `CLIPImageProcessor`；作为 `safety_checker` 的输入。

+   `requires_safety_checker` (`bool`, *可选*, 默认为 `True`) — 管道是否需要安全检查器组件。

使用潜在一致性模型进行图像生成的管道。

该模型继承自 DiffusionPipeline。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

该管道还继承了以下加载方法：

+   load_textual_inversion() 用于加载文本反演嵌入

+   load_lora_weights() 用于加载 LoRA 权重

+   save_lora_weights() 用于保存 LoRA 权重

+   from_single_file() 用于加载 `.ckpt` 文件

+   load_ip_adapter() 用于加载 IP 适配器

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_img2img.py#L692)

```py
( prompt: Union = None image: Union = None num_inference_steps: int = 4 strength: float = 0.8 original_inference_steps: int = None timesteps: List = None guidance_scale: float = 8.5 num_images_per_prompt: Optional = 1 generator: Union = None latents: Optional = None prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 用于指导图像生成的提示或提示。如果未定义，则需要传递 `prompt_embeds`。

+   `height` (`int`, *可选*, 默认为 `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为 `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *可选*, 默认为 50) — 降噪步骤的数量。更多的降噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `original_inference_steps` (`int`, *optional*) — 用于生成线性间隔时间步长计划的原始推断步数，从中我们将从中抽取 `num_inference_steps` 等间隔时间步长作为我们的最终时间步长计划，遵循论文中的 Skipping-Step 方法（参见第 4.3 节）。如果未设置，这将默认为调度程序的 `original_inference_steps` 属性。

+   `timesteps` (`List[int]`, *optional*) — 用于去噪过程的自定义时间步长。如果未定义，则使用原始 LCM 训练/蒸馏时间步长计划上等间隔的 `num_inference_steps` 时间步长。必须按降序排列。

+   `guidance_scale` (`float`, *optional*, defaults to 7.5) — 更高的引导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用引导比例。请注意，原始潜变量一致性模型论文使用不同的 CFG 公式，其中引导比例减少 1（因此在论文公式中，当 `guidance_scale > 0` 时启用 CFG）。

+   `num_images_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的图像数量。

+   `generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，则通过使用提供的随机 `generator` 进行采样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松微调文本输入（提示加权）。如果未提供，则从 `prompt` 输入参数生成文本嵌入。ip_adapter_image — (`PipelineImageInput`, *optional*): 与 IP 适配器一起使用的可选图像输入。

+   `output_type` (`str`, *optional*, defaults to `"pil"`) — 生成图像的输出格式。选择 `PIL.Image` 或 `np.array` 之间。

+   `return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回 StableDiffusionPipelineOutput 而不是普通元组。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定，则将此 kwargs 字典传递给 [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) 中定义的 `AttentionProcessor`。

+   `clip_skip` (`int`, *optional*) — 在计算提示嵌入时要从 CLIP 中跳过的层数。值为 1 表示将使用预终层的输出来计算提示嵌入。

+   `callback_on_step_end` (`Callable`, *optional*) — 在推断期间每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs` 将包括由 `callback_on_step_end_tensor_inputs` 指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) — `callback_on_step_end` 函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包含在您的管道类的 `._callback_tensor_inputs` 属性中列出的变量。

返回

StableDiffusionPipelineOutput 或 `tuple`

如果`return_dict`为`True`，则返回 StableDiffusionPipelineOutput，否则返回一个`tuple`，其中第一个元素是包含生成图像的列表，第二个元素是一个包含“不适宜工作”（nsfw）内容的`bool`列表。

用于生成的管道的调用函数。

示例：

```py
>>> from diffusers import AutoPipelineForImage2Image
>>> import torch
>>> import PIL

>>> pipe = AutoPipelineForImage2Image.from_pretrained("SimianLuo/LCM_Dreamshaper_v7")
>>> # To save GPU memory, torch.float16 can be used, but it may compromise image quality.
>>> pipe.to(torch_device="cuda", torch_dtype=torch.float32)

>>> prompt = "High altitude snowy mountains"
>>> image = PIL.Image.open("./snowy_mountains.png")

>>> # Can be set to 1~50 steps. LCM support fast inference even <= 4 steps. Recommend: 1~8 steps.
>>> num_inference_steps = 4
>>> images = pipe(
...     prompt=prompt, image=image, num_inference_steps=num_inference_steps, guidance_scale=8.0
... ).images

>>> images[0].save("image.png")
```

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_img2img.py#L246)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1` (`float`) — 第 1 阶段的缩放因子，用于减弱跳跃特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `s2` (`float`) — 第 2 阶段的缩放因子，用于减弱跳跃特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `b1` (`float`) — 第 1 阶段的缩放因子，用于放大骨干特征的贡献。

+   `b2` (`float`) — 第 2 阶段的缩放因子，用于放大骨干特征的贡献。

启用 FreeU 机制，如[`arxiv.org/abs/2309.11497`](https://arxiv.org/abs/2309.11497)中所述。

缩放因子后缀表示它们被应用的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)以获取已知适用于不同管道（如 Stable Diffusion v1、v2 和 Stable Diffusion XL）的值组合。

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_img2img.py#L269)

```py
( )
```

如果启用，则禁用 FreeU 机制。

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_img2img.py#L213)

```py
( )
```

启用切片 VAE 解码。当启用此选项时，VAE 将在几个步骤中将输入张量分割成片段进行解码。这对于节省一些内存并允许更大的批量大小非常有用。

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_img2img.py#L221)

```py
( )
```

禁用切片 VAE 解码。如果之前启用了`enable_vae_slicing`，则此方法将回到一步计算解码。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_img2img.py#L229)

```py
( )
```

启用平铺 VAE 解码。当启用此选项时，VAE 将把输入张量分成瓦片，以便在几个步骤中进行解码和编码。这对于节省大量内存和允许处理更大的图像非常有用。

#### `disable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_img2img.py#L238)

```py
( )
```

禁用平铺 VAE 解码。如果之前启用了`enable_vae_tiling`，则此方法将回到一步计算解码。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_img2img.py#L274)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` or `List[str]`, *optional*) — 要编码的提示设备 — (`torch.device`): torch 设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用无分类器指导

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用来指导图像生成的提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即如果`guidance_scale`小于`1`，则忽略）。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。

+   `lora_scale` (`float`, *可选*) — 如果加载了 LoRA 层，则将应用于文本编码器的所有 LoRA 层的 LoRA 比例。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要跳过的 CLIP 层数。值为 1 意味着将使用预终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

#### `get_guidance_scale_embedding`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_img2img.py#L584)

```py
( w embedding_dim = 512 dtype = torch.float32 ) → export const metadata = 'undefined';torch.FloatTensor
```

参数

+   `timesteps` (`torch.Tensor`) — 在这些时间步生成嵌入向量

+   `embedding_dim` (`int`, *可选*, 默认为 512) — 生成嵌入的维度 dtype — 生成的嵌入的数据类型

返回

`torch.FloatTensor`

形状为`(len(timesteps), embedding_dim)`的嵌入向量

参见 [`github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298`](https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298)

## StableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L10)

```py
( images: Union nsfw_content_detected: Optional )
```

参数

+   `images` (`List[PIL.Image.Image]` 或 `np.ndarray`) — 长度为`batch_size`的去噪 PIL 图像列表或形状为`(batch_size, height, width, num_channels)`的 NumPy 数组。

+   `nsfw_content_detected` (`List[bool]`) — 列表指示相应生成的图像是否包含“不适宜工作”（nsfw）内容，如果无法执行安全检查，则为`None`。

稳定扩散管道的输出类。
