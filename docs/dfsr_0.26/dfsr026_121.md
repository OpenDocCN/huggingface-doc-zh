# 一致性解码器

> 原始文本：[`huggingface.co/docs/diffusers/api/models/consistency_decoder_vae`](https://huggingface.co/docs/diffusers/api/models/consistency_decoder_vae)

一致性解码器可用于从 StableDiffusionPipeline 中的去噪 UNet 解码潜变量。此解码器在[DALL-E 3 技术报告](https://openai.com/dall-e-3)中引入。

原始代码库可在[openai/consistencydecoder](https://github.com/openai/consistencydecoder)找到。

目前仅支持 2 次迭代的推断。

在没有[madebyollin](https://github.com/madebyollin)和[mrsteyk](https://github.com/mrsteyk)在[此问题](https://github.com/openai/consistencydecoder/issues/1)的帮助下，该管道不可能被贡献。

## ConsistencyDecoderVAE

### `class diffusers.ConsistencyDecoderVAE`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/consistency_decoder_vae.py#L52)

```py
( scaling_factor: float = 0.18215 latent_channels: int = 4 encoder_act_fn: str = 'silu' encoder_block_out_channels: Tuple = (128, 256, 512, 512) encoder_double_z: bool = True encoder_down_block_types: Tuple = ('DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D') encoder_in_channels: int = 3 encoder_layers_per_block: int = 2 encoder_norm_num_groups: int = 32 encoder_out_channels: int = 4 decoder_add_attention: bool = False decoder_block_out_channels: Tuple = (320, 640, 1024, 1024) decoder_down_block_types: Tuple = ('ResnetDownsampleBlock2D', 'ResnetDownsampleBlock2D', 'ResnetDownsampleBlock2D', 'ResnetDownsampleBlock2D') decoder_downsample_padding: int = 1 decoder_in_channels: int = 7 decoder_layers_per_block: int = 3 decoder_norm_eps: float = 1e-05 decoder_norm_num_groups: int = 32 decoder_num_train_timesteps: int = 1024 decoder_out_channels: int = 6 decoder_resnet_time_scale_shift: str = 'scale_shift' decoder_time_embedding_type: str = 'learned' decoder_up_block_types: Tuple = ('ResnetUpsampleBlock2D', 'ResnetUpsampleBlock2D', 'ResnetUpsampleBlock2D', 'ResnetUpsampleBlock2D') )
```

用于 DALL-E 3 的一致性解码器。

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline, ConsistencyDecoderVAE

>>> vae = ConsistencyDecoderVAE.from_pretrained("openai/consistency-decoder", torch_dtype=torch.float16)
>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5", vae=vae, torch_dtype=torch.float16
... ).to("cuda")

>>> pipe("horse", generator=torch.manual_seed(0)).images
```

#### `wrapper`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/utils/accelerate_utils.py#L43)

```py
( *args **kwargs )
```

#### `disable_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/consistency_decoder_vae.py#L182)

```py
( )
```

禁用切片化 VAE 解码。如果之前启用了`enable_slicing`，则此方法将回到一步计算解码。

#### `disable_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/consistency_decoder_vae.py#L166)

```py
( )
```

禁用瓦片化 VAE 解码。如果之前启用了`enable_tiling`，则此方法将回到一步计算解码。

#### `enable_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/consistency_decoder_vae.py#L174)

```py
( )
```

启用切片化 VAE 解码。当启用此选项时，VAE 将将输入张量分割成片段，以便在多个步骤中计算解码。这对于节省一些内存并允许更大的批量大小非常有用。

#### `enable_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/consistency_decoder_vae.py#L157)

```py
( use_tiling: bool = True )
```

启用瓦片化 VAE 解码。当启用此选项时，VAE 将将输入张量分割成瓦片，以便在多个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

#### `forward`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/consistency_decoder_vae.py#L403)

```py
( sample: FloatTensor sample_posterior: bool = False return_dict: bool = True generator: Optional = None ) → export const metadata = 'undefined';DecoderOutput or tuple
```

参数

+   `sample`（`torch.FloatTensor`）— 输入样本。

+   `sample_posterior`（`bool`，*可选*，默认为`False`）— 是否从后验中抽样。

+   `return_dict`（`bool`，*可选*，默认为`True`）— 是否返回`DecoderOutput`而不是普通的 tuple。

+   `generator`（`torch.Generator`，*可选*，默认为`None`）— 用于抽样的生成器。

返回

`DecoderOutput`或`tuple`

如果`return_dict`为 True，则返回`DecoderOutput`，否则返回普通的`tuple`。

#### `set_attn_processor`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/consistency_decoder_vae.py#L215)

```py
( processor: Union )
```

参数

+   `processor`（`dict` of `AttentionProcessor`或仅`AttentionProcessor`）— 实例化的处理器类或一组处理器类的字典，将被设置为**所有**`Attention`层的处理器。

    如果`processor`是一个字典，则键需要定义到相应交叉注意力处理器的路径。在设置可训练的注意力处理器时，强烈建议这样做。

设置用于计算注意力的注意力处理器。

#### `set_default_attn_processor`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/consistency_decoder_vae.py#L250)

```py
( )
```

禁用自定义注意力处理器并设置默认的注意力实现。

#### `tiled_encode`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/consistency_decoder_vae.py#L348)

```py
( x: FloatTensor return_dict: bool = True ) → export const metadata = 'undefined';~models.consistency_decoder_vae.ConsistencyDecoderVAEOutput or tuple
```

参数

+   `x`（`torch.FloatTensor`）— 图像的输入批次。

+   `return_dict`（`bool`，*可选*，默认为`True`）— 是否返回`~models.consistency_decoder_vae.ConsistencyDecoderVAEOutput`而不是普通的 tuple。

返回

`~models.consistency_decoder_vae.ConsistencyDecoderVAEOutput`或`tuple`

如果 return_dict 为 True，则返回`~models.consistency_decoder_vae.ConsistencyDecoderVAEOutput`，否则返回一个普通的`tuple`。

使用瓦片编码器对一批图像进行编码。

当启用此选项时，VAE 将把输入张量分割成多个瓦片，以几个步骤计算编码。这对于保持内存使用量恒定而不受图像大小影响很有用。瓦片编码的最终结果与非瓦片编码不同，因为每个瓦片使用不同的编码器。为了避免瓦片伪影，瓦片重叠并混合在一起形成平滑的输出。您仍然可能在输出中看到瓦片大小的变化，但它们应该不太明显。
