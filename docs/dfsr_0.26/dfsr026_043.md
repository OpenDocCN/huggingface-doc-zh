# DiffEdit

> 原始文本：[`huggingface.co/docs/diffusers/using-diffusers/diffedit`](https://huggingface.co/docs/diffusers/using-diffusers/diffedit)

图像编辑通常需要提供要编辑的区域的蒙版。DiffEdit 根据文本查询自动生成蒙版，从而更容易地创建蒙版而无需图像编辑软件。DiffEdit 算法分为三个步骤：

1.  扩散模型对受某些查询文本和参考文本条件的图像进行去噪，为图像的不同区域产生不同的噪声估计；这种差异用于推断一个蒙版，以识别需要更改以匹配查询文本的图像区域

1.  输入图像使用 DDIM 编码到潜空间

1.  潜变量通过扩散模型解码，该模型以文本查询为条件，使用蒙版作为指导，使蒙版外的像素保持与输入图像相同

本指南将向您展示如何使用 DiffEdit 编辑图像，而无需手动创建蒙版。

在开始之前，请确保已安装以下库：

```py
# uncomment to install the necessary libraries in Colab
#!pip install -q diffusers transformers accelerate
```

StableDiffusionDiffEditPipeline 需要一个图像蒙版和一组部分反转的潜变量。图像蒙版是从 generate_mask()函数生成的，并包括两个参数，`source_prompt`和`target_prompt`。这些参数确定要在图像中进行编辑的内容。例如，如果您想将一个装满*水果*的碗更改为一个装满*梨*的碗，则：

```py
source_prompt = "a bowl of fruits"
target_prompt = "a bowl of pears"
```

部分反转的潜变量是通过 invert()函数生成的，通常最好包含一个描述图像的`prompt`或*标题*，以帮助指导反向潜变量采样过程。标题通常可以是您的`source_prompt`，但也可以尝试其他文本描述！

让我们加载管道、调度器、逆调度器，并启用一些优化以减少内存使用：

```py
import torch
from diffusers import DDIMScheduler, DDIMInverseScheduler, StableDiffusionDiffEditPipeline

pipeline = StableDiffusionDiffEditPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-1",
    torch_dtype=torch.float16,
    safety_checker=None,
    use_safetensors=True,
)
pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)
pipeline.enable_model_cpu_offload()
pipeline.enable_vae_slicing()
```

加载要编辑的图像：

```py
from diffusers.utils import load_image, make_image_grid

img_url = "https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"
raw_image = load_image(img_url).resize((768, 768))
raw_image
```

使用 generate_mask()函数生成图像蒙版。您需要传递`source_prompt`和`target_prompt`以指定要在图像中进行编辑的内容：

```py
from PIL import Image

source_prompt = "a bowl of fruits"
target_prompt = "a basket of pears"
mask_image = pipeline.generate_mask(
    image=raw_image,
    source_prompt=source_prompt,
    target_prompt=target_prompt,
)
Image.fromarray((mask_image.squeeze()*255).astype("uint8"), "L").resize((768, 768))
```

接下来，创建反转的潜变量并传递一个描述图像的标题：

```py
inv_latents = pipeline.invert(prompt=source_prompt, image=raw_image).latents
```

最后，将图像蒙版和反转的潜变量传递给管道。`target_prompt`现在成为`prompt`，而`source_prompt`则用作`negative_prompt`：

```py
output_image = pipeline(
    prompt=target_prompt,
    mask_image=mask_image,
    image_latents=inv_latents,
    negative_prompt=source_prompt,
).images[0]
mask_image = Image.fromarray((mask_image.squeeze()*255).astype("uint8"), "L").resize((768, 768))
make_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)
```

![](img/66e8c6b1baf21e3ade0ec71d71f6eff6.png)

原始图像

![](img/9cc02168a4ef69d9e1454293018a3a04.png)

编辑后的图像

## 生成源和目标嵌入

源和目标嵌入可以使用[Flan-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)模型自动生成，而不是手动创建。

从🤗 Transformers 库加载 Flan-T5 模型和分词器：

```py
import torch
from transformers import AutoTokenizer, T5ForConditionalGeneration

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-large")
model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large", device_map="auto", torch_dtype=torch.float16)
```

提供一些初始文本以提示模型生成源和目标提示。

```py
source_concept = "bowl"
target_concept = "basket"

source_text = f"Provide a caption for images containing a {source_concept}. "
"The captions should be in English and should be no longer than 150 characters."

target_text = f"Provide a caption for images containing a {target_concept}. "
"The captions should be in English and should be no longer than 150 characters."
```

接下来，创建一个实用函数来生成提示：

```py
@torch.no_grad()
def generate_prompts(input_prompt):
    input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids.to("cuda")

    outputs = model.generate(
        input_ids, temperature=0.8, num_return_sequences=16, do_sample=True, max_new_tokens=128, top_k=10
    )
    return tokenizer.batch_decode(outputs, skip_special_tokens=True)

source_prompts = generate_prompts(source_text)
target_prompts = generate_prompts(target_text)
print(source_prompts)
print(target_prompts)
```

如果您对生成不同质量文本的策略感兴趣，请查看[生成策略](https://huggingface.co/docs/transformers/main/en/generation_strategies)指南。

加载由 StableDiffusionDiffEditPipeline 使用的文本编码器模型以对文本进行编码。您将使用文本编码器来计算文本嵌入：

```py
import torch
from diffusers import StableDiffusionDiffEditPipeline

pipeline = StableDiffusionDiffEditPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16, use_safetensors=True
)
pipeline.enable_model_cpu_offload()
pipeline.enable_vae_slicing()

@torch.no_grad()
def embed_prompts(sentences, tokenizer, text_encoder, device="cuda"):
    embeddings = []
    for sent in sentences:
        text_inputs = tokenizer(
            sent,
            padding="max_length",
            max_length=tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt",
        )
        text_input_ids = text_inputs.input_ids
        prompt_embeds = text_encoder(text_input_ids.to(device), attention_mask=None)[0]
        embeddings.append(prompt_embeds)
    return torch.concatenate(embeddings, dim=0).mean(dim=0).unsqueeze(0)

source_embeds = embed_prompts(source_prompts, pipeline.tokenizer, pipeline.text_encoder)
target_embeds = embed_prompts(target_prompts, pipeline.tokenizer, pipeline.text_encoder)
```

最后，将嵌入传递给 generate_mask()和 invert()函数，以及管道来生成图像：

```py
  from diffusers import DDIMInverseScheduler, DDIMScheduler
  from diffusers.utils import load_image, make_image_grid
  from PIL import Image

  pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
  pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)

  img_url = "https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"
  raw_image = load_image(img_url).resize((768, 768))

  mask_image = pipeline.generate_mask(
      image=raw_image,
-     source_prompt=source_prompt,
-     target_prompt=target_prompt,
+     source_prompt_embeds=source_embeds,
+     target_prompt_embeds=target_embeds,
  )

  inv_latents = pipeline.invert(
-     prompt=source_prompt,
+     prompt_embeds=source_embeds,
      image=raw_image,
  ).latents

  output_image = pipeline(
      mask_image=mask_image,
      image_latents=inv_latents,
-     prompt=target_prompt,
-     negative_prompt=source_prompt,
+     prompt_embeds=target_embeds,
+     negative_prompt_embeds=source_embeds,
  ).images[0]
  mask_image = Image.fromarray((mask_image.squeeze()*255).astype("uint8"), "L")
  make_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)
```

## 生成反转的标题

当您可以使用`source_prompt`作为标题来帮助生成部分反转的潜在内容时，您也可以使用[BLIP](https://huggingface.co/docs/transformers/model_doc/blip)模型自动生成标题。

从🤗 Transformers 库中加载 BLIP 模型和处理器：

```py
import torch
from transformers import BlipForConditionalGeneration, BlipProcessor

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base", torch_dtype=torch.float16, low_cpu_mem_usage=True)
```

创建一个实用函数来从输入图像生成标题：

```py
@torch.no_grad()
def generate_caption(images, caption_generator, caption_processor):
    text = "a photograph of"

    inputs = caption_processor(images, text, return_tensors="pt").to(device="cuda", dtype=caption_generator.dtype)
    caption_generator.to("cuda")
    outputs = caption_generator.generate(**inputs, max_new_tokens=128)

    # offload caption generator
    caption_generator.to("cpu")

    caption = caption_processor.batch_decode(outputs, skip_special_tokens=True)[0]
    return caption
```

加载输入图像并使用`generate_caption`函数为其生成标题：

```py
from diffusers.utils import load_image

img_url = "https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"
raw_image = load_image(img_url).resize((768, 768))
caption = generate_caption(raw_image, model, processor)
```

![](img/66e8c6b1baf21e3ade0ec71d71f6eff6.png)

生成的标题："一张放在桌子上的水果碗的照片"

现在您可以将标题放入 invert()函数中，以生成部分反转的潜在内容！
