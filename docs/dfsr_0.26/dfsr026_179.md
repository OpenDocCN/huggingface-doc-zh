# unCLIP

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/diffusers/api/pipelines/unclip`](https://huggingface.co/docs/diffusers/api/pipelines/unclip)

[å…·æœ‰ CLIP æ½œåœ¨ç‰¹å¾çš„åˆ†å±‚æ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆ](https://huggingface.co/papers/2204.06125) æ˜¯ç”± Aditya Rameshï¼ŒPrafulla Dhariwalï¼ŒAlex Nicholï¼ŒCasey Chuï¼ŒMark Chen æ’°å†™çš„ã€‚ğŸ¤— Diffusers ä¸­çš„ unCLIP æ¨¡å‹æ¥è‡ª kakaobrain çš„[karlo](https://github.com/kakaobrain/karlo)ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*åƒ CLIP è¿™æ ·çš„å¯¹æ¯”æ¨¡å‹å·²ç»è¢«è¯æ˜å¯ä»¥å­¦ä¹ åˆ°æ•æ‰è¯­ä¹‰å’Œé£æ ¼çš„å›¾åƒçš„ç¨³å¥è¡¨ç¤ºã€‚ä¸ºäº†åˆ©ç”¨è¿™äº›è¡¨ç¤ºæ¥è¿›è¡Œå›¾åƒç”Ÿæˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µæ¨¡å‹ï¼šä¸€ä¸ªå…ˆéªŒæ¨¡å‹æ ¹æ®æ–‡æœ¬æ ‡é¢˜ç”Ÿæˆä¸€ä¸ª CLIP å›¾åƒåµŒå…¥ï¼Œä¸€ä¸ªè§£ç å™¨æ ¹æ®å›¾åƒåµŒå…¥ç”Ÿæˆä¸€ä¸ªå›¾åƒã€‚æˆ‘ä»¬å±•ç¤ºäº†æ˜ç¡®ç”Ÿæˆå›¾åƒè¡¨ç¤ºå¯ä»¥æé«˜å›¾åƒå¤šæ ·æ€§ï¼ŒåŒæ—¶æœ€å°åŒ–é€¼çœŸåº¦å’Œæ ‡é¢˜ç›¸ä¼¼æ€§çš„æŸå¤±ã€‚æˆ‘ä»¬çš„è§£ç å™¨æ ¹æ®å›¾åƒè¡¨ç¤ºå¯ä»¥äº§ç”Ÿä¿ç•™å…¶è¯­ä¹‰å’Œé£æ ¼çš„å›¾åƒå˜ä½“ï¼ŒåŒæ—¶å˜åŒ–éå¿…è¦çš„ç»†èŠ‚ï¼Œè¿™äº›ç»†èŠ‚åœ¨å›¾åƒè¡¨ç¤ºä¸­ä¸å­˜åœ¨ã€‚æ­¤å¤–ï¼ŒCLIP çš„è”åˆåµŒå…¥ç©ºé—´ä½¿å¾—å¯ä»¥é€šè¿‡è¯­è¨€å¼•å¯¼çš„å›¾åƒæ“ä½œä»¥é›¶æ ·æœ¬çš„æ–¹å¼è¿›è¡Œã€‚æˆ‘ä»¬ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè§£ç å™¨ï¼Œå¹¶å°è¯•å…ˆéªŒæ¨¡å‹ä½¿ç”¨è‡ªå›å½’æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œå‘ç°åè€…åœ¨è®¡ç®—ä¸Šæ›´æœ‰æ•ˆï¼Œå¹¶äº§ç”Ÿæ›´é«˜è´¨é‡çš„æ ·æœ¬ã€‚*

æ‚¨å¯ä»¥åœ¨[lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch)æ‰¾åˆ° lucidrains çš„ DALL-E 2 é‡å»ºã€‚

ç¡®ä¿æŸ¥çœ‹è°ƒåº¦ç¨‹åºæŒ‡å—ä»¥äº†è§£å¦‚ä½•åœ¨è°ƒåº¦ç¨‹åºé€Ÿåº¦å’Œè´¨é‡ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œå¹¶æŸ¥çœ‹é‡ç”¨ç»„ä»¶è·¨ç®¡é“éƒ¨åˆ†ä»¥äº†è§£å¦‚ä½•æœ‰æ•ˆåœ°å°†ç›¸åŒç»„ä»¶åŠ è½½åˆ°å¤šä¸ªç®¡é“ä¸­ã€‚

## UnCLIPPipeline

### `class diffusers.UnCLIPPipeline`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip.py#L34)

```py
( prior: PriorTransformer decoder: UNet2DConditionModel text_encoder: CLIPTextModelWithProjection tokenizer: CLIPTokenizer text_proj: UnCLIPTextProjModel super_res_first: UNet2DModel super_res_last: UNet2DModel prior_scheduler: UnCLIPScheduler decoder_scheduler: UnCLIPScheduler super_res_scheduler: UnCLIPScheduler )
```

å‚æ•°

+   `text_encoder` ([CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection)) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) â€” ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„ `CLIPTokenizer`ã€‚

+   `prior` (PriorTransformer) â€” ç”¨äºä»æ–‡æœ¬åµŒå…¥è¿‘ä¼¼å›¾åƒåµŒå…¥çš„è§„èŒƒ unCLIP å…ˆéªŒã€‚

+   `text_proj` (`UnCLIPTextProjModel`) â€” åœ¨ä¼ é€’ç»™è§£ç å™¨ä¹‹å‰å‡†å¤‡å’Œç»„åˆåµŒå…¥çš„å®ç”¨ç±»ã€‚

+   `decoder` (UNet2DConditionModel) â€” å°†å›¾åƒåµŒå…¥åè½¬ä¸ºå›¾åƒçš„è§£ç å™¨ã€‚

+   `super_res_first` (UNet2DModel) â€” è¶…åˆ†è¾¨ç‡ UNetã€‚ç”¨äºè¶…åˆ†è¾¨ç‡æ‰©æ•£è¿‡ç¨‹çš„é™¤æœ€åä¸€æ­¥ä¹‹å¤–çš„æ‰€æœ‰æ­¥éª¤ã€‚

+   `super_res_last` (UNet2DModel) â€” è¶…åˆ†è¾¨ç‡ UNetã€‚ç”¨äºè¶…åˆ†è¾¨ç‡æ‰©æ•£è¿‡ç¨‹çš„æœ€åä¸€æ­¥ã€‚

+   `prior_scheduler` (`UnCLIPScheduler`) â€” ç”¨äºå…ˆéªŒå»å™ªè¿‡ç¨‹çš„è°ƒåº¦ç¨‹åºï¼ˆä¿®æ”¹çš„ DDPMSchedulerï¼‰ã€‚

+   `decoder_scheduler` (`UnCLIPScheduler`) â€” ç”¨äºè§£ç å™¨å»å™ªè¿‡ç¨‹çš„è°ƒåº¦ç¨‹åºï¼ˆä¿®æ”¹çš„ DDPMSchedulerï¼‰ã€‚

+   `super_res_scheduler` (`UnCLIPScheduler`) â€” ç”¨äºè¶…åˆ†è¾¨ç‡å»å™ªè¿‡ç¨‹çš„è°ƒåº¦ç¨‹åºï¼ˆä¿®æ”¹çš„ DDPMSchedulerï¼‰ã€‚

ä½¿ç”¨ unCLIP è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç®¡é“ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª DiffusionPipelineã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–æ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip.py#L211)

```py
( prompt: Union = None num_images_per_prompt: int = 1 prior_num_inference_steps: int = 25 decoder_num_inference_steps: int = 25 super_res_num_inference_steps: int = 7 generator: Union = None prior_latents: Optional = None decoder_latents: Optional = None super_res_latents: Optional = None text_model_output: Union = None text_attention_mask: Optional = None prior_guidance_scale: float = 4.0 decoder_guidance_scale: float = 8.0 output_type: Optional = 'pil' return_dict: bool = True ) â†’ export const metadata = 'undefined';ImagePipelineOutput or tuple
```

å‚æ•°

+   `prompt` (`str` æˆ– `List[str]`) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºã€‚åªæœ‰åœ¨ä¼ é€’ `text_model_output` å’Œ `text_attention_mask` æ—¶æ‰èƒ½å°†å…¶ç•™ç©ºã€‚

+   `num_images_per_prompt` (`int`, *optional*, é»˜è®¤ä¸º 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `prior_num_inference_steps` (`int`, *optional*, é»˜è®¤ä¸º 25) â€” å…ˆéªŒçš„å»å™ªæ­¥éª¤æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚

+   `decoder_num_inference_steps` (`int`, *optional*, é»˜è®¤ä¸º 25) â€” è§£ç å™¨çš„å»å™ªæ­¥éª¤æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚

+   `super_res_num_inference_steps` (`int`, *optional*, é»˜è®¤ä¸º 7) â€” è¶…åˆ†è¾¨ç‡å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚

+   `generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *optional*) â€” ç”¨äºä½¿ç”Ÿæˆç»“æœç¡®å®šæ€§çš„ [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)ã€‚

+   `prior_latents` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º (batch size, embeddings dimension)ï¼Œ*optional*) â€” é¢„å…ˆç”Ÿæˆçš„å˜ˆæ‚æ½œå˜é‡ï¼Œç”¨ä½œå…ˆéªŒçš„è¾“å…¥ã€‚

+   `decoder_latents` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º (batch size, channels, height, width)ï¼Œ*optional*) â€” é¢„å…ˆç”Ÿæˆçš„å˜ˆæ‚æ½œå˜é‡ï¼Œç”¨ä½œè§£ç å™¨çš„è¾“å…¥ã€‚

+   `super_res_latents` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º (batch size, channels, super res height, super res width)ï¼Œ*optional*) â€” é¢„å…ˆç”Ÿæˆçš„å˜ˆæ‚æ½œå˜é‡ï¼Œç”¨ä½œè§£ç å™¨çš„è¾“å…¥ã€‚

+   `prior_guidance_scale` (`float`, *optional*, é»˜è®¤ä¸º 4.0) â€” æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å½“ `guidance_scale > 1` æ—¶å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚

+   `decoder_guidance_scale` (`float`, *optional*, é»˜è®¤ä¸º 4.0) â€” æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å½“ `guidance_scale > 1` æ—¶å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚

+   `text_model_output` (`CLIPTextModelOutput`, *optional*) â€” å¯ä»æ–‡æœ¬ç¼–ç å™¨æ´¾ç”Ÿçš„é¢„å®šä¹‰ `CLIPTextModel` è¾“å‡ºã€‚é¢„å®šä¹‰çš„æ–‡æœ¬è¾“å‡ºå¯ä»¥ç”¨äºè¯¸å¦‚æ–‡æœ¬åµŒå…¥æ’å€¼ä¹‹ç±»çš„ä»»åŠ¡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç¡®ä¿è¿˜ä¼ é€’ `text_attention_mask`ã€‚`prompt` å¯ä»¥ç•™ç©ºã€‚

+   `text_attention_mask` (`torch.Tensor`, *optional*) â€” é¢„å®šä¹‰çš„ CLIP æ–‡æœ¬æ³¨æ„åŠ›æ©ç ï¼Œå¯ä»¥ä»åˆ†è¯å™¨ä¸­æ´¾ç”Ÿã€‚å½“ä¼ é€’ `text_model_output` æ—¶ï¼Œé¢„å®šä¹‰çš„æ–‡æœ¬æ³¨æ„åŠ›æ©ç æ˜¯å¿…è¦çš„ã€‚

+   `output_type` (`str`, *optional*, é»˜è®¤ä¸º`"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚é€‰æ‹© `PIL.Image` æˆ– `np.array` ä¹‹é—´çš„ä¸€ä¸ªã€‚

+   `return_dict` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å› ImagePipelineOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„ tupleã€‚

è¿”å›

ImagePipelineOutput æˆ– `tuple`

å¦‚æœ `return_dict` ä¸º `True`ï¼Œåˆ™è¿”å› ImagePipelineOutputï¼Œå¦åˆ™è¿”å›ä¸€ä¸ª `tuple`ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯åŒ…å«ç”Ÿæˆå›¾åƒçš„åˆ—è¡¨ã€‚

ç”¨äºç”Ÿæˆçš„ç®¡é“çš„è°ƒç”¨å‡½æ•°ã€‚

## UnCLIPImageVariationPipeline

### `class diffusers.UnCLIPImageVariationPipeline`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py#L39)

```py
( decoder: UNet2DConditionModel text_encoder: CLIPTextModelWithProjection tokenizer: CLIPTokenizer text_proj: UnCLIPTextProjModel feature_extractor: CLIPImageProcessor image_encoder: CLIPVisionModelWithProjection super_res_first: UNet2DModel super_res_last: UNet2DModel decoder_scheduler: UnCLIPScheduler super_res_scheduler: UnCLIPScheduler )
```

å‚æ•°

+   `text_encoder`ï¼ˆ[CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection)ï¼‰â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚

+   `tokenizer`ï¼ˆ[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)ï¼‰â€” ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„ `CLIPTokenizer`ã€‚

+   `feature_extractor`ï¼ˆ[CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)ï¼‰â€” ä»ç”Ÿæˆçš„å›¾åƒä¸­æå–ç‰¹å¾çš„æ¨¡å‹ï¼Œç”¨ä½œ `image_encoder` çš„è¾“å…¥ã€‚

+   `image_encoder`ï¼ˆ[CLIPVisionModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection)ï¼‰â€” å†»ç»“çš„ CLIP å›¾åƒç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚

+   `text_proj`ï¼ˆ`UnCLIPTextProjModel`ï¼‰â€” åœ¨ä¼ é€’ç»™è§£ç å™¨ä¹‹å‰å‡†å¤‡å’Œç»„åˆåµŒå…¥çš„å®ç”¨ç±»ã€‚

+   `decoder`ï¼ˆUNet2DConditionModelï¼‰â€” å°†å›¾åƒåµŒå…¥åè½¬ä¸ºå›¾åƒçš„è§£ç å™¨ã€‚

+   `super_res_first`ï¼ˆUNet2DModelï¼‰â€” è¶…åˆ†è¾¨ç‡ UNetã€‚ç”¨äºè¶…åˆ†è¾¨ç‡æ‰©æ•£è¿‡ç¨‹ä¸­çš„æ‰€æœ‰æ­¥éª¤ï¼Œé™¤äº†æœ€åä¸€æ­¥ã€‚

+   `super_res_last`ï¼ˆUNet2DModelï¼‰â€” è¶…åˆ†è¾¨ç‡ UNetã€‚ç”¨äºè¶…åˆ†è¾¨ç‡æ‰©æ•£è¿‡ç¨‹çš„æœ€åä¸€æ­¥ã€‚

+   `decoder_scheduler`ï¼ˆ`UnCLIPScheduler`ï¼‰â€” ç”¨äºè§£ç å™¨å»å™ªè¿‡ç¨‹çš„è°ƒåº¦å™¨ï¼ˆä¿®æ”¹è‡ª DDPMSchedulerï¼‰ã€‚

+   `super_res_scheduler`ï¼ˆ`UnCLIPScheduler`ï¼‰â€” ç”¨äºè¶…åˆ†è¾¨ç‡å»å™ªè¿‡ç¨‹çš„è°ƒåº¦å™¨ï¼ˆä¿®æ”¹è‡ª DDPMSchedulerï¼‰ã€‚

ä½¿ç”¨ UnCLIP ä»è¾“å…¥å›¾åƒç”Ÿæˆå›¾åƒå˜åŒ–çš„æµæ°´çº¿ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª DiffusionPipelineã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–æ‰€æœ‰æµæ°´çº¿å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py#L199)

```py
( image: Union = None num_images_per_prompt: int = 1 decoder_num_inference_steps: int = 25 super_res_num_inference_steps: int = 7 generator: Optional = None decoder_latents: Optional = None super_res_latents: Optional = None image_embeddings: Optional = None decoder_guidance_scale: float = 8.0 output_type: Optional = 'pil' return_dict: bool = True ) â†’ export const metadata = 'undefined';ImagePipelineOutput or tuple
```

å‚æ•°

+   `image`ï¼ˆ`PIL.Image.Image` æˆ– `List[PIL.Image.Image]` æˆ– `torch.FloatTensor`ï¼‰â€” ä»£è¡¨å›¾åƒæ‰¹æ¬¡çš„ `Image` æˆ–å¼ é‡ï¼Œç”¨ä½œèµ·å§‹ç‚¹ã€‚å¦‚æœæä¾›å¼ é‡ï¼Œåˆ™éœ€è¦ä¸ `CLIPImageProcessor` çš„ [é…ç½®](https://huggingface.co/fusing/karlo-image-variations-diffusers/blob/main/feature_extractor/preprocessor_config.json) å…¼å®¹ã€‚åªæœ‰åœ¨ä¼ é€’ `image_embeddings` æ—¶æ‰èƒ½ä¿æŒä¸º `None`ã€‚

+   `num_images_per_prompt`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 1ï¼‰â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `decoder_num_inference_steps`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 25ï¼‰â€” è§£ç å™¨çš„å»å™ªæ­¥éª¤æ•°ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚

+   `super_res_num_inference_steps`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 7ï¼‰â€” è¶…åˆ†è¾¨ç‡çš„å»å™ªæ­¥éª¤æ•°ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚

+   `generator`ï¼ˆ`torch.Generator`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§çš„ [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)ã€‚

+   `decoder_latents`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸ºï¼ˆæ‰¹é‡å¤§å°ï¼Œé€šé“æ•°ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰ï¼Œ*å¯é€‰*ï¼‰â€” é¢„å…ˆç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œç”¨ä½œè§£ç å™¨çš„è¾“å…¥ã€‚

+   `super_res_latents` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º(batch size, channels, super res height, super res width)ï¼Œ*optional*) â€” é¢„ç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œç”¨ä½œè§£ç å™¨çš„è¾“å…¥ã€‚

+   `decoder_guidance_scale` (`float`, *optional*, é»˜è®¤å€¼ä¸º 4.0) â€” è¾ƒé«˜çš„å¼•å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚ å½“`guidance_scale > 1`æ—¶å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚

+   `image_embeddings` (`torch.Tensor`, *optional*) â€” å¯ä»å›¾åƒç¼–ç å™¨æ´¾ç”Ÿçš„é¢„å®šä¹‰å›¾åƒåµŒå…¥ã€‚ å¯ä»¥ä¼ é€’é¢„å®šä¹‰çš„å›¾åƒåµŒå…¥ä»¥ç”¨äºè¯¸å¦‚å›¾åƒæ’å€¼ä¹‹ç±»çš„ä»»åŠ¡ã€‚ `image` å¯ä»¥ä¿æŒä¸º `None`ã€‚

+   `output_type` (`str`, *optional*, é»˜è®¤å€¼ä¸º`"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚ é€‰æ‹©`PIL.Image`æˆ–`np.array`ä¹‹é—´ã€‚

+   `return_dict` (`bool`, *optional*, é»˜è®¤å€¼ä¸º`True`) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ImagePipelineOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

è¿”å›

ImagePipelineOutput æˆ– `tuple`

å¦‚æœ`return_dict`ä¸º`True`ï¼Œåˆ™è¿”å› ImagePipelineOutputï¼Œå¦åˆ™è¿”å›ä¸€ä¸ªå…ƒç»„ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯åŒ…å«ç”Ÿæˆå›¾åƒçš„åˆ—è¡¨ã€‚

ç”¨äºç”Ÿæˆçš„ç®¡é“çš„è°ƒç”¨å‡½æ•°ã€‚

## ImagePipelineOutput

### `class diffusers.ImagePipelineOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L116)

```py
( images: Union )
```

å‚æ•°

+   `images` (`List[PIL.Image.Image]` æˆ– `np.ndarray`) â€” é•¿åº¦ä¸º`batch_size`çš„å»å™ª PIL å›¾åƒåˆ—è¡¨æˆ–å½¢çŠ¶ä¸º`(batch_size, height, width, num_channels)`çš„ NumPy æ•°ç»„ã€‚

å›¾åƒç®¡é“çš„è¾“å‡ºç±»ã€‚
