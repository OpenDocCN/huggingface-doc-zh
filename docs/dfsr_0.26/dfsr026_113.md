# UNet2DConditionModel

> 原文链接：[`huggingface.co/docs/diffusers/api/models/unet2d-cond`](https://huggingface.co/docs/diffusers/api/models/unet2d-cond)

[UNet](https://huggingface.co/papers/1505.04597)模型最初由 Ronneberger 等人提出，用于生物医学图像分割，但它也常用于🤗扩散器，因为它输出与输入相同大小的图像。它是扩散系统中最重要的组件之一，因为它促进了实际的扩散过程。在🤗扩散器中有几个 UNet 模型的变体，取决于它的维度数量以及它是否是条件模型。这是一个 2D UNet 条件模型。

论文摘要如下：

成功训练深度网络需要成千上万个带注释的训练样本的大量一致性。在本文中，我们提出了一种依赖于数据增强的网络和训练策略，以更有效地利用可用的带注释样本。该架构包括一个收缩路径来捕获上下文和一个对称扩展路径，实现精确的定位。我们展示了这样一个网络可以从非常少的图像端到端训练，并且在 ISBI 挑战中表现优于先前最佳方法（滑动窗口卷积网络）用于电子显微镜堆叠中神经结构的分割。使用相同的网络在透射光显微镜图像（相差和 DIC）上训练，我们在 2015 年 ISBI 细胞跟踪挑战中在这些类别中大幅领先。此外，该网络速度很快。在最新的 GPU 上，对 512x512 图像的分割不到一秒。完整的实现（基于 Caffe）和训练的网络可在[`lmb.informatik.uni-freiburg.de/people/ronneber/u-net`](http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net)上找到。

## UNet2DConditionModel

### `class diffusers.UNet2DConditionModel`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_2d_condition.py#L71)

```py
( sample_size: Optional = None in_channels: int = 4 out_channels: int = 4 center_input_sample: bool = False flip_sin_to_cos: bool = True freq_shift: int = 0 down_block_types: Tuple = ('CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D') mid_block_type: Optional = 'UNetMidBlock2DCrossAttn' up_block_types: Tuple = ('UpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D') only_cross_attention: Union = False block_out_channels: Tuple = (320, 640, 1280, 1280) layers_per_block: Union = 2 downsample_padding: int = 1 mid_block_scale_factor: float = 1 dropout: float = 0.0 act_fn: str = 'silu' norm_num_groups: Optional = 32 norm_eps: float = 1e-05 cross_attention_dim: Union = 1280 transformer_layers_per_block: Union = 1 reverse_transformer_layers_per_block: Optional = None encoder_hid_dim: Optional = None encoder_hid_dim_type: Optional = None attention_head_dim: Union = 8 num_attention_heads: Union = None dual_cross_attention: bool = False use_linear_projection: bool = False class_embed_type: Optional = None addition_embed_type: Optional = None addition_time_embed_dim: Optional = None num_class_embeds: Optional = None upcast_attention: bool = False resnet_time_scale_shift: str = 'default' resnet_skip_time_act: bool = False resnet_out_scale_factor: int = 1.0 time_embedding_type: str = 'positional' time_embedding_dim: Optional = None time_embedding_act_fn: Optional = None timestep_post_act: Optional = None time_cond_proj_dim: Optional = None conv_in_kernel: int = 3 conv_out_kernel: int = 3 projection_class_embeddings_input_dim: Optional = None attention_type: str = 'default' class_embeddings_concat: bool = False mid_block_only_cross_attention: Optional = None cross_attention_norm: Optional = None addition_embed_type_num_heads = 64 )
```

参数

+   `sample_size`（`int`或`Tuple[int, int]`，*可选*，默认为`None`）— 输入/输出样本的高度和宽度。

+   `in_channels`（`int`，*可选*，默认为 4）— 输入样本中的通道数。

+   `out_channels`（`int`，*可选*，默认为 4）— 输出中的通道数。

+   `center_input_sample`（`bool`，*可选*，默认为`False`）— 是否将输入样本居中。

+   `flip_sin_to_cos`（`bool`，*可选*，默认为`False`）— 是否在时间嵌入中将 sin 翻转为 cos。

+   `freq_shift`（`int`，*可选*，默认为 0）— 应用于时间嵌入的频移。

+   `down_block_types`（`Tuple[str]`，*可选*，默认为`("CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "DownBlock2D")`）— 要使用的下采样块的元组。

+   `mid_block_type`（`str`，*可选*，默认为`"UNetMidBlock2DCrossAttn"`）— UNet 中间的块类型，可以是`UNetMidBlock2DCrossAttn`、`UNetMidBlock2D`或`UNetMidBlock2DSimpleCrossAttn`之一。如果为`None`，则跳过中间块层。

+   `up_block_types`（`Tuple[str]`，*可选*，默认为`("UpBlock2D", "CrossAttnUpBlock2D", "CrossAttnUpBlock2D", "CrossAttnUpBlock2D")`）— 要使用的上采样块的元组。

+   `only_cross_attention(bool`或`Tuple[bool]`，*可选*，默认为`False`）— 是否在基本变换器块中包含自注意力，参见`BasicTransformerBlock`。

+   `block_out_channels`（`Tuple[int]`，*可选*，默认为`(320, 640, 1280, 1280)`）— 每个块的输出通道的元组。

+   `layers_per_block`（`int`，*可选*，默认为 2）— 每个块的层数。

+   `downsample_padding`（`int`，*可选*，默认为 1）— 用于下采样卷积的填充。

+   `mid_block_scale_factor`（`float`，*可选*，默认为 1.0）— 用于中间块的比例因子。

+   `dropout` (`float`, *可选*, 默认为 0.0) — 要使用的 dropout 概率。

+   `act_fn` (`str`, *可选*, 默认为`"silu"`) — 要使用的激活函数。

+   `norm_num_groups` (`int`, *可选*, 默认为 32) — 用于规范化的组数。如果为`None`，则在后处理中跳过规范化和激活层。

+   `norm_eps` (`float`, *可选*, 默认为 1e-5) — 用于规范化的 epsilon 值。

+   `cross_attention_dim` (`int` 或 `Tuple[int]`, *可选*, 默认为 1280) — 交叉注意力特征的维度。

+   `transformer_layers_per_block` (`int`, `Tuple[int]`, 或 `Tuple[Tuple]`, *可选*, 默认为 1) — `BasicTransformerBlock` 类型的 transformer 块的数量。仅适用于 `~models.unet_2d_blocks.CrossAttnDownBlock2D`, `~models.unet_2d_blocks.CrossAttnUpBlock2D`, `~models.unet_2d_blocks.UNetMidBlock2DCrossAttn`。

一个有条件的 2D UNet 模型，接受一个带噪声的样本、条件状态和一个时间步，并返回一个形状输出的样本。

该模型继承自 ModelMixin。查看超类文档以了解为所有模型实现的通用方法（如下载或保存）。

reverse_transformer_layers_per_block：（`Tuple[Tuple]`，*可选*，默认为 None）：在 U-Net 的上采样块中，`BasicTransformerBlock`类型的 transformer 块的数量。仅在`transformer_layers_per_block`为`Tuple[Tuple]`类型且对于`~models.unet_2d_blocks.CrossAttnDownBlock2D`，`~models.unet_2d_blocks.CrossAttnUpBlock2D`，`~models.unet_2d_blocks.UNetMidBlock2DCrossAttn`时相关。encoder_hid_dim（`int`，*可选*，默认为 None）：如果定义了`encoder_hid_dim_type`，则将从`encoder_hid_dim`维度投影到`cross_attention_dim`的`encoder_hidden_states`。encoder_hid_dim_type（`str`，*可选*，默认为`None`）：如果给定，将根据`encoder_hid_dim_type`将`encoder_hidden_states`和可能的其他嵌入向下投影到维度为`cross_attention`的文本嵌入。attention_head_dim（`int`，*可选*，默认为 8）：注意力头的维度。num_attention_heads（`int`，*可选*）：注意力头的数量。如果未定义，则默认为`attention_head_dim`。resnet_time_scale_shift（`str`，*可选*，默认为`"default"`）：ResNet 块的时间尺度偏移配置（参见`ResnetBlock2D`）。选择`default`或`scale_shift`。class_embed_type（`str`，*可选*，默认为`None`）：要使用的类嵌入类型，最终将与时间嵌入相加。选择`None`、`"timestep"`、`"identity"`、`"projection"`或`"simple_projection"`。addition_embed_type（`str`，*可选*，默认为`None`）：配置一个可选的嵌入，将与时间嵌入相加。选择`None`或“text”。“text”将使用`TextTimeEmbedding`层。addition_time_embed_dim：（`int`，*可选*，默认为`None`）：时间步嵌入的维度。num_class_embeds（`int`，*可选*，默认为`None`）：要投影到`time_embed_dim`的可学习嵌入矩阵的输入维度，当使用`class_embed_type`等于`None`进行类别调节时。time_embedding_type（`str`，*可选*，默认为`positional`）：用于时间步的位置嵌入类型。选择`positional`或`fourier`。time_embedding_dim（`int`，*可选*，默认为`None`）：投影时间嵌入的维度的可选覆盖。time_embedding_act_fn（`str`，*可选*，默认为`None`）：仅在时间嵌入传递到 UNet 的其余部分之前使用的可选激活函数。选择`silu`、`mish`、`gelu`和`swish`。timestep_post_act（`str`，*可选*，默认为`None`）：在时间步嵌入中使用的第二个激活函数。选择`silu`、`mish`和`gelu`。time_cond_proj_dim（`int`，*可选*，默认为`None`）：时间步嵌入中`cond_proj`层的维度。conv_in_kernel（`int`，*可选*，默认为`3`）：`conv_in`层的内核大小。conv_out_kernel（`int`，*可选*，默认为`3`）：`conv_out`层的内核大小。projection_class_embeddings_input_dim（`int`，*可选*）：当`class_embed_type="projection"`时，`class_labels`输入的维度。在`class_embed_type="projection"`时需要。class_embeddings_concat（`bool`，*可选*，默认为`False`）：是否将时间嵌入与类嵌入连接起来。mid_block_only_cross_attention（`bool`，*可选*，默认为`None`）：在使用`UNetMidBlock2DSimpleCrossAttn`时是否在中间块中使用交叉注意力。如果`only_cross_attention`作为单个布尔值给出且`mid_block_only_cross_attention`为`None`，则`only_cross_attention`值将用作`mid_block_only_cross_attention`的值。否则默认为`False`。

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_2d_condition.py#L788)

```py
( )
```

禁用 FreeU 机制。

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_2d_condition.py#L764)

```py
( s1 s2 b1 b2 )
```

参数

+   `s1` (`float`) — 阶段 1 的缩放因子，用于减弱跳跃特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `s2` (`float`) — 阶段 2 的缩放因子，用于减弱跳跃特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `b1` (`float`) — 阶段 1 的缩放因子，用于放大主干特征的贡献。

+   `b2` (`float`) — 阶段 2 的缩放因子，用于放大主干特征的贡献。

启用来自[`arxiv.org/abs/2309.11497`](https://arxiv.org/abs/2309.11497)的 FreeU 机制。

缩放因子后缀表示它们被应用的阶段块。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)以获取已知适用于不同流水线（如 Stable Diffusion v1、v2 和 Stable Diffusion XL）的值组合。

#### `forward`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_2d_condition.py#L843)

```py
( sample: FloatTensor timestep: Union encoder_hidden_states: Tensor class_labels: Optional = None timestep_cond: Optional = None attention_mask: Optional = None cross_attention_kwargs: Optional = None added_cond_kwargs: Optional = None down_block_additional_residuals: Optional = None mid_block_additional_residual: Optional = None down_intrablock_additional_residuals: Optional = None encoder_attention_mask: Optional = None return_dict: bool = True ) → export const metadata = 'undefined';UNet2DConditionOutput or tuple
```

参数

+   `sample` (`torch.FloatTensor`) — 具有以下形状的嘈杂输入张量`(batch, channel, height, width)`。

+   `timestep` (`torch.FloatTensor`或`float`或`int`) — 对输入进行去噪的时间步数。

+   `encoder_hidden_states` (`torch.FloatTensor`) — 具有形状`(batch, sequence_length, feature_dim)`的编码器隐藏状态。

+   `class_labels` (`torch.Tensor`, *可选*, 默认为 `None`) — 用于条件化的可选类标签。它们的嵌入将与时间步嵌入相加。timestep_cond — (`torch.Tensor`, *可选*, 默认为 `None`): 时间步的条件嵌入。如果提供，这些嵌入将与通过`self.time_embedding`层传递的样本相加，以获得时间步嵌入。

+   `attention_mask` (`torch.Tensor`, *可选*, 默认为 `None`) — 形状为`(batch, key_tokens)`的注意力掩码被应用于`encoder_hidden_states`。如果为`1`，则保留掩码，否则如果为`0`，则丢弃掩码。掩码将被转换为偏置，该偏置会向“丢弃”标记对应的注意力分数添加大的负值。

+   `cross_attention_kwargs` (`dict`, *可选*) — 一个 kwargs 字典，如果指定，则将传递给`AttentionProcessor`，如[diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中的`self.processor`中定义的那样。added_cond_kwargs — (`dict`, *可选*): 一个 kwargs 字典，包含额外的嵌入，如果指定，则将添加到传递给 UNet 块的嵌入中。down_block_additional_residuals — (`torch.Tensor`的元组，*可选*): 一个张量的元组，如果指定，则将添加到 down unet 块的残差中。mid_block_additional_residual — (`torch.Tensor`, *可选*): 一个张量，如果指定，则将添加到中间 unet 块的残差中。

+   `encoder_attention_mask` (`torch.Tensor`) — 形状为`(batch, sequence_length)`的交叉注意力掩码被应用于`encoder_hidden_states`。如果为`True`，则保留掩码，否则如果为`False`，则丢弃掩码。掩码将被转换为偏置，该偏置会向“丢弃”标记对应的注意力分数添加大的负值。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回一个 UNet2DConditionOutput 而不是一个普通的元组。

+   `cross_attention_kwargs`（`dict`，*可选*）—如果指定，将传递给`AttnProcessor`的 kwargs 字典。added_cond_kwargs —（`dict`，*可选*）：包含额外嵌入的 kwargs 字典，如果指定，则将添加到传递给 UNet 块的嵌入中。

+   `down_block_additional_residuals`（`torch.Tensor`的`tuple`，*可选*）—要添加到 UNet 长跳连接的额外残差，例如来自 ControlNet 侧模型

+   `mid_block_additional_residual`（`torch.Tensor`，*可选*）—要添加到 UNet 中间块输出的额外残差，例如来自 ControlNet 侧模型

+   `down_intrablock_additional_residuals`（`torch.Tensor`的`tuple`，*可选*）—要在 UNet 下块内添加的额外残差，例如来自 T2I-Adapter 侧模型

返回

UNet2DConditionOutput 或`tuple`

如果`return_dict`为 True，则返回一个 UNet2DConditionOutput，否则返回一个`tuple`，其中第一个元素是样本张量。

UNet2DConditionModel 的前向方法。

#### `fuse_qkv_projections`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_2d_condition.py#L796)

```py
( )
```

启用融合的 QKV 投影。对于自注意力模块，所有投影矩阵（即查询、键、值）都被融合。对于交叉注意力模块，键和值投影矩阵被融合。

此 API 为🧪实验性质。

#### `set_attention_slice`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_2d_condition.py#L695)

```py
( slice_size )
```

参数

+   `slice_size`（`str`或`int`或`list(int)`，*可选*，默认为`"auto"`）—当为`"auto"`时，注意力头的输入减半，因此注意力在两个步骤中计算。如果为`"max"`，通过一次只运行一个切片来节省最大内存。如果提供了一个数字，则使用`attention_head_dim // slice_size`个切片。在这种情况下，`attention_head_dim`必须是`slice_size`的倍数。

启用切片注意力计算。

启用此选项时，注意力模块将输入张量分片以在多个步骤中计算注意力。这对于在节省一些内存的同时换取一点速度下降很有用。

#### `set_attn_processor`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_2d_condition.py#L646)

```py
( processor: Union )
```

参数

+   `processor`（`AttentionProcessor`的`dict`或仅`AttentionProcessor`）—已实例化的处理器类或处理器类的字典，将被设置为**所有**`Attention`层的处理器。

    如果`processor`是一个字典，则键需要定义相应交叉注意力处理器的路径。在设置可训练的注意力处理器时，强烈建议这样做。

设置用于计算注意力的注意力处理器。

#### `set_default_attn_processor`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_2d_condition.py#L680)

```py
( )
```

禁用自定义注意力处理器并设置默认的注意力实现。

#### `unfuse_qkv_projections`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_2d_condition.py#L819)

```py
( )
```

禁用了融合的 QKV 投影（如果已启用）。

此 API 为🧪实验性质。

#### `unload_lora`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_2d_condition.py#L832)

```py
( )
```

卸载 LoRA 权重。

## UNet2DConditionOutput

### `class diffusers.models.unets.unet_2d_condition.UNet2DConditionOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_2d_condition.py#L58)

```py
( sample: FloatTensor = None )
```

参数

+   `sample` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 在`encoder_hidden_states`输入上条件化的隐藏状态输出。模型的最后一层的输出。

UNet2DConditionModel 的输出。

## FlaxUNet2DConditionModel

### `class diffusers.FlaxUNet2DConditionModel`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_2d_condition_flax.py#L48)

```py
( sample_size: int = 32 in_channels: int = 4 out_channels: int = 4 down_block_types: Tuple = ('CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D') up_block_types: Tuple = ('UpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D') only_cross_attention: Union = False block_out_channels: Tuple = (320, 640, 1280, 1280) layers_per_block: int = 2 attention_head_dim: Union = 8 num_attention_heads: Union = None cross_attention_dim: int = 1280 dropout: float = 0.0 use_linear_projection: bool = False dtype: dtype = <class 'jax.numpy.float32'> flip_sin_to_cos: bool = True freq_shift: int = 0 use_memory_efficient_attention: bool = False split_head_dim: bool = False transformer_layers_per_block: Union = 1 addition_embed_type: Optional = None addition_time_embed_dim: Optional = None addition_embed_type_num_heads: int = 64 projection_class_embeddings_input_dim: Optional = None parent: Union = <flax.linen.module._Sentinel object at 0x7fbd557629e0> name: Optional = None )
```

参数

+   `sample_size` (`int`, *可选*) — 输入样本的大小。

+   `in_channels` (`int`, *可选*, 默认为 4) — 输入样本中的通道数。

+   `out_channels` (`int`, *可选*, 默认为 4) — 输出中的通道数。

+   `down_block_types` (`Tuple[str]`, *可选*, 默认为`("FlaxCrossAttnDownBlock2D", "FlaxCrossAttnDownBlock2D", "FlaxCrossAttnDownBlock2D", "FlaxDownBlock2D")`) — 要使用的下采样块的元组。

+   `up_block_types` (`Tuple[str]`, *可选*, 默认为`("FlaxUpBlock2D", "FlaxCrossAttnUpBlock2D", "FlaxCrossAttnUpBlock2D", "FlaxCrossAttnUpBlock2D")`) — 要使用的上采样块的元组。

+   `block_out_channels` (`Tuple[int]`, *可选*, 默认为`(320, 640, 1280, 1280)`) — 每个块的输出通道的元组。

+   `layers_per_block` (`int`, *可选*, 默认为 2) — 每个块的层数。

+   `attention_head_dim` (`int`或`Tuple[int]`, *可选*, 默认为 8) — 注意力头的维度。

+   `num_attention_heads` (`int`或`Tuple[int]`, *可选*) — 注意力头的数量。

+   `cross_attention_dim` (`int`, *可选*, 默认为 768) — 交叉注意力特征的维度。

+   `dropout` (`float`, *可选*, 默认为 0) — 下采样、上采样和瓶颈块的丢弃概率。

+   `flip_sin_to_cos` (`bool`, *可选*, 默认为`True`) — 是否在时间嵌入中将 sin 翻转为 cos。

+   `freq_shift` (`int`, *可选*, 默认为 0) — 应用于时间嵌入的频率偏移。

+   `use_memory_efficient_attention` (`bool`, *可选*, 默认为`False`) — 启用内存高效的注意力，如[此处](https://arxiv.org/abs/2112.05682)所述。

+   `split_head_dim` (`bool`, *可选*, 默认为`False`) — 是否将头维度拆分为新轴以进行自注意力计算。在大多数情况下，启用此标志应该加速 Stable Diffusion 2.x 和 Stable Diffusion XL 的计算。

一个有条件的 2D UNet 模型，接受一个嘈杂的样本、条件状态和一个时间步，并返回一个形状为样本的输出。

此模型继承自 FlaxModelMixin。查看超类文档以了解为所有模型实现的通用方法（如下载或保存）。

此模型也是 Flax Linen [flax.linen.Module](https://flax.readthedocs.io/en/latest/flax.linen.html#module)的子类。将其用作常规的 Flax Linen 模块，并参考 Flax 文档以了解与其一般使用和行为相关的所有事项。

支持诸如以下的内在 JAX 特性：

+   [即时（JIT）编译](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [向量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

## FlaxUNet2DConditionOutput

### `class diffusers.models.unets.unet_2d_condition_flax.FlaxUNet2DConditionOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_2d_condition_flax.py#L35)

```py
( sample: Array )
```

参数

+   `sample`（形状为`(batch_size, num_channels, height, width)`的`jnp.ndarray`）- 在`encoder_hidden_states`输入条件下输出的隐藏状态。模型的最后一层的输出。

FlaxUNet2DConditionModel 的输出。

#### `replace`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/flax/struct.py#L111)

```py
( **updates )
```

“返回一个用新值替换指定字段的新对象。
