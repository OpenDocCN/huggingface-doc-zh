# æ–‡æœ¬åˆ°è§†é¢‘

> åŸæ–‡é“¾æ¥ï¼š[`huggingface.co/docs/diffusers/api/pipelines/text_to_video`](https://huggingface.co/docs/diffusers/api/pipelines/text_to_video)

ğŸ§ª è¿™ä¸ªæµç¨‹ä»…ç”¨äºç ”ç©¶ç›®çš„ã€‚

[ModelScope æ–‡æœ¬åˆ°è§†é¢‘æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2308.06571)ç”±ç‹ä¹…ç‰›ã€è¢èˆªæ°ã€é™ˆå¤§æœ‰ã€å¼ é¢–é›…ã€ç‹ç¿”ã€å¼ ä¸–ä¼Ÿæ’°å†™ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*æœ¬æ–‡ä»‹ç»äº† ModelScopeT2Vï¼Œè¿™æ˜¯ä¸€ä¸ªä»æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ¨¡å‹ï¼ˆå³ç¨³å®šæ‰©æ•£ï¼‰å‘å±•è€Œæ¥çš„æ–‡æœ¬åˆ°è§†é¢‘åˆæˆæ¨¡å‹ã€‚ModelScopeT2V ç»“åˆäº†æ—¶ç©ºå—ï¼Œä»¥ç¡®ä¿ä¸€è‡´çš„å¸§ç”Ÿæˆå’Œå¹³æ»‘çš„è¿åŠ¨è¿‡æ¸¡ã€‚è¯¥æ¨¡å‹å¯ä»¥é€‚åº”è®­ç»ƒå’Œæ¨æ–­è¿‡ç¨‹ä¸­ä¸åŒçš„å¸§æ•°ï¼Œé€‚ç”¨äºå›¾åƒæ–‡æœ¬å’Œè§†é¢‘æ–‡æœ¬æ•°æ®é›†ã€‚ModelScopeT2V æ±‡é›†äº†ä¸‰ä¸ªç»„ä»¶ï¼ˆå³ VQGANã€æ–‡æœ¬ç¼–ç å™¨å’Œå»å™ª UNetï¼‰ï¼Œæ€»å…±åŒ…å« 17 äº¿ä¸ªå‚æ•°ï¼Œå…¶ä¸­ 5 äº¿ä¸ªå‚æ•°ä¸“é—¨ç”¨äºæ—¶é—´èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨ä¸‰ä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ä»£ç å’Œåœ¨çº¿æ¼”ç¤ºå¯åœ¨[`modelscope.cn/models/damo/text-to-video-synthesis/summary`](https://modelscope.cn/models/damo/text-to-video-synthesis/summary)æ‰¾åˆ°ã€‚*

æ‚¨å¯ä»¥åœ¨[é¡¹ç›®é¡µé¢](https://modelscope.cn/models/damo/text-to-video-synthesis/summary)ã€[åŸå§‹ä»£ç åº“](https://github.com/modelscope/modelscope/)å’Œ[æ¼”ç¤º](https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis)ä¸­æ‰¾åˆ°æœ‰å…³æ–‡æœ¬åˆ°è§†é¢‘çš„å…¶ä»–ä¿¡æ¯ã€‚å®˜æ–¹æ£€æŸ¥ç‚¹å¯ä»¥åœ¨[damo-vilab](https://huggingface.co/damo-vilab)å’Œ[cerspense](https://huggingface.co/cerspense)æ‰¾åˆ°ã€‚

## ä½¿ç”¨ç¤ºä¾‹

### text-to-video-ms-1.7b

è®©æˆ‘ä»¬ä»ç”Ÿæˆé»˜è®¤é•¿åº¦ä¸º 16 å¸§ï¼ˆ8 fps ä¸‹çš„ 2 ç§’ï¼‰çš„çŸ­è§†é¢‘å¼€å§‹ï¼š

```py
import torch
from diffusers import DiffusionPipeline
from diffusers.utils import export_to_video

pipe = DiffusionPipeline.from_pretrained("damo-vilab/text-to-video-ms-1.7b", torch_dtype=torch.float16, variant="fp16")
pipe = pipe.to("cuda")

prompt = "Spiderman is surfing"
video_frames = pipe(prompt).frames
video_path = export_to_video(video_frames)
video_path
```

Diffusers æ”¯æŒä¸åŒçš„ä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥æ”¹å–„æµç¨‹çš„å»¶è¿Ÿå’Œå†…å­˜å ç”¨ã€‚ç”±äºè§†é¢‘é€šå¸¸æ¯”å›¾åƒæ›´å ç”¨å†…å­˜ï¼Œæˆ‘ä»¬å¯ä»¥å¯ç”¨ CPU å¸è½½å’Œ VAE åˆ‡ç‰‡æ¥æ§åˆ¶å†…å­˜å ç”¨ã€‚

è®©æˆ‘ä»¬åœ¨åŒä¸€å° GPU ä¸Šä½¿ç”¨ CPU å¸è½½å’Œ VAE åˆ‡ç‰‡ç”Ÿæˆ 8 ç§’ï¼ˆ64 å¸§ï¼‰çš„è§†é¢‘ï¼š

```py
import torch
from diffusers import DiffusionPipeline
from diffusers.utils import export_to_video

pipe = DiffusionPipeline.from_pretrained("damo-vilab/text-to-video-ms-1.7b", torch_dtype=torch.float16, variant="fp16")
pipe.enable_model_cpu_offload()

# memory optimization
pipe.enable_vae_slicing()

prompt = "Darth Vader surfing a wave"
video_frames = pipe(prompt, num_frames=64).frames
video_path = export_to_video(video_frames)
video_path
```

ä½¿ç”¨ PyTorch 2.0ï¼Œâ€œfp16â€ç²¾åº¦å’Œä¸Šè¿°æŠ€æœ¯ç”Ÿæˆ 64 ä¸ªè§†é¢‘å¸§ä»…éœ€è¦**7 GB çš„ GPU å†…å­˜**ã€‚

æˆ‘ä»¬ä¹Ÿå¯ä»¥è½»æ¾åœ°ä½¿ç”¨ä¸åŒçš„è°ƒåº¦å™¨ï¼Œä½¿ç”¨ä¸ç¨³å®šæ‰©æ•£ç›¸åŒçš„æ–¹æ³•ï¼š

```py
import torch
from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler
from diffusers.utils import export_to_video

pipe = DiffusionPipeline.from_pretrained("damo-vilab/text-to-video-ms-1.7b", torch_dtype=torch.float16, variant="fp16")
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()

prompt = "Spiderman is surfing"
video_frames = pipe(prompt, num_inference_steps=25).frames
video_path = export_to_video(video_frames)
video_path
```

ä»¥ä¸‹æ˜¯ä¸€äº›ç¤ºä¾‹è¾“å‡ºï¼š

| ä¸€åå®‡èˆªå‘˜éª‘é©¬ã€‚![ä¸€åå®‡èˆªå‘˜éª‘é©¬ã€‚](img/7624b329ee96da305cc8cda11bf80fae.png) | è¾¾æ–¯ç»´è¾¾åœ¨æ³¢æµªä¸­å†²æµªã€‚![è¾¾æ–¯ç»´è¾¾åœ¨æ³¢æµªä¸­å†²æµªã€‚](img/fa05339350fb9cb455c843e7f524e44d.png) |
| --- | --- |

### cerspense/zeroscope_v2_576w å’Œ cerspense/zeroscope_v2_XL

Zeroscope æ˜¯æ— æ°´å°æ¨¡å‹ï¼Œå·²ç»åœ¨ç‰¹å®šå°ºå¯¸ï¼ˆå¦‚`576x320`å’Œ`1024x576`ï¼‰ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚é¦–å…ˆåº”ä½¿ç”¨è¾ƒä½åˆ†è¾¨ç‡æ£€æŸ¥ç‚¹[`cerspense/zeroscope_v2_576w`](https://huggingface.co/cerspense/zeroscope_v2_576w)å’Œ TextToVideoSDPipeline ç”Ÿæˆè§†é¢‘ï¼Œç„¶åå¯ä»¥ä½¿ç”¨ VideoToVideoSDPipeline å’Œ[`cerspense/zeroscope_v2_XL`](https://huggingface.co/cerspense/zeroscope_v2_XL)è¿›è¡Œæ”¾å¤§ã€‚

```py
import torch
from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler
from diffusers.utils import export_to_video
from PIL import Image

pipe = DiffusionPipeline.from_pretrained("cerspense/zeroscope_v2_576w", torch_dtype=torch.float16)
pipe.enable_model_cpu_offload()

# memory optimization
pipe.unet.enable_forward_chunking(chunk_size=1, dim=1)
pipe.enable_vae_slicing()

prompt = "Darth Vader surfing a wave"
video_frames = pipe(prompt, num_frames=24).frames
video_path = export_to_video(video_frames)
video_path
```

ç°åœ¨è§†é¢‘å¯ä»¥è¿›è¡Œæ”¾å¤§ï¼š

```py
pipe = DiffusionPipeline.from_pretrained("cerspense/zeroscope_v2_XL", torch_dtype=torch.float16)
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()

# memory optimization
pipe.unet.enable_forward_chunking(chunk_size=1, dim=1)
pipe.enable_vae_slicing()

video = [Image.fromarray(frame).resize((1024, 576)) for frame in video_frames]

video_frames = pipe(prompt, video=video, strength=0.6).frames
video_path = export_to_video(video_frames)
video_path
```

ä»¥ä¸‹æ˜¯ä¸€äº›ç¤ºä¾‹è¾“å‡ºï¼š

| è¾¾æ–¯ç»´è¾¾åœ¨æ³¢æµªä¸­å†²æµªã€‚![è¾¾æ–¯ç»´è¾¾åœ¨æ³¢æµªä¸­å†²æµªã€‚](img/aab508b5ff9217f4f86b14a309983b1d.png) |
| --- |

ç¡®ä¿æŸ¥çœ‹è°ƒåº¦å™¨æŒ‡å—ä»¥äº†è§£å¦‚ä½•æ¢ç´¢è°ƒåº¦å™¨é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æŸ¥çœ‹åœ¨æµç¨‹ä¹‹é—´é‡ç”¨ç»„ä»¶éƒ¨åˆ†ï¼Œä»¥äº†è§£å¦‚ä½•æœ‰æ•ˆåœ°å°†ç›¸åŒç»„ä»¶åŠ è½½åˆ°å¤šä¸ªæµç¨‹ä¸­ã€‚

## TextToVideoSDPipeline

### `class diffusers.TextToVideoSDPipeline`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L84)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet3DConditionModel scheduler: KarrasDiffusionSchedulers )
```

å‚æ•°

+   `vae` (AutoencoderKL) â€” å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ¨¡å‹ï¼Œç”¨äºå°†å›¾åƒç¼–ç å’Œè§£ç ä¸ºæ½œåœ¨è¡¨ç¤ºã€‚

+   `text_encoder` (`CLIPTextModel`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚

+   `tokenizer` (`CLIPTokenizer`) â€” ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)ã€‚

+   `unet` (UNet3DConditionModel) â€” ç”¨äºå»å™ªç¼–ç è§†é¢‘æ½œåœ¨è¡¨ç¤ºçš„ UNet3DConditionModelã€‚

+   `scheduler` (SchedulerMixin) â€” ä¸`unet`ç»“åˆä½¿ç”¨çš„è°ƒåº¦å™¨ï¼Œç”¨äºå»å™ªç¼–ç å›¾åƒæ½œåœ¨è¡¨ç¤ºã€‚å¯ä»¥æ˜¯ DDIMSchedulerã€LMSDiscreteScheduler æˆ– PNDMScheduler ä¹‹ä¸€ã€‚

æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„ç®¡é“ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª DiffusionPipelineã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

è¯¥ç®¡é“è¿˜ç»§æ‰¿äº†ä»¥ä¸‹åŠ è½½æ–¹æ³•ï¼š

+   load_textual_inversion() ç”¨äºåŠ è½½æ–‡æœ¬åæ¼”åµŒå…¥

+   load_lora_weights() ç”¨äºåŠ è½½ LoRA æƒé‡

+   save_lora_weights() ç”¨äºä¿å­˜ LoRA æƒé‡

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L527)

```py
( prompt: Union = None height: Optional = None width: Optional = None num_frames: int = 16 num_inference_steps: int = 50 guidance_scale: float = 9.0 negative_prompt: Union = None eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'np' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None clip_skip: Optional = None ) â†’ export const metadata = 'undefined';TextToVideoSDPipelineOutput or tuple
```

å‚æ•°

+   `prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’`prompt_embeds`ã€‚

+   `height` (`int`, *å¯é€‰*, é»˜è®¤ä¸º`self.unet.config.sample_size * self.vae_scale_factor`) â€” ç”Ÿæˆè§†é¢‘çš„åƒç´ é«˜åº¦ã€‚

+   `width` (`int`, *å¯é€‰*, é»˜è®¤ä¸º`self.unet.config.sample_size * self.vae_scale_factor`) â€” ç”Ÿæˆè§†é¢‘çš„åƒç´ å®½åº¦ã€‚

+   `num_frames` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 16) â€” ç”Ÿæˆçš„è§†é¢‘å¸§æ•°ã€‚é»˜è®¤ä¸º 16 å¸§ï¼Œæ¯ç§’ 8 å¸§ï¼Œç›¸å½“äº 2 ç§’çš„è§†é¢‘ã€‚

+   `num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 50) â€” å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„è§†é¢‘ï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚

+   `guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 7.5) â€” æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬`prompt`ç´§å¯†ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å½“`guidance_scale > 1`æ—¶å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚

+   `negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆä¸­ä¸åŒ…å«çš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’`negative_prompt_embeds`ã€‚å½“ä¸ä½¿ç”¨å¼•å¯¼æ—¶ï¼ˆ`guidance_scale < 1`ï¼‰å°†è¢«å¿½ç•¥ã€‚

+   `num_images_per_prompt` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `eta` (`float`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.0) â€” å¯¹åº”äº[DDIM](https://arxiv.org/abs/2010.02502)è®ºæ–‡ä¸­çš„å‚æ•° eta (Î·)ã€‚ä»…é€‚ç”¨äº DDIMSchedulerï¼Œåœ¨å…¶ä»–è°ƒåº¦ç¨‹åºä¸­å°†è¢«å¿½ç•¥ã€‚

+   `generator` (`torch.Generator`æˆ–`List[torch.Generator]`, *å¯é€‰*) â€” ç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§çš„[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)ã€‚

+   `latents` (`torch.FloatTensor`, *å¯é€‰*) â€” ä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·çš„é¢„ç”Ÿæˆå™ªå£°æ½œå˜é‡ï¼Œç”¨ä½œè§†é¢‘ç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºå¾®è°ƒç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™é€šè¿‡ä½¿ç”¨æä¾›çš„éšæœº`generator`è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚æ½œå˜é‡åº”è¯¥å…·æœ‰å½¢çŠ¶`(batch_size, num_channel, num_frames, height, width)`ã€‚

+   `prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾å¾®è°ƒæ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œæ–‡æœ¬åµŒå…¥å°†ä»`prompt`è¾“å…¥å‚æ•°ç”Ÿæˆã€‚

+   `negative_prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾å¾®è°ƒæ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œ`negative_prompt_embeds`å°†ä»`negative_prompt`è¾“å…¥å‚æ•°ç”Ÿæˆã€‚

+   `output_type` (`str`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º`"np"`) â€” ç”Ÿæˆè§†é¢‘çš„è¾“å‡ºæ ¼å¼ã€‚é€‰æ‹©`torch.FloatTensor`æˆ–`np.array`ä¹‹é—´ã€‚

+   `return_dict` (`bool`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`) â€” æ˜¯å¦è¿”å› TextToVideoSDPipelineOutput è€Œä¸æ˜¯æ™®é€šçš„ tupleã€‚

+   `callback` (`Callable`, *å¯é€‰*) â€” åœ¨æ¨æ–­è¿‡ç¨‹ä¸­æ¯`callback_steps`æ­¥è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback(step: int, timestep: int, latents: torch.FloatTensor)`ã€‚

+   `callback_steps` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 1) â€” è°ƒç”¨`callback`å‡½æ•°çš„é¢‘ç‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œåˆ™åœ¨æ¯ä¸€æ­¥è°ƒç”¨å›è°ƒå‡½æ•°ã€‚

+   `cross_attention_kwargs` (`dict`, *å¯é€‰*) â€” å¦‚æœæŒ‡å®šï¼Œå°†ä¼ é€’ç»™`AttentionProcessor`çš„ kwargs å­—å…¸ï¼Œå¦‚[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)ä¸­å®šä¹‰çš„ã€‚

+   `clip_skip` (`int`, *å¯é€‰*) â€” åœ¨è®¡ç®—æç¤ºåµŒå…¥æ—¶è¦ä» CLIP è·³è¿‡çš„å±‚æ•°ã€‚å€¼ä¸º 1 æ„å‘³ç€å°†ä½¿ç”¨é¢„æœ€ç»ˆå±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚

è¿”å›

TextToVideoSDPipelineOutput æˆ– `tuple`

å¦‚æœ`return_dict`ä¸º`True`ï¼Œåˆ™è¿”å› TextToVideoSDPipelineOutputï¼Œå¦åˆ™è¿”å›ä¸€ä¸ª`tuple`ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ç”Ÿæˆå¸§çš„åˆ—è¡¨ã€‚

ç”¨äºç”Ÿæˆçš„ç®¡é“çš„è°ƒç”¨å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from diffusers import TextToVideoSDPipeline
>>> from diffusers.utils import export_to_video

>>> pipe = TextToVideoSDPipeline.from_pretrained(
...     "damo-vilab/text-to-video-ms-1.7b", torch_dtype=torch.float16, variant="fp16"
... )
>>> pipe.enable_model_cpu_offload()

>>> prompt = "Spiderman is surfing"
>>> video_frames = pipe(prompt).frames
>>> video_path = export_to_video(video_frames)
>>> video_path
```

#### `disable_freeu`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L523)

```py
( )
```

å¦‚æœå¯ç”¨ï¼Œåˆ™ç¦ç”¨ FreeU æœºåˆ¶ã€‚

#### `disable_vae_slicing`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L141)

```py
( )
```

ç¦ç”¨åˆ‡ç‰‡çš„ VAE è§£ç ã€‚å¦‚æœä¹‹å‰å¯ç”¨äº†`enable_vae_slicing`ï¼Œåˆ™æ­¤æ–¹æ³•å°†è¿”å›åˆ°ä¸€æ­¥è®¡ç®—è§£ç ã€‚

#### `disable_vae_tiling`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L158)

```py
( )
```

ç¦ç”¨å¹³é“ºå¼ VAE è§£ç ã€‚å¦‚æœä¹‹å‰å¯ç”¨äº†`enable_vae_tiling`ï¼Œåˆ™æ­¤æ–¹æ³•å°†è¿”å›åˆ°ä¸€æ­¥è®¡ç®—è§£ç ã€‚

#### `enable_freeu`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L500)

```py
( s1: float s2: float b1: float b2: float )
```

å‚æ•°

+   `s1` (`float`) â€” ç¬¬ 1 é˜¶æ®µçš„ç¼©æ”¾å› å­ï¼Œç”¨äºå‡å¼±è·³è¿‡ç‰¹å¾çš„è´¡çŒ®ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†å‡è½»å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­çš„â€œè¿‡åº¦å¹³æ»‘æ•ˆåº”â€ã€‚

+   `s2` (`float`) â€” ç¬¬ 2 é˜¶æ®µçš„ç¼©æ”¾å› å­ï¼Œç”¨äºå‡å¼±è·³è¿‡ç‰¹å¾çš„è´¡çŒ®ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†å‡è½»å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­çš„â€œè¿‡åº¦å¹³æ»‘æ•ˆåº”â€ã€‚

+   `b1` (`float`) â€” ç¬¬ 1 é˜¶æ®µçš„ç¼©æ”¾å› å­ï¼Œç”¨äºæ”¾å¤§éª¨å¹²ç‰¹å¾çš„è´¡çŒ®ã€‚

+   `b2` (`float`) â€” ç¬¬ 2 é˜¶æ®µçš„ç¼©æ”¾å› å­ï¼Œç”¨äºæ”¾å¤§éª¨å¹²ç‰¹å¾çš„è´¡çŒ®ã€‚

å¯ç”¨ FreeU æœºåˆ¶ï¼Œå¦‚[`arxiv.org/abs/2309.11497`](https://arxiv.org/abs/2309.11497)ã€‚

ç¼©æ”¾å› å­åç¼€è¡¨ç¤ºå®ƒä»¬è¢«åº”ç”¨çš„é˜¶æ®µã€‚

è¯·å‚è€ƒ[å®˜æ–¹å­˜å‚¨åº“](https://github.com/ChenyangSi/FreeU)ï¼Œäº†è§£å·²çŸ¥é€‚ç”¨äºä¸åŒç®¡é“ï¼ˆå¦‚ Stable Diffusion v1ã€v2 å’Œ Stable Diffusion XLï¼‰çš„å€¼ç»„åˆã€‚

#### `enable_vae_slicing`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L133)

```py
( )
```

å¯ç”¨åˆ‡ç‰‡å¼ VAE è§£ç ã€‚å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼ŒVAE å°†å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆç‰‡æ®µï¼Œä»¥ä¾¿åœ¨å‡ ä¸ªæ­¥éª¤ä¸­è®¡ç®—è§£ç ã€‚è¿™å¯¹äºèŠ‚çœä¸€äº›å†…å­˜å¹¶å…è®¸æ›´å¤§çš„æ‰¹é‡å¤§å°éå¸¸æœ‰ç”¨ã€‚

#### `enable_vae_tiling`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L149)

```py
( )
```

å¯ç”¨å¹³é“ºå¼ VAE è§£ç ã€‚å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼ŒVAE å°†å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆå¤šä¸ªç“¦ç‰‡ï¼Œä»¥ä¾¿åœ¨å‡ ä¸ªæ­¥éª¤ä¸­è®¡ç®—è§£ç å’Œç¼–ç ã€‚è¿™å¯¹äºèŠ‚çœå¤§é‡å†…å­˜å¹¶å…è®¸å¤„ç†æ›´å¤§çš„å›¾åƒéå¸¸æœ‰ç”¨ã€‚

#### `encode_prompt`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py#L199)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

å‚æ•°

+   `prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” è¦ç¼–ç çš„æç¤º è®¾å¤‡ â€” (`torch.device`): torch è®¾å¤‡

+   `num_images_per_prompt` (`int`) â€” æ¯ä¸ªæç¤ºåº”ç”Ÿæˆçš„å›¾åƒæ•°é‡

+   `do_classifier_free_guidance` (`bool`) â€” æ˜¯å¦ä½¿ç”¨åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼

+   `negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸ç”¨æ¥æŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™å¿…é¡»ä¼ é€’`negative_prompt_embeds`ã€‚åœ¨ä¸ä½¿ç”¨æŒ‡å¯¼æ—¶è¢«å¿½ç•¥ï¼ˆå³å¦‚æœ`guidance_scale`å°äº`1`ï¼Œåˆ™è¢«å¿½ç•¥ï¼‰ã€‚

+   `prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»`prompt`è¾“å…¥å‚æ•°ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚

+   `negative_prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»`negative_prompt`è¾“å…¥å‚æ•°ç”Ÿæˆ`negative_prompt_embeds`ã€‚

+   `lora_scale` (`float`, *å¯é€‰*) â€” å¦‚æœåŠ è½½äº† LoRA å±‚ï¼Œåˆ™å°†åº”ç”¨äºæ–‡æœ¬ç¼–ç å™¨çš„æ‰€æœ‰ LoRA å±‚çš„ LoRA æ¯”ä¾‹ã€‚

+   `clip_skip` (`int`, *å¯é€‰*) â€” ä» CLIP ä¸­è·³è¿‡çš„å±‚æ•°ï¼Œç”¨äºè®¡ç®—æç¤ºåµŒå…¥ã€‚å€¼ä¸º 1 æ„å‘³ç€å°†ä½¿ç”¨é¢„æœ€ç»ˆå±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚

å°†æç¤ºç¼–ç ä¸ºæ–‡æœ¬ç¼–ç å™¨éšè—çŠ¶æ€ã€‚

## VideoToVideoSDPipeline

### `class diffusers.VideoToVideoSDPipeline`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L160)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet3DConditionModel scheduler: KarrasDiffusionSchedulers )
```

å‚æ•°

+   `vae`ï¼ˆAutoencoderKLï¼‰â€” å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ¨¡å‹ï¼Œç”¨äºå°†è§†é¢‘ç¼–ç å’Œè§£ç ä¸ºæ½œåœ¨è¡¨ç¤ºã€‚

+   `text_encoder`ï¼ˆ`CLIPTextModel`ï¼‰â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚

+   `tokenizer`ï¼ˆ`CLIPTokenizer`ï¼‰â€” ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)ã€‚

+   `unet`ï¼ˆUNet3DConditionModelï¼‰â€” ç”¨äºå»å™ªç¼–ç è§†é¢‘æ½œåœ¨ç‰¹å¾çš„ UNet3DConditionModelã€‚

+   `scheduler`ï¼ˆSchedulerMixinï¼‰â€” ç”¨äºä¸`unet`ç»“åˆä½¿ç”¨ä»¥å»å™ªç¼–ç å›¾åƒæ½œåœ¨ç‰¹å¾çš„è°ƒåº¦å™¨ã€‚å¯ä»¥æ˜¯ DDIMSchedulerã€LMSDiscreteScheduler æˆ– PNDMScheduler ä¹‹ä¸€ã€‚

ç”¨äºæ–‡æœ¬å¼•å¯¼çš„è§†é¢‘åˆ°è§†é¢‘ç”Ÿæˆçš„æµæ°´çº¿ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª DiffusionPipelineã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–æ‰€æœ‰æµæ°´çº¿å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

è¯¥æµæ°´çº¿è¿˜ç»§æ‰¿äº†ä»¥ä¸‹åŠ è½½æ–¹æ³•ï¼š

+   load_textual_inversion() ç”¨äºåŠ è½½æ–‡æœ¬åæ¼”åµŒå…¥

+   load_lora_weights() ç”¨äºåŠ è½½ LoRA æƒé‡

+   save_lora_weights() ç”¨äºä¿å­˜ LoRA æƒé‡

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L632)

```py
( prompt: Union = None video: Union = None strength: float = 0.6 num_inference_steps: int = 50 guidance_scale: float = 15.0 negative_prompt: Union = None eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'np' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None clip_skip: Optional = None ) â†’ export const metadata = 'undefined';TextToVideoSDPipelineOutput or tuple
```

å‚æ•°

+   `prompt`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’`prompt_embeds`ã€‚

+   `video`ï¼ˆ`List[np.ndarray]`æˆ–`torch.FloatTensor`ï¼‰â€” `video`å¸§æˆ–è¡¨ç¤ºè§†é¢‘æ‰¹æ¬¡çš„å¼ é‡ï¼Œç”¨ä½œè¿‡ç¨‹çš„èµ·ç‚¹ã€‚è¿˜å¯ä»¥æ¥å—è§†é¢‘æ½œåœ¨ç‰¹å¾ä½œä¸º`image`ï¼Œå¦‚æœç›´æ¥ä¼ é€’æ½œåœ¨ç‰¹å¾ï¼Œåˆ™ä¸ä¼šå†æ¬¡ç¼–ç ã€‚

+   `strength`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.8ï¼‰â€” è¡¨ç¤ºè½¬æ¢å‚è€ƒ`video`çš„ç¨‹åº¦ã€‚å¿…é¡»ä»‹äº 0 å’Œ 1 ä¹‹é—´ã€‚`video`ç”¨ä½œèµ·ç‚¹ï¼Œæ·»åŠ çš„å™ªéŸ³è¶Šå¤šï¼Œ`strength`è¶Šå¤§ã€‚å»å™ªæ­¥éª¤çš„æ•°é‡å–å†³äºæœ€åˆæ·»åŠ çš„å™ªéŸ³é‡ã€‚å½“`strength`ä¸º 1 æ—¶ï¼Œæ·»åŠ çš„å™ªéŸ³æœ€å¤§ï¼Œå»å™ªè¿‡ç¨‹å°†è¿è¡ŒæŒ‡å®šçš„`num_inference_steps`çš„å…¨éƒ¨è¿­ä»£æ¬¡æ•°ã€‚å€¼ä¸º 1 åŸºæœ¬ä¸Šå¿½ç•¥`video`ã€‚

+   `num_inference_steps`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 50ï¼‰â€” å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„è§†é¢‘ï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚

+   `guidance_scale`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 7.5ï¼‰â€” æ›´é«˜çš„æŒ‡å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬`prompt`ç´§å¯†ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å½“`guidance_scale > 1`æ—¶å¯ç”¨æŒ‡å¯¼æ¯”ä¾‹ã€‚

+   `negative_prompt`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºæŒ‡å¯¼åœ¨è§†é¢‘ç”Ÿæˆä¸­ä¸åŒ…æ‹¬çš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’`negative_prompt_embeds`ã€‚åœ¨ä¸ä½¿ç”¨æŒ‡å¯¼æ—¶è¢«å¿½ç•¥ï¼ˆ`guidance_scale < 1`ï¼‰ã€‚

+   `eta` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.0) â€” å¯¹åº”äº[DDIM](https://arxiv.org/abs/2010.02502)è®ºæ–‡ä¸­çš„å‚æ•° eta (Î·)ã€‚ä»…é€‚ç”¨äº DDIMSchedulerï¼Œåœ¨å…¶ä»–è°ƒåº¦ç¨‹åºä¸­å°†è¢«å¿½ç•¥ã€‚

+   `generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *å¯é€‰*) â€” ä¸€ä¸ª[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)ï¼Œç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚

+   `latents` (`torch.FloatTensor`, *å¯é€‰*) â€” ä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·çš„é¢„ç”Ÿæˆå™ªå£°æ½œå˜é‡ï¼Œç”¨ä½œè§†é¢‘ç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºå¾®è°ƒç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™é€šè¿‡ä½¿ç”¨æä¾›çš„éšæœº`generator`è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚æ½œå˜é‡åº”è¯¥å…·æœ‰å½¢çŠ¶`(batch_size, num_channel, num_frames, height, width)`ã€‚

+   `prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾å¾®è°ƒæ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œæ–‡æœ¬åµŒå…¥å°†ä»`prompt`è¾“å…¥å‚æ•°ç”Ÿæˆã€‚

+   `negative_prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾å¾®è°ƒæ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œ`negative_prompt_embeds`å°†ä»`negative_prompt`è¾“å…¥å‚æ•°ç”Ÿæˆã€‚

+   `output_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`"np"`) â€” ç”Ÿæˆè§†é¢‘çš„è¾“å‡ºæ ¼å¼ã€‚é€‰æ‹©`torch.FloatTensor`æˆ–`np.array`ä¹‹é—´ã€‚

+   `return_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦è¿”å› TextToVideoSDPipelineOutput è€Œä¸æ˜¯æ™®é€šçš„ tupleã€‚

+   `callback` (`Callable`, *å¯é€‰*) â€” ä¸€ä¸ªåœ¨æ¨æ–­è¿‡ç¨‹ä¸­æ¯`callback_steps`æ­¥è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°æ¥å—ä»¥ä¸‹å‚æ•°ï¼š`callback(step: int, timestep: int, latents: torch.FloatTensor)`ã€‚

+   `callback_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” è°ƒç”¨`callback`å‡½æ•°çš„é¢‘ç‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†åœ¨æ¯ä¸€æ­¥è°ƒç”¨å›è°ƒã€‚

+   `cross_attention_kwargs` (`dict`, *å¯é€‰*) â€” ä¸€ä¸ª kwargs å­—å…¸ï¼Œå¦‚æœæŒ‡å®šäº†ï¼Œå°†ä¼ é€’ç»™[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)ä¸­å®šä¹‰çš„`AttentionProcessor`ã€‚

+   `clip_skip` (`int`, *å¯é€‰*) â€” åœ¨è®¡ç®—æç¤ºåµŒå…¥æ—¶è¦è·³è¿‡çš„ CLIP å±‚æ•°ã€‚å€¼ä¸º 1 è¡¨ç¤ºå°†ä½¿ç”¨é¢„ç»ˆå±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚

è¿”å›

TextToVideoSDPipelineOutput æˆ–`tuple`

å¦‚æœ`return_dict`ä¸º`True`ï¼Œå°†è¿”å› TextToVideoSDPipelineOutputï¼Œå¦åˆ™å°†è¿”å›ä¸€ä¸ª`tuple`ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ç”Ÿæˆçš„å¸§çš„åˆ—è¡¨ã€‚

ç”¨äºç”Ÿæˆçš„ç®¡é“çš„è°ƒç”¨å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler
>>> from diffusers.utils import export_to_video

>>> pipe = DiffusionPipeline.from_pretrained("cerspense/zeroscope_v2_576w", torch_dtype=torch.float16)
>>> pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
>>> pipe.to("cuda")

>>> prompt = "spiderman running in the desert"
>>> video_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames
>>> # safe low-res video
>>> video_path = export_to_video(video_frames, output_video_path="./video_576_spiderman.mp4")

>>> # let's offload the text-to-image model
>>> pipe.to("cpu")

>>> # and load the image-to-image model
>>> pipe = DiffusionPipeline.from_pretrained(
...     "cerspense/zeroscope_v2_XL", torch_dtype=torch.float16, revision="refs/pr/15"
... )
>>> pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
>>> pipe.enable_model_cpu_offload()

>>> # The VAE consumes A LOT of memory, let's make sure we run it in sliced mode
>>> pipe.vae.enable_slicing()

>>> # now let's upscale it
>>> video = [Image.fromarray(frame).resize((1024, 576)) for frame in video_frames]

>>> # and denoise it
>>> video_frames = pipe(prompt, video=video, strength=0.6).frames
>>> video_path = export_to_video(video_frames, output_video_path="./video_1024_spiderman.mp4")
>>> video_path
```

#### `disable_freeu`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L628)

```py
( )
```

å¦‚æœå¯ç”¨ï¼Œå°†ç¦ç”¨ FreeU æœºåˆ¶ã€‚

#### `disable_vae_slicing`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L217)

```py
( )
```

ç¦ç”¨åˆ‡ç‰‡ VAE è§£ç ã€‚å¦‚æœä¹‹å‰å¯ç”¨äº†`enable_vae_slicing`ï¼Œåˆ™æ­¤æ–¹æ³•å°†è¿”å›åˆ°ä¸€æ­¥è®¡ç®—è§£ç ã€‚

#### `disable_vae_tiling`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L234)

```py
( )
```

ç¦ç”¨å¹³é“º VAE è§£ç ã€‚å¦‚æœä¹‹å‰å¯ç”¨äº†`enable_vae_tiling`ï¼Œåˆ™æ­¤æ–¹æ³•å°†è¿”å›åˆ°ä¸€æ­¥è®¡ç®—è§£ç ã€‚

#### `enable_freeu`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L605)

```py
( s1: float s2: float b1: float b2: float )
```

å‚æ•°

+   `s1` (`float`) â€” ç”¨äºå‡å¼±è·³è¿‡ç‰¹å¾è´¡çŒ®çš„ç¬¬ 1 é˜¶æ®µçš„ç¼©æ”¾å› å­ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†åœ¨å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­å‡è½»â€œè¿‡åº¦å¹³æ»‘æ•ˆåº”â€ã€‚

+   `s2` (`float`) â€” ç”¨äºå‡å¼±è·³è¿‡ç‰¹å¾è´¡çŒ®çš„ç¬¬ 2 é˜¶æ®µçš„ç¼©æ”¾å› å­ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†åœ¨å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­å‡è½»â€œè¿‡åº¦å¹³æ»‘æ•ˆåº”â€ã€‚

+   `b1` (`float`) â€” ç”¨äºæ”¾å¤§ç¬¬ 1 é˜¶æ®µçš„éª¨å¹²ç‰¹å¾è´¡çŒ®çš„ç¼©æ”¾å› å­ã€‚

+   `b2` (`float`) â€” ç”¨äºæ”¾å¤§ç¬¬ 2 é˜¶æ®µçš„éª¨å¹²ç‰¹å¾è´¡çŒ®çš„ç¼©æ”¾å› å­ã€‚

å¯ç”¨ FreeU æœºåˆ¶ï¼Œå¦‚[`arxiv.org/abs/2309.11497`](https://arxiv.org/abs/2309.11497)ã€‚

ç¼©æ”¾å› å­åç¼€è¡¨ç¤ºåº”ç”¨å®ƒä»¬çš„é˜¶æ®µã€‚

è¯·å‚è€ƒ[å®˜æ–¹å­˜å‚¨åº“](https://github.com/ChenyangSi/FreeU)ä»¥è·å–å·²çŸ¥é€‚ç”¨äºä¸åŒç®¡é“ï¼ˆå¦‚ Stable Diffusion v1ã€v2 å’Œ Stable Diffusion XLï¼‰çš„å€¼ç»„åˆã€‚

#### `enable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L209)

```py
( )
```

å¯ç”¨åˆ‡ç‰‡ VAE è§£ç ã€‚å½“å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼ŒVAE å°†å°†è¾“å…¥å¼ é‡åˆ‡ç‰‡ä»¥ä¾¿åœ¨å‡ ä¸ªæ­¥éª¤ä¸­è®¡ç®—è§£ç ã€‚è¿™å¯¹äºèŠ‚çœä¸€äº›å†…å­˜å¹¶å…è®¸æ›´å¤§çš„æ‰¹é‡å¤§å°éå¸¸æœ‰ç”¨ã€‚

#### `enable_vae_tiling`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L225)

```py
( )
```

å¯ç”¨å¹³é“º VAE è§£ç ã€‚å½“å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼ŒVAE å°†å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆå¤šä¸ªç“¦ç‰‡ï¼Œä»¥ä¾¿åœ¨å‡ ä¸ªæ­¥éª¤ä¸­è®¡ç®—è§£ç å’Œç¼–ç ã€‚è¿™å¯¹äºèŠ‚çœå¤§é‡å†…å­˜å¹¶å…è®¸å¤„ç†æ›´å¤§çš„å›¾åƒéå¸¸æœ‰ç”¨ã€‚

#### `encode_prompt`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py#L275)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

å‚æ•°

+   `prompt` (`str` or `List[str]`, *optional*) â€” è¦ç¼–ç çš„æç¤ºè®¾å¤‡ â€” (`torch.device`): torch è®¾å¤‡

+   `num_images_per_prompt` (`int`) â€” åº”è¯¥ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡

+   `do_classifier_free_guidance` (`bool`) â€” æ˜¯å¦ä½¿ç”¨åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼

+   `negative_prompt` (`str` or `List[str]`, *optional*) â€” ä¸ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™å¿…é¡»ä¼ é€’`negative_prompt_embeds`ã€‚å¦‚æœä¸ä½¿ç”¨æŒ‡å¯¼ï¼ˆå³ï¼Œå¦‚æœ`guidance_scale`å°äº`1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œæ–‡æœ¬åµŒå…¥å°†ä»`prompt`è¾“å…¥å‚æ•°ç”Ÿæˆã€‚

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»`negative_prompt`è¾“å…¥å‚æ•°ç”Ÿæˆ negative_prompt_embedsã€‚

+   `lora_scale` (`float`, *optional*) â€” å¦‚æœåŠ è½½äº† LoRA å±‚ï¼Œåˆ™å°†åº”ç”¨äºæ–‡æœ¬ç¼–ç å™¨çš„æ‰€æœ‰ LoRA å±‚çš„ LoRA æ¯”ä¾‹ã€‚

+   `clip_skip` (`int`, *optional*) â€” ä» CLIP ä¸­è·³è¿‡çš„å±‚æ•°ã€‚å€¼ä¸º 1 è¡¨ç¤ºå°†ä½¿ç”¨é¢„æœ€ç»ˆå±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚

å°†æç¤ºç¼–ç ä¸ºæ–‡æœ¬ç¼–ç å™¨éšè—çŠ¶æ€ã€‚

## TextToVideoSDPipelineOutput

### `class diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_output.py#L12)

```py
( frames: Union )
```

å‚æ•°

+   `frames` (`List[np.ndarray]` æˆ– `torch.FloatTensor`) â€” ä¸€ç³»åˆ—å»å™ªå¸§ï¼ˆåŸºæœ¬ä¸Šæ˜¯å›¾åƒï¼‰ï¼Œä½œä¸ºå½¢çŠ¶ä¸º`(height, width, num_channels)`çš„ NumPy æ•°ç»„æˆ–`torch`å¼ é‡ã€‚åˆ—è¡¨çš„é•¿åº¦è¡¨ç¤ºè§†é¢‘é•¿åº¦ï¼ˆå¸§æ•°ï¼‰ã€‚

ç”¨äºæ–‡æœ¬åˆ°è§†é¢‘æµæ°´çº¿çš„è¾“å‡ºç±»ã€‚
