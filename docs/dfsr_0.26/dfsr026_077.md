# 加速推理

> 原始文本：[`huggingface.co/docs/diffusers/optimization/fp16`](https://huggingface.co/docs/diffusers/optimization/fp16)

有几种方法可以优化🤗 Diffusers 以提高推理速度。作为一个经验法则，我们建议在 PyTorch 2.0 中使用 xFormers 或`torch.nn.functional.scaled_dot_product_attention`进行内存高效的注意力。

在许多情况下，为了提高速度或内存而进行优化会导致另一方面的性能提升，因此您应该尽可能在两者上进行优化。本指南侧重于推理速度，但您可以在减少内存使用指南中了解更多关于保留内存的信息。

下面的结果是在 Nvidia Titan RTX 上使用 50 个 DDIM 步骤从提示`a photo of an astronaut riding a horse on mars`生成单个 512x512 图像获得的，展示了您可以期望的加速。

|  | 延迟 | 加速 |
| --- | --- | --- |
| 原始 | 9.50 秒 | x1 |
| fp16 | 3.61 秒 | x2.63 |
| 通道最后 | 3.30 秒 | x2.88 |
| 追踪的 UNet | 3.21 秒 | x2.96 |
| 内存高效的注意力 | 2.63 秒 | x3.61 |

## 使用 TensorFloat-32

在安培及更高版本的 CUDA 设备上，矩阵乘法和卷积可以使用[张量浮点 32（TF32）](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)模式进行更快但略微不准确的计算。默认情况下，PyTorch 为卷积启用 TF32 模式，但不为矩阵乘法启用。除非您的网络需要完整的 float32 精度，否则我们建议为矩阵乘法启用 TF32。它可以显着加快计算速度，通常几乎不会损失数值精度。

```py
import torch

torch.backends.cuda.matmul.allow_tf32 = True
```

您可以在[混合精度训练](https://huggingface.co/docs/transformers/en/perf_train_gpu_one#tf32)指南中了解有关 TF32 的更多信息。

## 半精度权重

为了节省 GPU 内存并获得更快速度，请尝试直接在半精度或 float16 中加载和运行模型权重：

```py
import torch
from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
image = pipe(prompt).images[0]
```

在任何流水线中不要使用[`torch.autocast`](https://pytorch.org/docs/stable/amp.html#torch.autocast)，因为它可能导致黑色图像，并且始终比纯 float16 精度慢。
