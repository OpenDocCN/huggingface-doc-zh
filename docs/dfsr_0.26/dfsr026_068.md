# LoRA

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/diffusers/training/lora`](https://huggingface.co/docs/diffusers/training/lora)

è¿™æ˜¯å®éªŒæ€§çš„ï¼ŒAPI å¯èƒ½ä¼šåœ¨å°†æ¥æ›´æ”¹ã€‚

[LoRAï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹çš„ä½ç§©é€‚åº”ï¼‰](https://hf.co/papers/2106.09685)æ˜¯ä¸€ç§æµè¡Œä¸”è½»é‡çº§çš„è®­ç»ƒæŠ€æœ¯ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚å®ƒé€šè¿‡å‘æ¨¡å‹æ’å…¥è¾ƒå°‘æ•°é‡çš„æ–°æƒé‡ï¼Œå¹¶ä»…å¯¹è¿™äº›æƒé‡è¿›è¡Œè®­ç»ƒæ¥å·¥ä½œã€‚è¿™ä½¿å¾—ä½¿ç”¨ LoRA è¿›è¡Œè®­ç»ƒæ›´å¿«é€Ÿã€å†…å­˜æ•ˆç‡æ›´é«˜ï¼Œå¹¶ä¸”äº§ç”Ÿæ›´å°çš„æ¨¡å‹æƒé‡ï¼ˆå‡ ç™¾ MBï¼‰ï¼Œæ›´å®¹æ˜“å­˜å‚¨å’Œå…±äº«ã€‚LoRA è¿˜å¯ä»¥ä¸å…¶ä»–è®­ç»ƒæŠ€æœ¯ï¼ˆå¦‚ DreamBoothï¼‰ç»“åˆä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚

LoRA éå¸¸çµæ´»ï¼Œå¹¶æ”¯æŒ[DreamBooth](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora.py)ã€[Kandinsky 2.2](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_lora_decoder.py)ã€[Stable Diffusion XL](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora_sdxl.py)ã€[text-to-image](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py)å’Œ[Wuerstchen](https://github.com/huggingface/diffusers/blob/main/examples/wuerstchen/text_to_image/train_text_to_image_lora_prior.py)ã€‚

è¿™ä¸ªæŒ‡å—å°†æ¢è®¨[train_text_to_image_lora.py](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py)è„šæœ¬ï¼Œå¸®åŠ©æ‚¨æ›´ç†Ÿæ‚‰å®ƒï¼Œä»¥åŠå¦‚ä½•ä¸ºè‡ªå·±çš„ç”¨ä¾‹è¿›è¡Œè°ƒæ•´ã€‚

åœ¨è¿è¡Œè„šæœ¬ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨ä»æºä»£ç å®‰è£…åº“ï¼š

```py
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install .
```

å¯¼èˆªåˆ°åŒ…å«è®­ç»ƒè„šæœ¬çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ï¼Œå¹¶å®‰è£…æ‚¨æ­£åœ¨ä½¿ç”¨çš„è„šæœ¬æ‰€éœ€çš„ä¾èµ–é¡¹ï¼š

PyTorchFlax

```py
cd examples/text_to_image
pip install -r requirements.txt
```

ğŸ¤— Accelerate æ˜¯ä¸€ä¸ªå¸®åŠ©æ‚¨åœ¨å¤šä¸ª GPU/TPU ä¸Šè¿›è¡Œè®­ç»ƒæˆ–ä½¿ç”¨æ··åˆç²¾åº¦çš„åº“ã€‚å®ƒå°†æ ¹æ®æ‚¨çš„ç¡¬ä»¶å’Œç¯å¢ƒè‡ªåŠ¨é…ç½®æ‚¨çš„è®­ç»ƒè®¾ç½®ã€‚æŸ¥çœ‹ğŸ¤— Accelerate [å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/accelerate/quicktour)ä»¥äº†è§£æ›´å¤šã€‚

åˆå§‹åŒ–ğŸ¤— Accelerate ç¯å¢ƒï¼š

```py
accelerate config
```

è¦è®¾ç½®é»˜è®¤çš„ğŸ¤— Accelerate ç¯å¢ƒè€Œä¸é€‰æ‹©ä»»ä½•é…ç½®ï¼š

```py
accelerate config default
```

æˆ–è€…å¦‚æœæ‚¨çš„ç¯å¢ƒä¸æ”¯æŒäº¤äº’å¼ shellï¼Œæ¯”å¦‚ç¬”è®°æœ¬ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ï¼š

```py
from accelerate.utils import write_basic_config

write_basic_config()
```

æœ€åï¼Œå¦‚æœæ‚¨æƒ³åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹åˆ›å»ºç”¨äºè®­ç»ƒçš„æ•°æ®é›†æŒ‡å—ï¼Œäº†è§£å¦‚ä½•åˆ›å»ºä¸è®­ç»ƒè„šæœ¬å…¼å®¹çš„æ•°æ®é›†ã€‚

ä»¥ä¸‹éƒ¨åˆ†çªå‡ºæ˜¾ç¤ºäº†è®­ç»ƒè„šæœ¬çš„ä¸€äº›é‡è¦éƒ¨åˆ†ï¼Œä»¥å¸®åŠ©æ‚¨äº†è§£å¦‚ä½•ä¿®æ”¹å®ƒï¼Œä½†å¹¶æœªè¯¦ç»†æ¶µç›–è„šæœ¬çš„æ¯ä¸ªæ–¹é¢ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£æ›´å¤šï¼Œè¯·éšæ—¶é˜…è¯»[è„šæœ¬](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/text_to_image_lora.py)ï¼Œå¹¶å‘Šè¯‰æˆ‘ä»¬æ‚¨æ˜¯å¦æœ‰ä»»ä½•é—®é¢˜æˆ–ç–‘è™‘ã€‚

## è„šæœ¬å‚æ•°

è®­ç»ƒè„šæœ¬æœ‰è®¸å¤šå‚æ•°å¯å¸®åŠ©æ‚¨è‡ªå®šä¹‰è®­ç»ƒè¿è¡Œã€‚æ‰€æœ‰å‚æ•°åŠå…¶æè¿°éƒ½å¯ä»¥åœ¨[`parse_args()`](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L85)å‡½æ•°ä¸­æ‰¾åˆ°ã€‚å¤§å¤šæ•°å‚æ•°éƒ½æä¾›äº†é»˜è®¤å€¼ï¼Œä½†å¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥åœ¨è®­ç»ƒå‘½ä»¤ä¸­è®¾ç½®è‡ªå·±çš„å€¼ã€‚

ä¾‹å¦‚ï¼Œè¦å¢åŠ è®­ç»ƒçš„æ—¶ä»£æ•°ï¼š

```py
accelerate launch train_text_to_image_lora.py \
  --num_train_epochs=150 \
```

è®¸å¤šåŸºæœ¬å’Œé‡è¦çš„å‚æ•°åœ¨ Text-to-image è®­ç»ƒæŒ‡å—ä¸­æœ‰æè¿°ï¼Œå› æ­¤æœ¬æŒ‡å—åªå…³æ³¨ä¸ LoRA ç›¸å…³çš„å‚æ•°ï¼š

+   `--rank`ï¼šè¦è®­ç»ƒçš„ä½ç§©çŸ©é˜µçš„å†…éƒ¨ç»´åº¦ï¼›æ›´é«˜çš„ç§©æ„å‘³ç€æ›´å¤šçš„å¯è®­ç»ƒå‚æ•°

+   `--learning_rate`ï¼šé»˜è®¤å­¦ä¹ ç‡ä¸º 1e-4ï¼Œä½†ä½¿ç”¨ LoRA æ—¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡

## è®­ç»ƒè„šæœ¬

æ•°æ®é›†é¢„å¤„ç†ä»£ç å’Œè®­ç»ƒå¾ªç¯å¯ä»¥åœ¨ [`main()`](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L371) å‡½æ•°ä¸­æ‰¾åˆ°ï¼Œå¦‚æœæ‚¨éœ€è¦è°ƒæ•´è®­ç»ƒè„šæœ¬ï¼Œè¿™å°±æ˜¯æ‚¨éœ€è¦è¿›è¡Œæ›´æ”¹çš„åœ°æ–¹ã€‚

ä¸è„šæœ¬å‚æ•°ä¸€æ ·ï¼Œæ–‡æœ¬åˆ°å›¾åƒè®­ç»ƒæŒ‡å—ä¸­æä¾›äº†è®­ç»ƒè„šæœ¬çš„è¯¦ç»†æ­¥éª¤ã€‚è€Œè¿™ä»½æŒ‡å—åˆ™ç€çœ¼äºè„šæœ¬ä¸­ä¸ LoRA ç›¸å…³çš„éƒ¨åˆ†ã€‚

è„šæœ¬é¦–å…ˆé€šè¿‡å°† [æ–°çš„ LoRA æƒé‡](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L447) æ·»åŠ åˆ°æ³¨æ„åŠ›å±‚æ¥å¼€å§‹ã€‚è¿™æ¶‰åŠæ­£ç¡®é…ç½® UNet ä¸­æ¯ä¸ªå—çš„æƒé‡å¤§å°ã€‚æ‚¨ä¼šçœ‹åˆ° `rank` å‚æ•°è¢«ç”¨æ¥åˆ›å»º LoRAAttnProcessorï¼š

```py
lora_attn_procs = {}
for name in unet.attn_processors.keys():
    cross_attention_dim = None if name.endswith("attn1.processor") else unet.config.cross_attention_dim
    if name.startswith("mid_block"):
        hidden_size = unet.config.block_out_channels[-1]
    elif name.startswith("up_blocks"):
        block_id = int(name[len("up_blocks.")])
        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]
    elif name.startswith("down_blocks"):
        block_id = int(name[len("down_blocks.")])
        hidden_size = unet.config.block_out_channels[block_id]

    lora_attn_procs[name] = LoRAAttnProcessor(
        hidden_size=hidden_size,
        cross_attention_dim=cross_attention_dim,
        rank=args.rank,
    )

unet.set_attn_processor(lora_attn_procs)
lora_layers = AttnProcsLayers(unet.attn_processors)
```

[ä¼˜åŒ–å™¨](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L519) ä½¿ç”¨ `lora_layers` è¿›è¡Œåˆå§‹åŒ–ï¼Œå› ä¸ºè¿™äº›æ˜¯å”¯ä¸€ä¼šè¢«ä¼˜åŒ–çš„æƒé‡ï¼š

```py
optimizer = optimizer_cls(
    lora_layers.parameters(),
    lr=args.learning_rate,
    betas=(args.adam_beta1, args.adam_beta2),
    weight_decay=args.adam_weight_decay,
    eps=args.adam_epsilon,
)
```

é™¤äº†è®¾ç½® LoRA å±‚ä¹‹å¤–ï¼Œè®­ç»ƒè„šæœ¬åŸºæœ¬ä¸Šä¸ train_text_to_image.py ç›¸åŒï¼

## å¯åŠ¨è„šæœ¬

ä¸€æ—¦æ‚¨å®Œæˆäº†æ‰€æœ‰æ›´æ”¹æˆ–è€…æ‚¨å¯¹é»˜è®¤é…ç½®æ»¡æ„ï¼Œæ‚¨å°±å¯ä»¥å¯åŠ¨è®­ç»ƒè„šæœ¬ï¼ğŸš€

è®©æˆ‘ä»¬åœ¨ [PokÃ©mon BLIP æ ‡é¢˜](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions) æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥ç”Ÿæˆæˆ‘ä»¬è‡ªå·±çš„ PokÃ©monã€‚å°†ç¯å¢ƒå˜é‡ `MODEL_NAME` å’Œ `DATASET_NAME` åˆ†åˆ«è®¾ç½®ä¸ºæ¨¡å‹å’Œæ•°æ®é›†ã€‚æ‚¨è¿˜åº”è¯¥æŒ‡å®šåœ¨ `OUTPUT_DIR` ä¸­ä¿å­˜æ¨¡å‹çš„ä½ç½®ï¼Œå¹¶ä½¿ç”¨ `HUB_MODEL_ID` æŒ‡å®šè¦ä¿å­˜åˆ° Hub ä¸Šçš„æ¨¡å‹çš„åç§°ã€‚è„šæœ¬å°†åˆ›å»ºå¹¶ä¿å­˜ä»¥ä¸‹æ–‡ä»¶åˆ°æ‚¨çš„å­˜å‚¨åº“ä¸­ï¼š

+   ä¿å­˜çš„æ¨¡å‹æ£€æŸ¥ç‚¹

+   `pytorch_lora_weights.safetensors`ï¼ˆè®­ç»ƒçš„ LoRA æƒé‡ï¼‰

å¦‚æœæ‚¨åœ¨å¤šä¸ª GPU ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯·åœ¨ `accelerate launch` å‘½ä»¤ä¸­æ·»åŠ  `--multi_gpu` å‚æ•°ã€‚

åœ¨æ‹¥æœ‰ 11GB VRAM çš„ 2080 Ti GPU ä¸Šè¿›è¡Œå®Œæ•´è®­ç»ƒå¤§çº¦éœ€è¦ 5 å°æ—¶ã€‚

```py
export MODEL_NAME="runwayml/stable-diffusion-v1-5"
export OUTPUT_DIR="/sddata/finetune/lora/pokemon"
export HUB_MODEL_ID="pokemon-lora"
export DATASET_NAME="lambdalabs/pokemon-blip-captions"

accelerate launch --mixed_precision="fp16"  train_text_to_image_lora.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --dataset_name=$DATASET_NAME \
  --dataloader_num_workers=8 \
  --resolution=512 \
  --center_crop \
  --random_flip \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --max_train_steps=15000 \
  --learning_rate=1e-04 \
  --max_grad_norm=1 \
  --lr_scheduler="cosine" \
  --lr_warmup_steps=0 \
  --output_dir=${OUTPUT_DIR} \
  --push_to_hub \
  --hub_model_id=${HUB_MODEL_ID} \
  --report_to=wandb \
  --checkpointing_steps=500 \
  --validation_prompt="A pokemon with blue eyes." \
  --seed=1337
```

è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼š

```py
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16).to("cuda")
pipeline.load_lora_weights("path/to/lora/model", weight_name="pytorch_lora_weights.safetensors")
image = pipeline("A pokemon with blue eyes").images[0]
```

## ä¸‹ä¸€æ­¥

æ­å–œæ‚¨ä½¿ç”¨ LoRA è®­ç»ƒäº†ä¸€ä¸ªæ–°æ¨¡å‹ï¼è¦äº†è§£å¦‚ä½•ä½¿ç”¨æ‚¨çš„æ–°æ¨¡å‹ï¼Œä»¥ä¸‹æŒ‡å—å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼š

+   å­¦ä¹ å¦‚ä½• åŠ è½½ä¸åŒçš„ LoRA æ ¼å¼ ï¼Œè¿™äº›æ ¼å¼æ˜¯ä½¿ç”¨ Kohya å’Œ TheLastBen ç­‰ç¤¾åŒºè®­ç»ƒè€…è®­ç»ƒçš„ã€‚

+   å­¦ä¹ å¦‚ä½•ä½¿ç”¨ ç»“åˆå¤šä¸ª LoRA ä¸ PEFT è¿›è¡Œæ¨ç†ã€‚
