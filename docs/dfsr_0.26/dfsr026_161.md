# 文本到图像

> 原文链接：[`huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img)

稳定扩散模型是由[CompVis](https://github.com/CompVis)、[Stability AI](https://stability.ai/)、[Runway](https://github.com/runwayml)和[LAION](https://laion.ai/)的研究人员和工程师创建的。StableDiffusionPipeline 能够根据任何文本输入生成逼真的图像。它是在 LAION-5B 数据集的 512x512 图像上训练的。该模型使用冻结的 CLIP ViT-L/14 文本编码器来根据文本提示对模型进行条件化。该模型具有 860M UNet 和 123M 文本编码器，相对轻量级，可以在消费级 GPU 上运行。潜在扩散是稳定扩散构建在其之上的研究。它是由 Robin Rombach、Andreas Blattmann、Dominik Lorenz、Patrick Esser、Björn Ommer 在[High-Resolution Image Synthesis with Latent Diffusion Models](https://huggingface.co/papers/2112.10752)中提出的。

论文摘要：

*通过将图像形成过程分解为逐步应用去噪自动编码器、扩散模型（DMs），DMs 在图像数据及其他领域实现了最先进的合成结果。此外，它们的公式允许引导机制来控制图像生成过程而无需重新训练。然而，由于这些模型通常直接在像素空间中操作，优化强大的 DMs 通常消耗数百个 GPU 天，由于顺序评估，推断成本昂贵。为了在有限的计算资源上训练 DM 并保持其质量和灵活性，我们将它们应用于强大预训练自动编码器的潜在空间。与以往的工作相比，在这种表示上训练扩散模型首次实现了在复杂性减少和细节保留之间达到近乎最优点，极大地提升了视觉保真度。通过在模型架构中引入交叉注意力层，我们将扩散模型转变为强大且灵活的生成器，用于一般条件输入，如文本或边界框，高分辨率合成以卷积方式变得可能。我们的潜在扩散模型（LDMs）在图像修补和各种任务上实现了新的技术水平，包括无条件图像生成、语义场景合成和超分辨率，同时与基于像素的 DM 相比显著降低了计算要求。代码可在[`github.com/CompVis/latent-diffusion`](https://github.com/CompVis/latent-diffusion)上找到。*

确保查看稳定扩散 Tips 部分，了解如何探索调度程序速度和质量之间的权衡，以及如何有效地重用管道组件！

如果您有兴趣使用官方检查点执行任务，请探索[CompVis](https://huggingface.co/CompVis)、[Runway](https://huggingface.co/runwayml)和[Stability AI](https://huggingface.co/stabilityai) Hub 组织！

## StableDiffusionPipeline

### `class diffusers.StableDiffusionPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L118)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: CLIPVisionModelWithProjection = None requires_safety_checker: bool = True )
```

参数

+   `vae` (AutoencoderKL) — 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示形式。

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 一个`CLIPTokenizer`用于对文本进行标记化。

+   `unet` (UNet2DConditionModel) — 一个`UNet2DConditionModel`，用于去噪编码图像潜在特征。

+   `scheduler` (SchedulerMixin) — 与`unet`结合使用以去噪编码图像潜在特征的调度器。可以是 DDIMScheduler、LMSDiscreteScheduler 或 PNDMScheduler 之一。

+   `safety_checker` (`StableDiffusionSafetyChecker`) — 用于估计生成的图像是否可能被视为具有攻击性或有害的分类模块。请参考[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) — 一个`CLIPImageProcessor`，用于从生成的图像中提取特征；作为`安全检查器`的输入。

使用稳定扩散进行文本到图像生成的流水线。

该模型继承自 DiffusionPipeline。查看超类文档以获取所有流水线实现的通用方法（下载、保存、在特定设备上运行等）。

该流水线还继承以下加载方法：

+   load_textual_inversion() 用于加载文本反演嵌入

+   load_lora_weights() 用于加载 LoRA 权重

+   save_lora_weights() 用于保存 LoRA 权重

+   用于加载`.ckpt`文件的 from_single_file()

+   load_ip_adapter() 用于加载 IP 适配器

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L803)

```py
( prompt: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 50 timesteps: List = None guidance_scale: float = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None guidance_rescale: float = 0.0 clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` or `List[str]`, *可选*) — 用于指导图像生成的提示。如果未定义，则需要传递`prompt_embeds`。

+   `height` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的高度（以像素为单位）。

+   `width` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的宽度（以像素为单位）。

+   `num_inference_steps` (`int`, *可选*, 默认为 50) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `timesteps` (`List[int]`, *可选*) — 用于与支持在其`set_timesteps`方法中使用`timesteps`参数的调度器一起使用的自定义时间步。如果未定义，则将使用传递`num_inference_steps`时的默认行为。必须按降序排列。

+   `guidance_scale` (`float`, *可选*, 默认为 7.5) — 更高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 指导图像生成中不包括的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用指导时（`guidance_scale < 1`）时将被忽略。

+   `num_images_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *optional*, defaults to 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta（η）。仅适用于 DDIMScheduler，在其他调度程序中将被忽略。

+   `generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 用于使生成具有确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机`generator`进行采样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，文本嵌入将从`prompt`输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds`将从`negative_prompt`输入参数生成。ip_adapter_image — (`PipelineImageInput`, *optional*): 可选的图像输入以与 IP 适配器一起使用。

+   `output_type` (`str`, *optional*, defaults to `"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的一个。

+   `return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回 StableDiffusionPipelineOutput 而不是普通元组。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给`AttentionProcessor`的 kwargs 字典，如[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的。

+   `guidance_rescale` (`float`, *optional*, defaults to 0.0) — 来自[Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf)的指导重缩放因子。指导重缩放因子应在使用零终端 SNR 时修复过曝光问题。

+   `clip_skip` (`int`, *optional*) — 在计算提示嵌入时要从 CLIP 跳过的层数。值为 1 意味着将使用预最终层的输出来计算提示嵌入。

+   `callback_on_step_end` (`Callable`, *optional*) — 在推断过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) — `callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包括在您的管道类的`._callback_tensor_inputs`属性中列出的变量。

返回

StableDiffusionPipelineOutput 或`tuple`

如果`return_dict`为`True`，则返回 StableDiffusionPipelineOutput，否则返回一个`tuple`，其中第一个元素是包含生成图像的列表，第二个元素是一个包含“不适合工作”（nsfw）内容的生成图像的`bool`列表。

用于生成的管道的调用函数。

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> image = pipe(prompt).images[0]
```

#### `enable_attention_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

参数

+   `slice_size`（`str`或`int`，*可选*，默认为`"auto"`）— 当为`"auto"`时，将输入分成注意力头的一半，因此注意力将在两个步骤中计算。如果为`"max"`，将通过一次只运行一个切片来节省最大数量的内存。如果提供了一个数字，则使用`attention_head_dim // slice_size`个切片。在这种情况下，`attention_head_dim`必须是`slice_size`的倍数。

启用切片的注意力计算。当启用此选项时，注意力模块将输入张量分割成片段，以便在多个步骤中计算注意力。对于多个注意力头，计算将按顺序在每个头上执行。这对于节省一些内存以换取一点速度降低很有用。

⚠️ 如果您已经在使用 PyTorch 2.0 或 xFormers 的`scaled_dot_product_attention`（SDPA），请不要启用注意力切片。这些注意力计算已经非常高效，因此您不需要启用此功能。如果您在 SDPA 或 xFormers 上启用了注意力切片，可能会导致严重减速！

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

#### `disable_attention_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

禁用切片的注意力计算。如果之前调用了`enable_attention_slicing`，则注意力将在一步中计算。

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L251)

```py
( )
```

启用切片的 VAE 解码。当启用此选项时，VAE 将分割输入张量以在多个步骤中计算解码。这对于节省一些内存并允许更大的批量大小很有用。

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L258)

```py
( )
```

禁用切片的 VAE 解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `enable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

参数

+   `attention_op`（`Callable`，*可选*）— 用作`op`参数传递给[xFormers 的`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)函数的默认`None`操作符。

启用[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。当启用此选项时，您应该观察到较低的 GPU 内存使用量，并在推断期间可能加速。训练期间的加速不被保证。

⚠️ 当内存高效注意力和切片注意力都启用时，内存高效注意力优先。

示例：

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

禁用[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L265)

```py
( )
```

启用分块 VAE 解码。当启用此选项时，VAE 将将输入张量分割成多个块，以便在多个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

#### `disable_vae_tiling`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L273)

```py
( )
```

禁用分块 VAE 解码。如果之前启用了 `enable_vae_tiling`，此方法将返回到一步计算解码。

#### `load_textual_inversion`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)

```py
( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional = None text_encoder: Optional = None **kwargs )
```

参数

+   `pretrained_model_name_or_path` (`str` 或 `os.PathLike` 或 `List[str or os.PathLike]` 或 `Dict` 或 `List[Dict]`) — 可以是以下之一或它们的列表：

    +   一个字符串，预训练模型在 Hub 上托管的 *模型 ID*（例如 `sd-concepts-library/low-poly-hd-logos-icons`）。

    +   一个指向包含文本反转权重的 *目录* 的路径（例如 `./my_text_inversion_directory/`）。

    +   一个指向包含文本反转权重的 *文件* 的路径（例如 `./my_text_inversions.pt`）。

    +   一个 [torch state dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)。

+   `token` (`str` 或 `List[str]`, *optional*) — 覆盖用于文本反转权重的令牌。如果 `pretrained_model_name_or_path` 是列表，则 `token` 也必须是相同长度的列表。

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel), *optional*) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。如果未指定，函数将使用 self.tokenizer。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer), *optional*) — 用于对文本进行标记化的 `CLIPTokenizer`。如果未指定，函数将使用 self.tokenizer。

+   `weight_name` (`str`, *optional*) — 自定义权重文件的名称。应在以下情况下使用：

    +   保存的文本反转文件以 🤗 Diffusers 格式保存，但是保存在特定权重名称下，例如 `text_inv.bin`。

    +   保存的文本反转文件以 Automatic1111 格式保存。

+   `cache_dir` (`Union[str, os.PathLike]`, *optional*) — 下载的预训练模型配置缓存的目录路径，如果不使用标准缓存。

+   `force_download` (`bool`, *optional*, 默认为 `False`) — 是否强制下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`, *optional*, 默认为 `False`) — 是否恢复下载模型权重和配置文件。如果设置为 `False`，任何未完全下载的文件将被删除。

+   `proxies` (`Dict[str, str]`, *optional*) — 一个按协议或端点使用的代理服务器字典，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。代理服务器在每个请求上使用。

+   `local_files_only` (`bool`, *optional*, 默认为 `False`) — 是否仅加载本地模型权重和配置文件。如果设置为 `True`，模型将不会从 Hub 下载。

+   `token` (`str` 或 *bool*, *optional*) — 用作远程文件的 HTTP 令牌的令牌。如果为 `True`，则使用从 `diffusers-cli login` 生成的令牌（存储在 `~/.huggingface`）。

+   `revision` (`str`, *optional*, 默认为 `"main"`) — 要使用的特定模型版本。可以是分支名称、标签名称、提交 ID 或 Git 允许的任何标识符。

+   `subfolder` (`str`, *optional*, 默认为 `""`) — 模型文件在 Hub 或本地较大模型存储库中的子文件夹位置。

+   `mirror`（`str`，*可选*）— 如果您在下载中国的模型时遇到可访问性问题，请使用镜像源。我们不保证源的及时性或安全性，您应参考镜像站点获取更多信息。

将文本反转嵌入加载到 StableDiffusionPipeline 的文本编码器中（支持🤗 Diffusers 和 Automatic1111 格式）。

示例：

要在🤗 Diffusers 格式中加载文本反转嵌入向量：

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("sd-concepts-library/cat-toy")

prompt = "A <cat-toy> backpack"

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("cat-backpack.png")
```

要加载 Automatic1111 格式的文本反转嵌入向量，请确保首先下载向量（例如从[civitAI](https://civitai.com/models/3036?modelVersionId=9857)）然后加载向量

本地：

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("./charturnerv2.pt", token="charturnerv2")

prompt = "charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details."

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("character.png")
```

#### `from_single_file`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/single_file.py#L141)

```py
( pretrained_model_link_or_path **kwargs )
```

参数

+   `pretrained_model_link_or_path`（`str`或`os.PathLike`，*可选*）— 可以是：

    +   在 Hub 上链接到`.ckpt`文件（例如`"https://huggingface.co/<repo_id>/blob/main/<path_to_file>.ckpt"`）。

    +   包含所有管道权重的*文件*路径。

+   `torch_dtype`（`str`或`torch.dtype`，*可选*）— 覆盖默认的`torch.dtype`并使用另一种 dtype 加载模型。

+   `force_download`（`bool`，*可选*，默认为`False`）— 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `cache_dir`（`Union[str, os.PathLike]`，*可选*）— 下载预训练模型配置的缓存路径，如果未使用标准缓存。

+   `resume_download`（`bool`，*可选*，默认为`False`）— 是否恢复下载模型权重和配置文件。如果设置为`False`，则删除任何未完全下载的文件。

+   `proxies`（`Dict[str, str]`，*可选*）— 一个按协议或端点使用的代理服务器字典，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。代理在每个请求上使用。

+   `local_files_only`（`bool`，*可选*，默认为`False`）— 是否仅加载本地模型权重和配置文件。如果设置为`True`，则不会从 Hub 下载模型。

+   `token`（`str`或*bool*，*可选*）— 用作远程文件的 HTTP 令牌的令牌。如果为`True`，则使用从`diffusers-cli login`生成的令牌（存储在`~/.huggingface`中）。

+   `revision`（`str`，*可选*，默认为`"main"`）— 要使用的特定模型版本。可以是分支名称、标签名称、提交 ID 或 Git 允许的任何标识符。

+   `use_safetensors`（`bool`，*可选*，默认为`None`）— 如果设置为`None`，则在可用时下载 safetensors 权重**并且**如果已安装 safetensors 库。如果设置为`True`，则强制从 safetensors 权重加载模型。如果设置为`False`，则不加载 safetensors 权重。

从保存在`.ckpt`或`.safetensors`格式中的预训练管道权重实例化 DiffusionPipeline。默认情况下，管道设置为评估模式（`model.eval()`）。

示例：

```py
>>> from diffusers import StableDiffusionPipeline

>>> # Download pipeline from huggingface.co and cache.
>>> pipeline = StableDiffusionPipeline.from_single_file(
...     "https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors"
... )

>>> # Download pipeline from local file
>>> # file is downloaded under ./v1-5-pruned-emaonly.ckpt
>>> pipeline = StableDiffusionPipeline.from_single_file("./v1-5-pruned-emaonly")

>>> # Enable float16 and move to GPU
>>> pipeline = StableDiffusionPipeline.from_single_file(
...     "https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/v1-5-pruned-emaonly.ckpt",
...     torch_dtype=torch.float16,
... )
>>> pipeline.to("cuda")
```

#### `load_lora_weights`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L83)

```py
( pretrained_model_name_or_path_or_dict: Union adapter_name = None **kwargs )
```

参数

+   `pretrained_model_name_or_path_or_dict`（`str`或`os.PathLike`或`dict`）— 参见 lora_state_dict()。

+   `kwargs`（`dict`，*可选*）— 参见 lora_state_dict()。

+   `adapter_name`（`str`，*可选*）— 用于引用加载的适配器模型的适配器名称。如果未指定，将使用`default_{i}`，其中 i 是要加载的适配器总数。

将在 `pretrained_model_name_or_path_or_dict` 中指定的 LoRA 权重加载到 `self.unet` 和 `self.text_encoder` 中。

所有 kwargs 都将转发到 `self.lora_state_dict`。

查看 lora_state_dict() 以获取有关如何加载状态字典的更多详细信息。

查看 load_lora_into_unet() 以获取有关如何将状态字典加载到 `self.unet` 中的更多详细信息。

查看 load_lora_into_text_encoder() 以获取有关如何将状态字典加载到 `self.text_encoder` 中的更多详细信息。

#### `save_lora_weights`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L869)

```py
( save_directory: Union unet_lora_layers: Dict = None text_encoder_lora_layers: Dict = None transformer_lora_layers: Dict = None is_main_process: bool = True weight_name: str = None save_function: Callable = None safe_serialization: bool = True )
```

参数

+   `save_directory` (`str` 或 `os.PathLike`) — 保存 LoRA 参数的目录。如果不存在，将创建该目录。

+   `unet_lora_layers` (`Dict[str, torch.nn.Module]` 或 `Dict[str, torch.Tensor]`) — 与 `unet` 对应的 LoRA 层的状态字典。

+   `text_encoder_lora_layers` (`Dict[str, torch.nn.Module]` 或 `Dict[str, torch.Tensor]`) — 与 `text_encoder` 对应的 LoRA 层的状态字典。必须显式传递文本编码器 LoRA 状态字典，因为它来自 🤗 Transformers。

+   `is_main_process` (`bool`, *可选*, 默认为 `True`) — 调用此函数的进程是否为主进程。在分布式训练期间，当您需要在所有进程上调用此函数时，这很有用。在这种情况下，仅在主进程上设置 `is_main_process=True`，以避免竞争条件。

+   `save_function` (`Callable`) — 用于保存状态字典的函数。在分布式训练期间，当您需要用另一种方法替换 `torch.save` 时，这很有用。可以使用环境变量 `DIFFUSERS_SAVE_MODE` 进行配置。

+   `safe_serialization` (`bool`, *可选*, 默认为 `True`) — 是否使用 `safetensors` 保存模型，还是使用传统的 PyTorch 方法与 `pickle`。

保存与 UNet 和文本编码器对应的 LoRA 参数。

#### `disable_freeu`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L678)

```py
( )
```

如果已启用，则禁用 FreeU 机制。

#### `enable_freeu`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L656)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1` (`float`) — 第 1 阶段的缩放因子，用于减弱跳跃特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `s2` (`float`) — 第 2 阶段的缩放因子，用于减弱跳跃特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `b1` (`float`) — 第 1 阶段的缩放因子，用于放大骨干特征的贡献。

+   `b2` (`float`) — 第 2 阶段的缩放因子，用于放大骨干特征的贡献。

启用 FreeU 机制，如 [`arxiv.org/abs/2309.11497`](https://arxiv.org/abs/2309.11497) 中所述。

缩放因子后缀表示它们应用的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同管道（如 Stable Diffusion v1、v2 和 Stable Diffusion XL）的值组合。

#### `encode_prompt`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L312)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示设备 — (`torch.device`): torch 设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用分类器自由指导

+   `negative_prompt`（`str`或`List[str]`，*可选*）— 不用来引导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。在不使用引导时被忽略（即，如果`guidance_scale`小于`1`，则被忽略）。

+   `prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。

+   `negative_prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。

+   `lora_scale`（`float`，*可选*）— 如果加载了 LoRA 层，则将应用于文本编码器的所有 LoRA 层的 LoRA 比例。

+   `clip_skip`（`int`，*可选*）— 从 CLIP 中跳过的层数，用于计算提示嵌入。值为 1 意味着将使用预最终层的输出来计算提示嵌入。

将提示编码为文本编码器的隐藏状态。

#### `fuse_qkv_projections`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L683)

```py
( unet: bool = True vae: bool = True )
```

参数

+   `unet`（默认为`True`）— 对 UNet 应用融合。

+   `vae`（默认为`True`）— 对 VAE 应用融合。

启用融合的 QKV 投影。对于自注意力模块，所有投影矩阵（即查询、键、值）都被融合。对于交叉注意力模块，键和值投影矩阵被融合。

此 API 为🧪实验性质。

#### `get_guidance_scale_embedding`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L744)

```py
( w embedding_dim = 512 dtype = torch.float32 ) → export const metadata = 'undefined';torch.FloatTensor
```

参数

+   `timesteps`（`torch.Tensor`）— 在这些时间步生成嵌入向量

+   `embedding_dim`（`int`，*可选*，默认为 512）— 要生成的嵌入的维度 dtype — 生成的嵌入的数据类型。

返回

`torch.FloatTensor`

形状为`(len(timesteps), embedding_dim)`的嵌入向量

参见[`github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298`](https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298)

#### `unfuse_qkv_projections`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L715)

```py
( unet: bool = True vae: bool = True )
```

参数

+   `unet`（默认为`True`）— 对 UNet 应用融合。

+   `vae`（`bool`，默认为`True`）— 对 VAE 应用融合。

如果启用，禁用 QKV 投影融合。

此 API 为🧪实验性质。

## StableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L10)

```py
( images: Union nsfw_content_detected: Optional )
```

参数

+   `images`（`List[PIL.Image.Image]`或`np.ndarray`）— 长度为`batch_size`的去噪 PIL 图像列表或形状为`(batch_size, height, width, num_channels)`的 NumPy 数组。

+   `nsfw_content_detected`（`List[bool]`）— 表示相应生成的图像是否包含“不安全内容”（nsfw）的列表，如果无法执行安全检查，则为`None`。

稳定扩散管道的输出类。

## FlaxStableDiffusionPipeline

### `class diffusers.FlaxStableDiffusionPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py#L81)

```py
( vae: FlaxAutoencoderKL text_encoder: FlaxCLIPTextModel tokenizer: CLIPTokenizer unet: FlaxUNet2DConditionModel scheduler: Union safety_checker: FlaxStableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor dtype: dtype = <class 'jax.numpy.float32'> )
```

参数

+   `vae`（FlaxAutoencoderKL）— 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。

+   `text_encoder` ([FlaxCLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel)) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 一个用于对文本进行标记化的`CLIPTokenizer`。

+   `unet` (FlaxUNet2DConditionModel) — 一个用于去噪编码图像潜在变量的`FlaxUNet2DConditionModel`。

+   `scheduler` (SchedulerMixin) — 与`unet`结合使用的调度器，用于去噪编码图像潜在变量。可以是`FlaxDDIMScheduler`、`FlaxLMSDiscreteScheduler`、`FlaxPNDMScheduler`或`FlaxDPMSolverMultistepScheduler`之一。

+   `safety_checker` (`FlaxStableDiffusionSafetyChecker`) — 一个分类模块，用于估计生成的图像是否可能被认为是冒犯或有害的。请参考[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) — 一个用于从生成的图像中提取特征的`CLIPImageProcessor`；作为`安全检查器`的输入。

使用稳定扩散进行文本到图像生成的基于 Flax 的管道。

该模型继承自 FlaxDiffusionPipeline。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py#L310)

```py
( prompt_ids: array params: Union prng_seed: Array num_inference_steps: int = 50 height: Optional = None width: Optional = None guidance_scale: Union = 7.5 latents: Array = None neg_prompt_ids: Array = None return_dict: bool = True jit: bool = False ) → export const metadata = 'undefined';FlaxStableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 用于指导图像生成的提示或提示。

+   `height` (`int`, *可选*, 默认为 `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为 `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *可选*, 默认为 50) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *可选*, 默认为 7.5) — 更高的引导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。

+   `latents` (`jnp.ndarray`, *可选*) — 从高斯分布中预先生成的嘈杂潜在变量，用作图像生成的输入。可以用来调整相同生成与不同提示的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成一个潜在变量数组。

+   `jit` (`bool`, 默认为 `False`) — 是否运行生成和安全评分函数的`pmap`版本。

    这个参数存在是因为`__call__`目前还不能完全进行 pmap。它将在将来的版本中被移除。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回一个 FlaxStableDiffusionPipelineOutput 而不是一个普通的元组。

返回

FlaxStableDiffusionPipelineOutput 或 `tuple`

如果`return_dict`为`True`，则返回 FlaxStableDiffusionPipelineOutput，否则返回一个`tuple`，其中第一个元素是包含生成图像的列表，第二个元素是一个列表，指示相应生成的图像是否包含“不安全内容”（nsfw）。

用于生成的管道的调用函数。

示例：

```py
>>> import jax
>>> import numpy as np
>>> from flax.jax_utils import replicate
>>> from flax.training.common_utils import shard

>>> from diffusers import FlaxStableDiffusionPipeline

>>> pipeline, params = FlaxStableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5", revision="bf16", dtype=jax.numpy.bfloat16
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"

>>> prng_seed = jax.random.PRNGKey(0)
>>> num_inference_steps = 50

>>> num_samples = jax.device_count()
>>> prompt = num_samples * [prompt]
>>> prompt_ids = pipeline.prepare_inputs(prompt)
# shard inputs and rng

>>> params = replicate(params)
>>> prng_seed = jax.random.split(prng_seed, jax.device_count())
>>> prompt_ids = shard(prompt_ids)

>>> images = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images
>>> images = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))
```

## FlaxStableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L31)

```py
( images: ndarray nsfw_content_detected: List )
```

参数

+   `images` (`np.ndarray`) — 形状为`(batch_size, height, width, num_channels)`的去噪图像数组。

+   `nsfw_content_detected` (`List[bool]`) — 列表指示相应生成的图像是否包含“不安全内容”（nsfw），或者如果无法执行安全检查，则为`None`。

Flax 基础稳定扩散管道的输出类。

#### `replace`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/flax/struct.py#L111)

```py
( **updates )
```

“返回一个用新值替换指定字段的新对象。
