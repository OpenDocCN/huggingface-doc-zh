# åŠ è½½é€‚é…å™¨

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/diffusers/using-diffusers/loading_adapters`](https://huggingface.co/docs/diffusers/using-diffusers/loading_adapters)

æœ‰å‡ ç§è®­ç»ƒæŠ€æœ¯å¯ç”¨äºä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹ï¼Œä»¥ç”Ÿæˆç‰¹å®šä¸»é¢˜çš„å›¾åƒæˆ–ç‰¹å®šé£æ ¼çš„å›¾åƒã€‚æ¯ç§è®­ç»ƒæ–¹æ³•éƒ½ä¼šäº§ç”Ÿä¸åŒç±»å‹çš„é€‚é…å™¨ã€‚ä¸€äº›é€‚é…å™¨ä¼šç”Ÿæˆå…¨æ–°çš„æ¨¡å‹ï¼Œè€Œå…¶ä»–é€‚é…å™¨åªä¼šä¿®æ”¹ä¸€å°éƒ¨åˆ†åµŒå…¥æˆ–æƒé‡ã€‚è¿™æ„å‘³ç€æ¯ä¸ªé€‚é…å™¨çš„åŠ è½½è¿‡ç¨‹ä¹Ÿæ˜¯ä¸åŒçš„ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•åŠ è½½ DreamBoothã€æ–‡æœ¬åè½¬å’Œ LoRA æƒé‡ã€‚

éšæ„æµè§ˆ[ç¨³å®šæ‰©æ•£æ¦‚å¿µåŒ–å™¨](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer)ã€[LoRA æ¢é™©å®¶](https://huggingface.co/spaces/multimodalart/LoraTheExplorer)å’Œ[Diffusers æ¨¡å‹åº“](https://huggingface.co/spaces/huggingface-projects/diffusers-gallery)ä»¥è·å–æ£€æŸ¥ç‚¹å’ŒåµŒå…¥ä»¥ä¾›ä½¿ç”¨ã€‚

## DreamBooth

[DreamBooth](https://dreambooth.github.io/)åœ¨ä»…ä½¿ç”¨å‡ å¹…ä¸»é¢˜å›¾åƒå¯¹æ•´ä¸ªæ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ç”Ÿæˆè¯¥ä¸»é¢˜çš„æ–°é£æ ¼å’Œè®¾ç½®çš„å›¾åƒã€‚è¿™ç§æ–¹æ³•é€šè¿‡åœ¨æç¤ºä¸­ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šè¯ï¼Œæ¨¡å‹å­¦ä¼šå°†å…¶ä¸ä¸»é¢˜å›¾åƒå…³è”èµ·æ¥ã€‚åœ¨æ‰€æœ‰è®­ç»ƒæ–¹æ³•ä¸­ï¼ŒDreamBooth äº§ç”Ÿçš„æ–‡ä»¶å¤§å°æœ€å¤§ï¼ˆé€šå¸¸ä¸ºå‡  GBï¼‰ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå®Œæ•´çš„æ£€æŸ¥ç‚¹æ¨¡å‹ã€‚

è®©æˆ‘ä»¬åŠ è½½[herge_style](https://huggingface.co/sd-dreambooth-library/herge-style)æ£€æŸ¥ç‚¹ï¼Œè¯¥æ£€æŸ¥ç‚¹ä»…è®­ç»ƒäº†ç”± HergÃ©ç»˜åˆ¶çš„ 10 å¹…å›¾åƒï¼Œä»¥åœ¨è¯¥é£æ ¼ä¸­ç”Ÿæˆå›¾åƒã€‚ä¸ºäº†ä½¿å…¶å·¥ä½œï¼Œæ‚¨éœ€è¦åœ¨æç¤ºä¸­åŒ…å«ç‰¹æ®Šè¯`herge_style`æ¥è§¦å‘æ£€æŸ¥ç‚¹ï¼š

```py
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained("sd-dreambooth-library/herge-style", torch_dtype=torch.float16).to("cuda")
prompt = "A cute herge_style brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration"
image = pipeline(prompt).images[0]
image
```

![](img/97b7dec0aa8cde8c3b74690416b5a5ca.png)

## æ–‡æœ¬åè½¬

[æ–‡æœ¬åè½¬](https://textual-inversion.github.io/)ä¸ DreamBooth éå¸¸ç›¸ä¼¼ï¼Œå®ƒä¹Ÿå¯ä»¥ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹ï¼Œä»å‡ å¹…å›¾åƒä¸­ç”Ÿæˆç‰¹å®šæ¦‚å¿µï¼ˆé£æ ¼ã€å¯¹è±¡ï¼‰ã€‚è¿™ç§æ–¹æ³•é€šè¿‡è®­ç»ƒå¹¶æ‰¾åˆ°ä»£è¡¨æ‚¨åœ¨æç¤ºä¸­æä¾›çš„å›¾åƒçš„æ–°åµŒå…¥æ¥å·¥ä½œã€‚å› æ­¤ï¼Œæ‰©æ•£æ¨¡å‹çš„æƒé‡ä¿æŒä¸å˜ï¼Œè®­ç»ƒè¿‡ç¨‹äº§ç”Ÿä¸€ä¸ªç›¸å¯¹è¾ƒå°ï¼ˆå‡  KBï¼‰çš„æ–‡ä»¶ã€‚

å› ä¸ºæ–‡æœ¬åè½¬ä¼šåˆ›å»ºåµŒå…¥ï¼Œæ‰€ä»¥ä¸èƒ½åƒ DreamBooth é‚£æ ·å•ç‹¬ä½¿ç”¨ï¼Œéœ€è¦å¦ä¸€ä¸ªæ¨¡å‹ã€‚

```py
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16).to("cuda")
```

ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨ load_textual_inversion()æ–¹æ³•åŠ è½½æ–‡æœ¬åè½¬åµŒå…¥ï¼Œå¹¶ç”Ÿæˆä¸€äº›å›¾åƒã€‚è®©æˆ‘ä»¬åŠ è½½[sd-concepts-library/gta5-artwork](https://huggingface.co/sd-concepts-library/gta5-artwork)åµŒå…¥ï¼Œæ‚¨éœ€è¦åœ¨æç¤ºä¸­åŒ…å«ç‰¹æ®Šè¯`<gta5-artwork>`æ¥è§¦å‘å®ƒï¼š

```py
pipeline.load_textual_inversion("sd-concepts-library/gta5-artwork")
prompt = "A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration, <gta5-artwork> style"
image = pipeline(prompt).images[0]
image
```

![](img/c039eaa48a71bfa7f36cbc3d5bc64464.png)

æ–‡æœ¬åè½¬ä¹Ÿå¯ä»¥è®­ç»ƒä¸è‰¯äº‹ç‰©ï¼Œåˆ›å»º*è´ŸåµŒå…¥*ï¼Œä»¥é˜»æ­¢æ¨¡å‹ç”Ÿæˆå¸¦æœ‰è¿™äº›ä¸è‰¯äº‹ç‰©çš„å›¾åƒï¼Œå¦‚æ¨¡ç³Šå›¾åƒæˆ–æ‰‹ä¸Šé¢å¤–çš„æ‰‹æŒ‡ã€‚è¿™å¯ä»¥æ˜¯å¿«é€Ÿæ”¹è¿›æç¤ºçš„ç®€å•æ–¹æ³•ã€‚æ‚¨è¿˜å°†ä½¿ç”¨ load_textual_inversion()åŠ è½½åµŒå…¥ï¼Œä½†è¿™æ¬¡ï¼Œæ‚¨éœ€è¦ä¸¤ä¸ªé¢å¤–çš„å‚æ•°ï¼š

+   `weight_name`ï¼šæŒ‡å®šè¦åŠ è½½çš„æƒé‡æ–‡ä»¶ï¼Œå¦‚æœæ–‡ä»¶ä»¥ç‰¹å®šåç§°ä¿å­˜åœ¨ğŸ¤— Diffusers æ ¼å¼ä¸­ï¼Œæˆ–è€…æ–‡ä»¶å­˜å‚¨åœ¨ A1111 æ ¼å¼ä¸­

+   `token`ï¼šæŒ‡å®šåœ¨æç¤ºä¸­è§¦å‘åµŒå…¥çš„ç‰¹æ®Šè¯

è®©æˆ‘ä»¬åŠ è½½[sayakpaul/EasyNegative-test](https://huggingface.co/sayakpaul/EasyNegative-test)åµŒå…¥ï¼š

```py
pipeline.load_textual_inversion(
    "sayakpaul/EasyNegative-test", weight_name="EasyNegative.safetensors", token="EasyNegative"
)
```

ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨`token`ç”Ÿæˆå¸¦æœ‰è´ŸåµŒå…¥çš„å›¾åƒï¼š

```py
prompt = "A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration, EasyNegative"
negative_prompt = "EasyNegative"

image = pipeline(prompt, negative_prompt=negative_prompt, num_inference_steps=50).images[0]
image
```

![](img/e8cd1cf0f650f65d2b00c9db51b0e506.png)

## LoRA

[ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰](https://huggingface.co/papers/2106.09685)æ˜¯ä¸€ç§æµè¡Œçš„è®­ç»ƒæŠ€æœ¯ï¼Œå› ä¸ºå®ƒå¿«é€Ÿä¸”ç”Ÿæˆè¾ƒå°çš„æ–‡ä»¶å¤§å°ï¼ˆå‡ ç™¾ MBï¼‰ã€‚ä¸æœ¬æŒ‡å—ä¸­çš„å…¶ä»–æ–¹æ³•ä¸€æ ·ï¼ŒLoRA å¯ä»¥è®­ç»ƒæ¨¡å‹ä»…ä»å°‘é‡å›¾åƒä¸­å­¦ä¹ æ–°æ ·å¼ã€‚å®ƒé€šè¿‡å°†æ–°æƒé‡æ’å…¥æ‰©æ•£æ¨¡å‹ï¼Œç„¶åä»…è®­ç»ƒæ–°æƒé‡è€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹ã€‚è¿™ä½¿å¾— LoRA è®­ç»ƒæ›´å¿«ï¼Œå­˜å‚¨æ›´å®¹æ˜“ã€‚

LoRA æ˜¯ä¸€ç§éå¸¸é€šç”¨çš„è®­ç»ƒæŠ€æœ¯ï¼Œå¯ä»¥ä¸å…¶ä»–è®­ç»ƒæ–¹æ³•ä¸€èµ·ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œé€šå¸¸ä¼šä½¿ç”¨ DreamBooth å’Œ LoRA æ¥è®­ç»ƒæ¨¡å‹ã€‚

LoRA è¿˜éœ€è¦ä¸å¦ä¸€ä¸ªæ¨¡å‹ä¸€èµ·ä½¿ç”¨ï¼š

```py
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained("stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16).to("cuda")
```

ç„¶åä½¿ç”¨ load_lora_weights()æ–¹æ³•åŠ è½½[ostris/super-cereal-sdxl-lora](https://huggingface.co/ostris/super-cereal-sdxl-lora)æƒé‡ï¼Œå¹¶ä»å­˜å‚¨åº“ä¸­æŒ‡å®šæƒé‡æ–‡ä»¶åï¼š

```py
pipeline.load_lora_weights("ostris/super-cereal-sdxl-lora", weight_name="cereal_box_sdxl_v1.safetensors")
prompt = "bears, pizza bites"
image = pipeline(prompt).images[0]
image
```

![](img/e378109cda4606520af62966b167614f.png)

load_lora_weights()æ–¹æ³•å°† LoRA æƒé‡åŠ è½½åˆ° UNet å’Œæ–‡æœ¬ç¼–ç å™¨ä¸­ã€‚è¿™æ˜¯åŠ è½½ LoRA çš„é¦–é€‰æ–¹å¼ï¼Œå› ä¸ºå®ƒå¯ä»¥å¤„ç†ä»¥ä¸‹æƒ…å†µï¼š

+   LoRA æƒé‡æ²¡æœ‰ UNet å’Œæ–‡æœ¬ç¼–ç å™¨çš„å•ç‹¬æ ‡è¯†ç¬¦

+   LoRA æƒé‡å¯¹ UNet å’Œæ–‡æœ¬ç¼–ç å™¨æœ‰å•ç‹¬çš„æ ‡è¯†ç¬¦

ä½†æ˜¯ï¼Œå¦‚æœåªéœ€è¦å°† LoRA æƒé‡åŠ è½½åˆ° UNet ä¸­ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ load_attn_procs()æ–¹æ³•ã€‚è®©æˆ‘ä»¬åŠ è½½[jbilcke-hf/sdxl-cinematic-1](https://huggingface.co/jbilcke-hf/sdxl-cinematic-1) LoRAï¼š

```py
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained("stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16).to("cuda")
pipeline.unet.load_attn_procs("jbilcke-hf/sdxl-cinematic-1", weight_name="pytorch_lora_weights.safetensors")

# use cnmt in the prompt to trigger the LoRA
prompt = "A cute cnmt eating a slice of pizza, stunning color scheme, masterpiece, illustration"
image = pipeline(prompt).images[0]
image
```

![](img/b7bd2f5305aeb45b260a0d963df68046.png)

å¯¹äº load_lora_weights()å’Œ load_attn_procs()ï¼Œæ‚¨å¯ä»¥ä¼ é€’`cross_attention_kwargs={"scale": 0.5}`å‚æ•°æ¥è°ƒæ•´ä½¿ç”¨ LoRA æƒé‡çš„æ¯”ä¾‹ã€‚å€¼ä¸º`0`ç›¸å½“äºä»…ä½¿ç”¨åŸºæœ¬æ¨¡å‹æƒé‡ï¼Œå€¼ä¸º`1`ç›¸å½“äºä½¿ç”¨å®Œå…¨å¾®è°ƒçš„ LoRAã€‚

è¦å¸è½½ LoRA æƒé‡ï¼Œè¯·ä½¿ç”¨ unload_lora_weights()æ–¹æ³•ä¸¢å¼ƒ LoRA æƒé‡å¹¶å°†æ¨¡å‹æ¢å¤ä¸ºå…¶åŸå§‹æƒé‡ï¼š

```py
pipeline.unload_lora_weights()
```

### åŠ è½½å¤šä¸ª LoRA

å°†å¤šä¸ª LoRA ä¸€èµ·ä½¿ç”¨å¯ä»¥åˆ›å»ºå…¨æ–°ä¸”ç‹¬ç‰¹çš„ä¸œè¥¿ï¼Œè¿™å¾ˆæœ‰è¶£ã€‚fuse_lora()æ–¹æ³•å…è®¸æ‚¨å°† LoRA æƒé‡ä¸åŸºç¡€æ¨¡å‹çš„åŸå§‹æƒé‡èåˆã€‚

èåˆæƒé‡å¯ä»¥åŠ å¿«æ¨ç†å»¶è¿Ÿï¼Œå› ä¸ºæ‚¨æ— éœ€å•ç‹¬åŠ è½½åŸºæœ¬æ¨¡å‹å’Œ LoRAï¼æ‚¨å¯ä»¥ä½¿ç”¨ save_pretrained()ä¿å­˜èåˆçš„ç®¡é“ï¼Œä»¥é¿å…æ¯æ¬¡ä½¿ç”¨æ¨¡å‹æ—¶åŠ è½½å’Œèåˆæƒé‡ã€‚

åŠ è½½åˆå§‹æ¨¡å‹ï¼š

```py
from diffusers import StableDiffusionXLPipeline, AutoencoderKL
import torch

vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
pipeline = StableDiffusionXLPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    vae=vae,
    torch_dtype=torch.float16,
).to("cuda")
```

æ¥ä¸‹æ¥ï¼ŒåŠ è½½ LoRA æ£€æŸ¥ç‚¹å¹¶å°†å…¶ä¸åŸå§‹æƒé‡èåˆã€‚`lora_scale`å‚æ•°æ§åˆ¶ä½¿ç”¨ LoRA æƒé‡æ—¶è¾“å‡ºçš„ç¼©æ”¾æ¯”ä¾‹ã€‚åœ¨ fuse_lora()æ–¹æ³•ä¸­è¿›è¡Œ`lora_scale`è°ƒæ•´å¾ˆé‡è¦ï¼Œå› ä¸ºå¦‚æœå°è¯•åœ¨ç®¡é“ä¸­å°†`scale`ä¼ é€’ç»™`cross_attention_kwargs`ï¼Œåˆ™ä¸èµ·ä½œç”¨ã€‚

å¦‚æœå‡ºäºä»»ä½•åŸå› éœ€è¦é‡ç½®åŸå§‹æ¨¡å‹æƒé‡ï¼ˆä½¿ç”¨ä¸åŒçš„`lora_scale`ï¼‰ï¼Œåº”ä½¿ç”¨ unfuse_lora()æ–¹æ³•ã€‚

```py
pipeline.load_lora_weights("ostris/ikea-instructions-lora-sdxl")
pipeline.fuse_lora(lora_scale=0.7)

# to unfuse the LoRA weights
pipeline.unfuse_lora()
```

ç„¶åå°†æ­¤ç®¡é“ä¸ä¸‹ä¸€ç»„ LoRA æƒé‡èåˆï¼š

```py
pipeline.load_lora_weights("ostris/super-cereal-sdxl-lora")
pipeline.fuse_lora(lora_scale=0.7)
```

æ‚¨æ— æ³•è§£é™¤å¤šä¸ª LoRA æ£€æŸ¥ç‚¹çš„èåˆï¼Œå› æ­¤å¦‚æœéœ€è¦å°†æ¨¡å‹é‡ç½®ä¸ºå…¶åŸå§‹æƒé‡ï¼Œæ‚¨éœ€è¦é‡æ–°åŠ è½½å®ƒã€‚

ç°åœ¨æ‚¨å¯ä»¥ç”Ÿæˆä¸€å¼ ä½¿ç”¨ä¸¤ä¸ª LoRA æƒé‡çš„å›¾ç‰‡ï¼š

```py
prompt = "A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration"
image = pipeline(prompt).images[0]
image
```

### ğŸ¤— PEFT

é˜…è¯»ä½¿ç”¨ğŸ¤— PEFT è¿›è¡Œæ¨æ–­æ•™ç¨‹ï¼Œäº†è§£æ›´å¤šå…³äºå®ƒä¸ğŸ¤— Diffusers é›†æˆä»¥åŠå¦‚ä½•è½»æ¾ä½¿ç”¨å’Œåˆ‡æ¢å¤šä¸ªé€‚é…å™¨çš„ä¿¡æ¯ã€‚æ‚¨éœ€è¦ä»æºä»£ç å®‰è£…ğŸ¤— Diffusers å’Œ PEFT æ‰èƒ½è¿è¡Œæœ¬èŠ‚ä¸­çš„ç¤ºä¾‹ã€‚

å¦ä¸€ç§åŠ è½½å’Œä½¿ç”¨å¤šä¸ª LoRA çš„æ–¹æ³•æ˜¯åœ¨ load_lora_weights()ä¸­æŒ‡å®š`adapter_name`å‚æ•°ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨äº†ğŸ¤— PEFT é›†æˆã€‚ä¾‹å¦‚ï¼ŒåŠ è½½å¹¶å‘½åä¸¤ä¸ª LoRA æƒé‡ï¼š

```py
from diffusers import DiffusionPipeline
import torch

pipeline = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16).to("cuda")
pipeline.load_lora_weights("ostris/ikea-instructions-lora-sdxl", weight_name="ikea_instructions_xl_v1_5.safetensors", adapter_name="ikea")
pipeline.load_lora_weights("ostris/super-cereal-sdxl-lora", weight_name="cereal_box_sdxl_v1.safetensors", adapter_name="cereal")
```

ç°åœ¨ä½¿ç”¨ set_adapters()æ¥æ¿€æ´»ä¸¤ä¸ª LoRAï¼Œå¹¶å¯ä»¥é…ç½®æ¯ä¸ª LoRA åœ¨è¾“å‡ºä¸Šçš„æƒé‡ï¼š

```py
pipeline.set_adapters(["ikea", "cereal"], adapter_weights=[0.7, 0.5])
```

ç„¶åï¼Œç”Ÿæˆä¸€å¼ å›¾ç‰‡ï¼š

```py
prompt = "A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration"
image = pipeline(prompt, num_inference_steps=30, cross_attention_kwargs={"scale": 1.0}).images[0]
image
```

### Kohya å’Œ TheLastBen

ç¤¾åŒºä¸­å…¶ä»–æµè¡Œçš„ LoRA è®­ç»ƒå™¨åŒ…æ‹¬[Kohya](https://github.com/kohya-ss/sd-scripts/)å’Œ[TheLastBen](https://github.com/TheLastBen/fast-stable-diffusion)çš„è®­ç»ƒå™¨ã€‚è¿™äº›è®­ç»ƒå™¨åˆ›å»ºçš„ LoRA æ£€æŸ¥ç‚¹ä¸ğŸ¤— Diffusers è®­ç»ƒçš„ä¸åŒï¼Œä½†ä»ç„¶å¯ä»¥ä»¥ç›¸åŒçš„æ–¹å¼åŠ è½½ã€‚

è®©æˆ‘ä»¬ä»[Civitai](https://civitai.com/)ä¸‹è½½[Blueprintify SD XL 1.0](https://civitai.com/models/150986/blueprintify-sd-xl-10)çš„æ£€æŸ¥ç‚¹ï¼š

```py
!wget https://civitai.com/api/download/models/168776 -O blueprintify-sd-xl-10.safetensors
```

ä½¿ç”¨ load_lora_weights()æ–¹æ³•åŠ è½½ LoRA æ£€æŸ¥ç‚¹ï¼Œå¹¶åœ¨`weight_name`å‚æ•°ä¸­æŒ‡å®šæ–‡ä»¶åï¼š

```py
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained("stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16).to("cuda")
pipeline.load_lora_weights("path/to/weights", weight_name="blueprintify-sd-xl-10.safetensors")
```

ç”Ÿæˆä¸€å¼ å›¾ç‰‡ï¼š

```py
# use bl3uprint in the prompt to trigger the LoRA
prompt = "bl3uprint, a highly detailed blueprint of the eiffel tower, explaining how to build all parts, many txt, blueprint grid backdrop"
image = pipeline(prompt).images[0]
image
```

ä½¿ç”¨ Kohya LoRA ä¸ğŸ¤— Diffusers çš„ä¸€äº›é™åˆ¶åŒ…æ‹¬ï¼š

+   ç”±äºå¤šç§åŸå› ï¼Œç”Ÿæˆçš„å›¾ç‰‡å¯èƒ½ä¸åƒ UIsï¼ˆå¦‚ ComfyUIï¼‰ç”Ÿæˆçš„å›¾ç‰‡ï¼Œè¿™äº›åŸå› åœ¨[è¿™é‡Œ](https://github.com/huggingface/diffusers/pull/4287/#issuecomment-1655110736)æœ‰è§£é‡Šã€‚

+   [LyCORIS æ£€æŸ¥ç‚¹](https://github.com/KohakuBlueleaf/LyCORIS)ä¸å—å®Œå…¨æ”¯æŒã€‚load_lora_weights()æ–¹æ³•ä½¿ç”¨ LoRA å’Œ LoCon æ¨¡å—åŠ è½½ LyCORIS æ£€æŸ¥ç‚¹ï¼Œä½†ä¸æ”¯æŒ Hada å’Œ LoKRã€‚

åŠ è½½ TheLastBen çš„æ£€æŸ¥ç‚¹éå¸¸ç±»ä¼¼ã€‚ä¾‹å¦‚ï¼Œè¦åŠ è½½[TheLastBen/William_Eggleston_Style_SDXL](https://huggingface.co/TheLastBen/William_Eggleston_Style_SDXL)çš„æ£€æŸ¥ç‚¹ï¼š

```py
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained("stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16).to("cuda")
pipeline.load_lora_weights("TheLastBen/William_Eggleston_Style_SDXL", weight_name="wegg.safetensors")

# use by william eggleston in the prompt to trigger the LoRA
prompt = "a house by william eggleston, sunrays, beautiful, sunlight, sunrays, beautiful"
image = pipeline(prompt=prompt).images[0]
image
```

## IP-Adapter

[IP-Adapter](https://ip-adapter.github.io/)æ˜¯ä¸€ç§æœ‰æ•ˆä¸”è½»é‡çº§çš„é€‚é…å™¨ï¼Œå¯ä¸ºæ‰©æ•£æ¨¡å‹æ·»åŠ å›¾åƒæç¤ºåŠŸèƒ½ã€‚è¯¥é€‚é…å™¨é€šè¿‡è§£è€¦å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„äº¤å‰æ³¨æ„åŠ›å±‚æ¥å·¥ä½œã€‚æ‰€æœ‰å…¶ä»–æ¨¡å‹ç»„ä»¶éƒ½è¢«å†»ç»“ï¼Œåªæœ‰ UNet ä¸­çš„åµŒå…¥å›¾åƒç‰¹å¾è¢«è®­ç»ƒã€‚å› æ­¤ï¼ŒIP-Adapter æ–‡ä»¶é€šå¸¸åªæœ‰çº¦ 100MBã€‚

IP-Adapter é€‚ç”¨äºæˆ‘ä»¬çš„å¤§å¤šæ•°ç®¡é“ï¼ŒåŒ…æ‹¬ç¨³å®šæ‰©æ•£ã€ç¨³å®šæ‰©æ•£ XLï¼ˆSDXLï¼‰ã€ControlNetã€T2I-Adapterã€AnimateDiffã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ä»ç›¸åŒåŸºç¡€æ¨¡å‹å¾®è°ƒçš„ä»»ä½•è‡ªå®šä¹‰æ¨¡å‹ã€‚å®ƒè¿˜å¯ä»¥ä¸ LCM-Lora ç›´æ¥é…åˆä½¿ç”¨ã€‚

æ‚¨å¯ä»¥åœ¨[h94/IP-Adapter](https://huggingface.co/h94/IP-Adapter)ä¸­æ‰¾åˆ°å®˜æ–¹ IP-Adapter æ£€æŸ¥ç‚¹ã€‚

IP-Adapter ç”±[okotaku](https://github.com/okotaku)è´¡çŒ®ã€‚

è®©æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªç¨³å®šæ‰©æ•£ç®¡é“ã€‚

```py
from diffusers import AutoPipelineForText2Image
import torch
from diffusers.utils import load_image

pipeline = AutoPipelineForText2Image.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16).to("cuda")
```

ç°åœ¨ä½¿ç”¨ load_ip_adapter()æ–¹æ³•åŠ è½½[h94/IP-Adapter](https://huggingface.co/h94/IP-Adapter)çš„æƒé‡ã€‚

```py
pipeline.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter_sd15.bin")
```

IP-é€‚é…å™¨ä¾èµ–äºå›¾åƒç¼–ç å™¨æ¥ç”Ÿæˆå›¾åƒç‰¹å¾ï¼Œå¦‚æœæ‚¨çš„ IP-é€‚é…å™¨æƒé‡æ–‡ä»¶å¤¹åŒ…å«ä¸€ä¸ªåä¸ºâ€œimage_encoderâ€çš„å­æ–‡ä»¶å¤¹ï¼Œåˆ™å›¾åƒç¼–ç å™¨å°†è‡ªåŠ¨åŠ è½½å¹¶æ³¨å†Œåˆ°ç®¡é“ä¸­ã€‚å¦åˆ™ï¼Œæ‚¨å¯ä»¥åŠ è½½ä¸€ä¸ª[CLIPVisionModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection)æ¨¡å‹ï¼Œå¹¶åœ¨åˆ›å»ºæ—¶å°†å…¶ä¼ é€’ç»™ç¨³å®šæ‰©æ•£ç®¡é“ã€‚

```py
from diffusers import AutoPipelineForText2Image
from transformers import CLIPVisionModelWithProjection
import torch

image_encoder = CLIPVisionModelWithProjection.from_pretrained(
    "h94/IP-Adapter", 
    subfolder="models/image_encoder",
    torch_dtype=torch.float16,
).to("cuda")

pipeline = AutoPipelineForText2Image.from_pretrained("runwayml/stable-diffusion-v1-5", image_encoder=image_encoder, torch_dtype=torch.float16).to("cuda")
```

IP-é€‚é…å™¨å…è®¸æ‚¨åŒæ—¶ä½¿ç”¨å›¾åƒå’Œæ–‡æœ¬æ¥è°ƒèŠ‚å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªæ–‡æœ¬åè½¬éƒ¨åˆ†çš„ç†Šå›¾åƒä½œä¸ºå›¾åƒæç¤ºï¼ˆ`ip_adapter_image`ï¼‰ï¼Œå¹¶é™„ä¸Šä¸€ä¸ªæ–‡æœ¬æç¤ºæ·»åŠ â€œå¤ªé˜³é•œâ€ã€‚ ğŸ˜

```py
pipeline.set_ip_adapter_scale(0.6)
image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_neg_embed.png")
generator = torch.Generator(device="cpu").manual_seed(33)
images = pipeline(
Â  Â  prompt='best quality, high quality, wearing sunglasses', 
Â  Â  ip_adapter_image=image,
Â  Â  negative_prompt="monochrome, lowres, bad anatomy, worst quality, low quality", 
Â  Â  num_inference_steps=50,
Â  Â  generator=generator,
).images
images[0]
```

![](img/344a4a031aa6d71b9b57dc0b5139e9be.png)

æ‚¨å¯ä»¥ä½¿ç”¨`set_ip_adapter_scale()`æ–¹æ³•æ¥è°ƒæ•´æ–‡æœ¬æç¤ºå’Œå›¾åƒæç¤ºçš„æ¡ä»¶æ¯”ä¾‹ã€‚å¦‚æœæ‚¨åªä½¿ç”¨å›¾åƒæç¤ºï¼Œåº”å°†æ¯”ä¾‹è®¾ç½®ä¸º`1.0`ã€‚æ‚¨å¯ä»¥é™ä½æ¯”ä¾‹ä»¥è·å¾—æ›´å¤šçš„ç”Ÿæˆå¤šæ ·æ€§ï¼Œä½†å®ƒå°†ä¸æç¤ºä¸å¤ªä¸€è‡´ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå½“æ‚¨åŒæ—¶ä½¿ç”¨æ–‡æœ¬å’Œå›¾åƒæç¤ºæ—¶ï¼Œ`scale=0.5`å¯ä»¥è·å¾—è‰¯å¥½çš„ç»“æœã€‚

IP-é€‚é…å™¨ä¹Ÿä¸å›¾åƒåˆ°å›¾åƒå’Œä¿®è¡¥ç®¡é“éå¸¸é…åˆã€‚è¯·çœ‹ä¸‹é¢å¦‚ä½•å°†å…¶ä¸å›¾åƒåˆ°å›¾åƒå’Œä¿®è¡¥ä¸€èµ·ä½¿ç”¨çš„ç¤ºä¾‹ã€‚

å›¾åƒåˆ°å›¾åƒä¿®è¡¥

```py
from diffusers import AutoPipelineForImage2Image
import torch
from diffusers.utils import load_image

pipeline = AutoPipelineForImage2Image.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16).to("cuda")

image = load_image("https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/vermeer.jpg")
ip_image = load_image("https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/river.png")

pipeline.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter_sd15.bin")
generator = torch.Generator(device="cpu").manual_seed(33)
images = pipeline(
Â  Â  prompt='best quality, high quality', 
Â  Â  image = image,
Â  Â  ip_adapter_image=ip_image,
Â  Â  num_inference_steps=50,
Â  Â  generator=generator,
Â  Â  strength=0.6,
).images
images[0]
```

IP-é€‚é…å™¨ä¹Ÿå¯ä»¥ä¸ SDXL ä¸€èµ·ä½¿ç”¨

```py
from diffusers import AutoPipelineForText2Image
from diffusers.utils import load_image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16
).to("cuda")

image = load_image("https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/watercolor_painting.jpeg")

pipeline.load_ip_adapter("h94/IP-Adapter", subfolder="sdxl_models", weight_name="ip-adapter_sdxl.bin")

generator = torch.Generator(device="cpu").manual_seed(33)
image = pipeline(
    prompt="best quality, high quality", 
    ip_adapter_image=image,
    negative_prompt="monochrome, lowres, bad anatomy, worst quality, low quality", 
    num_inference_steps=25,
    generator=generator,
).images[0]
image.save("sdxl_t2i.png")
```

![](img/7714d8061d7ab0a16f36c9923a9a1d91.png)

è¾“å…¥å›¾åƒ

![](img/6d3dc0e54f1d38caddd7daf1bd93c3fb.png)

è°ƒæ•´åçš„å›¾åƒ

æ‚¨å¯ä»¥ä½¿ç”¨ IP-é€‚é…å™¨é¢éƒ¨æ¨¡å‹å°†ç‰¹å®šé¢éƒ¨åº”ç”¨äºæ‚¨çš„å›¾åƒã€‚è¿™æ˜¯åœ¨å›¾åƒç”Ÿæˆä¸­ä¿æŒä¸€è‡´è§’è‰²çš„æœ‰æ•ˆæ–¹æ³•ã€‚æƒé‡çš„åŠ è½½æ–¹å¼ä¸å…¶ä»– IP-é€‚é…å™¨ç›¸åŒã€‚

```py
# Load ip-adapter-full-face_sd15.bin
pipeline.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter-full-face_sd15.bin")
```

å»ºè®®ä½¿ç”¨`DDIMScheduler`å’Œ`EulerDiscreteScheduler`æ¥è¿›è¡Œé¢éƒ¨æ¨¡å‹ã€‚

```py
import torch
from diffusers import StableDiffusionPipeline, DDIMScheduler
from diffusers.utils import load_image

pipeline = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
).to("cuda")
pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
pipeline.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter-full-face_sd15.bin")

pipeline.set_ip_adapter_scale(0.7)

image = load_image("https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/ai_face2.png")

generator = torch.Generator(device="cpu").manual_seed(33)

image = pipeline(
    prompt="A photo of a girl wearing a black dress, holding red roses in hand, upper body, behind is the Eiffel Tower",
    ip_adapter_image=image,
    negative_prompt="monochrome, lowres, bad anatomy, worst quality, low quality", 
    num_inference_steps=50, num_images_per_prompt=1, width=512, height=704,
    generator=generator,
).images[0]
```

![](img/d45f8afb266397140047ea513967ec17.png)

è¾“å…¥å›¾åƒ

![](img/1f753c6cacd994b80a20d9f822857c7e.png)

è¾“å‡ºå›¾åƒ

æ‚¨å¯ä»¥åŒæ—¶åŠ è½½å¤šä¸ª IP-é€‚é…å™¨æ¨¡å‹å¹¶ä½¿ç”¨å¤šä¸ªå‚è€ƒå›¾åƒã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ IP-é€‚é…å™¨-Plus é¢éƒ¨æ¨¡å‹åˆ›å»ºä¸€è‡´çš„è§’è‰²ï¼Œå¹¶åŒæ—¶ä½¿ç”¨ IP-é€‚é…å™¨-Plus æ¨¡å‹ä»¥åŠ 10 ä¸ªå›¾åƒåˆ›å»ºæˆ‘ä»¬ç”Ÿæˆçš„å›¾åƒä¸­çš„è¿è´¯æ ·å¼ã€‚

```py
import torch
from diffusers import AutoPipelineForText2Image, DDIMScheduler
from transformers import CLIPVisionModelWithProjection
from diffusers.utils import load_image

image_encoder = CLIPVisionModelWithProjection.from_pretrained(
    "h94/IP-Adapter", 
    subfolder="models/image_encoder",
    torch_dtype=torch.float16,
)

pipeline = AutoPipelineForText2Image.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16,
    image_encoder=image_encoder,
)
pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
pipeline.load_ip_adapter(
  "h94/IP-Adapter", 
  subfolder="sdxl_models", 
  weight_name=["ip-adapter-plus_sdxl_vit-h.safetensors", "ip-adapter-plus-face_sdxl_vit-h.safetensors"]
)
pipeline.set_ip_adapter_scale([0.7, 0.3])
pipeline.enable_model_cpu_offload()

face_image = load_image("https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/women_input.png")
style_folder = "https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/style_ziggy"
style_images =  [load_image(f"{style_folder}/img{i}.png") for i in range(10)]

generator = torch.Generator(device="cpu").manual_seed(0)

image = pipeline(
    prompt="wonderwoman",
    ip_adapter_image=[style_images, face_image],
    negative_prompt="monochrome, lowres, bad anatomy, worst quality, low quality", 
    num_inference_steps=50, num_images_per_prompt=1,
    generator=generator,
).images[0]
```

![](img/708cd0f9abdbb4eb3ad70ec47c4651d9.png)

æ ·å¼è¾“å…¥å›¾åƒ

![](img/b4c08b99e9974c3a3e5bb5f7f2d1054f.png)

é¢éƒ¨è¾“å…¥å›¾åƒ

![](img/0f5d4fe5f2525dd88bd660385a4b080e.png)

è¾“å‡ºå›¾åƒ

### LCM-Lora

æ‚¨å¯ä»¥ä½¿ç”¨ IP-é€‚é…å™¨ä¸ LCM-Lora ä¸€èµ·å®ç°ä½¿ç”¨è‡ªå®šä¹‰å›¾åƒçš„â€œå³æ—¶å¾®è°ƒâ€ã€‚è¯·æ³¨æ„ï¼Œåœ¨åŠ è½½ LCM-Lora æƒé‡ä¹‹å‰ï¼Œæ‚¨éœ€è¦åŠ è½½ IP-é€‚é…å™¨æƒé‡ã€‚

```py
from diffusers import DiffusionPipeline, LCMScheduler
import torch
from diffusers.utils import load_image

model_id =  "sd-dreambooth-library/herge-style"
lcm_lora_id = "latent-consistency/lcm-lora-sdv1-5"

pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)

pipe.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter_sd15.bin")
pipe.load_lora_weights(lcm_lora_id)
pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()

prompt = "best quality, high quality"
image = load_image("https://user-images.githubusercontent.com/24734142/266492875-2d50d223-8475-44f0-a7c6-08b51cb53572.png")
images = pipe(
    prompt=prompt,
    ip_adapter_image=image,
    num_inference_steps=4,
    guidance_scale=1,
).images[0]
```

### å…¶ä»–ç®¡é“

IP-é€‚é…å™¨ä¸ä»»ä½•ä½¿ç”¨æ–‡æœ¬æç¤ºå’Œä½¿ç”¨ç¨³å®šæ‰©æ•£æˆ–ç¨³å®šæ‰©æ•£ XL æ£€æŸ¥ç‚¹çš„ç®¡é“å…¼å®¹ã€‚è¦åœ¨ä¸åŒç®¡é“ä¸­ä½¿ç”¨ IP-é€‚é…å™¨ï¼Œæ‚¨åªéœ€è¦åœ¨åˆ›å»ºç®¡é“åè¿è¡Œ`load_ip_adapter()`æ–¹æ³•ï¼Œç„¶åå°†æ‚¨çš„å›¾åƒä½œä¸º`ip_adapter_image`ä¼ é€’ç»™ç®¡é“

ğŸ¤—æ‰©æ•£å™¨ç›®å‰ä»…æ”¯æŒä¸ä¸€äº›æœ€å—æ¬¢è¿çš„ç®¡é“ä¸€èµ·ä½¿ç”¨ IP-é€‚é…å™¨ï¼Œå¦‚æœæ‚¨æœ‰ä¸€ä¸ªå¾ˆé…·çš„ç”¨ä¾‹å¹¶éœ€è¦å°† IP é€‚é…å™¨é›†æˆåˆ°å°šä¸æ”¯æŒçš„ç®¡é“ä¸­ï¼Œè¯·éšæ—¶æå‡º[åŠŸèƒ½è¯·æ±‚](https://github.com/huggingface/diffusers/issues/new/choose)ï¼

æ‚¨å¯ä»¥åœ¨ä¸‹é¢æ‰¾åˆ°å¦‚ä½•ä½¿ç”¨ IP-é€‚é…å™¨ä¸ ControlNet å’Œ AnimateDiff çš„ç¤ºä¾‹ã€‚

ControlNetAnimateDiff

```py
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
import torch
from diffusers.utils import load_image

controlnet_model_path = "lllyasviel/control_v11f1p_sd15_depth"
controlnet = ControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.float16)

pipeline = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16)
pipeline.to("cuda")

image = load_image("https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/statue.png")
depth_map = load_image("https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/depth.png")

pipeline.load_ip_adapter("h94/IP-Adapter", subfolder="models", weight_name="ip-adapter_sd15.bin")

generator = torch.Generator(device="cpu").manual_seed(33)
images = pipeline(
    prompt='best quality, high quality', 
    image=depth_map,
    ip_adapter_image=image,
    negative_prompt="monochrome, lowres, bad anatomy, worst quality, low quality", 
    num_inference_steps=50,
    generator=generator,
).images
images[0]
```

![](img/16fb69a794e63d70afc3ca739b1eee43.png)

è¾“å…¥å›¾åƒ

![](img/e6383b3af214075c7c04400114af5b74.png)

è°ƒæ•´åçš„å›¾åƒ
