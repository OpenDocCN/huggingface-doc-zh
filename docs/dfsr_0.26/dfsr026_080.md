# xFormers

> 原文链接：[`huggingface.co/docs/diffusers/optimization/xformers`](https://huggingface.co/docs/diffusers/optimization/xformers)

我们推荐使用[xFormers](https://github.com/facebookresearch/xformers)进行推理和训练。在我们的测试中，注意力块中进行的优化使得速度更快，内存消耗更少。

从`pip`安装 xFormers：

```py
pip install xformers
```

xFormers 的`pip`包需要最新版本的 PyTorch。如果您需要使用之前的 PyTorch 版本，我们建议从源代码中[安装 xFormers](https://github.com/facebookresearch/xformers#installing-xformers)。

安装完 xFormers 后，您可以使用`enable_xformers_memory_efficient_attention()`来进行更快的推理和减少内存消耗，如本部分所示。

根据这个[问题](https://github.com/huggingface/diffusers/issues/2234#issuecomment-1416931212)，xFormers `v0.0.16`在某些 GPU 上不能用于训练（微调或 DreamBooth）。如果您遇到这个问题，请按照问题评论中的指示安装开发版本。
