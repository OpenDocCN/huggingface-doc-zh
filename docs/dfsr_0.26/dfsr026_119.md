# AsymmetricAutoencoderKL

> 原始文本：[`huggingface.co/docs/diffusers/api/models/asymmetricautoencoderkl`](https://huggingface.co/docs/diffusers/api/models/asymmetricautoencoderkl)

改进的更大的变分自动编码器（VAE）模型，具有用于修复任务的 KL 损失：[为 StableDiffusion 设计更好的不对称 VQGAN](https://arxiv.org/abs/2306.04632) 作者：Zixin Zhu，Xuelu Feng，Dongdong Chen，Jianmin Bao，Le Wang，Yinpeng Chen，Lu Yuan，Gang Hua。

论文摘要：

*StableDiffusion 是一种革命性的文本到图像生成器，在图像生成和编辑领域引起轰动。与传统方法不同，传统方法在像素空间中学习扩散模型，StableDiffusion 通过 VQGAN 在潜在空间中学习扩散模型，确保了效率和质量。它不仅支持图像生成任务，还能够对真实图像进行编辑，例如图像修复和局部编辑。然而，我们观察到 StableDiffusion 中使用的普通 VQGAN 会导致信息严重丢失，甚至在非编辑的图像区域也会出现失真伪影。因此，我们提出了一种新的不对称 VQGAN，具有两个简单的设计。首先，除了来自编码器的输入外，解码器还包含一个条件分支，该分支将任务特定的先验信息（例如修复中的未遮罩图像区域）合并到其中。其次，解码器比编码器更重，可以更详细地恢复，同时仅略微增加总推理成本。我们的不对称 VQGAN 的训练成本低廉，我们只需要重新训练一个新的不对称解码器，同时保持普通 VQGAN 编码器和 StableDiffusion 不变。我们的不对称 VQGAN 可以广泛应用于基于 StableDiffusion 的修复和局部编辑方法。大量实验证明，它可以显著提高修复和编辑性能，同时保持原始的文本到图像能力。代码可在 [`github.com/buxiangzhiren/Asymmetric_VQGAN`](https://github.com/buxiangzhiren/Asymmetric_VQGAN) 找到*

评估结果可以在原始论文的第 4.1 节中找到。

## 可用的检查点

+   [`huggingface.co/cross-attention/asymmetric-autoencoder-kl-x-1-5`](https://huggingface.co/cross-attention/asymmetric-autoencoder-kl-x-1-5)

+   [`huggingface.co/cross-attention/asymmetric-autoencoder-kl-x-2`](https://huggingface.co/cross-attention/asymmetric-autoencoder-kl-x-2)

## 示例用法

```py
from diffusers import AsymmetricAutoencoderKL, StableDiffusionInpaintPipeline
from diffusers.utils import load_image, make_image_grid

prompt = "a photo of a person with beard"
img_url = "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/repaint/celeba_hq_256.png"
mask_url = "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/repaint/mask_256.png"

original_image = load_image(img_url).resize((512, 512))
mask_image = load_image(mask_url).resize((512, 512))

pipe = StableDiffusionInpaintPipeline.from_pretrained("runwayml/stable-diffusion-inpainting")
pipe.vae = AsymmetricAutoencoderKL.from_pretrained("cross-attention/asymmetric-autoencoder-kl-x-1-5")
pipe.to("cuda")

image = pipe(prompt=prompt, image=original_image, mask_image=mask_image).images[0]
make_image_grid([original_image, mask_image, image], rows=1, cols=3)
```

## AsymmetricAutoencoderKL

### `class diffusers.AsymmetricAutoencoderKL`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_asym_kl.py#L26)

```py
( in_channels: int = 3 out_channels: int = 3 down_block_types: Tuple = ('DownEncoderBlock2D',) down_block_out_channels: Tuple = (64,) layers_per_down_block: int = 1 up_block_types: Tuple = ('UpDecoderBlock2D',) up_block_out_channels: Tuple = (64,) layers_per_up_block: int = 1 act_fn: str = 'silu' latent_channels: int = 4 norm_num_groups: int = 32 sample_size: int = 32 scaling_factor: float = 0.18215 )
```

参数

+   `in_channels` (int, *可选*, 默认为 3) — 输入图像中的通道数。

+   `out_channels` (int, *可选*, 默认为 3) — 输出中的通道数。

+   `down_block_types` (`Tuple[str]`, *可选*, 默认为 `("DownEncoderBlock2D",)`) — 下采样块类型的元组。

+   `down_block_out_channels` (`Tuple[int]`, *可选*, 默认为 `(64,)`) — 下采样块输出通道的元组。

+   `layers_per_down_block` (`int`, *可选*, 默认为 `1`) — 下采样块的层数。

+   `up_block_types` (`Tuple[str]`, *可选*, 默认为 `("UpDecoderBlock2D",)`) — 上采样块类型的元组。

+   `up_block_out_channels` (`Tuple[int]`, *可选*, 默认为 `(64,)`) — 上采样块输出通道的元组。

+   `layers_per_up_block` (`int`, *可选*, 默认为 `1`) — 上采样块的层数。

+   `act_fn` (`str`, *可选*, 默认为 `"silu"`) — 要使用的激活函数。

+   `latent_channels` (`int`, *可选*, 默认为 4) — 潜在空间中的通道数。

+   `sample_size` (`int`, *可选*, 默认为 `32`) — 样本输入大小。

+   `norm_num_groups` (`int`, *可选*, 默认为 `32`) — 在 ResNet 块中用于第一个归一化层的组数。

+   `scaling_factor` (`float`，*optional*，默认为 0.18215) — 使用训练集的第一批计算的训练潜在空间的分量标准差。在训练扩散模型时，用于将潜在空间缩放为单位方差。在传递给扩散模型之前，潜在空间使用公式 `z = z * scaling_factor` 进行缩放。解码时，潜在空间使用公式缩放回原始比例：`z = 1 / scaling_factor * z`。有关更多详细信息，请参考 [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) 论文的 4.3.2 和 D.1 节。

为 StableDiffusion 设计更好的不对称 VQGAN [`arxiv.org/abs/2306.04632`](https://arxiv.org/abs/2306.04632)。一个具有 KL 损失的 VAE 模型，用于将图像编码为潜在空间，并将潜在表示解码为图像。

该模型继承自 ModelMixin。查看超类文档以获取所有模型实现的通用方法（如下载或保存）。

#### `forward`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_asym_kl.py#L158)

```py
( sample: FloatTensor mask: Optional = None sample_posterior: bool = False return_dict: bool = True generator: Optional = None )
```

参数

+   `sample` (`torch.FloatTensor`) — 输入样本。

+   `mask` (`torch.FloatTensor`，*optional*，默认为 `None`) — 可选的修复遮罩。

+   `sample_posterior` (`bool`，*optional*，默认为 `False`) — 是否从后验中采样。

+   `return_dict` (`bool`, *optional*, 默认为 `True`) — 是否返回一个 `DecoderOutput` 而不是一个普通的元组。

## AutoencoderKLOutput

### `class diffusers.models.modeling_outputs.AutoencoderKLOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_outputs.py#L6)

```py
( latent_dist: DiagonalGaussianDistribution )
```

参数

+   `latent_dist` (`DiagonalGaussianDistribution`) — `Encoder` 编码输出表示为 `DiagonalGaussianDistribution` 的均值和对数方差。`DiagonalGaussianDistribution` 允许从分布中采样潜在空间。

AutoencoderKL 编码方法的输出。

## DecoderOutput

### `class diffusers.models.autoencoders.vae.DecoderOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/vae.py#L33)

```py
( sample: FloatTensor )
```

参数

+   `sample` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height, width)`) — 模型最后一层的解码输出样本。

解码方法的输出。
