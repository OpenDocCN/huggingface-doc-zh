# Attend-and-Excite

> 原始文本：[`huggingface.co/docs/diffusers/api/pipelines/attend_and_excite`](https://huggingface.co/docs/diffusers/api/pipelines/attend_and_excite)

Attend-and-Excite for Stable Diffusion was proposed in [Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models](https://attendandexcite.github.io/Attend-and-Excite/) and provides textual attention control over image generation.

论文摘要为：

*最近的文本到图像生成模型展示了通过目标文本提示引导生成多样化和创造性图像的无与伦比的能力。尽管具有革命性，但当前最先进的扩散模型仍可能在生成图像时未能充分传达给定文本提示中的语义。我们分析了公开可用的 Stable Diffusion 模型，并评估了灾难性忽视的存在，即模型未能生成输入提示中的一个或多个主题。此外，我们发现在某些情况下，模型还未能正确地将属性（例如颜色）绑定到其对应的主题上。为了帮助减轻这些失败情况，我们引入了生成语义护理（GSN）的概念，我们试图在推理时干预生成过程，以改善生成图像的忠实度。使用基于注意力的 GSN 公式，称为 Attend-and-Excite，我们引导模型改进交叉注意力单元，以关注文本提示中的所有主题标记，并加强 - 或激发 - 它们的激活，鼓励模型生成文本提示中描述的所有主题。我们将我们的方法与替代方法进行比较，并证明它在一系列文本提示中更忠实地传达了所需的概念。*

您可以在[项目页面](https://attendandexcite.github.io/Attend-and-Excite/)上找到有关 Attend-and-Excite 的更多信息，[原始代码库](https://github.com/AttendAndExcite/Attend-and-Excite)，或在[demo](https://huggingface.co/spaces/AttendAndExcite/Attend-and-Excite)中尝试它。

请务必查看调度器指南，了解如何探索调度器速度和质量之间的权衡，并查看重用管道中的组件部分，以了解如何有效地将相同组件加载到多个管道中。

## StableDiffusionAttendAndExcitePipeline

### `class diffusers.StableDiffusionAttendAndExcitePipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_attend_and_excite/pipeline_stable_diffusion_attend_and_excite.py#L173)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor requires_safety_checker: bool = True )
```

参数

+   `vae`（AutoencoderKL）。

+   `tokenizer`（[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer））- 用于对文本进行标记的`CLIPTokenizer`。

+   `unet`（UNet2DConditionModel) — 用于与`unet`结合使用以去噪编码图像潜在特征的调度器。可以是 DDIMScheduler、LMSDiscreteScheduler 或 PNDMScheduler 之一。

+   `safety_checker` (`StableDiffusionSafetyChecker`) — 用于估计生成图像是否可能具有冒犯性或有害性的分类模块。请参考[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) — 用于从生成的图像中提取特征的`CLIPImageProcessor`；作为`safety_checker`的输入。

使用稳定扩散和 Attend-and-Excite 进行文本到图像生成的流水线。

该模型继承自 DiffusionPipeline。查看超类文档以获取为所有流水线实现的通用方法（下载、保存、在特定设备上运行等）。

该流水线还继承了以下加载方法：

+   load_textual_inversion() 用于加载文本反转嵌入

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_attend_and_excite/pipeline_stable_diffusion_attend_and_excite.py#L746)

```py
( prompt: Union token_indices: Union height: Optional = None width: Optional = None num_inference_steps: int = 50 guidance_scale: float = 7.5 negative_prompt: Union = None num_images_per_prompt: int = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None max_iter_to_alter: int = 25 thresholds: dict = {0: 0.05, 10: 0.5, 20: 0.8} scale_factor: int = 20 attn_res: Optional = (16, 16) clip_skip: Optional = None ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` or `List[str]`, *optional*) — 用于指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。

+   `token_indices` (`List[int]`) — 用于进行 attend-and-excite 的令牌索引。

+   `height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *optional*, defaults to 50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *optional*, defaults to 7.5) — 更高的引导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。 当`guidance_scale > 1`时启用引导比例。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 指导图像生成中不包含的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用引导时（`guidance_scale < 1`）将被忽略。

+   `num_images_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *optional*, defaults to 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta（η）。仅适用于 DDIMScheduler，在其他调度器中将被忽略。

+   `generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中采样的预生成噪声潜在特征，用作图像生成的输入。可以用来使用不同提示调整相同生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜在特征张量。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，文本嵌入将从`prompt`输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回 StableDiffusionPipelineOutput 而不是普通的元组。

+   `回调` (`Callable`, *可选*) — 推理过程中每`callback_steps`步调用的函数。该函数使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *可选*, 默认为 1) — 调用`callback`函数的频率。如果未指定，将在每一步调用回调。

+   `cross_attention_kwargs` (`dict`, *可选*) — 如果指定，将传递给`AttentionProcessor`的 kwargs 字典，如[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中所定义。

+   `max_iter_to_alter` (`int`, *可选*, 默认为`25`) — 应用 attend-and-excite 的去噪步骤的次数。`max_iter_to_alter`去噪步骤是应用 attend-and-excite 的步骤。例如，如果`max_iter_to_alter`为`25`，总共有`30`个去噪步骤，则前`25`个去噪步骤应用 attend-and-excite，最后的`5`个不会。

+   `thresholds` (`dict`, *可选*, 默认为`{0 -- 0.05, 10: 0.5, 20: 0.8}`): 定义迭代和期望阈值以应用迭代潜在细化的字典。

+   `scale_factor` (`int`, *可选*, 默认为 20) — 控制每次 attend-and-excite 更新的步长的比例因子。

+   `attn_res` (`tuple`, *可选*, 默认从宽度和高度计算) — 语义注意力图的 2D 分辨率。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要从 CLIP 跳过的层数。值为 1 意味着将使用预最终层的输出来计算提示嵌入。

返回

StableDiffusionPipelineOutput 或`tuple`

如果`return_dict`为`True`，则返回 StableDiffusionPipelineOutput，否则返回一个元组，其中第一个元素是生成的图像列表，第二个元素是一个包含相应生成图像是否包含“不适宜工作”（nsfw）内容的`bool`列表。

用于生成的管道的调用函数。

示例:

```py
>>> import torch
>>> from diffusers import StableDiffusionAttendAndExcitePipeline

>>> pipe = StableDiffusionAttendAndExcitePipeline.from_pretrained(
...     "CompVis/stable-diffusion-v1-4", torch_dtype=torch.float16
... ).to("cuda")

>>> prompt = "a cat and a frog"

>>> # use get_indices function to find out indices of the tokens you want to alter
>>> pipe.get_indices(prompt)
{0: '<|startoftext|>', 1: 'a</w>', 2: 'cat</w>', 3: 'and</w>', 4: 'a</w>', 5: 'frog</w>', 6: '<|endoftext|>'}

>>> token_indices = [2, 5]
>>> seed = 6141
>>> generator = torch.Generator("cuda").manual_seed(seed)

>>> images = pipe(
...     prompt=prompt,
...     token_indices=token_indices,
...     guidance_scale=7.5,
...     generator=generator,
...     num_inference_steps=50,
...     max_iter_to_alter=25,
... ).images

>>> image = images[0]
>>> image.save(f"../images/{prompt}_{seed}.png")
```

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_attend_and_excite/pipeline_stable_diffusion_attend_and_excite.py#L258)

```py
( )
```

禁用切片的 VAE 解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_attend_and_excite/pipeline_stable_diffusion_attend_and_excite.py#L250)

```py
( )
```

启用切片 VAE 解码。启用此选项时，VAE 将将输入张量分割成片段，以便在几个步骤中计算解码。这对于节省一些内存并允许更大的批量大小很有用。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_attend_and_excite/pipeline_stable_diffusion_attend_and_excite.py#L299)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）— 要编码的提示 device —（`torch.device`）：torch 设备

+   `num_images_per_prompt`（`int`）— 每个提示应生成的图像数量

+   `do_classifier_free_guidance`（`bool`）— 是否使用无分类器引导

+   `negative_prompt`（`str`或`List[str]`，*可选*）— 不用来引导图像生成的提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用引导（即如果`guidance_scale`小于`1`，则忽略）。

+   `prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。

+   `negative_prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成 negative_prompt_embeds。

+   `lora_scale`（`float`，*可选*）— 将应用于文本编码器的所有 LoRA 层的 LoRA 比例，如果加载了 LoRA 层。

+   `clip_skip`（`int`，*可选*）— 在计算提示嵌入时要跳过的 CLIP 层数。值为 1 意味着将使用预终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

#### `get_indices`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_attend_and_excite/pipeline_stable_diffusion_attend_and_excite.py#L740)

```py
( prompt: str )
```

列出您希望更改的令牌的索引的实用函数

## StableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L10)

```py
( images: Union nsfw_content_detected: Optional )
```

参数

+   `images`（`List[PIL.Image.Image]`或`np.ndarray`）— 长度为`batch_size`的去噪 PIL 图像列表或形状为`(batch_size, height, width, num_channels)`的 NumPy 数组。

+   `nsfw_content_detected`（`List[bool]`）— 列表，指示相应生成的图像是否包含“不适宜工作”（nsfw）内容，如果无法执行安全检查，则为`None`。

稳定扩散管道的输出类。
