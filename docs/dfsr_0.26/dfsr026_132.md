# AudioLDM 2

> 原文链接：[`huggingface.co/docs/diffusers/api/pipelines/audioldm2`](https://huggingface.co/docs/diffusers/api/pipelines/audioldm2)

AudioLDM 2 是由刘浩和其他人在[AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining](https://arxiv.org/abs/2308.05734)中提出的。AudioLDM 2 以文本提示作为输入，预测相应的音频。它可以生成文本条件的音效、人类语音和音乐。

受[Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview)启发，AudioLDM 2 是一个文本到音频的*潜在扩散模型（LDM）*，从文本嵌入中学习连续的音频表示。使用两个文本编码器模型从提示输入计算文本嵌入：[CLAP](https://huggingface.co/docs/transformers/main/en/model_doc/clap)的文本分支和[Flan-T5](https://huggingface.co/docs/transformers/main/en/model_doc/flan-t5)的编码器。然后，这些文本嵌入通过[AudioLDM2ProjectionModel](https://huggingface.co/docs/diffusers/main/api/pipelines/audioldm2#diffusers.AudioLDM2ProjectionModel)投影到共享的嵌入空间。使用[GPT2](https://huggingface.co/docs/transformers/main/en/model_doc/gpt2)的*语言模型（LM）*自回归地预测八个新的嵌入向量，条件是基于投影的 CLAP 和 Flan-T5 嵌入。生成的嵌入向量和 Flan-T5 文本嵌入用作 LDM 中的交叉注意力调节。AudioLDM 2 的[UNet](https://huggingface.co/docs/diffusers/main/en/api/pipelines/audioldm2#diffusers.AudioLDM2UNet2DConditionModel)在于它采用**两个**交叉注意力嵌入，而不是大多数其他 LDM 中的一个交叉注意力调节。

该论文的摘要如下：

*尽管音频生成在不同类型的音频中有共同之处，比如语音、音乐和音效，但为每种类型设计模型需要仔细考虑特定目标和偏见，这些偏见可能与其他类型的偏见有很大不同。为了让我们更接近音频生成的统一视角，本文提出了一个框架，利用相同的学习方法来生成语音、音乐和音效。我们的框架引入了一个称为“音频语言”（LOA）的音频的通用表示。任何音频都可以基于 AudioMAE 转换为 LOA，这是一个自监督预训练表示学习模型。在生成过程中，我们使用 GPT-2 模型将任何形式转换为 LOA，并使用条件于 LOA 的潜在扩散模型进行自监督音频生成学习。所提出的框架自然地带来了优势，如上下文学习能力和可重复使用的自监督预训练的 AudioMAE 和潜在扩散模型。对文本到音频、文本到音乐和文本到语音的主要基准进行的实验表明，与以前的方法相比，我们的性能达到了最先进或具有竞争力。我们的代码、预训练模型和演示可在[此 https URL](https://audioldm.github.io/audioldm2)上找到。*

此管道由[sanchit-gandhi](https://huggingface.co/sanchit-gandhi)贡献。原始代码库可在[haoheliu/audioldm2](https://github.com/haoheliu/audioldm2)找到。

## 提示

### 选择检查点

AudioLDM2 有三个变体。其中两个检查点适用于文本到音频生成的一般任务。第三个检查点专门用于文本到音乐生成。

所有检查点在文本编码器和 VAE 的模型大小上是相同的。它们在 UNet 的大小和深度上有所不同。有关三个检查点的详细信息，请参见下表：

| 检查点 | 任务 | UNet 模型大小 | 总模型大小 | 训练数据/小时 |
| --- | --- | --- | --- | --- |
| [audioldm2](https://huggingface.co/cvssp/audioldm2) | 文本到音频 | 350M | 1.1B | 1150k |
| [audioldm2-large](https://huggingface.co/cvssp/audioldm2-large) | 文本到音频 | 750M | 1.5B | 1150k |
| [audioldm2-music](https://huggingface.co/cvssp/audioldm2-music) | 文本到音乐 | 350M | 1.1B | 665k |

### 构建提示

+   描述性提示输入效果最佳：使用形容词描述声音（例如“高质量”或“清晰”），并使提示具体化（例如“森林中的水流”而不是“流”）。

+   最好使用“猫”或“狗”等一般术语，而不是模型可能不熟悉的具体名称或抽象对象。

+   使用**负面提示**可以显著提高生成波形的质量，通过引导生成远离对应于低质量音频的术语。尝试使用“低质量”的负面提示。

### 控制推理

+   预测音频样本的*质量*可以通过`num_inference_steps`参数进行控制；更多步骤会提供更高质量的音频，但会降低推理速度。

+   预测音频样本的*长度*可以通过变化`audio_length_in_s`参数进行控制。

### 评估生成的波形：

+   生成的波形的质量可以根据种子而有很大差异。尝试使用不同的种子生成，直到找到令人满意的生成。

+   可以一次生成多个波形：将`num_waveforms_per_prompt`设置为大于 1 的值。将在生成的波形和提示文本之间执行自动评分，并相应地将音频从最佳到最差进行排名。

以下示例演示了如何使用上述提示构建良好的音乐生成：[示例](https://huggingface.co/docs/diffusers/main/en/api/pipelines/audioldm2#diffusers.AudioLDM2Pipeline.__call__.example)。

确保查看调度器指南以了解如何探索调度器速度和质量之间的权衡，并查看跨管道重用组件部分，以了解如何有效地将相同组件加载到多个管道中。

## AudioLDM2Pipeline

### `class diffusers.AudioLDM2Pipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L103)

```py
( vae: AutoencoderKL text_encoder: ClapModel text_encoder_2: T5EncoderModel projection_model: AudioLDM2ProjectionModel language_model: GPT2Model tokenizer: Union tokenizer_2: Union feature_extractor: ClapFeatureExtractor unet: AudioLDM2UNet2DConditionModel scheduler: KarrasDiffusionSchedulers vocoder: SpeechT5HifiGan )
```

参数

+   `vae`（AutoencoderKL）- 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。

+   `text_encoder`（[ClapModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel)）- 第一个冻结的文本编码器。AudioLDM2 使用联合音频文本嵌入模型[CLAP](https://huggingface.co/docs/transformers/model_doc/clap#transformers.CLAPTextModelWithProjection)，具体来说是[laion/clap-htsat-unfused](https://huggingface.co/laion/clap-htsat-unfused)变体。文本分支用于将文本提示编码为提示嵌入。完整的音频文本模型用于通过计算相似性分数对生成的波形与文本提示进行排名。

+   `text_encoder_2`（[T5EncoderModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/t5#transformers.T5EncoderModel)）- 第二个冻结的文本编码器。AudioLDM2 使用[T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5EncoderModel)的编码器，具体来说是[google/flan-t5-large](https://huggingface.co/google/flan-t5-large)变体。

+   `projection_model`（AudioLDM2ProjectionModel）- 用于线性投影第一个和第二个文本编码器模型的隐藏状态，并插入学习的 SOS 和 EOS 令牌嵌入的训练模型。来自两个文本编码器的投影隐藏状态被连接在一起，以提供输入给语言模型。

+   `language_model`（[GPT2Model](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2Model)）- 用于生成一系列隐藏状态的自回归语言模型，条件是从两个文本编码器的投影输出。

+   `tokenizer`（[RobertaTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)）- 用于为第一个冻结文本编码器对文本进行标记的标记器。

+   `tokenizer_2`（[T5Tokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mt5#transformers.T5Tokenizer)）- 用于为第二个冻结文本编码器对文本进行标记的标记器。

+   `feature_extractor`（[ClapFeatureExtractor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor)）- 用于将生成的音频波形预处理为自动评分的 log-mel 频谱图的特征提取器。

+   `unet`（UNet2DConditionModel）- 用于去噪编码音频潜在空间的`UNet2DConditionModel`。

+   `scheduler`（SchedulerMixin）- 用于与`unet`结合使用以去噪编码音频潜在空间的调度器。可以是 DDIMScheduler、LMSDiscreteScheduler 或 PNDMScheduler 之一。

+   `vocoder`（[SpeechT5HifiGan](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5HifiGan)）- 类`SpeechT5HifiGan`的声码器，用于将 mel 频谱潜在空间转换为最终音频波形。

使用 AudioLDM2 生成文本到音频的流水线。

该模型继承自 DiffusionPipeline。查看超类文档以获取所有流水线实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L732)

```py
( prompt: Union = None audio_length_in_s: Optional = None num_inference_steps: int = 200 guidance_scale: float = 3.5 negative_prompt: Union = None num_waveforms_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None generated_prompt_embeds: Optional = None negative_generated_prompt_embeds: Optional = None attention_mask: Optional = None negative_attention_mask: Optional = None max_new_tokens: Optional = None return_dict: bool = True callback: Optional = None callback_steps: Optional = 1 cross_attention_kwargs: Optional = None output_type: Optional = 'np' ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）- 用于指导音频生成的提示。如果未定义，则需要传递`prompt_embeds`。

+   `audio_length_in_s`（`int`，*可选*，默认为 10.24）- 生成的音频样本的长度（秒）。

+   `num_inference_steps`（`int`，*可选*，默认为 200）- 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的音频，但会降低推理速度。

+   `guidance_scale`（`float`，*可选*，默认为 3.5）- 更高的指导比例值鼓励模型生成与文本`prompt`密切相关的音频，但会降低声音质量。当`guidance_scale > 1`时启用指导比例。

+   `negative_prompt`（`str`或`List[str]`，*可选*）- 用于指导音频生成中不包含的提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用指导时（`guidance_scale < 1`）将被忽略。

+   `num_waveforms_per_prompt`（`int`，*可选*，默认为 1）- 每个提示生成的波形数量。如果`num_waveforms_per_prompt > 1`，则在生成的输出和文本提示之间执行自动评分。此评分根据生成的波形在联合文本-音频嵌入空间中与文本输入的余弦相似度对生成的波形进行排名。

+   `eta`（`float`，*可选*，默认为 0.0）- 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta（η）。仅适用于 DDIMScheduler，在其他调度器中将被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 用于使生成具有确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中预生成的嘈杂潜在变量，用作频谱图生成的输入。可用于使用不同提示调整相同生成。如果未提供，将通过使用提供的随机 `generator` 进行采样生成潜在变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，文本嵌入将从 `prompt` 输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负面文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds` 将从 `negative_prompt` 输入参数生成。

+   `generated_prompt_embeds` (`torch.FloatTensor`, *optional*) — 来自 GPT2 语言模型的预生成文本嵌入。可用于轻松调整文本输入，*例如* 提示加权。如果未提供，文本嵌入将从 `prompt` 输入参数生成。

+   `negative_generated_prompt_embeds` (`torch.FloatTensor`, *optional*) — 来自 GPT2 语言模型的预生成的负面文本嵌入。可用于轻松调整文本输入，*例如* 提示加权。如果未提供，将从 `negative_prompt` 输入参数计算负面提示嵌入。

+   `attention_mask` (`torch.LongTensor`, *optional*) — 预先计算的注意力掩码，应用于 `prompt_embeds`。如果未提供，注意力掩码将从 `prompt` 输入参数计算。

+   `negative_attention_mask` (`torch.LongTensor`, *optional*) — 预先计算的注意力掩码，应用于 `negative_prompt_embeds`。如果未提供，注意力掩码将从 `negative_prompt` 输入参数计算。

+   `max_new_tokens` (`int`, *optional*, 默认为 None) — 使用 GPT2 语言模型生成的新标记数量。如果未提供，将从模型的配置中获取标记数量。

+   `return_dict` (`bool`, *optional*, 默认为 `True`) — 是否返回 StableDiffusionPipelineOutput 而不是普通元组。

+   `callback` (`Callable`, *optional*) — 在推断期间每 `callback_steps` 步调用的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *optional*, 默认为 1) — 调用 `callback` 函数的频率。如果未指定，将在每一步调用回调。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给 [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) 中定义的 `AttentionProcessor` 的 kwargs 字典。

+   `output_type` (`str`, *optional*, 默认为 `"np"`) — 生成音频的输出格式。选择 `"np"` 返回一个 NumPy `np.ndarray` 或 `"pt"` 返回一个 PyTorch `torch.Tensor` 对象。设置为 `"latent"` 以返回潜在扩散模型（LDM）输出。

返回

StableDiffusionPipelineOutput 或 `tuple`

如果 `return_dict` 为 `True`，将返回 StableDiffusionPipelineOutput，否则将返回一个 `tuple`，其中第一个元素是包含生成音频的列表。

用于生成的管道的调用函数。

示例：

```py
>>> import scipy
>>> import torch
>>> from diffusers import AudioLDM2Pipeline

>>> repo_id = "cvssp/audioldm2"
>>> pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")

>>> # define the prompts
>>> prompt = "The sound of a hammer hitting a wooden surface."
>>> negative_prompt = "Low quality."

>>> # set the seed for generator
>>> generator = torch.Generator("cuda").manual_seed(0)

>>> # run the generation
>>> audio = pipe(
...     prompt,
...     negative_prompt=negative_prompt,
...     num_inference_steps=200,
...     audio_length_in_s=10.0,
...     num_waveforms_per_prompt=3,
...     generator=generator,
... ).audios

>>> # save the best audio sample (index 0) as a .wav file
>>> scipy.io.wavfile.write("techno.wav", rate=16000, data=audio[0])
```

#### `disable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L185)

```py
( )
```

禁用切片 VAE 解码。如果先前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `enable_model_cpu_offload`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L192)

```py
( gpu_id = 0 )
```

使用加速将所有模型转移到 CPU，减少内存使用量对性能影响较小。与`enable_sequential_cpu_offload`相比，此方法在调用其`forward`方法时一次将一个完整模型移动到 GPU，并且模型保持在 GPU 中，直到下一个模型运行。与`enable_sequential_cpu_offload`相比，内存节省较低，但由于`unet`的迭代执行，性能要好得多。

#### `enable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L177)

```py
( )
```

启用切片 VAE 解码。当启用此选项时，VAE 将在几个步骤中将输入张量分割成片段以计算解码。这对节省一些内存并允许更大的批量大小很有用。

#### `encode_prompt`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L270)

```py
( prompt device num_waveforms_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None generated_prompt_embeds: Optional = None negative_generated_prompt_embeds: Optional = None attention_mask: Optional = None negative_attention_mask: Optional = None max_new_tokens: Optional = None ) → export const metadata = 'undefined';prompt_embeds (torch.FloatTensor)
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示

+   `device` (`torch.device`) — torch 设备

+   `num_waveforms_per_prompt` (`int`) — 每个提示应生成的波形数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用无分类器指导

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用来指导音频生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即如果`guidance_scale`小于`1`，则忽略）。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 从 Flan T5 模型预计算的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`prompt`输入参数计算文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 从 Flan T5 模型预计算的负文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数计算负面提示嵌入。

+   `generated_prompt_embeds` (`torch.FloatTensor`, *可选*) — 从 GPT2 语言模型预生成的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。

+   `negative_generated_prompt_embeds` (`torch.FloatTensor`, *可选*) — 从 GPT2 语言模型预生成的负文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数计算负面提示嵌入。

+   `attention_mask` (`torch.LongTensor`, *可选*) — 覦计算的注意力掩码，应用于`prompt_embeds`。如果未提供，注意力掩码将从`prompt`输入参数计算。

+   `negative_attention_mask` (`torch.LongTensor`, *可选*) — 预计算的注意力掩码，应用于`negative_prompt_embeds`。如果未提供，注意力掩码将从`negative_prompt`输入参数计算。

+   `max_new_tokens` (`int`, *可选*, 默认为 None) — 与 GPT2 语言模型生成的新令牌数量。

返回

prompt_embeds (`torch.FloatTensor`)

来自 Flan T5 模型的文本嵌入。attention_mask (`torch.LongTensor`): 应用于`prompt_embeds`的注意力掩码。generated_prompt_embeds (`torch.FloatTensor`): 从 GPT2 语言模型生成的文本嵌入。

将提示编码为文本编码器隐藏状态。

示例：

```py
>>> import scipy
>>> import torch
>>> from diffusers import AudioLDM2Pipeline

>>> repo_id = "cvssp/audioldm2"
>>> pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")

>>> # Get text embedding vectors
>>> prompt_embeds, attention_mask, generated_prompt_embeds = pipe.encode_prompt(
...     prompt="Techno music with a strong, upbeat tempo and high melodic riffs",
...     device="cuda",
...     do_classifier_free_guidance=True,
... )

>>> # Pass text embeddings to pipeline for text-conditional audio generation
>>> audio = pipe(
...     prompt_embeds=prompt_embeds,
...     attention_mask=attention_mask,
...     generated_prompt_embeds=generated_prompt_embeds,
...     num_inference_steps=200,
...     audio_length_in_s=10.0,
... ).audios[0]

>>> # save generated audio sample
>>> scipy.io.wavfile.write("techno.wav", rate=16000, data=audio)
```

#### `generate_language_model`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py#L229)

```py
( inputs_embeds: Tensor = None max_new_tokens: int = 8 **model_kwargs ) → export const metadata = 'undefined';inputs_embeds (torch.FloatTensorof shape(batch_size, sequence_length, hidden_size)`)
```

参数

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) — 用作生成提示的序列。

+   `max_new_tokens` (`int`) — 要生成的新标记数。

+   `model_kwargs` (`Dict[str, Any]`, *optional*) — 附加模型特定 kwargs 的特定参数化，将被转发到模型的`forward`函数。

返回

`inputs_embeds (`torch.FloatTensor`of shape`(batch_size, sequence_length, hidden_size)`)

生成的隐藏状态序列。

从语言模型生成一系列隐藏状态，条件是嵌入输入。

## AudioLDM2ProjectionModel

### `class diffusers.AudioLDM2ProjectionModel`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/modeling_audioldm2.py#L82)

```py
( text_encoder_dim text_encoder_1_dim langauge_model_dim )
```

参数

+   `text_encoder_dim` (`int`) — 第一个文本编码器（CLAP）中文本嵌入的维度。

+   `text_encoder_1_dim` (`int`) — 第二个文本编码器（T5 或 VITS）中文本嵌入的维度。

+   `langauge_model_dim` (`int`) — 语言模型（GPT2）中文本嵌入的维度。

一个简单的线性投影模型，将两个文本嵌入映射到共享的潜在空间。它还分别在每个文本嵌入序列的开头和结尾插入了学习到的嵌入向量。每个附加`_1`的变量指的是第二个文本编码器对应的变量。否则，它来自第一个。

#### `forward`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/modeling_audioldm2.py#L111)

```py
( hidden_states: Optional = None hidden_states_1: Optional = None attention_mask: Optional = None attention_mask_1: Optional = None )
```

## AudioLDM2UNet2DConditionModel

### `class diffusers.AudioLDM2UNet2DConditionModel`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/modeling_audioldm2.py#L148)

```py
( sample_size: Optional = None in_channels: int = 4 out_channels: int = 4 flip_sin_to_cos: bool = True freq_shift: int = 0 down_block_types: Tuple = ('CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D') mid_block_type: Optional = 'UNetMidBlock2DCrossAttn' up_block_types: Tuple = ('UpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D') only_cross_attention: Union = False block_out_channels: Tuple = (320, 640, 1280, 1280) layers_per_block: Union = 2 downsample_padding: int = 1 mid_block_scale_factor: float = 1 act_fn: str = 'silu' norm_num_groups: Optional = 32 norm_eps: float = 1e-05 cross_attention_dim: Union = 1280 transformer_layers_per_block: Union = 1 attention_head_dim: Union = 8 num_attention_heads: Union = None use_linear_projection: bool = False class_embed_type: Optional = None num_class_embeds: Optional = None upcast_attention: bool = False resnet_time_scale_shift: str = 'default' time_embedding_type: str = 'positional' time_embedding_dim: Optional = None time_embedding_act_fn: Optional = None timestep_post_act: Optional = None time_cond_proj_dim: Optional = None conv_in_kernel: int = 3 conv_out_kernel: int = 3 projection_class_embeddings_input_dim: Optional = None class_embeddings_concat: bool = False )
```

参数

+   `sample_size` (`int` or `Tuple[int, int]`, *optional*, defaults to `None`) — 输入/输出样本的高度和宽度。

+   `in_channels` (`int`, *optional*, defaults to 4) — 输入样本中的通道数。

+   `out_channels` (`int`, *optional*, defaults to 4) — 输出中的通道数。

+   `flip_sin_to_cos` (`bool`, *optional*, defaults to `False`) — 是否在时间嵌入中将 sin 翻转为 cos。

+   `freq_shift` (`int`, *optional*, defaults to 0) — 要应用于时间嵌入的频率偏移。

+   `down_block_types` (`Tuple[str]`, *optional*, defaults to `("CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "DownBlock2D")`) — 要使用的下采样块的元组。

+   `mid_block_type` (`str`, *optional*, defaults to `"UNetMidBlock2DCrossAttn"`) — UNet 中间部分的块类型，对于 AudioLDM2，它只能是`UNetMidBlock2DCrossAttn`。

+   `up_block_types` (`Tuple[str]`, *optional*, defaults to `("UpBlock2D", "CrossAttnUpBlock2D", "CrossAttnUpBlock2D", "CrossAttnUpBlock2D")`) — 要使用的上采样块的元组。

+   `only_cross_attention` (`bool` or `Tuple[bool]`, *optional*, default to `False`) — 是否在基本变换器块中包含自注意力，参见`BasicTransformerBlock`。

+   `block_out_channels` (`Tuple[int]`, *optional*, defaults to `(320, 640, 1280, 1280)`) — 每个块的输出通道的元组。

+   `layers_per_block` (`int`, *optional*, defaults to 2) — 每个块的层数。

+   `downsample_padding` (`int`, *optional*, defaults to 1) — 用于下采样卷积的填充。

+   `mid_block_scale_factor` (`float`, *optional*, defaults to 1.0) — 中间块使用的比例因子。

+   `act_fn` (`str`, *optional*, defaults to `"silu"`) — 要使用的激活函数。

+   `norm_num_groups` (`int`, *optional*, defaults to 32) — 用于规范化的组数。如果为`None`，则跳过后处理中的规范化和激活层。

+   `norm_eps` (`float`, *optional*, defaults to 1e-5) — 用于规范化的 epsilon。

+   `cross_attention_dim`（`int`或`Tuple[int]`，*可选*，默认为 1280）— 交叉注意力特征的维度。

+   `transformer_layers_per_block`（`int`或`Tuple[int]`，*可选*，默认为 1）— `BasicTransformerBlock`类型的 transformer 块的数量。仅适用于`~models.unet_2d_blocks.CrossAttnDownBlock2D`、`~models.unet_2d_blocks.CrossAttnUpBlock2D`、`~models.unet_2d_blocks.UNetMidBlock2DCrossAttn`。

+   `attention_head_dim`（`int`，*可选*，默认为 8）— 注意力头的维度。

+   `num_attention_heads`（`int`，*可选*）— 注意力头的数量。如果未定义，默认为`attention_head_dim`。

+   `resnet_time_scale_shift`（`str`，*可选*，默认为`"default"`）— ResNet 块的时间尺度偏移配置（参见`ResnetBlock2D`）。选择`default`或`scale_shift`。

+   `class_embed_type`（`str`，*可选*，默认为`None`）— 要使用的类嵌入类型，最终与时间嵌入相加。选择`None`、`"timestep"`、`"identity"`、`"projection"`或`"simple_projection"`。

+   `num_class_embeds`（`int`，*可选*，默认为`None`）— 要投影到`time_embed_dim`的可学习嵌入矩阵的输入维度，在执行`class_embed_type`等于`None`的类条件时。

+   `time_embedding_type`（`str`，*可选*，默认为`positional`）— 用于时间步的位置嵌入类型。选择`positional`或`fourier`。

+   `time_embedding_dim`（`int`，*可选*，默认为`None`）— 用于投影时间嵌入的维度的可选覆盖。

+   `time_embedding_act_fn`（`str`，*可选*，默认为`None`）— 仅在时间嵌入传递到 UNet 的其余部分之前使用的可选激活函数。选择`silu`、`mish`、`gelu`和`swish`。

+   `timestep_post_act`（`str`，*可选*，默认为`None`）— 用于时间步嵌入中使用的第二个激活函数。选择`silu`、`mish`和`gelu`。

+   `time_cond_proj_dim`（`int`，*可选*，默认为`None`）— 时间步嵌入中`cond_proj`层的维度。

+   `conv_in_kernel`（`int`，*可选*，默认为`3`）— `conv_in`层的内核大小。

+   `conv_out_kernel`（`int`，*可选*，默认为`3`）— `conv_out`层的内核大小。

+   `projection_class_embeddings_input_dim`（`int`，*可选*）— 当`class_embed_type="projection"`时，`class_labels`输入的维度。在`class_embed_type="projection"`时需要。

+   `class_embeddings_concat`（`bool`，*可选*，默认为`False`）— 是否将时间嵌入与类嵌入连接起来。

一个条件 2D UNet 模型，接受一个带噪声的样本、条件状态和一个时间步，并返回一个形状为样本的输出。与普通的 UNet2DConditionModel 相比，这个变体在每个 Transformer 块中可选地包括一个额外的自注意力层，以及多个交叉注意力层。它还允许最多两个交叉注意力嵌入，`encoder_hidden_states`和`encoder_hidden_states_1`。

这个模型继承自 ModelMixin。查看超类文档以了解为所有模型实现的通用方法（如下载或保存）。

`forward`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm2/modeling_audioldm2.py#L662)

```py
( sample: FloatTensor timestep: Union encoder_hidden_states: Tensor class_labels: Optional = None timestep_cond: Optional = None attention_mask: Optional = None cross_attention_kwargs: Optional = None encoder_attention_mask: Optional = None return_dict: bool = True encoder_hidden_states_1: Optional = None encoder_attention_mask_1: Optional = None ) → export const metadata = 'undefined';UNet2DConditionOutput or tuple
```

参数

+   `sample`（`torch.FloatTensor`）— 具有以下形状的带噪声输入张量`(batch, channel, height, width)`。

+   `timestep`（`torch.FloatTensor`或`float`或`int`）— 对输入进行去噪的时间步数。

+   `encoder_hidden_states`（`torch.FloatTensor`）— 具有形状`(batch, sequence_length, feature_dim)`的编码器隐藏状态。

+   `encoder_attention_mask` (`torch.Tensor`) — 形状为`(batch, sequence_length)`的交叉注意力掩码应用于`encoder_hidden_states`。如果为`True`，则保留掩码，否则为`False`则丢弃。掩码将被转换为偏置，将大的负值添加到对应于“丢弃”标记的注意力分数。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回一个 UNet2DConditionOutput 而不是一个普通的元组。

+   `cross_attention_kwargs` (`dict`, *可选*) — 一个 kwargs 字典，如果指定了，则传递给`AttnProcessor`。

+   `encoder_hidden_states_1` (`torch.FloatTensor`, *可选*) — 第二组形状为`(batch, sequence_length_2, feature_dim_2)`的编码器隐藏状态。可用于将模型条件化为与`encoder_hidden_states`不同的嵌入集。

+   `encoder_attention_mask_1` (`torch.Tensor`, *可选*) — 形状为`(batch, sequence_length_2)`的交叉注意力掩码应用于`encoder_hidden_states_1`。如果为`True`，则保留掩码，否则为`False`则丢弃。掩码将被转换为偏置，将大的负值添加到对应于“丢弃”标记的注意力分数。

返回值

UNet2DConditionOutput 或`tuple`

如果`return_dict`为 True，则返回一个 UNet2DConditionOutput，否则返回一个`tuple`，其中第一个元素是样本张量。

AudioLDM2UNet2DConditionModel 的前向方法。

## AudioPipelineOutput

### `class diffusers.AudioPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L130)

```py
( audios: ndarray )
```

参数

+   `audios` (`np.ndarray`) — 形状为`(batch_size, num_channels, sample_rate)`的 NumPy 数组的去噪音频样本列表。

音频管道的输出类。
