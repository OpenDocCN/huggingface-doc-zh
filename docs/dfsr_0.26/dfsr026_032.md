# ä½¿ç”¨å¤šä¸ª GPU è¿›è¡Œåˆ†å¸ƒå¼æ¨ç†

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/diffusers/training/distributed_inference`](https://huggingface.co/docs/diffusers/training/distributed_inference)

åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index)æˆ–[PyTorch Distributed](https://pytorch.org/tutorials/beginner/dist_overview.html)åœ¨å¤šä¸ª GPU ä¸Šè¿è¡Œæ¨ç†ï¼Œè¿™å¯¹äºå¹¶è¡Œç”Ÿæˆå¤šä¸ªæç¤ºå¾ˆæœ‰ç”¨ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ğŸ¤— Accelerate å’Œ PyTorch Distributed è¿›è¡Œåˆ†å¸ƒå¼æ¨ç†ã€‚

## ğŸ¤— Accelerate

ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index)æ˜¯ä¸€ä¸ªæ—¨åœ¨ç®€åŒ–è·¨åˆ†å¸ƒå¼è®¾ç½®è®­ç»ƒæˆ–è¿è¡Œæ¨ç†çš„åº“ã€‚å®ƒç®€åŒ–äº†è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒçš„è¿‡ç¨‹ï¼Œè®©æ‚¨å¯ä»¥ä¸“æ³¨äºæ‚¨çš„ PyTorch ä»£ç ã€‚

é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ª Python æ–‡ä»¶å¹¶åˆå§‹åŒ–ä¸€ä¸ª[accelerate.PartialState](https://huggingface.co/docs/accelerate/v0.27.0/en/package_reference/state#accelerate.PartialState)æ¥åˆ›å»ºä¸€ä¸ªåˆ†å¸ƒå¼ç¯å¢ƒï¼›æ‚¨çš„è®¾ç½®ä¼šè‡ªåŠ¨æ£€æµ‹ï¼Œå› æ­¤ä¸éœ€è¦æ˜¾å¼å®šä¹‰`rank`æˆ–`world_size`ã€‚å°† DiffusionPipeline ç§»åŠ¨åˆ°`distributed_state.device`ä»¥ä¸ºæ¯ä¸ªè¿›ç¨‹åˆ†é…ä¸€ä¸ª GPUã€‚

ç°åœ¨ä½¿ç”¨[split_between_processes](https://huggingface.co/docs/accelerate/v0.27.0/en/package_reference/state#accelerate.PartialState.split_between_processes)å®ç”¨ç¨‹åºä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè‡ªåŠ¨åœ¨è¿›ç¨‹æ•°é‡ä¹‹é—´åˆ†å‘æç¤ºã€‚

```py
import torch
from accelerate import PartialState
from diffusers import DiffusionPipeline

pipeline = DiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, use_safetensors=True
)
distributed_state = PartialState()
pipeline.to(distributed_state.device)

with distributed_state.split_between_processes(["a dog", "a cat"]) as prompt:
    result = pipeline(prompt).images[0]
    result.save(f"result_{distributed_state.process_index}.png")
```

ä½¿ç”¨`--num_processes`å‚æ•°æŒ‡å®šè¦ä½¿ç”¨çš„ GPU æ•°é‡ï¼Œå¹¶è°ƒç”¨`accelerate launch`æ¥è¿è¡Œè„šæœ¬ï¼š

```py
accelerate launch run_distributed.py --num_processes=2
```

è¦äº†è§£æ›´å¤šï¼Œè¯·æŸ¥çœ‹[ä½¿ç”¨ğŸ¤— Accelerate è¿›è¡Œåˆ†å¸ƒå¼æ¨ç†](https://huggingface.co/docs/accelerate/en/usage_guides/distributed_inference#distributed-inference-with-accelerate)æŒ‡å—ã€‚

## PyTorch Distributed

PyTorch æ”¯æŒ[`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)ï¼Œå®ƒå®ç°äº†æ•°æ®å¹¶è¡Œã€‚

é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ª Python æ–‡ä»¶ï¼Œå¯¼å…¥`torch.distributed`å’Œ`torch.multiprocessing`æ¥è®¾ç½®åˆ†å¸ƒå¼è¿›ç¨‹ç»„ï¼Œå¹¶åœ¨æ¯ä¸ª GPU ä¸Šç”Ÿæˆæ¨ç†è¿›ç¨‹ã€‚æ‚¨è¿˜åº”è¯¥åˆå§‹åŒ–ä¸€ä¸ª DiffusionPipelineï¼š

```py
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

from diffusers import DiffusionPipeline

sd = DiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, use_safetensors=True
)
```

æ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªè¿è¡Œæ¨ç†çš„å‡½æ•°ï¼›[`init_process_group`](https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group)ç”¨äºåˆ›å»ºä¸€ä¸ªåˆ†å¸ƒå¼ç¯å¢ƒï¼ŒæŒ‡å®šè¦ä½¿ç”¨çš„åç«¯ç±»å‹ã€å½“å‰è¿›ç¨‹çš„`rank`å’Œå‚ä¸çš„è¿›ç¨‹æ•°é‡`world_size`ã€‚å¦‚æœæ‚¨è¦åœ¨ 2 ä¸ª GPU ä¸Šå¹¶è¡Œè¿è¡Œæ¨ç†ï¼Œåˆ™`world_size`ä¸º 2ã€‚

å°† DiffusionPipeline ç§»åŠ¨åˆ°`rank`å¹¶ä½¿ç”¨`get_rank`ä¸ºæ¯ä¸ªè¿›ç¨‹åˆ†é…ä¸€ä¸ª GPUï¼Œæ¯ä¸ªè¿›ç¨‹å¤„ç†ä¸åŒçš„æç¤ºï¼š

```py
def run_inference(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

    sd.to(rank)

    if torch.distributed.get_rank() == 0:
        prompt = "a dog"
    elif torch.distributed.get_rank() == 1:
        prompt = "a cat"

    image = sd(prompt).images[0]
    image.save(f"./{'_'.join(prompt)}.png")
```

è¦è¿è¡Œåˆ†å¸ƒå¼æ¨ç†ï¼Œè°ƒç”¨[`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn)åœ¨`world_size`ä¸­å®šä¹‰çš„ GPU æ•°é‡ä¸Šè¿è¡Œ`run_inference`å‡½æ•°ï¼š

```py
def main():
    world_size = 2
    mp.spawn(run_inference, args=(world_size,), nprocs=world_size, join=True)

if __name__ == "__main__":
    main()
```

å®Œæˆæ¨ç†è„šæœ¬åï¼Œä½¿ç”¨`--nproc_per_node`å‚æ•°æŒ‡å®šè¦ä½¿ç”¨çš„ GPU æ•°é‡ï¼Œå¹¶è°ƒç”¨`torchrun`æ¥è¿è¡Œè„šæœ¬ï¼š

```py
torchrun run_distributed.py --nproc_per_node=2
```
