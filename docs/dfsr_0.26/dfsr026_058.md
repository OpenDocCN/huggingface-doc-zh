# 文本到图像

> 原始文本：[`huggingface.co/docs/diffusers/training/text2image`](https://huggingface.co/docs/diffusers/training/text2image)

文本到图像脚本是实验性的，很容易过拟合并遇到灾难性遗忘等问题。尝试探索不同的超参数，以获得数据集的最佳结果。

文本到图像模型如 Stable Diffusion 是根据文本提示生成图像的。

训练模型可能会对您的硬件造成负担，但如果启用`gradient_checkpointing`和`mixed_precision`，可以在单个 24GB GPU 上训练模型。如果您使用更大的批量大小进行训练或希望训练速度更快，最好使用具有超过 30GB 内存的 GPU。您可以通过启用 xFormers 的内存高效注意力来减少内存占用。JAX/Flax 训练也支持在 TPU 和 GPU 上进行高效训练，但不支持梯度检查点、梯度累积或 xFormers。建议使用至少 30GB 内存的 GPU 或 TPU v3 进行 Flax 训练。

本指南将探索[train_text_to_image.py](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py)训练脚本，以帮助您熟悉它，以及如何为自己的用例进行调整。

在运行脚本之前，请确保您从源代码安装库：

```py
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install .
```

然后转到包含训练脚本的示例文件夹，并安装您正在使用的脚本所需的依赖项：

PyTorchFlax

```py
cd examples/text_to_image
pip install -r requirements.txt
```

🤗 Accelerate 是一个帮助您在多个 GPU/TPU 上训练或使用混合精度的库。它将根据您的硬件和环境自动配置您的训练设置。查看🤗 Accelerate [快速入门](https://huggingface.co/docs/accelerate/quicktour)以了解更多。

初始化🤗 Accelerate 环境：

```py
accelerate config
```

要设置默认的🤗 Accelerate 环境而不选择任何配置：

```py
accelerate config default
```

或者，如果您的环境不支持交互式 shell，比如笔记本，您可以使用：

```py
from accelerate.utils import write_basic_config

write_basic_config()
```

最后，如果您想在自己的数据集上训练模型，请查看创建用于训练的数据集指南，了解如何创建适用于训练脚本的数据集。

## 脚本参数

以下部分突出显示了训练脚本的一些重要部分，以帮助您了解如何修改它，但并未详细涵盖脚本的每个方面。如果您有兴趣了解更多，请随时阅读[脚本](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py)，并告诉我们您是否有任何问题或疑虑。

训练脚本提供了许多参数，帮助您自定义训练运行。所有参数及其描述都可以在[`parse_args()`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L193)函数中找到。该函数为每个参数提供默认值，例如训练批量大小和学习率，但如果您愿意，也可以在训练命令中设置自己的值。

例如，要使用 fp16 格式加快混合精度训练，请在训练命令中添加`--mixed_precision`参数：

```py
accelerate launch train_text_to_image.py \
  --mixed_precision="fp16"
```

一些基本且重要的参数包括：

+   `--pretrained_model_name_or_path`：Hub 上模型的名称或本地预训练模型的路径

+   `--dataset_name`：Hub 上数据集的名称或要训练的数据集的本地路径

+   `--image_column`：数据集中要训练的图像列的名称

+   `--caption_column`：数据集中要训练的文本列的名称

+   `--output_dir`：保存训练好的模型的位置

+   `--push_to_hub`：是否将训练好的模型推送到 Hub

+   `--checkpointing_steps`：保存检查点的频率，当训练中断时，可以通过在训练命令中添加 `--resume_from_checkpoint` 从该检查点继续训练

### 最小-SNR 加权

[最小-SNR](https://huggingface.co/papers/2303.09556) 加权策略可以通过重新平衡损失来帮助训练，以实现更快的收敛。训练脚本支持预测 `epsilon`（噪声）或 `v_prediction`，但最小-SNR 与两种预测类型兼容。这种加权策略仅受 PyTorch 支持，在 Flax 训练脚本中不可用。

添加 `--snr_gamma` 参数，并将其设置为推荐值 5.0：

```py
accelerate launch train_text_to_image.py \
  --snr_gamma=5.0
```

您可以在这个 [Weights and Biases](https://wandb.ai/sayakpaul/text2image-finetune-minsnr) 报告中比较不同 `snr_gamma` 值的损失曲面。对于较小的数据集，与较大数据集相比，最小-SNR 的影响可能不那么明显。

## 训练脚本

数据集预处理代码和训练循环在 [`main()`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L490) 函数中。如果需要调整训练脚本，这就是您需要进行更改的地方。

`train_text_to_image` 脚本首先[加载调度器](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L543)和分词器。如果需要，您可以选择在这里使用不同的调度器：

```py
noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler")
tokenizer = CLIPTokenizer.from_pretrained(
    args.pretrained_model_name_or_path, subfolder="tokenizer", revision=args.revision
)
```

然后脚本[加载 UNet](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L619) 模型：

```py
load_model = UNet2DConditionModel.from_pretrained(input_dir, subfolder="unet")
model.register_to_config(**load_model.config)

model.load_state_dict(load_model.state_dict())
```

接下来，需要对数据集的文本和图像列进行预处理。[`tokenize_captions`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L724) 函数处理输入的分词，[`train_transforms`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L742) 函数指定要应用于图像的转换类型。这两个函数都打包到 `preprocess_train` 中：

```py
def preprocess_train(examples):
    images = [image.convert("RGB") for image in examples[image_column]]
    examples["pixel_values"] = [train_transforms(image) for image in images]
    examples["input_ids"] = tokenize_captions(examples)
    return examples
```

最后，[训练循环](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L878) 处理其他所有事务。它将图像编码为潜在空间，向潜在空间添加噪声，计算文本嵌入以进行条件化，更新模型参数，并将模型保存并推送到 Hub。如果您想了解训练循环的工作原理，请查看 了解管道、模型和调度器 教程，该教程详细介绍了去噪过程的基本模式。

## 启动脚本

一旦您完成所有更改或对默认配置满意，您就可以启动训练脚本了！🚀

PyTorchFlax

让我们在 [Pokémon BLIP 标题](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions) 数据集上训练，生成您自己的 Pokémon。将环境变量 `MODEL_NAME` 和 `dataset_name` 设置为模型和数据集（来自 Hub 或本地路径）。如果您要在多个 GPU 上进行训练，请在 `accelerate launch` 命令中添加 `--multi_gpu` 参数。

要在本地数据集上训练，请将 `TRAIN_DIR` 和 `OUTPUT_DIR` 环境变量设置为数据集的路径和要保存模型的位置。

```py
export MODEL_NAME="runwayml/stable-diffusion-v1-5"
export dataset_name="lambdalabs/pokemon-blip-captions"

accelerate launch --mixed_precision="fp16"  train_text_to_image.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --dataset_name=$dataset_name \
  --use_ema \
  --resolution=512 --center_crop --random_flip \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --gradient_checkpointing \
  --max_train_steps=15000 \
  --learning_rate=1e-05 \
  --max_grad_norm=1 \
  --enable_xformers_memory_efficient_attention
  --lr_scheduler="constant" --lr_warmup_steps=0 \
  --output_dir="sd-pokemon-model" \
  --push_to_hub
```

训练完成后，您可以使用新训练的模型进行推理：

PyTorchFlax

```py
from diffusers import StableDiffusionPipeline
import torch

pipeline = StableDiffusionPipeline.from_pretrained("path/to/saved_model", torch_dtype=torch.float16, use_safetensors=True).to("cuda")

image = pipeline(prompt="yoda").images[0]
image.save("yoda-pokemon.png")
```

## 下一步

祝贺您训练成功自己的文本到图像模型！要了解如何使用您的新模型，以下指南可能会有所帮助：

+   学习如何在推理时加载 LoRA 权重，如果您使用 LoRA 训练了您的模型。

+   了解更多关于如何通过指导尺度等参数或技术，如提示加权，来帮助您控制在文本到图像任务指南中的推理。
