# 稳定扩散 XL

> 原始文本：[`huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl)

稳定扩散 XL（SDXL）由 Dustin Podell、Zion English、Kyle Lacey、Andreas Blattmann、Tim Dockhorn、Jonas Müller、Joe Penna 和 Robin Rombach 在[SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis](https://huggingface.co/papers/2307.01952)中提出。

论文摘要如下：

*我们提出了 SDXL，一个用于文本到图像合成的潜在扩散模型。与之前的稳定扩散版本相比，SDXL 利用了一个三倍大的 UNet 主干：模型参数的增加主要是由于更多的注意力块和更大的交叉注意力上下文，因为 SDXL 使用了第二个文本编码器。我们设计了多种新颖的条件方案，并在多个纵横比上训练了 SDXL。我们还引入了一个细化模型，用于通过后处理的图像到图像技术改进 SDXL 生成的样本的视觉保真度。我们证明，与之前版本的稳定扩散相比，SDXL 表现出了显著改进的性能，并且取得了与黑盒最先进图像生成器相媲美的结果。*

## 提示

+   已知使用 DPM++调度程序进行少于 50 步的 SDXL 会产生[视觉伪影](https://github.com/huggingface/diffusers/issues/5433)，因为求解器变得数值不稳定。要解决此问题，请查看此[PR](https://github.com/huggingface/diffusers/pull/5541)，该 PR 建议 ODE/SDE 求解器：

    +   设置`use_karras_sigmas=True`或`lu_lambdas=True`以提高图像质量。

    +   如果使用具有均匀步长的求解器（DPM++2M 或 DPM++2M SDE），请设置`euler_at_final=True`。

+   大多数 SDXL 检查点在图像尺寸为 1024x1024 时效果最佳。支持 768x768 和 512x512 的图像尺寸，但结果不太好。低于 512x512 的任何尺寸都不建议使用，并且默认检查点如[stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)可能不适用。

+   SDXL 可以为其训练的每个文本编码器传递不同的提示。我们甚至可以将同一提示的不同部分传递给文本编码器。

+   SDXL 输出的图像可以通过在图像到图像设置中利用一个细化模型来改进。

+   SDXL 提供`negative_original_size`、`negative_crops_coords_top_left`和`negative_target_size`，以在图像分辨率和裁剪参数上对模型进行负面条件。

要了解如何在各种任务中使用 SDXL，如何优化性能以及其他用法示例，请查看 Stable Diffusion XL 指南。

查看[Stability AI](https://huggingface.co/stabilityai) Hub 组织以获取官方基础和细化模型检查点！

## StableDiffusionXLPipeline

`class diffusers.StableDiffusionXLPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L149)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel text_encoder_2: CLIPTextModelWithProjection tokenizer: CLIPTokenizer tokenizer_2: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers image_encoder: CLIPVisionModelWithProjection = None feature_extractor: CLIPImageProcessor = None force_zeros_for_empty_prompt: bool = True add_watermarker: Optional = None )
```

参数

+   `vae`（AutoencoderKL 的文本部分，具体是[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)变体。

+   `text_encoder_2`（`CLIPTextModelWithProjection`）- 第二个冻结的文本编码器。Stable Diffusion XL 使用[CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection)的文本和池部分，具体是[laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)变体。

+   `tokenizer`（CLIPTokenizer）- 类的分词器 CLIPTokenizer。

+   `tokenizer_2`（CLIPTokenizer）- 类的第二个分词器 CLIPTokenizer。

+   `unet`（UNet2DConditionModel）- 有条件的 U-Net 架构，用于去噪编码图像潜在特征。

+   `scheduler`（SchedulerMixin）- 与`unet`一起用于去噪编码图像潜在特征的调度程序。可以是 DDIMScheduler、LMSDiscreteScheduler 或 PNDMScheduler 之一。

+   `force_zeros_for_empty_prompt`（`bool`，*可选*，默认为`"True"`）- 是否强制将负提示嵌入始终设置为 0。还请参阅`stabilityai/stable-diffusion-xl-base-1-0`的配置。

+   `add_watermarker`（`bool`，*可选*）- 是否使用 invisible_watermark 库对输出图像进行水印处理。如果未定义，如果安装了该软件包，则默认为 True，否则将不使用水印。

用于使用 Stable Diffusion XL 进行文本到图像生成的管道。

该模型继承自 DiffusionPipeline。查看超类文档以了解库为所有管道实现的通用方法（例如下载或保存，在特定设备上运行等）。

该管道还继承以下加载方法：

+   加载文本反演嵌入

+   用于加载`.ckpt`文件

+   加载 LoRA 权重

+   保存 LoRA 权重

+   加载 IP 适配器

`__call__`

来源

```py
( prompt: Union = None prompt_2: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 50 timesteps: List = None denoising_end: Optional = None guidance_scale: float = 5.0 negative_prompt: Union = None negative_prompt_2: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None guidance_rescale: float = 0.0 original_size: Optional = None crops_coords_top_left: Tuple = (0, 0) target_size: Optional = None negative_original_size: Optional = None negative_crops_coords_top_left: Tuple = (0, 0) negative_target_size: Optional = None clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput or tuple
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）- 用于指导图像生成的提示。如果未定义，则必须传递`prompt_embeds`。

+   `prompt_2`（`str`或`List[str]`，*可选*）- 要发送到`tokenizer_2`和`text_encoder_2`的提示。如果未定义，则在两个文本编码器中都使用`prompt`。

+   `height`（`int`，*可选*，默认为 self.unet.config.sample_size * self.vae_scale_factor）- 生成图像的像素高度。默认情况下设置为 1024 以获得最佳结果。低于 512 像素的任何内容对于 stabilityai/stable-diffusion-xl-base-1.0 和未经专门调整以适应低分辨率的检查点效果不佳。

+   `width` (`int`, *optional*, 默认为 self.unet.config.sample_size * self.vae_scale_factor) — 生成图像的像素宽度。默认设置为 1024 以获得最佳结果。低于 512 像素的任何内容都不适用于[stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)和未经专门调整以适应低分辨率的检查点。

+   `num_inference_steps` (`int`, *optional*, 默认为 50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但推理速度较慢。

+   `timesteps` (`List[int]`, *optional*) — 用于具有在其`set_timesteps`方法中支持`timesteps`参数的调度器的降噪过程的自定义时间步。如果未定义，则将使用传递`num_inference_steps`时的默认行为。必须按降序排列。

+   `denoising_end` (`float`, *optional*) — 当指定时，确定在意图提前终止之前完成的总降噪过程的比例（介于 0.0 和 1.0 之间）。因此，返回的样本仍将保留由调度器选择的离散时间步确定的大量噪音。当此管道形成“去噪器混合”多管道设置的一部分时，应理想地利用`denoising_end`参数，如[`Refining the Image Output`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)中所述。

+   `guidance_scale` (`float`, *optional*, 默认为 5.0) — 如[Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)中定义的指导比例。`guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程式 2 的`w`。通过设置`guidance_scale > 1`启用指导比例。更高的指导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `negative_prompt` (`str` 或 `List[str]`, *optional*) — 不指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。在不使用指导时被忽略（即如果`guidance_scale`小于`1`，则被忽略）。

+   `negative_prompt_2` (`str` 或 `List[str]`, *optional*) — 不指导图像生成的提示或提示，将发送到`tokenizer_2`和`text_encoder_2`。如果未定义，`negative_prompt`将在两个文本编码器中使用。

+   `num_images_per_prompt` (`int`, *optional*, 默认为 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *optional*, 默认为 0.0) — 对应于 DDIM 论文中的参数 eta (η)：[`arxiv.org/abs/2010.02502`](https://arxiv.org/abs/2010.02502)。仅适用于 schedulers.DDIMScheduler，对于其他情况将被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 一个或多个[torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *optional*) — 预生成的嘈杂潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机`generator`进行采样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。

+   `pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从 `prompt` 输入参数生成池化文本嵌入。

+   `negative_pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负面池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从 `negative_prompt` 输入参数生成负面池化文本嵌入。ip_adapter_image — (`PipelineImageInput`, *optional*): 可选的图像输入，用于与 IP 适配器一起使用。

+   `output_type` (`str`, *optional*, 默认为 `"pil"`) — 生成图像的输出格式。可选择 [PIL](https://pillow.readthedocs.io/en/stable/)：`PIL.Image.Image` 或 `np.array`。

+   `return_dict` (`bool`, *optional*, 默认为 `True`) — 是否返回 `~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput` 而不是普通元组。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给 `AttentionProcessor` 中定义的 `self.processor` 下的 kwargs 字典，详见 [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)。

+   `guidance_rescale` (`float`, *optional*, 默认为 0.0) — [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf) 提出的指导缩放因子。`guidance_scale` 在方程式 16 中定义为 `φ`。指导缩放因子应该在使用零终端信噪比时修复过曝光问题。

+   `original_size` (`Tuple[int]`, *optional*, 默认为 (1024, 1024)) — 如果 `original_size` 与 `target_size` 不同，图像将呈现为缩小或放大。如果未指定，`original_size` 默认为 `(height, width)`。这是 SDXL 的微调节的一部分，详见 [`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952) 的第 2.2 节。

+   `crops_coords_top_left` (`Tuple[int]`, *optional*, 默认为 (0, 0)) — `crops_coords_top_left` 可用于生成一个看起来被“裁剪”到位置 `crops_coords_top_left` 下方的图像。通常通过将 `crops_coords_top_left` 设置为 (0, 0) 可以实现有利且居中的图像。这是 SDXL 的微调节的一部分，详见 [`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952) 的第 2.2 节。

+   `target_size` (`Tuple[int]`, *optional*, 默认为 (1024, 1024)) — 对于大多数情况，`target_size` 应设置为生成图像的期望高度和宽度。如果未指定，将默认为 `(height, width)`。这是 SDXL 的微调节的一部分，详见 [`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952) 的第 2.2 节。

+   `negative_original_size` (`Tuple[int]`, *optional*, 默认为 (1024, 1024)) — 基于特定图像分辨率对生成过程进行负面调节。这是 SDXL 的微调节的一部分，详见 [`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952) 的第 2.2 节。更多信息，请参考此问题线程：[`github.com/huggingface/diffusers/issues/4208`](https://github.com/huggingface/diffusers/issues/4208)。

+   `negative_crops_coords_top_left` (`Tuple[int]`, *optional*, 默认为 (0, 0)) — 基于特定裁剪坐标对生成过程进行负面调节。这是 SDXL 的微调节的一部分，详见 [`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952) 的第 2.2 节。更多信息，请参考此问题线程：[`github.com/huggingface/diffusers/issues/4208`](https://github.com/huggingface/diffusers/issues/4208)。

+   `negative_target_size` (`Tuple[int]`, *可选*, 默认为 (1024, 1024)) — 基于目标图像分辨率对生成过程进行负条件化。对于大多数情况，它应与 `target_size` 相同。作为 SDXL 的微条件化的一部分，如 [`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952) 第 2.2 节中所解释的。有关更多信息，请参考此问题线程：[`github.com/huggingface/diffusers/issues/4208`](https://github.com/huggingface/diffusers/issues/4208)。

+   `callback_on_step_end` (`Callable`, *可选*) — 在推断期间每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs` 将包括由 `callback_on_step_end_tensor_inputs` 指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *可选*) — `callback_on_step_end` 函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包含在管道类的 `._callback_tensor_inputs` 属性中列出的变量。

返回

`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput` 或 `tuple`

`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput` 如果 `return_dict` 为 True，则返回一个 `tuple`。当返回一个 `tuple` 时，第一个元素是一个包含生成图像的列表。

在调用生成管道时调用的函数。

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionXLPipeline

>>> pipe = StableDiffusionXLPipeline.from_pretrained(
...     "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> image = pipe(prompt).images[0]
```

#### `disable_freeu`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L758)

```py
( )
```

如果已启用 FreeU 机制，则禁用它。

#### `disable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L269)

```py
( )
```

禁用切片 VAE 解码。如果之前启用了 `enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `disable_vae_tiling`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L286)

```py
( )
```

禁用平铺 VAE 解码。如果之前启用了 `enable_vae_tiling`，则此方法将返回到一步计算解码。

#### `enable_freeu`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L735)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1` (`float`) — 阶段 1 的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效果”。

+   `s2` (`float`) — 阶段 2 的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效果”。

+   `b1` (`float`) — 阶段 1 的缩放因子，用于放大主干特征的贡献。

+   `b2` (`float`) — 阶段 2 的缩放因子，用于放大主干特征的贡献。

启用 FreeU 机制，如 [`arxiv.org/abs/2309.11497`](https://arxiv.org/abs/2309.11497) 中所述。

缩放因子后缀表示它们被应用的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同管道（如 Stable Diffusion v1、v2 和 Stable Diffusion XL）的值组合。

#### `enable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L261)

```py
( )
```

启用切片 VAE 解码。启用此选项时，VAE 将将输入张量分割成片段，以便在多个步骤中计算解码。这对于节省一些内存并允许更大的批量大小非常有用。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L277)

```py
( )
```

启用平铺的 VAE 解码。启用此选项时，VAE 将将输入张量分割成瓦片，以便在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L293)

```py
( prompt: str prompt_2: Optional = None device: Optional = None num_images_per_prompt: int = 1 do_classifier_free_guidance: bool = True negative_prompt: Optional = None negative_prompt_2: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示

+   `prompt_2` (`str` 或 `List[str]`, *可选*) — 发送到 `tokenizer_2` 和 `text_encoder_2` 的提示或提示。如果未定义，则在两个文本编码器中使用 `prompt`。设备 — (`torch.device`): torch 设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用无分类器指导

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不指导图像生成的提示或提示。如果未定义，则必须传递 `negative_prompt_embeds`。当不使用指导时（即，如果 `guidance_scale` 小于 `1`，则忽略）。

+   `negative_prompt_2` (`str` 或 `List[str]`, *可选*) — 不指导图像生成的提示或提示，发送到 `tokenizer_2` 和 `text_encoder_2`。如果未定义，则在两个文本编码器中使用 `negative_prompt`。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，文本嵌入将从 `prompt` 输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从 `negative_prompt` 输入参数生成负的文本嵌入。

+   `pooled_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的池化文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从 `prompt` 输入参数生成池化文本嵌入。

+   `negative_pooled_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负池化文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从 `negative_prompt` 输入参数生成负的池化文本嵌入。

+   `lora_scale` (`float`, *可选*) — 如果加载了 LoRA 层，则将应用于文本编码器的所有 LoRA 层的 lora 比例。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要跳过的层数。值为 1 表示将使用预终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

#### `fuse_qkv_projections`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L762)

```py
( unet: bool = True vae: bool = True )
```

参数

+   `unet` (`bool`, 默认为 `True`) — 在 UNet 上应用融合。

+   `vae` (`bool`, 默认为 `True`) — 在 VAE 上应用融合。

启用融合的 QKV 投影。对于自注意力模块，所有投影矩阵（即查询、键、值）都被融合。对于交叉注意力模块，键和值投影矩阵被融合。

此 API 为🧪实验性质。

#### `get_guidance_scale_embedding`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L822)

```py
( w embedding_dim = 512 dtype = torch.float32 ) → export const metadata = 'undefined';torch.FloatTensor
```

参数

+   `timesteps` (`torch.Tensor`) — 在这些时间步生成嵌入向量

+   `embedding_dim` (`int`, *可选*, 默认为 512) — 要生成的嵌入的维度 dtype — 生成的嵌入的数据类型

返回

`torch.FloatTensor`

形状为`(len(timesteps), embedding_dim)`的嵌入向量

请参阅[`github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298`](https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298)

#### `unfuse_qkv_projections`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L793)

```py
( unet: bool = True vae: bool = True )
```

参数

+   `unet`（`bool`，默认为`True`）- 在 UNet 上应用融合。

+   `vae`（`bool`，默认为`True`）- 在 VAE 上应用融合。

如果启用，则禁用 QKV 投影融合。

此 API 为🧪实验性质。

## StableDiffusionXLImg2ImgPipeline

### `class diffusers.StableDiffusionXLImg2ImgPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py#L167)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel text_encoder_2: CLIPTextModelWithProjection tokenizer: CLIPTokenizer tokenizer_2: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers image_encoder: CLIPVisionModelWithProjection = None feature_extractor: CLIPImageProcessor = None requires_aesthetics_score: bool = False force_zeros_for_empty_prompt: bool = True add_watermarker: Optional = None )
```

参数

+   `vae`（AutoencoderKL）- 变分自动编码器（VAE）模型，用于对图像进行编码和解码，以及从潜在表示中解码图像。

+   `text_encoder`（`CLIPTextModel`）- 冻结的文本编码器。 Stable Diffusion XL 使用[CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)的文本部分，具体来说是[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)变体。

+   `text_encoder_2`（`CLIPTextModelWithProjection`）- 第二个冻结的文本编码器。 Stable Diffusion XL 使用[CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection)的文本和池部分，具体来说是[laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)变体。

+   `tokenizer`（`CLIPTokenizer`）- 类[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)的分词器。

+   `tokenizer_2`（`CLIPTokenizer`）- 类[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)的第二个分词器。

+   `unet`（UNet2DConditionModel）- 用于去噪编码图像潜变量的条件 U-Net 架构。

+   `scheduler`（SchedulerMixin）- 与`unet`结合使用以去噪编码图像潜变量的调度程序。可以是 DDIMScheduler，LMSDiscreteScheduler，或 PNDMScheduler 之一。

+   `requires_aesthetics_score`（`bool`，*可选*，默认为`"False"`）- 在推断期间，`unet`是否需要传递`aesthetic_score`条件。还请参阅`stabilityai/stable-diffusion-xl-refiner-1-0`的配置。

+   `force_zeros_for_empty_prompt`（`bool`，*可选*，默认为`"True"`）- 是否强制将负提示嵌入始终设置为 0。还请参阅`stabilityai/stable-diffusion-xl-base-1-0`的配置。

+   `add_watermarker`（`bool`，*可选*）- 是否使用[invisible_watermark 库](https://github.com/ShieldMnt/invisible-watermark/)来给输出图像添加水印。如果未定义，则如果安装了该软件包，则默认为 True，否则将不使用水印。

使用 Stable Diffusion XL 进行文本到图像生成的管道。

此模型继承自 DiffusionPipeline。检查库为所有管道实现的通用方法（如下载或保存，运行在特定设备上等）的超类文档。

该管道还继承以下加载方法：

+   load_textual_inversion() 用于加载文本反演嵌入

+   from_single_file() 用于加载`.ckpt`文件

+   load_lora_weights() 用于加载 LoRA 权重

+   `save_lora_weights()` 用于保存 LoRA 权重

+   load_ip_adapter() 用于加载 IP 适配器

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py#L1026)

```py
( prompt: Union = None prompt_2: Union = None image: Union = None strength: float = 0.3 num_inference_steps: int = 50 timesteps: List = None denoising_start: Optional = None denoising_end: Optional = None guidance_scale: float = 5.0 negative_prompt: Union = None negative_prompt_2: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None guidance_rescale: float = 0.0 original_size: Tuple = None crops_coords_top_left: Tuple = (0, 0) target_size: Tuple = None negative_original_size: Optional = None negative_crops_coords_top_left: Tuple = (0, 0) negative_target_size: Optional = None aesthetic_score: float = 6.0 negative_aesthetic_score: float = 2.5 clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';~pipelines.stable_diffusion.StableDiffusionXLPipelineOutput or tuple
```

参数

+   `prompt` (`str` or `List[str]`, *optional*) — 用于引导图像生成的提示或提示。如果未定义，则必须传递`prompt_embeds`。

+   `prompt_2` (`str` or `List[str]`, *optional*) — 要发送到`tokenizer_2`和`text_encoder_2`的提示或提示。如果未定义，则`prompt`将在两个文本编码器中使用。

+   `image` (`torch.FloatTensor` or `PIL.Image.Image` or `np.ndarray` or `List[torch.FloatTensor]` or `List[PIL.Image.Image]` or `List[np.ndarray]`) — 要使用管道修改的图像。

+   `strength` (`float`, *optional*, 默认为 0.3) — 在概念上，指示要如何转换参考`image`的程度。必须介于 0 和 1 之间。`image`将被用作起点，添加更多的噪音，`strength`越大。去噪步骤的数量取决于最初添加的噪音量。当`strength`为 1 时，添加的噪音将达到最大值，并且去噪过程将运行指定的`num_inference_steps`的完整迭代次数。因此，值为 1 基本上忽略`image`。请注意，在将`denoising_start`声明为整数的情况下，将忽略`strength`的值。

+   `num_inference_steps` (`int`, *optional*, 默认为 50) — 去噪步骤的数量。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `timesteps` (`List[int]`, *optional*) — 用于具有支持`timesteps`参数的调度器的去噪过程的自定义时间步长，在其`set_timesteps`方法中。如果未定义，则将使用传递`num_inference_steps`时的默认行为。必须按降序排列。

+   `denoising_start` (`float`, *optional*) — 当指定时，指示在启动之前要绕过的总去噪过程的比例（介于 0.0 和 1.0 之间）。因此，将跳过去噪过程的初始部分，并假定传递的`image`是部分去噪的图像。请注意，当指定此参数时，将忽略强度。当此管道被集成到“去噪器混合”多管道设置中时，`denoising_start`参数特别有益，如[`Refine Image Quality`](https://huggingface.co/docs/diffusers/using-diffusers/sdxl#refine-image-quality)中所述。

+   `denoising_end` (`float`, *optional*) — 当指定时，确定在故意提前终止之前完成的去噪过程的比例（介于 0.0 和 1.0 之间）。因此，返回的样本仍将保留相当多的噪音（约剩余 20%的时间步仍然需要），应由后续管道进行去噪，该管道的`denoising_start`设置为 0.8，以便仅对调度程序的最后 20%进行去噪。当此管道作为“去噪器混合”多管道设置的一部分时，应理想地利用`denoising_end`参数，如[`Refine Image Quality`](https://huggingface.co/docs/diffusers/using-diffusers/sdxl#refine-image-quality)中所述。

+   `guidance_scale` (`float`, *可选*, 默认为 7.5) — 在[无分类器扩散指导](https://arxiv.org/abs/2207.12598)中定义的指导尺度。`guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程 2 的`w`。通过设置`guidance_scale > 1`启用指导尺度。更高的指导尺度鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用来指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。在不使用指导时被忽略（即如果`guidance_scale`小于`1`，则被忽略）。

+   `negative_prompt_2` (`str` 或 `List[str]`, *可选*) — 用于指导发送到`tokenizer_2`和`text_encoder_2`的图像生成的提示或提示。如果未定义，将在两个文本编码器中使用`negative_prompt`。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *可选*, 默认为 0.0) — 对应于 DDIM 论文中的参数 eta (η)：[`arxiv.org/abs/2010.02502`](https://arxiv.org/abs/2010.02502)。仅适用于 schedulers.DDIMScheduler，对其他情况将被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个 [torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *可选*) — 预生成的嘈杂潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机`generator`进行采样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负面文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成 negative_prompt_embeds。

+   `pooled_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成池化的文本嵌入。

+   `negative_pooled_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负面池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成池化的 negative_prompt_embeds。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。在 [PIL](https://pillow.readthedocs.io/en/stable/) 中选择：`PIL.Image.Image` 或 `np.array`。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回`~pipelines.stable_diffusion.StableDiffusionXLPipelineOutput`而不是普通元组。

+   `cross_attention_kwargs` (`dict`, *可选*) — 如果指定，将传递给`AttentionProcessor`的 kwargs 字典，如在 [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) 中的`self.processor`中定义的那样。

+   `guidance_rescale` (`float`, *optional*, defaults to 0.0) — [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf)提出的指导缩放因子。`guidance_scale`在[Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf)的方程式 16 中定义为`φ`。指导缩放因子应在使用零终端信噪比时修复过曝光问题。

+   `original_size` (`Tuple[int]`, *optional*, defaults to (1024, 1024)) — 如果`original_size`与`target_size`不同，图像将呈现为缩小或放大。如果未指定，`original_size`默认为`(height, width)`。这是 SDXL 微调的一部分，如[`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952)第 2.2 节所述。

+   `crops_coords_top_left` (`Tuple[int]`, *optional*, defaults to (0, 0)) — `crops_coords_top_left`可用于生成一个看起来从位置`crops_coords_top_left`向下“裁剪”的图像。通常通过将`crops_coords_top_left`设置为(0, 0)来实现令人满意、居中的图像。这是 SDXL 微调的一部分，如[`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952)第 2.2 节所述。

+   `target_size` (`Tuple[int]`, *optional*, defaults to (1024, 1024)) — 对于大多数情况，`target_size`应设置为生成图像的期望高度和宽度。如果未指定，它将默认为`(height, width)`。这是 SDXL 微调的一部分，如[`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952)第 2.2 节所述。

+   `negative_original_size` (`Tuple[int]`, *optional*, defaults to (1024, 1024)) — 基于特定图像分辨率对生成过程进行负面调节。这是 SDXL 微调的一部分，如[`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952)第 2.2 节所述。有关更多信息，请参考此问题线程：[`github.com/huggingface/diffusers/issues/4208`](https://github.com/huggingface/diffusers/issues/4208)。

+   `negative_crops_coords_top_left` (`Tuple[int]`, *optional*, defaults to (0, 0)) — 基于特定裁剪坐标对生成过程进行负面调节。这是 SDXL 微调的一部分，如[`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952)第 2.2 节所述。有关更多信息，请参考此问题线程：[`github.com/huggingface/diffusers/issues/4208`](https://github.com/huggingface/diffusers/issues/4208)。

+   `negative_target_size` (`Tuple[int]`, *optional*, defaults to (1024, 1024)) — 基于目标图像分辨率对生成过程进行负面调节。对于大多数情况，它应与`target_size`相同。这是 SDXL 微调的一部分，如[`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952)第 2.2 节所述。有关更多信息，请参考此问题线程：[`github.com/huggingface/diffusers/issues/4208`](https://github.com/huggingface/diffusers/issues/4208)。

+   `aesthetic_score` (`float`, *optional*, defaults to 6.0) — 通过影响正文本条件来模拟生成图像的审美评分。这是 SDXL 微调的一部分，如[`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952)第 2.2 节所述。

+   `negative_aesthetic_score` (`float`, *optional*, defaults to 2.5) — 这是 SDXL 微调的一部分，如[`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952)第 2.2 节所述。可用于通过影响负文本条件来模拟生成图像的审美评分。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要从 CLIP 中跳过的层数。值为 1 意味着将使用预最终层的输出来计算提示嵌入。

+   `callback_on_step_end` (`Callable`, *可选*) — 在推断期间每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *可选*) — `callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在您的管道类的`._callback_tensor_inputs`属性中列出的变量。

返回

`~pipelines.stable_diffusion.StableDiffusionXLPipelineOutput` 或 `tuple`

`~pipelines.stable_diffusion.StableDiffusionXLPipelineOutput` 如果`return_dict`为 True，否则为`tuple`。当返回一个元组时，第一个元素是包含生成图像的列表。

调用管道生成时调用的函数。

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionXLImg2ImgPipeline
>>> from diffusers.utils import load_image

>>> pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(
...     "stabilityai/stable-diffusion-xl-refiner-1.0", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")
>>> url = "https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/aa_xl/000000009.png"

>>> init_image = load_image(url).convert("RGB")
>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> image = pipe(prompt, image=init_image).images[0]
```

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py#L893)

```py
( )
```

如果已启用，则禁用 FreeU 机制。

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py#L290)

```py
( )
```

禁用切片 VAE 解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `disable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py#L307)

```py
( )
```

禁用平铺 VAE 解码。如果之前启用了`enable_vae_tiling`，则此方法将返回到一步计算解码。

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py#L870)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1` (`float`) — 阶段 1 的缩放因子，用于减弱跳跃特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效果”。

+   `s2` (`float`) — 阶段 2 的缩放因子，用于减弱跳跃特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效果”。

+   `b1` (`float`) — 阶段 1 的缩放因子，用于放大骨干特征的贡献。

+   `b2` (`float`) — 阶段 2 的缩放因子，用于放大骨干特征的贡献。

启用 FreeU 机制，如[`arxiv.org/abs/2309.11497`](https://arxiv.org/abs/2309.11497)中所述。

缩放因子后缀表示它们被应用的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同管道（如 Stable Diffusion v1、v2 和 Stable Diffusion XL）的值组合。

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py#L282)

```py
( )
```

启用切片 VAE 解码。当启用此选项时，VAE 将将输入张量分割成片段，以便在多个步骤中计算解码。这对于节省一些内存并允许更大的批量大小很有用。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py#L298)

```py
( )
```

启用平铺的 VAE 解码。当启用此选项时，VAE 将将输入张量分割成瓦片，以便在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py#L315)

```py
( prompt: str prompt_2: Optional = None device: Optional = None num_images_per_prompt: int = 1 do_classifier_free_guidance: bool = True negative_prompt: Optional = None negative_prompt_2: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str`或`List[str]`，*可选*) — 要编码的提示

+   `prompt_2` (`str`或`List[str]`，*可选*) — 要发送到`tokenizer_2`和`text_encoder_2`的提示或提示。如果未定义，则在两个文本编码器中都使用`prompt`设备 — (`torch.device`): torch 设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用无分类器指导

+   `negative_prompt` (`str`或`List[str]`，*可选*) — 不指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。当不使用指导时（即，如果`guidance_scale`小于`1`，则忽略）。

+   `negative_prompt_2` (`str`或`List[str]`，*可选*) — 不指导图像生成的提示或提示，要发送到`tokenizer_2`和`text_encoder_2`。如果未定义，则在两个文本编码器中都使用`negative_prompt`

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数生成负文本嵌入。

+   `pooled_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的汇总文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`prompt`输入参数生成汇总文本嵌入。

+   `negative_pooled_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负汇总文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数生成负汇总文本嵌入。

+   `lora_scale` (`float`，*可选*) — 如果加载了 LoRA 层，则将应用于文本编码器的所有 LoRA 层的 lora 比例。

+   `clip_skip` (`int`，*可选*) — 在计算提示嵌入时要从 CLIP 中跳过的层数。值为 1 意味着将使用预最终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

#### `fuse_qkv_projections`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py#L898)

```py
( unet: bool = True vae: bool = True )
```

参数

+   `unet` (`bool`，默认为`True`) — 在 UNet 上应用融合。

+   `vae` (`bool`，默认为`True`) — 在 VAE 上应用融合。

启用融合的 QKV 投影。对于自注意力模块，所有投影矩阵（即查询、键、值）都被融合。对于交叉注意力模块，键和值投影矩阵被融合。

此 API 为🧪实验性质。

#### `get_guidance_scale_embedding`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py#L959)

```py
( w embedding_dim = 512 dtype = torch.float32 ) → export const metadata = 'undefined';torch.FloatTensor
```

参数

+   `timesteps` (`torch.Tensor`) — 在这些时间步生成嵌入向量

+   `embedding_dim` (`int`，*可选*，默认为 512) — 要生成的嵌入的维度 dtype — 生成的嵌入的数据类型

返回

`torch.FloatTensor`

形状为`(len(timesteps), embedding_dim)`的嵌入向量

参见[`github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298`](https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298)

#### `unfuse_qkv_projections`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py#L930)

```py
( unet: bool = True vae: bool = True )
```

参数

+   `unet` (`bool`, 默认为 `True`) — 对 UNet 应用融合。

+   `vae` (`bool`, 默认为 `True`) — 对 VAE 应用融合。

如果启用，则禁用 QKV 投影融合。

此 API 为🧪实验性质。

## StableDiffusionXLInpaintPipeline

### `class diffusers.StableDiffusionXLInpaintPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py#L312)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel text_encoder_2: CLIPTextModelWithProjection tokenizer: CLIPTokenizer tokenizer_2: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers image_encoder: CLIPVisionModelWithProjection = None feature_extractor: CLIPImageProcessor = None requires_aesthetics_score: bool = False force_zeros_for_empty_prompt: bool = True add_watermarker: Optional = None )
```

参数

+   `vae` (AutoencoderKL) — 变分自动编码器（VAE）模型，用于对图像进行编码和解码以及从潜在表示中解码图像。

+   `text_encoder` (`CLIPTextModel`) — 冻结的文本编码器。Stable Diffusion XL 使用[CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)的文本部分，具体来说是[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)变体。

+   `text_encoder_2` (`CLIPTextModelWithProjection`) — 第二个冻结的文本编码器。Stable Diffusion XL 使用[CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection)的文本和池部分，具体来说是[laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)变体。

+   `tokenizer` (`CLIPTokenizer`) — 类[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)的 Tokenizer。

+   `tokenizer_2` (`CLIPTokenizer`) — 第二个 Tokenizer 的类[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)。

+   `unet` (UNet2DConditionModel) — 用于去噪编码图像潜在特征的条件 U-Net 架构。

+   `scheduler` (SchedulerMixin) — 用于与`unet`结合使用以去噪编码图像潜在特征的调度器。可以是 DDIMScheduler、LMSDiscreteScheduler 或 PNDMScheduler 之一。

+   `requires_aesthetics_score` (`bool`, *可选*, 默认为 `"False"`) — `unet`在推断过程中是否需要通过审美分数条件。还请参阅`stabilityai/stable-diffusion-xl-refiner-1-0`的配置。

+   `force_zeros_for_empty_prompt` (`bool`, *可选*, 默认为 `"True"`) — 是否强制将负提示嵌入始终设置为 0。还请参阅`stabilityai/stable-diffusion-xl-base-1-0`的配置。

+   `add_watermarker` (`bool`, *可选*) — 是否使用[invisible_watermark 库](https://github.com/ShieldMnt/invisible-watermark/)来给输出图像添加水印。如果未定义，且该软件包已安装，则默认为 True，否则将不使用水印。

使用 Stable Diffusion XL 进行文本到图像生成的管道。

该模型继承自 DiffusionPipeline。查看超类文档以了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

该管道还继承了以下加载方法：

+   用于加载文本反演嵌入的 load_textual_inversion()

+   用于加载 `.ckpt` 文件的 from_single_file()

+   加载 LoRA 权重的 load_lora_weights()

+   用于保存 LoRA 权重的 `save_lora_weights()`

+   用于加载 IP 适配器的 load_ip_adapter()

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py#L1262)

```py
( prompt: Union = None prompt_2: Union = None image: Union = None mask_image: Union = None masked_image_latents: FloatTensor = None height: Optional = None width: Optional = None padding_mask_crop: Optional = None strength: float = 0.9999 num_inference_steps: int = 50 timesteps: List = None denoising_start: Optional = None denoising_end: Optional = None guidance_scale: float = 7.5 negative_prompt: Union = None negative_prompt_2: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None guidance_rescale: float = 0.0 original_size: Tuple = None crops_coords_top_left: Tuple = (0, 0) target_size: Tuple = None negative_original_size: Optional = None negative_crops_coords_top_left: Tuple = (0, 0) negative_target_size: Optional = None aesthetic_score: float = 6.0 negative_aesthetic_score: float = 2.5 clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';~pipelines.stable_diffusion.StableDiffusionXLPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 用于指导图像生成的提示或提示。如果未定义，则必须传递 `prompt_embeds`。

+   `prompt_2` (`str` 或 `List[str]`, *可选*) — 发送到 `tokenizer_2` 和 `text_encoder_2` 的提示或提示。如果未定义，则在两个文本编码器中使用 `prompt`。

+   `image` (`PIL.Image.Image`) — 将被修复的图像，或表示将被遮罩并根据 `prompt` 重新绘制部分的图像批次的张量。

+   `mask_image` (`PIL.Image.Image`) — 用于遮罩 `image` 的 `Image` 或表示图像批次的张量。遮罩中的白色像素将被重新绘制，而黑色像素将被保留。如果 `mask_image` 是 PIL 图像，则在使用之前将其转换为单通道（亮度）。如果是张量，则应该包含一个颜色通道（L）而不是 3，因此预期形状将是 `(B, H, W, 1)`。

+   `height` (`int`, *可选*, 默认为 self.unet.config.sample_size * self.vae_scale_factor) — 生成图像的像素高度。默认设置为 1024 以获得最佳结果。低于 512 像素的任何内容都不适用于 [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) 和未经专门调整低分辨率的检查点。

+   `width` (`int`, *可选*, 默认为 self.unet.config.sample_size * self.vae_scale_factor) — 生成图像的像素宽度。默认设置为 1024 以获得最佳结果。低于 512 像素的任何内容都不适用于 [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) 和未经专门调整低分辨率的检查点。

+   `padding_mask_crop` (`int`, *可选*, 默认为 `None`) — 用于在图像和遮罩上应用的裁剪边缘大小。如果为 `None`，则不对图像和 mask_image 应用裁剪。如果 `padding_mask_crop` 不是 `None`，它将首先找到一个具有与图像相同纵横比且包含所有遮罩区域的矩形区域，然后根据 `padding_mask_crop` 扩展该区域。然后根据扩展区域对图像和 mask_image 进行裁剪，然后将其调整为修复图像的原始大小。当遮罩区域较小而图像较大且包含与修复无关的信息（例如背景）时，这是有用的。

+   `strength` (`float`, *optional*, defaults to 0.9999) — 在概念上，指示要转换参考`image`的遮罩部分的程度。必须在 0 和 1 之间。`image`将被用作起点，添加的噪音越多，`strength`越大。降噪步骤的数量取决于最初添加的噪音量。当`strength`为 1 时，添加的噪音将达到最大值，并且降噪过程将运行指定的`num_inference_steps`中的全部迭代次数。因此，值为 1 的情况下，基本上忽略了参考`image`的遮罩部分。请注意，在将`denoising_start`声明为整数的情况下，将忽略`strength`的值。

+   `num_inference_steps` (`int`, *optional*, defaults to 50) — 降噪步骤的数量。更多的降噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `timesteps` (`List[int]`, *optional*) — 用于具有支持`timesteps`参数的调度器的自定义时间步长，其在其`set_timesteps`方法中支持。如果未定义，则将使用传递`num_inference_steps`时的默认行为。必须按降序排列。

+   `denoising_start` (`float`, *optional*) — 当指定时，指示在启动之前要绕过的总降噪过程的部分比例（介于 0.0 和 1.0 之间）。因此，跳过降噪过程的初始部分，并假定传递的`image`是部分降噪的图像。请注意，当指定此参数时，将忽略`strength`。当此管道集成到“混合降噪器”多管道设置中时，`denoising_start`参数特别有益，如[`Refining the Image Output`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)中详细说明的那样。

+   `denoising_end` (`float`, *optional*) — 当指定时，确定在故意提前终止之前要完成的总降噪过程的部分比例（介于 0.0 和 1.0 之间）。因此，返回的样本仍将保留相当数量的噪音（约还需要最后 20%的时间步），并且应由具有`denoising_start`设置为 0.8 的后续管道进行降噪，以便仅对调度器的最后 20%进行降噪。当此管道形成“混合降噪器”多管道设置的一部分时，应理想地利用`denoising_end`参数，如[`Refining the Image Output`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)中详细说明的那样。

+   `guidance_scale` (`float`, *optional*, defaults to 7.5) — 如[Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)中定义的指导比例。`guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程 2 的`w`。通过设置`guidance_scale > 1`来启用指导比例。更高的指导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 不指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即，如果`guidance_scale`小于`1`，则忽略）。

+   `negative_prompt_2` (`str` or `List[str]`, *optional*) — 不指导图像生成的提示或提示，将发送到`tokenizer_2`和`text_encoder_2`。如果未定义，则`negative_prompt`将在两个文本编码器中使用。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预先生成的负文本嵌入。可以用来轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成 negative_prompt_embeds。

+   `pooled_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预先生成的池化文本嵌入。可以用来轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成池化文本嵌入。

+   `negative_pooled_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预先生成的负池化文本嵌入。可以用来轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成池化的 negative_prompt_embeds。ip_adapter_image — (`PipelineImageInput`, *可选*): 可选的图像输入，用于与 IP 适配器一起使用。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *可选*, 默认为 0.0) — 对应于 DDIM 论文中的参数 eta (η)：[`arxiv.org/abs/2010.02502`](https://arxiv.org/abs/2010.02502)。仅适用于 schedulers.DDIMScheduler，对其他情况将被忽略。

+   `generator` (`torch.Generator`, *可选*) — 一个或多个[torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *可选*) — 预先生成的噪声潜变量，从高斯分布中采样，用作图像生成的输入。可以用来通过不同提示微调相同的生成。如果未提供，将使用提供的随机`generator`进行采样生成潜变量张量。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。在[PIL](https://pillow.readthedocs.io/en/stable/)之间选择：`PIL.Image.Image`或`np.array`。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回 StableDiffusionPipelineOutput 而不是普通元组。

+   `cross_attention_kwargs` (`dict`, *可选*) — 如果指定，将作为 kwargs 字典传递给`AttentionProcessor`，如[diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中的`self.processor`中定义的那样。

+   `original_size` (`Tuple[int]`, *可选*, 默认为(1024, 1024)) — 如果`original_size`与`target_size`不同，图像将呈现为缩小或放大。如果未指定，`original_size`默认为`(height, width)`。作为 SDXL 微调的一部分，如[`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952)第 2.2 节中所述。

+   `crops_coords_top_left` (`Tuple[int]`, *可选*, 默认为(0, 0)) — `crops_coords_top_left`可用于生成一个看起来从位置`crops_coords_top_left`向下“裁剪”的图像。通常通过将`crops_coords_top_left`设置为(0, 0)来实现有利的、居中的图像。作为 SDXL 微调的一部分，如[`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952)第 2.2 节中所述。

+   `target_size` (`Tuple[int]`, *可选*, 默认为(1024, 1024)) — 对于大多数情况，`target_size`应设置为生成图像的期望高度和宽度。如果未指定，它将默认为`(height, width)`。作为 SDXL 微调的一部分，如[`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952)第 2.2 节中所述。

+   `negative_original_size` (`Tuple[int]`, *optional*, 默认为 (1024, 1024)) — 根据特定图像分辨率对生成过程进行负面条件。作为 SDXL 的微调节的一部分，详见 [`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952) 的第 2.2 节。有关更多信息，请参考此问题线程：[`github.com/huggingface/diffusers/issues/4208`](https://github.com/huggingface/diffusers/issues/4208)。

+   `negative_crops_coords_top_left` (`Tuple[int]`, *optional*, 默认为 (0, 0)) — 根据特定裁剪坐标对生成过程进行负面条件。作为 SDXL 的微调节的一部分，详见 [`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952) 的第 2.2 节。有关更多信息，请参考此问题线程：[`github.com/huggingface/diffusers/issues/4208`](https://github.com/huggingface/diffusers/issues/4208)。

+   `negative_target_size` (`Tuple[int]`, *optional*, 默认为 (1024, 1024)) — 根据目标图像分辨率对生成过程进行负面条件。对于大多数情况，它应该与 `target_size` 相同。作为 SDXL 的微调节的一部分，详见 [`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952) 的第 2.2 节。有关更多信息，请参考此问题线程：[`github.com/huggingface/diffusers/issues/4208`](https://github.com/huggingface/diffusers/issues/4208)。

+   `aesthetic_score` (`float`, *optional*, 默认为 6.0) — 用于通过影响正面文本条件来模拟生成图像的美学评分。作为 SDXL 的微调节的一部分，详见 [`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952) 的第 2.2 节。

+   `negative_aesthetic_score` (`float`, *optional*, 默认为 2.5) — 作为 SDXL 的微调节的一部分，详见 [`huggingface.co/papers/2307.01952`](https://huggingface.co/papers/2307.01952) 的第 2.2 节。可用于通过影响负面文本条件来模拟生成图像的美学评分。

+   `clip_skip` (`int`, *optional*) — 在计算提示嵌入时要跳过的 CLIP 层的数量。值为 1 表示将使用前一层的输出来计算提示嵌入。

+   `callback_on_step_end` (`Callable`, *optional*) — 在推理期间每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs` 将包含由 `callback_on_step_end_tensor_inputs` 指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) — 用于 `callback_on_step_end` 函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包含在管道类的 `._callback_tensor_inputs` 属性中列出的变量。

返回

`~pipelines.stable_diffusion.StableDiffusionXLPipelineOutput` 或 `tuple`

如果 `return_dict` 为 True，则返回 `~pipelines.stable_diffusion.StableDiffusionXLPipelineOutput`，否则返回一个 `tuple`。返回元组时，第一个元素是包含生成图像的列表。

调用管道生成时调用的函数。

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionXLInpaintPipeline
>>> from diffusers.utils import load_image

>>> pipe = StableDiffusionXLInpaintPipeline.from_pretrained(
...     "stabilityai/stable-diffusion-xl-base-1.0",
...     torch_dtype=torch.float16,
...     variant="fp16",
...     use_safetensors=True,
... )
>>> pipe.to("cuda")

>>> img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
>>> mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"

>>> init_image = load_image(img_url).convert("RGB")
>>> mask_image = load_image(mask_url).convert("RGB")

>>> prompt = "A majestic tiger sitting on a bench"
>>> image = pipe(
...     prompt=prompt, image=init_image, mask_image=mask_image, num_inference_steps=50, strength=0.80
... ).images[0]
```

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py#L1129)

```py
( )
```

如果启用，将禁用 FreeU 机制。

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py#L441)

```py
( )
```

禁用切片 VAE 解码。如果之前启用了 `enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `disable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py#L458)

```py
( )
```

禁用平铺 VAE 解码。如果之前启用了 `enable_vae_tiling`，则此方法将返回到一步计算解码。

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py#L1106)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1` (`float`) — 第 1 阶段的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `s2` (`float`) — 第 2 阶段的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `b1` (`float`) — 第 1 阶段的缩放因子，用于放大骨干特征的贡献。

+   `b2` (`float`) — 第 2 阶段的缩放因子，用于放大骨干特征的贡献。

启用 FreeU 机制，如 [`arxiv.org/abs/2309.11497`](https://arxiv.org/abs/2309.11497)。

缩放因子后缀表示它们被应用的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同管道（如 Stable Diffusion v1、v2 和 Stable Diffusion XL）的值组合。

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py#L433)

```py
( )
```

启用切片 VAE 解码。启用此选项时，VAE 将分割输入张量以在多个步骤中计算解码。这对于节省一些内存并允许更大的批量大小非常有用。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py#L449)

```py
( )
```

启用平铺 VAE 解码。启用此选项时，VAE 将将输入张量分割成瓦片以在多个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py#L520)

```py
( prompt: str prompt_2: Optional = None device: Optional = None num_images_per_prompt: int = 1 do_classifier_free_guidance: bool = True negative_prompt: Optional = None negative_prompt_2: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示

+   `prompt_2` (`str` 或 `List[str]`, *可选*) — 发送到 `tokenizer_2` 和 `text_encoder_2` 的提示或提示。如果未定义，则在两个文本编码器设备中使用 `prompt` — (`torch.device`): torch 设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用分类器自由指导

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用来指导图像生成的提示或提示。如果未定义，则必须传递 `negative_prompt_embeds`。当不使用指导时被忽略（即如果 `guidance_scale` 小于 `1`，则被忽略）。

+   `negative_prompt_2` (`str` 或 `List[str]`, *可选*) — 用于发送到 `tokenizer_2` 和 `text_encoder_2` 的不用来指导图像生成的提示或提示。如果未定义，则在两个文本编码器中使用 `negative_prompt`

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从 `prompt` 输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从 `negative_prompt` 输入参数生成 `negative_prompt_embeds`。

+   `pooled_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从 `prompt` 输入参数生成池化文本嵌入。

+   `negative_pooled_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从 `negative_prompt` 输入参数生成池化负文本嵌入。

+   `lora_scale` (`float`, *可选*) — 将应用于文本编码器的所有 LoRA 层的 lora 比例，如果加载了 LoRA 层。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要从 CLIP 跳过的层数。值为 1 表示将使用前一层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

#### `fuse_qkv_projections`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py#L1134)

```py
( unet: bool = True vae: bool = True )
```

参数

+   `unet` (`bool`, 默认为 `True`) — 在 UNet 上应用融合。

+   `vae` (`bool`, 默认为 `True`) — 在 VAE 上应用融合。

启用融合的 QKV 投影。对于自注意力模块，所有投影矩阵（即查询、键、值）都被融合。对于交叉注意力模块，键和值投影矩阵被融合。

此 API 是 🧪 实验性的。

#### `get_guidance_scale_embedding`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py#L1195)

```py
( w embedding_dim = 512 dtype = torch.float32 ) → export const metadata = 'undefined';torch.FloatTensor
```

参数

+   `timesteps` (`torch.Tensor`) — 在这些时间步生成嵌入向量

+   `embedding_dim` (`int`, *可选*, 默认为 512) — 要生成的嵌入的维度 dtype — 生成的嵌入的数据类型

返回

`torch.FloatTensor`

形状为 `(len(timesteps), embedding_dim)` 的嵌入向量

参见 [`github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298`](https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298)

#### `unfuse_qkv_projections`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py#L1166)

```py
( unet: bool = True vae: bool = True )
```

参数

+   `unet` (`bool`, 默认为 `True`) — 在 UNet 上应用融合。

+   `vae` (`bool`, 默认为 `True`) — 在 VAE 上应用融合。

如果启用，禁用 QKV 投影融合。

此 API 是 🧪 实验性的。
