# aMUSEd

> 原文链接：[`huggingface.co/docs/diffusers/api/pipelines/amused`](https://huggingface.co/docs/diffusers/api/pipelines/amused)

aMUSEd 是由 Suraj Patil、William Berman、Robin Rombach 和 Patrick von Platen 在[aMUSEd: An Open MUSE Reproduction](https://huggingface.co/papers/2401.01808)中介绍的。

Amused 是基于[MUSE](https://arxiv.org/abs/2301.00704)架构的轻量级文本到图像模型。Amused 在需要轻量级和快速模型的应用中特别有用，比如一次快速生成多张图像。

Amused 是一个基于 vqvae 令牌的变换器，可以在比许多扩散模型更少的前向传递中生成图像。与 muse 相比，它使用较小的文本编码器 CLIP-L/14，而不是 t5-xxl。由于其较小的参数数量和较少的前向传递生成过程，amused 可以快速生成许多图像。这种好处在较大的批量大小下尤为明显。

论文摘要如下：

*我们提出了 aMUSEd，这是一个基于 MUSE 的开源、轻量级的遮罩图像模型（MIM），用于文本到图像的生成。与 MUSE 相比，aMUSEd 的参数量仅为 MUSE 的 10%，专注于快速图像生成。我们认为 MIM 相对于潜在扩散是未被充分探索的，后者是文本到图像生成的主流方法。与潜在扩散相比，MIM 需要较少的推理步骤，并且更具可解释性。此外，MIM 可以通过仅使用单个图像进行微调来学习其他风格。我们希望通过展示其在大规模文本到图像生成上的有效性并发布可复现的训练代码，鼓励进一步探索 MIM。我们还发布了两个直接生成 256x256 和 512x512 分辨率图像的模型的检查点。*

| 模型 | 参数 |
| --- | --- |
| [amused-256](https://huggingface.co/amused/amused-256) | 603M |
| [amused-512](https://huggingface.co/amused/amused-512) | 608M |

## AmusedPipeline

### `class diffusers.AmusedPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/amused/pipeline_amused.py#L44)

```py
( vqvae: VQModel tokenizer: CLIPTokenizer text_encoder: CLIPTextModelWithProjection transformer: UVit2DModel scheduler: AmusedScheduler )
```

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/amused/pipeline_amused.py#L74)

```py
( prompt: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 12 guidance_scale: float = 10.0 negative_prompt: Union = None num_images_per_prompt: Optional = 1 generator: Optional = None latents: Optional = None prompt_embeds: Optional = None encoder_hidden_states: Optional = None negative_prompt_embeds: Optional = None negative_encoder_hidden_states: Optional = None output_type = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None micro_conditioning_aesthetic_score: int = 6 micro_conditioning_crop_coord: Tuple = (0, 0) temperature: Union = (2, 0) ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt` (`str` or `List[str]`, *optional*) — 用于引导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。

+   `height` (`int`, *optional*, defaults to `self.transformer.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *optional*, defaults to 16) — 修复步骤的数量。更多的修复步骤通常会导致图像质量更高，但推理速度较慢。

+   `guidance_scale` (`float`, *optional*, defaults to 10.0) — 更高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 用于指导图像生成中不包括的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用引导时（`guidance_scale < 1`）将被忽略。

+   `num_images_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的图像数量。

+   `generator` (`torch.Generator`, *optional*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.IntTensor`, *optional*) — 表示在`self.vqvae`中使用的潜在向量的预生成令牌，用作图像生成的输入。如果未提供，则起始潜在向量将完全被屏蔽。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，文本嵌入将从`prompt`输入参数生成。来自池化和投影的最终隐藏状态的单个向量。

+   `encoder_hidden_states` (`torch.FloatTensor`, *可选*) — 文本编码器生成的倒数第二隐藏状态，提供额外的文本调节。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds`将从`negative_prompt`输入参数生成。

+   `negative_encoder_hidden_states` (`torch.FloatTensor`, *可选*) — 与正面提示的`encoder_hidden_states`类似。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的一个。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回 StableDiffusionPipelineOutput 而不是普通元组。

+   `callback` (`Callable`, *可选*) — 在推理期间每`callback_steps`步调用的函数。该函数使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *可选*, 默认为 1) — 调用`callback`函数的频率。如果未指定，将在每一步调用回调。

+   `cross_attention_kwargs` (`dict`, *可选*) — 如果指定，则作为`AttentionProcessor`的 kwargs 字典传递给`self.processor`中定义的`AttentionProcessor`。

+   `micro_conditioning_aesthetic_score` (`int`, *可选*, 默认为 6) — 根据 laion 审美分类器的目标审美评分。参见[`laion.ai/blog/laion-aesthetics/`](https://laion.ai/blog/laion-aesthetics/)和[`arxiv.org/abs/2307.01952`](https://arxiv.org/abs/2307.01952)的微调节部分。

+   `micro_conditioning_crop_coord` (`Tuple[int]`, *可选*, 默认为(0, 0)) — 目标高度、宽度裁剪坐标。参见[`arxiv.org/abs/2307.01952`](https://arxiv.org/abs/2307.01952)的微调节部分。

+   `temperature` (`Union[int, Tuple[int, int], List[int]]`, *可选*, 默认为(2, 0)) — 配置`self.scheduler`上的温度调度器，参见`AmusedScheduler#set_timesteps`。

返回

ImagePipelineOutput 或`tuple`

如果`return_dict`为`True`，则返回 ImagePipelineOutput，否则返回一个`tuple`，其中第一个元素是包含生成图像的列表。

用于生成的管道的调用函数。

示例：

```py
>>> import torch
>>> from diffusers import AmusedPipeline

>>> pipe = AmusedPipeline.from_pretrained(
...     "amused/amused-512", variant="fp16", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> image = pipe(prompt).images[0]
```

#### `enable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

参数

+   `attention_op` (`Callable`, *可选*) — 用作`xFormers`的[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)函数的`op`参数的默认`None`操作符的覆盖。

从[xFormers](https://facebookresearch.github.io/xformers/)启用内存高效注意力。启用此选项时，您应该观察到较低的 GPU 内存使用量和推理期间的潜在加速。训练期间的加速不被保证。

⚠️ 当启用内存高效注意力和切片注意力时，内存高效注意力优先。

示例：

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

从[xFormers](https://facebookresearch.github.io/xformers/)中禁用内存高效注意力。

### `class diffusers.AmusedImg2ImgPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/amused/pipeline_amused_img2img.py#L52)

```py
( vqvae: VQModel tokenizer: CLIPTokenizer text_encoder: CLIPTextModelWithProjection transformer: UVit2DModel scheduler: AmusedScheduler )
```

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/amused/pipeline_amused_img2img.py#L87)

```py
( prompt: Union = None image: Union = None strength: float = 0.5 num_inference_steps: int = 12 guidance_scale: float = 10.0 negative_prompt: Union = None num_images_per_prompt: Optional = 1 generator: Optional = None prompt_embeds: Optional = None encoder_hidden_states: Optional = None negative_prompt_embeds: Optional = None negative_encoder_hidden_states: Optional = None output_type = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None micro_conditioning_aesthetic_score: int = 6 micro_conditioning_crop_coord: Tuple = (0, 0) temperature: Union = (2, 0) ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 指导图像生成的提示。如果未定义，则需要传递 `prompt_embeds`。

+   `image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, 或 `List[np.ndarray]`) — 代表要用作起点的图像批次的图像、numpy 数组或张量。对于 numpy 数组和 pytorch 张量，期望值范围在 `[0, 1]` 之间。如果是张量或张量列表，则期望形状应为 `(B, C, H, W)` 或 `(C, H, W)`。如果是 numpy 数组或数组列表，则期望形状应为 `(B, H, W, C)` 或 `(H, W, C)`。也可以将图像潜变量作为 `image`，但如果直接传递潜变量，则不会再次编码。

+   `strength` (`float`, *可选*, 默认为 0.5) — 表示转换参考 `image` 的程度。必须在 0 和 1 之间。`image` 用作起点，`strength` 越高，添加的噪音越多。去噪步骤的数量取决于最初添加的噪音量。当 `strength` 为 1 时，添加的噪音最大，去噪过程将运行指定的 `num_inference_steps` 的完整迭代次数。值为 1 实质上忽略了 `image`。

+   `num_inference_steps` (`int`, *可选*, 默认为 16) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *可选*, 默认为 10.0) — 更高的指导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用指导比例。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 指导图像生成时不包含的提示。如果未定义，则需要传递 `negative_prompt_embeds`。在不使用指导时被忽略 (`guidance_scale < 1`)。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `generator` (`torch.Generator`, *可选*) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入 (提示加权)。如果未提供，则文本嵌入将从 `prompt` 输入参数生成。来自汇总和投影的最终隐藏状态的单个向量。

+   `encoder_hidden_states` (`torch.FloatTensor`, *可选*) — 从文本编码器中预生成的倒数第二隐藏状态，提供额外的文本调节。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负面文本嵌入。可用于轻松调整文本输入 (提示加权)。如果未提供，`negative_prompt_embeds` 将从 `negative_prompt` 输入参数生成。

+   `negative_encoder_hidden_states` (`torch.FloatTensor`, *可选*) — 与正面提示的 `encoder_hidden_states` 类似。

+   `output_type` (`str`, *可选*, 默认为 `"pil"`) — 生成图像的输出格式。选择 `PIL.Image` 或 `np.array` 之间。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回 StableDiffusionPipelineOutput 而不是普通元组。

+   `callback` (`Callable`, *可选*) — 在推断期间每`callback_steps`步调用的函数。该函数使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *可选*, 默认为 1) — 调用`callback`函数的频率。如果未指定，则在每一步调用回调。

+   `cross_attention_kwargs` (`dict`, *可选*) — 如果指定，将传递给`AttentionProcessor`的 kwargs 字典，如在[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的。

+   `micro_conditioning_aesthetic_score` (`int`, *可选*, 默认为 6) — 根据 laion 审美分类器的目标审美评分。请参阅[`laion.ai/blog/laion-aesthetics/`](https://laion.ai/blog/laion-aesthetics/)和[`arxiv.org/abs/2307.01952`](https://arxiv.org/abs/2307.01952)的微调节部分。

+   `micro_conditioning_crop_coord` (`Tuple[int]`, *可选*, 默认为(0, 0)) — 目标高度、宽度裁剪坐标。请参阅[`arxiv.org/abs/2307.01952`](https://arxiv.org/abs/2307.01952)的微调节部分。

+   `temperature` (`Union[int, Tuple[int, int], List[int]]`, *可选*, 默认为(2, 0)) — 配置`self.scheduler`上的温度调度程序，参见`AmusedScheduler#set_timesteps`。

返回

ImagePipelineOutput 或`tuple`

如果`return_dict`为`True`，则返回 ImagePipelineOutput，否则返回一个`tuple`，其中第一个元素是包含生成图像的列表。

用于生成的管道的调用函数。

示例：

```py
>>> import torch
>>> from diffusers import AmusedImg2ImgPipeline
>>> from diffusers.utils import load_image

>>> pipe = AmusedImg2ImgPipeline.from_pretrained(
...     "amused/amused-512", variant="fp16", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> prompt = "winter mountains"
>>> input_image = (
...     load_image(
...         "https://huggingface.co/datasets/diffusers/docs-images/resolve/main/open_muse/mountains.jpg"
...     )
...     .resize((512, 512))
...     .convert("RGB")
... )
>>> image = pipe(prompt, input_image).images[0]
```

#### `enable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

参数

+   `attention_op` (`Callable`, *可选*) — 用作 xFormers 的[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)函数的`op`参数的默认`None`操作符的覆盖。

启用[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。启用此选项时，您应该观察到较低的 GPU 内存使用量和潜在的推断加速。训练期间的加速不被保证。

⚠️ 当启用内存高效注意力和切片注意力时，内存高效注意力优先。

示例：

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

禁用[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。

### `class diffusers.AmusedInpaintPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/amused/pipeline_amused_inpaint.py#L60)

```py
( vqvae: VQModel tokenizer: CLIPTokenizer text_encoder: CLIPTextModelWithProjection transformer: UVit2DModel scheduler: AmusedScheduler )
```

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/amused/pipeline_amused_inpaint.py#L103)

```py
( prompt: Union = None image: Union = None mask_image: Union = None strength: float = 1.0 num_inference_steps: int = 12 guidance_scale: float = 10.0 negative_prompt: Union = None num_images_per_prompt: Optional = 1 generator: Optional = None prompt_embeds: Optional = None encoder_hidden_states: Optional = None negative_prompt_embeds: Optional = None negative_encoder_hidden_states: Optional = None output_type = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None micro_conditioning_aesthetic_score: int = 6 micro_conditioning_crop_coord: Tuple = (0, 0) temperature: Union = (2, 0) ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 用于引导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。

+   `image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, 或 `List[np.ndarray]`) — 代表要用作起点的图像批次的`Image`、numpy 数组或张量。对于 numpy 数组和 pytorch 张量，期望的值范围在`[0, 1]`之间。如果是张量或张量列表，则期望的形状应为`(B, C, H, W)`或`(C, H, W)`。如果是 numpy 数组或数组列表，则期望的形状应为`(B, H, W, C)`或`(H, W, C)`。它还可以接受图像潜变量作为`image`，但如果直接传递潜变量，则不会再次编码。

+   `mask_image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, 或 `List[np.ndarray]`) — 代表要遮罩`image`的图像批次的`Image`、numpy 数组或张量。遮罩中的白色像素被重新绘制，而黑色像素被保留。如果`mask_image`是 PIL 图像，则在使用之前将其转换为单通道（亮度）。如果是 numpy 数组或 pytorch 张量，则应包含一个颜色通道（L），而不是 3 个，因此 pytorch 张量的预期形状为`(B, 1, H, W)`、`(B, H, W)`、`(1, H, W)`、`(H, W)`。对于 numpy 数组，预期形状为`(B, H, W, 1)`、`(B, H, W)`、`(H, W, 1)`或`(H, W)`。

+   `strength` (`float`, *optional*, 默认为 1.0) — 指示转换参考`image`的程度。必须在 0 和 1 之间。`image`用作起点，`strength`越高，添加的噪音越多。降噪步骤的数量取决于最初添加的噪音量。当`strength`为 1 时，添加的噪音最大，降噪过程将运行指定的`num_inference_steps`的完整迭代次数。值为 1 基本上忽略`image`。

+   `num_inference_steps` (`int`, *optional*, 默认为 16) — 降噪步骤的数量。更多的降噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `guidance_scale` (`float`, *optional*, 默认为 10.0) — 更高的指导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用指导比例。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 用于指导在图像生成中不包括什么的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用指导时被忽略（`guidance_scale < 1`）。

+   `num_images_per_prompt` (`int`, *optional*, 默认为 1) — 每个提示生成的图像数量。

+   `generator` (`torch.Generator`, *optional*) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从`prompt`输入参数生成文本嵌入。来自汇总和投影的最终隐藏状态的单个向量。

+   `encoder_hidden_states` (`torch.FloatTensor`, *optional*) — 从文本编码器中预生成的倒数第二隐藏状态，提供额外的文本调节。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds`将从`negative_prompt`输入参数生成。

+   `negative_encoder_hidden_states` (`torch.FloatTensor`, *optional*) — 与正提示的`encoder_hidden_states`类似。

+   `output_type` (`str`, *optional*, 默认为`"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间。

+   `return_dict` (`bool`, *optional*, 默认为 `True`) — 是否返回 StableDiffusionPipelineOutput 而不是普通的 tuple。

+   `callback` (`Callable`, *optional*) — 在推理过程中每 `callback_steps` 步调用的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *optional*, 默认为 1) — 调用 `callback` 函数的频率。如果未指定，则在每一步都会调用回调函数。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给 [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) 中定义的 `AttentionProcessor` 的 kwargs 字典。

+   `micro_conditioning_aesthetic_score` (`int`, *optional*, 默认为 6) — 根据 laion 美学分类器的目标美学分数。请参阅 [`laion.ai/blog/laion-aesthetics/`](https://laion.ai/blog/laion-aesthetics/) 和 [`arxiv.org/abs/2307.01952`](https://arxiv.org/abs/2307.01952) 的微调节部分。

+   `micro_conditioning_crop_coord` (`Tuple[int]`, *optional*, 默认为 (0, 0)) — 目标高度、宽度裁剪坐标。请参阅 [`arxiv.org/abs/2307.01952`](https://arxiv.org/abs/2307.01952) 的微调节部分。

+   `temperature` (`Union[int, Tuple[int, int], List[int]]`, *optional*, 默认为 (2, 0)) — 配置 `self.scheduler` 上的温度调度器，参见 `AmusedScheduler#set_timesteps`。

返回

ImagePipelineOutput 或 `tuple`

如果 `return_dict` 为 `True`，则返回 ImagePipelineOutput，否则返回一个 `tuple`，其中第一个元素是生成的图像的列表。

用于生成的管道的调用函数。

示例：

```py
>>> import torch
>>> from diffusers import AmusedInpaintPipeline
>>> from diffusers.utils import load_image

>>> pipe = AmusedInpaintPipeline.from_pretrained(
...     "amused/amused-512", variant="fp16", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> prompt = "fall mountains"
>>> input_image = (
...     load_image(
...         "https://huggingface.co/datasets/diffusers/docs-images/resolve/main/open_muse/mountains_1.jpg"
...     )
...     .resize((512, 512))
...     .convert("RGB")
... )
>>> mask = (
...     load_image(
...         "https://huggingface.co/datasets/diffusers/docs-images/resolve/main/open_muse/mountains_1_mask.png"
...     )
...     .resize((512, 512))
...     .convert("L")
... )
>>> pipe(prompt, input_image, mask).images[0].save("out.png")
```

#### `enable_xformers_memory_efficient_attention`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

参数

+   `attention_op` (`Callable`, *optional*) — 覆盖用作 xFormers 的 [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention) 函数的 `op` 参数的默认 `None` 操作符。

启用来自 [xFormers](https://facebookresearch.github.io/xformers/) 的内存高效注意力。启用此选项时，您应该观察到更低的 GPU 内存使用量，并在推理过程中可能加速。训练过程中的加速不被保证。

⚠️ 当内存高效注意力和切片注意力同时启用时，内存高效注意力优先。

示例：

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

禁用来自 [xFormers](https://facebookresearch.github.io/xformers/) 的内存高效注意力。
