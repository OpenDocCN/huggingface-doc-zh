# UVit2DModel

> 原始文本：[`huggingface.co/docs/diffusers/api/models/uvit2d`](https://huggingface.co/docs/diffusers/api/models/uvit2d)

[U-ViT](https://hf.co/papers/2301.11093) 模型是基于视觉变换器（ViT）的 UNet。该模型结合了 ViT（将所有输入，如时间、条件和嘈杂的图像补丁视为标记）和 UNet（浅层和深层之间的长跳连接）的元素。跳连接对于预测像素级特征很重要。在最终输出之前应用了额外的 3x3 卷积块以提高图像质量。

论文摘要如下：

*目前，在高分辨率图像的像素空间中应用扩散模型是困难的。相反，现有方法侧重于在较低维度空间（潜在扩散）中进行扩散，或者具有多个被称为级联的超分辨率生成级别。缺点是这些方法会给扩散框架增加额外的复杂性。本文旨在改进高分辨率图像的去噪扩散，同时尽可能保持模型简单。本文围绕研究问题展开：如何在高分辨率图像上训练标准的去噪扩散模型，并且仍然获得与这些替代方法可比的性能？四个主要发现是：1）噪声计划应针对高分辨率图像进行调整，2）只需对架构的特定部分进行缩放即可，3）应在架构的特定位置添加辍学，4）下采样是避免高分辨率特征图的有效策略。结合这些简单而有效的技术，我们在 ImageNet 上实现了扩散模型中的图像生成的最新技术。*

## UVit2DModel

### `class diffusers.UVit2DModel`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L39)

```py
( hidden_size: int = 1024 use_bias: bool = False hidden_dropout: float = 0.0 cond_embed_dim: int = 768 micro_cond_encode_dim: int = 256 micro_cond_embed_dim: int = 1280 encoder_hidden_size: int = 768 vocab_size: int = 8256 codebook_size: int = 8192 in_channels: int = 768 block_out_channels: int = 768 num_res_blocks: int = 3 downsample: bool = False upsample: bool = False block_num_heads: int = 12 num_hidden_layers: int = 22 num_attention_heads: int = 16 attention_dropout: float = 0.0 intermediate_size: int = 2816 layer_norm_eps: float = 1e-06 ln_elementwise_affine: bool = True sample_size: int = 64 )
```

#### `set_attn_processor`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L241)

```py
( processor: Union )
```

参数

+   `processor`（`AttentionProcessor`的字典或仅`AttentionProcessor`）— 实例化的处理器类或将设置为**所有**`Attention`层的处理器的处理器类字典。

    如果`processor`是一个字典，则键需要定义到相应交叉注意力处理器的路径。在设置可训练的注意力处理器时，强烈建议这样做。

设置用于计算注意力的注意力处理器。

#### `set_default_attn_processor`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L276)

```py
( )
```

禁用自定义注意力处理器并设置默认的注意力实现。

## UVit2DConvEmbed

### `class diffusers.models.unets.uvit_2d.UVit2DConvEmbed`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L292)

```py
( in_channels block_out_channels vocab_size elementwise_affine eps bias )
```

## UVitBlock

### `class diffusers.models.unets.uvit_2d.UVitBlock`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L307)

```py
( channels num_res_blocks: int hidden_size hidden_dropout ln_elementwise_affine layer_norm_eps use_bias block_num_heads attention_dropout downsample: bool upsample: bool )
```

## ConvNextBlock

### `class diffusers.models.unets.uvit_2d.ConvNextBlock`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L406)

```py
( channels layer_norm_eps ln_elementwise_affine use_bias hidden_dropout hidden_size res_ffn_factor = 4 )
```

## ConvMlmLayer

### `class diffusers.models.unets.uvit_2d.ConvMlmLayer`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L451)

```py
( block_out_channels: int in_channels: int use_bias: bool ln_elementwise_affine: bool layer_norm_eps: float codebook_size: int )
```
