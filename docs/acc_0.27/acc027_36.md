# 在 TPU 上使用🤗 Accelerate 进行训练

> 原始文本：[`huggingface.co/docs/accelerate/concept_guides/training_tpu`](https://huggingface.co/docs/accelerate/concept_guides/training_tpu)

在 TPU 上训练可能与在多 GPU 上训练略有不同，即使使用🤗 Accelerate。本指南旨在向您展示应该注意的地方以及原因，以及一般的最佳实践。

## 在笔记本中训练

在 TPU 上训练时的主要关注点来自 notebook_launcher()。如在 notebook 教程中提到的，您需要将训练代码重构为一个可以传递给 notebook_launcher()函数的函数，并注意不要在 GPU 上声明任何张量。

在 TPU 上，最后一部分并不那么重要，需要理解的一个关键部分是，当您从笔记本启动代码时，您是通过一种称为**forking**的过程进行的。当从命令行启动时，您执行**spawning**，其中 python 进程当前未运行，您*生成*一个新进程。由于您的 Jupyter 笔记本已经在使用 python 进程，因此您需要从中*fork*一个新进程来启动您的代码。

这一点的重要性在于声明您的模型。在分叉的 TPU 进程中，建议您实例化您的模型*一次*，并将其传递给您的训练函数。这与在 GPU 上训练时创建`n`个在某些时刻同步梯度并反向传播的模型不同。相反，一个模型实例在所有节点之间共享，并来回传递。这在训练低资源 TPU 时尤为重要，例如在 Kaggle 内核或 Google Colaboratory 中提供的 TPU 上。

以下是一个传递给 notebook_launcher()的训练函数示例，如果在 CPU 或 GPU 上训练：

此代码片段基于`simple_nlp_example`笔记本中的代码，稍作修改以简化

```py
def training_function():
    # Initialize accelerator
    accelerator = Accelerator()
    model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=2)
    train_dataloader, eval_dataloader = create_dataloaders(
        train_batch_size=hyperparameters["train_batch_size"], eval_batch_size=hyperparameters["eval_batch_size"]
    )

    # Instantiate optimizer
    optimizer = AdamW(params=model.parameters(), lr=hyperparameters["learning_rate"])

    # Prepare everything
    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the
    # prepare method.
    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader
    )

    num_epochs = hyperparameters["num_epochs"]
    # Now we train the model
    for epoch in range(num_epochs):
        model.train()
        for step, batch in enumerate(train_dataloader):
            outputs = model(**batch)
            loss = outputs.loss
            accelerator.backward(loss)

            optimizer.step()
            optimizer.zero_grad()
```

```py
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

如果🤗 Accelerate 已配置为 TPU，则`notebook_launcher`将默认为 8 个进程

如果您在训练循环中使用此示例并在其中声明模型，则在低资源系统上可能会看到错误，如下所示：

```py
ProcessExitedException: process 0 terminated with signal SIGSEGV
```

这个错误*极其*难以理解，但基本解释是您的系统 RAM 用完了。您可以通过重新配置训练函数以接受单个`model`参数并在外部单元格中声明它来完全避免这种情况：

```py
# In another Jupyter cell
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=2)
```

```py
+ def training_function(model):
      # Initialize accelerator
      accelerator = Accelerator()
-     model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=2)
      train_dataloader, eval_dataloader = create_dataloaders(
          train_batch_size=hyperparameters["train_batch_size"], eval_batch_size=hyperparameters["eval_batch_size"]
      )
  ...
```

最后调用训练函数：

```py
  from accelerate import notebook_launcher
- notebook_launcher(training_function)
+ notebook_launcher(training_function, (model,))
```

上述解决方法仅在从 Jupyter Notebook 启动 TPU 实例时需要，例如在 Google Colaboratory 或 Kaggle 等低资源服务器上。如果使用脚本或在更强大的服务器上启动，则不需要事先声明模型。

## 混合精度和全局变量

如在混合精度教程中提到的，🤗 Accelerate 支持 fp16 和 bf16，两者都可以在 TPU 上使用。也就是说，理想情况下应该使用`bf16`，因为它非常高效。

在 TPU 上使用`bf16`和🤗 Accelerate 时，有两个“层”，即基本级别和操作级别。

在基本级别上，当将`mixed_precision="bf16"`传递给`Accelerator`时，这是启用的，例如：

```py
accelerator = Accelerator(mixed_precision="bf16")
```

默认情况下，在 TPU 上将`torch.float`和`torch.double`转换为`bfloat16`。设置的具体配置是将环境变量`XLA_USE_BF16`设置为`1`。

您可以执行进一步的配置，即设置`XLA_DOWNCAST_BF16`环境变量。如果设置为`1`，那么`torch.float`是`bfloat16`，`torch.double`是`float32`。

这是在传递`downcast_bf16=True`时在`Accelerator`对象中执行的。

```py
accelerator = Accelerator(mixed_precision="bf16", downcast_bf16=True)
```

在计算指标、记录值等需要使用原始 bf16 张量时，使用 downcasting 而不是 bf16 是很好的选择。

## TPU 上的训练时间

当您启动脚本时，您可能会注意到训练一开始似乎异常缓慢。这是因为 TPU 首先运行几批数据，以查看需要分配多少内存，然后才能极其高效地利用这个配置的内存分配。

如果您注意到评估代码计算模型指标所需的时间更长，这是因为使用了更大的批量大小，建议如果速度太慢，则保持批量大小与训练数据相同。否则，在前几次迭代之后，内存将重新分配到这个新的批量大小。

仅仅因为内存被分配并不意味着它会被使用，或者当返回到训练数据加载器时批量大小会增加。
