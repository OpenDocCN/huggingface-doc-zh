# 使用 Local SGD 与🤗 Accelerate

> 原始文本：[`huggingface.co/docs/accelerate/usage_guides/local_sgd`](https://huggingface.co/docs/accelerate/usage_guides/local_sgd)

Local SGD 是一种分布式训练技术，其中梯度不是每一步同步的。因此，每个进程更新自己的模型权重版本，并在一定数量的步骤之后通过对所有进程进行平均来同步这些权重。这提高了通信效率，尤其是在计算机缺乏更快的互连（如 NVLink）时，可以显著加快训练速度。与梯度累积不同（其中提高通信效率需要增加有效批量大小），Local SGD 不需要更改批量大小或学习率/调度。但是，如果必要，Local SGD 也可以与梯度累积结合使用。

在本教程中，您将看到如何快速设置 Local SGD 🤗 Accelerate。与标准的 Accelerate 设置相比，这只需要额外两行代码。

这个例子将使用一个非常简单的 PyTorch 训练循环，每两个批次执行一次梯度累积：

```py
device = "cuda"
model.to(device)

gradient_accumulation_steps = 2

for index, batch in enumerate(training_dataloader):
    inputs, targets = batch
    inputs = inputs.to(device)
    targets = targets.to(device)
    outputs = model(inputs)
    loss = loss_function(outputs, targets)
    loss = loss / gradient_accumulation_steps
    loss.backward()
    if (index + 1) % gradient_accumulation_steps == 0:
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
```

## 转换为🤗 Accelerate

首先，之前显示的代码将被转换为使用🤗 Accelerate，既不使用 LocalSGD 也不使用梯度累积助手：

```py
+ from accelerate import Accelerator
+ accelerator = Accelerator()

+ model, optimizer, training_dataloader, scheduler = accelerator.prepare(
+     model, optimizer, training_dataloader, scheduler
+ )

  for index, batch in enumerate(training_dataloader):
      inputs, targets = batch
-     inputs = inputs.to(device)
-     targets = targets.to(device)
      outputs = model(inputs)
      loss = loss_function(outputs, targets)
      loss = loss / gradient_accumulation_steps
+     accelerator.backward(loss)
      if (index+1) % gradient_accumulation_steps == 0:
          optimizer.step()
          scheduler.step()
```

## 让🤗 Accelerate 处理模型同步

现在剩下的就是让🤗 Accelerate 处理模型参数同步**和**梯度累积。为简单起见，让我们假设我们需要每 8 步同步一次。通过添加一个`with LocalSGD`语句和在每个优化器步骤之后调用`local_sgd.step()`来实现：

```py
+local_sgd_steps=8

+with LocalSGD(accelerator=accelerator, model=model, local_sgd_steps=8, enabled=True) as local_sgd:
    for batch in training_dataloader:
        with accelerator.accumulate(model):
            inputs, targets = batch
            outputs = model(inputs)
            loss = loss_function(outputs, targets)
            accelerator.backward(loss)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
+           local_sgd.step()
```

在底层，Local SGD 代码**禁用**了自动梯度同步（但累积仍然按预期工作！）。相反，它每`local_sgd_steps`步平均模型参数（以及在训练循环结束时）。

## 限制

当前实现仅适用于基本的多 GPU（或多 CPU）训练，不包括例如[DeepSpeed.](https://github.com/microsoft/DeepSpeed)。

## 参考资料

尽管我们不清楚这种简单方法的真正起源，但本地 SGD 的想法非常古老，至少可以追溯到：

Zhang, J., De Sa, C., Mitliagkas, I., & Ré, C. (2016). [Parallel SGD: When does averaging help?. arXiv preprint arXiv:1606.07365.](https://arxiv.org/abs/1606.07365)

我们将 Local SGD 这个术语归功于以下论文（但可能有我们不知道的更早的参考文献）。

Stich, Sebastian Urban. [“Local SGD Converges Fast and Communicates Little.” ICLR 2019-International Conference on Learning Representations. No. CONF. 2019.](https://arxiv.org/abs/1805.09767)
