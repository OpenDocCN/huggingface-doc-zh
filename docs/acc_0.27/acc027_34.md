# æ¢¯åº¦åŒæ­¥

> åŸæ–‡é“¾æ¥ï¼š[`huggingface.co/docs/accelerate/concept_guides/gradient_synchronization`](https://huggingface.co/docs/accelerate/concept_guides/gradient_synchronization)

PyTorch çš„åˆ†å¸ƒå¼æ¨¡å—é€šè¿‡åœ¨ç³»ç»Ÿä¸­çš„æ‰€æœ‰ GPU ä¹‹é—´æ¥å›é€šä¿¡æ¥è¿è¡Œã€‚è¿™ç§é€šä¿¡éœ€è¦æ—¶é—´ï¼Œå¹¶ä¸”ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åœ¨ä½¿ç”¨`ddp`æ¨¡å—æ—¶åœ¨ç‰¹å®šè§¦å‘ç‚¹çŸ¥é“å½¼æ­¤çš„çŠ¶æ€ã€‚

è¿™äº›è§¦å‘ç‚¹è¢«æ·»åŠ åˆ° PyTorch æ¨¡å‹ä¸­ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬çš„`forward()`å’Œ`backward()`æ–¹æ³•ã€‚å½“æ¨¡å‹è¢«åŒ…è£…ä¸º`DistributedDataParallel`æ—¶ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µï¼š

```py
import torch.nn as nn
from torch.nn.parallel import DistributedDataParallel

model = nn.Linear(10, 10)
ddp_model = DistributedDataParallel(model)
```

åœ¨ğŸ¤— Accelerate ä¸­ï¼Œå½“è°ƒç”¨ prepare()å¹¶ä¼ å…¥æ‚¨çš„æ¨¡å‹æ—¶ï¼Œæ­¤è½¬æ¢ä¼šè‡ªåŠ¨å‘ç”Ÿã€‚

```py
+ from accelerate import Accelerator
+ accelerator = Accelerator()
  import torch.nn as nn
- from torch.nn.parallel import DistributedDataParallel

  model = nn.Linear(10,10)
+ model = accelerator.prepare(model)
```

## æ¢¯åº¦ç´¯ç§¯ä¸­çš„å‡é€Ÿ

æ‚¨ç°åœ¨äº†è§£åˆ°ï¼Œåœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­è®­ç»ƒæ—¶ï¼ŒPyTorch ä¼šå‘æ‚¨çš„ PyTorch æ¨¡å‹çš„`forward`å’Œ`backward`æ–¹æ³•æ·»åŠ é’©å­ã€‚ä½†æ˜¯è¿™å¦‚ä½•ä¼šå¯¼è‡´æ‚¨çš„ä»£ç å‡é€Ÿå‘¢ï¼Ÿ

åœ¨ DDPï¼ˆåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼‰ä¸­ï¼Œæ‰§è¡Œå’Œè¿è¡Œè¿›ç¨‹çš„ç‰¹å®šé¡ºåºé¢„æœŸåœ¨ç‰¹å®šç‚¹å‘ç”Ÿï¼Œå¹¶ä¸”åœ¨ç»§ç»­ä¹‹å‰å¿…é¡»å¤§è‡´åŒæ—¶å‘ç”Ÿã€‚

æœ€ç›´æ¥çš„ä¾‹å­æ˜¯å½“æ‚¨é€šè¿‡`optimizer.step()`æ›´æ–°æ¨¡å‹å‚æ•°æ—¶ã€‚æ²¡æœ‰æ¢¯åº¦ç´¯ç§¯ï¼Œæ‰€æœ‰æ¨¡å‹å®ä¾‹éœ€è¦åœ¨ç»§ç»­ä¸‹ä¸€ä¸ªæ•°æ®æ‰¹æ¬¡ä¹‹å‰æ›´æ–°å®ƒä»¬çš„æ¢¯åº¦è®¡ç®—ã€æ•´åˆå’Œæ›´æ–°ã€‚è¿›è¡Œæ¢¯åº¦ç´¯ç§¯æ—¶ï¼Œæ‚¨ç´¯ç§¯`n`ä¸ªæŸå¤±æ¢¯åº¦ï¼Œå¹¶åœ¨è¾¾åˆ°`n`ä¸ªæ‰¹æ¬¡ä¹‹å‰è·³è¿‡`optimizer.step()`ã€‚ç”±äºæ‰€æœ‰è®­ç»ƒè¿‡ç¨‹åªéœ€è¦åœ¨è°ƒç”¨`optimizer.step()`æ—¶åŒæ­¥ï¼Œè€Œä¸éœ€è¦å¯¹è®­ç»ƒæ­¥éª¤è¿›è¡Œä»»ä½•ä¿®æ”¹ï¼Œè¿™ç§ä¸å¿…è¦çš„è¿›ç¨‹é—´é€šä¿¡å¯èƒ½ä¼šå¯¼è‡´æ˜¾è‘—çš„å‡é€Ÿã€‚

å¦‚ä½•é¿å…è¿™ç§å¼€é”€ï¼Ÿ

## è§£å†³å‡é€Ÿé—®é¢˜

ç”±äºåœ¨è¿™äº›æ‰¹æ¬¡ä¸Šè®­ç»ƒæ—¶è·³è¿‡æ¨¡å‹å‚æ•°æ›´æ–°ï¼Œå®ƒä»¬çš„æ¢¯åº¦ä¸éœ€è¦åœ¨å®é™…è°ƒç”¨`optimizer.step()`æ—¶åŒæ­¥ã€‚PyTorch æ— æ³•è‡ªåŠ¨åˆ¤æ–­ä½•æ—¶éœ€è¦æ‰§è¡Œæ­¤æ“ä½œï¼Œä½†ä»–ä»¬æä¾›äº†ä¸€ä¸ªå·¥å…·æ¥å¸®åŠ©ï¼Œé€šè¿‡åœ¨å°†æ¨¡å‹è½¬æ¢ä¸º DDP åæ·»åŠ åˆ°æ‚¨çš„æ¨¡å‹çš„[`no_sync`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync)ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚

åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸‹ï¼Œå½“è°ƒç”¨`.backward()`æ—¶ï¼ŒPyTorch å°†è·³è¿‡åŒæ­¥æ¢¯åº¦ï¼Œå¹¶ä¸”åœ¨æ­¤ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¹‹å¤–ç¬¬ä¸€æ¬¡è°ƒç”¨`.backward()`æ—¶å°†è§¦å‘åŒæ­¥ã€‚è¯·å‚è§ä¸‹é¢çš„ç¤ºä¾‹ï¼š

```py
ddp_model, dataloader, optimizer = accelerator.prepare(model, dataloader, optimizer)

for index, batch in enumerate(dataloader):
    inputs, targets = batch
    # Trigger gradient synchronization on the last batch
    if index != (len(dataloader) - 1):
        with ddp_model.no_sync():
            # Gradients only accumulate
            outputs = ddp_model(inputs)
            loss = loss_func(outputs)
            accelerator.backward(loss)
    else:
        # Gradients finally sync
        outputs = ddp_model(inputs)
        loss = loss_func(outputs)
        accelerator.backward(loss)
        optimizer.step()
```

åœ¨ğŸ¤— Accelerate ä¸­ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªå¯ä»¥è°ƒç”¨çš„ APIï¼Œæ— è®ºè®­ç»ƒè®¾å¤‡å¦‚ä½•ï¼ˆå°½ç®¡å¦‚æœæ‚¨ä¸åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­å¯èƒ½ä¸ä¼šæ‰§è¡Œä»»ä½•æ“ä½œï¼ï¼‰ï¼Œ`ddp_model.no_sync`è¢«æ›¿æ¢ä¸º no_sync()ï¼Œå¹¶ä¸”æ“ä½œæ–¹å¼ç›¸åŒï¼š

```py
  ddp_model, dataloader, optimizer = accelerator.prepare(model, dataloader, optimizer)

  for index, batch in enumerate(dataloader):
      inputs, targets = batch
      # Trigger gradient synchronization on the last batch
      if index != (len(dataloader)-1):
-         with ddp_model.no_sync():
+         with accelerator.no_sync(model):
              # Gradients only accumulate
              outputs = ddp_model(inputs)
              loss = loss_func(outputs, targets)
              accelerator.backward(loss)
      else:
          # Gradients finally sync
          outputs = ddp_model(inputs)
          loss = loss_func(outputs)
          accelerator.backward(loss)
          optimizer.step()
          optimizer.zero_grad()
```

æ­£å¦‚æ‚¨æ‰€æœŸæœ›çš„é‚£æ ·ï¼Œaccumulate()å‡½æ•°é€šè¿‡è·Ÿè¸ªå½“å‰æ‰¹æ¬¡å·ç æ¥åŒ…è£…è¿™ä¸ªæ¡ä»¶æ£€æŸ¥ï¼Œä½¿æ‚¨å¾—åˆ°æœ€ç»ˆçš„æ¢¯åº¦ç´¯ç§¯ APIï¼š

```py
ddp_model, dataloader, optimizer = accelerator.prepare(model, dataloader, optimizer)

for batch in dataloader:
    with accelerator.accumulate(model):
        optimizer.zero_grad()
        inputs, targets = batch
        outputs = model(inputs)
        loss = loss_function(outputs, targets)
        accelerator.backward(loss)
        optimizer.step()
        optimizer.zero_grad()
```

å› æ­¤ï¼Œåœ¨é€‰æ‹© API æ—¶ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨*`accelerator.accumulate`æˆ–`accelerator.no_sync`*ã€‚

## æœ‰å¤šå°‘å‡é€Ÿï¼Œä»¥åŠæ‚¨å¯èƒ½çŠ¯çš„ç®€å•é”™è¯¯

å»ºç«‹ä¸€ä¸ªç°å®çš„ä¾‹å­ï¼Œè€ƒè™‘ä»¥ä¸‹è®¾ç½®ï¼š

+   ä¸¤ä¸ªå• GPU T4 èŠ‚ç‚¹å’Œä¸€ä¸ªæœ‰ä¸¤ä¸ª GPU çš„èŠ‚ç‚¹

+   æ¯ä¸ª GPU éƒ½æ˜¯ T4ï¼Œå¹¶ä¸”æ‰˜ç®¡åœ¨ GCP ä¸Š

+   ä½¿ç”¨çš„è„šæœ¬æ˜¯[NLP ç¤ºä¾‹](https://github.com/muellerzr/timing_experiments/blob/main/baseline.py)è„šæœ¬çš„ä¿®æ”¹

+   æ¯ä¸ª GPU çš„æ‰¹é‡å¤§å°ä¸º 16ï¼Œå¹¶ä¸”æ¯ 4 æ­¥ç´¯ç§¯æ¢¯åº¦ã€‚

æ‰€æœ‰è„šæœ¬éƒ½åœ¨[æ­¤å­˜å‚¨åº“](https://github.com/muellerzr/timing_experiments)ä¸­å¯ç”¨ã€‚

å¦‚æœä¸æ³¨æ„æ¢¯åº¦åŒæ­¥å’Œ GPU é€šä¿¡ï¼Œé‚£ä¹ˆè¿™äº› GPU åœ¨ä¸å¿…è¦çš„æ—¶æœŸè¿›è¡Œé€šä¿¡æ—¶ä¼šæµªè´¹å¤§é‡æ—¶é—´ã€‚

å¤šå°‘ï¼Ÿ

å‚è€ƒï¼š

+   åŸºçº¿ï¼šä¸ä½¿ç”¨æ­¤å¤„è®¨è®ºçš„ä»»ä½•åŒæ­¥å®è·µ

+   `no_sync`ä¸æ­£ç¡®ï¼š`no_sync`ä»…åœ¨`backward`è°ƒç”¨å‘¨å›´ï¼Œè€Œä¸æ˜¯`forward`

+   `no_sync`: æ­£ç¡®ä½¿ç”¨`no_sync`æ¨¡å¼

+   `accumulate`: ä½¿ç”¨ accumulate()æ­£ç¡®åœ°

ä»¥ä¸‹æ˜¯åœ¨å•èŠ‚ç‚¹å’ŒåŒèŠ‚ç‚¹è®¾ç½®ä¸Šè¿­ä»£ 29 æ‰¹æ•°æ®çš„å¹³å‡ç§’æ•°ï¼Œå¯¹æ¯”æ¯ç§è®¾ç½®ï¼š

|  | åŸºçº¿ | `no_sync`ä¸æ­£ç¡® | `no_sync` | `accumulate` |
| :-: | :-: | :-: | :-: | :-: |
| å¤šèŠ‚ç‚¹ | 2Â±0.01 ç§’ | 2.13Â±0.08 ç§’ | **0.91Â±0.11 ç§’** | **0.91Â±0.11 ç§’** |
| å•èŠ‚ç‚¹ | 0.50Â±0.01 ç§’ | 0.50Â±0.01 ç§’ | **0.41Â±0.015 ç§’** | **0.41Â±0.015 ç§’** |

æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œå¦‚æœä¸æ³¨æ„å¦‚ä½•è®¾ç½®æ¢¯åº¦åŒæ­¥ï¼Œè®­ç»ƒæœŸé—´å¯èƒ½ä¼šå‡ºç°è¶…è¿‡ 2 å€çš„å‡é€Ÿï¼

å¦‚æœæ‚¨æ‹…å¿ƒç¡®ä¿ä¸€åˆ‡éƒ½åšå¾—æ­£ç¡®ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®åˆ©ç”¨ accumulate()å‡½æ•°ï¼Œå¹¶å‘ Accelerator å¯¹è±¡ä¼ é€’`gradient_accumulation_steps`æˆ–`gradient_accumulation_plugin`ï¼Œä»¥ä¾¿ Accelerate å¯ä»¥ä¸ºæ‚¨å¤„ç†è¿™äº›ã€‚
