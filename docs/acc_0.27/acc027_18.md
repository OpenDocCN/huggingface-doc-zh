# 使用🤗 Accelerate 执行梯度累积

> 原始文本：[`huggingface.co/docs/accelerate/usage_guides/gradient_accumulation`](https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation)

梯度累积是一种技术，您可以在比您的机器通常能够容纳的更大批次大小上进行训练。这是通过在几个批次上累积梯度，并且只有在执行了一定数量的批次之后才会调整优化器。

尽管在技术上，标准的梯度累积代码在分布式设置中可以正常工作，但这并不是最有效的方法，您可能会遇到相当大的减速！

在本教程中，您将看到如何快速设置梯度累积并使用🤗 Accelerate 提供的实用程序执行梯度累积，这可能只需要添加一行新代码！

本示例将使用一个非常简单的 PyTorch 训练循环，每两个批次执行一次梯度累积：

```py
device = "cuda"
model.to(device)

gradient_accumulation_steps = 2

for index, batch in enumerate(training_dataloader):
    inputs, targets = batch
    inputs = inputs.to(device)
    targets = targets.to(device)
    outputs = model(inputs)
    loss = loss_function(outputs, targets)
    loss = loss / gradient_accumulation_steps
    loss.backward()
    if (index + 1) % gradient_accumulation_steps == 0:
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
```

## 将其转换为🤗 Accelerate

首先，先前显示的代码将被转换为利用🤗 Accelerate 而不使用特殊的梯度累积助手：

```py
+ from accelerate import Accelerator
+ accelerator = Accelerator()

+ model, optimizer, training_dataloader, scheduler = accelerator.prepare(
+     model, optimizer, training_dataloader, scheduler
+ )

  for index, batch in enumerate(training_dataloader):
      inputs, targets = batch
-     inputs = inputs.to(device)
-     targets = targets.to(device)
      outputs = model(inputs)
      loss = loss_function(outputs, targets)
      loss = loss / gradient_accumulation_steps
+     accelerator.backward(loss)
      if (index+1) % gradient_accumulation_steps == 0:
          optimizer.step()
          scheduler.step()
          optimizer.zero_grad()
```

在当前状态下，由于梯度同步的过程，这段代码不会有效地执行梯度累积。在 Concepts tutorial 中了解更多信息！

## 让🤗 Accelerate 处理梯度累积

现在剩下的就是让🤗 Accelerate 为我们处理梯度累积。为此，您应该在 Accelerator 中传入一个`gradient_accumulation_steps`参数，指定在每次调用`step()`之前执行的步数以及如何在调用 backward()期间自动调整损失：

```py
  from accelerate import Accelerator
- accelerator = Accelerator()
+ accelerator = Accelerator(gradient_accumulation_steps=2)
```

或者，您可以在 Accelerator 对象的`__init__`中传入一个`gradient_accumulation_plugin`参数，这将允许您进一步自定义梯度累积行为。在 GradientAccumulationPlugin 文档中了解更多信息。

从这里开始，您可以在训练循环中使用 accumulate()上下文管理器，自动为您执行梯度累积！您只需将其包装在我们代码的整个训练部分周围：

```py
- for index, batch in enumerate(training_dataloader):
+ for batch in training_dataloader:
+     with accelerator.accumulate(model):
          inputs, targets = batch
          outputs = model(inputs)
```

您可以删除所有关于步数和损失调整的特殊检查：

```py
- loss = loss / gradient_accumulation_steps
  accelerator.backward(loss)
- if (index+1) % gradient_accumulation_steps == 0:
  optimizer.step()
  scheduler.step()
  optimizer.zero_grad()
```

如您所见，Accelerator 能够跟踪您所在的批次号，并且它将自动知道是否要通过准备好的优化器进行步进以及如何调整损失。

通常，使用梯度累积，您需要调整步数以反映您正在训练的总批次的变化。🤗 Accelerate 默认会自动为您执行此操作。在幕后，我们实例化了一个配置为执行此操作的`GradientAccumulationPlugin`。

state.GradientState 与正在迭代的活动数据加载器进行同步。因此，它天真地假设当我们到达数据加载器的末尾时，一切都会同步并执行一步。要禁用此功能，请在`GradientAccumulationPlugin`中将`sync_with_dataloader`设置为`False`：

```py
from accelerate import Accelerator
from accelerate.utils import GradientAccumulationPlugin

plugin = GradientAccumulationPlugin(sync_with_dataloader=False)
accelerator = Accelerator(..., gradient_accumulation_plugin=plugin)
```

## 完成的代码

以下是使用🤗 Accelerate 执行梯度累积的完成实现

```py
from accelerate import Accelerator
accelerator = Accelerator(gradient_accumulation_steps=2)
model, optimizer, training_dataloader, scheduler = accelerator.prepare(
    model, optimizer, training_dataloader, scheduler
)
for batch in training_dataloader:
    with accelerator.accumulate(model):
        inputs, targets = batch
        outputs = model(inputs)
        loss = loss_function(outputs, targets)
        accelerator.backward(loss)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
```

**在加速器.accumulate(model)的上下文管理器中只应该执行一次前向/后向操作**。

要了解这个包装的魔力是什么，请阅读梯度同步概念指南。

## 自包含示例

这里是一个自包含示例，您可以运行它来查看🤗 Accelerate 中的梯度累积。

```py
import torch
import copy
from accelerate import Accelerator
from accelerate.utils import set_seed
from torch.utils.data import TensorDataset, DataLoader

# seed
set_seed(0)

# define toy inputs and labels
x = torch.tensor([1., 2., 3., 4., 5., 6., 7., 8.])
y = torch.tensor([2., 4., 6., 8., 10., 12., 14., 16.])
gradient_accumulation_steps = 4
batch_size = len(x) // gradient_accumulation_steps

# define dataset and dataloader
dataset = TensorDataset(x, y)
dataloader = DataLoader(dataset, batch_size=batch_size)

# define model, optimizer and loss function
model = torch.zeros((1, 1), requires_grad=True)
model_clone = copy.deepcopy(model)
criterion = torch.nn.MSELoss()
model_optimizer = torch.optim.SGD([model], lr=0.02)
accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)
model, model_optimizer, dataloader = accelerator.prepare(model, model_optimizer, dataloader)
model_clone_optimizer = torch.optim.SGD([model_clone], lr=0.02)
print(f"initial model weight is {model.mean().item():.5f}")
print(f"initial model weight is {model_clone.mean().item():.5f}")
for i, (inputs, labels) in enumerate(dataloader):
    with accelerator.accumulate(model):
        inputs = inputs.view(-1, 1)
        print(i, inputs.flatten())
        labels = labels.view(-1, 1)
        outputs = inputs @ model
        loss = criterion(outputs, labels)
        accelerator.backward(loss)
        model_optimizer.step()
        model_optimizer.zero_grad()
loss = criterion(x.view(-1, 1) @ model_clone, y.view(-1, 1))
model_clone_optimizer.zero_grad()
loss.backward()
model_clone_optimizer.step()
print(f"w/ accumulation, the final model weight is {model.mean().item():.5f}")
print(f"w/o accumulation, the final model weight is {model_clone.mean().item():.5f}")
```

```py
initial model weight is 0.00000
initial model weight is 0.00000
0 tensor([1., 2.])
1 tensor([3., 4.])
2 tensor([5., 6.])
3 tensor([7., 8.])
w/ accumulation, the final model weight is 2.04000
w/o accumulation, the final model weight is 2.04000
```
