# ä½¿ç”¨ğŸ¤— Accelerate æ‰§è¡Œæ¢¯åº¦ç´¯ç§¯

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/accelerate/usage_guides/gradient_accumulation`](https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation)

æ¢¯åº¦ç´¯ç§¯æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œæ‚¨å¯ä»¥åœ¨æ¯”æ‚¨çš„æœºå™¨é€šå¸¸èƒ½å¤Ÿå®¹çº³çš„æ›´å¤§æ‰¹æ¬¡å¤§å°ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¿™æ˜¯é€šè¿‡åœ¨å‡ ä¸ªæ‰¹æ¬¡ä¸Šç´¯ç§¯æ¢¯åº¦ï¼Œå¹¶ä¸”åªæœ‰åœ¨æ‰§è¡Œäº†ä¸€å®šæ•°é‡çš„æ‰¹æ¬¡ä¹‹åæ‰ä¼šè°ƒæ•´ä¼˜åŒ–å™¨ã€‚

å°½ç®¡åœ¨æŠ€æœ¯ä¸Šï¼Œæ ‡å‡†çš„æ¢¯åº¦ç´¯ç§¯ä»£ç åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­å¯ä»¥æ­£å¸¸å·¥ä½œï¼Œä½†è¿™å¹¶ä¸æ˜¯æœ€æœ‰æ•ˆçš„æ–¹æ³•ï¼Œæ‚¨å¯èƒ½ä¼šé‡åˆ°ç›¸å½“å¤§çš„å‡é€Ÿï¼

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°å¦‚ä½•å¿«é€Ÿè®¾ç½®æ¢¯åº¦ç´¯ç§¯å¹¶ä½¿ç”¨ğŸ¤— Accelerate æä¾›çš„å®ç”¨ç¨‹åºæ‰§è¡Œæ¢¯åº¦ç´¯ç§¯ï¼Œè¿™å¯èƒ½åªéœ€è¦æ·»åŠ ä¸€è¡Œæ–°ä»£ç ï¼

æœ¬ç¤ºä¾‹å°†ä½¿ç”¨ä¸€ä¸ªéå¸¸ç®€å•çš„ PyTorch è®­ç»ƒå¾ªç¯ï¼Œæ¯ä¸¤ä¸ªæ‰¹æ¬¡æ‰§è¡Œä¸€æ¬¡æ¢¯åº¦ç´¯ç§¯ï¼š

```py
device = "cuda"
model.to(device)

gradient_accumulation_steps = 2

for index, batch in enumerate(training_dataloader):
    inputs, targets = batch
    inputs = inputs.to(device)
    targets = targets.to(device)
    outputs = model(inputs)
    loss = loss_function(outputs, targets)
    loss = loss / gradient_accumulation_steps
    loss.backward()
    if (index + 1) % gradient_accumulation_steps == 0:
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
```

## å°†å…¶è½¬æ¢ä¸ºğŸ¤— Accelerate

é¦–å…ˆï¼Œå…ˆå‰æ˜¾ç¤ºçš„ä»£ç å°†è¢«è½¬æ¢ä¸ºåˆ©ç”¨ğŸ¤— Accelerate è€Œä¸ä½¿ç”¨ç‰¹æ®Šçš„æ¢¯åº¦ç´¯ç§¯åŠ©æ‰‹ï¼š

```py
+ from accelerate import Accelerator
+ accelerator = Accelerator()

+ model, optimizer, training_dataloader, scheduler = accelerator.prepare(
+     model, optimizer, training_dataloader, scheduler
+ )

  for index, batch in enumerate(training_dataloader):
      inputs, targets = batch
-     inputs = inputs.to(device)
-     targets = targets.to(device)
      outputs = model(inputs)
      loss = loss_function(outputs, targets)
      loss = loss / gradient_accumulation_steps
+     accelerator.backward(loss)
      if (index+1) % gradient_accumulation_steps == 0:
          optimizer.step()
          scheduler.step()
          optimizer.zero_grad()
```

åœ¨å½“å‰çŠ¶æ€ä¸‹ï¼Œç”±äºæ¢¯åº¦åŒæ­¥çš„è¿‡ç¨‹ï¼Œè¿™æ®µä»£ç ä¸ä¼šæœ‰æ•ˆåœ°æ‰§è¡Œæ¢¯åº¦ç´¯ç§¯ã€‚åœ¨ Concepts tutorial ä¸­äº†è§£æ›´å¤šä¿¡æ¯ï¼

## è®©ğŸ¤— Accelerate å¤„ç†æ¢¯åº¦ç´¯ç§¯

ç°åœ¨å‰©ä¸‹çš„å°±æ˜¯è®©ğŸ¤— Accelerate ä¸ºæˆ‘ä»¬å¤„ç†æ¢¯åº¦ç´¯ç§¯ã€‚ä¸ºæ­¤ï¼Œæ‚¨åº”è¯¥åœ¨ Accelerator ä¸­ä¼ å…¥ä¸€ä¸ª`gradient_accumulation_steps`å‚æ•°ï¼ŒæŒ‡å®šåœ¨æ¯æ¬¡è°ƒç”¨`step()`ä¹‹å‰æ‰§è¡Œçš„æ­¥æ•°ä»¥åŠå¦‚ä½•åœ¨è°ƒç”¨ backward()æœŸé—´è‡ªåŠ¨è°ƒæ•´æŸå¤±ï¼š

```py
  from accelerate import Accelerator
- accelerator = Accelerator()
+ accelerator = Accelerator(gradient_accumulation_steps=2)
```

æˆ–è€…ï¼Œæ‚¨å¯ä»¥åœ¨ Accelerator å¯¹è±¡çš„`__init__`ä¸­ä¼ å…¥ä¸€ä¸ª`gradient_accumulation_plugin`å‚æ•°ï¼Œè¿™å°†å…è®¸æ‚¨è¿›ä¸€æ­¥è‡ªå®šä¹‰æ¢¯åº¦ç´¯ç§¯è¡Œä¸ºã€‚åœ¨ GradientAccumulationPlugin æ–‡æ¡£ä¸­äº†è§£æ›´å¤šä¿¡æ¯ã€‚

ä»è¿™é‡Œå¼€å§‹ï¼Œæ‚¨å¯ä»¥åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨ accumulate()ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè‡ªåŠ¨ä¸ºæ‚¨æ‰§è¡Œæ¢¯åº¦ç´¯ç§¯ï¼æ‚¨åªéœ€å°†å…¶åŒ…è£…åœ¨æˆ‘ä»¬ä»£ç çš„æ•´ä¸ªè®­ç»ƒéƒ¨åˆ†å‘¨å›´ï¼š

```py
- for index, batch in enumerate(training_dataloader):
+ for batch in training_dataloader:
+     with accelerator.accumulate(model):
          inputs, targets = batch
          outputs = model(inputs)
```

æ‚¨å¯ä»¥åˆ é™¤æ‰€æœ‰å…³äºæ­¥æ•°å’ŒæŸå¤±è°ƒæ•´çš„ç‰¹æ®Šæ£€æŸ¥ï¼š

```py
- loss = loss / gradient_accumulation_steps
  accelerator.backward(loss)
- if (index+1) % gradient_accumulation_steps == 0:
  optimizer.step()
  scheduler.step()
  optimizer.zero_grad()
```

å¦‚æ‚¨æ‰€è§ï¼ŒAccelerator èƒ½å¤Ÿè·Ÿè¸ªæ‚¨æ‰€åœ¨çš„æ‰¹æ¬¡å·ï¼Œå¹¶ä¸”å®ƒå°†è‡ªåŠ¨çŸ¥é“æ˜¯å¦è¦é€šè¿‡å‡†å¤‡å¥½çš„ä¼˜åŒ–å™¨è¿›è¡Œæ­¥è¿›ä»¥åŠå¦‚ä½•è°ƒæ•´æŸå¤±ã€‚

é€šå¸¸ï¼Œä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œæ‚¨éœ€è¦è°ƒæ•´æ­¥æ•°ä»¥åæ˜ æ‚¨æ­£åœ¨è®­ç»ƒçš„æ€»æ‰¹æ¬¡çš„å˜åŒ–ã€‚ğŸ¤— Accelerate é»˜è®¤ä¼šè‡ªåŠ¨ä¸ºæ‚¨æ‰§è¡Œæ­¤æ“ä½œã€‚åœ¨å¹•åï¼Œæˆ‘ä»¬å®ä¾‹åŒ–äº†ä¸€ä¸ªé…ç½®ä¸ºæ‰§è¡Œæ­¤æ“ä½œçš„`GradientAccumulationPlugin`ã€‚

state.GradientState ä¸æ­£åœ¨è¿­ä»£çš„æ´»åŠ¨æ•°æ®åŠ è½½å™¨è¿›è¡ŒåŒæ­¥ã€‚å› æ­¤ï¼Œå®ƒå¤©çœŸåœ°å‡è®¾å½“æˆ‘ä»¬åˆ°è¾¾æ•°æ®åŠ è½½å™¨çš„æœ«å°¾æ—¶ï¼Œä¸€åˆ‡éƒ½ä¼šåŒæ­¥å¹¶æ‰§è¡Œä¸€æ­¥ã€‚è¦ç¦ç”¨æ­¤åŠŸèƒ½ï¼Œè¯·åœ¨`GradientAccumulationPlugin`ä¸­å°†`sync_with_dataloader`è®¾ç½®ä¸º`False`ï¼š

```py
from accelerate import Accelerator
from accelerate.utils import GradientAccumulationPlugin

plugin = GradientAccumulationPlugin(sync_with_dataloader=False)
accelerator = Accelerator(..., gradient_accumulation_plugin=plugin)
```

## å®Œæˆçš„ä»£ç 

ä»¥ä¸‹æ˜¯ä½¿ç”¨ğŸ¤— Accelerate æ‰§è¡Œæ¢¯åº¦ç´¯ç§¯çš„å®Œæˆå®ç°

```py
from accelerate import Accelerator
accelerator = Accelerator(gradient_accumulation_steps=2)
model, optimizer, training_dataloader, scheduler = accelerator.prepare(
    model, optimizer, training_dataloader, scheduler
)
for batch in training_dataloader:
    with accelerator.accumulate(model):
        inputs, targets = batch
        outputs = model(inputs)
        loss = loss_function(outputs, targets)
        accelerator.backward(loss)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
```

**åœ¨åŠ é€Ÿå™¨.accumulate(model)çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­åªåº”è¯¥æ‰§è¡Œä¸€æ¬¡å‰å‘/åå‘æ“ä½œ**ã€‚

è¦äº†è§£è¿™ä¸ªåŒ…è£…çš„é­”åŠ›æ˜¯ä»€ä¹ˆï¼Œè¯·é˜…è¯»æ¢¯åº¦åŒæ­¥æ¦‚å¿µæŒ‡å—ã€‚

## è‡ªåŒ…å«ç¤ºä¾‹

è¿™é‡Œæ˜¯ä¸€ä¸ªè‡ªåŒ…å«ç¤ºä¾‹ï¼Œæ‚¨å¯ä»¥è¿è¡Œå®ƒæ¥æŸ¥çœ‹ğŸ¤— Accelerate ä¸­çš„æ¢¯åº¦ç´¯ç§¯ã€‚

```py
import torch
import copy
from accelerate import Accelerator
from accelerate.utils import set_seed
from torch.utils.data import TensorDataset, DataLoader

# seed
set_seed(0)

# define toy inputs and labels
x = torch.tensor([1., 2., 3., 4., 5., 6., 7., 8.])
y = torch.tensor([2., 4., 6., 8., 10., 12., 14., 16.])
gradient_accumulation_steps = 4
batch_size = len(x) // gradient_accumulation_steps

# define dataset and dataloader
dataset = TensorDataset(x, y)
dataloader = DataLoader(dataset, batch_size=batch_size)

# define model, optimizer and loss function
model = torch.zeros((1, 1), requires_grad=True)
model_clone = copy.deepcopy(model)
criterion = torch.nn.MSELoss()
model_optimizer = torch.optim.SGD([model], lr=0.02)
accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)
model, model_optimizer, dataloader = accelerator.prepare(model, model_optimizer, dataloader)
model_clone_optimizer = torch.optim.SGD([model_clone], lr=0.02)
print(f"initial model weight is {model.mean().item():.5f}")
print(f"initial model weight is {model_clone.mean().item():.5f}")
for i, (inputs, labels) in enumerate(dataloader):
    with accelerator.accumulate(model):
        inputs = inputs.view(-1, 1)
        print(i, inputs.flatten())
        labels = labels.view(-1, 1)
        outputs = inputs @ model
        loss = criterion(outputs, labels)
        accelerator.backward(loss)
        model_optimizer.step()
        model_optimizer.zero_grad()
loss = criterion(x.view(-1, 1) @ model_clone, y.view(-1, 1))
model_clone_optimizer.zero_grad()
loss.backward()
model_clone_optimizer.step()
print(f"w/ accumulation, the final model weight is {model.mean().item():.5f}")
print(f"w/o accumulation, the final model weight is {model_clone.mean().item():.5f}")
```

```py
initial model weight is 0.00000
initial model weight is 0.00000
0 tensor([1., 2.])
1 tensor([3., 4.])
2 tensor([5., 6.])
3 tensor([7., 8.])
w/ accumulation, the final model weight is 2.04000
w/o accumulation, the final model weight is 2.04000
```
