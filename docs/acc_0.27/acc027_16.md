# 量化

> 原文链接：[`huggingface.co/docs/accelerate/usage_guides/quantization`](https://huggingface.co/docs/accelerate/usage_guides/quantization)

## bitsandbytes 集成

🤗 Accelerate 将 `bitsandbytes` 量化引入到您的模型中。您现在可以使用几行代码在 8 位或 4 位中加载任何 pytorch 模型。

如果您想要使用带有 `bitsandbytes` 的 🤗 Transformers 模型，您应该遵循这个[文档](https://huggingface.co/docs/transformers/main_classes/quantization)。

要了解 `bitsandbytes` 量化的工作原理，请查看关于[8 位量化](https://huggingface.co/blog/hf-bitsandbytes-integration)和[4 位量化](https://huggingface.co/blog/4bit-transformers-bitsandbytes)的博客文章。

### 预备条件

您需要安装以下要求：

+   安装 `bitsandbytes` 库

```py
pip install bitsandbytes
```

+   从源代码安装最新的 `accelerate`

```py
pip install git+https://github.com/huggingface/accelerate.git
```

+   安装 `minGPT` 和 `huggingface_hub` 以运行示例

```py
git clone https://github.com/karpathy/minGPT.git
pip install minGPT/
pip install huggingface_hub
```

### 工作原理

首先，我们需要初始化我们的模型。为了节省内存，我们可以使用上下文管理器 init_empty_weights() 初始化一个空模型。

让我们以 minGPT 库中的 GPT2 模型为例。

```py
from accelerate import init_empty_weights
from mingpt.model import GPT

model_config = GPT.get_default_config()
model_config.model_type = 'gpt2-xl'
model_config.vocab_size = 50257
model_config.block_size = 1024

with init_empty_weights():
    empty_model = GPT(model_config)
```

然后，我们需要获取您模型的权重路径。路径可以是 state_dict 文件（例如“pytorch_model.bin”）或包含分片检查点的文件夹。

```py
from huggingface_hub import snapshot_download
weights_location = snapshot_download(repo_id="marcsun13/gpt2-xl-linear-sharded")
```

最后，您需要使用 BnbQuantizationConfig 设置您的量化配置。

这里是一个 8 位量化的例子：

```py
from accelerate.utils import BnbQuantizationConfig
bnb_quantization_config = BnbQuantizationConfig(load_in_8bit=True, llm_int8_threshold = 6)
```

这里是一个 4 位量化的例子：

```py
from accelerate.utils import BnbQuantizationConfig
bnb_quantization_config = BnbQuantizationConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type="nf4")
```

要使用所选配置量化您的空模型，您需要使用 load_and_quantize_model()。

```py
from accelerate.utils import load_and_quantize_model
quantized_model = load_and_quantize_model(empty_model, weights_location=weights_location, bnb_quantization_config=bnb_quantization_config, device_map = "auto")
```

### 保存和加载 8 位模型

您可以使用 save_model() 使用 accelerate 保存您的 8 位模型。

```py
from accelerate import Accelerator
accelerate = Accelerator()
new_weights_location = "path/to/save_directory"
accelerate.save_model(quantized_model, new_weights_location)

quantized_model_from_saved = load_and_quantize_model(empty_model, weights_location=new_weights_location, bnb_quantization_config=bnb_quantization_config, device_map = "auto")
```

请注意，目前不支持 4 位模型序列化。

### 将模块卸载到 CPU 和磁盘

如果您的 GPU 上没有足够的空间来存储整个模型，您可以将一些模块卸载到 CPU/磁盘。这在底层使用大模型推断。查看这个[文档](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)以获取更多详细信息。

对于 8 位量化，所选模块将转换为 8 位精度。

对于 4 位量化，所选模块将保留在用户在 `BnbQuantizationConfig` 中传递的 `torch_dtype` 中。当 4 位序列化可行时，我们将添加支持将这些卸载的模块转换为 4 位。

您只需要传递一个自定义的 `device_map`，以便将模块卸载到 CPU/磁盘。当需要时，卸载的模块将被分派到 GPU 上。这里是一个例子：

```py
device_map = {
    "transformer.wte": 0,
    "transformer.wpe": 0,
    "transformer.drop": 0,
    "transformer.h": "cpu",
    "transformer.ln_f": "disk",
    "lm_head": "disk",
}
```

### 微调一个量化模型

在这些模型上无法执行纯 8 位或 4 位训练。但是，您可以通过利用参数高效的微调方法（PEFT）来训练这些模型，并在其上训练适配器等。请查看 [peft](https://github.com/huggingface/peft) 库以获取更多详细信息。

目前，您无法在任何量化模型之上添加适配器。然而，通过 🤗 Transformers 模型的官方适配器支持，您可以微调量化模型。如果您想要微调一个 🤗 Transformers 模型，请参考这个[文档](https://huggingface.co/docs/transformers/main_classes/quantization)。查看这个[演示](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing)，了解如何微调一个 4 位 🤗 Transformers 模型。

请注意，在加载模型进行训练时，您无需传递 `device_map`。它将自动将您的模型加载到 GPU 上。请注意，`device_map=auto`应仅用于推断。

### 示例演示 - 在 Google Colab 上运行 GPT2 1.5b

查看在 Google Colab 上运行量化模型的演示。GPT2-1.5B 模型检查点使用 FP32，占用 6GB 内存。量化后，使用 8 位模块占用 1.6GB，使用 4 位模块占用 1.2GB。
