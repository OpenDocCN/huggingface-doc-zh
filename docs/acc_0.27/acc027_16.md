# é‡åŒ–

> åŸæ–‡é“¾æ¥ï¼š[`huggingface.co/docs/accelerate/usage_guides/quantization`](https://huggingface.co/docs/accelerate/usage_guides/quantization)

## bitsandbytes é›†æˆ

ğŸ¤— Accelerate å°† `bitsandbytes` é‡åŒ–å¼•å…¥åˆ°æ‚¨çš„æ¨¡å‹ä¸­ã€‚æ‚¨ç°åœ¨å¯ä»¥ä½¿ç”¨å‡ è¡Œä»£ç åœ¨ 8 ä½æˆ– 4 ä½ä¸­åŠ è½½ä»»ä½• pytorch æ¨¡å‹ã€‚

å¦‚æœæ‚¨æƒ³è¦ä½¿ç”¨å¸¦æœ‰ `bitsandbytes` çš„ ğŸ¤— Transformers æ¨¡å‹ï¼Œæ‚¨åº”è¯¥éµå¾ªè¿™ä¸ª[æ–‡æ¡£](https://huggingface.co/docs/transformers/main_classes/quantization)ã€‚

è¦äº†è§£ `bitsandbytes` é‡åŒ–çš„å·¥ä½œåŸç†ï¼Œè¯·æŸ¥çœ‹å…³äº[8 ä½é‡åŒ–](https://huggingface.co/blog/hf-bitsandbytes-integration)å’Œ[4 ä½é‡åŒ–](https://huggingface.co/blog/4bit-transformers-bitsandbytes)çš„åšå®¢æ–‡ç« ã€‚

### é¢„å¤‡æ¡ä»¶

æ‚¨éœ€è¦å®‰è£…ä»¥ä¸‹è¦æ±‚ï¼š

+   å®‰è£… `bitsandbytes` åº“

```py
pip install bitsandbytes
```

+   ä»æºä»£ç å®‰è£…æœ€æ–°çš„ `accelerate`

```py
pip install git+https://github.com/huggingface/accelerate.git
```

+   å®‰è£… `minGPT` å’Œ `huggingface_hub` ä»¥è¿è¡Œç¤ºä¾‹

```py
git clone https://github.com/karpathy/minGPT.git
pip install minGPT/
pip install huggingface_hub
```

### å·¥ä½œåŸç†

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åˆå§‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ã€‚ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ init_empty_weights() åˆå§‹åŒ–ä¸€ä¸ªç©ºæ¨¡å‹ã€‚

è®©æˆ‘ä»¬ä»¥ minGPT åº“ä¸­çš„ GPT2 æ¨¡å‹ä¸ºä¾‹ã€‚

```py
from accelerate import init_empty_weights
from mingpt.model import GPT

model_config = GPT.get_default_config()
model_config.model_type = 'gpt2-xl'
model_config.vocab_size = 50257
model_config.block_size = 1024

with init_empty_weights():
    empty_model = GPT(model_config)
```

ç„¶åï¼Œæˆ‘ä»¬éœ€è¦è·å–æ‚¨æ¨¡å‹çš„æƒé‡è·¯å¾„ã€‚è·¯å¾„å¯ä»¥æ˜¯ state_dict æ–‡ä»¶ï¼ˆä¾‹å¦‚â€œpytorch_model.binâ€ï¼‰æˆ–åŒ…å«åˆ†ç‰‡æ£€æŸ¥ç‚¹çš„æ–‡ä»¶å¤¹ã€‚

```py
from huggingface_hub import snapshot_download
weights_location = snapshot_download(repo_id="marcsun13/gpt2-xl-linear-sharded")
```

æœ€åï¼Œæ‚¨éœ€è¦ä½¿ç”¨ BnbQuantizationConfig è®¾ç½®æ‚¨çš„é‡åŒ–é…ç½®ã€‚

è¿™é‡Œæ˜¯ä¸€ä¸ª 8 ä½é‡åŒ–çš„ä¾‹å­ï¼š

```py
from accelerate.utils import BnbQuantizationConfig
bnb_quantization_config = BnbQuantizationConfig(load_in_8bit=True, llm_int8_threshold = 6)
```

è¿™é‡Œæ˜¯ä¸€ä¸ª 4 ä½é‡åŒ–çš„ä¾‹å­ï¼š

```py
from accelerate.utils import BnbQuantizationConfig
bnb_quantization_config = BnbQuantizationConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type="nf4")
```

è¦ä½¿ç”¨æ‰€é€‰é…ç½®é‡åŒ–æ‚¨çš„ç©ºæ¨¡å‹ï¼Œæ‚¨éœ€è¦ä½¿ç”¨ load_and_quantize_model()ã€‚

```py
from accelerate.utils import load_and_quantize_model
quantized_model = load_and_quantize_model(empty_model, weights_location=weights_location, bnb_quantization_config=bnb_quantization_config, device_map = "auto")
```

### ä¿å­˜å’ŒåŠ è½½ 8 ä½æ¨¡å‹

æ‚¨å¯ä»¥ä½¿ç”¨ save_model() ä½¿ç”¨ accelerate ä¿å­˜æ‚¨çš„ 8 ä½æ¨¡å‹ã€‚

```py
from accelerate import Accelerator
accelerate = Accelerator()
new_weights_location = "path/to/save_directory"
accelerate.save_model(quantized_model, new_weights_location)

quantized_model_from_saved = load_and_quantize_model(empty_model, weights_location=new_weights_location, bnb_quantization_config=bnb_quantization_config, device_map = "auto")
```

è¯·æ³¨æ„ï¼Œç›®å‰ä¸æ”¯æŒ 4 ä½æ¨¡å‹åºåˆ—åŒ–ã€‚

### å°†æ¨¡å—å¸è½½åˆ° CPU å’Œç£ç›˜

å¦‚æœæ‚¨çš„ GPU ä¸Šæ²¡æœ‰è¶³å¤Ÿçš„ç©ºé—´æ¥å­˜å‚¨æ•´ä¸ªæ¨¡å‹ï¼Œæ‚¨å¯ä»¥å°†ä¸€äº›æ¨¡å—å¸è½½åˆ° CPU/ç£ç›˜ã€‚è¿™åœ¨åº•å±‚ä½¿ç”¨å¤§æ¨¡å‹æ¨æ–­ã€‚æŸ¥çœ‹è¿™ä¸ª[æ–‡æ¡£](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

å¯¹äº 8 ä½é‡åŒ–ï¼Œæ‰€é€‰æ¨¡å—å°†è½¬æ¢ä¸º 8 ä½ç²¾åº¦ã€‚

å¯¹äº 4 ä½é‡åŒ–ï¼Œæ‰€é€‰æ¨¡å—å°†ä¿ç•™åœ¨ç”¨æˆ·åœ¨ `BnbQuantizationConfig` ä¸­ä¼ é€’çš„ `torch_dtype` ä¸­ã€‚å½“ 4 ä½åºåˆ—åŒ–å¯è¡Œæ—¶ï¼Œæˆ‘ä»¬å°†æ·»åŠ æ”¯æŒå°†è¿™äº›å¸è½½çš„æ¨¡å—è½¬æ¢ä¸º 4 ä½ã€‚

æ‚¨åªéœ€è¦ä¼ é€’ä¸€ä¸ªè‡ªå®šä¹‰çš„ `device_map`ï¼Œä»¥ä¾¿å°†æ¨¡å—å¸è½½åˆ° CPU/ç£ç›˜ã€‚å½“éœ€è¦æ—¶ï¼Œå¸è½½çš„æ¨¡å—å°†è¢«åˆ†æ´¾åˆ° GPU ä¸Šã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªä¾‹å­ï¼š

```py
device_map = {
    "transformer.wte": 0,
    "transformer.wpe": 0,
    "transformer.drop": 0,
    "transformer.h": "cpu",
    "transformer.ln_f": "disk",
    "lm_head": "disk",
}
```

### å¾®è°ƒä¸€ä¸ªé‡åŒ–æ¨¡å‹

åœ¨è¿™äº›æ¨¡å‹ä¸Šæ— æ³•æ‰§è¡Œçº¯ 8 ä½æˆ– 4 ä½è®­ç»ƒã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥é€šè¿‡åˆ©ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼ˆPEFTï¼‰æ¥è®­ç»ƒè¿™äº›æ¨¡å‹ï¼Œå¹¶åœ¨å…¶ä¸Šè®­ç»ƒé€‚é…å™¨ç­‰ã€‚è¯·æŸ¥çœ‹ [peft](https://github.com/huggingface/peft) åº“ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

ç›®å‰ï¼Œæ‚¨æ— æ³•åœ¨ä»»ä½•é‡åŒ–æ¨¡å‹ä¹‹ä¸Šæ·»åŠ é€‚é…å™¨ã€‚ç„¶è€Œï¼Œé€šè¿‡ ğŸ¤— Transformers æ¨¡å‹çš„å®˜æ–¹é€‚é…å™¨æ”¯æŒï¼Œæ‚¨å¯ä»¥å¾®è°ƒé‡åŒ–æ¨¡å‹ã€‚å¦‚æœæ‚¨æƒ³è¦å¾®è°ƒä¸€ä¸ª ğŸ¤— Transformers æ¨¡å‹ï¼Œè¯·å‚è€ƒè¿™ä¸ª[æ–‡æ¡£](https://huggingface.co/docs/transformers/main_classes/quantization)ã€‚æŸ¥çœ‹è¿™ä¸ª[æ¼”ç¤º](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing)ï¼Œäº†è§£å¦‚ä½•å¾®è°ƒä¸€ä¸ª 4 ä½ ğŸ¤— Transformers æ¨¡å‹ã€‚

è¯·æ³¨æ„ï¼Œåœ¨åŠ è½½æ¨¡å‹è¿›è¡Œè®­ç»ƒæ—¶ï¼Œæ‚¨æ— éœ€ä¼ é€’ `device_map`ã€‚å®ƒå°†è‡ªåŠ¨å°†æ‚¨çš„æ¨¡å‹åŠ è½½åˆ° GPU ä¸Šã€‚è¯·æ³¨æ„ï¼Œ`device_map=auto`åº”ä»…ç”¨äºæ¨æ–­ã€‚

### ç¤ºä¾‹æ¼”ç¤º - åœ¨ Google Colab ä¸Šè¿è¡Œ GPT2 1.5b

æŸ¥çœ‹åœ¨ Google Colab ä¸Šè¿è¡Œé‡åŒ–æ¨¡å‹çš„æ¼”ç¤ºã€‚GPT2-1.5B æ¨¡å‹æ£€æŸ¥ç‚¹ä½¿ç”¨ FP32ï¼Œå ç”¨ 6GB å†…å­˜ã€‚é‡åŒ–åï¼Œä½¿ç”¨ 8 ä½æ¨¡å—å ç”¨ 1.6GBï¼Œä½¿ç”¨ 4 ä½æ¨¡å—å ç”¨ 1.2GBã€‚
