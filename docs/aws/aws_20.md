# å°†æ¨¡å‹å¯¼å‡ºåˆ° Inferentia

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/optimum-neuron/guides/export_model`](https://huggingface.co/docs/optimum-neuron/guides/export_model)

## æ€»ç»“

å°† PyTorch æ¨¡å‹å¯¼å‡ºä¸º Neuron æ¨¡å‹å°±åƒè¿™æ ·ç®€å•

```py
optimum-cli export neuron \
  --model bert-base-uncased \
  --sequence_length 128 \
  --batch_size 1 \
  bert_neuron/
```

æŸ¥çœ‹æ›´å¤šé€‰é¡¹çš„å¸®åŠ©ï¼š

```py
optimum-cli export neuron --help
```

## ä¸ºä»€ä¹ˆè¦ç¼–è¯‘æˆ Neuron æ¨¡å‹ï¼Ÿ

AWS æä¾›äº†ä¸¤ä»£ç”¨äºæœºå™¨å­¦ä¹ æ¨ç†çš„ Inferentia åŠ é€Ÿå™¨ï¼Œå…·æœ‰æ›´é«˜çš„ååé‡ã€æ›´ä½çš„å»¶è¿Ÿä½†æ›´ä½çš„æˆæœ¬ï¼š[inf2ï¼ˆNeuronCore-v2ï¼‰](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/inf2-arch.html) å’Œ [inf1ï¼ˆNeuronCore-v1ï¼‰](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/inf1-arch.html#aws-inf1-arch)ã€‚

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¦åœ¨ Neuron è®¾å¤‡ä¸Šéƒ¨ç½²ğŸ¤— [Transformers](https://huggingface.co/docs/transformers/index) æ¨¡å‹ï¼Œæ‚¨éœ€è¦åœ¨æ¨ç†ä¹‹å‰å°†æ¨¡å‹ç¼–è¯‘å¹¶å¯¼å‡ºåˆ°åºåˆ—åŒ–æ ¼å¼ã€‚é€šè¿‡ä½¿ç”¨ Neuron ç¼–è¯‘å™¨ï¼ˆ[neuronx-cc](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/compiler/neuronx-cc/index.html) æˆ– [neuron-cc](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/compiler/neuron-cc/neuron-cc.html) ï¼‰è¿›è¡Œæå‰ç¼–è¯‘ï¼Œæ‚¨çš„æ¨¡å‹å°†è¢«è½¬æ¢ä¸ºåºåˆ—åŒ–å’Œä¼˜åŒ–çš„[TorchScript æ¨¡å—](https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html)ã€‚

ä¸ºäº†æ›´å¥½åœ°äº†è§£ç¼–è¯‘è¿‡ç¨‹ï¼Œè¿™é‡Œæ˜¯åœ¨å¹•åæ‰§è¡Œçš„ä¸€èˆ¬æ­¥éª¤ï¼š![ç¼–è¯‘æµç¨‹](img/3da5312c8a92cea4b2c1cbc3d4bb83f0.png "ç¼–è¯‘æµç¨‹")

**NEFF**ï¼šNeuron å¯æ‰§è¡Œæ–‡ä»¶æ ¼å¼ï¼Œæ˜¯ Neuron è®¾å¤‡ä¸Šçš„äºŒè¿›åˆ¶å¯æ‰§è¡Œæ–‡ä»¶ã€‚

å°½ç®¡é¢„ç¼–è¯‘å¯ä»¥é¿å…æ¨ç†æœŸé—´çš„å¼€é”€ï¼Œä½†è·Ÿè¸ªçš„ Neuron æ¨¡å—æœ‰ä¸€äº›é™åˆ¶ï¼š

+   è·Ÿè¸ªçš„ Neuron æ¨¡å—å°†æ˜¯é™æ€çš„ï¼Œè¿™éœ€è¦åœ¨ç¼–è¯‘æœŸé—´ä½¿ç”¨å›ºå®šçš„è¾“å…¥å½¢çŠ¶å’Œæ•°æ®ç±»å‹ã€‚ç”±äºæ¨¡å‹ä¸ä¼šåŠ¨æ€é‡æ–°ç¼–è¯‘ï¼Œå¦‚æœä¸Šè¿°æ¡ä»¶ä¸­çš„ä»»ä½•ä¸€ä¸ªå‘ç”Ÿå˜åŒ–ï¼Œæ¨ç†å°†å¤±è´¥ã€‚(*ä½†è¿™äº›é™åˆ¶å¯ä»¥é€šè¿‡[åŠ¨æ€æ‰¹å¤„ç†](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/inference/api-torch-neuronx-trace.html#dynamic-batching)å’Œ[åˆ†æ¡¶](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/torch-neuron/bucketing-app-note.html#bucketing-app-note)*)ã€‚

+   Neuron æ¨¡å‹æ˜¯ç¡¬ä»¶ä¸“ç”¨çš„ï¼Œè¿™æ„å‘³ç€ï¼š

    +   ä½¿ç”¨ Neuron è·Ÿè¸ªçš„æ¨¡å‹å°†æ— æ³•åœ¨é Neuron ç¯å¢ƒä¸­æ‰§è¡Œã€‚

    +   ä¸º inf1ï¼ˆNeuronCore-v1ï¼‰ç¼–è¯‘çš„æ¨¡å‹ä¸ inf2ï¼ˆNeuronCore-v2ï¼‰ä¸å…¼å®¹ï¼Œåä¹‹äº¦ç„¶ã€‚

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•å°†æ‚¨çš„æ¨¡å‹å¯¼å‡ºä¸ºé’ˆå¯¹ Neuron è®¾å¤‡è¿›è¡Œä¼˜åŒ–çš„åºåˆ—åŒ–æ¨¡å‹ã€‚

ğŸ¤— Optimum é€šè¿‡åˆ©ç”¨é…ç½®å¯¹è±¡æä¾›äº†å¯¹ Neuron å¯¼å‡ºçš„æ”¯æŒã€‚è¿™äº›é…ç½®å¯¹è±¡å·²ç»ä¸ºè®¸å¤šæ¨¡å‹æ¶æ„å‡†å¤‡å¥½ï¼Œå¹¶ä¸”è®¾è®¡ä¸ºæ˜“äºæ‰©å±•åˆ°å…¶ä»–æ¶æ„ã€‚

**è¦æ£€æŸ¥æ”¯æŒçš„æ¶æ„ï¼Œè¯·è½¬åˆ°é…ç½®å‚è€ƒé¡µé¢ã€‚**

## ä½¿ç”¨ CLI å°†æ¨¡å‹å¯¼å‡ºåˆ° Neuron

å°†ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºåˆ° Neuronï¼Œæ‚¨é¦–å…ˆéœ€è¦å®‰è£…ä¸€äº›é¢å¤–çš„ä¾èµ–é¡¹ï¼š

**å¯¹äº Inf2**

```py
pip install optimum[neuronx]
```

**å¯¹äº Inf1**

```py
pip install optimum[neuron]
```

æœ€ä½³çš„ Neuron å¯¼å‡ºå¯ä»¥é€šè¿‡ Optimum å‘½ä»¤è¡Œä½¿ç”¨ï¼š

```py
optimum-cli export neuron --help

usage: optimum-cli export neuron [-h] -m MODEL [--task TASK] [--atol ATOL] [--cache_dir CACHE_DIR] [--trust-remote-code]
                                 [--compiler_workdir COMPILER_WORKDIR] [--disable-validation] [--auto_cast {none,matmul,all}]
                                 [--auto_cast_type {bf16,fp16,tf32}] [--dynamic-batch-size] [--num_cores NUM_CORES] [--unet UNET]
                                 [--output_hidden_states] [--output_attentions] [--batch_size BATCH_SIZE]
                                 [--sequence_length SEQUENCE_LENGTH] [--num_beams NUM_BEAMS] [--num_choices NUM_CHOICES]
                                 [--num_channels NUM_CHANNELS] [--width WIDTH] [--height HEIGHT]
                                 [--num_images_per_prompt NUM_IMAGES_PER_PROMPT] [-O1 | -O2 | -O3]
                                 output

optional arguments:
  -h, --help            show this help message and exit
  -O1                   Enables the core performance optimizations in the compiler, while also minimizing compile time.
  -O2                   [Default] Provides the best balance between model performance and compile time.
  -O3                   May provide additional model execution performance but may incur longer compile times and higher host
                        memory usage during model compilation.

Required arguments:
  -m MODEL, --model MODEL
                        Model ID on huggingface.co or path on disk to load model from.
  output                Path indicating the directory where to store generated Neuronx compiled TorchScript model.

Optional arguments:
  --task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on the model.
                        Available tasks depend on the model, but are among: ['audio-classification', 'audio-frame-
                        classification', 'audio-xvector', 'automatic-speech-recognition', 'conversational', 'depth-estimation',
                        'feature-extraction', 'fill-mask', 'image-classification', 'image-segmentation', 'image-to-image',
                        'image-to-text', 'mask-generation', 'masked-im', 'multiple-choice', 'object-detection', 'question-
                        answering', 'semantic-segmentation', 'text-to-audio', 'text-generation', 'text2text-generation', 'text-
                        classification', 'token-classification', 'zero-shot-image-classification', 'zero-shot-object-detection',
                        'stable-diffusion', 'stable-diffusion-xl'].
  --atol ATOL           If specified, the absolute difference tolerance when validating the model. Otherwise, the default atol
                        for the model will be used.
  --cache_dir CACHE_DIR
                        Path indicating where to store cache.
  --trust-remote-code   Allow to use custom code for the modeling hosted in the model repository. This option should only be set
                        for repositories you trust and in which you have read the code, as it will execute on your local machine
                        arbitrary code present in the model repository.
  --compiler_workdir COMPILER_WORKDIR
                        Path indicating the directory where to store intermediary files generated by Neuronx compiler.
  --disable-validation  Whether to disable the validation of inference on neuron device compared to the outputs of original
                        PyTorch model on CPU.
  --auto_cast {none,matmul,all}
                        Whether to cast operations from FP32 to lower precision to speed up the inference. Can be `"none"`,
                        `"matmul"` or `"all"`.
  --auto_cast_type {bf16,fp16,tf32}
                        The data type to cast FP32 operations to when auto-cast mode is enabled. Can be `"bf16"`, `"fp16"` or
                        `"tf32"`.
  --dynamic-batch-size  Enable dynamic batch size for neuron compiled model. If this option is enabled, the input batch size can
                        be a multiple of the batch size during the compilation, but it comes with a potential tradeoff in terms
                        of latency.
  --num_cores NUM_CORES
                        The number of cores on which the model should be deployed (text-generation only).
  --unet UNET           UNet model ID on huggingface.co or path on disk to load model from. This will replace the unet in the
                        original Stable Diffusion pipeline.
  --output_hidden_states
                        Whether or not for the traced model to return the hidden states of all layers.
  --output_attentions   Whether or not for the traced model to return the attentions tensors of all attention layers.

Input shapes:
  --batch_size BATCH_SIZE
                        Batch size that the Neuronx-cc compiler exported model will be able to take as input.
  --sequence_length SEQUENCE_LENGTH
                        Sequence length that the Neuronx-cc compiler exported model will be able to take as input.
  --num_beams NUM_BEAMS
                        Number of beams for beam search that the Neuronx-cc compiler exported model will be able to take as
                        input.
  --num_choices NUM_CHOICES
                        Only for the multiple-choice task. Num choices that the Neuronx-cc compiler exported model will be able
                        to take as input.
  --num_channels NUM_CHANNELS
                        Image tasks only. Number of channels that the Neuronx-cc compiler exported model will be able to take as
                        input.
  --width WIDTH         Image tasks only. Width that the Neuronx-cc compiler exported model will be able to take as input.
  --height HEIGHT       Image tasks only. Height that the Neuronx-cc compiler exported model will be able to take as input.
  --num_images_per_prompt NUM_IMAGES_PER_PROMPT
                        Stable diffusion only. Number of images per prompt that the Neuronx-cc compiler exported model will be
                        able to take as input.

```

åœ¨æœ€åä¸€èŠ‚ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°ä¸€äº›è¾“å…¥å½¢çŠ¶é€‰é¡¹ï¼Œç”¨äºä¼ é€’ç»™å¯¼å‡ºé™æ€ç¥ç»å…ƒæ¨¡å‹ï¼Œè¿™æ„å‘³ç€åœ¨æ¨ç†æœŸé—´åº”ä½¿ç”¨ä¸ç¼–è¯‘æœŸé—´ç»™å®šçš„ç¡®åˆ‡å½¢çŠ¶è¾“å…¥ç›¸åŒçš„è¾“å…¥ã€‚å¦‚æœæ‚¨è¦ä½¿ç”¨å¯å˜å¤§å°çš„è¾“å…¥ï¼Œæ‚¨å¯ä»¥å°†è¾“å…¥å¡«å……åˆ°ç”¨äºç¼–è¯‘çš„å½¢çŠ¶ä½œä¸ºè§£å†³æ–¹æ³•ã€‚å¦‚æœè¦ä½¿æ‰¹å¤„ç†å¤§å°æ˜¯åŠ¨æ€çš„ï¼Œå¯ä»¥ä¼ é€’`--dynamic-batch-size`ä»¥å¯ç”¨åŠ¨æ€æ‰¹å¤„ç†ï¼Œè¿™æ„å‘³ç€æ‚¨å°†èƒ½å¤Ÿåœ¨æ¨ç†æœŸé—´ä½¿ç”¨ä¸åŒæ‰¹å¤„ç†å¤§å°çš„è¾“å…¥ï¼Œä½†è¿™å¯èƒ½ä¼šåœ¨å»¶è¿Ÿæ–¹é¢äº§ç”Ÿæ½œåœ¨çš„æŠ˜è¡·ã€‚

å¯¼å‡ºæ£€æŸ¥ç‚¹å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼å®Œæˆï¼š

```py
optimum-cli export neuron --model distilbert-base-uncased-distilled-squad --batch_size 1 --sequence_length 16 distilbert_base_uncased_squad_neuron/
```

æ‚¨åº”è¯¥çœ‹åˆ°ä»¥ä¸‹æ—¥å¿—ï¼Œé€šè¿‡ä¸ CPU ä¸Šçš„ PyTorch æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼ŒéªŒè¯åœ¨ Neuron è®¾å¤‡ä¸Šçš„æ¨¡å‹ï¼š

```py
Validating Neuron model...
        -[âœ“] Neuron model output names match reference model (last_hidden_state)
        - Validating Neuron Model output "last_hidden_state":
                -[âœ“] (1, 16, 32) matches (1, 16, 32)
                -[âœ“] all values close (atol: 0.0001)
The Neuronx export succeeded and the exported model was saved at: distilbert_base_uncased_squad_neuron/
```

è¿™å°†å¯¼å‡ºç”±`--model`å‚æ•°å®šä¹‰çš„æ£€æŸ¥ç‚¹çš„ç¥ç»å…ƒç¼–è¯‘çš„ TorchScript æ¨¡å—ã€‚

å¦‚æ‚¨æ‰€è§ï¼Œä»»åŠ¡å·²è¢«è‡ªåŠ¨æ£€æµ‹åˆ°ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹åœ¨ Hub ä¸Šã€‚å¯¹äºæœ¬åœ°æ¨¡å‹ï¼Œéœ€è¦æä¾›`--task`å‚æ•°ï¼Œå¦åˆ™å°†é»˜è®¤ä¸ºæ²¡æœ‰ä»»ä½•ç‰¹å®šä»»åŠ¡å¤´çš„æ¨¡å‹æ¶æ„ï¼š

```py
optimum-cli export neuron --model local_path --task question-answering --batch_size 1 --sequence_length 16 --dynamic-batch-size distilbert_base_uncased_squad_neuron/
```

è¯·æ³¨æ„ï¼Œå¯¹äº Hub ä¸Šçš„æ¨¡å‹ï¼Œæä¾›`--task`å‚æ•°å°†ç¦ç”¨è‡ªåŠ¨ä»»åŠ¡æ£€æµ‹ã€‚ç„¶åï¼Œç”Ÿæˆçš„`model.neuron`æ–‡ä»¶å¯ä»¥åŠ è½½å¹¶åœ¨ Neuron è®¾å¤‡ä¸Šè¿è¡Œã€‚

## é€šè¿‡ NeuronModel å°†æ¨¡å‹å¯¼å‡ºåˆ° Neuron

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨`optimum.neuron.NeuronModelForXXX`æ¨¡å‹ç±»å°†æ¨¡å‹å¯¼å‡ºåˆ° Neuron æ ¼å¼ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªä¾‹å­ï¼š

```py
>>> from optimum.neuron import NeuronModelForSequenceClassification

>>> input_shapes = {"batch_size": 1, "sequence_length": 64}  # mandatory shapes
>>> model = NeuronModelForSequenceClassification.from_pretrained(
...   "distilbert-base-uncased-finetuned-sst-2-english", export=True, **input_shapes
... )

# Save the model
>>> model.save_pretrained("./distilbert-base-uncased-finetuned-sst-2-english_neuron/")
```

å¯¼å‡ºçš„æ¨¡å‹å¯ä»¥ç›´æ¥ä½¿ç”¨`NeuronModelForXXX`ç±»è¿›è¡Œæ¨æ–­ï¼š

```py
>>> from transformers import AutoTokenizer
>>> from optimum.neuron import NeuronModelForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("./distilbert-base-uncased-finetuned-sst-2-english_neuron/")
>>> model = NeuronModelForSequenceClassification.from_pretrained("./distilbert-base-uncased-finetuned-sst-2-english_neuron/")

>>> inputs = tokenizer("Hamilton is considered to be the best musical of human history.", return_tensors="pt")
>>> logits = model(**inputs).logits
>>> print(model.config.id2label[logits.argmax().item()])
'POSITIVE'
```

## å°†ç¨³å®šæ‰©æ•£å¯¼å‡ºåˆ° Neuron

ä½¿ç”¨ Optimum CLIï¼Œæ‚¨å¯ä»¥ç¼–è¯‘ç¨³å®šæ‰©æ•£ç®¡é“ä¸­çš„ç»„ä»¶ï¼Œä»¥åœ¨æ¨æ–­æœŸé—´åœ¨ç¥ç»å…ƒè®¾å¤‡ä¸Šè·å¾—åŠ é€Ÿã€‚

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ”¯æŒåœ¨ç®¡é“ä¸­å¯¼å‡ºä»¥ä¸‹ç»„ä»¶ï¼š

+   CLIP æ–‡æœ¬ç¼–ç å™¨

+   U-Net

+   VAE ç¼–ç å™¨

+   VAE è§£ç å™¨

â€œé€‰æ‹©è¿™äº›å—æ˜¯å› ä¸ºå®ƒä»¬ä»£è¡¨ç®¡é“ä¸­çš„å¤§éƒ¨åˆ†è®¡ç®—é‡ï¼Œå¹¶ä¸”æ€§èƒ½åŸºå‡†æµ‹è¯•è¡¨æ˜åœ¨ Neuron ä¸Šè¿è¡Œå®ƒä»¬ä¼šå¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚â€

æ­¤å¤–ï¼Œè¯·éšæ—¶è°ƒæ•´ç¼–è¯‘é…ç½®ï¼Œä»¥åœ¨æ‚¨çš„ç”¨ä¾‹ä¸­æ‰¾åˆ°æ€§èƒ½ä¸å‡†ç¡®æ€§ä¹‹é—´çš„æœ€ä½³æƒè¡¡ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å»ºè®®å°† FP32 çŸ©é˜µä¹˜æ³•è¿ç®—è½¬æ¢ä¸º BF16ï¼Œè¿™åœ¨é€‚åº¦ç‰ºç‰²å‡†ç¡®æ€§çš„æƒ…å†µä¸‹æä¾›è‰¯å¥½çš„æ€§èƒ½ã€‚æŸ¥çœ‹[AWS Neuron æ–‡æ¡£](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#neuronx-cc-training-mixed-precision)ä¸­çš„æŒ‡å—ï¼Œä»¥æ›´å¥½åœ°äº†è§£ç¼–è¯‘é€‰é¡¹ã€‚

å¯ä»¥ä½¿ç”¨ CLI å¯¼å‡ºç¨³å®šæ‰©æ•£æ£€æŸ¥ç‚¹ï¼š

```py
optimum-cli export neuron --model stabilityai/stable-diffusion-2-1-base \
  --task stable-diffusion \
  --batch_size 1 \
  --height 512 `# height in pixels of generated image, eg. 512, 768` \
  --width 512 `# width in pixels of generated image, eg. 512, 768` \
  --num_images_per_prompt 4 `# number of images to generate per prompt, defaults to 1` \
  --auto_cast matmul `# cast only matrix multiplication operations` \
  --auto_cast_type bf16 `# cast operations from FP32 to BF16` \
  sd_neuron/
```

## å°†ç¨³å®šæ‰©æ•£ XL å¯¼å‡ºåˆ° Neuron

ä¸ç¨³å®šæ‰©æ•£ç±»ä¼¼ï¼Œæ‚¨å°†èƒ½å¤Ÿä½¿ç”¨ Optimum CLI åœ¨ SDXL ç®¡é“ä¸Šç¼–è¯‘ç»„ä»¶ï¼Œä»¥ä¾¿åœ¨ç¥ç»å…ƒè®¾å¤‡ä¸Šè¿›è¡Œæ¨æ–­ã€‚

æˆ‘ä»¬æ”¯æŒå°†ä»¥ä¸‹ç»„ä»¶å¯¼å‡ºåˆ°ç®¡é“ä¸­ä»¥æé«˜é€Ÿåº¦ï¼š

+   æ–‡æœ¬ç¼–ç å™¨

+   ç¬¬äºŒä¸ªæ–‡æœ¬ç¼–ç å™¨

+   U-Netï¼ˆæ¯”ç¨³å®šæ‰©æ•£ç®¡é“ä¸­çš„ UNet å¤§ä¸‰å€ï¼‰

+   VAE ç¼–ç å™¨

+   VAE è§£ç å™¨

â€œç¨³å®šæ‰©æ•£ XL åœ¨ 768 åˆ° 1024 ä¹‹é—´çš„å›¾åƒä¸Šè¡¨ç°ç‰¹åˆ«å¥½ã€‚â€

å¯ä»¥ä½¿ç”¨ CLI å¯¼å‡º SDXL æ£€æŸ¥ç‚¹ï¼š

```py
optimum-cli export neuron --model stabilityai/stable-diffusion-xl-base-1.0 \
  --task stable-diffusion-xl \
  --batch_size 1 \
  --height 1024 `# height in pixels of generated image, eg. 768, 1024` \
  --width 1024 `# width in pixels of generated image, eg. 768, 1024` \
  --num_images_per_prompt 4 `# number of images to generate per prompt, defaults to 1` \
  --auto_cast matmul `# cast only matrix multiplication operations` \
  --auto_cast_type bf16 `# cast operations from FP32 to BF16` \
  sd_neuron/
```

## é€‰æ‹©ä¸€ä¸ªä»»åŠ¡

åœ¨ä» Hugging Face Hub ä¸Šçš„æ¨¡å‹å¯¼å‡ºæ—¶ï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹ä¸éœ€è¦æŒ‡å®š`--task`ã€‚

ä½†æ˜¯ï¼Œå¦‚æœæ‚¨éœ€è¦æ£€æŸ¥ç»™å®šæ¨¡å‹æ¶æ„çš„ Neuron å¯¼å‡ºæ”¯æŒå“ªäº›ä»»åŠ¡ï¼Œæˆ‘ä»¬å·²ç»ä¸ºæ‚¨æä¾›äº†ã€‚é¦–å…ˆï¼Œæ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co/docs/optimum/exporters/task_manager#pytorch)æ£€æŸ¥æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ã€‚

å¯¹äºæ¯ä¸ªæ¨¡å‹æ¶æ„ï¼Œæ‚¨å¯ä»¥é€šè¿‡`~exporters.tasks.TasksManager`æ‰¾åˆ°æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ã€‚ä¾‹å¦‚ï¼Œå¯¹äº DistilBERTï¼Œå¯¹äº Neuron å¯¼å‡ºï¼Œæˆ‘ä»¬æœ‰ï¼š

```py
>>> from optimum.exporters.tasks import TasksManager
>>> from optimum.exporters.neuron.model_configs import *  # Register neuron specific configs to the TasksManager

>>> distilbert_tasks = list(TasksManager.get_supported_tasks_for_model_type("distilbert", "neuron").keys())
>>> print(distilbert_tasks)
['feature-extraction', 'fill-mask', 'multiple-choice', 'question-answering', 'text-classification', 'token-classification']
```

ç„¶åï¼Œæ‚¨å¯ä»¥å°†è¿™äº›ä»»åŠ¡ä¹‹ä¸€ä¼ é€’ç»™`optimum-cli export neuron`å‘½ä»¤ä¸­çš„`--task`å‚æ•°ï¼Œå¦‚ä¸Šæ‰€è¿°ã€‚
