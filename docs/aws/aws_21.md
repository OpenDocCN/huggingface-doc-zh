# ç¥ç»å…ƒæ¨¡å‹æ¨ç†

> åŸæ–‡é“¾æ¥ï¼š[`huggingface.co/docs/optimum-neuron/guides/models`](https://huggingface.co/docs/optimum-neuron/guides/models)

*ä»¥ä¸‹æ–‡æ¡£ä¸­ä»‹ç»çš„ API é€‚ç”¨äº[inf2](https://aws.amazon.com/ec2/instance-types/inf2/)ã€[trn1](https://aws.amazon.com/ec2/instance-types/trn1/)å’Œ[inf1](https://aws.amazon.com/ec2/instance-types/inf1/)ä¸Šçš„æ¨ç†ã€‚*

`NeuronModelForXXX`ç±»æœ‰åŠ©äºä» Hugging Face Hub åŠ è½½æ¨¡å‹å¹¶å°†å…¶ç¼–è¯‘ä¸ºé’ˆå¯¹ç¥ç»å…ƒè®¾å¤‡ä¼˜åŒ–çš„åºåˆ—åŒ–æ ¼å¼ã€‚ç„¶åï¼Œæ‚¨å°†èƒ½å¤ŸåŠ è½½æ¨¡å‹å¹¶é€šè¿‡ AWS Neuron è®¾å¤‡æä¾›çš„åŠ é€Ÿè¿è¡Œæ¨ç†ã€‚

## ä» Transformers åˆ‡æ¢åˆ° Optimum

`optimum.neuron.NeuronModelForXXX`æ¨¡å‹ç±»æ˜¯ä¸ Hugging Face Transformers æ¨¡å‹å…¼å®¹çš„ APIã€‚è¿™æ„å‘³ç€ä¸ Hugging Face çš„ç”Ÿæ€ç³»ç»Ÿæ— ç¼é›†æˆã€‚æ‚¨åªéœ€åœ¨`optimum.neuron`ä¸­ç”¨ç›¸åº”çš„`NeuronModelForXXX`ç±»æ›¿æ¢æ‚¨çš„`AutoModelForXXX`ç±»ã€‚

å¦‚æœæ‚¨å·²ç»ä½¿ç”¨ Transformersï¼Œæ‚¨å°†èƒ½å¤Ÿé€šè¿‡æ›¿æ¢æ¨¡å‹ç±»æ¥é‡ç”¨æ‚¨çš„ä»£ç ï¼š

```py
from transformers import AutoTokenizer
-from transformers import AutoModelForSequenceClassification
+from optimum.neuron import NeuronModelForSequenceClassification

# PyTorch checkpoint
-model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

+model = NeuronModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english",
+                                                             export=True, **neuron_kwargs)
```

å¦‚ä¸Šæ‰€ç¤ºï¼Œå½“æ‚¨ç¬¬ä¸€æ¬¡ä½¿ç”¨`NeuronModelForXXX`æ—¶ï¼Œæ‚¨éœ€è¦è®¾ç½®`export=True`å°†æ‚¨çš„æ¨¡å‹ä» PyTorch ç¼–è¯‘ä¸ºç¥ç»å…ƒå…¼å®¹æ ¼å¼ã€‚

æ‚¨è¿˜éœ€è¦ä¼ é€’ç¥ç»å…ƒç‰¹å®šçš„å‚æ•°æ¥é…ç½®å¯¼å‡ºã€‚æ¯ä¸ªæ¨¡å‹æ¶æ„éƒ½æœ‰è‡ªå·±çš„ä¸€ç»„å‚æ•°ï¼Œå¦‚ä¸‹ä¸€æ®µè¯¦ç»†è¯´æ˜ã€‚

ä¸€æ—¦æ‚¨çš„æ¨¡å‹è¢«å¯¼å‡ºï¼Œæ‚¨å¯ä»¥å°†å…¶ä¿å­˜åœ¨æœ¬åœ°æˆ–[Hugging Face Model Hub](https://hf.co/models)ä¸­ï¼š

```py
# Save the neuron model
>>> model.save_pretrained("a_local_path_for_compiled_neuron_model")

# Push the neuron model to HF Hub
>>> model.push_to_hub(
...     "a_local_path_for_compiled_neuron_model", repository_id="my-neuron-repo", use_auth_token=True
... )
```

ä¸‹æ¬¡å½“æ‚¨æƒ³è¦è¿è¡Œæ¨ç†æ—¶ï¼Œåªéœ€åŠ è½½æ‚¨ç¼–è¯‘çš„æ¨¡å‹ï¼Œè¿™å°†èŠ‚çœæ‚¨çš„ç¼–è¯‘æ—¶é—´ï¼š

```py
>>> from optimum.neuron import NeuronModelForSequenceClassification
>>> model = NeuronModelForSequenceClassification.from_pretrained("my-neuron-repo")
```

æ­£å¦‚æ‚¨æ‰€è§ï¼Œæ— éœ€ä¼ é€’å¯¼å‡ºæœŸé—´ä½¿ç”¨çš„ç¥ç»å…ƒå‚æ•°ï¼Œå› ä¸ºå®ƒä»¬ä¿å­˜åœ¨`config.json`æ–‡ä»¶ä¸­ï¼Œå¹¶å°†ç”±`NeuronModelForXXX`ç±»è‡ªåŠ¨æ¢å¤ã€‚

ç¬¬ä¸€æ¬¡è¿è¡Œæ¨ç†æ—¶ï¼Œå½“æ‚¨ç¬¬ä¸€æ¬¡è¿è¡Œç®¡é“æ—¶ä¼šæœ‰ä¸€ä¸ªé¢„çƒ­é˜¶æ®µã€‚è¿™æ¬¡è¿è¡Œçš„å»¶è¿Ÿæ¯”å¸¸è§„è¿è¡Œé«˜ 3 å€è‡³ 4 å€ã€‚

## æ­§è§†æ€§ NLP æ¨¡å‹

å¦‚å‰æ‰€è¿°ï¼Œæ‚¨åªéœ€è¦å¯¹ Transformers ä»£ç è¿›è¡Œå°‘é‡ä¿®æ”¹ï¼Œå³å¯å¯¼å‡ºå’Œè¿è¡Œ NLP æ¨¡å‹ï¼š

```py
from transformers import AutoTokenizer
-from transformers import AutoModelForSequenceClassification
+from optimum.neuron import NeuronModelForSequenceClassification

# PyTorch checkpoint
-model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

# Compile your model during the first time
+compiler_args = {"auto_cast": "matmul", "auto_cast_type": "bf16"}
+input_shapes = {"batch_size": 1, "sequence_length": 64}
+model = NeuronModelForSequenceClassification.from_pretrained(
+    "distilbert-base-uncased-finetuned-sst-2-english", export=True, **compiler_args, **input_shapes,
+)

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
inputs = tokenizer("Hamilton is considered to be the best musical of human history.", return_tensors="pt")

logits = model(**inputs).logits
print(model.config.id2label[logits.argmax().item()])
# 'POSITIVE'
```

`compiler_args`æ˜¯ç¼–è¯‘å™¨çš„å¯é€‰å‚æ•°ï¼Œè¿™äº›å‚æ•°é€šå¸¸æ§åˆ¶ç¼–è¯‘å™¨åœ¨æ¨ç†æ€§èƒ½ï¼ˆå»¶è¿Ÿå’Œååé‡ï¼‰å’Œå‡†ç¡®æ€§ä¹‹é—´åšå‡ºæƒè¡¡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ç¥ç»å…ƒçŸ©é˜µä¹˜æ³•å¼•æ“å°† FP32 æ“ä½œè½¬æ¢ä¸º BF16ã€‚

`input_shapes`æ˜¯æ‚¨éœ€è¦å‘é€ç»™ç¥ç»å…ƒç¼–è¯‘å™¨çš„å¼ºåˆ¶é™æ€å½¢çŠ¶ä¿¡æ¯ã€‚æƒ³çŸ¥é“æ‚¨çš„æ¨¡å‹éœ€è¦å“ªäº›å¼ºåˆ¶å½¢çŠ¶ï¼Ÿä½¿ç”¨ä»¥ä¸‹ä»£ç æŸ¥çœ‹ï¼š

```py
>>> from transformers import AutoModelForSequenceClassification
>>> from optimum.exporters import TasksManager

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

# Infer the task name if you don't know
>>> task = TasksManager.infer_task_from_model(model)  # 'text-classification'

>>> neuron_config_constructor = TasksManager.get_exporter_config_constructor(
...     model=model, exporter="neuron", task='text-classification'
... )
>>> print(neuron_config_constructor.func.get_mandatory_axes_for_task(task))
# ('batch_size', 'sequence_length')
```

è¯·æ³¨æ„ï¼Œç”¨äºç¼–è¯‘çš„è¾“å…¥å½¢çŠ¶åº”è¯¥å°äºæ‚¨åœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†é¦ˆé€åˆ°æ¨¡å‹ä¸­çš„è¾“å…¥å¤§å°ã€‚

+   å¦‚æœè¾“å…¥å¤§å°å°äºç¼–è¯‘è¾“å…¥å½¢çŠ¶æ€ä¹ˆåŠï¼Ÿ

åˆ«æ‹…å¿ƒï¼Œ`NeuronModelForXXX`ç±»å°†å¡«å……æ‚¨çš„è¾“å…¥åˆ°ä¸€ä¸ªåˆé€‚çš„å½¢çŠ¶ã€‚æ­¤å¤–ï¼Œæ‚¨å¯ä»¥åœ¨`from_pretrained`æ–¹æ³•ä¸­è®¾ç½®`dynamic_batch_size=True`æ¥å¯ç”¨åŠ¨æ€æ‰¹å¤„ç†ï¼Œè¿™æ„å‘³ç€æ‚¨çš„è¾“å…¥å¯ä»¥å…·æœ‰å¯å˜çš„æ‰¹å¤„ç†å¤§å°ã€‚

ï¼ˆåªéœ€è®°ä½ï¼šåŠ¨æ€æ€§å’Œå¡«å……ä¸ä»…å¸¦æ¥äº†çµæ´»æ€§ï¼Œè¿˜å¸¦æ¥äº†æ€§èƒ½ä¸‹é™ã€‚å¤Ÿå…¬å¹³ï¼ï¼‰

## ç”Ÿæˆæ€§ NLP æ¨¡å‹

å¦‚å‰æ‰€è¿°ï¼Œæ‚¨åªéœ€è¦å¯¹ Transformers ä»£ç è¿›è¡Œå°‘é‡ä¿®æ”¹ï¼Œå³å¯å¯¼å‡ºå’Œè¿è¡Œ NLP æ¨¡å‹ï¼š

### é…ç½®ç”Ÿæˆæ¨¡å‹çš„å¯¼å‡º

å¯¹äºéç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥ä¼ é€’ä¸¤ç»„å‚æ•°ç»™`from_pretrained()`æ–¹æ³•ï¼Œä»¥é…ç½®å¦‚ä½•å°† transformers æ£€æŸ¥ç‚¹å¯¼å‡ºä¸ºç¥ç»å…ƒä¼˜åŒ–æ¨¡å‹ï¼š

+   `compiler_args = { num_cores, auto_cast_type }`æ˜¯ç¼–è¯‘å™¨çš„å¯é€‰å‚æ•°ï¼Œè¿™äº›å‚æ•°é€šå¸¸æ§åˆ¶ç¼–è¯‘å™¨åœ¨æ¨ç†å»¶è¿Ÿã€ååé‡å’Œå‡†ç¡®æ€§ä¹‹é—´åšå‡ºæƒè¡¡ã€‚

+   `input_shapes = { batch_size, sequence_length }`å¯¹åº”äºæ¨¡å‹è¾“å…¥å’Œ KV-cacheï¼ˆè¿‡å»æ ‡è®°çš„æ³¨æ„åŠ›é”®å’Œå€¼ï¼‰çš„é™æ€å½¢çŠ¶ã€‚

+   `num_cores`æ˜¯å®ä¾‹åŒ–æ¨¡å‹æ—¶ä½¿ç”¨çš„ç¥ç»å…ƒæ ¸å¿ƒæ•°é‡ã€‚æ¯ä¸ªç¥ç»å…ƒæ ¸å¿ƒæœ‰ 16Gb çš„å†…å­˜ï¼Œè¿™æ„å‘³ç€æ›´å¤§çš„æ¨¡å‹éœ€è¦åˆ†å‰²åˆ°å¤šä¸ªæ ¸å¿ƒä¸Šã€‚é»˜è®¤ä¸º 1ï¼Œ

+   `auto_cast_type`æŒ‡å®šç¼–ç æƒé‡çš„æ ¼å¼ã€‚å¯ä»¥æ˜¯`fp32`ï¼ˆ`float32`ï¼‰ï¼Œ`fp16`ï¼ˆ`float16`ï¼‰æˆ–`bf16`ï¼ˆ`bfloat16`ï¼‰ã€‚é»˜è®¤ä¸º`fp32`ã€‚

+   `batch_size`æ˜¯æ¨¡å‹å°†æ¥å—çš„è¾“å…¥åºåˆ—çš„æ•°é‡ã€‚é»˜è®¤ä¸º 1ï¼Œ

+   `sequence_length`æ˜¯è¾“å…¥åºåˆ—ä¸­æ ‡è®°çš„æœ€å¤§æ•°é‡ã€‚é»˜è®¤ä¸º`max_position_embeddings`ï¼ˆæ—§æ¨¡å‹çš„`n_positions`ï¼‰ã€‚

```py
from transformers import AutoTokenizer
-from transformers import AutoModelForCausalLM
+from optimum.neuron import NeuronModelForCausalLM

# Instantiate and convert to Neuron a PyTorch checkpoint
+compiler_args = {"num_cores": 1, "auto_cast_type": 'fp32'}
+input_shapes = {"batch_size": 1, "sequence_length": 512}
-model = AutoModelForCausalLM.from_pretrained("gpt2")
+model = NeuronModelForCausalLM.from_pretrained("gpt2", export=True, **compiler_args, **input_shapes)
```

å¦‚å‰æ‰€è¿°ï¼Œè¿™äº›å‚æ•°åªèƒ½åœ¨å¯¼å‡ºæœŸé—´é…ç½®ã€‚è¿™æ„å‘³ç€åœ¨æ¨ç†æœŸé—´ï¼š

+   è¾“å…¥çš„`batch_size`åº”ç­‰äºå¯¼å‡ºæ—¶ä½¿ç”¨çš„`batch_size`ï¼Œ

+   è¾“å…¥åºåˆ—çš„`length`åº”ä½äºå¯¼å‡ºæ—¶ä½¿ç”¨çš„`sequence_length`ï¼Œ

+   æœ€å¤§æ ‡è®°æ•°ï¼ˆè¾“å…¥+ç”Ÿæˆï¼‰ä¸èƒ½è¶…è¿‡å¯¼å‡ºæ—¶ä½¿ç”¨çš„`sequence_length`ã€‚

### æ–‡æœ¬ç”Ÿæˆæ¨ç†

ä¸åŸå§‹ transformers æ¨¡å‹ä¸€æ ·ï¼Œä½¿ç”¨`generate()`è€Œä¸æ˜¯`forward()`æ¥ç”Ÿæˆæ–‡æœ¬åºåˆ—ã€‚

```py
from transformers import AutoTokenizer
-from transformers import AutoModelForCausalLM
+from optimum.neuron import NeuronModelForCausalLM

# Instantiate and convert to Neuron a PyTorch checkpoint
-model = AutoModelForCausalLM.from_pretrained("gpt2")
+model = NeuronModelForCausalLM.from_pretrained("gpt2", export=True)

tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token_id = tokenizer.eos_token_id

tokens = tokenizer("I really wish ", return_tensors="pt")
with torch.inference_mode():
    sample_output = model.generate(
        **tokens,
        do_sample=True,
        min_length=128,
        max_length=256,
        temperature=0.7,
    )
    outputs = [tokenizer.decode(tok) for tok in sample_output]
    print(outputs)
```

ç”Ÿæˆæ˜¯é«˜åº¦å¯é…ç½®çš„ã€‚è¯·å‚è€ƒ[`huggingface.co/docs/transformers/generation_strategies`](https://huggingface.co/docs/transformers/generation_strategies)è·å–è¯¦ç»†ä¿¡æ¯ã€‚

è¯·æ³¨æ„ï¼š

+   å¯¹äºæ¯ä¸ªæ¨¡å‹æ¶æ„ï¼Œä¸ºæ‰€æœ‰å‚æ•°æä¾›äº†é»˜è®¤å€¼ï¼Œä½†ä¼ é€’ç»™`generate`æ–¹æ³•çš„å€¼å°†ä¼˜å…ˆè€ƒè™‘ï¼Œ

+   ç”Ÿæˆå‚æ•°å¯ä»¥å­˜å‚¨åœ¨`generation_config.json`æ–‡ä»¶ä¸­ã€‚å½“æ¨¡å‹ç›®å½•ä¸­å­˜åœ¨è¿™æ ·ä¸€ä¸ªæ–‡ä»¶æ—¶ï¼Œå°†è§£æå®ƒä»¥è®¾ç½®é»˜è®¤å‚æ•°ï¼ˆä¼ é€’ç»™`generate`æ–¹æ³•çš„å€¼ä»ç„¶ä¼˜å…ˆï¼‰ã€‚

ç¥æ‚¨åœ¨ Neuron ä¸­è¿›è¡Œæ„‰å¿«çš„æ¨ç†ï¼ğŸš€
