# 预处理

> 原始文本：[`huggingface.co/docs/datasets/use_dataset`](https://huggingface.co/docs/datasets/use_dataset)

除了加载数据集外，🤗 数据集的另一个主要目标是提供多样化的预处理函数，以使数据集以适合您的机器学习框架进行训练的格式。

有许多可能的数据集预处理方式，这完全取决于您的具体数据集。有时您可能需要重命名列，而其他时候您可能需要展开嵌套字段。🤗 数据集提供了执行这些大部分操作的方法。但在几乎所有预处理情况下，根据您的数据集模态性，您需要：

+   对文本数据集进行标记化。

+   重新采样音频数据集。

+   对图像数据集应用转换。

最后的预处理步骤通常是将数据集格式设置为与您的机器学习框架期望的输入格式兼容。

在本教程中，您还需要安装🤗 Transformers 库：

```py
pip install transformers
```

选择一个数据集并跟随操作！

## 标记化文本

模型无法处理原始文本，因此您需要将文本转换为数字。通过将文本分成称为*标记*的单词，分词提供了一种方法来实现这一点。最终将标记转换为数字。

查看 Hugging Face 课程第二章中的[Tokenizers](https://huggingface.co/course/chapter2/4?fw=pt)部分，了解有关标记化和不同标记化算法的更多信息。

**1**. 首先加载[rotten_tomatoes](https://huggingface.co/datasets/rotten_tomatoes)数据集和与预训练的[BERT](https://huggingface.co/bert-base-uncased)模型对应的分词器。使用与预训练模型相同的分词器很重要，因为您希望确保文本以相同的方式分割。

```py
>>> from transformers import AutoTokenizer
>>> from datasets import load_dataset

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
>>> dataset = load_dataset("rotten_tomatoes", split="train")
```

**2**. 在数据集中的`text`的第一行上调用您的分词器：

```py
>>> tokenizer(dataset[0]["text"])
{'input_ids': [101, 1103, 2067, 1110, 17348, 1106, 1129, 1103, 6880, 1432, 112, 188, 1207, 107, 14255, 1389, 107, 1105, 1115, 1119, 112, 188, 1280, 1106, 1294, 170, 24194, 1256, 3407, 1190, 170, 11791, 5253, 188, 1732, 7200, 10947, 12606, 2895, 117, 179, 7766, 118, 172, 15554, 1181, 3498, 6961, 3263, 1137, 188, 1566, 7912, 14516, 6997, 119, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

分词器返回一个包含三个项目的字典：

+   `input_ids`: 代表文本中标记的数字。

+   `token_type_ids`: 如果有多个序列，指示一个标记属于哪个序列。

+   `attention_mask`: 指示标记是否应该被屏蔽。

这些值实际上是模型的输入。

**3**. 对整个数据集进行标记化的最快方法是使用 map()函数。该函数通过将分词器应用于示例批次而不是单个示例来加快标记化速度。将`batched`参数设置为`True`：

```py
>>> def tokenization(example):
...     return tokenizer(example["text"])

>>> dataset = dataset.map(tokenization, batched=True)
```

**4**. 将数据集格式设置为与您的机器学习框架兼容：

Pytorch 隐藏 Pytorch 内容

使用 set_format()函数将数据集格式设置为与 PyTorch 兼容：

```py
>>> dataset.set_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "label"])
>>> dataset.format['type']
'torch'
```

TensorFlow 隐藏 TensorFlow 内容

使用 to_tf_dataset()函数将数据集格式设置为与 TensorFlow 兼容。您还需要从🤗 Transformers 导入一个[数据收集器](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorWithPadding)，将不同长度的序列组合成一个相同长度的批次：

```py
>>> from transformers import DataCollatorWithPadding

>>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
>>> tf_dataset = dataset.to_tf_dataset(
...     columns=["input_ids", "token_type_ids", "attention_mask"],
...     label_cols=["label"],
...     batch_size=2,
...     collate_fn=data_collator,
...     shuffle=True
... )
```

**5**. 数据集现在已准备好与您的机器学习框架进行训练！

## 重新采样音频信号

像文本数据集一样，音频输入需要被划分为离散数据点。这被称为*采样*；采样率告诉您每秒捕获了多少语音信号。重要的是确保数据集的采样率与用于预训练模型的数据的采样率匹配。如果采样率不同，预训练模型可能在您的数据集上表现不佳，因为它无法识别采样率的差异。

**1**. 首先加载[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)数据集，Audio 特征，以及对应预训练的[Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h)模型的特征提取器：

```py
>>> from transformers import AutoFeatureExtractor
>>> from datasets import load_dataset, Audio

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")
>>> dataset = load_dataset("PolyAI/minds14", "en-US", split="train")
```

**2**. 对数据集的第一行进行索引。当您调用数据集的`audio`列时，它会被自动解码和重新采样：

```py
>>> dataset[0]["audio"]
{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,
         0.        ,  0.        ], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
 'sampling_rate': 8000}
```

**3**. 阅读数据集卡片非常有用，可以为您提供关于数据集的许多信息。快速查看 MInDS-14 数据集卡片告诉您采样率为 8kHz。同样，您可以从模型卡片中获取有关模型的许多细节。Wav2Vec2 模型卡片显示其在 16kHz 语音音频上采样。这意味着您需要将 MInDS-14 数据集上采样以匹配模型的采样率。

使用 cast_column()函数，并在 Audio 特征中设置`sampling_rate`参数来上采样音频信号。当您现在调用`audio`列时，它会被解码并重新采样为 16kHz：

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))
>>> dataset[0]["audio"]
{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,
         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
 'sampling_rate': 16000}
```

**4**. 使用 map()函数将整个数据集重新采样为 16kHz。此函数通过将特征提取器应用于批量示例而不是单个示例来加快重新采样速度。将`batched`参数设置为`True`：

```py
>>> def preprocess_function(examples):
...     audio_arrays = [x["array"] for x in examples["audio"]]
...     inputs = feature_extractor(
...         audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True
...     )
...     return inputs

>>> dataset = dataset.map(preprocess_function, batched=True)
```

**5**. 现在数据集已准备好与您的机器学习框架进行训练！

## 应用数据增强

您将对图像数据集进行的最常见的预处理是*数据增强*，这个过程会在不改变数据含义的情况下引入图像的随机变化。这可能意味着改变图像的颜色属性或随机裁剪图像。您可以自由选择任何您喜欢的数据增强库，🤗 Datasets 将帮助您将数据增强应用到您的数据集上。

**1**. 首先加载[Beans](https://huggingface.co/datasets/beans)数据集，`Image`特征，以及对应预训练的[ViT](https://huggingface.co/google/vit-base-patch16-224-in21k)模型的特征提取器：

```py
>>> from transformers import AutoFeatureExtractor
>>> from datasets import load_dataset, Image

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224-in21k")
>>> dataset = load_dataset("beans", split="train")
```

**2**. 对数据集的第一行进行索引。当您调用数据集的`image`列时，底层的 PIL 对象会自动解码为图像。

```py
>>> dataset[0]["image"]
```

**3**. 现在，您可以对图像应用一些变换。随意查看 torchvision 中提供的[各种可用变换](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py)，并选择一个您想要尝试的变换。这个示例应用了一个随机旋转图像的变换：

```py
>>> from torchvision.transforms import RandomRotation

>>> rotate = RandomRotation(degrees=(0, 90))
>>> def transforms(examples):
...     examples["pixel_values"] = [rotate(image.convert("RGB")) for image in examples["image"]]
...     return examples
```

**4**. 使用 set_transform()函数来实时应用变换。当您索引到图像的`pixel_values`时，变换会被应用，您的图像会被旋转。

```py
>>> dataset.set_transform(transforms)
>>> dataset[0]["pixel_values"]
```

**5**. 现在数据集已准备好与您的机器学习框架进行训练！
