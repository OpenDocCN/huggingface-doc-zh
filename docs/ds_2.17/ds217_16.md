# 加载

> 原文：[`huggingface.co/docs/datasets/loading`](https://huggingface.co/docs/datasets/loading)

您的数据可以存储在各种位置；它们可以在本地计算机的磁盘上、在 Github 存储库中以及在 Python 字典和 Pandas DataFrames 等内存数据结构中。无论数据集存储在哪里，🤗 数据集都可以帮助您加载它。

本指南将向您展示如何从以下位置加载数据集：

+   没有数据集加载脚本的 Hub

+   本地加载脚本

+   本地文件

+   内存数据

+   离线

+   分割的特定切片

有关加载其他数据集模态的更多详细信息，请查看加载音频数据集指南、加载图像数据集指南或加载文本数据集指南。

## Hugging Face Hub

数据集是从下载和生成数据集的数据集加载脚本中加载的。但是，您也可以从 Hub 上的任何数据集存储库加载数据集而无需加载脚本！首先创建一个数据集存储库，并上传您的数据文件。现在您可以使用 load_dataset()函数来加载数据集。

例如，尝试通过提供存储库命名空间和数据集名称从这个[demo 存储库](https://huggingface.co/datasets/lhoestq/demo1)加载文件。这个数据集存储库包含 CSV 文件，下面的代码从 CSV 文件加载数据集：

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("lhoestq/demo1")
```

某些数据集可能基于 Git 标签、分支或提交有多个版本。使用`revision`参数指定要加载的数据集版本：

```py
>>> dataset = load_dataset(
...   "lhoestq/custom_squad",
...   revision="main"  # tag name, or branch name, or commit hash
... )
```

有关如何在 Hub 上创建数据集存储库以及如何上传数据文件的更多详细信息，请参考上传数据集到 Hub 教程。

默认情况下，没有加载脚本的数据集将所有数据加载到`train`分割中。使用`data_files`参数将数据文件映射到`train`、`validation`和`test`等分割：

```py
>>> data_files = {"train": "train.csv", "test": "test.csv"}
>>> dataset = load_dataset("namespace/your_dataset_name", data_files=data_files)
```

如果您没有指定要使用哪些数据文件，load_dataset()将返回所有数据文件。如果加载像 C4 这样的大型数据集，这可能需要很长时间，C4 大约有 13TB 的数据。

您还可以使用`data_files`或`data_dir`参数加载特定的文件子集。这些参数可以接受一个相对路径，该路径解析为数据集加载的基本路径。

```py
>>> from datasets import load_dataset

# load files that match the grep pattern
>>> c4_subset = load_dataset("allenai/c4", data_files="en/c4-train.0000*-of-01024.json.gz")

# load dataset from the en directory on the Hub
>>> c4_subset = load_dataset("allenai/c4", data_dir="en")
```

`split`参数还可以将数据文件映射到特定的分割：

```py
>>> data_files = {"validation": "en/c4-validation.*.json.gz"}
>>> c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation")
```

## 本地加载脚本

您可能在计算机上本地拥有一个🤗 数据集加载脚本。在这种情况下，通过将以下路径之一传递给 load_dataset()来加载数据集：

+   加载脚本文件的本地路径。

+   包含加载脚本文件的目录的本地路径（仅当脚本文件与目录同名时）。

传递`trust_remote_code=True`以允许🤗 数据集执行加载脚本：

```py
>>> dataset = load_dataset("path/to/local/loading_script/loading_script.py", split="train", trust_remote_code=True)
>>> dataset = load_dataset("path/to/local/loading_script", split="train", trust_remote_code=True)  # equivalent because the file has the same name as the directory
```

### 编辑加载脚本

您还可以编辑 Hub 中的加载脚本以添加自己的修改。将数据集存储库下载到本地，以便加载脚本中相对路径引用的任何数据文件都可以被加载：

```py
git clone https://huggingface.co/datasets/eli5
```

编辑加载脚本，然后通过将其本地路径传递给 load_dataset()来加载它：

```py
>>> from datasets import load_dataset
>>> eli5 = load_dataset("path/to/local/eli5")
```

## 本地和远程文件

数据集可以从存储在计算机上的本地文件和远程文件加载。这些数据集很可能存储为`csv`、`json`、`txt`或`parquet`文件。load_dataset()函数可以加载每种文件类型。

### CSV

🤗 数据集可以读取由一个或多个 CSV 文件组成的数据集（在这种情况下，将 CSV 文件作为列表传递）：

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("csv", data_files="my_file.csv")
```

有关更多详细信息，请查看如何从 CSV 文件加载表格数据集指南。

### JSON

JSON 文件直接使用 load_dataset()加载，如下所示：

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("json", data_files="my_file.json")
```

JSON 文件有各种格式，但我们认为最有效的格式是具有多个 JSON 对象；每行代表一个数据行。例如：

```py
{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true}
```

您可能会遇到的另一种 JSON 格式是嵌套字段，这种情况下您需要像下面这样指定`field`参数：

```py
{"version": "0.1.0",
 "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
          {"a": 4, "b": -5.5, "c": null, "d": true}]
}

>>> from datasets import load_dataset
>>> dataset = load_dataset("json", data_files="my_file.json", field="data")
```

通过 HTTP 加载远程 JSON 文件，传递 URLs：

```py
>>> base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
>>> dataset = load_dataset("json", data_files={"train": base_url + "train-v1.1.json", "validation": base_url + "dev-v1.1.json"}, field="data")
```

虽然这些是最常见的 JSON 格式，但您会看到其他格式不同的数据集。🤗数据集识别这些其他格式，并将相应地回退到 Python JSON 加载方法来处理它们。

### Parquet

Parquet 文件以列格式存储，不同于像 CSV 这样的基于行的文件。由于其更高效和更快速地返回查询结果，大型数据集可能存储在 Parquet 文件中。

加载 Parquet 文件：

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'})
```

通过 HTTP 加载远程 Parquet 文件，传递 URLs：

```py
>>> base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
>>> data_files = {"train": base_url + "wikipedia-train.parquet"}
>>> wiki = load_dataset("parquet", data_files=data_files, split="train")
```

### Arrow

Arrow 文件以内存中的列格式存储，不同于基于行的格式如 CSV 和未压缩格式如 Parquet。

加载 Arrow 文件：

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("arrow", data_files={'train': 'train.arrow', 'test': 'test.arrow'})
```

通过 HTTP 加载远程 Arrow 文件，传递 URLs：

```py
>>> base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
>>> data_files = {"train": base_url + "wikipedia-train.arrow"}
>>> wiki = load_dataset("arrow", data_files=data_files, split="train")
```

Arrow 是🤗数据集底层使用的文件格式，因此您可以直接使用 Dataset.from_file()加载本地 Arrow 文件：

```py
>>> from datasets import Dataset
>>> dataset = Dataset.from_file("data.arrow")
```

与 load_dataset()不同，Dataset.from_file()会在不准备缓存中的数据集的情况下内存映射 Arrow 文件，节省磁盘空间。在这种情况下，用于存储中间处理结果的缓存目录将是 Arrow 文件目录。

目前仅支持 Arrow 流格式。不支持 Arrow IPC 文件格式（也称为 Feather V2）。

### SQL

通过指定连接到数据库的 URI 使用 from_sql()读取数据库内容。您可以读取表名和查询：

```py
>>> from datasets import Dataset
# load entire table
>>> dataset = Dataset.from_sql("data_table_name", con="sqlite:///sqlite_file.db")
# load from query
>>> dataset = Dataset.from_sql("SELECT text FROM table WHERE length(text) > 100 LIMIT 10", con="sqlite:///sqlite_file.db")
```

有关更多详细信息，请查看如何从 SQL 数据库加载表格数据集指南。

### WebDataset

[WebDataset](https://github.com/webdataset/webdataset)格式基于 TAR 存档，适用于大型图像数据集。由于其大小，WebDatasets 通常以流模式加载（使用`streaming=True`）。

您可以这样加载 WebDataset：

```py
>>> from datasets import load_dataset
>>>
>>> path = "path/to/train/*.tar"
>>> dataset = load_dataset("webdataset", data_files={"train": path}, split="train", streaming=True)
```

通过 HTTP 加载远程 WebDatasets，传递 URLs：

```py
>>> from datasets import load_dataset
>>>
>>> base_url = "https://huggingface.co/datasets/lhoestq/small-publaynet-wds/resolve/main/publaynet-train-{i:06d}.tar"
>>> urls = [base_url.format(i=i) for i in range(4)]
>>> dataset = load_dataset("webdataset", data_files={"train": urls}, split="train", streaming=True)
```

## 多进程

当数据集由多个文件（我们称之为“分片”）组成时，可以显著加快数据集下载和准备步骤。

您可以选择使用`num_proc`并行准备数据集的进程数。在这种情况下，每个进程将获得一组分片来准备：

```py
from datasets import load_dataset

imagenet = load_dataset("imagenet-1k", num_proc=8)
ml_librispeech_spanish = load_dataset("facebook/multilingual_librispeech", "spanish", num_proc=8)
```

## 内存数据

🤗数据集还允许您直接从内存数据结构（如 Python 字典和 Pandas DataFrames）创建 Dataset。

### Python 字典

使用 from_dict()加载 Python 字典：

```py
>>> from datasets import Dataset
>>> my_dict = {"a": [1, 2, 3]}
>>> dataset = Dataset.from_dict(my_dict)
```

### Python 字典列表

使用`from_list()`加载 Python 字典列表：

```py
>>> from datasets import Dataset
>>> my_list = [{"a": 1}, {"a": 2}, {"a": 3}]
>>> dataset = Dataset.from_list(my_list)
```

### Python 生成器

使用 from_generator()从 Python 生成器创建数据集：

```py
>>> from datasets import Dataset
>>> def my_gen():
...     for i in range(1, 4):
...         yield {"a": i}
...
>>> dataset = Dataset.from_generator(my_gen)
```

这种方法支持加载大于可用内存的数据。

您还可以通过将列表传递给`gen_kwargs`来定义分片数据集：

```py
>>> def gen(shards):
...     for shard in shards:
...         with open(shard) as f:
...             for line in f:
...                 yield {"line": line}
...
>>> shards = [f"data{i}.txt" for i in range(32)]
>>> ds = IterableDataset.from_generator(gen, gen_kwargs={"shards": shards})
>>> ds = ds.shuffle(seed=42, buffer_size=10_000)  # shuffles the shards order + uses a shuffle buffer
>>> from torch.utils.data import DataLoader
>>> dataloader = DataLoader(ds.with_format("torch"), num_workers=4)  # give each worker a subset of 32/4=8 shards
```

### Pandas DataFrame

使用 from_pandas()加载 Pandas 数据框：

```py
>>> from datasets import Dataset
>>> import pandas as pd
>>> df = pd.DataFrame({"a": [1, 2, 3]})
>>> dataset = Dataset.from_pandas(df)
```

有关更多详细信息，请查看如何从 Pandas 数据框加载表格数据集指南。

## 离线

即使没有互联网连接，仍然可以加载数据集。只要之前从 Hub 存储库下载过数据集，它应该已被缓存。这意味着您可以从缓存中重新加载数据集并离线使用。

如果您知道自己将无法访问互联网，可以在完全离线模式下运行🤗数据集。这样做可以节省时间，因为🤗数据集不会等待数据集构建器下载超时，而是直接查找缓存。将环境变量`HF_DATASETS_OFFLINE`设置为`1`以启用完全离线模式。

## 切片拆分

您还可以选择仅加载拆分的特定切片。有两种切分拆分的选项：使用字符串或 ReadInstruction API。对于简单情况，字符串更紧凑且易读，而 ReadInstruction 更容易使用具有可变切片参数。

通过连接`train`和`test`拆分来连接：

```py
>>> train_test_ds = datasets.load_dataset("bookcorpus", split="train+test") Select specific rows of the `train` split:  

```

>>> train_10_20_ds = datasets.load_dataset("bookcorpus", split="train[10:20]") 或选择拆分的百分比：

```py
>>> train_10pct_ds = datasets.load_dataset("bookcorpus", split="train[:10%]") Select a combination of percentages from each split:  

```

>>> train_10_80pct_ds = datasets.load_dataset("bookcorpus", split="train[:10%]+train[-80%:]") 最后，您甚至可以创建交叉验证拆分。下面的示例创建了 10 折交叉验证拆分。每个验证数据集是 10%的块，训练数据集占剩余的补充 90%的块：

```py
>>> val_ds = datasets.load_dataset("bookcorpus", split=[f"train[{k}%:{k+10}%]" for k in range(0, 100, 10)])
>>> train_ds = datasets.load_dataset("bookcorpus", split=[f"train[:{k}%]+train[{k+10}%:]" for k in range(0, 100, 10)])    Percent slicing and rounding The default behavior is to round the boundaries to the nearest integer for datasets where the requested slice boundaries do not divide evenly by 100\. As shown below, some slices may contain more examples than others. For instance, if the following train split includes 999 records, then:  

```

# 19 条记录，从 500（包括）到 519（不包括）。

>>> train_50_52_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%]")

# 20 条记录，从 519（包括）到 539（不包括）。

>>> train_52_54_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%]")

```py

If you want equal sized splits, use `pct1_dropremainder` rounding instead. This treats the specified percentage boundaries as multiples of 1%.

```

# 18 条记录，从 450（包括）到 468（不包括）。

>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train", from_=50, to=52, unit="%", rounding="pct1_dropremainder"))

# 18 条记录，从 468（包括）到 486（不包括）。

>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train",from_=52, to=54, unit="%", rounding="pct1_dropremainder"))

# 或者等效地：

>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split="train50%:52%")

>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split="train52%:54%")

```py

`pct1_dropremainder` rounding may truncate the last examples in a dataset if the number of examples in your dataset don’t divide evenly by 100.

##   Troubleshooting

Sometimes, you may get unexpected results when you load a dataset. Two of the most common issues you may encounter are manually downloading a dataset and specifying features of a dataset.

###   Manual download

Certain datasets require you to manually download the dataset files due to licensing incompatibility or if the files are hidden behind a login page. This causes load_dataset() to throw an `AssertionError`. But 🤗 Datasets provides detailed instructions for downloading the missing files. After you’ve downloaded the files, use the `data_dir` argument to specify the path to the files you just downloaded.

For example, if you try to download a configuration from the [MATINF](https://huggingface.co/datasets/matinf) dataset:

```

>>> 数据集 = load_dataset("matinf", "summarization")

下载和准备数据集 matinf/summarization（下载：未知大小，生成：246.89 MiB，后处理：未知大小，总共：246.89 MiB）到/root/.cache/huggingface/datasets/matinf/summarization/1.0.0/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...

断言错误：配置 summarization 的数据集 matinf 需要手动数据。

请按照手动下载说明操作：要使用 MATINF，您必须手动下载。请填写此 google 表格（https://forms.gle/nkH4LVE4iNQeDzsc9）。完成表格后，您将收到下载链接和密码。请将所有文件提取到一个文件夹中，并使用以下方式加载数据集：*datasets.load_dataset('matinf', data_dir='path/to/folder/folder_name')*。

可以使用`datasets.load_dataset(matinf, data_dir='<path/to/manual/data>')`加载手动数据

```py

If you’ve already downloaded a dataset from the *Hub with a loading script* to your computer, then you need to pass an absolute path to the `data_dir` or `data_files` parameter to load that dataset. Otherwise, if you pass a relative path, load_dataset() will load the directory from the repository on the Hub instead of the local directory.

###   Specify features

When you create a dataset from local files, the Features are automatically inferred by [Apache Arrow](https://arrow.apache.org/docs/). However, the dataset’s features may not always align with your expectations, or you may want to define the features yourself. The following example shows how you can add custom labels with the ClassLabel feature.

Start by defining your own labels with the Features class:

```

>>> class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]

>>> 情感特征 = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})

```py

Next, specify the `features` parameter in load_dataset() with the features you just created:

```

>>> dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)

```py

Now when you look at your dataset features, you can see it uses the custom labels you defined:

```

>>> dataset['train'].features

{'text': Value(dtype='string', id=None),

'label': ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], names_file=None, id=None)}

```py

##   Metrics

Metrics is deprecated in 🤗 Datasets. To learn more about how to use metrics, take a look at the library 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index)! In addition to metrics, you can find more tools for evaluating models and datasets.

When the metric you want to use is not supported by 🤗 Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:

```

从数据集导入加载度量

度量=load_metric('PATH/TO/MY/METRIC/SCRIPT')

# 典型用法示例

对于数据集中的每个批次：

输入、参考=批次

预测=model(inputs)

度量.add_batch(predictions=predictions, references=references)

得分=度量计算()

```py

See the Metrics guide for more details on how to write your own metric loading script.

###   Load configurations

It is possible for a metric to have different configurations. The configurations are stored in the `config_name` parameter in MetricInfo attribute. When you load a metric, provide the configuration name as shown in the following:

```

从数据集导入加载度量

度量=load_metric('bleurt', name='bleurt-base-128')

度量=load_metric('bleurt', name='bleurt-base-512')

```py

###   Distributed setup

When working in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. 🤗 Datasets supports distributed usage with a few additional arguments when you load a metric.

For example, imagine you are training and evaluating on eight parallel processes. Here’s how you would load a metric in this distributed setting:

1.  Define the total number of processes with the `num_process` argument.

2.  Set the process `rank` as an integer between zero and `num_process - 1`.

3.  Load your metric with load_metric() with these arguments:

```

从数据集导入加载度量

度量=load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)

```py

Once you’ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, Metric.compute() gathers all the predictions and references from the nodes, and computes the final metric.

In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an `experiment_id` to distinguish the separate evaluations:

```

从数据集导入加载度量

度量=load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id, experiment_id="My_experiment_10")

```py

```

```py

```

```py

```
