# 流

> 原始文本：[`huggingface.co/docs/datasets/stream`](https://huggingface.co/docs/datasets/stream)

数据集流式传输使您可以在不下载数据集的情况下使用数据集。当您遍历数据集时，数据会随之流动。这在以下情况下特别有帮助：

+   您不想等待极大的数据集下载。

+   数据集大小超过计算机上可用磁盘空间的量。

+   您想快速探索数据集的一些样本。

![](img/3a852aa4a279bb2476adf697aa4b6865.png) ![](img/a71d3ad700730cf461b4f099cd4a8c4b.png)

例如，[oscar-corpus/OSCAR-2201](https://huggingface.co/datasets/oscar-corpus/OSCAR-2201) 数据集的英文部分为 1.2TB，但您可以立即使用流式传输。通过在 load_dataset()中设置`streaming=True`来流式传输数据集，如下所示：

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('oscar-corpus/OSCAR-2201', 'en', split='train', streaming=True)
>>> print(next(iter(dataset)))
{'id': 0, 'text': 'Founded in 2015, Golden Bees is a leading programmatic recruitment platform dedicated to employers, HR agencies and job boards. The company has developed unique HR-custom technologies and predictive algorithms to identify and attract the best candidates for a job opportunity.', ...
```

数据集流式传输还使您可以处理由本地文件组成的数据集，而无需进行任何转换。在这种情况下，当您遍历数据集时，数据将从本地文件流动。这在以下情况下特别有帮助：

+   您不想等待极大的本地数据集转换为 Arrow。

+   转换后的文件大小将超过计算机上可用磁盘空间的量。

+   您想快速探索数据集的一些样本。

例如，您可以通过流式传输本地包含数百个压缩的 JSONL 文件的数据集，如[oscar-corpus/OSCAR-2201](https://huggingface.co/datasets/oscar-corpus/OSCAR-2201)：

```py
>>> from datasets import load_dataset
>>> data_files = {'train': 'path/to/OSCAR-2201/compressed/en_meta/*.jsonl.gz'}
>>> dataset = load_dataset('json', data_files=data_files, split='train', streaming=True)
>>> print(next(iter(dataset)))
{'id': 0, 'text': 'Founded in 2015, Golden Bees is a leading programmatic recruitment platform dedicated to employers, HR agencies and job boards. The company has developed unique HR-custom technologies and predictive algorithms to identify and attract the best candidates for a job opportunity.', ...
```

以流式传输模式加载数据集会创建一个新的数据集类型实例（而不是经典的 Dataset 对象），称为 IterableDataset。这种特殊类型的数据集具有自己的一组处理方法，如下所示。

IterableDataset 对于像训练模型这样的迭代作业非常有用。您不应该将 IterableDataset 用于需要随机访问示例的作业，因为您必须使用 for 循环遍历整个数据集。获取可迭代数据集中的最后一个示例需要您遍历所有先前的示例。您可以在数据集与 IterableDataset 指南中找到更多详细信息。

## 从数据集转换

如果您有现有的 Dataset 对象，可以使用 to_iterable_dataset()函数将其转换为 IterableDataset。这实际上比在 load_dataset()中设置`streaming=True`参数更快，因为数据是从本地文件流式传输的。

```py
>>> from datasets import load_dataset

# faster 🐇
>>> dataset = load_dataset("food101")
>>> iterable_dataset = dataset.to_iterable_dataset()

# slower 🐢
>>> iterable_dataset = load_dataset("food101", streaming=True)
```

to_iterable_dataset()函数在实例化 IterableDataset 时支持分片。当处理大型数据集并且希望对数据集进行洗牌或使用 PyTorch DataLoader 进行快速并行加载时，这将非常有用。

```py
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("food101")
>>> iterable_dataset = dataset.to_iterable_dataset(num_shards=64) # shard the dataset
>>> iterable_dataset = iterable_dataset.shuffle(buffer_size=10_000)  # shuffles the shards order and use a shuffle buffer when you start iterating
dataloader = torch.utils.data.DataLoader(iterable_dataset, num_workers=4)  # assigns 64 / 4 = 16 shards from the shuffled list of shards to each worker when you start iterating
```

## 洗牌

与常规 Dataset 对象一样，您也可以使用 IterableDataset.shuffle()对 IterableDataset 进行洗牌。

`buffer_size`参数控制从中随机抽样示例的缓冲区的大小。假设您的数据集有一百万个示例，并且您将`buffer_size`设置为一万。IterableDataset.shuffle()将从缓冲区中的前一万个示例中随机选择示例。缓冲区中的选定示例将被新示例替换。默认情况下，缓冲区大小为 1,000。

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
>>> shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10_000)
```

IterableDataset.shuffle()还将对分片的顺序进行洗牌，如果数据集被分片到多个文件中。

## 重新洗牌

有时您可能希望在每个时期之后重新洗牌数据集。这将需要您为每个时期设置不同的种子。在时期之间使用`IterableDataset.set_epoch()`告诉数据集您在哪个时期。

您的种子实际上变为：`初始种子 + 当前时期`。

```py
>>> for epoch in range(epochs):
...     shuffled_dataset.set_epoch(epoch)
...     for example in shuffled_dataset:
...         ...
```

## 拆分数据集

您可以通过以下两种方式拆分数据集：

+   IterableDataset.take()返回数据集中的前`n`个示例：

```py
>>> dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
>>> dataset_head = dataset.take(2)
>>> list(dataset_head)
[{'id': 0, 'text': 'Mtendere Village was...'}, {'id': 1, 'text': 'Lily James cannot fight the music...'}]
```

+   IterableDataset.skip()省略数据集中的前`n`个示例，并返回剩余的示例：

```py
>>> train_dataset = shuffled_dataset.skip(1000)
```

`take`和`skip`阻止对`shuffle`的未来调用，因为它们锁定了分片的顺序。在拆分数据集之前，应该对数据集进行`shuffle`。

## 交错

interleave_datasets()可以将 IterableDataset 与其他数据集组合。组合数据集返回每个原始数据集的交替示例。

```py
>>> from datasets import interleave_datasets
>>> en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True, trust_remote_code=True)
>>> fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr", split='train', streaming=True, trust_remote_code=True)

>>> multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
>>> list(multilingual_dataset.take(2))
[{'text': 'Mtendere Village was inspired by the vision...'}, {'text': "Média de débat d'idées, de culture et de littérature..."}]
```

从每个原始数据集定义采样概率，以更好地控制如何对它们进行采样和组合。使用您期望的采样概率设置`probabilities`参数：

```py
>>> multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)
>>> list(multilingual_dataset_with_oversampling.take(2))
[{'text': 'Mtendere Village was inspired by the vision...'}, {'text': 'Lily James cannot fight the music...'}]
```

最终数据集的约 80%由`en_dataset`组成，20%由`fr_dataset`组成。

您还可以指定`stopping_strategy`。默认策略`first_exhausted`是一种子采样策略，即数据集构建在其中一个数据集耗尽样本时停止。您可以指定`stopping_strategy=all_exhausted`来执行一种过采样策略。在这种情况下，数据集构建将在每个数据集的每个样本至少添加一次后停止。实际上，这意味着如果一个数据集耗尽，它将返回到该数据集的开头，直到达到停止标准。请注意，如果未指定采样概率，则新数据集将具有`max_length_datasets*nb_dataset`个样本。

## 重命名、删除和转换

以下方法允许您修改数据集的列。这些方法对于重命名或删除列以及将列更改为新的特征集非常有用。

### 重命名

当您需要在数据集中重命名列时，请使用 IterableDataset.rename_column()。与原始列相关的特征实际上是移动到新列名下，而不仅仅是替换原始列。

使用 IterableDataset.rename_column()提供原始列的名称和新列名：

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('mc4', 'en', streaming=True, split='train', trust_remote_code=True)
>>> dataset = dataset.rename_column("text", "content")
```

### 删除

当您需要删除一个或多个列时，请给 IterableDataset.remove_columns()要删除的列的名称。通过提供列名称的列表来删除多个列：

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('mc4', 'en', streaming=True, split='train', trust_remote_code=True)
>>> dataset = dataset.remove_columns('timestamp')
```

### 转换

IterableDataset.cast()更改一个或多个列的特征类型。此方法将您的新`Features`作为其参数。以下示例代码显示了如何更改`ClassLabel`和`Value`的特征类型：

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('glue', 'mrpc', split='train', streaming=True)
>>> dataset.features
{'sentence1': Value(dtype='string', id=None),
'sentence2': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
'idx': Value(dtype='int32', id=None)}

>>> from datasets import ClassLabel, Value
>>> new_features = dataset.features.copy()
>>> new_features["label"] = ClassLabel(names=['negative', 'positive'])
>>> new_features["idx"] = Value('int64')
>>> dataset = dataset.cast(new_features)
>>> dataset.features
{'sentence1': Value(dtype='string', id=None),
'sentence2': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None),
'idx': Value(dtype='int64', id=None)}
```

仅当原始特征类型和新特征类型兼容时，转换才有效。例如，如果原始列仅包含 1 和 0，则可以将具有特征类型`Value('int32')`的列转换为`Value('bool')`。

使用 IterableDataset.cast_column()来更改一个列的特征类型。将列名和其新特征类型作为参数传递：

```py
>>> dataset.features
{'audio': Audio(sampling_rate=44100, mono=True, id=None)}

>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> dataset.features
{'audio': Audio(sampling_rate=16000, mono=True, id=None)}
```

## 映射

类似于常规 Dataset 的 Dataset.map()函数，🤗 Datasets 功能 IterableDataset.map()用于处理 IterableDataset。当示例被流式传输时，IterableDataset.map()会实时应用处理。

它允许您对数据集中的每个示例应用处理函数，独立或批处理。该函数甚至可以创建新的行和列。

以下示例演示了如何对 IterableDataset 进行标记化。该函数需要接受并输出一个`dict`：

```py
>>> def add_prefix(example):
...     example['text'] = 'My text: ' + example['text']
...     return example
```

接下来，使用 IterableDataset.map()将此函数应用于数据集：

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train', trust_remote_code=True)
>>> updated_dataset = dataset.map(add_prefix)
>>> list(updated_dataset.take(3))
[{'id': 0, 'text': 'My text: Mtendere Village was inspired by...'},
 {'id': 1, 'text': 'My text: Lily James cannot fight the music...'},
 {'id': 2, 'text': 'My text: "I\'d love to help kickstart...'}]
```

让我们看另一个例子，除了这次您将使用 IterableDataset.map()删除一列。当删除列时，仅在将示例提供给映射函数后才会删除。这使得映射函数可以在删除之前使用列的内容。

使用 IterableDataset.map()中的`remove_columns`参数指定要删除的列：

```py
>>> updated_dataset = dataset.map(add_prefix, remove_columns=["id"])
>>> list(updated_dataset.take(3))
[{'text': 'My text: Mtendere Village was inspired by...'},
 {'text': 'My text: Lily James cannot fight the music...'},
 {'text': 'My text: "I\'d love to help kickstart...'}]
```

### 批处理

IterableDataset.map()还支持对示例批次进行操作。通过设置`batched=True`来操作批次。默认批处理大小为 1000，但您可以使用`batch_size`参数进行调整。这为许多有趣的应用打开了大门，例如标记化、将长句子拆分为较短的块以及数据增强。

#### 标记化

```py
>>> from datasets import load_dataset
>>> from transformers import AutoTokenizer
>>> dataset = load_dataset("mc4", "en", streaming=True, split="train", trust_remote_code=True)
>>> tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
>>> def encode(examples):
...     return tokenizer(examples['text'], truncation=True, padding='max_length')
>>> dataset = dataset.map(encode, batched=True, remove_columns=["text", "timestamp", "url"])
>>> next(iter(dataset))
{'input_ids': [101, 8466, 1018, 1010, 4029, 2475, 2062, 18558, 3100, 2061, ...,1106, 3739, 102],
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 1, 1]}
```

查看批处理映射处理文档中的其他批处理示例。它们对可迭代数据集的工作方式相同。

### 过滤

您可以使用 Dataset.filter()基于谓词函数过滤数据集中的行。它返回符合指定条件的行：

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train', trust_remote_code=True)
>>> start_with_ar = dataset.filter(lambda example: example['text'].startswith('Ar'))
>>> next(iter(start_with_ar))
{'id': 4, 'text': 'Are you looking for Number the Stars (Essential Modern Classics)?...'}
```

Dataset.filter()还可以通过设置`with_indices=True`按索引进行过滤：

```py
>>> even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)
>>> list(even_dataset.take(3))
[{'id': 0, 'text': 'Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, ...'},
 {'id': 2, 'text': '"I\'d love to help kickstart continued development! And 0 EUR/month...'},
 {'id': 4, 'text': 'Are you looking for Number the Stars (Essential Modern Classics)? Normally, ...'}]
```

## 在训练循环中流式传输

IterableDataset 可以集成到训练循环中。首先，对数据集进行洗牌：

Pytorch 隐藏 Pytorch 内容

```py
>>> seed, buffer_size = 42, 10_000
>>> dataset = dataset.shuffle(seed, buffer_size=buffer_size)
```

最后，创建一个简单的训练循环并开始训练：

```py
>>> import torch
>>> from torch.utils.data import DataLoader
>>> from transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling
>>> from tqdm import tqdm
>>> dataset = dataset.with_format("torch")
>>> dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
>>> device = 'cuda' if torch.cuda.is_available() else 'cpu' 
>>> model = AutoModelForMaskedLM.from_pretrained("distilbert-base-uncased")
>>> model.train().to(device)
>>> optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
>>> for epoch in range(3):
...     dataset.set_epoch(epoch)
...     for i, batch in enumerate(tqdm(dataloader, total=5)):
...         if i == 5:
...             break
...         batch = {k: v.to(device) for k, v in batch.items()}
...         outputs = model(**batch)
...         loss = outputs[0]
...         loss.backward()
...         optimizer.step()
...         optimizer.zero_grad()
...         if i % 10 == 0:
...             print(f"loss: {loss}")
```
