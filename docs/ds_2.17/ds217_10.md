# 评估预测

> 原始文本：[`huggingface.co/docs/datasets/metrics`](https://huggingface.co/docs/datasets/metrics)

🤗 Datasets 中的 Metrics 已被弃用。要了解有关如何使用指标的更多信息，请查看库🤗 [Evaluate](https://huggingface.co/docs/evaluate/index)！除了指标，您还可以找到更多用于评估模型和数据集的工具。

🤗 Datasets 提供各种常见和面向 NLP 的[指标](https://huggingface.co/metrics)，供您衡量模型的性能。在本教程的这一部分，您将加载一个指标并使用它来评估您的模型预测。

您可以使用 list_metrics()查看可用的指标：

```py
>>> from datasets import list_metrics
>>> metrics_list = list_metrics()
>>> len(metrics_list)
28
>>> print(metrics_list)
['accuracy', 'bertscore', 'bleu', 'bleurt', 'cer', 'comet', 'coval', 'cuad', 'f1', 'gleu', 'glue', 'indic_glue', 'matthews_correlation', 'meteor', 'pearsonr', 'precision', 'recall', 'rouge', 'sacrebleu', 'sari', 'seqeval', 'spearmanr', 'squad', 'squad_v2', 'super_glue', 'wer', 'wiki_split', 'xnli']
```

## 加载指标

使用🤗 Datasets 加载指标非常容易。实际上，您会注意到它与加载数据集非常相似！使用 load_metric()从 Hub 加载指标：

```py
>>> from datasets import load_metric
>>> metric = load_metric('glue', 'mrpc')
```

这将加载与 GLUE 基准测试中的 MRPC 数据集相关联的指标。

## 选择一个配置

如果您正在使用基准数据集，则需要选择与您正在使用的配置相关联的指标。通过提供配置名称来选择指标配置：

```py
>>> metric = load_metric('glue', 'mrpc')
```

## 指标对象

在开始使用 Metric 对象之前，您应该更加了解它。与数据集一样，您可以返回有关指标的一些基本信息。例如，访问 datasets.MetricInfo 中的`inputs_description`参数，以获取有关指标预期输入格式和一些使用示例的更多信息：

```py
>>> print(metric.inputs_description)
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:
    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}
    ...
    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}
    ...
```

请注意，对于 MRPC 配置，该指标期望输入格式为零或一。要查看您的指标可以返回的属性的完整列表，请查看 MetricInfo。

## 计算指标

一旦您加载了一个指标，您就可以使用它来评估模型的预测。将模型预测和参考提供给 compute()：

```py
>>> model_predictions = model(model_inputs)
>>> final_score = metric.compute(predictions=model_predictions, references=gold_references)
```
