# è¯„ä¼°é¢„æµ‹

> åŽŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/datasets/metrics`](https://huggingface.co/docs/datasets/metrics)

ðŸ¤— Datasets ä¸­çš„ Metrics å·²è¢«å¼ƒç”¨ã€‚è¦äº†è§£æœ‰å…³å¦‚ä½•ä½¿ç”¨æŒ‡æ ‡çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹åº“ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)ï¼é™¤äº†æŒ‡æ ‡ï¼Œæ‚¨è¿˜å¯ä»¥æ‰¾åˆ°æ›´å¤šç”¨äºŽè¯„ä¼°æ¨¡åž‹å’Œæ•°æ®é›†çš„å·¥å…·ã€‚

ðŸ¤— Datasets æä¾›å„ç§å¸¸è§å’Œé¢å‘ NLP çš„[æŒ‡æ ‡](https://huggingface.co/metrics)ï¼Œä¾›æ‚¨è¡¡é‡æ¨¡åž‹çš„æ€§èƒ½ã€‚åœ¨æœ¬æ•™ç¨‹çš„è¿™ä¸€éƒ¨åˆ†ï¼Œæ‚¨å°†åŠ è½½ä¸€ä¸ªæŒ‡æ ‡å¹¶ä½¿ç”¨å®ƒæ¥è¯„ä¼°æ‚¨çš„æ¨¡åž‹é¢„æµ‹ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ list_metrics()æŸ¥çœ‹å¯ç”¨çš„æŒ‡æ ‡ï¼š

```py
>>> from datasets import list_metrics
>>> metrics_list = list_metrics()
>>> len(metrics_list)
28
>>> print(metrics_list)
['accuracy', 'bertscore', 'bleu', 'bleurt', 'cer', 'comet', 'coval', 'cuad', 'f1', 'gleu', 'glue', 'indic_glue', 'matthews_correlation', 'meteor', 'pearsonr', 'precision', 'recall', 'rouge', 'sacrebleu', 'sari', 'seqeval', 'spearmanr', 'squad', 'squad_v2', 'super_glue', 'wer', 'wiki_split', 'xnli']
```

## åŠ è½½æŒ‡æ ‡

ä½¿ç”¨ðŸ¤— Datasets åŠ è½½æŒ‡æ ‡éžå¸¸å®¹æ˜“ã€‚å®žé™…ä¸Šï¼Œæ‚¨ä¼šæ³¨æ„åˆ°å®ƒä¸ŽåŠ è½½æ•°æ®é›†éžå¸¸ç›¸ä¼¼ï¼ä½¿ç”¨ load_metric()ä»Ž Hub åŠ è½½æŒ‡æ ‡ï¼š

```py
>>> from datasets import load_metric
>>> metric = load_metric('glue', 'mrpc')
```

è¿™å°†åŠ è½½ä¸Ž GLUE åŸºå‡†æµ‹è¯•ä¸­çš„ MRPC æ•°æ®é›†ç›¸å…³è”çš„æŒ‡æ ‡ã€‚

## é€‰æ‹©ä¸€ä¸ªé…ç½®

å¦‚æžœæ‚¨æ­£åœ¨ä½¿ç”¨åŸºå‡†æ•°æ®é›†ï¼Œåˆ™éœ€è¦é€‰æ‹©ä¸Žæ‚¨æ­£åœ¨ä½¿ç”¨çš„é…ç½®ç›¸å…³è”çš„æŒ‡æ ‡ã€‚é€šè¿‡æä¾›é…ç½®åç§°æ¥é€‰æ‹©æŒ‡æ ‡é…ç½®ï¼š

```py
>>> metric = load_metric('glue', 'mrpc')
```

## æŒ‡æ ‡å¯¹è±¡

åœ¨å¼€å§‹ä½¿ç”¨ Metric å¯¹è±¡ä¹‹å‰ï¼Œæ‚¨åº”è¯¥æ›´åŠ äº†è§£å®ƒã€‚ä¸Žæ•°æ®é›†ä¸€æ ·ï¼Œæ‚¨å¯ä»¥è¿”å›žæœ‰å…³æŒ‡æ ‡çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œè®¿é—® datasets.MetricInfo ä¸­çš„`inputs_description`å‚æ•°ï¼Œä»¥èŽ·å–æœ‰å…³æŒ‡æ ‡é¢„æœŸè¾“å…¥æ ¼å¼å’Œä¸€äº›ä½¿ç”¨ç¤ºä¾‹çš„æ›´å¤šä¿¡æ¯ï¼š

```py
>>> print(metric.inputs_description)
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:
    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}
    ...
    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}
    ...
```

è¯·æ³¨æ„ï¼Œå¯¹äºŽ MRPC é…ç½®ï¼Œè¯¥æŒ‡æ ‡æœŸæœ›è¾“å…¥æ ¼å¼ä¸ºé›¶æˆ–ä¸€ã€‚è¦æŸ¥çœ‹æ‚¨çš„æŒ‡æ ‡å¯ä»¥è¿”å›žçš„å±žæ€§çš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·æŸ¥çœ‹ MetricInfoã€‚

## è®¡ç®—æŒ‡æ ‡

ä¸€æ—¦æ‚¨åŠ è½½äº†ä¸€ä¸ªæŒ‡æ ‡ï¼Œæ‚¨å°±å¯ä»¥ä½¿ç”¨å®ƒæ¥è¯„ä¼°æ¨¡åž‹çš„é¢„æµ‹ã€‚å°†æ¨¡åž‹é¢„æµ‹å’Œå‚è€ƒæä¾›ç»™ compute()ï¼š

```py
>>> model_predictions = model(model_inputs)
>>> final_score = metric.compute(predictions=model_predictions, references=gold_references)
```
