# 数据集和 IterableDataset 之间的区别

> 原文链接：[`huggingface.co/docs/datasets/about_mapstyle_vs_iterable`](https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable)

数据集对象有两种类型，一个是数据集，另一个是 IterableDataset。您选择使用或创建哪种类型的数据集取决于数据集的大小。一般来说，由于其惰性行为和速度优势，IterableDataset 非常适合大型数据集（想想数百 GB！），而数据集非常适合其他情况。本页将比较数据集和 IterableDataset 之间的区别，以帮助您选择适合您的正确数据集对象。

## 下载和流式传输

当您拥有一个常规的数据集时，您可以使用`my_dataset[0]`来访问它。这提供了对行的随机访问。这种数据集也被称为“映射样式”数据集。例如，您可以像这样下载 ImageNet-1k 并访问任何行：

```py
from datasets import load_dataset

imagenet = load_dataset("imagenet-1k", split="train")  # downloads the full dataset
print(imagenet[0])
```

但需要注意的是，您必须将整个数据集存储在磁盘或内存中，这会阻止您访问大于磁盘的数据集。因为对于大型数据集来说这可能会变得不方便，存在另一种类型的数据集，即 IterableDataset。当您拥有一个`IterableDataset`时，您可以使用`for`循环来逐步加载数据，只有一小部分示例被加载到内存中，而且您不会在磁盘上写入任何内容。

例如，您可以在不下载到磁盘上的情况下流式传输 ImageNet-1k 数据集：

```py
from datasets import load_dataset

imagenet = load_dataset("imagenet-1k", split="train", streaming=True)  # will start loading the data when iterated over
for example in imagenet:
    print(example)
    break
```

流式传输可以在不将任何文件写入磁盘的情况下读取在线数据。例如，您可以流式传输由多个分片组成的数据集，每个分片都有数百 GB，比如[C4](https://huggingface.co/datasets/c4)、[OSCAR](https://huggingface.co/datasets/oscar)或[LAION-2B](https://huggingface.co/datasets/laion/laion2B-en)。了解有关如何在数据集流式传输指南中流式传输数据集的更多信息。

然而，这并不是唯一的区别，因为在数据集的创建和处理方面，`IterableDataset`的“惰性”行为也存在。

## 创建映射样式数据集和可迭代数据集

您可以使用列表或字典创建一个数据集，数据完全转换为 Arrow，因此您可以轻松访问任何行：

```py
my_dataset = Dataset.from_dict({"col_1": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]})
print(my_dataset[0])
```

另一方面，要创建一个`IterableDataset`，您必须提供一种“惰性”加载数据的方式。在 Python 中，我们通常使用生成器函数。这些函数每次`yield`一个示例，这意味着您无法像常规`数据集`那样通过切片访问行：

```py
def my_generator(n):
    for i in range(n):
        yield {"col_1": i}

my_iterable_dataset = IterableDataset.from_generator(my_generator, gen_kwargs={"n": 10})
for example in my_iterable_dataset:
    print(example)
    break
```

## 完全和逐步加载本地文件

可以使用 load_dataset()将本地或远程数据文件转换为 Arrow 数据集：

```py
data_files = {"train": ["path/to/data.csv"]}
my_dataset = load_dataset("csv", data_files=data_files, split="train")
print(my_dataset[0])
```

然而，这需要从 CSV 转换为 Arrow 格式，如果您的数据集很大，这将需要时间和磁盘空间。

为了节省磁盘空间并跳过转换步骤，您可以通过直接从本地文件流式传输来定义一个`IterableDataset`。这样，当您在数据集上进行迭代时，数据会逐步从本地文件中读取：

```py
data_files = {"train": ["path/to/data.csv"]}
my_iterable_dataset = load_dataset("csv", data_files=data_files, split="train", streaming=True)
for example in my_iterable_dataset:  # this reads the CSV file progressively as you iterate over the dataset
    print(example)
    break
```

支持许多文件格式，如 CSV、JSONL 和 Parquet，以及图像和音频文件。您可以在相应的指南中找到有关加载表格、文本、视觉和音频数据集的更多信息。

## 急切数据处理和惰性数据处理

当您使用 Dataset.map()处理 Dataset 对象时，整个数据集会立即被处理并返回。这类似于例如`pandas`的工作方式。

```py
my_dataset = my_dataset.map(process_fn)  # process_fn is applied on all the examples of the dataset
print(my_dataset[0])
```

另一方面，由于“惰性”`IterableDataset`的特性，调用 IterableDataset.map()不会将您的`map`函数应用于整个数据集。相反，您的`map`函数是实时应用的。

因此，您可以链接多个处理步骤，当您开始迭代数据集时，它们将同时运行：

```py
my_iterable_dataset = my_iterable_dataset.map(process_fn_1)
my_iterable_dataset = my_iterable_dataset.filter(filter_fn)
my_iterable_dataset = my_iterable_dataset.map(process_fn_2)

# process_fn_1, filter_fn and process_fn_2 are applied on-the-fly when iterating over the dataset
for example in my_iterable_dataset:  
    print(example)
    break
```

## 精确和快速近似洗牌

当您使用 Dataset.shuffle()对 Dataset 进行洗牌时，您会对数据集进行精确的洗牌。它通过获取索引列表`[0, 1, 2, ... len(my_dataset) - 1]`并对该列表进行洗牌来工作。然后，访问`my_dataset[0]`将返回由已洗牌的索引映射的第一个元素定义的行和索引：

```py
my_dataset = my_dataset.shuffle(seed=42)
print(my_dataset[0])
```

由于在`IterableDataset`的情况下无法随机访问行，因此无法使用洗牌后的索引列表并在任意位置访问行。这阻止了使用精确的洗牌。相反，在 IterableDataset.shuffle()中使用了快速近似洗牌。它使用洗牌缓冲区从数据集中迭代地抽样随机示例。由于数据集仍然是迭代读取的，因此提供了出色的速度性能：

```py
my_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)
for example in my_iterable_dataset:
    print(example)
    break
```

但是，仅使用洗牌缓冲区无法为机器学习模型训练提供令人满意的洗牌。因此，如果您的数据集由多个文件或来源组成，则 IterableDataset.shuffle()也会对数据集片段进行洗牌：

```py
# Stream from the internet
my_iterable_dataset = load_dataset("deepmind/code_contests", split="train", streaming=True)
my_iterable_dataset.n_shards  # 39

# Stream from local files
data_files = {"train": [f"path/to/data_{i}.csv" for i in range(1024)]}
my_iterable_dataset = load_dataset("csv", data_files=data_files, split="train", streaming=True)
my_iterable_dataset.n_shards  # 1024

# From a generator function
def my_generator(n, sources):
    for source in sources:
        for example_id_for_current_source in range(n):
            yield {"example_id": f"{source}_{example_id_for_current_source}"}

gen_kwargs = {"n": 10, "sources": [f"path/to/data_{i}" for i in range(1024)]}
my_iterable_dataset = IterableDataset.from_generator(my_generator, gen_kwargs=gen_kwargs)
my_iterable_dataset.n_shards  # 1024
```

## 速度差异

常规 Dataset 对象基于 Arrow，提供对行的快速随机访问。由于内存映射和 Arrow 是内存格式，从磁盘读取数据不会进行昂贵的系统调用和反序列化。通过在连续的 Arrow 记录批次上进行迭代，使用`for`循环进行迭代时，甚至可以提供更快的数据加载。

但是，一旦您的 Dataset 具有索引映射（例如通过 Dataset.shuffle()），速度可能会变慢 10 倍。这是因为需要额外的步骤来获取要使用索引映射读取的行索引，最重要的是，您不再读取连续的数据块。要恢复速度，您需要再次使用 Dataset.flatten_indices()在磁盘上重新编写整个数据集，以删除索引映射。不过，这可能需要很长时间，具体取决于数据集的大小：

```py
my_dataset[0]  # fast
my_dataset = my_dataset.shuffle(seed=42)
my_dataset[0]  # up to 10x slower
my_dataset = my_dataset.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data
my_dataset[0]  # fast again
```

在这种情况下，我们建议切换到 IterableDataset，并利用其快速近似洗牌方法 IterableDataset.shuffle()。它只会对碎片顺序进行洗牌，并向您的数据集添加一个洗牌缓冲区，从而保持数据集的速度最佳。您也可以轻松地重新洗牌数据集：

```py
for example in enumerate(my_iterable_dataset):  # fast
    pass

shuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)

for example in enumerate(shuffled_iterable_dataset):  # as fast as before
    pass

shuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=1337, buffer_size=100)  # reshuffling using another seed is instantaneous

for example in enumerate(shuffled_iterable_dataset):  # still as fast as before
    pass
```

如果您在多个时期使用数据集，则用于在洗牌缓冲区中洗牌碎片顺序的有效种子是`seed + epoch`。这样可以很容易地在时期之间重新洗牌数据集：

```py
for epoch in range(n_epochs):
    my_iterable_dataset.set_epoch(epoch)
    for example in my_iterable_dataset:  # fast + reshuffled at each epoch using `effective_seed = seed + epoch`
        pass
```

## 从映射样式切换到可迭代样式

如果您想要从 IterableDataset 的“惰性”行为或其速度优势中受益，可以将您的映射样式 Dataset 切换为 IterableDataset：

```py
my_iterable_dataset = my_dataset.to_iterable_dataset()
```

如果您想要洗牌您的数据集或将其与 PyTorch DataLoader 一起使用，我们建议生成一个分片的 IterableDataset：

```py
my_iterable_dataset = my_dataset.to_iterable_dataset(num_shards=1024)
my_iterable_dataset.n_shards  # 1024
```
