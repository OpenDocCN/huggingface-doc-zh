# 量化

> 原文链接：[`huggingface.co/docs/optimum/concept_guides/quantization`](https://huggingface.co/docs/optimum/concept_guides/quantization)

量化是一种通过使用低精度数据类型（如 8 位整数（`int8`））代替通常的 32 位浮点数（`float32`）来表示权重和激活来减少运行推断的计算和内存成本的技术。

减少位数意味着结果模型需要更少的内存存储，消耗更少的能量（理论上），并且像矩阵乘法这样的操作可以通过整数运算更快地执行。它还允许在嵌入式设备上运行模型，有时这些设备只支持整数数据类型。

## 理论

量化的基本思想非常简单：从高精度表示（通常是常规的 32 位浮点数）转换为较低精度数据类型，权重和激活。最常见的较低精度数据类型是：

+   `float16`，累积数据类型`float16`

+   `bfloat16`，累积数据类型`float32`

+   `int16`，累积数据类型`int32`

+   `int8`，累积数据类型`int32`

累积数据类型指定累积（添加、乘法等）数据类型值的结果类型。例如，让我们考虑两个`int8`值`A = 127`，`B = 127`，并且让`C`为`A`和`B`的和：

```py
C = A + B
```

这里的结果比`int8`中最大的可表示值`127`要大得多。因此，需要更大精度的数据类型以避免巨大的精度损失，否则整个量化过程将变得无用。

## 量化

最常见的量化情况是`float32 -> float16`和`float32 -> int8`。

### 量化为 float16

将`float32`转换为`float16`的量化是非常简单的，因为这两种数据类型遵循相同的表示方案。在将操作量化为`float16`时，要问自己的问题是：

+   我的操作是否有`float16`实现？

+   我的硬件是否支持`float16`？例如，英特尔 CPU[一直支持`float16`作为存储类型，但是计算是在转换为`float32`之后进行的](https://scicomp.stackexchange.com/a/35193)。完全支持将在 Cooper Lake 和 Sapphire Rapids 中实现。

+   我的操作对较低精度敏感吗？例如，在`LayerNorm`中 epsilon 的值通常非常小（~ `1e-12`），但在`float16`中最小的可表示值为~ `6e-5`，这可能会导致`NaN`问题。对于大值也是一样。

### 量化为 int8

将`float32`转换为`int8`的量化更加棘手。在`int8`中只能表示 256 个值，而`float32`可以表示非常广泛的值。关键是找到将我们的`float32`值范围`[a, b]`投影到`int8`空间的最佳方法。

让我们考虑一个在`[a, b]`中的浮点数`x`，然后我们可以编写以下量化方案，也称为*仿射量化方案*：

```py
x = S * (x_q - Z)
```

其中：

+   `x_q`是与`x`相关联的量化`int8`值

+   `S`和`Z`是量化参数

    +   `S`是比例，是正的`float32`

    +   `Z`称为零点，它是与`float32`领域中值`0`对应的`int8`值。这很重要，因为它用于机器学习模型中的各个地方来准确表示值`0`。

`x`在`[a, b]`中的量化值`x_q`可以计算如下：

```py
x_q = round(x/S + Z)
```

并且超出`[a, b]`范围的`float32`值被剪切到最接近的可表示值，因此对于任何浮点数`x`：

```py
x_q = clip(round(x/S + Z), round(a/S + Z), round(b/S + Z))

```

通常，`round(a/S + Z)` 对应于考虑的数据类型中最小的可表示值，而`round(b/S + Z)` 对应于最大的可表示值。但是这可能会有所变化，例如当使用*对称量化方案*时，您将在下一段中看到。

### 对称和仿射量化方案

上面的方程被称为*仿射量化方案*，因为从`[a, b]`到`int8`的映射是仿射的。

这个方案的一个常见特例是*对称量化方案*，其中我们考虑浮点值的对称范围`[-a, a]`。在这种情况下，整数空间通常是`[-127, 127]`，这意味着`-128`被排除在常规的`[-128, 127]`有符号`int8`范围之外。原因是两个范围都对称允许有`Z = 0`。虽然失去了 256 个可表示值中的一个，但它可以提供加速，因为很多加法操作可以被跳过。

**注意**：要了解量化参数`S`和`Z`是如何计算的，可以阅读[用于高效整数运算推断的神经网络量化和训练](https://arxiv.org/abs/1712.05877)论文，或者关于这个主题的[Lei Mao 的博客文章](https://leimao.github.io/article/Neural-Networks-Quantization)。

### 每个张量和每个通道的量化

根据目标准确性/延迟权衡，可以调整量化参数的粒度：

+   量化参数可以根据*每个张量*的基础计算，这意味着每个张量将使用一个`(S, Z)`对。

+   量化参数可以根据*每个通道*的基础计算，这意味着可以在张量的一个维度上存储一个`(S, Z)`对。例如，对于形状为`[N, C, H, W]`的张量，对第二个维度进行*每个通道*的量化参数计算将导致有`C`对`(S, Z)`。虽然这可能会提供更好的准确性，但需要更多的内存。

### 校准

上面的部分描述了如何从`float32`到`int8`的量化工作，但一个问题仍然存在：如何确定`float32`值的`[a, b]`范围？这就是校准发挥作用的地方。

校准是量化过程中计算`float32`范围的步骤。对于权重来说，这相当容易，因为在*量化时间*已知实际范围。但对于激活来说，情况不太清楚，存在不同的方法：

1.  训练后**动态量化**：每个激活的范围在*运行时*动态计算。虽然这样做可以获得很好的结果而不需要太多工作，但由于每次计算范围引入的开销，它可能比静态量化慢一点。在某些硬件上也不是一个选项。

1.  训练后**静态量化**：每个激活的范围在*量化时间*提前计算，通常通过将代表性数据通过模型并记录激活值来实现。在实践中，步骤是：

    1.  在激活上放置观察器以记录它们的值。

    1.  在校准数据集上进行一定数量的前向传递（大约`200`个示例足够）。

    1.  根据某种*校准技术*计算每个计算的范围。

1.  **量化感知训练**：每个激活的范围在*训练时间*计算，遵循与训练后静态量化相同的思想。但是使用“伪量化”操作符代替观察器：它们记录值就像观察器一样，但也模拟由量化引起的误差，让模型适应它。

对于训练后静态量化和量化感知训练，需要定义校准技术，最常见的是：

+   最小-最大：计算的范围是`[观察到的最小值，观察到的最大值]`，这对权重效果很好。

+   移动平均最小-最大：计算的范围是`[移动平均最小观察值，移动平均最大观察值]`，这对激活效果很好。

+   直方图：记录值的直方图以及最小和最大值，然后根据某些标准进行选择：

    +   熵：范围是通过最小化完整精度和量化数据之间的误差来计算的。

    +   均方误差：范围是通过最小化全精度和量化数据之间的均方误差来计算的。

    +   百分位数：使用给定的百分位数值`p`在观察值上计算范围。其思想是尝试使计算范围中有`p%`的观察值。在进行仿射量化时，这是可能的，但在进行对称量化时，不总是可能完全匹配。您可以查看[ONNX Runtime 中的实现方式](https://github.com/microsoft/onnxruntime/blob/2cb12caf9317f1ded37f6db125cb03ba99320c40/onnxruntime/python/tools/quantization/calibrate.py#L698)以获取更多详细信息。

### 将模型量化为 int8 的实用步骤

要有效地将模型量化为`int8`，需要遵循以下步骤：

1.  选择要量化的运算符。好的运算符是在计算时间方面占主导地位的运算符，例如线性投影和矩阵乘法。

1.  尝试后训练动态量化，如果足够快则在此停止，否则继续到第 3 步。

1.  尝试后训练静态量化，这可能比动态量化更快，但通常会导致精度下降。在希望量化的模型中应用观察器。

1.  选择校准技术并执行。

1.  将模型转换为其量化形式：观察器被移除，`float32`运算符被转换为它们的`int8`对应项。

1.  评估量化模型：精度是否足够好？如果是，就在这里停止，否则从第 3 步重新开始，但这次进行量化感知训练。

## 支持在🤗 Optimum 中执行量化的工具

🤗 Optimum 提供了使用不同工具为不同目标执行量化的 API。

+   `optimum.onnxruntime`包允许[量化和运行 ONNX 模型](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization)使用 ONNX Runtime 工具。

+   `optimum.intel`包使得可以在尊重精度和延迟约束的情况下[量化](https://huggingface.co/docs/optimum/intel/optimization_inc)🤗 Transformers 模型。

+   `optimum.fx`包提供了对[PyTorch 量化函数](https://pytorch.org/docs/stable/quantization-support.html#torch-quantization-quantize-fx)的包装，以允许在 PyTorch 中以图模式量化🤗 Transformers 模型。与上述两种方法相比，这是一个更低级别的 API，提供更多灵活性，但需要您做更多工作。

+   `optimum.gptq`包允许使用 GPTQ 量化和运行 LLM 模型。

## 进一步了解：机器如何表示数字？

本节对于理解其余部分并不是必不可少的。它简要解释了计算机中数字的表示方式。由于量化是从一种表示形式到另一种表示形式的转换，了解一些基础知识可能会有所帮助，但绝对不是强制性的。

计算机的最基本的表示单位是位。计算机中的所有内容都表示为一系列位，包括数字。但是，表示方式取决于所讨论的数字是整数还是实数。

#### 整数表示

整数通常使用以下位长度表示：`8`、`16`、`32`、`64`。在表示整数时，考虑两种情况：

1.  无符号（正）整数：它们简单地表示为一系列位。每个位对应于二的幂（从`0`到`n-1`，其中`n`是位长度），结果数字是这些二的幂的和。

示例：`19`表示为无符号 int8 为`00010011`，因为：

```py
19 = 0 x 2⁷ + 0 x 2⁶ + 0 x 2⁵ + 1 x 2⁴ + 0 x 2³ + 0 x 2² + 1 x 2¹ + 1 x 2⁰
```

1.  有符号整数：表示有符号整数不那么直接，存在多种方法，最常见的是*二进制补码*。有关更多信息，您可以查看有关该主题的[Wikipedia 页面](https://en.wikipedia.org/wiki/Signed_number_representations)。

#### 实数表示

实数通常用以下位长度表示：`16`，`32`，`64`。表示实数的两种主要方法是：

1.  定点：用于表示整数部分和小数部分的固定位数。

1.  浮点：用于表示整数和小数部分的位数可以变化。

浮点表示可以表示更大范围的值，这是我们将重点关注的，因为它是最常用的。浮点表示中有三个组成部分：

1.  符号位：这是指定数字符号的位。

1.  指数部分

+   在`float16`中有 5 位

+   在`bfloat16`中有 8 位

+   在`float32`中有 8 位

+   在`float64`中有 11 位

1.  尾数

+   在`float16`中有 11 位（10 位明确存储）

+   在`bfloat16`中有 8 位（7 位明确存储）

+   在`float32`中有 24 位（23 位明确存储）

+   在`float64`中有 53 位（52 位明确存储）

有关每种数据类型的位分配的更多信息，请查看维基百科页面上关于[bfloat16 浮点格式](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)的漂亮插图。

对于实数`x`，我们有：

```py
x = sign x mantissa x (2^exponent)
```

## 参考资料

+   神经网络的量化和训练以进行高效的整数算术推断

+   机器学习（ML）中量化基础入门的博文

+   加速和压缩神经网络的量化方法的博文

+   整数表示的维基百科页面[这里](https://en.wikipedia.org/wiki/Integer_(computer_science))和[这里](https://en.wikipedia.org/wiki/Signed_number_representations)

+   关于维基百科页面

    +   [bfloat16 浮点格式](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)

    +   [半精度浮点格式](https://en.wikipedia.org/wiki/Half-precision_floating-point_format)

    +   [单精度浮点格式](https://en.wikipedia.org/wiki/Single-precision_floating-point_format)

    +   [双精度浮点格式](https://en.wikipedia.org/wiki/Double-precision_floating-point_format)
