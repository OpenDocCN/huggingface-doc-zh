# å°†æ¨¡å‹éƒ¨ç½²åˆ° Amazon SageMaker

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/sagemaker/inference`](https://huggingface.co/docs/sagemaker/inference)

åœ¨ SageMaker ä¸­éƒ¨ç½²ğŸ¤— Transformers æ¨¡å‹è¿›è¡Œæ¨ç†å°±åƒè¿™æ ·ç®€å•ï¼š

```py
from sagemaker.huggingface import HuggingFaceModel

# create Hugging Face Model Class and deploy it as SageMaker endpoint
huggingface_model = HuggingFaceModel(...).deploy()
```

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨[æ¨ç†å·¥å…·åŒ…](https://github.com/aws/sagemaker-huggingface-inference-toolkit)é›¶ä»£ç éƒ¨ç½²æ¨¡å‹ã€‚æ¨ç†å·¥å…·åŒ…å»ºç«‹åœ¨ğŸ¤— Transformers çš„[`pipeline`åŠŸèƒ½](https://huggingface.co/docs/transformers/main_classes/pipelines)ä¹‹ä¸Šã€‚å­¦ä¹ å¦‚ä½•ï¼š

+   å®‰è£…å’Œè®¾ç½®æ¨ç†å·¥å…·åŒ…ã€‚

+   åœ¨ SageMaker ä¸­éƒ¨ç½²ç»è¿‡è®­ç»ƒçš„ğŸ¤— Transformers æ¨¡å‹ã€‚

+   [ä» Hugging Face [æ¨¡å‹ Hub](https://huggingface.co/models)éƒ¨ç½²ğŸ¤— Transformers æ¨¡å‹](#deploy-a-model-from-the-hub)ã€‚

+   ä½¿ç”¨ğŸ¤— Transformers å’Œ Amazon SageMaker è¿è¡Œæ‰¹é‡è½¬æ¢ä½œä¸šã€‚

+   åˆ›å»ºè‡ªå®šä¹‰æ¨ç†æ¨¡å—ã€‚

## å®‰è£…å’Œè®¾ç½®

åœ¨å°†ğŸ¤— Transformers æ¨¡å‹éƒ¨ç½²åˆ° SageMaker ä¹‹å‰ï¼Œæ‚¨éœ€è¦æ³¨å†Œ AWS è´¦æˆ·ã€‚å¦‚æœæ‚¨è¿˜æ²¡æœ‰ AWS è´¦æˆ·ï¼Œè¯·åœ¨[æ­¤å¤„](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html)äº†è§£æ›´å¤šã€‚

ä¸€æ—¦æ‚¨æ‹¥æœ‰ AWS è´¦æˆ·ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€å¼€å§‹ä½¿ç”¨ï¼š

+   [SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html)

+   [SageMaker ç¬”è®°æœ¬å®ä¾‹](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html)

+   æœ¬åœ°ç¯å¢ƒ

è¦å¼€å§‹æœ¬åœ°è®­ç»ƒï¼Œæ‚¨éœ€è¦è®¾ç½®é€‚å½“çš„[IAM è§’è‰²](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)ã€‚

å‡çº§åˆ°æœ€æ–°çš„`sagemaker`ç‰ˆæœ¬ã€‚

```py
pip install sagemaker --upgrade
```

**SageMaker ç¯å¢ƒ**

æŒ‰ç…§ä¸‹é¢æ‰€ç¤ºè®¾ç½®æ‚¨çš„ SageMaker ç¯å¢ƒï¼š

```py
import sagemaker
sess = sagemaker.Session()
role = sagemaker.get_execution_role()
```

*æ³¨æ„ï¼šæ‰§è¡Œè§’è‰²ä»…åœ¨ SageMaker å†…è¿è¡Œç¬”è®°æœ¬æ—¶å¯ç”¨ã€‚å¦‚æœåœ¨ä¸åœ¨ SageMaker ä¸Šè¿è¡Œçš„ç¬”è®°æœ¬ä¸­è¿è¡Œ`get_execution_role`ï¼Œåˆ™ä¼šå‡ºç°`region`é”™è¯¯ã€‚*

**æœ¬åœ°ç¯å¢ƒ**

æŒ‰ç…§ä¸‹é¢æ‰€ç¤ºè®¾ç½®æ‚¨çš„æœ¬åœ°ç¯å¢ƒï¼š

```py
import sagemaker
import boto3

iam_client = boto3.client('iam')
role = iam_client.get_role(RoleName='role-name-of-your-iam-role-with-right-permissions')['Role']['Arn']
sess = sagemaker.Session()
```

## åœ¨ SageMaker ä¸­éƒ¨ç½²ç»è¿‡è®­ç»ƒçš„ğŸ¤— Transformers æ¨¡å‹

[`www.youtube.com/embed/pfBGgSGnYLs`](https://www.youtube.com/embed/pfBGgSGnYLs)

æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥éƒ¨ç½²åœ¨ SageMaker ä¸­è®­ç»ƒçš„ Hugging Face æ¨¡å‹ï¼š

+   åœ¨è®­ç»ƒå®Œæˆåéƒ¨ç½²å®ƒã€‚

+   ç¨åå¯ä»¥ä½¿ç”¨`model_data`ä» S3 éƒ¨ç½²ä¿å­˜çš„æ¨¡å‹ã€‚

ğŸ““æ‰“å¼€[ç¬”è®°æœ¬](https://github.com/huggingface/notebooks/blob/main/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb)æŸ¥çœ‹å¦‚ä½•å°†æ¨¡å‹ä» S3 éƒ¨ç½²åˆ° SageMaker è¿›è¡Œæ¨ç†çš„ç¤ºä¾‹ã€‚

### è®­ç»ƒåéƒ¨ç½²

åœ¨è®­ç»ƒåç›´æ¥éƒ¨ç½²æ‚¨çš„æ¨¡å‹ï¼Œè¯·ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„æ–‡ä»¶éƒ½ä¿å­˜åœ¨æ‚¨çš„è®­ç»ƒè„šæœ¬ä¸­ï¼ŒåŒ…æ‹¬åˆ†è¯å™¨å’Œæ¨¡å‹ã€‚

å¦‚æœä½¿ç”¨ Hugging Face çš„`Trainer`ï¼Œæ‚¨å¯ä»¥å°†åˆ†è¯å™¨ä½œä¸ºå‚æ•°ä¼ é€’ç»™`Trainer`ã€‚å½“æ‚¨è°ƒç”¨`trainer.save_model()`æ—¶ï¼Œå®ƒå°†è‡ªåŠ¨ä¿å­˜ã€‚

```py
from sagemaker.huggingface import HuggingFace

############ pseudo code start ############

# create Hugging Face Estimator for training
huggingface_estimator = HuggingFace(....)

# start the train job with our uploaded datasets as input
huggingface_estimator.fit(...)

############ pseudo code end ############

# deploy model to SageMaker Inference
predictor = hf_estimator.deploy(initial_instance_count=1, instance_type="ml.m5.xlarge")

# example request: you always need to define "inputs"
data = {
   "inputs": "Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days."
}

# request
predictor.predict(data)
```

è¿è¡Œè¯·æ±‚åï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼åˆ é™¤ç«¯ç‚¹ï¼š

```py
# delete endpoint
predictor.delete_endpoint()
```

### ä½¿ç”¨ model_data éƒ¨ç½²

å¦‚æœæ‚¨å·²ç»è®­ç»ƒäº†æ¨¡å‹å¹¶å¸Œæœ›åœ¨ä»¥åéƒ¨ç½²å®ƒï¼Œè¯·ä½¿ç”¨`model_data`å‚æ•°æŒ‡å®šæ‚¨çš„åˆ†è¯å™¨å’Œæ¨¡å‹æƒé‡çš„ä½ç½®ã€‚

```py
from sagemaker.huggingface.model import HuggingFaceModel

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
   model_data="s3://models/my-bert-model/model.tar.gz",  # path to your trained SageMaker model
   role=role,                                            # IAM role with permissions to create an endpoint
   transformers_version="4.26",                           # Transformers version used
   pytorch_version="1.13",                                # PyTorch version used
   py_version='py39',                                    # Python version used
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
   initial_instance_count=1,
   instance_type="ml.m5.xlarge"
)

# example request: you always need to define "inputs"
data = {
   "inputs": "Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days."
}

# request
predictor.predict(data)
```

è¿è¡Œæˆ‘ä»¬çš„è¯·æ±‚åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹é“¾æ¥å†æ¬¡åˆ é™¤ç«¯ç‚¹ï¼š

```py
# delete endpoint
predictor.delete_endpoint()
```

### ä¸ºéƒ¨ç½²åˆ›å»ºæ¨¡å‹å·¥ä»¶

ç¨åéƒ¨ç½²æ—¶ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å¿…éœ€æ–‡ä»¶çš„`model.tar.gz`æ–‡ä»¶ï¼Œä¾‹å¦‚ï¼š

+   `pytorch_model.bin`

+   `tf_model.h5`

+   `tokenizer.json`

+   `tokenizer_config.json`

ä¾‹å¦‚ï¼Œæ‚¨çš„æ–‡ä»¶åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
model.tar.gz/
|- pytorch_model.bin
|- vocab.txt
|- tokenizer_config.json
|- config.json
|- special_tokens_map.json
```

ä»ğŸ¤— Hub åˆ›å»ºè‡ªå·±çš„`model.tar.gz`æ–‡ä»¶ï¼š

1.  ä¸‹è½½æ¨¡å‹ï¼š

```py
git lfs install
git clone git@hf.co:{repository}
```

1.  åˆ›å»ºä¸€ä¸ª`tar`æ–‡ä»¶ï¼š

```py
cd {repository}
tar zcvf model.tar.gz *
```

1.  å°†`model.tar.gz`ä¸Šä¼ åˆ° S3ï¼š

```py
aws s3 cp model.tar.gz <s3://{my-s3-path}>
```

ç°åœ¨æ‚¨å¯ä»¥æä¾› S3 URI ç»™`model_data`å‚æ•°ï¼Œä»¥ä¾¿ç¨åéƒ¨ç½²æ‚¨çš„æ¨¡å‹ã€‚

## ä»ğŸ¤— Hub éƒ¨ç½²æ¨¡å‹

[`www.youtube.com/embed/l9QZuazbzWM`](https://www.youtube.com/embed/l9QZuazbzWM)

è¦ç›´æ¥ä»ğŸ¤— Hub éƒ¨ç½²æ¨¡å‹åˆ° SageMakerï¼Œåˆ›å»º`HuggingFaceModel`æ—¶å®šä¹‰ä¸¤ä¸ªç¯å¢ƒå˜é‡ï¼š

+   `HF_MODEL_ID` å®šä¹‰äº†æ¨¡å‹ IDï¼Œå½“æ‚¨åˆ›å»º SageMaker ç«¯ç‚¹æ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨ä»[huggingface.co/models](http://huggingface.co/models)åŠ è½½ã€‚é€šè¿‡è¿™ä¸ªç¯å¢ƒå˜é‡å¯ä»¥è®¿é—® ğŸ¤— Hub ä¸Šçš„ 10,000 å¤šä¸ªæ¨¡å‹ã€‚

+   `HF_TASK` å®šä¹‰äº† ğŸ¤— Transformers `pipeline` çš„ä»»åŠ¡ã€‚å®Œæ•´çš„ä»»åŠ¡åˆ—è¡¨å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co/docs/transformers/main_classes/pipelines)æ‰¾åˆ°ã€‚

```py
from sagemaker.huggingface.model import HuggingFaceModel

# Hub model configuration <https://huggingface.co/models>
hub = {
  'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad', # model_id from hf.co/models
  'HF_TASK':'question-answering'                           # NLP task you want to use for predictions
}

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
   env=hub,                                                # configuration for loading model from Hub
   role=role,                                              # IAM role with permissions to create an endpoint
   transformers_version="4.26",                             # Transformers version used
   pytorch_version="1.13",                                  # PyTorch version used
   py_version='py39',                                      # Python version used
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
   initial_instance_count=1,
   instance_type="ml.m5.xlarge"
)

# example request: you always need to define "inputs"
data = {
"inputs": {
	"question": "What is used for inference?",
	"context": "My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference."
	}
}

# request
predictor.predict(data)
```

åœ¨æ‚¨è¿è¡Œæˆ‘ä»¬çš„è¯·æ±‚åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å†æ¬¡åˆ é™¤ç«¯ç‚¹ï¼š

```py
# delete endpoint
predictor.delete_endpoint()
```

ğŸ““ æ‰“å¼€[ç¬”è®°æœ¬](https://github.com/huggingface/notebooks/blob/main/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)æŸ¥çœ‹å¦‚ä½•å°†æ¨¡å‹ä» ğŸ¤— Hub éƒ¨ç½²åˆ° SageMaker è¿›è¡Œæ¨æ–­çš„ç¤ºä¾‹ã€‚

## ä½¿ç”¨ ğŸ¤— Transformers å’Œ SageMaker è¿è¡Œæ‰¹é‡è½¬æ¢

[`www.youtube.com/embed/lnTixz0tUBg`](https://www.youtube.com/embed/lnTixz0tUBg)

è®­ç»ƒæ¨¡å‹åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[SageMaker æ‰¹é‡è½¬æ¢](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)æ¥æ‰§è¡Œæ¨¡å‹æ¨æ–­ã€‚æ‰¹é‡è½¬æ¢æ¥å—æ‚¨çš„æ¨æ–­æ•°æ®ä½œä¸º S3 URIï¼Œç„¶å SageMaker å°†è´Ÿè´£ä¸‹è½½æ•°æ®ï¼Œè¿è¡Œé¢„æµ‹ï¼Œå¹¶å°†ç»“æœä¸Šä¼ åˆ° S3ã€‚æœ‰å…³æ‰¹é‡è½¬æ¢çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[è¿™é‡Œ](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html)ã€‚

âš ï¸ ç›®å‰ Hugging Face æ¨æ–­ DLC ä»…æ”¯æŒ `.jsonl` ç”¨äºæ‰¹é‡è½¬æ¢ï¼Œå› ä¸ºæ–‡æœ¬æ•°æ®çš„ç»“æ„å¤æ‚ã€‚

*æ³¨æ„ï¼šç¡®ä¿æ‚¨çš„ `inputs` åœ¨é¢„å¤„ç†æœŸé—´ç¬¦åˆæ¨¡å‹çš„ `max_length`ã€‚*

å¦‚æœæ‚¨ä½¿ç”¨ Hugging Face Estimator è®­ç»ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œè¯·è°ƒç”¨ `transformer()` æ–¹æ³•ä¸ºåŸºäºè®­ç»ƒä½œä¸šçš„æ¨¡å‹åˆ›å»ºä¸€ä¸ªè½¬æ¢ä½œä¸šï¼ˆæœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[è¿™é‡Œ](https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-batch-transform)ï¼‰ï¼š

```py
batch_job = huggingface_estimator.transformer(
    instance_count=1,
    instance_type='ml.p3.2xlarge',
    strategy='SingleRecord')

batch_job.transform(
    data='s3://s3-uri-to-batch-data',
    content_type='application/json',    
    split_type='Line')
```

å¦‚æœæ‚¨æƒ³ç¨åè¿è¡Œæ‰¹é‡è½¬æ¢ä½œä¸šæˆ–ä½¿ç”¨ ğŸ¤— Hub ä¸­çš„æ¨¡å‹ï¼Œåˆ›å»ºä¸€ä¸ª `HuggingFaceModel` å®ä¾‹ï¼Œç„¶åè°ƒç”¨ `transformer()` æ–¹æ³•ï¼š

```py
from sagemaker.huggingface.model import HuggingFaceModel

# Hub model configuration <https://huggingface.co/models>
hub = {
	'HF_MODEL_ID':'distilbert-base-uncased-finetuned-sst-2-english',
	'HF_TASK':'text-classification'
}

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
   env=hub,                                                # configuration for loading model from Hub
   role=role,                                              # IAM role with permissions to create an endpoint
   transformers_version="4.26",                             # Transformers version used
   pytorch_version="1.13",                                  # PyTorch version used
   py_version='py39',                                      # Python version used
)

# create transformer to run a batch job
batch_job = huggingface_model.transformer(
    instance_count=1,
    instance_type='ml.p3.2xlarge',
    strategy='SingleRecord'
)

# starts batch transform job and uses S3 data as input
batch_job.transform(
    data='s3://sagemaker-s3-demo-test/samples/input.jsonl',
    content_type='application/json',    
    split_type='Line'
)
```

`input.jsonl` å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
{"inputs":"this movie is terrible"}
{"inputs":"this movie is amazing"}
{"inputs":"SageMaker is pretty cool"}
{"inputs":"SageMaker is pretty cool"}
{"inputs":"this movie is terrible"}
{"inputs":"this movie is amazing"}
```

ğŸ““ æ‰“å¼€[ç¬”è®°æœ¬](https://github.com/huggingface/notebooks/blob/main/sagemaker/12_batch_transform_inference/sagemaker-notebook.ipynb)æŸ¥çœ‹å¦‚ä½•è¿è¡Œç”¨äºæ¨æ–­çš„æ‰¹é‡è½¬æ¢ä½œä¸šçš„ç¤ºä¾‹ã€‚

## ç”¨æˆ·å®šä¹‰çš„ä»£ç å’Œæ¨¡å—

Hugging Face æ¨æ–­å·¥å…·åŒ…å…è®¸ç”¨æˆ·è¦†ç›– `HuggingFaceHandlerService` çš„é»˜è®¤æ–¹æ³•ã€‚æ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªåä¸º `code/` çš„æ–‡ä»¶å¤¹ï¼Œå¹¶åœ¨å…¶ä¸­æ”¾ç½®ä¸€ä¸ª `inference.py` æ–‡ä»¶ã€‚æœ‰å…³å¦‚ä½•å½’æ¡£æ¨¡å‹å·¥ä»¶çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿™é‡Œã€‚ä¾‹å¦‚ï¼š

```py
model.tar.gz/
|- pytorch_model.bin
|- ....
|- code/
  |- inference.py
  |- requirements.txt 
```

`inference.py` æ–‡ä»¶åŒ…å«æ‚¨çš„è‡ªå®šä¹‰æ¨æ–­æ¨¡å—ï¼Œ`requirements.txt` æ–‡ä»¶åŒ…å«åº”æ·»åŠ çš„é™„åŠ ä¾èµ–é¡¹ã€‚è‡ªå®šä¹‰æ¨¡å—å¯ä»¥è¦†ç›–ä»¥ä¸‹æ–¹æ³•ï¼š

+   `model_fn(model_dir)` è¦†ç›–äº†åŠ è½½æ¨¡å‹çš„é»˜è®¤æ–¹æ³•ã€‚è¿”å›å€¼ `model` å°†åœ¨ `predict` ä¸­ç”¨äºé¢„æµ‹ã€‚`predict` æ¥æ”¶å‚æ•° `model_dir`ï¼Œå³æ‚¨è§£å‹åçš„ `model.tar.gz` çš„è·¯å¾„ã€‚

+   `transform_fn(model, data, content_type, accept_type)` è¦†ç›–äº†é»˜è®¤çš„è½¬æ¢å‡½æ•°ï¼Œä½¿ç”¨æ‚¨è‡ªå®šä¹‰çš„å®ç°ã€‚æ‚¨éœ€è¦åœ¨ `transform_fn` ä¸­å®ç°è‡ªå·±çš„ `preprocess`ã€`predict` å’Œ `postprocess` æ­¥éª¤ã€‚æ­¤æ–¹æ³•ä¸èƒ½ä¸ä¸‹é¢æåˆ°çš„ `input_fn`ã€`predict_fn` æˆ– `output_fn` ç»“åˆä½¿ç”¨ã€‚

+   `input_fn(input_data, content_type)` è¦†ç›–äº†é¢„å¤„ç†çš„é»˜è®¤æ–¹æ³•ã€‚è¿”å›å€¼ `data` å°†åœ¨ `predict` ä¸­ç”¨äºé¢„æµ‹ã€‚è¾“å…¥æ˜¯ï¼š

    +   `input_data` æ˜¯æ‚¨è¯·æ±‚çš„åŸå§‹ä¸»ä½“ã€‚

    +   `content_type` æ˜¯è¯·æ±‚å¤´ä¸­çš„å†…å®¹ç±»å‹ã€‚

+   `predict_fn(processed_data, model)` è¦†ç›–äº†é¢„æµ‹çš„é»˜è®¤æ–¹æ³•ã€‚è¿”å›å€¼ `predictions` å°†åœ¨ `postprocess` ä¸­ä½¿ç”¨ã€‚è¾“å…¥æ˜¯ `processed_data`ï¼Œæ˜¯ä» `preprocess` å¾—åˆ°çš„ç»“æœã€‚

+   `output_fn(prediction, accept)` è¦†ç›–äº†åå¤„ç†çš„é»˜è®¤æ–¹æ³•ã€‚è¿”å›å€¼ `result` å°†æ˜¯æ‚¨è¯·æ±‚çš„å“åº”ï¼ˆä¾‹å¦‚ `JSON`ï¼‰ã€‚è¾“å…¥æ˜¯ï¼š

    +   `predictions`æ˜¯æ¥è‡ª`predict`çš„ç»“æœã€‚

    +   `accept`æ˜¯æ¥è‡ª HTTP è¯·æ±‚çš„è¿”å›æ¥å—ç±»å‹ï¼Œä¾‹å¦‚`application/json`ã€‚

è¿™é‡Œæ˜¯ä¸€ä¸ªå¸¦æœ‰`model_fn`ã€`input_fn`ã€`predict_fn`å’Œ`output_fn`çš„è‡ªå®šä¹‰æ¨ç†æ¨¡å—ç¤ºä¾‹ï¼š

```py
from sagemaker_huggingface_inference_toolkit import decoder_encoder

def model_fn(model_dir):
    # implement custom code to load the model
    loaded_model = ...

    return loaded_model 

def input_fn(input_data, content_type):
    # decode the input data  (e.g. JSON string -> dict)
    data = decoder_encoder.decode(input_data, content_type)
    return data

def predict_fn(data, model):
    # call your custom model with the data
    outputs = model(data , ... )
    return predictions

def output_fn(prediction, accept):
    # convert the model output to the desired output format (e.g. dict -> JSON string)
    response = decoder_encoder.encode(prediction, accept)
    return response
```

åªä½¿ç”¨`model_fn`å’Œ`transform_fn`è‡ªå®šä¹‰æ‚¨çš„æ¨ç†æ¨¡å—ï¼š

```py
from sagemaker_huggingface_inference_toolkit import decoder_encoder

def model_fn(model_dir):
    # implement custom code to load the model
    loaded_model = ...

    return loaded_model 

def transform_fn(model, input_data, content_type, accept):
     # decode the input data (e.g. JSON string -> dict)
    data = decoder_encoder.decode(input_data, content_type)

    # call your custom model with the data
    outputs = model(data , ... ) 

    # convert the model output to the desired output format (e.g. dict -> JSON string)
    response = decoder_encoder.encode(output, accept)

    return response
```
