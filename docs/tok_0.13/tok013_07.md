# 从内存中训练

> 原始文本：[`huggingface.co/docs/tokenizers/training_from_memory`](https://huggingface.co/docs/tokenizers/training_from_memory)

在快速入门中，我们看到了如何使用文本文件构建和训练分词器，但实际上我们可以使用任何 Python 迭代器。在本节中，我们将看到几种不同的训练分词器的方法。

对于下面列出的所有示例，我们将使用相同的 Tokenizer 和`Trainer`，构建如下：

```py
from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers
tokenizer = Tokenizer(models.Unigram())
tokenizer.normalizer = normalizers.NFKC()
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()
tokenizer.decoder = decoders.ByteLevel()
trainer = trainers.UnigramTrainer(
    vocab_size=20000,
    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),
    special_tokens=["<PAD>", "<BOS>", "<EOS>"],
)
```

这个分词器基于 Unigram 模型。它通过 NFKC Unicode 规范化方法对输入进行处理，并使用具有相应解码器的 ByteLevel 预分词器。

有关此处使用的组件的更多信息，您可以在此处查看。

## 最基本的方式

您可能已经猜到了，训练我们的分词器最简单的方法是使用`List`{.interpreted-text role=“obj”}：

```py
# First few lines of the "Zen of Python" https://www.python.org/dev/peps/pep-0020/
data = [
    "Beautiful is better than ugly."
    "Explicit is better than implicit."
    "Simple is better than complex."
    "Complex is better than complicated."
    "Flat is better than nested."
    "Sparse is better than dense."
    "Readability counts."
]
tokenizer.train_from_iterator(data, trainer=trainer)
```

简单，对吧？您可以在这里使用任何作为迭代器的东西，无论是`List`{.interpreted-text role=“obj”}、`Tuple`{.interpreted-text role=“obj”}还是`np.Array`{.interpreted-text role=“obj”}。只要提供字符串，任何东西都可以使用。

## 使用🤗数据集库

访问众多现有数据集之一的绝佳方式是使用🤗数据集库。有关更多信息，请查看[此处的官方文档](https://huggingface.co/docs/datasets/)。

让我们从加载数据集开始：

```py
import datasets
dataset = datasets.load_dataset("wikitext", "wikitext-103-raw-v1", split="train+test+validation")
```

下一步是构建一个在这个数据集上的迭代器。可能最简单的方法是使用生成器：

```py
def batch_iterator(batch_size=1000):
    for i in range(0, len(dataset), batch_size):
        yield dataset[i : i + batch_size]["text"]
```

正如您在这里所看到的，为了提高效率，我们实际上可以提供一批用于训练的示例，而不是逐个迭代它们。通过这样做，我们可以期望获得与直接从文件训练时相似的性能。

有了准备好的迭代器，我们只需要启动训练。为了改善进度条的外观，我们可以指定数据集的总长度：

```py
tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))
```

就是这样！

## 使用 gzip 文件

由于 Python 中的 gzip 文件可以用作迭代器，因此在这些文件上进行训练非常简单：

```py
import gzip
with gzip.open("data/my-file.0.gz", "rt") as f:
    tokenizer.train_from_iterator(f, trainer=trainer)
```

现在，如果我们想要从多个 gzip 文件进行训练，那也不会更难：

```py
files = ["data/my-file.0.gz", "data/my-file.1.gz", "data/my-file.2.gz"]
def gzip_iterator():
    for path in files:
        with gzip.open(path, "rt") as f:
            for line in f:
                yield line
tokenizer.train_from_iterator(gzip_iterator(), trainer=trainer)
```

然后就完成了！
