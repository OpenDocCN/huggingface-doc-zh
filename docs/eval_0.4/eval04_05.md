# å¿«é€Ÿæµè§ˆ

> åŸæ–‡é“¾æ¥ï¼š[`huggingface.co/docs/evaluate/a_quick_tour`](https://huggingface.co/docs/evaluate/a_quick_tour)

ğŸ¤— Evaluate æä¾›äº†å¹¿æ³›çš„è¯„ä¼°å·¥å…·ã€‚å®ƒæ¶µç›–äº†æ–‡æœ¬ã€è®¡ç®—æœºè§†è§‰ã€éŸ³é¢‘ç­‰å¤šç§å½¢å¼ï¼Œä»¥åŠç”¨äºè¯„ä¼°æ¨¡å‹æˆ–æ•°æ®é›†çš„å·¥å…·ã€‚è¿™äº›å·¥å…·åˆ†ä¸ºä¸‰ç±»ã€‚

## è¯„ä¼°ç±»å‹

å…¸å‹æœºå™¨å­¦ä¹ æµæ°´çº¿çš„ä¸åŒæ–¹é¢å¯ä»¥è¿›è¡Œè¯„ä¼°ï¼Œå¯¹äºæ¯ä¸ªæ–¹é¢ğŸ¤— Evaluate éƒ½æä¾›äº†ä¸€ä¸ªå·¥å…·ï¼š

+   **åº¦é‡**ï¼šåº¦é‡ç”¨äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œé€šå¸¸æ¶‰åŠæ¨¡å‹çš„é¢„æµ‹ä»¥åŠä¸€äº›åœ°é¢çœŸå®æ ‡ç­¾ã€‚æ‚¨å¯ä»¥åœ¨[evaluate-metric](https://huggingface.co/evaluate-metric)æ‰¾åˆ°æ‰€æœ‰é›†æˆçš„åº¦é‡ã€‚

+   **æ¯”è¾ƒ**ï¼šæ¯”è¾ƒç”¨äºæ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥é€šè¿‡å°†å®ƒä»¬çš„é¢„æµ‹ä¸åœ°é¢çœŸå®æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒå¹¶è®¡ç®—å®ƒä»¬çš„ä¸€è‡´æ€§æ¥è¿›è¡Œæ¯”è¾ƒã€‚æ‚¨å¯ä»¥åœ¨[evaluate-comparison](https://huggingface.co/evaluate-comparison)æ‰¾åˆ°æ‰€æœ‰é›†æˆçš„æ¯”è¾ƒã€‚

+   **æµ‹é‡**ï¼šæ•°æ®é›†å’Œè®­ç»ƒæ¨¡å‹ä¸€æ ·é‡è¦ã€‚é€šè¿‡æµ‹é‡ï¼Œå¯ä»¥ç ”ç©¶æ•°æ®é›†çš„å±æ€§ã€‚æ‚¨å¯ä»¥åœ¨[evaluate-measurement](https://huggingface.co/evaluate-measurement)æ‰¾åˆ°æ‰€æœ‰é›†æˆçš„æµ‹é‡ã€‚

è¿™äº›è¯„ä¼°æ¨¡å—ä¸­çš„æ¯ä¸€ä¸ªéƒ½å­˜åœ¨äº Hugging Face Hub ä¸Šä½œä¸ºä¸€ä¸ª Spaceã€‚å®ƒä»¬å¸¦æœ‰ä¸€ä¸ªäº¤äº’å¼å°éƒ¨ä»¶å’Œä¸€ä¸ªæ–‡æ¡£å¡ï¼Œè®°å½•äº†å…¶ä½¿ç”¨å’Œé™åˆ¶ã€‚ä¾‹å¦‚[accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy)ï¼š

![](img/ebd2d536b44a838c1365ef03fe7fa028.png)

æ¯ä¸ªåº¦é‡ã€æ¯”è¾ƒå’Œæµ‹é‡éƒ½æ˜¯ä¸€ä¸ªå•ç‹¬çš„ Python æ¨¡å—ï¼Œä½†è¦ä½¿ç”¨å…¶ä¸­ä»»ä½•ä¸€ä¸ªï¼Œéƒ½æœ‰ä¸€ä¸ªå•ä¸€çš„å…¥å£ç‚¹ï¼ševaluate.load()!

## åŠ è½½

ä»»ä½•åº¦é‡ã€æ¯”è¾ƒæˆ–æµ‹é‡éƒ½å¯ä»¥ä½¿ç”¨`evaluate.load`å‡½æ•°åŠ è½½ï¼š

```py
>>> import evaluate
>>> accuracy = evaluate.load("accuracy")
```

å¦‚æœæ‚¨æƒ³ç¡®ä¿åŠ è½½æ­£ç¡®ç±»å‹çš„è¯„ä¼°ï¼ˆç‰¹åˆ«æ˜¯å¦‚æœå­˜åœ¨åç§°å†²çªï¼‰ï¼Œæ‚¨å¯ä»¥æ˜ç¡®ä¼ é€’ç±»å‹ï¼š

```py
>>> word_length = evaluate.load("word_length", module_type="measurement")
```

### ç¤¾åŒºæ¨¡å—

é™¤äº†ğŸ¤— Evaluate ä¸­å®ç°çš„æ¨¡å—å¤–ï¼Œæ‚¨è¿˜å¯ä»¥é€šè¿‡æŒ‡å®šåº¦é‡å®ç°çš„å­˜å‚¨åº“ ID åŠ è½½ä»»ä½•ç¤¾åŒºæ¨¡å—ï¼š

```py
>>> element_count = evaluate.load("lvwerra/element_count", module_type="measurement")
```

æœ‰å…³ä¸Šä¼ è‡ªå®šä¹‰åº¦é‡çš„ä¿¡æ¯ï¼Œè¯·å‚é˜…åˆ›å»ºå’Œå…±äº«æŒ‡å—ã€‚

### åˆ—å‡ºå¯ç”¨æ¨¡å—

ä½¿ç”¨ list_evaluation_modules()å¯ä»¥æ£€æŸ¥ Hub ä¸Šæœ‰å“ªäº›æ¨¡å—å¯ç”¨ã€‚æ‚¨è¿˜å¯ä»¥è¿‡æ»¤ç‰¹å®šæ¨¡å—å¹¶è·³è¿‡ç¤¾åŒºåº¦é‡ã€‚æ‚¨è¿˜å¯ä»¥æŸ¥çœ‹é¢å¤–ä¿¡æ¯ï¼Œå¦‚å–œæ¬¢æ•°ï¼š

```py
>>> evaluate.list_evaluation_modules(
...   module_type="comparison",
...   include_community=False,
...   with_details=True)

[{'name': 'mcnemar', 'type': 'comparison', 'community': False, 'likes': 1},
 {'name': 'exact_match', 'type': 'comparison', 'community': False, 'likes': 0}]
```

## æ¨¡å—å±æ€§

æ‰€æœ‰è¯„ä¼°æ¨¡å—éƒ½å¸¦æœ‰ä¸€ç³»åˆ—æœ‰ç”¨çš„å±æ€§ï¼Œå¯å¸®åŠ©ä½¿ç”¨å­˜å‚¨åœ¨ EvaluationModuleInfo å¯¹è±¡ä¸­çš„æ¨¡å—ã€‚

| å±æ€§ | æè¿° |
| --- | --- |
| `description` | è¯„ä¼°æ¨¡å—çš„ç®€è¦æè¿°ã€‚ |
| `citation` | å½“å¯ç”¨æ—¶ç”¨äºå¼•ç”¨çš„ BibTex å­—ç¬¦ä¸²ã€‚ |
| `features` | å®šä¹‰è¾“å…¥æ ¼å¼çš„`Features`å¯¹è±¡ã€‚ |
| `inputs_description` | è¿™ç›¸å½“äºæ¨¡å—çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚ |
| `homepage` | æ¨¡å—çš„ä¸»é¡µã€‚ |
| `license` | æ¨¡å—çš„è®¸å¯è¯ã€‚ |
| `codebase_urls` | æ¨¡å—èƒŒåçš„ä»£ç é“¾æ¥ã€‚ |
| `reference_urls` | é™„åŠ å‚è€ƒé“¾æ¥ã€‚ |

è®©æˆ‘ä»¬çœ‹å‡ ä¸ªä¾‹å­ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å‡†ç¡®åº¦åº¦é‡çš„`description`å±æ€§ï¼š

```py
>>> accuracy = evaluate.load("accuracy")
>>> accuracy.description
Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:
Accuracy = (TP + TN) / (TP + TN + FP + FN)
 Where:
TP: True positive
TN: True negative
FP: False positive
FN: False negative
```

æ‚¨å¯ä»¥çœ‹åˆ°å®ƒæè¿°äº†åº¦é‡åœ¨ç†è®ºä¸Šçš„å·¥ä½œåŸç†ã€‚å¦‚æœæ‚¨åœ¨å·¥ä½œä¸­ä½¿ç”¨æ­¤åº¦é‡ï¼Œç‰¹åˆ«æ˜¯å¦‚æœå®ƒæ˜¯å­¦æœ¯å‡ºç‰ˆç‰©ï¼Œæ‚¨å¸Œæœ›æ­£ç¡®å¼•ç”¨å®ƒã€‚ä¸ºæ­¤ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹`citation`å±æ€§ï¼š

```py
>>> accuracy.citation
@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}
```

åœ¨æˆ‘ä»¬å¯ä»¥å°†åº¦é‡æˆ–å…¶ä»–è¯„ä¼°æ¨¡å—åº”ç”¨äºç”¨ä¾‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“åº¦é‡çš„è¾“å…¥æ ¼å¼æ˜¯ä»€ä¹ˆï¼š

```py
>>> accuracy.features
{
    'predictions': Value(dtype='int32', id=None),
    'references': Value(dtype='int32', id=None)
}
```

è¯·æ³¨æ„ï¼Œfeatures å§‹ç»ˆæè¿°å•ä¸ªè¾“å…¥å…ƒç´ çš„ç±»å‹ã€‚é€šå¸¸æˆ‘ä»¬ä¼šæ·»åŠ å…ƒç´ åˆ—è¡¨ï¼Œå› æ­¤æ‚¨å¯ä»¥å§‹ç»ˆå°†`features`ä¸­çš„ç±»å‹æƒ³è±¡ä¸ºåˆ—è¡¨ã€‚Evaluate æ¥å—å„ç§è¾“å…¥æ ¼å¼ï¼ˆPython åˆ—è¡¨ã€NumPy æ•°ç»„ã€PyTorch å¼ é‡ç­‰ï¼‰ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºé€‚åˆå­˜å‚¨å’Œè®¡ç®—çš„æ ¼å¼ã€‚

## è®¡ç®—

ç°åœ¨æˆ‘ä»¬çŸ¥é“è¯„ä¼°æ¨¡å—çš„å·¥ä½œåŸç†å’Œåº”è¯¥æ”¾å…¥å…¶ä¸­çš„å†…å®¹ï¼Œæˆ‘ä»¬æƒ³è¦å®é™…ä½¿ç”¨å®ƒï¼åœ¨è®¡ç®—å®é™…å¾—åˆ†æ—¶ï¼Œæœ‰ä¸¤ç§ä¸»è¦æ–¹æ³•å¯ä»¥åšåˆ°ï¼š

1.  ä¸€ä½“åŒ–

1.  å¢é‡

åœ¨å¢é‡æ–¹æ³•ä¸­ï¼Œå¿…è¦çš„è¾“å…¥é€šè¿‡ EvaluationModule.add()æˆ– EvaluationModule.add_batch()æ·»åŠ åˆ°æ¨¡å—ä¸­ï¼Œå¹¶ä¸”æœ€ç»ˆå¾—åˆ†é€šè¿‡ EvaluationModule.compute()è®¡ç®—ã€‚æˆ–è€…ï¼Œæ‚¨å¯ä»¥ä¸€æ¬¡æ€§å°†æ‰€æœ‰è¾“å…¥ä¼ é€’ç»™`compute()`ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸¤ç§æ–¹æ³•ã€‚

### å¦‚ä½•è®¡ç®—

è®¡ç®—è¯„ä¼°æ¨¡å—å¾—åˆ†çš„æœ€ç®€å•æ–¹æ³•æ˜¯ç›´æ¥è°ƒç”¨`compute()`å¹¶å°†å¿…è¦çš„è¾“å…¥ä¼ é€’ç»™`compute()`æ–¹æ³•ä¸­çš„`features`ã€‚

```py
>>> accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1])
{'accuracy': 0.5}
```

è¯„ä¼°æ¨¡å—ä»¥å­—å…¸å½¢å¼è¿”å›ç»“æœã€‚ç„¶è€Œï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥è¿­ä»£åœ°æˆ–ä»¥åˆ†å¸ƒå¼æ–¹å¼æ„å»ºé¢„æµ‹ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹`add()`æˆ–`add_batch()`ä¼šå¾ˆæœ‰ç”¨ã€‚

### è®¡ç®—å•ä¸ªæŒ‡æ ‡æˆ–ä¸€æ‰¹æŒ‡æ ‡

åœ¨è®¸å¤šè¯„ä¼°æµæ°´çº¿ä¸­ï¼Œæ‚¨ä¼šè¿­ä»£åœ°æ„å»ºé¢„æµ‹ï¼Œæ¯”å¦‚åœ¨ for å¾ªç¯ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥å°†é¢„æµ‹å­˜å‚¨åœ¨åˆ—è¡¨ä¸­ï¼Œå¹¶åœ¨æœ€åå°†å®ƒä»¬ä¼ é€’ç»™`compute()`ã€‚ä½¿ç”¨`add()`å’Œ`add_batch()`å¯ä»¥é¿å…å•ç‹¬å­˜å‚¨é¢„æµ‹çš„æ­¥éª¤ã€‚å¦‚æœæ‚¨ä¸€æ¬¡åªåˆ›å»ºå•ä¸ªé¢„æµ‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`add()`ï¼š

```py
>>> for ref, pred in zip([0,1,0,1], [1,0,0,1]):
>>>     accuracy.add(references=ref, predictions=pred)
>>> accuracy.compute()
{'accuracy': 0.5}
```

ä¸€æ—¦æ”¶é›†åˆ°æ‰€æœ‰é¢„æµ‹ï¼Œæ‚¨å¯ä»¥è°ƒç”¨`compute()`æ ¹æ®æ‰€æœ‰å­˜å‚¨çš„å€¼è®¡ç®—å¾—åˆ†ã€‚å½“ä»¥æ‰¹é‡æ–¹å¼è·å–é¢„æµ‹å’Œå‚è€ƒå€¼æ—¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`add_batch()`ï¼Œå®ƒä¸ºä»¥åå¤„ç†æ·»åŠ äº†ä¸€ä¸ªå…ƒç´ åˆ—è¡¨ã€‚å…¶ä½™æ“ä½œä¸`add()`ç›¸åŒï¼š

```py
>>> for refs, preds in zip([[0,1],[0,1]], [[1,0],[0,1]]):
>>>     accuracy.add_batch(references=refs, predictions=preds)
>>> accuracy.compute()
{'accuracy': 0.5}
```

å½“æ‚¨éœ€è¦æ‰¹é‡ä»æ¨¡å‹è·å–é¢„æµ‹æ—¶ï¼Œè¿™ç§æ–¹æ³•å°¤å…¶æœ‰ç”¨ï¼š

```py
>>> for model_inputs, gold_standards in evaluation_dataset:
>>>     predictions = model(model_inputs)
>>>     metric.add_batch(references=gold_standards, predictions=predictions)
>>> metric.compute()
```

### åˆ†å¸ƒå¼è¯„ä¼°

åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è®¡ç®—æŒ‡æ ‡å¯èƒ½ä¼šå¾ˆæ£˜æ‰‹ã€‚æŒ‡æ ‡è¯„ä¼°åœ¨ä¸åŒæ•°æ®é›†å­é›†ä¸Šçš„å•ç‹¬ Python è¿›ç¨‹æˆ–èŠ‚ç‚¹ä¸­æ‰§è¡Œã€‚é€šå¸¸ï¼Œå½“æŒ‡æ ‡å¾—åˆ†æ˜¯å¯åŠ çš„ï¼ˆ`f(AuB) = f(A) + f(B)`ï¼‰æ—¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åˆ†å¸ƒå¼ reduce æ“ä½œæ¥æ”¶é›†æ¯ä¸ªæ•°æ®é›†å­é›†çš„å¾—åˆ†ã€‚ä½†æ˜¯å½“æŒ‡æ ‡æ˜¯éå¯åŠ çš„ï¼ˆ`f(AuB) â‰  f(A) + f(B)`ï¼‰æ—¶ï¼Œæƒ…å†µå°±ä¸é‚£ä¹ˆç®€å•äº†ã€‚ä¾‹å¦‚ï¼Œæ‚¨ä¸èƒ½å°†æ¯ä¸ªæ•°æ®å­é›†çš„[F1](https://huggingface.co/spaces/evaluate-metric/f1)å¾—åˆ†æ±‚å’Œä½œä¸ºæ‚¨çš„**æœ€ç»ˆæŒ‡æ ‡**ã€‚

å…‹æœè¿™ä¸ªé—®é¢˜çš„å¸¸è§æ–¹æ³•æ˜¯å›é€€åˆ°å•è¿›ç¨‹è¯„ä¼°ã€‚æŒ‡æ ‡åœ¨å•ä¸ª GPU ä¸Šè¯„ä¼°ï¼Œè¿™å˜å¾—ä½æ•ˆã€‚

ğŸ¤— Evaluate é€šè¿‡ä»…åœ¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä¸Šè®¡ç®—æœ€ç»ˆæŒ‡æ ‡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚é¢„æµ‹å’Œå‚è€ƒå€¼åˆ†åˆ«ä¸ºæ¯ä¸ªèŠ‚ç‚¹è®¡ç®—å¹¶æä¾›ç»™æŒ‡æ ‡ã€‚è¿™äº›æš‚æ—¶å­˜å‚¨åœ¨ Apache Arrow è¡¨ä¸­ï¼Œé¿å…äº† GPU æˆ– CPU å†…å­˜çš„æ··ä¹±ã€‚å½“æ‚¨å‡†å¤‡å¥½`compute()`æœ€ç»ˆæŒ‡æ ‡æ—¶ï¼Œç¬¬ä¸€ä¸ªèŠ‚ç‚¹èƒ½å¤Ÿè®¿é—®æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ä¸Šå­˜å‚¨çš„é¢„æµ‹å’Œå‚è€ƒå€¼ã€‚ä¸€æ—¦æ”¶é›†åˆ°æ‰€æœ‰é¢„æµ‹å’Œå‚è€ƒå€¼ï¼Œ`compute()`å°†æ‰§è¡Œæœ€ç»ˆæŒ‡æ ‡è¯„ä¼°ã€‚

è¿™ä¸ªè§£å†³æ–¹æ¡ˆå…è®¸ğŸ¤— Evaluate æ‰§è¡Œåˆ†å¸ƒå¼é¢„æµ‹ï¼Œè¿™å¯¹äºåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­æé«˜è¯„ä¼°é€Ÿåº¦å¾ˆé‡è¦ã€‚åŒæ—¶ï¼Œæ‚¨è¿˜å¯ä»¥ä½¿ç”¨å¤æ‚çš„éå¯åŠ æŒ‡æ ‡ï¼Œè€Œä¸ä¼šæµªè´¹å®è´µçš„ GPU æˆ– CPU å†…å­˜ã€‚

## å°†å¤šä¸ªè¯„ä¼°ç»“æœåˆå¹¶

é€šå¸¸ï¼Œäººä»¬ä¸ä»…æƒ³è¯„ä¼°å•ä¸ªæŒ‡æ ‡ï¼Œè€Œä¸”æƒ³è¯„ä¼°æ•æ‰æ¨¡å‹ä¸åŒæ–¹é¢çš„ä¸€ç³»åˆ—ä¸åŒæŒ‡æ ‡ã€‚ä¾‹å¦‚ï¼Œå¯¹äºåˆ†ç±»ï¼Œé€šå¸¸è®¡ç®— F1 åˆ†æ•°ã€å¬å›ç‡å’Œç²¾ç¡®åº¦æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œä»¥ä¾¿é™¤äº†å‡†ç¡®åº¦å¤–è·å¾—æ¨¡å‹æ€§èƒ½çš„æ›´å…¨é¢çš„å›¾ç‰‡ã€‚å½“ç„¶ï¼Œæ‚¨å¯ä»¥åŠ è½½ä¸€å †æŒ‡æ ‡å¹¶ä¾æ¬¡è°ƒç”¨å®ƒä»¬ã€‚ç„¶è€Œï¼Œä¸€ä¸ªæ›´æ–¹ä¾¿çš„æ–¹æ³•æ˜¯ä½¿ç”¨ combine()å‡½æ•°å°†å®ƒä»¬æ†ç»‘åœ¨ä¸€èµ·ï¼š

```py
>>> clf_metrics = evaluate.combine(["accuracy", "f1", "precision", "recall"])
```

`combine`å‡½æ•°æ¥å—æŒ‡æ ‡åç§°åˆ—è¡¨ä»¥åŠå®ä¾‹åŒ–çš„æ¨¡å—ã€‚ç„¶å`compute`è°ƒç”¨è®¡ç®—æ¯ä¸ªæŒ‡æ ‡ï¼š

```py
>>> clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])

{
  'accuracy': 0.667,
  'f1': 0.667,
  'precision': 1.0,
  'recall': 0.5
}
```

## ä¿å­˜å¹¶æ¨é€åˆ° Hub

ä¿å­˜å’Œåˆ†äº«è¯„ä¼°ç»“æœæ˜¯ä¸€ä¸ªé‡è¦çš„æ­¥éª¤ã€‚æˆ‘ä»¬æä¾›äº† evaluate.save()å‡½æ•°æ¥è½»æ¾ä¿å­˜æŒ‡æ ‡ç»“æœã€‚æ‚¨å¯ä»¥ä¼ é€’ä¸€ä¸ªç‰¹å®šçš„æ–‡ä»¶åæˆ–ç›®å½•ã€‚åœ¨åä¸€ç§æƒ…å†µä¸‹ï¼Œç»“æœå°†ä¿å­˜åœ¨ä¸€ä¸ªè‡ªåŠ¨åˆ›å»ºçš„æ–‡ä»¶åçš„æ–‡ä»¶ä¸­ã€‚é™¤äº†ç›®å½•æˆ–æ–‡ä»¶åï¼Œè¯¥å‡½æ•°è¿˜æ¥å—ä»»ä½•é”®å€¼å¯¹ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†å®ƒä»¬å­˜å‚¨åœ¨ä¸€ä¸ª JSON æ–‡ä»¶ä¸­ã€‚

```py
>>> result = accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1])

>>> hyperparams = {"model": "bert-base-uncased"}
>>> evaluate.save("./results/"experiment="run 42", **result, **hyperparams)
PosixPath('results/result-2022_05_30-22_09_11.json')
```

JSON æ–‡ä»¶çš„å†…å®¹å¦‚ä¸‹ï¼š

```py
{
    "experiment": "run 42",
    "accuracy": 0.5,
    "model": "bert-base-uncased",
    "_timestamp": "2022-05-30T22:09:11.959469",
    "_git_commit_hash": "123456789abcdefghijkl",
    "_evaluate_version": "0.1.0",
    "_python_version": "3.9.12 (main, Mar 26 2022, 15:51:15) \n[Clang 13.1.6 (clang-1316.0.21.2)]",
    "_interpreter_path": "/Users/leandro/git/evaluate/env/bin/python"
}
```

é™¤äº†æŒ‡å®šçš„å­—æ®µï¼Œå®ƒè¿˜åŒ…å«æœ‰ç”¨çš„ç³»ç»Ÿä¿¡æ¯ï¼Œä»¥ä¾¿é‡ç°ç»“æœã€‚

é™¤äº†åœ¨æœ¬åœ°å­˜å‚¨ç»“æœï¼Œæ‚¨è¿˜åº”è¯¥å°†å®ƒä»¬æŠ¥å‘Šåˆ° Hub ä¸Šçš„æ¨¡å‹å­˜å‚¨åº“ã€‚ä½¿ç”¨ evaluate.push_to_hub()å‡½æ•°ï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°å°†è¯„ä¼°ç»“æœæŠ¥å‘Šåˆ°æ¨¡å‹å­˜å‚¨åº“ï¼š

```py
evaluate.push_to_hub(
  model_id="huggingface/gpt2-wikitext2",  # model repository on hub
  metric_value=0.5,                       # metric value
  metric_type="bleu",                     # metric name, e.g. accuracy.name
  metric_name="BLEU",                     # pretty name which is displayed
  dataset_type="wikitext",                # dataset name on the hub
  dataset_name="WikiText",                # pretty name
  dataset_split="test",                   # dataset split used
  task_type="text-generation",            # task id, see https://github.com/huggingface/datasets/blob/master/src/datasets/utils/resources/tasks.json
  task_name="Text Generation"             # pretty name for task
)
```

## è¯„ä¼°å™¨

evaluate.evaluator()æä¾›äº†è‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œåªéœ€è¦ä¸€ä¸ªæ¨¡å‹ã€æ•°æ®é›†å’ŒæŒ‡æ ‡ï¼Œä¸`EvaluationModule`ä¸­éœ€è¦æ¨¡å‹é¢„æµ‹çš„æŒ‡æ ‡ç›¸æ¯”ï¼Œæ›´å®¹æ˜“è¯„ä¼°ä¸€ä¸ªæ¨¡å‹åœ¨ç»™å®šæŒ‡æ ‡ä¸‹çš„æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œå› ä¸ºæ¨ç†æ˜¯åœ¨å†…éƒ¨å¤„ç†çš„ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œå®ƒä½¿ç”¨äº†`transformers`ä¸­çš„[pipeline](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/pipelines#transformers.pipeline)æŠ½è±¡ã€‚ç„¶è€Œï¼Œåªè¦éµå¾ª`pipeline`æ¥å£ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨è‡ªå·±çš„æ¡†æ¶ã€‚

è¦ä½¿ç”¨`evaluator`è¿›è¡Œè¯„ä¼°ï¼Œè®©æˆ‘ä»¬åŠ è½½ä¸€ä¸ª`transformers`ç®¡é“ï¼ˆä½†æ‚¨å¯ä»¥ä¼ é€’è‡ªå·±çš„è‡ªå®šä¹‰æ¨ç†ç±»ï¼Œåªè¦å®ƒéµå¾ªç®¡é“è°ƒç”¨ APIï¼‰ï¼Œè¯¥ç®¡é“ä½¿ç”¨åœ¨ IMDb ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼ŒIMDb æµ‹è¯•æ‹†åˆ†å’Œå‡†ç¡®åº¦æŒ‡æ ‡ã€‚

```py
from transformers import pipeline
from datasets import load_dataset
from evaluate import evaluator
import evaluate

pipe = pipeline("text-classification", model="lvwerra/distilbert-imdb", device=0)
data = load_dataset("imdb", split="test").shuffle().select(range(1000))
metric = evaluate.load("accuracy")
```

ç„¶åï¼Œæ‚¨å¯ä»¥ä¸ºæ–‡æœ¬åˆ†ç±»åˆ›å»ºä¸€ä¸ªè¯„ä¼°å™¨ï¼Œå¹¶å°†è¿™ä¸‰ä¸ªå¯¹è±¡ä¼ é€’ç»™`compute()`æ–¹æ³•ã€‚é€šè¿‡æ ‡ç­¾æ˜ å°„ï¼Œ`evaluate`æä¾›äº†ä¸€ç§æ–¹æ³•æ¥å°†ç®¡é“è¾“å‡ºä¸æ•°æ®é›†ä¸­çš„æ ‡ç­¾åˆ—å¯¹é½ï¼š

```py
>>> task_evaluator = evaluator("text-classification")

>>> results = task_evaluator.compute(model_or_pipeline=pipe, data=data, metric=metric,
...                        label_mapping={"NEGATIVE": 0, "POSITIVE": 1},)

>>> print(results)
{'accuracy': 0.934}
```

ä»…è®¡ç®—æŒ‡æ ‡çš„å€¼é€šå¸¸ä¸è¶³ä»¥çŸ¥é“ä¸€ä¸ªæ¨¡å‹æ˜¯å¦æ¯”å¦ä¸€ä¸ªæ¨¡å‹è¡¨ç°æ˜¾è‘—æ›´å¥½ã€‚é€šè¿‡*bootstrapping*ï¼Œ`evaluate`è®¡ç®—ç½®ä¿¡åŒºé—´å’Œæ ‡å‡†è¯¯å·®ï¼Œè¿™æœ‰åŠ©äºä¼°è®¡å¾—åˆ†çš„ç¨³å®šæ€§æœ‰å¤šé«˜ï¼š

```py
>>> results = eval.compute(model_or_pipeline=pipe, data=data, metric=metric,
...                        label_mapping={"NEGATIVE": 0, "POSITIVE": 1},
...                        strategy="bootstrap", n_resamples=200)

>>> print(results)
{'accuracy':
    {
      'confidence_interval': (0.906, 0.9406749892841922),
      'standard_error': 0.00865213251082787,
      'score': 0.923
    }
}
```

è¯„ä¼°å™¨æœŸæœ›æ•°æ®è¾“å…¥ä¸­æœ‰ä¸€ä¸ª`"text"`å’Œ`"label"`åˆ—ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†ä¸åŒï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å…³é”®å­—`input_column="text"`å’Œ`label_column="label"`æä¾›åˆ—ã€‚ç›®å‰ä»…æ”¯æŒ`"text-classification"`ï¼Œæœªæ¥å°†æ·»åŠ æ›´å¤šä»»åŠ¡ã€‚

## å¯è§†åŒ–

å½“æ¯”è¾ƒå‡ ä¸ªæ¨¡å‹æ—¶ï¼Œæœ‰æ—¶ä»…é€šè¿‡æŸ¥çœ‹å®ƒä»¬çš„åˆ†æ•°å¾ˆéš¾å‘ç°å®ƒä»¬æ€§èƒ½çš„å·®å¼‚ã€‚é€šå¸¸ä¹Ÿä¸æ˜¯ä¸€ä¸ªæœ€ä½³æ¨¡å‹ï¼Œè€Œæ˜¯åœ¨å»¶è¿Ÿå’Œå‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œä¾‹å¦‚æ›´å¤§çš„æ¨¡å‹å¯èƒ½æ€§èƒ½æ›´å¥½ï¼Œä½†ä¹Ÿæ›´æ…¢ã€‚æˆ‘ä»¬æ­£åœ¨é€æ­¥æ·»åŠ ä¸åŒçš„å¯è§†åŒ–æ–¹æ³•ï¼Œä¾‹å¦‚å›¾è¡¨ï¼Œä»¥ä¾¿æ›´å®¹æ˜“åœ°ä¸ºç”¨ä¾‹é€‰æ‹©æœ€ä½³æ¨¡å‹ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰æ¥è‡ªå¤šä¸ªæ¨¡å‹çš„ç»“æœåˆ—è¡¨ï¼ˆä½œä¸ºå­—å…¸ï¼‰ï¼Œæ‚¨å¯ä»¥å°†å®ƒä»¬è¾“å…¥åˆ°`radar_plot()`å‡½æ•°ä¸­ï¼š

```py
import evaluate
from evaluate.visualization import radar_plot

>>> data = [
   {"accuracy": 0.99, "precision": 0.8, "f1": 0.95, "latency_in_seconds": 33.6},
   {"accuracy": 0.98, "precision": 0.87, "f1": 0.91, "latency_in_seconds": 11.2},
   {"accuracy": 0.98, "precision": 0.78, "f1": 0.88, "latency_in_seconds": 87.6}, 
   {"accuracy": 0.88, "precision": 0.78, "f1": 0.81, "latency_in_seconds": 101.6}
   ]
>>> model_names = ["Model 1", "Model 2", "Model 3", "Model 4"]
>>> plot = radar_plot(data=data, model_names=model_names)
>>> plot.show()
```

è®©æ‚¨å¯ä»¥é€šè¿‡å¯è§†åŒ–æ¯”è¾ƒ 4 ä¸ªæ¨¡å‹ï¼Œå¹¶æ ¹æ®ä¸€ä¸ªæˆ–å¤šä¸ªæŒ‡æ ‡é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼š

![](img/9e9e21f42abd000f112f5e7dfcdd322c.png)

## åœ¨ä¸€ç³»åˆ—ä»»åŠ¡ä¸Šè¿è¡Œè¯„ä¼°

åœ¨å„ç§ä¸åŒä»»åŠ¡ä¸Šè¯„ä¼°æ¨¡å‹å¯èƒ½å¾ˆæœ‰ç”¨ï¼Œä»¥äº†è§£å®ƒä»¬çš„ä¸‹æ¸¸æ€§èƒ½ã€‚EvaluationSuite ä½¿å¾—å¯ä»¥åœ¨ä¸€ç»„ä»»åŠ¡ä¸Šè¯„ä¼°æ¨¡å‹ã€‚ä»»åŠ¡å¯ä»¥æ„å»ºä¸ºï¼ˆevaluatorã€æ•°æ®é›†ã€æŒ‡æ ‡ï¼‰å…ƒç»„ï¼Œå¹¶ä¼ é€’ç»™å­˜å‚¨åœ¨ Hugging Face Hub ä¸Šçš„ EvaluationSuite ä½œä¸ºä¸€ä¸ª Spaceï¼Œæˆ–è€…æœ¬åœ°ä½œä¸º Python è„šæœ¬ã€‚è¯·å‚é˜…è¯„ä¼°å™¨æ–‡æ¡£ä»¥è·å–å½“å‰æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ã€‚

`EvaluationSuite`è„šæœ¬å¯ä»¥å®šä¹‰å¦‚ä¸‹ï¼Œå¹¶æ”¯æŒç”¨äºæ•°æ®é¢„å¤„ç†çš„ Python ä»£ç ã€‚

```py
import evaluate
from evaluate.evaluation_suite import SubTask

class Suite(evaluate.EvaluationSuite):

    def __init__(self, name):
        super().__init__(name)

        self.suite = [
            SubTask(
                task_type="text-classification",
                data="imdb",
                split="test[:1]",
                args_for_task={
                    "metric": "accuracy",
                    "input_column": "text",
                    "label_column": "label",
                    "label_mapping": {
                        "LABEL_0": 0.0,
                        "LABEL_1": 1.0
                    }
                }
            ),
            SubTask(
                task_type="text-classification",
                data="sst2",
                split="test[:1]",
                args_for_task={
                    "metric": "accuracy",
                    "input_column": "sentence",
                    "label_column": "label",
                    "label_mapping": {
                        "LABEL_0": 0.0,
                        "LABEL_1": 1.0
                    }
                }
            )
        ]
```

è¯„ä¼°å¯ä»¥é€šè¿‡åŠ è½½`EvaluationSuite`å¹¶è°ƒç”¨`run()`æ–¹æ³•æ¥è¿è¡Œï¼Œä¼ å…¥ä¸€ä¸ªæ¨¡å‹æˆ– pipelineã€‚

```py
>>> from evaluate import EvaluationSuite
>>> suite = EvaluationSuite.load('mathemakitten/sentiment-evaluation-suite')
>>> results = suite.run("huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli")
```

| å‡†ç¡®ç‡ | æ€»æ—¶é—´ï¼ˆç§’ï¼‰ | æ¯ç§’æ ·æœ¬æ•° | å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ | ä»»åŠ¡åç§° |
| --: | --: | --: | :-- | :-- |
| 0.3 | 4.62804 | 2.16074 | 0.462804 | imdb |
| 0 | 0.686388 | 14.569 | 0.0686388 | sst2 |
