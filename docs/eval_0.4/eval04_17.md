# 🤗评估中的评估类型

> 原文链接：[`huggingface.co/docs/evaluate/types_of_evaluations`](https://huggingface.co/docs/evaluate/types_of_evaluations)

🤗评估库的目标是支持不同类型的评估，取决于不同的目标、数据集和模型。

以下是当前支持的评估类型及其各自的几个示例：

## 度量

度量标准衡量模型在给定数据集上的性能。这通常基于现有的地面真相（即一组参考），但也有*无参考度量标准*，允许通过利用预训练模型（如[GPT-2](https://huggingface.co/gpt2)）来评估生成的文本。

度量的示例包括：

+   [准确率](https://huggingface.co/metrics/accuracy)：在处理的总案例中，正确预测的比例。

+   [精确匹配](https://huggingface.co/metrics/exact_match)：输入预测字符串与其参考完全匹配的速率。

+   [平均交集联合（IoUO）](https://huggingface.co/metrics/mean_iou)：图像预测分割与地面真相之间的重叠区域与图像预测分割与地面真相之间的联合区域之比。

度量标准通常用于跟踪模型在基准数据集上的性能，并报告诸如[机器翻译](https://huggingface.co/tasks/translation)和[图像分类](https://huggingface.co/tasks/image-classification)等任务的进展。

## 比较

比较可以用于比较单个测试数据集上两个或多个模型的性能。

例如，[麦克马尔检验](https://github.com/huggingface/evaluate/tree/main/comparisons/mcnemar)是一种成对的非参数统计假设检验，它接受两个模型的预测并比较它们，旨在衡量模型的预测是否发散。它输出的 p 值范围从`0.0`到`1.0`，指示两个模型预测之间的差异，较低的 p 值表示更显著的差异。

然而，在比较和报告模型性能时，比较尚未系统地使用，但它们是超越简单比较排行榜分数并获取有关模型预测差异方式的有用工具。

## 测量

在🤗评估库中，测量是获取有关数据集和模型预测更多见解的工具。

例如，在数据集的情况下，可以计算数据集条目的[平均词长](https://github.com/huggingface/evaluate/tree/main/measurements/word_length)，以及其分布情况——这在选择[Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer)的最大输入长度时会有所帮助。

在模型预测的情况下，可以帮助使用不同模型（如[GPT-2](https://huggingface.co/gpt2)和[BERT](https://huggingface.co/bert-base-uncased)）计算模型预测的平均[困惑度](https://huggingface.co/metrics/perplexity)，这可以指示在没有参考文本时生成文本的质量。

🤗评估库支持的三种评估类型旨在相互补充，并帮助我们的社区进行更加审慎和负责任的评估。

在未来几个月中，我们将继续添加更多类型的度量、测量和比较，并依靠社区参与（通过[PRs](https://github.com/huggingface/evaluate/compare)和[issues](https://github.com/huggingface/evaluate/issues/new/choose)）使该库尽可能广泛和包容！
