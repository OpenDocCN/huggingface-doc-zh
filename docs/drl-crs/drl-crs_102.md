# 额外阅读

> 原文：[`huggingface.co/learn/deep-rl-course/unit8/additional-readings`](https://huggingface.co/learn/deep-rl-course/unit8/additional-readings)

如果您想深入了解，这些是**可选阅读**。

## PPO 解释

+   [通过 Daniel Bick 提供的一致自包含解释向 Proximal Policy Optimization 迈进](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)

+   如何理解强化学习中的 Proximal Policy Optimization 算法的方法？

+   [深度强化学习基础系列，L4 TRPO 和 PPO，由 Pieter Abbeel](https://youtu.be/KjWF8VIMGiY)

+   [OpenAI PPO 博文](https://openai.com/blog/openai-baselines-ppo/)

+   [Spinning Up RL PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html)

+   [论文 Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)

## PPO 实现细节

+   [Proximal Policy Optimization 的 37 个实现细节](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)

+   [第 1 部分- Proximal Policy Optimization 实现：11 个核心实现细节](https://www.youtube.com/watch?v=MEt6rrxH8W4)

## 重要性采样

+   重要性采样解释
