# 术语表

> 原文：[`huggingface.co/learn/deep-rl-course/unit3/glossary`](https://huggingface.co/learn/deep-rl-course/unit3/glossary)

这是一个由社区创建的术语表。欢迎贡献！

+   **表格方法：**状态和动作空间足够小，可以将值函数近似表示为数组和表格的问题类型。**Q 学习**是表格方法的一个例子，因为使用表格来表示不同状态-动作对的值。

+   **深度 Q 学习：**训练神经网络来近似给定状态下每个可能动作的不同**Q 值**。它用于解决观察空间太大无法应用表格 Q 学习方法的问题。

+   **时间限制**是当环境状态由帧表示时出现的困难。单独的帧不提供时间信息。为了获得时间信息，我们需要将一定数量的帧**堆叠**在一起。

+   **深度 Q 学习的阶段：**

    +   **抽样：**执行动作，并将观察到的经验元组存储在**回放内存**中。

    +   **训练：**随机选择元组的批次，并使用梯度下降更新神经网络的权重。

+   **稳定深度 Q 学习的解决方案：**

    +   **经验回放：**创建一个回放内存来保存可以在训练过程中重复使用的经验样本。这使得代理可以多次从相同的经验中学习。此外，它帮助代理避免忘记之前的经验，因为它获得了新的经验。

    +   **从回放缓冲区中随机抽样**可以消除观察序列中的相关性，并防止动作值振荡或灾难性分歧。

    +   **固定 Q-目标：**为了计算**Q-目标**，我们需要使用贝尔曼方程估计下一个状态的折扣最优**Q 值**。问题在于计算**Q-目标**和**Q 值**时使用相同的网络权重。这意味着每次修改**Q 值**时，**Q-目标**也会随之移动。为了避免这个问题，使用具有固定参数的单独网络来估计时间差异目标。目标网络通过从我们的深度 Q 网络复制参数来更新，在一定的**C 步**之后。

    +   **双重 DQN：**处理**Q 值**的**高估**的方法。此解决方案使用两个网络来解耦动作选择和目标**值生成**：

        +   **DQN 网络**选择下一个状态的最佳动作（具有最高**Q 值**的动作）

        +   **目标网络**用于计算在下一个状态采取该动作的目标**Q 值**。这种方法减少了**Q 值**的高估，有助于更快地训练并具有更稳定的学习。

如果您想改进课程，您可以[发起一个拉取请求。](https://github.com/huggingface/deep-rl-class/pulls)

这个术语表得以实现，感谢：

+   [Dario Paez](https://github.com/dario248)
