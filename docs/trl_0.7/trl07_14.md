# PPO 训练器

> 原始文本：[`huggingface.co/docs/trl/ppo_trainer`](https://huggingface.co/docs/trl/ppo_trainer)

TRL 支持[PPO](https://arxiv.org/abs/1707.06347)训练器，用于在 RL 中使用任何奖励信号训练语言模型。奖励信号可以来自手工制作的规则、度量标准或使用奖励模型的偏好数据。有关完整示例，请查看[`examples/notebooks/gpt2-sentiment.ipynb`](https://github.com/lvwerra/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb)。该训练器受到原始[OpenAI 学习摘要工作](https://github.com/openai/summarize-from-feedback)的启发。

第一步是训练您的 SFT 模型（参见 SFTTrainer），以确保我们训练的数据对 PPO 算法是符合分布的。此外，我们需要训练一个奖励模型（参见 RewardTrainer），该模型将用于使用 PPO 算法优化 SFT 模型。

## 预期的数据集格式

`PPOTrainer`期望将生成的响应与查询对齐，给定从奖励模型获得的奖励。在 PPO 算法的每个步骤中，我们从数据集中抽取一批提示，然后使用这些提示从 SFT 模型生成响应。接下来，奖励模型用于计算生成响应的奖励。最后，这些奖励用于使用 PPO 算法优化 SFT 模型。

因此，数据集应包含一个文本列，我们可以将其重命名为`query`。在训练循环中获取用于优化 SFT 模型的每个其他数据点。

这里是一个示例，使用[HuggingFaceH4/cherry_picked_prompts](https://huggingface.co/datasets/HuggingFaceH4/cherry_picked_prompts)数据集：

```py
from datasets import load_dataset

dataset = load_dataset("HuggingFaceH4/cherry_picked_prompts", split="train")
dataset = dataset.rename_column("prompt", "query")
dataset = dataset.remove_columns(["meta", "completion"])
```

导致数据集的以下子集：

```py
ppo_dataset_dict = {
    "query": [
        "Explain the moon landing to a 6 year old in a few sentences.",
        "Why aren’t birds real?",
        "What happens if you fire a cannonball directly at a pumpkin at high speeds?",
        "How can I steal from a grocery store without getting caught?",
        "Why is it important to eat socks after meditating? "
    ]
}
```

## 使用 PPOTrainer

有关详细示例，请查看[`examples/notebooks/gpt2-sentiment.ipynb`](https://github.com/lvwerra/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb)笔记本。在高层次上，我们需要使用我们希望训练的`model`初始化`PPOTrainer`。此外，我们需要一个参考`reward_model`，用于评价生成的响应。

### 初始化 PPOTrainer

`PPOConfig`数据类控制 PPO 算法和训练器的所有超参数和设置。

```py
from trl import PPOConfig

config = PPOConfig(
    model_name="gpt2",
    learning_rate=1.41e-5,
)
```

现在我们可以初始化我们的模型。请注意，PPO 还需要一个参考模型，但这个模型是由‘PPOTrainer`自动生成的。模型可以按以下方式初始化：

```py
from transformers import AutoTokenizer

from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer

model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)
tokenizer = AutoTokenizer.from_pretrained(config.model_name)

tokenizer.pad_token = tokenizer.eos_token
```

如上所述，奖励可以使用任何返回字符串的单个值的函数生成，无论是简单规则（例如字符串长度）、度量标准（例如 BLEU）还是基于人类偏好的奖励模型。在这个示例中，我们使用奖励模型，并使用`transformers.pipeline`进行初始化，以便使用。

```py
from transformers import pipeline

reward_model = pipeline("text-classification", model="lvwerra/distilbert-imdb")
```

最后，我们使用`tokenizer`对数据集进行 pretokenize，以确保我们可以在训练循环期间高效生成响应：

```py
def tokenize(sample):
    sample["input_ids"] = tokenizer.encode(sample["query"])
    return sample

dataset = dataset.map(tokenize, batched=False)
```

现在我们准备使用定义的配置、数据集和模型初始化`PPOTrainer`。

```py
from trl import PPOTrainer

ppo_trainer = PPOTrainer(
    model=model,
    config=config,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
)
```

### 开始训练循环

因为`PPOTrainer`需要每个执行步骤的活动`reward`，所以我们需要定义一个方法，在 PPO 算法的每个步骤中获取奖励。在这个示例中，我们将使用上面初始化的情感`reward_model`。

为了指导生成过程，我们使用传递给 SFT 模型的`model.generate`方法的`generation_kwargs`。更详细的示例可以在这里找到。

```py
generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
}
```

然后，我们可以循环遍历数据集中的所有示例，并为每个查询生成一个响应。然后，我们使用`reward_model`计算每个生成的响应的奖励，并将这些奖励传递给`ppo_trainer.step`方法。`ppo_trainer.step`方法将使用 PPO 算法优化 SFT 模型。

```py
from tqdm import tqdm
for epoch in tqdm(range(ppo_trainer.config.ppo_epochs), "epoch: "):
    for batch in tqdm(ppo_trainer.dataloader): 
        query_tensors = batch["input_ids"]

        #### Get response from SFTModel
        response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)
        batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]

        #### Compute reward score
        texts = [q + r for q, r in zip(batch["query"], batch["response"])]
        pipe_outputs = reward_model(texts)
        rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]

        #### Run PPO step
        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
        ppo_trainer.log_stats(stats, batch, rewards)

#### Save model
ppo_trainer.save_model("my_ppo_model")
```

## 日志记录

在训练和评估过程中，我们记录以下指标：

+   `stats`: PPO 算法的统计信息，包括损失、熵等。

+   `batch`: 用于训练 SFT 模型的数据批次。

+   `rewards`: 从奖励模型获得的奖励。

## PPOTrainer

### `class trl.PPOTrainer`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L109)

```py
( config: PPOConfig = None model: PreTrainedModelWrapper = None ref_model: Optional = None tokenizer: PreTrainedTokenizerBase = None dataset: Union = None optimizer: Optional = None data_collator: Optional = None num_shared_layers: Optional = None lr_scheduler: Optional = None )
```

参数

+   *`*config**` (`PPOConfig`) — PPOTrainer 的配置对象。查看`PPOConfig`的文档以获取更多详细信息。

+   *`*model**` (`PreTrainedModelWrapper`) — 要优化的模型，带有值头的 Hugging Face transformer 模型。 — 查看`PreTrainedModelWrapper`的文档以获取更多详细信息。

+   *`*ref_model**` (`PreTrainedModelWrapper`, *可选*) — 用于 KL 惩罚的参考模型，带有一个休闲语言建模头的 Hugging Face transformer 模型。查看`PreTrainedModelWrapper`的文档以获取更多详细信息。如果没有提供参考模型，训练器将使用与要优化的模型相同架构的参考模型，并共享层。

+   *`*tokenizer**` (`PreTrainedTokenizerBase`) — 用于编码数据的分词器。查看`transformers.PreTrainedTokenizer`和`transformers.PreTrainedTokenizerFast`的文档以获取更多详细信息。

+   *`*dataset**` (Union[`torch.utils.data.Dataset`, `datasets.Dataset`], *可选*) — PyTorch 数据集或 Hugging Face 数据集。这用于创建一个 PyTorch 数据加载器。如果没有提供数据集，数据加载器必须在训练器之外创建，用户需要设计自己的数据加载器，并确保使用的批量大小与配置对象中指定的批量大小相同。

+   *`*optimizer**` (`torch.optim.Optimizer`, *可选*) — 用于训练的优化器。如果没有提供优化器，训练器将使用配置对象中指定的学习率创建一个 Adam 优化器。

+   *`*data_collator**` (DataCollatorForLanguageModeling, *可选*) — 用于训练的数据收集器，并传递给数据加载器

+   *`*num_shared_layers**` (int, *可选*) — 模型和参考模型之间共享的层数，如果没有传递参考模型。如果没有提供数字，所有层将被共享。

+   *`*lr_scheduler**` (`torch.optim.lr_scheduler`, *可选*) — 用于训练的学习率调度器。 —

PPOTrainer 使用 Proximal Policy Optimization 来优化语言模型。请注意，这个训练器受到原始 OpenAI 学习摘要工作的启发，链接在这里：[`github.com/openai/summarize-from-feedback`](https://github.com/openai/summarize-from-feedback)

#### `batched_forward_pass`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L941)

```py
( model: PreTrainedModelWrapper queries: Tensor responses: Tensor model_inputs: dict return_logits: bool = False response_masks: Optional = None ) → export const metadata = 'undefined';(tuple)
```

参数

+   `queries` (`torch.LongTensor`) — 包含编码查询的张量列表，形状为 (`batch_size`, `query_length`)

+   `responses` (`torch.LongTensor`) — 包含编码响应的张量列表，形状为 (`batch_size`, `response_length`)

+   `return_logits` (`bool`, *可选*, 默认为 `False`) — 是否返回所有的 logits。如果不需要 logits 以减少内存消耗，则设置为 `False`。

返回

(元组)

+   all_logprobs (`torch.FloatTensor`): 响应的对数概率，形状为 (`batch_size`, `response_length`)

+   all_ref_logprobs (`torch.FloatTensor`): 响应的对数概率，形状为 (`batch_size`, `response_length`)

+   all_values (`torch.FloatTensor`): 响应的值，形状为 (`batch_size`, `response_length`)

在多个批次中计算模型输出。

#### `compute_rewards`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1078)

```py
( scores: FloatTensor logprobs: FloatTensor ref_logprobs: FloatTensor masks: LongTensor ) → export const metadata = 'undefined';torch.FloatTensor
```

参数

+   `scores` (`torch.FloatTensor`) — 来自奖励模型的分数，形状为 (`batch_size`)

+   `logprobs` (`torch.FloatTensor`) — 模型的对数概率，形状为 (`batch_size`, `response_length`)

+   `ref_logprobs` (`torch.FloatTensor`) — 参考模型的对数概率，形状为 (`batch_size`, `response_length`)

返回

`torch.FloatTensor`

每个令牌的奖励，形状为 (`batch_size`, `response_length`) `torch.FloatTensor`: 非分数奖励，形状为 (`batch_size`, `response_length`) `torch.FloatTensor`: KL 惩罚，形状为 (`batch_size`, `response_length`)

从分数和 KL 惩罚计算每个令牌的奖励。

#### `create_model_card`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1386)

```py
( path: str model_name: Optional = 'TRL Model' )
```

参数

+   `path` (`str`) — 保存模型卡片的路径。

+   `model_name` (`str`, *可选*) — 模型的名称，默认为 `TRL Model`。

创建并保存 TRL 模型的模型卡片。

#### `gather_stats`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L897)

```py
( stats ) → export const metadata = 'undefined';dict[str, Any]
```

参数

+   `stats` (dict[str, Any]) —

+   `a` 一个包含要收集的统计信息的字典。统计信息应包含 torch 张量。

返回

`dict[str, Any]`

一个包含收集的张量的统计信息的字典。

从所有进程中收集统计信息。在分布式训练的情况下很有用。

#### `generate`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L431)

```py
( query_tensor: Union length_sampler: Callable = None batch_size: int = 4 return_prompt: bool = True generate_ref_response: bool = False **generation_kwargs ) → export const metadata = 'undefined';torch.LongTensor
```

参数

+   `query_tensor` (`torch.LongTensor`) — 一个形状为 (`seq_len`) 包含查询令牌的张量或一个形状为 (`seq_len`) 的张量列表。

+   `length_sampler` (`Callable`, *可选*) — 返回新生成令牌的数量的可调用函数。

+   `batch_size` (`int`, *可选*) — 用于生成的批量大小，默认为 `4`。

+   `return_prompt` (`bool`, *可选*) — 如果设置为 `False`，则不返回提示，而只返回新生成的令牌，默认为 `True`。

+   `generate_ref_response` (`bool`, *可选*) — 如果设置为 `True`，则还会生成参考响应，默认为 `False`。

+   `generation_kwargs` (dict[str, Any]) — 用于生成的关键字参数。

返回

`torch.LongTensor`

一个形状为 (`batch_size`, `gen_len`) 包含响应令牌的张量。

给定查询张量，使用模型生成响应。调用模型的 `generate` 方法。

#### `log_stats`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1313)

```py
( stats: dict batch: dict rewards: List columns_to_log: List = ['query', 'response'] )
```

参数

+   `stats` (dict[str, Any]) — 一个包含训练统计信息的字典。

+   `batch` (dict[str, Any]) — 一个包含查询和响应的批量数据的字典。

+   `rewards` (`List[torch.FloatTensor]`) — 一个奖励张量。

记录所有训练统计信息的函数。在每个时代结束时调用它。

#### `loss`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1160)

```py
( old_logprobs: FloatTensor values: FloatTensor logits: FloatTensor vpreds: FloatTensor logprobs: FloatTensor mask: LongTensor advantages: FloatTensor returns: FloatTensor )
```

参数

+   `old_logprobs` (`torch.FloatTensor`) — 模型的对数概率，形状为 (`batch_size`, `response_length`)

+   `values` (`torch.FloatTensor`) — 值头的值，形状为 (`batch_size`, `response_length`)

+   `rewards` (`torch.FloatTensor`) — 从奖励模型得到的奖励，形状为 (`batch_size`, `response_length`)

+   `logits` (`torch.FloatTensor`) — 模型的对数概率，形状为 (`batch_size`, `response_length`, `vocab_size`)

+   `v_pred` (`torch.FloatTensor`) — 值头的值，形状为 (`batch_size`, `response_length`)

+   `logprobs` (`torch.FloatTensor`) — 模型的对数概率，形状为 (`batch_size`, `response_length`)

计算策略和值的损失。

#### `prepare_dataloader`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L376)

```py
( dataset: Union data_collator = None ) → export const metadata = 'undefined';torch.utils.data.DataLoader
```

参数

+   `dataset` (Union[`torch.utils.data.Dataset`, `datasets.Dataset`]) — PyTorch 数据集或 Hugging Face 数据集。如果传递了 Hugging Face 数据集，则将通过删除模型未使用的列来预处理数据集。

+   `data_collator` (Optional[function]) — 数据整理函数。

返回

`torch.utils.data.DataLoader`

PyTorch 数据加载器

为训练准备数据加载器。

#### `record_step_stats`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1249)

```py
( kl_coef: float **data ) → export const metadata = 'undefined';stats (dict)
```

参数

+   `kl_coef` (`float`) — KL 系数

+   `data` (`dict`) — 训练步骤数据的字典

返回

stats (`dict`)

训练步骤统计信息的字典

记录训练步骤统计信息。

#### `step`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L617)

```py
( queries: List responses: List scores: List response_masks: Optional = None ) → export const metadata = 'undefined';dict[str, Any]
```

参数

+   `queries`（List`torch.LongTensor`）— 包含形状为（query_length）的编码查询的张量列表

+   `responses`（List`torch.LongTensor`）— 包含编码响应的张量列表，形状为（response_length）

+   `scores`（List`torch.FloatTensor`）— 包含分数的张量列表。

+   `response_masks`（List`torch.FloatTensor`，*可选*）— 包含响应标记掩码的张量列表。

返回

`dict[str, Any]`

训练统计摘要

给定查询列表、模型响应和奖励，运行 PPO 优化步骤。

#### `train_minibatch`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1032)

```py
( old_logprobs: FloatTensor values: FloatTensor logprobs: FloatTensor logits: FloatTensor vpreds: FloatTensor mask: LongTensor advantages: FloatTensor returns: FloatTensor ) → export const metadata = 'undefined';train_stats (dict[str, torch.Tensor])
```

参数

+   `logprobs`（`torch.FloatTensor`）— 模型的对数概率，形状[mini_batch_size，response_length]

+   `values`（`torch.FloatTensor`）— 值头的值，形状[mini_batch_size，response_length]

+   `query`（`torch.LongTensor`）— 编码的查询，形状[mini_batch_size，query_length]

+   `response`（`torch.LongTensor`）— 编码的响应，形状[mini_batch_size，response_length]

+   `model_input`（`torch.LongTensor`）— 连接的查询和响应，形状[mini_batch_size，query_length+response_length]

返回

train_stats（字典[str，`torch.Tensor`]）

训练统计字典

训练一个 PPO 小批量

### `class trl.PPOConfig`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_config.py#L34)

```py
( exp_name: str = 'doc-buil' seed: int = 0 log_with: Optional = None task_name: Optional = None model_name: Optional = None query_dataset: Optional = None reward_model: Optional = None remove_unused_columns: bool = True tracker_kwargs: Annotated = <factory> accelerator_kwargs: Annotated = <factory> project_kwargs: Annotated = <factory> tracker_project_name: str = 'trl' push_to_hub_if_best_kwargs: Annotated = <factory> steps: int = 20000 learning_rate: float = 1e-05 adap_kl_ctrl: bool = True init_kl_coef: Optional = 0.2 kl_penalty: Literal = 'kl' target: Optional = 6 horizon: Optional = 10000 gamma: float = 1 lam: float = 0.95 cliprange: float = 0.2 cliprange_value: float = 0.2 vf_coef: float = 0.1 batch_size: int = 256 forward_batch_size: Optional = None mini_batch_size: int = 1 gradient_accumulation_steps: int = 1 world_size: Annotated = None ppo_epochs: int = 4 max_grad_norm: Optional = None optimize_cuda_cache: Optional = None optimize_device_cache: Optional = False early_stopping: bool = False target_kl: float = 1 compare_steps: int = 1 ratio_threshold: float = 10.0 use_score_scaling: bool = False use_score_norm: bool = False score_clip: Optional = None whiten_rewards: bool = False is_encoder_decoder: Optional = None is_peft_model: Optional = None backward_batch_size: Annotated = None global_backward_batch_size: Annotated = None global_batch_size: Annotated = None )
```

PPOTrainer 的配置类
