# 训练器

> 原始文本: [`huggingface.co/docs/trl/trainer`](https://huggingface.co/docs/trl/trainer)

在 TRL 中，我们支持使用 PPO（Proximal Policy Optimization）进行优化语言模型，其实现在很大程度上遵循了 D. Ziegler 等人在论文“Fine-Tuning Language Models from Human Preferences”中介绍的结构。[[paper](https://arxiv.org/pdf/1909.08593.pdf), [code](https://github.com/openai/lm-human-preferences)]。Trainer 和 model 类在很大程度上受到`transformers.Trainer`和`transformers.AutoModel`类的启发，并为 RL 进行了调整。我们还支持一个`RewardTrainer`，可用于训练奖励模型。

## PPOConfig

### `class trl.PPOConfig`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_config.py#L34)

```py
( exp_name: str = 'doc-buil' seed: int = 0 log_with: Optional = None task_name: Optional = None model_name: Optional = None query_dataset: Optional = None reward_model: Optional = None remove_unused_columns: bool = True tracker_kwargs: Annotated = <factory> accelerator_kwargs: Annotated = <factory> project_kwargs: Annotated = <factory> tracker_project_name: str = 'trl' push_to_hub_if_best_kwargs: Annotated = <factory> steps: int = 20000 learning_rate: float = 1e-05 adap_kl_ctrl: bool = True init_kl_coef: Optional = 0.2 kl_penalty: Literal = 'kl' target: Optional = 6 horizon: Optional = 10000 gamma: float = 1 lam: float = 0.95 cliprange: float = 0.2 cliprange_value: float = 0.2 vf_coef: float = 0.1 batch_size: int = 256 forward_batch_size: Optional = None mini_batch_size: int = 1 gradient_accumulation_steps: int = 1 world_size: Annotated = None ppo_epochs: int = 4 max_grad_norm: Optional = None optimize_cuda_cache: Optional = None optimize_device_cache: Optional = False early_stopping: bool = False target_kl: float = 1 compare_steps: int = 1 ratio_threshold: float = 10.0 use_score_scaling: bool = False use_score_norm: bool = False score_clip: Optional = None whiten_rewards: bool = False is_encoder_decoder: Optional = None is_peft_model: Optional = None backward_batch_size: Annotated = None global_backward_batch_size: Annotated = None global_batch_size: Annotated = None )
```

PPOTrainer 的配置类

## PPOTrainer

### `class trl.PPOTrainer`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L109)

```py
( config: PPOConfig = None model: PreTrainedModelWrapper = None ref_model: Optional = None tokenizer: PreTrainedTokenizerBase = None dataset: Union = None optimizer: Optional = None data_collator: Optional = None num_shared_layers: Optional = None lr_scheduler: Optional = None )
```

参数

+   *`*config**` (`PPOConfig`) — PPOTrainer 的配置对象。查看`PPOConfig`的文档以获取更多细节。

+   *`*model**` (`PreTrainedModelWrapper`) — 要优化的模型，带有值头的 Hugging Face 变换器模型。查看`PreTrainedModelWrapper`的文档以获取更多细节。

+   *`*ref_model**` (`PreTrainedModelWrapper`, *可选*) — 用于 KL 惩罚的参考模型，Hugging Face 变换器模型带有一个非正式语言建模头。查看`PreTrainedModelWrapper`的文档以获取更多细节。如果没有提供参考模型，训练器将创建一个具有与要优化的模型相同架构的参考模型，共享层。

+   *`*tokenizer**` (`PreTrainedTokenizerBase`) — 用于编码数据的分词器。查看`transformers.PreTrainedTokenizer`和`transformers.PreTrainedTokenizerFast`的文档以获取更多细节。

+   *`*dataset**` (Union[`torch.utils.data.Dataset`, `datasets.Dataset`], *可选*) — PyTorch 数据集或 Hugging Face 数据集。这用于创建一个 PyTorch 数据加载器。如果没有提供数据集，数据加载器必须在训练器之外创建，用户需要设计自己的数据加载器，并确保使用的批量大小与配置对象中指定的相同。

+   *`*optimizer**` (`torch.optim.Optimizer`, *可选*) — 用于训练的优化器。如果没有提供优化器，训练器将使用配置对象中指定的学习率创建一个 Adam 优化器。

+   *`*data_collator**` (DataCollatorForLanguageModeling, *可选*) — 用于训练和传递给数据加载器的数据整合器

+   *`*num_shared_layers**` (int, *可选*) — 要在模型和参考模型之间共享的层数，如果没有传递参考模型。如果没有提供数字，所有层将被共享。

+   *`*lr_scheduler**` (`torch.optim.lr_scheduler`, *可选*) — 用于训练的学习率调度器

PPOTrainer 使用 Proximal Policy Optimization 来优化语言模型。请注意，这个训练器在很大程度上受到原始 OpenAI 学习总结工作的启发，链接在这里：[`github.com/openai/summarize-from-feedback`](https://github.com/openai/summarize-from-feedback)

#### `batched_forward_pass`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L941)

```py
( model: PreTrainedModelWrapper queries: Tensor responses: Tensor model_inputs: dict return_logits: bool = False response_masks: Optional = None ) → export const metadata = 'undefined';(tuple)
```

参数

+   `queries` (`torch.LongTensor`) — 包含编码查询的张量列表，形状为(`batch_size`, `query_length`)

+   `responses` (`torch.LongTensor`) — 包含编码响应的张量列表，形状为(`batch_size`, `response_length`)

+   `return_logits` (`bool`, *可选*, 默认为`False`) — 是否返回所有 logits。如果不需要 logits 以减少内存消耗，则设置为`False`。

返回

(tuple)

+   all_logprobs (`torch.FloatTensor`): 响应的对数概率，形状为(`batch_size`, `response_length`)

+   all_ref_logprobs (`torch.FloatTensor`): 参考响应的对数概率，形状为(`batch_size`, `response_length`)

+   all_values (`torch.FloatTensor`): 响应的值，形状为(`batch_size`, `response_length`)

在多个批次中计算模型输出。

#### `compute_rewards`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1078)

```py
( scores: FloatTensor logprobs: FloatTensor ref_logprobs: FloatTensor masks: LongTensor ) → export const metadata = 'undefined';torch.FloatTensor
```

参数

+   `scores` (`torch.FloatTensor`) — 来自奖励模型的分数，形状为(`batch_size`)

+   `logprobs` (`torch.FloatTensor`) — 模型的对数概率，形状为(`batch_size`, `response_length`)

+   `ref_logprobs` (`torch.FloatTensor`) — 参考模型的对数概率，形状为(`batch_size`, `response_length`)

返回

`torch.FloatTensor`

每个标记的奖励，形状为(`batch_size`, `response_length`) `torch.FloatTensor`: 非分数奖励，形状为(`batch_size`, `response_length`) `torch.FloatTensor`: KL 惩罚，形状为(`batch_size`, `response_length`)

从分数和 KL 惩罚计算每个标记的奖励。

#### `create_model_card`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1386)

```py
( path: str model_name: Optional = 'TRL Model' )
```

参数

+   `path` (`str`) — 保存模型卡的路径。

+   `model_name` (`str`, *optional*) — 模型的名称，默认为`TRL Model`。

创建并保存一个 TRL 模型的模型卡。

#### `gather_stats`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L897)

```py
( stats ) → export const metadata = 'undefined';dict[str, Any]
```

参数

+   `stats` (dict[str, Any]) —

+   一个要收集的统计信息字典。统计信息应包含 torch 张量。 —

返回

`dict[str, Any]`

一个包含收集到的张量的统计字典。

从所有进程中收集统计信息。在分布式训练的情况下很有用。

#### `generate`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L431)

```py
( query_tensor: Union length_sampler: Callable = None batch_size: int = 4 return_prompt: bool = True generate_ref_response: bool = False **generation_kwargs ) → export const metadata = 'undefined';torch.LongTensor
```

参数

+   `query_tensor` (`torch.LongTensor`) — 一个形状为(`seq_len`)的查询标记张量或一个形状为(`seq_len`)的张量列表。

+   `length_sampler` (`Callable`, *optional*) — 返回新生成的标记数的可调用函数。

+   `batch_size` (`int`, *optional) — 用于生成的批量大小，默认为`4`。

+   `return_prompt` (`bool`, *optional*) — 如果设置为`False`，则不返回提示，而只返回新生成的标记，默认为`True`。

+   `generate_ref_response` (`bool`, *optional*) — 如果设置为`True`，则还会生成参考响应，默认为`False`。

+   `generation_kwargs` (dict[str, Any]) — 用于生成的关键字参数。

返回

`torch.LongTensor`

一个形状为(`batch_size`, `gen_len`)的包含响应标记的张量。

使用给定查询张量的模型生成响应。调用模型的`generate`方法。

#### `log_stats`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1313)

```py
( stats: dict batch: dict rewards: List columns_to_log: List = ['query', 'response'] )
```

参数

+   `stats` (dict[str, Any]) — 一个包含训练统计信息的字典。

+   `batch` (dict[str, Any]) — 一个包含查询和响应的批量数据的字典。

+   `rewards` (`List[torch.FloatTensor]`) — 一组奖励的张量。

一个记录所有训练统计数据的函数。在每个时代结束时调用它。

#### `loss`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1160)

```py
( old_logprobs: FloatTensor values: FloatTensor logits: FloatTensor vpreds: FloatTensor logprobs: FloatTensor mask: LongTensor advantages: FloatTensor returns: FloatTensor )
```

参数

+   `old_logprobs` (`torch.FloatTensor`) — 模型的对数概率，形状为(`batch_size`, `response_length`)

+   `values` (`torch.FloatTensor`) — 值头的值，形状为(`batch_size`, `response_length`)

+   `rewards` (`torch.FloatTensor`) — 来自奖励模型的奖励，形状为(`batch_size`, `response_length`)

+   `logits` (`torch.FloatTensor`) — 模型的对数概率，形状为(`batch_size`, `response_length`, `vocab_size`)

+   `v_pred` (`torch.FloatTensor`) — 值头的值，形状为(`batch_size`, `response_length`)

+   `logprobs` (`torch.FloatTensor`) — 模型的对数概率，形状为(`batch_size`, `response_length`)

计算策略和值的损失。

#### `prepare_dataloader`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L376)

```py
( dataset: Union data_collator = None ) → export const metadata = 'undefined';torch.utils.data.DataLoader
```

参数

+   `dataset`（Union[`torch.utils.data.Dataset`，`datasets.Dataset`]）- PyTorch 数据集或 Hugging Face 数据集。如果传递了 Hugging Face 数据集，则将通过删除模型未使用的列来预处理数据集。

+   `data_collator`（可选[函数]）- 数据整理函数。

返回

`torch.utils.data.DataLoader`

PyTorch 数据加载器

准备数据加载器进行训练。

#### `record_step_stats`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1249)

```py
( kl_coef: float **data ) → export const metadata = 'undefined';stats (dict)
```

参数

+   `kl_coef`（`float`）- KL 系数

+   `data`（`dict`）- 训练步骤数据的字典

返回

stats（`dict`）

训练步骤统计字典

记录训练步骤统计。

#### `step`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L617)

```py
( queries: List responses: List scores: List response_masks: Optional = None ) → export const metadata = 'undefined';dict[str, Any]
```

参数

+   `queries`（List`torch.LongTensor`）- 包含编码查询的张量列表，形状（`query_length`）

+   `responses`（List`torch.LongTensor`）- 包含编码响应的张量列表，形状（`response_length`）

+   `scores`（List`torch.FloatTensor`）- 包含分数的张量列表。

+   `response_masks`（List`torch.FloatTensor`，*可选*）- 包含响应标记掩码的张量列表。

返回

`dict[str，Any]`

训练统计摘要

给定一组查询、模型响应和奖励，运行一个 PPO 优化步骤。

#### `train_minibatch`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1032)

```py
( old_logprobs: FloatTensor values: FloatTensor logprobs: FloatTensor logits: FloatTensor vpreds: FloatTensor mask: LongTensor advantages: FloatTensor returns: FloatTensor ) → export const metadata = 'undefined';train_stats (dict[str, torch.Tensor])
```

参数

+   `logprobs`（`torch.FloatTensor`）- 模型的对数概率，形状[mini_batch_size，response_length]

+   `values`（`torch.FloatTensor`）- 值头的值，形状[mini_batch_size，response_length]

+   `query`（`torch.LongTensor`）- 编码的查询，形状[mini_batch_size，query_length]

+   `response`（`torch.LongTensor`）- 编码的响应，形状[mini_batch_size，response_length]

+   `model_input`（`torch.LongTensor`）- 连接的查询和响应，形状[mini_batch_size，query_length+response_length]

返回

train_stats（dict[str，`torch.Tensor`]）

训练统计字典

训练一个 PPO 小批量

## RewardConfig

### `class trl.RewardConfig`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/reward_config.py#L21)

```py
( output_dir: str overwrite_output_dir: bool = False do_train: bool = False do_eval: bool = False do_predict: bool = False evaluation_strategy: Union = 'no' prediction_loss_only: bool = False per_device_train_batch_size: int = 8 per_device_eval_batch_size: int = 8 per_gpu_train_batch_size: Optional = None per_gpu_eval_batch_size: Optional = None gradient_accumulation_steps: int = 1 eval_accumulation_steps: Optional = None eval_delay: Optional = 0 learning_rate: float = 5e-05 weight_decay: float = 0.0 adam_beta1: float = 0.9 adam_beta2: float = 0.999 adam_epsilon: float = 1e-08 max_grad_norm: float = 1.0 num_train_epochs: float = 3.0 max_steps: int = -1 lr_scheduler_type: Union = 'linear' lr_scheduler_kwargs: Optional = <factory> warmup_ratio: float = 0.0 warmup_steps: int = 0 log_level: Optional = 'passive' log_level_replica: Optional = 'warning' log_on_each_node: bool = True logging_dir: Optional = None logging_strategy: Union = 'steps' logging_first_step: bool = False logging_steps: float = 500 logging_nan_inf_filter: bool = True save_strategy: Union = 'steps' save_steps: float = 500 save_total_limit: Optional = None save_safetensors: Optional = True save_on_each_node: bool = False save_only_model: bool = False no_cuda: bool = False use_cpu: bool = False use_mps_device: bool = False seed: int = 42 data_seed: Optional = None jit_mode_eval: bool = False use_ipex: bool = False bf16: bool = False fp16: bool = False fp16_opt_level: str = 'O1' half_precision_backend: str = 'auto' bf16_full_eval: bool = False fp16_full_eval: bool = False tf32: Optional = None local_rank: int = -1 ddp_backend: Optional = None tpu_num_cores: Optional = None tpu_metrics_debug: bool = False debug: Union = '' dataloader_drop_last: bool = False eval_steps: Optional = None dataloader_num_workers: int = 0 past_index: int = -1 run_name: Optional = None disable_tqdm: Optional = None remove_unused_columns: Optional = True label_names: Optional = None load_best_model_at_end: Optional = False metric_for_best_model: Optional = None greater_is_better: Optional = None ignore_data_skip: bool = False fsdp: Union = '' fsdp_min_num_params: int = 0 fsdp_config: Optional = None fsdp_transformer_layer_cls_to_wrap: Optional = None deepspeed: Optional = None label_smoothing_factor: float = 0.0 optim: Union = 'adamw_torch' optim_args: Optional = None adafactor: bool = False group_by_length: bool = False length_column_name: Optional = 'length' report_to: Optional = None ddp_find_unused_parameters: Optional = None ddp_bucket_cap_mb: Optional = None ddp_broadcast_buffers: Optional = None dataloader_pin_memory: bool = True dataloader_persistent_workers: bool = False skip_memory_metrics: bool = True use_legacy_prediction_loop: bool = False push_to_hub: bool = False resume_from_checkpoint: Optional = None hub_model_id: Optional = None hub_strategy: Union = 'every_save' hub_token: Optional = None hub_private_repo: bool = False hub_always_push: bool = False gradient_checkpointing: Optional = True gradient_checkpointing_kwargs: Optional = None include_inputs_for_metrics: bool = False fp16_backend: str = 'auto' push_to_hub_model_id: Optional = None push_to_hub_organization: Optional = None push_to_hub_token: Optional = None mp_parameters: str = '' auto_find_batch_size: bool = False full_determinism: bool = False torchdynamo: Optional = None ray_scope: Optional = 'last' ddp_timeout: Optional = 1800 torch_compile: bool = False torch_compile_backend: Optional = None torch_compile_mode: Optional = None dispatch_batches: Optional = None split_batches: Optional = False include_tokens_per_second: Optional = False include_num_input_tokens_seen: Optional = False neftune_noise_alpha: float = None max_length: Optional = None )
```

参数

+   `max_length`（`int`，*可选*，默认为`None`）- 批次中序列的最大长度。如果要使用默认数据整理器，则需要此参数。

+   `gradient_checkpointing`（`bool`，*可选*，默认为`True`）- 如果为 True，则使用梯度检查点来节省内存，但会导致反向传播变慢。

RewardConfig 收集与 RewardTrainer 类相关的所有训练参数。

使用`HfArgumentParser`，我们可以将这个类转换为[argparse](https://docs.python.org/3/library/argparse#module-argparse)参数，可以在命令行上指定。

## RewardTrainer

### `class trl.RewardTrainer`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/reward_trainer.py#L36)

```py
( model: Union = None args: Optional = None data_collator: Optional = None train_dataset: Optional = None eval_dataset: Union = None tokenizer: Optional = None model_init: Optional = None compute_metrics: Optional = None callbacks: Optional = None optimizers: Tuple = (None, None) preprocess_logits_for_metrics: Optional = None max_length: Optional = None peft_config: Optional = None )
```

RewardTrainer 可用于训练自定义奖励模型。它是`transformers.Trainer`类的子类，继承了所有属性和方法。建议使用`AutoModelForSequenceClassification`作为奖励模型。奖励模型应该在成对示例的数据集上进行训练，其中每个示例是两个序列的元组。奖励模型应该被训练以预测哪个示例对于当前任务更相关。

奖励训练器对数据集有非常特定的格式要求。数据集应该至少包含两个条目，如果不使用默认的`RewardDataCollatorWithPadding`数据整理器。这些条目应该被命名

+   `input_ids_chosen`

+   `attention_mask_chosen`

+   `input_ids_rejected`

+   `attention_mask_rejected`

可选地，您还可以向数据集传递一个`margin`条目。此条目应包含用于调节奖励模型损失的边界，如[`ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/`](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)中所述。如果不传递边界，将不使用边界。

## SFTTrainer

### `class trl.SFTTrainer`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/sft_trainer.py#L54)

```py
( model: Union = None args: TrainingArguments = None data_collator: Optional = None train_dataset: Optional = None eval_dataset: Union = None tokenizer: Optional = None model_init: Optional = None compute_metrics: Optional = None callbacks: Optional = None optimizers: Tuple = (None, None) preprocess_logits_for_metrics: Optional = None peft_config: Optional = None dataset_text_field: Optional = None packing: Optional = False formatting_func: Optional = None max_seq_length: Optional = None infinite: Optional = None num_of_sequences: Optional = 1024 chars_per_token: Optional = 3.6 dataset_num_proc: Optional = None dataset_batch_size: int = 1000 neftune_noise_alpha: Optional = None model_init_kwargs: Optional = None dataset_kwargs: Optional = None )
```

参数

+   `model`（Union[`transformers.PreTrainedModel`，`nn.Module`，`str`]）— 要训练的模型，可以是`PreTrainedModel`，`torch.nn.Module`或包含要从缓存加载或下载的模型名称的字符串。如果将`PeftConfig`对象传递给`peft_config`参数，则模型也可以转换为`PeftModel`。

+   `args`（Optional[transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.36.2/en/main_classes/trainer#transformers.TrainingArguments)）— 用于调整训练的参数。有关更多信息，请参阅`transformers.TrainingArguments`的官方文档。

+   `data_collator`（Optional`transformers.DataCollator`）— 用于训练的数据收集器。

+   `train_dataset`（Optional[datasets.Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)）— 用于训练的数据集。我们建议用户使用`trl.trainer.ConstantLengthDataset`来创建他们的数据集。

+   `eval_dataset`（Optional[Union[`datasets.Dataset`，Dict[`str`，`datasets.Dataset`]]）— 用于评估的数据集。我们建议用户使用`trl.trainer.ConstantLengthDataset`来创建他们的数据集。

+   `tokenizer`（Optional[transformers.PreTrainedTokenizer](https://huggingface.co/docs/transformers/v4.36.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)）— 用于训练的分词器。如果未指定，将使用与模型相关联的分词器。

+   `model_init`（`Callable[[], transformers.PreTrainedModel]`）— 用于训练的模型初始化器。如果指定为 None，则将使用默认的模型初始化器。

+   `compute_metrics`（`Callable[[transformers.EvalPrediction]，Dict]`，*可选*默认为 None）— 用于在评估期间计算指标的函数。它应返回一个将指标名称映射到指标值的字典。如果未指定，评估期间将仅计算损失。

+   `callbacks`（`List[transformers.TrainerCallback]`）— 用于训练的回调函数。

+   `optimizers`（`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`）— 用于训练的优化器和调度器。

+   `preprocess_logits_for_metrics`（`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`）— 用于在计算指标之前预处理 logits 的函数。

+   `peft_config`（`Optional[PeftConfig]`）— 用于初始化 PeftModel 的 PeftConfig 对象。

+   `dataset_text_field`（`Optional[str]`）— 数据集的文本字段的名称，如果用户传递了此参数，训练器将根据`dataset_text_field`参数自动创建一个`ConstantLengthDataset`。

+   `formatting_func`（`Optional[Callable]`）— 用于创建`ConstantLengthDataset`的格式化函数。

+   `max_seq_length`（`Optional[int]`）— 用于`ConstantLengthDataset`和自动创建数据集的最大序列长度。默认为`512`。

+   `infinite`（`Optional[bool]`）— 是否使用无限数据集。默认为`False`。

+   `num_of_sequences`（`Optional[int]`）— 用于`ConstantLengthDataset`的序列数。默认为`1024`。

+   `chars_per_token` (`Optional[float]`) — 用于`ConstantLengthDataset`的每个标记的字符数。默认为`3.6`。您可以在 stack-llama 示例中查看如何计算这个值：[`github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53`](https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53)。

+   `packing` (`Optional[bool]`) — 仅在传递了`dataset_text_field`时使用。这个参数由`ConstantLengthDataset`用于打包数据集的序列。 

+   `dataset_num_proc` (`Optional[int]`) — 用于标记数据的工作进程数。仅在`packing=False`时使用。默认为 None。

+   `dataset_batch_size` (`int`) — 每批要标记化的示例数。如果 batch_size <= 0 或 batch_size == None，则将整个数据集标记为单个批次。默认为 1000。

+   `neftune_noise_alpha` (`Optional[float]`) — 如果不是`None`，这将激活 NEFTune 噪声嵌入。已经证明这可以极大地提高指令微调的模型性能。查看原始论文：[`arxiv.org/abs/2310.05914`](https://arxiv.org/abs/2310.05914) 和原始代码：[`github.com/neelsjain/NEFTune`](https://github.com/neelsjain/NEFTune) model_init_kwargs — (`Optional[Dict]`, *可选*): 传递给实例化模型时的可选 kwargs 字典 dataset_kwargs — (`Optional[Dict]`, *可选*): 创建打包或非打包数据集时要传递的可选 kwargs 字典

监督微调训练器（SFT Trainer）的类定义。这个类是`transformers.Trainer`类的包装器，并继承了它的所有属性和方法。训练器负责在用户传递`PeftConfig`对象时正确初始化 PeftModel。

## DPOTrainer

### `class trl.DPOTrainer`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L64)

```py
( model: Union = None ref_model: Union = None beta: float = 0.1 label_smoothing: float = 0 loss_type: Literal = 'sigmoid' args: TrainingArguments = None data_collator: Optional = None label_pad_token_id: int = -100 padding_value: int = 0 truncation_mode: str = 'keep_end' train_dataset: Optional = None eval_dataset: Union = None tokenizer: Optional = None model_init: Optional = None callbacks: Optional = None optimizers: Tuple = (None, None) preprocess_logits_for_metrics: Optional = None max_length: Optional = None max_prompt_length: Optional = None max_target_length: Optional = None peft_config: Optional = None is_encoder_decoder: Optional = None disable_dropout: bool = True generate_during_eval: bool = False compute_metrics: Optional = None precompute_ref_log_probs: bool = False model_init_kwargs: Optional = None ref_model_init_kwargs: Optional = None model_adapter_name: str = None ref_adapter_name: str = None )
```

参数

+   `model` (`transformers.PreTrainedModel`) — 要训练的模型，最好是`AutoModelForSequenceClassification`。

+   `ref_model` (`PreTrainedModelWrapper`) — 带有非正式语言建模头的 Hugging Face 变换器模型。用于隐式奖励计算和损失。如果没有提供参考模型，训练器将创建一个与要优化的模型具有相同架构的参考模型。

+   `beta` (`float`, 默认为 0.1）— DPO 损失中的 beta 因子。较高的 beta 意味着与初始策略的偏差较小。对于 IPO 损失，beta 是论文中表示为 tau 的正则化参数。

+   `label_smoothing` (`float`, 默认为 0) — 来自[cDPO](https://ericmitchell.ai/cdpo.pdf)报告的鲁棒 DPO 标签平滑参数，应该在 0 和 0.5 之间。

+   `loss_type` (`str`, 默认为`"sigmoid"`) — 要使用的 DPO 损失类型。可以是`"sigmoid"`默认的 DPO 损失，`"hinge"`来自[SLiC](https://arxiv.org/abs/2305.10425)论文的损失，`"ipo"`来自[IPO](https://arxiv.org/abs/2310.12036)论文，或者`"kto"`来自 HALOs[报告](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf)。

+   `args` (`transformers.TrainingArguments`) — 用于训练的参数。

+   `data_collator` (`transformers.DataCollator`) — 用于训练的数据整理器。如果未指定 None，则将使用默认数据整理器（`DPODataCollatorWithPadding`），该整理器将序列填充到批次中序列的最大长度，给定一组成对序列的数据集。

+   `label_pad_token_id` (`int`, 默认为`-100`) — 标签填充标记 id。如果要使用默认数据整理器，则需要这个参数。

+   `padding_value` (`int`, 默认为`0`) — 如果与分词器的 pad_token_id 不同，则为填充值。

+   `truncation_mode` (`str`，默认为`keep_end`) — 要使用的截断模式，可以是`keep_end`或`keep_start`。如果要使用默认的数据收集器，则此参数是必需的。

+   `train_dataset` (`datasets.Dataset`) — 用于训练的数据集。

+   `eval_dataset` (`datasets.Dataset`) — 用于评估的数据集。

+   `tokenizer` (`transformers.PreTrainedTokenizerBase`) — 用于训练的分词器。如果要使用默认的数据收集器，则此参数是必需的。

+   `model_init` (`Callable[[], transformers.PreTrainedModel]`) — 用于训练的模型初始化器。如果指定为 None，则将使用默认的模型初始化器。

+   `callbacks` (`List[transformers.TrainerCallback]`) — 用于训练的回调函数。

+   `optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`) — 用于训练的优化器和调度器。

+   `preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`) — 用于在计算指标之前预处理 logits 的函数。

+   `max_length` (`int`，默认为`None`) — 批次中序列的最大长度。如果要使用默认的数据收集器，则此参数是必需的。

+   `max_prompt_length` (`int`，默认为`None`) — 提示的最大长度。如果要使用默认的数据收集器，则此参数是必需的。

+   `max_target_length` (`int`，默认为`None`) — 目标的最大长度。如果要使用默认的数据收集器并且您的模型是编码器-解码器，则此参数是必需的。

+   `peft_config` (`Dict`，默认为`None`) — 用于训练的 PEFT 配置。如果传递 PEFT 配置，则模型将包装在 PEFT 模型中。

+   `is_encoder_decoder` (`Optional[bool]`，*可选*，默认为`None`) — 如果没有提供模型，则需要知道 model_init 是否返回编码器-解码器。

+   `disable_dropout` (`bool`，默认为`True`) — 是否在`model`和`ref_model`中禁用 dropout。

+   `generate_during_eval` (`bool`，默认为`False`) — 是否在评估步骤中进行采样和记录生成结果。

+   `compute_metrics` (`Callable[[EvalPrediction], Dict]`，*可选*) — 用于计算指标的函数。必须接受`EvalPrediction`并返回一个字符串到指标值的字典。

+   `precompute_ref_log_probs` (`bool`，默认为`False`) — 预计算参考模型的对数概率和评估数据集的标志。如果要在没有参考模型的情况下进行训练并减少所需的总 GPU 内存，则这很有用。model_init_kwargs — (`Optional[Dict]`，*可选*): 传递给从字符串实例化模型时的可选 kwargs 字典 ref_model_init_kwargs — (`Optional[Dict]`，*可选*): 传递给从字符串实例化参考模型时的可选 kwargs 字典

+   `model_adapter_name` (`str`，默认为`None`) — 使用 LoRA 时训练目标 PEFT 适配器的名称。

+   `ref_adapter_name` (`str`，默认为`None`) — 使用 LoRA 时多个适配器的参考 PEFT 适配器的名称。

初始化 DPOTrainer。

#### `build_tokenized_answer`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L523)

```py
( prompt answer )
```

Llama tokenizer 确实满足`enc(a + b) = enc(a) + enc(b)`。它确保`enc(a + b) = enc(a) + enc(a + b)[len(enc(a)):]`。参考：[`github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257`](https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257)

#### `compute_reference_log_probs`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L731)

```py
( padded_batch: Dict )
```

计算 DPO 特定数据集的单个填充批次的参考模型的对数概率。

#### `concatenated_forward`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L936)

```py
( model: Module batch: Dict )
```

在给定的输入批次上运行给定的模型，将选择的和拒绝的输入连接在一起。

我们这样做是为了避免进行两次前向传递，因为对于 FSDP 来说更快。

#### `concatenated_inputs`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L755)

```py
( batch: Dict is_encoder_decoder: bool = False label_pad_token_id: int = -100 padding_value: int = 0 device: Optional = None )
```

将所选和拒绝的输入连接成单个张量。

#### `dpo_loss`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L817)

```py
( policy_chosen_logps: FloatTensor policy_rejected_logps: FloatTensor reference_chosen_logps: FloatTensor reference_rejected_logps: FloatTensor reference_free: bool = False ) → export const metadata = 'undefined';A tuple of three tensors
```

返回

三个张量的元组

（损失、所选奖励、拒绝奖励）。损失张量包含批次中每个示例的 DPO 损失。所选奖励和拒绝奖励张量分别包含所选和拒绝响应的奖励。

计算一批策略和参考模型对数概率的 DPO 损失。

#### `evaluation_loop`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1154)

```py
( dataloader: DataLoader description: str prediction_loss_only: Optional = None ignore_keys: Optional = None metric_key_prefix: str = 'eval' )
```

覆盖内置评估循环以存储每个批次的指标。预测/评估循环，由`Trainer.evaluate()`和`Trainer.predict()`共享。

适用于有或无标签的情况。

#### `get_batch_logps`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L898)

```py
( logits: FloatTensor labels: LongTensor average_log_prob: bool = False label_pad_token_id: int = -100 is_encoder_decoder: bool = False )
```

计算给定标签在给定对数下的对数概率。

#### `get_batch_loss_metrics`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L982)

```py
( model batch: Dict train_eval: Literal = 'train' )
```

计算给定输入批次的 DPO 损失和其他指标，用于训练或测试。

#### `get_batch_samples`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1064)

```py
( model batch: Dict )
```

为给定输入批次的模型和参考模型生成样本。

#### `get_eval_dataloader`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L471)

```py
( eval_dataset: Optional = None )
```

参数

+   `eval_dataset`（`torch.utils.data.Dataset`，*可选*）- 如果提供，将覆盖`self.eval_dataset`。如果是一个[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，不被`model.forward()`方法接受的列将被自动删除。它必须实现`__len__`。

返回评估`~torch.utils.data.DataLoader`。

继承自 transformers.src.transformers.trainer.get_eval_dataloader 以预计算`ref_log_probs`。

#### `get_train_dataloader`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L428)

```py
( )
```

返回训练`~torch.utils.data.DataLoader`。

继承自 transformers.src.transformers.trainer.get_train_dataloader 以预计算`ref_log_probs`。

#### `log`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1204)

```py
( logs: Dict )
```

参数

+   `logs`（`Dict[str, float]`）- 要记录的值。

在观察训练的各种对象上记录日志，包括存储的指标。

#### `null_ref_context`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L719)

```py
( )
```

用于处理空引用模型（即 peft 适配器操作）的上下文管理器。

#### `tokenize_row`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L573)

```py
( feature model: Union = None )
```

对来自 DPO 特定数据集的单行进行标记化。

在这个阶段，我们还没有转换为 PyTorch 张量；我们只是处理截断，以防提示+所选或提示+拒绝响应太长。首先我们截断提示；如果仍然太长，我们截断所选/拒绝。

我们还为所选/拒绝的响应创建标签，其长度等于提示和所选/拒绝响应的长度之和，对于提示标记使用 label_pad_token_id。

## DDPOConfig

### `class trl.DDPOConfig`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ddpo_config.py#L11)

```py
( exp_name: str = 'doc-buil' run_name: Optional = '' seed: int = 0 log_with: Optional = None tracker_kwargs: dict = <factory> accelerator_kwargs: dict = <factory> project_kwargs: dict = <factory> tracker_project_name: str = 'trl' logdir: str = 'logs' num_epochs: int = 100 save_freq: int = 1 num_checkpoint_limit: int = 5 mixed_precision: str = 'fp16' allow_tf32: bool = True resume_from: Optional = '' sample_num_steps: int = 50 sample_eta: float = 1.0 sample_guidance_scale: float = 5.0 sample_batch_size: int = 1 sample_num_batches_per_epoch: int = 2 train_batch_size: int = 1 train_use_8bit_adam: bool = False train_learning_rate: float = 0.0003 train_adam_beta1: float = 0.9 train_adam_beta2: float = 0.999 train_adam_weight_decay: float = 0.0001 train_adam_epsilon: float = 1e-08 train_gradient_accumulation_steps: int = 1 train_max_grad_norm: float = 1.0 train_num_inner_epochs: int = 1 train_cfg: bool = True train_adv_clip_max: float = 5 train_clip_range: float = 0.0001 train_timestep_fraction: float = 1.0 per_prompt_stat_tracking: bool = False per_prompt_stat_tracking_buffer_size: int = 16 per_prompt_stat_tracking_min_count: int = 16 async_reward_computation: bool = False max_workers: int = 2 negative_prompts: Optional = '' )
```

DDPOTrainer 的配置类

## DDPOTrainer

### `class trl.DDPOTrainer`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ddpo_trainer.py#L55)

```py
( config: DDPOConfig reward_function: Callable prompt_function: Callable sd_pipeline: DDPOStableDiffusionPipeline image_samples_hook: Optional = None )
```

参数

+   *`*config**`（`DDPOConfig`）- DDPOTrainer 的配置对象。查看`PPOConfig`的文档以获取更多详细信息。

+   *`*reward_function**`（Callable[[torch.Tensor，Tuple[str]，Tuple[Any]]，torch.Tensor]）- 要使用的奖励函数

+   *`*prompt_function**`（Callable[[], Tuple[str，Any]）- 生成提示以指导模型的函数

+   *`*sd_pipeline**`（`DDPOStableDiffusionPipeline`）- 用于训练的稳定扩散管道。

+   *`*image_samples_hook**`（可选[Callable[[Any，Any，Any]，Any]）- 要调用以记录图像的挂钩

DDPOTrainer 使用深度扩散策略优化来优化扩散模型。请注意，此训练器受到这里的工作的启发：[`github.com/kvablack/ddpo-pytorch`](https://github.com/kvablack/ddpo-pytorch) 目前仅支持基于稳定扩散的管道

#### `calculate_loss`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ddpo_trainer.py#L340)

```py
( latents timesteps next_latents log_probs advantages embeds )
```

参数

+   `latents`（torch.Tensor）- 从扩散模型中采样的潜变量，形状：[batch_size，num_channels_latents，height，width]

+   `timesteps`（torch.Tensor）- 从扩散模型中采样的时间步长，形状：[batch_size]

+   `next_latents`（torch.Tensor）- 从扩散模型中采样的下一个潜变量，形状：[batch_size，num_channels_latents，height，width]

+   `log_probs`（torch.Tensor）- 潜变量的对数概率，形状：[batch_size]

+   `advantages`（torch.Tensor）- 潜变量的优势，形状：[batch_size]

+   `embeds`（torch.Tensor）- 提示的嵌入，形状：[2*batch_size 或 batch_size，…] 注意：因为如果 train_cfg 为 True，则期望将负提示连接到 embeds。

计算解包样本批次的损失

#### `create_model_card`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ddpo_trainer.py#L606)

```py
( path: str model_name: Optional = 'TRL DDPO Model' )
```

参数

+   `path`（`str`）- 要保存模型卡的路径。

+   `model_name`（`str`，*可选*）- 模型的名称，默认为`TRL DDPO Model`。

为 TRL 模型创建并保存模型卡。

#### `step`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ddpo_trainer.py#L234)

```py
( epoch: int global_step: int ) → export const metadata = 'undefined';global_step (int)
```

参数

+   `epoch`（int）- 当前时期。

+   `global_step`（int）- 当前全局步骤。

返回

global_step（int）

更新后的全局步骤。

执行一步训练。

副作用：

+   模型权重已更新

+   将统计信息记录到加速器跟踪器中。

+   如果`self.image_samples_callback`不为 None，则将使用 prompt_image_pairs、global_step 和加速器跟踪器调用它。

#### `train`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ddpo_trainer.py#L596)

```py
( epochs: Optional = None )
```

为给定的时期训练模型

## IterativeSFTTrainer

### `class trl.IterativeSFTTrainer`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/iterative_sft_trainer.py#L39)

```py
( model: PreTrainedModel = None args: TrainingArguments = None tokenizer: PreTrainedTokenizerBase = None optimizers: Tuple = (None, None) data_collator: Optional = None eval_dataset: Union = None max_length: Optional = None truncation_mode: Optional = 'keep_end' preprocess_logits_for_metrics: Optional = None compute_metrics: Optional = None optimize_device_cache: Optional = False )
```

参数

+   *`*model**`（`PreTrainedModel`）- 要优化的模型，可以是'AutoModelForCausalLM'或'AutoModelForSeq2SeqLM'。查看`PreTrainedModel`的文档以获取更多详细信息。

+   *`*args**`（`transformers.TrainingArguments`）- 用于训练的参数。

+   *`*tokenizer**`（`PreTrainedTokenizerBase`）- 用于编码数据的分词器。查看`transformers.PreTrainedTokenizer`和`transformers.PreTrainedTokenizerFast`的文档以获取更多详细信息。

+   *`*optimizers**`（`Tuple[torch.optim.Optimizer，torch.optim.lr_scheduler.LambdaLR]`）- 用于训练的优化器和调度程序。

+   *`*data_collator**`（Union[DataCollatorForLanguageModeling，DataCollatorForSeq2Seq]，*可选*）- 用于训练和传递给数据加载器的数据整理器。

+   *`*eval_dataset**`（`datasets.Dataset`）- 用于评估的数据集。

+   *`*max_length**`（`int`，默认为`None`）- 输入的最大长度。

+   *`*truncation_mode**`（`str`，默认为`keep_end`）- 要使用的截断模式，可以是`keep_end`或`keep_start`。

+   *`*preprocess_logits_for_metrics**`（`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`）- 用于在计算指标之前预处理 logits 的函数。

+   *`*compute_metrics**`（`Callable[[EvalPrediction], Dict]`，*可选*）- 用于计算指标的函数。必须接受`EvalPrediction`并返回一个字符串字典以表示指标值。

+   *`*optimize_device_cache` * *（`bool`，*可选*，默认为`False`）- 优化 CUDA 缓存以实现略微更节省内存的训练。

IterativeSFTTrainer 可用于微调需要在优化之间执行一些步骤的方法的模型。

#### `step`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/iterative_sft_trainer.py#L229)

```py
( input_ids: Optional = None attention_mask: Optional = None labels: Optional = None texts: Optional = None texts_labels: Optional = None ) → export const metadata = 'undefined';dict[str, Any]
```

参数

+   `input_ids`（List`torch.LongTensor`）- 包含 input_ids 的张量列表（如果未提供，将使用文本）

+   `attention_mask`（List`torch.LongTensor`，*可选*）- 包含注意力掩码的张量列表

+   `labels`（List`torch.FloatTensor`，*可选*）- 包含标签的张量列表（如果设置为 None，将默认为 input_ids）

+   `texts`（List`str`，*可选*）- 包含文本输入的字符串列表（如果未提供，将直接使用 input_ids）

+   `texts_labels`（List`str`，*可选*）- 包含文本标签的字符串列表（如果设置为 None，将默认为文本）

返回

`dict[str, Any]`

训练统计摘要

给定 input_ids、attention_mask 和 labels 列表或文本和 text_labels 列表，运行优化步骤。

## set_seed

#### `trl.set_seed`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/core.py#L209)

```py
( seed: int )
```

参数

+   `seed`（`int`）- 要设置的种子。

用于设置`random`，`numpy`和`torch`种子以获得可重现行为的辅助函数。
