# DPO 训练师

> 原始文本：[`huggingface.co/docs/trl/dpo_trainer`](https://huggingface.co/docs/trl/dpo_trainer)

TRL 支持 DPO 训练师，用于从偏好数据中训练语言模型，如 Rafailov 等人在 2023 年的论文[直接偏好优化：您的语言模型暗中是一个奖励模型](https://arxiv.org/abs/2305.18290)中所述。有关完整示例，请查看[`examples/scripts/dpo.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py)。

首先始终是训练您的 SFT 模型，以确保我们训练的数据对于 DPO 算法是分布内的。

## 期望的数据集格式

DPO 训练师期望数据集具有非常特定的格式。由于模型将被训练直接优化哪个句子在给定两个句子的情况下最相关的偏好，我们在下面提供了一个来自[`Anthropic/hh-rlhf`](https://huggingface.co/datasets/Anthropic/hh-rlhf)数据集的示例：

![](img/108b7079a2ffc651b11289080d0cdc7d.png)

因此，如果使用默认的`DPODataCollatorWithPadding`数据整理器，则最终数据集对象应包含这 3 个条目。这些条目应命名为：

+   `prompt`

+   `chosen`

+   `rejected`

例如：

```py
dpo_dataset_dict = {
    "prompt": [
        "hello",
        "how are you",
        "What is your name?",
        "What is your name?",
        "Which is the best programming language?",
        "Which is the best programming language?",
        "Which is the best programming language?",
    ],
    "chosen": [
        "hi nice to meet you",
        "I am fine",
        "My name is Mary",
        "My name is Mary",
        "Python",
        "Python",
        "Java",
    ],
    "rejected": [
        "leave me alone",
        "I am not fine",
        "Whats it to you?",
        "I dont have a name",
        "Javascript",
        "C++",
        "C++",
    ],
}
```

其中`prompt`包含上下文输入，`chosen`包含相应的选择响应，`rejected`包含相应的负面（被拒绝的）响应。可以看到一个提示可以有多个响应，这在字典值数组中的条目重复中得到体现。

## 期望的模型格式

DPO 训练师期望一个`AutoModelForCausalLM`模型，而 PPO 则期望`AutoModelForCausalLMWithValueHead`用于值函数。

## 使用 DPOTrainer

有关详细示例，请查看`examples/scripts/dpo.py`脚本。在高层次上，我们需要使用要训练的`model`初始化`DPOTrainer`，使用`ref_model`来计算首选和被拒绝响应的隐式奖励，`beta`是隐式奖励的超参数，数据集包含上述 3 个条目。请注意，`model`和`ref_model`需要具有相同的架构（即仅解码器或编码器-解码器）。

```py
 dpo_trainer = DPOTrainer(
    model,
    model_ref,
    args=training_args,
    beta=0.1,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
)
```

之后可以调用：

```py
dpo_trainer.train()
```

请注意，`beta`是 DPO 损失的温度参数，通常在`0.1`到`0.5`的范围内。我们忽略参考模型，因为`beta` -> 0。

## 损失函数

根据偏好数据，我们可以根据 Bradley-Terry 模型拟合二元分类器，事实上，DPO 作者提出使用通过`logsigmoid`对归一化似然进行 sigmoid 损失拟合逻辑回归。

[RSO](https://arxiv.org/abs/2309.06657)的作者建议使用来自[SLiC](https://arxiv.org/abs/2305.10425)论文的归一化似然的铰链损失。`DPOTrainer`可以通过`loss_type="hinge"`参数切换到此损失，此时`beta`是边际的倒数。

[IPO](https://arxiv.org/abs/2310.12036)的作者提供了对 DPO 算法的更深入的理论理解，并确定了过拟合的问题，并提出了一种替代损失，可以通过`loss_type="ipo"`参数传递给训练师。

[cDPO](https://ericmitchell.ai/cdpo.pdf)是对 DPO 损失的调整，其中我们假设偏好标签存在一定概率的噪声，可以通过`label_smoothing`参数（介于 0 和 0.5 之间）传递给`DPOTrainer`，然后使用保守的 DPO 损失。使用`loss_type="cdpo"`参数来使用它。

[KTO](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf)损失被推导出来，直接最大化 LLM 生成的效用，而不是偏好的对数似然。因此，数据集不一定是偏好，而是理想与不理想的完成。对于`DPOTrainer`所需的成对偏好数据，请使用`loss_type="kto_pair"`参数来利用这种损失，而对于期望和不期望数据的更一般情况，请使用尚未实现的`KTOTrainer`。

## 日志记录

在训练和评估过程中，我们记录以下奖励指标：

+   `rewards/chosen`：策略模型和参考模型对于所选响应的对数概率之间的平均差异，按 beta 缩放

+   `rewards/rejected`：策略模型和参考模型对于被拒绝响应的对数概率之间的平均差异，按 beta 缩放

+   `rewards/accuracies`：所选奖励比相应被拒绝奖励高的平均值

+   `rewards/margins`：所选奖励和相应被拒绝奖励之间的平均差异

## 使用 unsloth 加速 DPO 微调

您还可以使用与`SFTTrainer`完全兼容的[`unsloth`](https://github.com/unslothai/unsloth)库来进一步加速 QLoRA / LoRA（速度提高 2 倍，内存减少 60%）。目前，`unsloth`仅支持 Llama（Yi，TinyLlama，Qwen，Deepseek 等）和 Mistral 架构。以下是 DPO 的一些基准测试：

| GPU | Model | Dataset | 🤗 | 🤗 + Flash Attention 2 | 🦥 Unsloth | 🦥 VRAM saved |
| --- | --- | --- | --- | --- | --- | --- |
| A100 40G | Zephyr 7b | Ultra Chat | 1x | 1.24x | **1.88x** | -11.6% |
| Tesla T4 | Zephyr 7b | Ultra Chat | 1x | 1.09x | **1.55x** | -18.6% |

根据[官方文档](https://github.com/unslothai/unsloth)安装`unsloth`。安装完成后，您可以以非常简单的方式将 unsloth 整合到您的工作流程中；您只需要加载`FastLanguageModel`，而不是加载`AutoModelForCausalLM`，如下所示：

```py
import torch
from transformers import TrainingArguments
from trl import DPOTrainer
from unsloth import FastLanguageModel

max_seq_length = 2048 # Supports automatic RoPE Scaling, so choose any number.

# Load model
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/zephyr-sft",
    max_seq_length = max_seq_length,
    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
    load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False.
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Dropout = 0 is currently optimized
    bias = "none",    # Bias = "none" is currently optimized
    use_gradient_checkpointing = True,
    random_state = 3407,
)

args = TrainingArguments(output_dir="./output")

dpo_trainer = DPOTrainer(
    model,
    model_ref=None,
    args=training_args,
    beta=0.1,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
)
dpo_trainer.train()
```

保存的模型与 Hugging Face 的 transformers 库完全兼容。在他们的[官方存储库](https://github.com/unslothai/unsloth)中了解更多关于 unsloth 的信息。

## 参考模型在 PEFT 中的考虑

在使用 PEFT 时，您有三个主要选项（以及几个变体）来确定参考模型的工作方式，假设您希望使用 DPO 进一步增强的模型是使用（Q）LoRA 进行调整的。

1.  简单地创建两个模型实例，每个加载您的适配器 - 运行良好但效率很低。

1.  将适配器合并到基础模型中，顶部创建另一个适配器，然后将`model_ref`参数保留为空，这样 DPOTrainer 将卸载用于参考推理的适配器 - 高效，但存在下面讨论的潜在缺点。

1.  两次加载适配器，使用不同的名称，然后在训练过程中使用`set_adapter`来在 DPO 适配器和参考适配器之间进行切换 - 与 2 相比稍微不那么高效（适配器大小 VRAM 开销），但避免了缺点。

### 在 DPO 之前合并 QLoRA 的缺点（方法 2）

如[Tim Dettmers](https://twitter.com/Tim_Dettmers/status/1694654191325573456)建议的，合并 QLoRA 适配器的最佳方法是首先对基础模型进行量化，然后合并适配器，然后转换回 bf16。类似于[此脚本](https://github.com/jondurbin/qlora/blob/main/qmerge.py)

您也可以只是以标准方式合并适配器，而不对基础模型进行量化，但这样会导致性能降低 1-2%（显然，还会出现更多空响应的问题）。

如果您使用推荐的方法，即对模型进行量化，那么现在您处于这样一种情况：要使用 QLoRA 进行 DPO，您将需要再次对合并的模型进行量化，或者使用性能较低的未量化合并。

### 使用选项 3 - 两次加载适配器

为了避免选项 2 的缺点，可以将微调的适配器加载到模型中两次，使用不同的名称，并在 DPOTrainer 中设置模型/参考适配器的名称，以略微增加 VRAM。

例如：

```py
# Load the base model.
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/mixtral-8x7b-v0.1",
    load_in_4bit=True,
    quantization_config=bnb_config,
    attn_implementation="flash_attention_2",
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
model.config.use_cache = False

# Load the adapter.
model = PeftModel.from_pretrained(
    model,
    "/path/to/peft",
    is_trainable=True,
    adapter_name="train",
)
# Load the adapter a second time, with a different name, which will be our reference model.
model.load_adapter("/path/to/peft", adapter_name="reference")

# Initialize the trainer, without a ref_model param.
dpo_trainer = DPOTrainer(
    model,
    ...
    model_adapter_name="train",
    ref_adapter_name="reference",
)
```

## DPOTrainer

### `class trl.DPOTrainer`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L64)

```py
( model: Union = None ref_model: Union = None beta: float = 0.1 label_smoothing: float = 0 loss_type: Literal = 'sigmoid' args: TrainingArguments = None data_collator: Optional = None label_pad_token_id: int = -100 padding_value: int = 0 truncation_mode: str = 'keep_end' train_dataset: Optional = None eval_dataset: Union = None tokenizer: Optional = None model_init: Optional = None callbacks: Optional = None optimizers: Tuple = (None, None) preprocess_logits_for_metrics: Optional = None max_length: Optional = None max_prompt_length: Optional = None max_target_length: Optional = None peft_config: Optional = None is_encoder_decoder: Optional = None disable_dropout: bool = True generate_during_eval: bool = False compute_metrics: Optional = None precompute_ref_log_probs: bool = False model_init_kwargs: Optional = None ref_model_init_kwargs: Optional = None model_adapter_name: str = None ref_adapter_name: str = None )
```

参数

+   `model` (`transformers.PreTrainedModel`) — 要训练的模型，最好是 `AutoModelForSequenceClassification`。

+   `ref_model` (`PreTrainedModelWrapper`) — Hugging Face 转换器模型，带有一个随意语言建模头。用于隐式奖励计算和损失。如果没有提供参考模型，训练器将创建一个与要优化的模型具有相同架构的参考模型。

+   `beta` (`float`，默认为 0.1) — DPO 损失中的 beta 因子。较高的 beta 意味着与初始策略的发散较小。对于 IPO 损失，beta 是论文中表示的正则化参数 tau。

+   `label_smoothing` (`float`，默认为 0) — 来自 [cDPO](https://ericmitchell.ai/cdpo.pdf) 报告的鲁棒 DPO 标签平滑参数，应该在 0 和 0.5 之间。

+   `loss_type` (`str`，默认为 `"sigmoid"`) — 要使用的 DPO 损失类型。可以是 `"sigmoid"` 默认的 DPO 损失，来自 [SLiC](https://arxiv.org/abs/2305.10425) 论文的 `"hinge"` 损失，来自 [IPO](https://arxiv.org/abs/2310.12036) 论文的 `"ipo"`，或来自 HALOs [报告](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf) 的 `"kto"`。

+   `args` (`transformers.TrainingArguments`) — 用于训练的参数。

+   `data_collator` (`transformers.DataCollator`) — 用于训练的数据整理器。如果未指定为 None，则将使用默认数据整理器（`DPODataCollatorWithPadding`），该整理器将将序列填充到批次中序列的最大长度，给定一组成对序列的数据集。

+   `label_pad_token_id` (`int`，默认为 `-100`) — 标签填充标记 id。如果要使用默认数据整理器，则需要此参数。

+   `padding_value` (`int`，默认为 `0`) — 如果与标记器的 pad_token_id 不同，则为填充值。

+   `truncation_mode` (`str`，默认为 `keep_end`) — 要使用的截断模式，可以是 `keep_end` 或 `keep_start`。如果要使用默认数据整理器，则需要此参数。

+   `train_dataset` (`datasets.Dataset`) — 用于训练的数据集。

+   `eval_dataset` (`datasets.Dataset`) — 用于评估的数据集。

+   `tokenizer` (`transformers.PreTrainedTokenizerBase`) — 用于训练的标记器。如果要使用默认数据整理器，则需要此参数。

+   `model_init` (`Callable[[], transformers.PreTrainedModel]`) — 用于训练的模型初始化器。如果未指定为 None，则将使用默认模型初始化器。

+   `callbacks` (`List[transformers.TrainerCallback]`) — 用于训练的回调函数。

+   `optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`) — 用于训练的优化器和调度器。

+   `preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`) — 在计算指标之前用于预处理对数的函数。

+   `max_length` (`int`，默认为 `None`) — 批次中序列的最大长度。如果要使用默认数据整理器，则需要此参数。

+   `max_prompt_length` (`int`，默认为 `None`) — 提示的最大长度。如果要使用默认数据整理器，则需要此参数。

+   `max_target_length` (`int`，默认为 `None`) — 目标的最大长度。如果要使用默认数据整理器并且您的模型是编码器-解码器，则需要此参数。

+   `peft_config` (`Dict`，默认为 `None`) — 用于训练的 PEFT 配置。如果传递 PEFT 配置，模型将被包装在 PEFT 模型中。

+   `is_encoder_decoder` (`Optional[bool]`, `可选`, 默认为 `None`) — 如果没有提供模型，我们需要知道模型初始化是否返回编码器-解码器。

+   `disable_dropout` (`bool`, 默认为 `True`) — 是否在 `model` 和 `ref_model` 中禁用 dropout。

+   `generate_during_eval` (`bool`, 默认为 `False`) — 是否在评估步骤中对生成进行采样和记录。

+   `compute_metrics` (`Callable[[EvalPrediction], Dict]`, *可选*) — 用于计算指标的函数。必须接受一个 `EvalPrediction` 并返回一个字符串到指标值的字典。

+   `precompute_ref_log_probs` (`bool`, 默认为 `False`) — 用于预计算参考模型对数概率和评估数据集的标志。如果要在没有参考模型的情况下训练并减少所需的总 GPU 内存，则这很有用。model_init_kwargs — (`Optional[Dict]`, *可选*): 传递给从字符串实例化模型时的可选 kwargs 字典 ref_model_init_kwargs — (`Optional[Dict]`, *可选*): 传递给从字符串实例化参考模型时的可选 kwargs 字典

+   `model_adapter_name` (`str`, 默认为 `None`) — 使用 LoRA 时的训练目标 PEFT 适配器的名称，当有多个适配器时。

+   `ref_adapter_name` (`str`, 默认为 `None`) — 使用 LoRA 时参考 PEFT 适配器的名称，当有多个适配器时。

初始化 DPOTrainer。

#### `build_tokenized_answer`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L523)

```py
( prompt answer )
```

Llama 分词器确实满足 `enc(a + b) = enc(a) + enc(b)`。它确保 `enc(a + b) = enc(a) + enc(a + b)[len(enc(a)):]`。参考：[`github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257`](https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257)

#### `compute_reference_log_probs`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L731)

```py
( padded_batch: Dict )
```

计算 DPO 特定数据集的单个填充批次的参考模型的对数概率。

#### `concatenated_forward`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L936)

```py
( model: Module batch: Dict )
```

在给定的输入批次上运行给定模型，将所选和被拒绝的输入连接在一起。

我们这样做是为了避免进行两次前向传递，因为对于 FSDP 来说更快。

#### `concatenated_inputs`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L755)

```py
( batch: Dict is_encoder_decoder: bool = False label_pad_token_id: int = -100 padding_value: int = 0 device: Optional = None )
```

将所选和被拒绝的输入连接成一个张量。

#### `dpo_loss`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L817)

```py
( policy_chosen_logps: FloatTensor policy_rejected_logps: FloatTensor reference_chosen_logps: FloatTensor reference_rejected_logps: FloatTensor reference_free: bool = False ) → export const metadata = 'undefined';A tuple of three tensors
```

返回

一个包含三个张量的元组

(损失，所选奖励，被拒绝奖励)。损失张量包含批次中每个示例的 DPO 损失。所选奖励和被拒绝奖励张量分别包含所选和被拒绝响应的奖励。

为一批策略和参考模型对数概率计算 DPO 损失。

#### `evaluation_loop`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1154)

```py
( dataloader: DataLoader description: str prediction_loss_only: Optional = None ignore_keys: Optional = None metric_key_prefix: str = 'eval' )
```

覆盖内置评估循环以存储每个批次的指标。预测/评估循环，由 `Trainer.evaluate()` 和 `Trainer.predict()` 共享。

无论是否有标签都可以使用。

#### `get_batch_logps`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L898)

```py
( logits: FloatTensor labels: LongTensor average_log_prob: bool = False label_pad_token_id: int = -100 is_encoder_decoder: bool = False )
```

计算给定标签在给定对数下的对数概率。

#### `get_batch_loss_metrics`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L982)

```py
( model batch: Dict train_eval: Literal = 'train' )
```

为给定的输入批次计算 DPO 损失和其他指标，用于训练或测试。

#### `get_batch_samples`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1064)

```py
( model batch: Dict )
```

为给定的输入批次从模型和参考模型生成样本。

#### `get_eval_dataloader`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L471)

```py
( eval_dataset: Optional = None )
```

参数

+   `eval_dataset`（`torch.utils.data.Dataset`，*可选*）- 如果提供，将覆盖`self.eval_dataset`。如果它是一个[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，则不被`model.forward()`方法接受的列将被自动删除。它必须实现`__len__`。

返回评估`~torch.utils.data.DataLoader`。

transformers.src.transformers.trainer.get_eval_dataloader 的子类，用于预计算`ref_log_probs`。

#### `get_train_dataloader`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L428)

```py
( )
```

返回训练`~torch.utils.data.DataLoader`。

transformers.src.transformers.trainer.get_train_dataloader 的子类，用于预计算`ref_log_probs`。

#### `log`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1204)

```py
( logs: Dict )
```

参数

+   `logs`（`Dict[str, float]`）- 要记录的值。

在观察训练的各种对象上记录`logs`，包括存储的指标。

#### `null_ref_context`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L719)

```py
( )
```

用于处理空引用模型（即 peft 适配器操作）的上下文管理器。

#### `tokenize_row`

[<来源>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L573)

```py
( feature model: Union = None )
```

对来自 DPO 特定数据集的单行进行标记化。

在这个阶段，我们还没有将其转换为 PyTorch 张量；我们只是处理截断，以防提示+所选或提示+拒绝响应太长。首先截断提示；如果仍然太长，我们截断所选/拒绝。

我们还为所选/拒绝的响应创建标签，其长度等于提示和所选/拒绝响应的长度之和，对于提示标记使用 label_pad_token_id。
