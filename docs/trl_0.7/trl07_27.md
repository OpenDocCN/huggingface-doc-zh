# 多适配器强化学习（MARL）- 一个用于所有的单一基础模型

> 原始文本：[`huggingface.co/docs/trl/multi_adapter_rl`](https://huggingface.co/docs/trl/multi_adapter_rl)

这里我们介绍了一种方法，该方法使用单个基础模型来执行整个 PPO 算法 - 包括检索参考 logits、计算活跃 logits 和奖励。这个功能是实验性的，因为我们没有测试这种方法的收敛性。我们鼓励社区告诉我们是否可能遇到任何问题。

## 要求

您只需要安装`peft`，如果您想要更加内存高效的微调，还可以选择安装`bitsandbytes`。

## 摘要

您需要按照以下三个阶段来处理这种方法：

1- 在目标领域（例如`imdb`数据集）上训练一个基础模型 - 这是监督微调阶段 - 可以利用 TRL 中的`SFTTrainer`。2- 使用`peft`训练一个奖励模型。这是为了在 RL 优化过程中重复使用适配器所必需的（下面的第 3 步）。我们在[此示例](https://github.com/huggingface/trl/tree/main/examples/scripts/reward_modeling.py)中展示了如何利用 TRL 中的`RewardTrainer`。3- 使用 PPO 和奖励适配器在基础模型上微调新的适配器。（“0 抽象 RL”）

确保在阶段 2 和 3 中使用相同的模型（即相同的架构和相同的权重）。

## 快速开始

假设您已经使用`RewardTrainer`在`llama-7b`模型上训练了您的奖励适配器，并将权重推送到了 hub 下的`trl-lib/llama-7b-hh-rm-adapter`。在进行 PPO 之前，将您的模型创建如下：

```py
model_name = "huggyllama/llama-7b"
rm_adapter_id = "trl-lib/llama-7b-hh-rm-adapter"

# PPO adapter
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = AutoModelForCausalLMWithValueHead.from_pretrained(
    model_name,
    peft_config=lora_config,
    reward_adapter=rm_adapter_id,
)

...
trainer = PPOTrainer(
    model=model,
    ...
)

...
```

然后在您的 PPO 训练循环中，通过访问`PPOTrainer`的`model`属性调用`compute_reward_score`方法。

```py
rewards = trainer.model.compute_reward_score(**inputs)
```

## 高级用法

### 对适配器名称的控制

如果您熟悉`peft`库，您就知道可以在同一个模型中使用多个适配器。您可以在同一个基础模型上训练多个适配器，以便在不同策略上进行微调。在这种情况下，您希望在检索奖励后激活要激活的适配器名称。为此，只需在调用`compute_reward_score`时将适当的`adapter_name`传递给`ppo_adapter_name`参数。

```py
adapter_name_policy_1 = "policy_1"
rewards = trainer.model.compute_reward_score(**inputs, ppo_adapter_name=adapter_name_policy_1)
...
```

### 使用 4 位和 8 位基础模型

为了更加内存高效的微调，您可以将基础模型加载为 8 位或 4 位，同时保持适配器的默认精度（float32）。只需将适当的参数（即`load_in_8bit=True`或`load_in_4bit=True`）传递给`AutoModelForCausalLMWithValueHead.from_pretrained`，如下所示（假设您已安装`bitsandbytes`）：

```py
model_name = "llama-7b"
rm_adapter_id = "trl-lib/llama-7b-hh-rm-adapter"

# PPO adapter
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = AutoModelForCausalLMWithValueHead.from_pretrained(
    model_name,
    peft_config=lora_config,
    reward_adapter=rm_adapter_id,
    load_in_8bit=True,
)

...
trainer = PPOTrainer(
    model=model,
    ...
)
...
```
