# 提示调整

> 原文链接：[https://huggingface.co/docs/peft/package_reference/prompt_tuning](https://huggingface.co/docs/peft/package_reference/prompt_tuning)

[Prompt tuning](https://hf.co/papers/2104.08691) 将任务特定的提示添加到输入中，这些提示参数独立于冻结的预训练模型参数进行更新。

论文摘要如下：

*在这项工作中，我们探索了“提示调整”，这是一种简单而有效的机制，用于学习“软提示”，以使冻结的语言模型执行特定的下游任务。与GPT-3使用的离散文本提示不同，软提示是通过反向传播学习的，可以调整以包含任意数量的标记示例的信号。我们的端到端学习方法在“少样本”学习方面远远优于GPT-3。更为显著的是，通过使用T5对模型规模进行消融，我们发现随着模型超过数十亿个参数，提示调整变得更具竞争力：当所有模型权重都被调整时，我们的方法“缩小了差距”并与模型调整的强大性能相匹配。这一发现尤其重要，因为大型模型的共享和服务成本高昂，而重复使用一个冻结模型来执行多个下游任务可以减轻这一负担。我们的方法可以看作是李和梁（2021年）最近提出的“前缀调整”的简化版本，并且我们将其与此及其他类似方法进行比较。最后，我们展示了使用软提示对冻结模型进行调整在对领域转移的鲁棒性方面具有益处，相比之下，与完全模型调整相比*。

## PromptTuningConfig

### `class peft.PromptTuningConfig`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/prompt_tuning/config.py#L28)

```py
( peft_type: Union = None auto_mapping: Optional = None base_model_name_or_path: Optional = None revision: Optional = None task_type: Union = None inference_mode: bool = False num_virtual_tokens: int = None token_dim: int = None num_transformer_submodules: Optional = None num_attention_heads: Optional = None num_layers: Optional = None prompt_tuning_init: Union = <PromptTuningInit.RANDOM: 'RANDOM'> prompt_tuning_init_text: Optional = None tokenizer_name_or_path: Optional = None tokenizer_kwargs: Optional = None )
```

参数

+   `prompt_tuning_init` (Union[`PromptTuningInit`, `str`]) — 提示嵌入的初始化。

+   `prompt_tuning_init_text` (`str`, *可选*) — 用于初始化提示嵌入的文本。仅在`prompt_tuning_init`为`TEXT`时使用。

+   `tokenizer_name_or_path` (`str`, *可选*) — 分词器的名称或路径。仅在`prompt_tuning_init`为`TEXT`时使用。

+   `tokenizer_kwargs` (`dict`, *可选*) — 传递给`AutoTokenizer.from_pretrained`的关键字参数。仅在`prompt_tuning_init`为`TEXT`时使用。

这是一个配置类，用于存储[PromptEmbedding](/docs/peft/v0.8.2/en/package_reference/prompt_tuning#peft.PromptEmbedding)的配置。

## PromptEmbedding

### `class peft.PromptEmbedding`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/prompt_tuning/model.py#L22)

```py
( config word_embeddings )
```

参数

+   `config` ([PromptTuningConfig](/docs/peft/v0.8.2/en/package_reference/prompt_tuning#peft.PromptTuningConfig)) — 提示嵌入的配置。

+   `word_embeddings` (`torch.nn.Module`) — 基础变换器模型的词嵌入。

将虚拟标记编码为提示嵌入的模型。

**属性**：

+   `embedding` (`torch.nn.Embedding`) — 提示嵌入的嵌入层。

示例：

```py
>>> from peft import PromptEmbedding, PromptTuningConfig

>>> config = PromptTuningConfig(
...     peft_type="PROMPT_TUNING",
...     task_type="SEQ_2_SEQ_LM",
...     num_virtual_tokens=20,
...     token_dim=768,
...     num_transformer_submodules=1,
...     num_attention_heads=12,
...     num_layers=12,
...     prompt_tuning_init="TEXT",
...     prompt_tuning_init_text="Predict if sentiment of this review is positive, negative or neutral",
...     tokenizer_name_or_path="t5-base",
... )

>>> # t5_model.shared is the word embeddings of the base model
>>> prompt_embedding = PromptEmbedding(config, t5_model.shared)
```

输入形状：(`batch_size`, `total_virtual_tokens`)

输出形状：(`batch_size`, `total_virtual_tokens`, `token_dim`)
