["```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"phi-2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"phi-2\")\n\n>>> inputs = tokenizer('Can you help me write a formal email to a potential business partner proposing a joint venture?', return_tensors=\"pt\", return_attention_mask=False)\n\n>>> outputs = model.generate(**inputs, max_length=30)\n>>> text = tokenizer.batch_decode(outputs)[0]\n>>> print(text)\n'Can you help me write a formal email to a potential business partner proposing a joint venture?\\nInput: Company A: ABC Inc.\\nCompany B: XYZ Ltd.\\nJoint Venture: A new online platform for e-commerce'\n```", "```py\n>>> from transformers import PhiForCausalLM, AutoTokenizer\n\n>>> # define the model and tokenizer.\n>>> model = PhiForCausalLM.from_pretrained(\"microsoft/phi-1_5\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n\n>>> # feel free to change the prompt to your liking.\n>>> prompt = \"If I were an AI that had just achieved\"\n\n>>> # apply the tokenizer.\n>>> tokens = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> # use the model to generate new tokens.\n>>> generated_output = model.generate(**tokens, use_cache=True, max_new_tokens=10)\n\n>>> tokenizer.batch_decode(generated_output)[0]\n'If I were an AI that had just achieved a breakthrough in machine learning, I would be thrilled'\n```", "```py\npip install -U flash-attn --no-build-isolation\n```", "```py\n>>> import torch\n>>> from transformers import PhiForCausalLM, AutoTokenizer\n\n>>> # define the model and tokenizer and push the model and tokens to the GPU.\n>>> model = PhiForCausalLM.from_pretrained(\"microsoft/phi-1_5\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(\"cuda\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n\n>>> # feel free to change the prompt to your liking.\n>>> prompt = \"If I were an AI that had just achieved\"\n\n>>> # apply the tokenizer.\n>>> tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n>>> # use the model to generate new tokens.\n>>> generated_output = model.generate(**tokens, use_cache=True, max_new_tokens=10)\n\n>>> tokenizer.batch_decode(generated_output)[0]\n'If I were an AI that had just achieved a breakthrough in machine learning, I would be thrilled'\n```", "```py\n>>> from transformers import PhiModel, PhiConfig\n\n>>> # Initializing a Phi-1 style configuration\n>>> configuration = PhiConfig.from_pretrained(\"microsoft/phi-1\")\n\n>>> # Initializing a model from the configuration\n>>> model = PhiModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoTokenizer, PhiForCausalLM\n\n>>> model = PhiForCausalLM.from_pretrained(\"microsoft/phi-1\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1\")\n\n>>> prompt = \"This is an example script .\"\n>>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> # Generate\n>>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n>>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n'This is an example script .\\n\\n\\n\\nfrom typing import List\\n\\ndef find_most_common_letter(words: List[str'\n```", "```py\n>>> from transformers import AutoTokenizer, PhiForTokenClassification\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1\")\n>>> model = PhiForTokenClassification.from_pretrained(\"microsoft/phi-1\")\n\n>>> inputs = tokenizer(\n...     \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n... )\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_token_class_ids = logits.argmax(-1)\n\n>>> # Note that tokens are classified rather then input words which means that\n>>> # there might be more predicted token classes than words.\n>>> # Multiple token classes might account for the same word\n>>> predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n\n>>> labels = predicted_token_class_ids\n>>> loss = model(**inputs, labels=labels).loss\n```"]