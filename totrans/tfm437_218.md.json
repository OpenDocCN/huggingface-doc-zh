["```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"phi-2\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"phi-2\")\n\n>>> inputs = tokenizer('Can you help me write a formal email to a potential business partner proposing a joint venture?', return_tensors=\"pt\", return_attention_mask=False)\n\n>>> outputs = model.generate(**inputs, max_length=30)\n>>> text = tokenizer.batch_decode(outputs)[0]\n>>> print(text)\n'Can you help me write a formal email to a potential business partner proposing a joint venture?\\nInput: Company A: ABC Inc.\\nCompany B: XYZ Ltd.\\nJoint Venture: A new online platform for e-commerce'\n```", "```py\n>>> from transformers import PhiForCausalLM, AutoTokenizer\n\n>>> # define the model and tokenizer.\n>>> model = PhiForCausalLM.from_pretrained(\"microsoft/phi-1_5\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n\n>>> # feel free to change the prompt to your liking.\n>>> prompt = \"If I were an AI that had just achieved\"\n\n>>> # apply the tokenizer.\n>>> tokens = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> # use the model to generate new tokens.\n>>> generated_output = model.generate(**tokens, use_cache=True, max_new_tokens=10)\n\n>>> tokenizer.batch_decode(generated_output)[0]\n'If I were an AI that had just achieved a breakthrough in machine learning, I would be thrilled'\n```", "```py\npip install -U flash-attn --no-build-isolation\n```", "```py\n>>> import torch\n>>> from transformers import PhiForCausalLM, AutoTokenizer\n\n>>> # define the model and tokenizer and push the model and tokens to the GPU.\n>>> model = PhiForCausalLM.from_pretrained(\"microsoft/phi-1_5\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(\"cuda\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n\n>>> # feel free to change the prompt to your liking.\n>>> prompt = \"If I were an AI that had just achieved\"\n\n>>> # apply the tokenizer.\n>>> tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n>>> # use the model to generate new tokens.\n>>> generated_output = model.generate(**tokens, use_cache=True, max_new_tokens=10)\n\n>>> tokenizer.batch_decode(generated_output)[0]\n'If I were an AI that had just achieved a breakthrough in machine learning, I would be thrilled'\n```", "```py\n( vocab_size = 51200 hidden_size = 2048 intermediate_size = 8192 num_hidden_layers = 24 num_attention_heads = 32 num_key_value_heads = None resid_pdrop = 0.0 embd_pdrop = 0.0 attention_dropout = 0.0 hidden_act = 'gelu_new' max_position_embeddings = 2048 initializer_range = 0.02 layer_norm_eps = 1e-05 use_cache = True tie_word_embeddings = False rope_theta = 10000.0 rope_scaling = None partial_rotary_factor = 0.5 qk_layernorm = False bos_token_id = 1 eos_token_id = 2 **kwargs )\n```", "```py\n>>> from transformers import PhiModel, PhiConfig\n\n>>> # Initializing a Phi-1 style configuration\n>>> configuration = PhiConfig.from_pretrained(\"microsoft/phi-1\")\n\n>>> # Initializing a model from the configuration\n>>> model = PhiModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config: PhiConfig )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, PhiForCausalLM\n\n>>> model = PhiForCausalLM.from_pretrained(\"microsoft/phi-1\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1\")\n\n>>> prompt = \"This is an example script .\"\n>>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> # Generate\n>>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n>>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n'This is an example script .\\n\\n\\n\\nfrom typing import List\\n\\ndef find_most_common_letter(words: List[str'\n```", "```py\n( inputs: Optional = None generation_config: Optional = None logits_processor: Optional = None stopping_criteria: Optional = None prefix_allowed_tokens_fn: Optional = None synced_gpus: Optional = None assistant_model: Optional = None streamer: Optional = None negative_prompt_ids: Optional = None negative_prompt_attention_mask: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';ModelOutput or torch.LongTensor\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( config: PhiConfig )\n```", "```py\n( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **deprecated_arguments ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, PhiForTokenClassification\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1\")\n>>> model = PhiForTokenClassification.from_pretrained(\"microsoft/phi-1\")\n\n>>> inputs = tokenizer(\n...     \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n... )\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_token_class_ids = logits.argmax(-1)\n\n>>> # Note that tokens are classified rather then input words which means that\n>>> # there might be more predicted token classes than words.\n>>> # Multiple token classes might account for the same word\n>>> predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n\n>>> labels = predicted_token_class_ids\n>>> loss = model(**inputs, labels=labels).loss\n```"]