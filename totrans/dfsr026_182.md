# 香肠

> 原文链接：[https://huggingface.co/docs/diffusers/api/pipelines/wuerstchen](https://huggingface.co/docs/diffusers/api/pipelines/wuerstchen)

![](../Images/e29b341d4d756c0a6cf4d1a787243bfc.png)

[Wuerstchen：大规模文本到图像扩散模型的高效架构](https://huggingface.co/papers/2306.00637) 由Pablo Pernias、Dominic Rampas、Mats L. Richter、Christopher Pal和Marc Aubreville撰写。

论文摘要如下：

*我们介绍了香肠，这是一种新颖的文本到图像合成架构，结合了竞争性能和大规模文本到图像扩散模型的前所未有的成本效益。我们工作的一个关键贡献是开发一种潜在扩散技术，通过这种技术我们学习了一种详细但极其紧凑的语义图像表示，用于引导扩散过程。与语言的潜在表示相比，这种高度压缩的图像表示提供了更详细的引导，这显著降低了实现最先进结果所需的计算要求。我们的方法还根据用户偏好研究改进了基于文本条件的图像生成的质量。我们的方法的训练需求为24,602个A100-GPU小时 - 相比于Stable Diffusion 2.1的200,000个GPU小时。我们的方法还需要更少的训练数据来实现这些结果。此外，我们紧凑的潜在表示使我们能够执行两倍速度的推断，大幅削减了一种最先进（SOTA）扩散模型的通常成本和碳足迹，而不会影响最终性能。在与SOTA模型的更广泛比较中，我们的方法在效率上更为显著，并在图像质量方面具有可比性。我们相信这项工作促使更多关注性能和计算可访问性的优先考虑。*

## 香肠概述

香肠是一种扩散模型，其文本条件模型在图像的高度压缩潜在空间中运行。为什么这很重要？压缩数据可以将训练和推断的计算成本降低数倍。在1024x1024图像上训练比在32x32图像上训练要昂贵得多。通常，其他作品使用相对较小的压缩，范围在4倍至8倍的空间压缩。香肠将这一点推向了极端。通过其新颖的设计，我们实现了42倍的空间压缩。这之前是看不到的，因为常见方法在16倍空间压缩后无法忠实地重建详细图像。香肠采用了两阶段压缩，我们称之为A阶段和B阶段。A阶段是VQGAN，B阶段是扩散自动编码器（更多细节可以在[论文](https://huggingface.co/papers/2306.00637)中找到）。第三个模型，C阶段，在这个高度压缩的潜在空间中学习。这种训练只需要当前性能最佳模型使用的计算的一小部分，同时还可以实现更便宜和更快的推断。

## 香肠v2来到Diffusers

在最初的论文发布后，我们在架构、训练和采样方面改进了许多内容，使香肠在许多方面与当前最先进的模型竞争力十足。我们很高兴与Diffusers一起发布这个新版本。以下是改进列表。

+   更高分辨率（从1024x1024到2048x2048）

+   更快的推断

+   多方面分辨率采样

+   更好的质量

我们发布了文本条件图像生成模型（C阶段）的3个检查点。它们是：

+   v2-基础

+   v2-美学

+   （默认）v2-插值（在v2-基础和v2-美学之间进行50%的插值）

我们建议使用v2-插值，因为它既具有照片逼真感又具有美学感。对于微调，请使用v2-基础，因为它没有风格偏见，对于非常艺术的生成，请使用v2-美学。可以在这里看到比较：

![](../Images/b06dcd2bc467532747508fc8c277921a.png)

## 文本到图像生成

为了提高可用性，Würstchen 可以与单个管道一起使用。可以按以下方式使用此管道：

```py
import torch
from diffusers import AutoPipelineForText2Image
from diffusers.pipelines.wuerstchen import DEFAULT_STAGE_C_TIMESTEPS

pipe = AutoPipelineForText2Image.from_pretrained("warp-ai/wuerstchen", torch_dtype=torch.float16).to("cuda")

caption = "Anthropomorphic cat dressed as a fire fighter"
images = pipe(
    caption,
    width=1024,
    height=1536,
    prior_timesteps=DEFAULT_STAGE_C_TIMESTEPS,
    prior_guidance_scale=4.0,
    num_images_per_prompt=2,
).images
```

为了解释目的，我们还可以单独初始化 Würstchen 的两个主要管道。Würstchen 由 3 个阶段组成：阶段 C、阶段 B、阶段 A。它们都有不同的工作，并且只能一起工作。在生成文本条件图像时，阶段 C 将首先在非常压缩的潜在空间中生成潜在变量。这就是在 `prior_pipeline` 中发生的事情。然后，生成的潜在变量将传递给阶段 B，后者将潜在变量解压缩为 VQGAN 的更大潜在空间。然后，这些潜在变量可以由阶段 A 解码为像素空间。阶段 B 和阶段 A 都封装在 `decoder_pipeline` 中。有关更多详细信息，请查看 [paper](https://huggingface.co/papers/2306.00637)。

```py
import torch
from diffusers import WuerstchenDecoderPipeline, WuerstchenPriorPipeline
from diffusers.pipelines.wuerstchen import DEFAULT_STAGE_C_TIMESTEPS

device = "cuda"
dtype = torch.float16
num_images_per_prompt = 2

prior_pipeline = WuerstchenPriorPipeline.from_pretrained(
    "warp-ai/wuerstchen-prior", torch_dtype=dtype
).to(device)
decoder_pipeline = WuerstchenDecoderPipeline.from_pretrained(
    "warp-ai/wuerstchen", torch_dtype=dtype
).to(device)

caption = "Anthropomorphic cat dressed as a fire fighter"
negative_prompt = ""

prior_output = prior_pipeline(
    prompt=caption,
    height=1024,
    width=1536,
    timesteps=DEFAULT_STAGE_C_TIMESTEPS,
    negative_prompt=negative_prompt,
    guidance_scale=4.0,
    num_images_per_prompt=num_images_per_prompt,
)
decoder_output = decoder_pipeline(
    image_embeddings=prior_output.image_embeddings,
    prompt=caption,
    negative_prompt=negative_prompt,
    guidance_scale=0.0,
    output_type="pil",
).images[0]
decoder_output
```

## 加速推理

您可以使用 `torch.compile` 函数，获得大约 2-3 倍的加速：

```py
prior_pipeline.prior = torch.compile(prior_pipeline.prior, mode="reduce-overhead", fullgraph=True)
decoder_pipeline.decoder = torch.compile(decoder_pipeline.decoder, mode="reduce-overhead", fullgraph=True)
```

## 限制

+   由于 Würstchen 使用了高度压缩，生成的图像可能缺乏大量细节。对于我们的肉眼来说，这在面部、手部等方面尤为明显。

+   **图像只能以 128 像素的步长生成**，例如，1024x1024 之后的下一个更高分辨率是 1152x1152

+   该模型缺乏在图像中正确呈现文本的能力

+   该模型通常无法实现照片级逼真度

+   复杂的组合提示对模型来说很难

原始代码库以及实验性想法可以在 [dome272/Wuerstchen](https://github.com/dome272/Wuerstchen) 找到。

## WuerstchenCombinedPipeline

### `class diffusers.WuerstchenCombinedPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L43)

```py
( tokenizer: CLIPTokenizer text_encoder: CLIPTextModel decoder: WuerstchenDiffNeXt scheduler: DDPMWuerstchenScheduler vqgan: PaellaVQModel prior_tokenizer: CLIPTokenizer prior_text_encoder: CLIPTextModel prior_prior: WuerstchenPrior prior_scheduler: DDPMWuerstchenScheduler )
```

参数

+   `tokenizer` (`CLIPTokenizer`) — 用于文本输入的解码器分词器。

+   `text_encoder` (`CLIPTextModel`) — 用于文本输入的解码器文本编码器。

+   `decoder` (`WuerstchenDiffNeXt`) — 用于解码器图像生成管道的解码器模型。

+   `scheduler` (`DDPMWuerstchenScheduler`) — 用于解码器图像生成管道的调度器。

+   `vqgan` (`PaellaVQModel`) — 用于解码器图像生成管道的 VQGAN 模型。

+   `prior_tokenizer` (`CLIPTokenizer`) — 用于文本输入的先前分词器。

+   `prior_text_encoder` (`CLIPTextModel`) — 用于文本输入的先前文本编码器。

+   `prior_prior` (`WuerstchenPrior`) — 用于先前管道的先前模型。

+   `prior_scheduler` (`DDPMWuerstchenScheduler`) — 用于先前管道的调度器。

使用 Wuerstchen 进行文本到图像生成的组合管道

该模型继承自 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。检查超类文档以了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L143)

```py
( prompt: Union = None height: int = 512 width: int = 512 prior_num_inference_steps: int = 60 prior_timesteps: Optional = None prior_guidance_scale: float = 4.0 num_inference_steps: int = 12 decoder_timesteps: Optional = None decoder_guidance_scale: float = 0.0 negative_prompt: Union = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None num_images_per_prompt: int = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pil' return_dict: bool = True prior_callback_on_step_end: Optional = None prior_callback_on_step_end_tensor_inputs: List = ['latents'] callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs )
```

参数

+   `prompt` (`str` or `List[str]`) — 用于指导先前和解码器图像生成的提示或提示。

+   `negative_prompt` (`str` or `List[str]`, *可选*) — 不指导图像生成的提示或提示。如果不使用指导（即如果 `guidance_scale` 小于 `1`，则忽略）。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 用于先前的预生成文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从 `prompt` 输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 用于先前的预生成负面文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从 `negative_prompt` 输入参数生成负面提示嵌入。

+   `num_images_per_prompt` (`int`, *optional*, 默认为1) — 每个提示生成的图像数量。

+   `height` (`int`, *optional*, 默认为512) — 生成图像的像素高度。

+   `width` (`int`, *optional*, 默认为512) — 生成图像的像素宽度。

+   `prior_guidance_scale` (`float`, *optional*, 默认为4.0) — 如[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`prior_guidance_scale`定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程2的`w`。通过设置`prior_guidance_scale > 1`来启用引导比例。更高的引导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `prior_num_inference_steps` (`Union[int, Dict[float, int]]`, *optional*, 默认为60) — 先验去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。要获得更具体的时间步长间距，可以传递自定义的`prior_timesteps`。

+   `num_inference_steps` (`int`, *optional*, 默认为12) — 解码器去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。要获得更具体的时间步长间距，可以传递自定义的`timesteps`。

+   `prior_timesteps` (`List[float]`, *optional*) — 用于先验去噪过程的自定义时间步长。如果未定义，则使用等间距的`prior_num_inference_steps`时间步长。必须按降序排列。

+   `decoder_timesteps` (`List[float]`, *optional*) — 用于解码器去噪过程的自定义时间步长。如果未定义，则使用等间距的`num_inference_steps`时间步长。必须按降序排列。

+   `decoder_guidance_scale` (`float`, *optional*, 默认为0.0) — 如[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`guidance_scale`定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程2的`w`。通过设置`guidance_scale > 1`来启用引导比例。更高的引导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `generator` (`torch.Generator`或`List[torch.Generator]`, *optional*) — 一个或多个[torch生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *optional*) — 预先生成的嘈杂潜在向量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示微调相同的生成。如果未提供，则将使用提供的随机`generator`进行采样生成潜在向量。

+   `output_type` (`str`, *optional*, 默认为`"pil"`) — 生成图像的输出格式。可在`"pil"`（`PIL.Image.Image`）、`"np"`（`np.array`）或`"pt"`（`torch.Tensor`）之间选择。

+   `return_dict` (`bool`, *optional*, 默认为`True`) — 是否返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)而不是普通元组。

+   `prior_callback_on_step_end` (`Callable`, *optional*) — 在推理过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`prior_callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。

+   `prior_callback_on_step_end_tensor_inputs` (`List`, *optional*) — `prior_callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在您的管道类的`._callback_tensor_inputs`属性中列出的变量。

+   `callback_on_step_end` (`Callable`, *可选*) — 在推断期间每个去噪步骤结束时调用的函数。该函数使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *可选*) — `callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在管道类的`._callback_tensor_inputs`属性中列出的变量。

调用管道以进行生成时调用的函数。

示例：

```py
>>> from diffusions import WuerstchenCombinedPipeline

>>> pipe = WuerstchenCombinedPipeline.from_pretrained("warp-ai/Wuerstchen", torch_dtype=torch.float16).to(
...     "cuda"
... )
>>> prompt = "an image of a shiba inu, donning a spacesuit and helmet"
>>> images = pipe(prompt=prompt)
```

#### `enable_model_cpu_offload`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L115)

```py
( gpu_id = 0 )
```

使用加速将所有模型转移到CPU，减少内存使用并对性能影响较小。与`enable_sequential_cpu_offload`相比，此方法在调用其`forward`方法时一次将一个完整模型移至GPU，并且该模型保持在GPU中，直到下一个模型运行。与`enable_sequential_cpu_offload`相比，内存节省较低，但由于`unet`的迭代执行，性能更好。

#### `enable_sequential_cpu_offload`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L125)

```py
( gpu_id = 0 )
```

使用🤗 Accelerate将所有模型（`unet`、`text_encoder`、`vae`和`safety checker`状态字典）全部转移到CPU，显著减少内存使用。模型被移动到`torch.device('meta')`，仅当调用其特定子模块的`forward`方法时才会在GPU上加载。转移是基于子模块的。内存节省高于使用`enable_model_cpu_offload`，但性能较低。

## WuerstchenPriorPipeline

### `class diffusers.WuerstchenPriorPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L65)

```py
( tokenizer: CLIPTokenizer text_encoder: CLIPTextModel prior: WuerstchenPrior scheduler: DDPMWuerstchenScheduler latent_mean: float = 42.0 latent_std: float = 1.0 resolution_multiple: float = 42.67 )
```

参数

+   `prior` (`Prior`) — 用于从文本嵌入逼近图像嵌入的经典unCLIP先验。

+   `text_encoder` (`CLIPTextModelWithProjection`) — 冻结的文本编码器。

+   `tokenizer` (`CLIPTokenizer`) — 类[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)的分词器。

+   `scheduler` (`DDPMWuerstchenScheduler`) — 与`prior`结合使用的调度器，用于生成图像嵌入。

+   `latent_mean`（‘float’, *可选*, 默认为42.0）— 潜在扩散的均值。

+   `latent_std`（‘float’, *可选*, 默认为1.0）— 潜在扩散的标准值。

+   `resolution_multiple`（‘float’, *可选*, 默认为42.67）— 生成多个图像的默认分辨率。

用于生成Wuerstchen图像先验的管道。

此模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档，了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

该管道还继承了以下加载方法：

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) 用于加载LoRA权重

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) 用于保存LoRA权重

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L280)

```py
( prompt: Union = None height: int = 1024 width: int = 1024 num_inference_steps: int = 60 timesteps: List = None guidance_scale: float = 8.0 negative_prompt: Union = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None num_images_per_prompt: Optional = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pt' return_dict: bool = True callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs )
```

参数

+   `prompt` (`str` 或 `List[str]`) — 用于指导图像生成的提示或提示。

+   `height` (`int`, *可选*, 默认为1024) — 生成图像的像素高度。

+   `width` (`int`, *optional*, defaults to 1024) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *optional*, defaults to 60) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但推理速度会变慢。

+   `timesteps` (`List[int]`, *optional*) — 用于去噪过程的自定义时间步。如果未定义，则使用等间距的 `num_inference_steps` 个时间步。必须按降序排列。

+   `guidance_scale` (`float`, *optional*, defaults to 8.0) — 如 [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598) 中定义的引导比例。`decoder_guidance_scale` 定义为 [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf) 方程式 2 的 `w`。通过设置 `decoder_guidance_scale > 1` 启用引导比例。更高的引导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 不用来引导图像生成的提示或提示。当不使用引导时（即，如果 `decoder_guidance_scale` 小于 `1`，则忽略）。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如调整提示权重。如果未提供，文本嵌入将从 `prompt` 输入参数中生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如调整提示权重。如果未提供，将从 `negative_prompt` 输入参数中生成负文本嵌入。

+   `num_images_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的图像数量。

+   `generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 一个或多个 [torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *optional*) — 预生成的噪声潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机 `generator` 进行采样生成潜变量张量。

+   `output_type` (`str`, *optional*, defaults to `"pil"`) — 生成图像的输出格式。可选择的格式包括：`"pil"` (`PIL.Image.Image`)、`"np"` (`np.array`) 或 `"pt"` (`torch.Tensor`)。

+   `return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回一个 [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) 而不是一个普通的元组。

+   `callback_on_step_end` (`Callable`, *optional*) — 在推理过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs` 将包括由 `callback_on_step_end_tensor_inputs` 指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) — `callback_on_step_end` 函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包含在您的管道类的 `._callback_tensor_inputs` 属性中列出的变量。

调用管道进行生成时调用的函数。

示例：

```py
>>> import torch
>>> from diffusers import WuerstchenPriorPipeline

>>> prior_pipe = WuerstchenPriorPipeline.from_pretrained(
...     "warp-ai/wuerstchen-prior", torch_dtype=torch.float16
... ).to("cuda")

>>> prompt = "an image of a shiba inu, donning a spacesuit and helmet"
>>> prior_output = pipe(prompt)
```

## WuerstchenPriorPipelineOutput

### `class diffusers.pipelines.wuerstchen.pipeline_wuerstchen_prior.WuerstchenPriorPipelineOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L51)

```py
( image_embeddings: Union )
```

参数

+   `image_embeddings` (`torch.FloatTensor` or `np.ndarray`) — 用于文本提示的先验图像嵌入。

WuerstchenPriorPipeline 的输出类。

## WuerstchenDecoderPipeline

### `class diffusers.WuerstchenDecoderPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen.py#L51)

```py
( tokenizer: CLIPTokenizer text_encoder: CLIPTextModel decoder: WuerstchenDiffNeXt scheduler: DDPMWuerstchenScheduler vqgan: PaellaVQModel latent_dim_scale: float = 10.67 )
```

参数

+   `tokenizer` (`CLIPTokenizer`) — CLIP 分词器。

+   `text_encoder` (`CLIPTextModel`) — CLIP 文本编码器。

+   `decoder` (`WuerstchenDiffNeXt`) — WuerstchenDiffNeXt unet 解码器。

+   `vqgan` (`PaellaVQModel`) — VQGAN 模型。

+   `scheduler` (`DDPMWuerstchenScheduler`) — 用于与`prior`结合使用以生成图像嵌入的调度器。

+   `latent_dim_scale` (float, *可选*, 默认为10.67) — 从图像嵌入确定 VQ 潜变量空间大小的乘数。如果图像嵌入的高度=24，宽度=24，则 VQ 潜变量形状需要为高度=int(24*10.67)=256，宽度=int(24*10.67)=256，以匹配训练条件。

用于从 Wuerstchen 模型生成图像的管道。

该模型继承自 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen.py#L208)

```py
( image_embeddings: Union prompt: Union = None num_inference_steps: int = 12 timesteps: Optional = None guidance_scale: float = 0.0 negative_prompt: Union = None num_images_per_prompt: int = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs )
```

参数

+   `image_embedding` (`torch.FloatTensor` 或 `List[torch.FloatTensor]`) — 图像嵌入，可以是从图像中提取的，也可以是由先验模型生成的。

+   `prompt` (`str` 或 `List[str]`) — 用于指导图像生成的提示。

+   `num_inference_steps` (`int`, *可选*, 默认为12) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `timesteps` (`List[int]`, *可选*) — 用于去噪过程的自定义时间步。如果未定义，则使用等间隔的`num_inference_steps`时间步。必须按降序排列。

+   `guidance_scale` (`float`, *可选*, 默认为0.0) — 如[无分类器扩散指导](https://arxiv.org/abs/2207.12598)中定义的指导比例。`decoder_guidance_scale`定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程2的`w`。通过设置`decoder_guidance_scale > 1`启用指导比例。更高的指导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用于指导图像生成的提示。如果不使用指导（即如果`decoder_guidance_scale`小于`1`），则忽略。

+   `num_images_per_prompt` (`int`, *可选*, 默认为1) — 每个提示生成的图像数量。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个[torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *可选*) — 预先生成的噪声潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，则将使用提供的随机`generator`进行采样生成潜变量张量。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。可选择 `"pil"` (`PIL.Image.Image`)、`"np"` (`np.array`) 或 `"pt"` (`torch.Tensor`)。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回 [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) 而不是普通元组。

+   `callback_on_step_end`（`Callable`，*可选*）—在推断过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs`（`List`，*可选*）—`callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在您的管道类的`._callback_tensor_inputs`属性中列出的变量。

在调用管道生成时调用的函数。

示例：

```py
>>> import torch
>>> from diffusers import WuerstchenPriorPipeline, WuerstchenDecoderPipeline

>>> prior_pipe = WuerstchenPriorPipeline.from_pretrained(
...     "warp-ai/wuerstchen-prior", torch_dtype=torch.float16
... ).to("cuda")
>>> gen_pipe = WuerstchenDecoderPipeline.from_pretrain("warp-ai/wuerstchen", torch_dtype=torch.float16).to(
...     "cuda"
... )

>>> prompt = "an image of a shiba inu, donning a spacesuit and helmet"
>>> prior_output = pipe(prompt)
>>> images = gen_pipe(prior_output.image_embeddings, prompt=prompt)
```

## 引用

```py
      @misc{pernias2023wuerstchen,
            title={Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models},
            author={Pablo Pernias and Dominic Rampas and Mats L. Richter and Christopher J. Pal and Marc Aubreville},
            year={2023},
            eprint={2306.00637},
            archivePrefix={arXiv},
            primaryClass={cs.CV}
      }
```
