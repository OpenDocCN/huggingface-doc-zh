# é¦™è‚ 

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/diffusers/api/pipelines/wuerstchen](https://huggingface.co/docs/diffusers/api/pipelines/wuerstchen)

![](../Images/e29b341d4d756c0a6cf4d1a787243bfc.png)

[Wuerstchenï¼šå¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆæ¶æ„](https://huggingface.co/papers/2306.00637) ç”±Pablo Perniasã€Dominic Rampasã€Mats L. Richterã€Christopher Palå’ŒMarc Aubrevilleæ’°å†™ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*æˆ‘ä»¬ä»‹ç»äº†é¦™è‚ ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ¶æ„ï¼Œç»“åˆäº†ç«äº‰æ€§èƒ½å’Œå¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å‰æ‰€æœªæœ‰çš„æˆæœ¬æ•ˆç›Šã€‚æˆ‘ä»¬å·¥ä½œçš„ä¸€ä¸ªå…³é”®è´¡çŒ®æ˜¯å¼€å‘ä¸€ç§æ½œåœ¨æ‰©æ•£æŠ€æœ¯ï¼Œé€šè¿‡è¿™ç§æŠ€æœ¯æˆ‘ä»¬å­¦ä¹ äº†ä¸€ç§è¯¦ç»†ä½†æå…¶ç´§å‡‘çš„è¯­ä¹‰å›¾åƒè¡¨ç¤ºï¼Œç”¨äºå¼•å¯¼æ‰©æ•£è¿‡ç¨‹ã€‚ä¸è¯­è¨€çš„æ½œåœ¨è¡¨ç¤ºç›¸æ¯”ï¼Œè¿™ç§é«˜åº¦å‹ç¼©çš„å›¾åƒè¡¨ç¤ºæä¾›äº†æ›´è¯¦ç»†çš„å¼•å¯¼ï¼Œè¿™æ˜¾è‘—é™ä½äº†å®ç°æœ€å…ˆè¿›ç»“æœæ‰€éœ€çš„è®¡ç®—è¦æ±‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜æ ¹æ®ç”¨æˆ·åå¥½ç ”ç©¶æ”¹è¿›äº†åŸºäºæ–‡æœ¬æ¡ä»¶çš„å›¾åƒç”Ÿæˆçš„è´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„è®­ç»ƒéœ€æ±‚ä¸º24,602ä¸ªA100-GPUå°æ—¶ - ç›¸æ¯”äºStable Diffusion 2.1çš„200,000ä¸ªGPUå°æ—¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜éœ€è¦æ›´å°‘çš„è®­ç»ƒæ•°æ®æ¥å®ç°è¿™äº›ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºä½¿æˆ‘ä»¬èƒ½å¤Ÿæ‰§è¡Œä¸¤å€é€Ÿåº¦çš„æ¨æ–­ï¼Œå¤§å¹…å‰Šå‡äº†ä¸€ç§æœ€å…ˆè¿›ï¼ˆSOTAï¼‰æ‰©æ•£æ¨¡å‹çš„é€šå¸¸æˆæœ¬å’Œç¢³è¶³è¿¹ï¼Œè€Œä¸ä¼šå½±å“æœ€ç»ˆæ€§èƒ½ã€‚åœ¨ä¸SOTAæ¨¡å‹çš„æ›´å¹¿æ³›æ¯”è¾ƒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•ˆç‡ä¸Šæ›´ä¸ºæ˜¾è‘—ï¼Œå¹¶åœ¨å›¾åƒè´¨é‡æ–¹é¢å…·æœ‰å¯æ¯”æ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹å·¥ä½œä¿ƒä½¿æ›´å¤šå…³æ³¨æ€§èƒ½å’Œè®¡ç®—å¯è®¿é—®æ€§çš„ä¼˜å…ˆè€ƒè™‘ã€‚*

## é¦™è‚ æ¦‚è¿°

é¦™è‚ æ˜¯ä¸€ç§æ‰©æ•£æ¨¡å‹ï¼Œå…¶æ–‡æœ¬æ¡ä»¶æ¨¡å‹åœ¨å›¾åƒçš„é«˜åº¦å‹ç¼©æ½œåœ¨ç©ºé—´ä¸­è¿è¡Œã€‚ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼Ÿå‹ç¼©æ•°æ®å¯ä»¥å°†è®­ç»ƒå’Œæ¨æ–­çš„è®¡ç®—æˆæœ¬é™ä½æ•°å€ã€‚åœ¨1024x1024å›¾åƒä¸Šè®­ç»ƒæ¯”åœ¨32x32å›¾åƒä¸Šè®­ç»ƒè¦æ˜‚è´µå¾—å¤šã€‚é€šå¸¸ï¼Œå…¶ä»–ä½œå“ä½¿ç”¨ç›¸å¯¹è¾ƒå°çš„å‹ç¼©ï¼ŒèŒƒå›´åœ¨4å€è‡³8å€çš„ç©ºé—´å‹ç¼©ã€‚é¦™è‚ å°†è¿™ä¸€ç‚¹æ¨å‘äº†æç«¯ã€‚é€šè¿‡å…¶æ–°é¢–çš„è®¾è®¡ï¼Œæˆ‘ä»¬å®ç°äº†42å€çš„ç©ºé—´å‹ç¼©ã€‚è¿™ä¹‹å‰æ˜¯çœ‹ä¸åˆ°çš„ï¼Œå› ä¸ºå¸¸è§æ–¹æ³•åœ¨16å€ç©ºé—´å‹ç¼©åæ— æ³•å¿ å®åœ°é‡å»ºè¯¦ç»†å›¾åƒã€‚é¦™è‚ é‡‡ç”¨äº†ä¸¤é˜¶æ®µå‹ç¼©ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºAé˜¶æ®µå’ŒBé˜¶æ®µã€‚Aé˜¶æ®µæ˜¯VQGANï¼ŒBé˜¶æ®µæ˜¯æ‰©æ•£è‡ªåŠ¨ç¼–ç å™¨ï¼ˆæ›´å¤šç»†èŠ‚å¯ä»¥åœ¨[è®ºæ–‡](https://huggingface.co/papers/2306.00637)ä¸­æ‰¾åˆ°ï¼‰ã€‚ç¬¬ä¸‰ä¸ªæ¨¡å‹ï¼ŒCé˜¶æ®µï¼Œåœ¨è¿™ä¸ªé«˜åº¦å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­å­¦ä¹ ã€‚è¿™ç§è®­ç»ƒåªéœ€è¦å½“å‰æ€§èƒ½æœ€ä½³æ¨¡å‹ä½¿ç”¨çš„è®¡ç®—çš„ä¸€å°éƒ¨åˆ†ï¼ŒåŒæ—¶è¿˜å¯ä»¥å®ç°æ›´ä¾¿å®œå’Œæ›´å¿«çš„æ¨æ–­ã€‚

## é¦™è‚ v2æ¥åˆ°Diffusers

åœ¨æœ€åˆçš„è®ºæ–‡å‘å¸ƒåï¼Œæˆ‘ä»¬åœ¨æ¶æ„ã€è®­ç»ƒå’Œé‡‡æ ·æ–¹é¢æ”¹è¿›äº†è®¸å¤šå†…å®¹ï¼Œä½¿é¦™è‚ åœ¨è®¸å¤šæ–¹é¢ä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ç«äº‰åŠ›åè¶³ã€‚æˆ‘ä»¬å¾ˆé«˜å…´ä¸Diffusersä¸€èµ·å‘å¸ƒè¿™ä¸ªæ–°ç‰ˆæœ¬ã€‚ä»¥ä¸‹æ˜¯æ”¹è¿›åˆ—è¡¨ã€‚

+   æ›´é«˜åˆ†è¾¨ç‡ï¼ˆä»1024x1024åˆ°2048x2048ï¼‰

+   æ›´å¿«çš„æ¨æ–­

+   å¤šæ–¹é¢åˆ†è¾¨ç‡é‡‡æ ·

+   æ›´å¥½çš„è´¨é‡

æˆ‘ä»¬å‘å¸ƒäº†æ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆæ¨¡å‹ï¼ˆCé˜¶æ®µï¼‰çš„3ä¸ªæ£€æŸ¥ç‚¹ã€‚å®ƒä»¬æ˜¯ï¼š

+   v2-åŸºç¡€

+   v2-ç¾å­¦

+   ï¼ˆé»˜è®¤ï¼‰v2-æ’å€¼ï¼ˆåœ¨v2-åŸºç¡€å’Œv2-ç¾å­¦ä¹‹é—´è¿›è¡Œ50%çš„æ’å€¼ï¼‰

æˆ‘ä»¬å»ºè®®ä½¿ç”¨v2-æ’å€¼ï¼Œå› ä¸ºå®ƒæ—¢å…·æœ‰ç…§ç‰‡é€¼çœŸæ„Ÿåˆå…·æœ‰ç¾å­¦æ„Ÿã€‚å¯¹äºå¾®è°ƒï¼Œè¯·ä½¿ç”¨v2-åŸºç¡€ï¼Œå› ä¸ºå®ƒæ²¡æœ‰é£æ ¼åè§ï¼Œå¯¹äºéå¸¸è‰ºæœ¯çš„ç”Ÿæˆï¼Œè¯·ä½¿ç”¨v2-ç¾å­¦ã€‚å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°æ¯”è¾ƒï¼š

![](../Images/b06dcd2bc467532747508fc8c277921a.png)

## æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ

ä¸ºäº†æé«˜å¯ç”¨æ€§ï¼ŒWÃ¼rstchen å¯ä»¥ä¸å•ä¸ªç®¡é“ä¸€èµ·ä½¿ç”¨ã€‚å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼ä½¿ç”¨æ­¤ç®¡é“ï¼š

```py
import torch
from diffusers import AutoPipelineForText2Image
from diffusers.pipelines.wuerstchen import DEFAULT_STAGE_C_TIMESTEPS

pipe = AutoPipelineForText2Image.from_pretrained("warp-ai/wuerstchen", torch_dtype=torch.float16).to("cuda")

caption = "Anthropomorphic cat dressed as a fire fighter"
images = pipe(
    caption,
    width=1024,
    height=1536,
    prior_timesteps=DEFAULT_STAGE_C_TIMESTEPS,
    prior_guidance_scale=4.0,
    num_images_per_prompt=2,
).images
```

ä¸ºäº†è§£é‡Šç›®çš„ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥å•ç‹¬åˆå§‹åŒ– WÃ¼rstchen çš„ä¸¤ä¸ªä¸»è¦ç®¡é“ã€‚WÃ¼rstchen ç”± 3 ä¸ªé˜¶æ®µç»„æˆï¼šé˜¶æ®µ Cã€é˜¶æ®µ Bã€é˜¶æ®µ Aã€‚å®ƒä»¬éƒ½æœ‰ä¸åŒçš„å·¥ä½œï¼Œå¹¶ä¸”åªèƒ½ä¸€èµ·å·¥ä½œã€‚åœ¨ç”Ÿæˆæ–‡æœ¬æ¡ä»¶å›¾åƒæ—¶ï¼Œé˜¶æ®µ C å°†é¦–å…ˆåœ¨éå¸¸å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆæ½œåœ¨å˜é‡ã€‚è¿™å°±æ˜¯åœ¨ `prior_pipeline` ä¸­å‘ç”Ÿçš„äº‹æƒ…ã€‚ç„¶åï¼Œç”Ÿæˆçš„æ½œåœ¨å˜é‡å°†ä¼ é€’ç»™é˜¶æ®µ Bï¼Œåè€…å°†æ½œåœ¨å˜é‡è§£å‹ç¼©ä¸º VQGAN çš„æ›´å¤§æ½œåœ¨ç©ºé—´ã€‚ç„¶åï¼Œè¿™äº›æ½œåœ¨å˜é‡å¯ä»¥ç”±é˜¶æ®µ A è§£ç ä¸ºåƒç´ ç©ºé—´ã€‚é˜¶æ®µ B å’Œé˜¶æ®µ A éƒ½å°è£…åœ¨ `decoder_pipeline` ä¸­ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [paper](https://huggingface.co/papers/2306.00637)ã€‚

```py
import torch
from diffusers import WuerstchenDecoderPipeline, WuerstchenPriorPipeline
from diffusers.pipelines.wuerstchen import DEFAULT_STAGE_C_TIMESTEPS

device = "cuda"
dtype = torch.float16
num_images_per_prompt = 2

prior_pipeline = WuerstchenPriorPipeline.from_pretrained(
    "warp-ai/wuerstchen-prior", torch_dtype=dtype
).to(device)
decoder_pipeline = WuerstchenDecoderPipeline.from_pretrained(
    "warp-ai/wuerstchen", torch_dtype=dtype
).to(device)

caption = "Anthropomorphic cat dressed as a fire fighter"
negative_prompt = ""

prior_output = prior_pipeline(
    prompt=caption,
    height=1024,
    width=1536,
    timesteps=DEFAULT_STAGE_C_TIMESTEPS,
    negative_prompt=negative_prompt,
    guidance_scale=4.0,
    num_images_per_prompt=num_images_per_prompt,
)
decoder_output = decoder_pipeline(
    image_embeddings=prior_output.image_embeddings,
    prompt=caption,
    negative_prompt=negative_prompt,
    guidance_scale=0.0,
    output_type="pil",
).images[0]
decoder_output
```

## åŠ é€Ÿæ¨ç†

æ‚¨å¯ä»¥ä½¿ç”¨ `torch.compile` å‡½æ•°ï¼Œè·å¾—å¤§çº¦ 2-3 å€çš„åŠ é€Ÿï¼š

```py
prior_pipeline.prior = torch.compile(prior_pipeline.prior, mode="reduce-overhead", fullgraph=True)
decoder_pipeline.decoder = torch.compile(decoder_pipeline.decoder, mode="reduce-overhead", fullgraph=True)
```

## é™åˆ¶

+   ç”±äº WÃ¼rstchen ä½¿ç”¨äº†é«˜åº¦å‹ç¼©ï¼Œç”Ÿæˆçš„å›¾åƒå¯èƒ½ç¼ºä¹å¤§é‡ç»†èŠ‚ã€‚å¯¹äºæˆ‘ä»¬çš„è‚‰çœ¼æ¥è¯´ï¼Œè¿™åœ¨é¢éƒ¨ã€æ‰‹éƒ¨ç­‰æ–¹é¢å°¤ä¸ºæ˜æ˜¾ã€‚

+   **å›¾åƒåªèƒ½ä»¥ 128 åƒç´ çš„æ­¥é•¿ç”Ÿæˆ**ï¼Œä¾‹å¦‚ï¼Œ1024x1024 ä¹‹åçš„ä¸‹ä¸€ä¸ªæ›´é«˜åˆ†è¾¨ç‡æ˜¯ 1152x1152

+   è¯¥æ¨¡å‹ç¼ºä¹åœ¨å›¾åƒä¸­æ­£ç¡®å‘ˆç°æ–‡æœ¬çš„èƒ½åŠ›

+   è¯¥æ¨¡å‹é€šå¸¸æ— æ³•å®ç°ç…§ç‰‡çº§é€¼çœŸåº¦

+   å¤æ‚çš„ç»„åˆæç¤ºå¯¹æ¨¡å‹æ¥è¯´å¾ˆéš¾

åŸå§‹ä»£ç åº“ä»¥åŠå®éªŒæ€§æƒ³æ³•å¯ä»¥åœ¨ [dome272/Wuerstchen](https://github.com/dome272/Wuerstchen) æ‰¾åˆ°ã€‚

## WuerstchenCombinedPipeline

### `class diffusers.WuerstchenCombinedPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L43)

```py
( tokenizer: CLIPTokenizer text_encoder: CLIPTextModel decoder: WuerstchenDiffNeXt scheduler: DDPMWuerstchenScheduler vqgan: PaellaVQModel prior_tokenizer: CLIPTokenizer prior_text_encoder: CLIPTextModel prior_prior: WuerstchenPrior prior_scheduler: DDPMWuerstchenScheduler )
```

å‚æ•°

+   `tokenizer` (`CLIPTokenizer`) â€” ç”¨äºæ–‡æœ¬è¾“å…¥çš„è§£ç å™¨åˆ†è¯å™¨ã€‚

+   `text_encoder` (`CLIPTextModel`) â€” ç”¨äºæ–‡æœ¬è¾“å…¥çš„è§£ç å™¨æ–‡æœ¬ç¼–ç å™¨ã€‚

+   `decoder` (`WuerstchenDiffNeXt`) â€” ç”¨äºè§£ç å™¨å›¾åƒç”Ÿæˆç®¡é“çš„è§£ç å™¨æ¨¡å‹ã€‚

+   `scheduler` (`DDPMWuerstchenScheduler`) â€” ç”¨äºè§£ç å™¨å›¾åƒç”Ÿæˆç®¡é“çš„è°ƒåº¦å™¨ã€‚

+   `vqgan` (`PaellaVQModel`) â€” ç”¨äºè§£ç å™¨å›¾åƒç”Ÿæˆç®¡é“çš„ VQGAN æ¨¡å‹ã€‚

+   `prior_tokenizer` (`CLIPTokenizer`) â€” ç”¨äºæ–‡æœ¬è¾“å…¥çš„å…ˆå‰åˆ†è¯å™¨ã€‚

+   `prior_text_encoder` (`CLIPTextModel`) â€” ç”¨äºæ–‡æœ¬è¾“å…¥çš„å…ˆå‰æ–‡æœ¬ç¼–ç å™¨ã€‚

+   `prior_prior` (`WuerstchenPrior`) â€” ç”¨äºå…ˆå‰ç®¡é“çš„å…ˆå‰æ¨¡å‹ã€‚

+   `prior_scheduler` (`DDPMWuerstchenScheduler`) â€” ç”¨äºå…ˆå‰ç®¡é“çš„è°ƒåº¦å™¨ã€‚

ä½¿ç”¨ Wuerstchen è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç»„åˆç®¡é“

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L143)

```py
( prompt: Union = None height: int = 512 width: int = 512 prior_num_inference_steps: int = 60 prior_timesteps: Optional = None prior_guidance_scale: float = 4.0 num_inference_steps: int = 12 decoder_timesteps: Optional = None decoder_guidance_scale: float = 0.0 negative_prompt: Union = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None num_images_per_prompt: int = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pil' return_dict: bool = True prior_callback_on_step_end: Optional = None prior_callback_on_step_end_tensor_inputs: List = ['latents'] callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs )
```

å‚æ•°

+   `prompt` (`str` or `List[str]`) â€” ç”¨äºæŒ‡å¯¼å…ˆå‰å’Œè§£ç å™¨å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚

+   `negative_prompt` (`str` or `List[str]`, *å¯é€‰*) â€” ä¸æŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœä¸ä½¿ç”¨æŒ‡å¯¼ï¼ˆå³å¦‚æœ `guidance_scale` å°äº `1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚

+   `prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” ç”¨äºå…ˆå‰çš„é¢„ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œæ–‡æœ¬åµŒå…¥å°†ä» `prompt` è¾“å…¥å‚æ•°ç”Ÿæˆã€‚

+   `negative_prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” ç”¨äºå…ˆå‰çš„é¢„ç”Ÿæˆè´Ÿé¢æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä» `negative_prompt` è¾“å…¥å‚æ•°ç”Ÿæˆè´Ÿé¢æç¤ºåµŒå…¥ã€‚

+   `num_images_per_prompt` (`int`, *optional*, é»˜è®¤ä¸º1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `height` (`int`, *optional*, é»˜è®¤ä¸º512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚

+   `width` (`int`, *optional*, é»˜è®¤ä¸º512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚

+   `prior_guidance_scale` (`float`, *optional*, é»˜è®¤ä¸º4.0) â€” å¦‚[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`prior_guidance_scale`å®šä¹‰ä¸º[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹2çš„`w`ã€‚é€šè¿‡è®¾ç½®`prior_guidance_scale > 1`æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚

+   `prior_num_inference_steps` (`Union[int, Dict[float, int]]`, *optional*, é»˜è®¤ä¸º60) â€” å…ˆéªŒå»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚è¦è·å¾—æ›´å…·ä½“çš„æ—¶é—´æ­¥é•¿é—´è·ï¼Œå¯ä»¥ä¼ é€’è‡ªå®šä¹‰çš„`prior_timesteps`ã€‚

+   `num_inference_steps` (`int`, *optional*, é»˜è®¤ä¸º12) â€” è§£ç å™¨å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚è¦è·å¾—æ›´å…·ä½“çš„æ—¶é—´æ­¥é•¿é—´è·ï¼Œå¯ä»¥ä¼ é€’è‡ªå®šä¹‰çš„`timesteps`ã€‚

+   `prior_timesteps` (`List[float]`, *optional*) â€” ç”¨äºå…ˆéªŒå»å™ªè¿‡ç¨‹çš„è‡ªå®šä¹‰æ—¶é—´æ­¥é•¿ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™ä½¿ç”¨ç­‰é—´è·çš„`prior_num_inference_steps`æ—¶é—´æ­¥é•¿ã€‚å¿…é¡»æŒ‰é™åºæ’åˆ—ã€‚

+   `decoder_timesteps` (`List[float]`, *optional*) â€” ç”¨äºè§£ç å™¨å»å™ªè¿‡ç¨‹çš„è‡ªå®šä¹‰æ—¶é—´æ­¥é•¿ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™ä½¿ç”¨ç­‰é—´è·çš„`num_inference_steps`æ—¶é—´æ­¥é•¿ã€‚å¿…é¡»æŒ‰é™åºæ’åˆ—ã€‚

+   `decoder_guidance_scale` (`float`, *optional*, é»˜è®¤ä¸º0.0) â€” å¦‚[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`guidance_scale`å®šä¹‰ä¸º[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹2çš„`w`ã€‚é€šè¿‡è®¾ç½®`guidance_scale > 1`æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚

+   `generator` (`torch.Generator`æˆ–`List[torch.Generator]`, *optional*) â€” ä¸€ä¸ªæˆ–å¤šä¸ª[torchç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ï¼Œç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚

+   `latents` (`torch.FloatTensor`, *optional*) â€” é¢„å…ˆç”Ÿæˆçš„å˜ˆæ‚æ½œåœ¨å‘é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºå¾®è°ƒç›¸åŒçš„ç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†ä½¿ç”¨æä¾›çš„éšæœº`generator`è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œåœ¨å‘é‡ã€‚

+   `output_type` (`str`, *optional*, é»˜è®¤ä¸º`"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚å¯åœ¨`"pil"`ï¼ˆ`PIL.Image.Image`ï¼‰ã€`"np"`ï¼ˆ`np.array`ï¼‰æˆ–`"pt"`ï¼ˆ`torch.Tensor`ï¼‰ä¹‹é—´é€‰æ‹©ã€‚

+   `return_dict` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦è¿”å›[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `prior_callback_on_step_end` (`Callable`, *optional*) â€” åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¯ä¸ªå»å™ªæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`prior_callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`ã€‚

+   `prior_callback_on_step_end_tensor_inputs` (`List`, *optional*) â€” `prior_callback_on_step_end`å‡½æ•°çš„å¼ é‡è¾“å…¥åˆ—è¡¨ã€‚åˆ—è¡¨ä¸­æŒ‡å®šçš„å¼ é‡å°†ä½œä¸º`callback_kwargs`å‚æ•°ä¼ é€’ã€‚æ‚¨åªèƒ½åŒ…å«åœ¨æ‚¨çš„ç®¡é“ç±»çš„`._callback_tensor_inputs`å±æ€§ä¸­åˆ—å‡ºçš„å˜é‡ã€‚

+   `callback_on_step_end` (`Callable`, *å¯é€‰*) â€” åœ¨æ¨æ–­æœŸé—´æ¯ä¸ªå»å™ªæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`ã€‚`callback_kwargs`å°†åŒ…æ‹¬ç”±`callback_on_step_end_tensor_inputs`æŒ‡å®šçš„æ‰€æœ‰å¼ é‡çš„åˆ—è¡¨ã€‚

+   `callback_on_step_end_tensor_inputs` (`List`, *å¯é€‰*) â€” `callback_on_step_end`å‡½æ•°çš„å¼ é‡è¾“å…¥åˆ—è¡¨ã€‚åˆ—è¡¨ä¸­æŒ‡å®šçš„å¼ é‡å°†ä½œä¸º`callback_kwargs`å‚æ•°ä¼ é€’ã€‚æ‚¨åªèƒ½åŒ…å«åœ¨ç®¡é“ç±»çš„`._callback_tensor_inputs`å±æ€§ä¸­åˆ—å‡ºçš„å˜é‡ã€‚

è°ƒç”¨ç®¡é“ä»¥è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from diffusions import WuerstchenCombinedPipeline

>>> pipe = WuerstchenCombinedPipeline.from_pretrained("warp-ai/Wuerstchen", torch_dtype=torch.float16).to(
...     "cuda"
... )
>>> prompt = "an image of a shiba inu, donning a spacesuit and helmet"
>>> images = pipe(prompt=prompt)
```

#### `enable_model_cpu_offload`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L115)

```py
( gpu_id = 0 )
```

ä½¿ç”¨åŠ é€Ÿå°†æ‰€æœ‰æ¨¡å‹è½¬ç§»åˆ°CPUï¼Œå‡å°‘å†…å­˜ä½¿ç”¨å¹¶å¯¹æ€§èƒ½å½±å“è¾ƒå°ã€‚ä¸`enable_sequential_cpu_offload`ç›¸æ¯”ï¼Œæ­¤æ–¹æ³•åœ¨è°ƒç”¨å…¶`forward`æ–¹æ³•æ—¶ä¸€æ¬¡å°†ä¸€ä¸ªå®Œæ•´æ¨¡å‹ç§»è‡³GPUï¼Œå¹¶ä¸”è¯¥æ¨¡å‹ä¿æŒåœ¨GPUä¸­ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹è¿è¡Œã€‚ä¸`enable_sequential_cpu_offload`ç›¸æ¯”ï¼Œå†…å­˜èŠ‚çœè¾ƒä½ï¼Œä½†ç”±äº`unet`çš„è¿­ä»£æ‰§è¡Œï¼Œæ€§èƒ½æ›´å¥½ã€‚

#### `enable_sequential_cpu_offload`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py#L125)

```py
( gpu_id = 0 )
```

ä½¿ç”¨ğŸ¤— Accelerateå°†æ‰€æœ‰æ¨¡å‹ï¼ˆ`unet`ã€`text_encoder`ã€`vae`å’Œ`safety checker`çŠ¶æ€å­—å…¸ï¼‰å…¨éƒ¨è½¬ç§»åˆ°CPUï¼Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨ã€‚æ¨¡å‹è¢«ç§»åŠ¨åˆ°`torch.device('meta')`ï¼Œä»…å½“è°ƒç”¨å…¶ç‰¹å®šå­æ¨¡å—çš„`forward`æ–¹æ³•æ—¶æ‰ä¼šåœ¨GPUä¸ŠåŠ è½½ã€‚è½¬ç§»æ˜¯åŸºäºå­æ¨¡å—çš„ã€‚å†…å­˜èŠ‚çœé«˜äºä½¿ç”¨`enable_model_cpu_offload`ï¼Œä½†æ€§èƒ½è¾ƒä½ã€‚

## WuerstchenPriorPipeline

### `class diffusers.WuerstchenPriorPipeline`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L65)

```py
( tokenizer: CLIPTokenizer text_encoder: CLIPTextModel prior: WuerstchenPrior scheduler: DDPMWuerstchenScheduler latent_mean: float = 42.0 latent_std: float = 1.0 resolution_multiple: float = 42.67 )
```

å‚æ•°

+   `prior` (`Prior`) â€” ç”¨äºä»æ–‡æœ¬åµŒå…¥é€¼è¿‘å›¾åƒåµŒå…¥çš„ç»å…¸unCLIPå…ˆéªŒã€‚

+   `text_encoder` (`CLIPTextModelWithProjection`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚

+   `tokenizer` (`CLIPTokenizer`) â€” ç±»[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)çš„åˆ†è¯å™¨ã€‚

+   `scheduler` (`DDPMWuerstchenScheduler`) â€” ä¸`prior`ç»“åˆä½¿ç”¨çš„è°ƒåº¦å™¨ï¼Œç”¨äºç”Ÿæˆå›¾åƒåµŒå…¥ã€‚

+   `latent_mean`ï¼ˆâ€˜floatâ€™, *å¯é€‰*, é»˜è®¤ä¸º42.0ï¼‰â€” æ½œåœ¨æ‰©æ•£çš„å‡å€¼ã€‚

+   `latent_std`ï¼ˆâ€˜floatâ€™, *å¯é€‰*, é»˜è®¤ä¸º1.0ï¼‰â€” æ½œåœ¨æ‰©æ•£çš„æ ‡å‡†å€¼ã€‚

+   `resolution_multiple`ï¼ˆâ€˜floatâ€™, *å¯é€‰*, é»˜è®¤ä¸º42.67ï¼‰â€” ç”Ÿæˆå¤šä¸ªå›¾åƒçš„é»˜è®¤åˆ†è¾¨ç‡ã€‚

ç”¨äºç”ŸæˆWuerstchenå›¾åƒå…ˆéªŒçš„ç®¡é“ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

è¯¥ç®¡é“è¿˜ç»§æ‰¿äº†ä»¥ä¸‹åŠ è½½æ–¹æ³•ï¼š

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) ç”¨äºåŠ è½½LoRAæƒé‡

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) ç”¨äºä¿å­˜LoRAæƒé‡

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L280)

```py
( prompt: Union = None height: int = 1024 width: int = 1024 num_inference_steps: int = 60 timesteps: List = None guidance_scale: float = 8.0 negative_prompt: Union = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None num_images_per_prompt: Optional = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pt' return_dict: bool = True callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs )
```

å‚æ•°

+   `prompt` (`str` æˆ– `List[str]`) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚

+   `height` (`int`, *å¯é€‰*, é»˜è®¤ä¸º1024) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚

+   `width` (`int`, *optional*, defaults to 1024) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚

+   `num_inference_steps` (`int`, *optional*, defaults to 60) â€” å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†æ¨ç†é€Ÿåº¦ä¼šå˜æ…¢ã€‚

+   `timesteps` (`List[int]`, *optional*) â€” ç”¨äºå»å™ªè¿‡ç¨‹çš„è‡ªå®šä¹‰æ—¶é—´æ­¥ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™ä½¿ç”¨ç­‰é—´è·çš„ `num_inference_steps` ä¸ªæ—¶é—´æ­¥ã€‚å¿…é¡»æŒ‰é™åºæ’åˆ—ã€‚

+   `guidance_scale` (`float`, *optional*, defaults to 8.0) â€” å¦‚ [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598) ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`decoder_guidance_scale` å®šä¹‰ä¸º [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf) æ–¹ç¨‹å¼ 2 çš„ `w`ã€‚é€šè¿‡è®¾ç½® `decoder_guidance_scale > 1` å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚

+   `negative_prompt` (`str` or `List[str]`, *optional*) â€” ä¸ç”¨æ¥å¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å½“ä¸ä½¿ç”¨å¼•å¯¼æ—¶ï¼ˆå³ï¼Œå¦‚æœ `decoder_guidance_scale` å°äº `1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚è°ƒæ•´æç¤ºæƒé‡ã€‚å¦‚æœæœªæä¾›ï¼Œæ–‡æœ¬åµŒå…¥å°†ä» `prompt` è¾“å…¥å‚æ•°ä¸­ç”Ÿæˆã€‚

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚è°ƒæ•´æç¤ºæƒé‡ã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä» `negative_prompt` è¾“å…¥å‚æ•°ä¸­ç”Ÿæˆè´Ÿæ–‡æœ¬åµŒå…¥ã€‚

+   `num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” ä¸€ä¸ªæˆ–å¤šä¸ª [torch ç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ï¼Œç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚

+   `latents` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºè°ƒæ•´ç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä½¿ç”¨æä¾›çš„éšæœº `generator` è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚

+   `output_type` (`str`, *optional*, defaults to `"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚å¯é€‰æ‹©çš„æ ¼å¼åŒ…æ‹¬ï¼š`"pil"` (`PIL.Image.Image`)ã€`"np"` (`np.array`) æˆ– `"pt"` (`torch.Tensor`)ã€‚

+   `return_dict` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

+   `callback_on_step_end` (`Callable`, *optional*) â€” åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¯ä¸ªå»å™ªæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`ã€‚`callback_kwargs` å°†åŒ…æ‹¬ç”± `callback_on_step_end_tensor_inputs` æŒ‡å®šçš„æ‰€æœ‰å¼ é‡çš„åˆ—è¡¨ã€‚

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) â€” `callback_on_step_end` å‡½æ•°çš„å¼ é‡è¾“å…¥åˆ—è¡¨ã€‚åˆ—è¡¨ä¸­æŒ‡å®šçš„å¼ é‡å°†ä½œä¸º `callback_kwargs` å‚æ•°ä¼ é€’ã€‚æ‚¨åªèƒ½åŒ…å«åœ¨æ‚¨çš„ç®¡é“ç±»çš„ `._callback_tensor_inputs` å±æ€§ä¸­åˆ—å‡ºçš„å˜é‡ã€‚

è°ƒç”¨ç®¡é“è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from diffusers import WuerstchenPriorPipeline

>>> prior_pipe = WuerstchenPriorPipeline.from_pretrained(
...     "warp-ai/wuerstchen-prior", torch_dtype=torch.float16
... ).to("cuda")

>>> prompt = "an image of a shiba inu, donning a spacesuit and helmet"
>>> prior_output = pipe(prompt)
```

## WuerstchenPriorPipelineOutput

### `class diffusers.pipelines.wuerstchen.pipeline_wuerstchen_prior.WuerstchenPriorPipelineOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py#L51)

```py
( image_embeddings: Union )
```

å‚æ•°

+   `image_embeddings` (`torch.FloatTensor` or `np.ndarray`) â€” ç”¨äºæ–‡æœ¬æç¤ºçš„å…ˆéªŒå›¾åƒåµŒå…¥ã€‚

WuerstchenPriorPipeline çš„è¾“å‡ºç±»ã€‚

## WuerstchenDecoderPipeline

### `class diffusers.WuerstchenDecoderPipeline`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen.py#L51)

```py
( tokenizer: CLIPTokenizer text_encoder: CLIPTextModel decoder: WuerstchenDiffNeXt scheduler: DDPMWuerstchenScheduler vqgan: PaellaVQModel latent_dim_scale: float = 10.67 )
```

å‚æ•°

+   `tokenizer` (`CLIPTokenizer`) â€” CLIP åˆ†è¯å™¨ã€‚

+   `text_encoder` (`CLIPTextModel`) â€” CLIP æ–‡æœ¬ç¼–ç å™¨ã€‚

+   `decoder` (`WuerstchenDiffNeXt`) â€” WuerstchenDiffNeXt unet è§£ç å™¨ã€‚

+   `vqgan` (`PaellaVQModel`) â€” VQGAN æ¨¡å‹ã€‚

+   `scheduler` (`DDPMWuerstchenScheduler`) â€” ç”¨äºä¸`prior`ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒåµŒå…¥çš„è°ƒåº¦å™¨ã€‚

+   `latent_dim_scale` (float, *å¯é€‰*, é»˜è®¤ä¸º10.67) â€” ä»å›¾åƒåµŒå…¥ç¡®å®š VQ æ½œå˜é‡ç©ºé—´å¤§å°çš„ä¹˜æ•°ã€‚å¦‚æœå›¾åƒåµŒå…¥çš„é«˜åº¦=24ï¼Œå®½åº¦=24ï¼Œåˆ™ VQ æ½œå˜é‡å½¢çŠ¶éœ€è¦ä¸ºé«˜åº¦=int(24*10.67)=256ï¼Œå®½åº¦=int(24*10.67)=256ï¼Œä»¥åŒ¹é…è®­ç»ƒæ¡ä»¶ã€‚

ç”¨äºä» Wuerstchen æ¨¡å‹ç”Ÿæˆå›¾åƒçš„ç®¡é“ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen.py#L208)

```py
( image_embeddings: Union prompt: Union = None num_inference_steps: int = 12 timesteps: Optional = None guidance_scale: float = 0.0 negative_prompt: Union = None num_images_per_prompt: int = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs )
```

å‚æ•°

+   `image_embedding` (`torch.FloatTensor` æˆ– `List[torch.FloatTensor]`) â€” å›¾åƒåµŒå…¥ï¼Œå¯ä»¥æ˜¯ä»å›¾åƒä¸­æå–çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯ç”±å…ˆéªŒæ¨¡å‹ç”Ÿæˆçš„ã€‚

+   `prompt` (`str` æˆ– `List[str]`) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºã€‚

+   `num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º12) â€” å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚

+   `timesteps` (`List[int]`, *å¯é€‰*) â€” ç”¨äºå»å™ªè¿‡ç¨‹çš„è‡ªå®šä¹‰æ—¶é—´æ­¥ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™ä½¿ç”¨ç­‰é—´éš”çš„`num_inference_steps`æ—¶é—´æ­¥ã€‚å¿…é¡»æŒ‰é™åºæ’åˆ—ã€‚

+   `guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.0) â€” å¦‚[æ— åˆ†ç±»å™¨æ‰©æ•£æŒ‡å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„æŒ‡å¯¼æ¯”ä¾‹ã€‚`decoder_guidance_scale`å®šä¹‰ä¸º[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹2çš„`w`ã€‚é€šè¿‡è®¾ç½®`decoder_guidance_scale > 1`å¯ç”¨æŒ‡å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„æŒ‡å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚

+   `negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºã€‚å¦‚æœä¸ä½¿ç”¨æŒ‡å¯¼ï¼ˆå³å¦‚æœ`decoder_guidance_scale`å°äº`1`ï¼‰ï¼Œåˆ™å¿½ç•¥ã€‚

+   `num_images_per_prompt` (`int`, *å¯é€‰*, é»˜è®¤ä¸º1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *å¯é€‰*) â€” ä¸€ä¸ªæˆ–å¤šä¸ª[torch ç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ï¼Œç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚

+   `latents` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„å…ˆç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºå¾®è°ƒç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†ä½¿ç”¨æä¾›çš„éšæœº`generator`è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚

+   `output_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚å¯é€‰æ‹© `"pil"` (`PIL.Image.Image`)ã€`"np"` (`np.array`) æˆ– `"pt"` (`torch.Tensor`)ã€‚

+   `return_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦è¿”å› [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `callback_on_step_end`ï¼ˆ`Callable`ï¼Œ*å¯é€‰*ï¼‰â€”åœ¨æ¨æ–­è¿‡ç¨‹ä¸­æ¯ä¸ªå»å™ªæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`ã€‚`callback_kwargs`å°†åŒ…æ‹¬ç”±`callback_on_step_end_tensor_inputs`æŒ‡å®šçš„æ‰€æœ‰å¼ é‡çš„åˆ—è¡¨ã€‚

+   `callback_on_step_end_tensor_inputs`ï¼ˆ`List`ï¼Œ*å¯é€‰*ï¼‰â€”`callback_on_step_end`å‡½æ•°çš„å¼ é‡è¾“å…¥åˆ—è¡¨ã€‚åˆ—è¡¨ä¸­æŒ‡å®šçš„å¼ é‡å°†ä½œä¸º`callback_kwargs`å‚æ•°ä¼ é€’ã€‚æ‚¨åªèƒ½åŒ…å«åœ¨æ‚¨çš„ç®¡é“ç±»çš„`._callback_tensor_inputs`å±æ€§ä¸­åˆ—å‡ºçš„å˜é‡ã€‚

åœ¨è°ƒç”¨ç®¡é“ç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from diffusers import WuerstchenPriorPipeline, WuerstchenDecoderPipeline

>>> prior_pipe = WuerstchenPriorPipeline.from_pretrained(
...     "warp-ai/wuerstchen-prior", torch_dtype=torch.float16
... ).to("cuda")
>>> gen_pipe = WuerstchenDecoderPipeline.from_pretrain("warp-ai/wuerstchen", torch_dtype=torch.float16).to(
...     "cuda"
... )

>>> prompt = "an image of a shiba inu, donning a spacesuit and helmet"
>>> prior_output = pipe(prompt)
>>> images = gen_pipe(prior_output.image_embeddings, prompt=prompt)
```

## å¼•ç”¨

```py
      @misc{pernias2023wuerstchen,
            title={Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models},
            author={Pablo Pernias and Dominic Rampas and Mats L. Richter and Christopher J. Pal and Marc Aubreville},
            year={2023},
            eprint={2306.00637},
            archivePrefix={arXiv},
            primaryClass={cs.CV}
      }
```
