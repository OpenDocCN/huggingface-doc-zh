["```py\n( vision_config = None qformer_config = None text_config = None num_query_tokens = 32 **kwargs )\n```", "```py\n>>> from transformers import (\n...     InstructBlipVisionConfig,\n...     InstructBlipQFormerConfig,\n...     OPTConfig,\n...     InstructBlipConfig,\n...     InstructBlipForConditionalGeneration,\n... )\n\n>>> # Initializing a InstructBlipConfig with Salesforce/instruct-blip-flan-t5 style configuration\n>>> configuration = InstructBlipConfig()\n\n>>> # Initializing a InstructBlipForConditionalGeneration (with random weights) from the Salesforce/instruct-blip-flan-t5 style configuration\n>>> model = InstructBlipForConditionalGeneration(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a InstructBlipConfig from a InstructBlipVisionConfig, InstructBlipQFormerConfig and any PretrainedConfig\n\n>>> # Initializing InstructBLIP vision, InstructBLIP Q-Former and language model configurations\n>>> vision_config = InstructBlipVisionConfig()\n>>> qformer_config = InstructBlipQFormerConfig()\n>>> text_config = OPTConfig()\n\n>>> config = InstructBlipConfig.from_text_vision_configs(vision_config, qformer_config, text_config)\n```", "```py\n( vision_config: InstructBlipVisionConfig qformer_config: InstructBlipQFormerConfig text_config: PretrainedConfig **kwargs ) \u2192 export const metadata = 'undefined';InstructBlipConfig\n```", "```py\n( hidden_size = 1408 intermediate_size = 6144 num_hidden_layers = 39 num_attention_heads = 16 image_size = 224 patch_size = 14 hidden_act = 'gelu' layer_norm_eps = 1e-06 attention_dropout = 0.0 initializer_range = 1e-10 qkv_bias = True **kwargs )\n```", "```py\n>>> from transformers import InstructBlipVisionConfig, InstructBlipVisionModel\n\n>>> # Initializing a InstructBlipVisionConfig with Salesforce/instruct-blip-flan-t5 style configuration\n>>> configuration = InstructBlipVisionConfig()\n\n>>> # Initializing a InstructBlipVisionModel (with random weights) from the Salesforce/instruct-blip-flan-t5 style configuration\n>>> model = InstructBlipVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 position_embedding_type = 'absolute' cross_attention_frequency = 2 encoder_hidden_size = 1408 **kwargs )\n```", "```py\n>>> from transformers import InstructBlipQFormerConfig, InstructBlipQFormerModel\n\n>>> # Initializing a InstructBLIP Salesforce/instruct-blip-flan-t5 style configuration\n>>> configuration = InstructBlipQFormerConfig()\n\n>>> # Initializing a model (with random weights) from the Salesforce/instruct-blip-flan-t5 style configuration\n>>> model = InstructBlipQFormerModel(configuration)\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( image_processor tokenizer qformer_tokenizer )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config: InstructBlipVisionConfig )\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n( config: InstructBlipQFormerConfig )\n```", "```py\n( input_ids: LongTensor attention_mask: Optional = None position_ids: Optional = None query_embeds: Optional = None head_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( config: InstructBlipConfig )\n```", "```py\n( pixel_values: FloatTensor qformer_input_ids: FloatTensor qformer_attention_mask: Optional = None input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.instructblip.modeling_instructblip.InstructBlipForConditionalGenerationModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n>>> import torch\n>>> from PIL import Image\n>>> import requests\n\n>>> model = InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n>>> processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model.to(device)\n>>> url = \"https://raw.githubusercontent.com/salesforce/LAVIS/main/docs/_static/Confusing-Pictures.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n>>> prompt = \"What is unusual about this image?\"\n>>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n\n>>> outputs = model.generate(\n...     **inputs,\n...     do_sample=False,\n...     num_beams=5,\n...     max_length=256,\n...     min_length=1,\n...     top_p=0.9,\n...     repetition_penalty=1.5,\n...     length_penalty=1.0,\n...     temperature=1,\n... )\n>>> generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n>>> print(generated_text)\nThe unusual aspect of this image is that a man is ironing clothes on the back of a yellow SUV, which is parked in the middle of a busy city street. This is an unconventional approach to ironing clothes, as it requires the man to balance himself and his ironing equipment on top of the vehicle while navigating through traffic. Additionally, the presence of taxis and other vehicles in the scene further emphasizes the unusual nature of this situation.\n```", "```py\n( pixel_values: FloatTensor qformer_input_ids: Optional = None qformer_attention_mask: Optional = None input_ids: Optional = None attention_mask: Optional = None **generate_kwargs ) \u2192 export const metadata = 'undefined';captions (list)\n```"]