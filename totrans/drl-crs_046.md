# 深度Q网络（DQN）

> 原文链接：[https://huggingface.co/learn/deep-rl-course/unit3/deep-q-network](https://huggingface.co/learn/deep-rl-course/unit3/deep-q-network)

这是我们深度Q学习网络的架构：

![深度Q网络](../Images/ae1d15faad114a179990b8f1f33104d4.png)

作为输入，我们将通过网络传递的4个帧堆叠在一起作为状态，并输出该状态下每个可能动作的Q值向量。然后，就像Q学习一样，我们只需要使用epsilon-greedy策略来选择要采取的动作。

当神经网络初始化时，Q值的估计很糟糕。但在训练过程中，我们的深度Q网络代理将把一种情况与适当的动作联系起来，并学会玩游戏。

## 对输入进行预处理和时间限制。

我们需要对输入进行预处理。这是一个必不可少的步骤，因为我们希望减少状态的复杂性，以减少训练所需的计算时间。

为了实现这一点，我们将状态空间减少到84x84并将其灰度化。我们可以这样做，因为Atari环境中的颜色不会添加重要信息。这是一个很大的改进，因为我们将我们的三个颜色通道（RGB）减少到1。

在某些游戏中，如果屏幕上没有重要信息，我们也可以裁剪一部分。然后我们将四个帧堆叠在一起。

![预处理](../Images/30478bfb5d8bf377a5c066b55e0d3f0d.png)

为什么我们要将四个帧堆叠在一起？我们将帧堆叠在一起是因为它帮助我们处理时间限制的问题。让我们以乒乓球游戏为例。当你看到这个帧时：

![时间限制](../Images/916225d18ad696514245f8c4e88a5a56.png)

你能告诉我球要去哪里吗？不行，因为一个帧不足以让我们感受到运动！但如果我再加三个帧呢？在这里你可以看到球要往右边走。

![时间限制](../Images/e35a3e3cfeefe6f7a16b681ab91dfa7b.png) 这就是为什么为了捕捉时间信息，我们将四个帧堆叠在一起。

然后，堆叠的帧经过三个卷积层处理。这些层使我们能够捕捉和利用图像中的空间关系。但也因为帧被堆叠在一起，我们可以利用这些帧之间的一些时间属性。

如果你不知道什么是卷积层，不用担心。你可以查看Udacity的这门免费深度学习课程的第4课。

最后，我们有几个全连接层，为该状态下的每个可能动作输出一个Q值。

![深度Q网络](../Images/ae1d15faad114a179990b8f1f33104d4.png)

因此，我们看到深度Q学习使用神经网络来近似，在给定状态时，该状态下每个可能动作的不同Q值。现在让我们来研究深度Q学习算法。
