- en: CPMAnt
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CPMAnt
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cpmant](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cpmant)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cpmant](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cpmant)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: CPM-Ant is an open-source Chinese pre-trained language model (PLM) with 10B
    parameters. It is also the first milestone of the live training process of CPM-Live.
    The training process is cost-effective and environment-friendly. CPM-Ant also
    achieves promising results with delta tuning on the CUGE benchmark. Besides the
    full model, we also provide various compressed versions to meet the requirements
    of different hardware configurations. [See more](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: CPM-Ant是一个拥有10B参数的开源中文预训练语言模型（PLM）。它也是CPM-Live实时训练过程的第一个里程碑。训练过程具有成本效益且环保。CPM-Ant在CUGE基准测试中通过增量调整取得了令人满意的结果。除了完整模型，我们还提供各种压缩版本以满足不同硬件配置的要求。[查看更多](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live)
- en: This model was contributed by [OpenBMB](https://huggingface.co/openbmb). The
    original code can be found [here](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[OpenBMB](https://huggingface.co/openbmb)贡献。原始代码可在[此处](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live)找到。
- en: Resources
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A tutorial on [CPM-Live](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live).
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个关于[CPM-Live](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live)的教程。
- en: CpmAntConfig
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CpmAntConfig
- en: '### `class transformers.CpmAntConfig`'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CpmAntConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/configuration_cpmant.py#L29)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/configuration_cpmant.py#L29)'
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 30720) — Vocabulary size of the
    CPMAnt model. Defines the number of different tokens that can be represented by
    the `input` passed when calling [CpmAntModel](/docs/transformers/v4.37.2/en/model_doc/cpmant#transformers.CpmAntModel).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *可选*, 默认为30720) — CPMAnt模型的词汇量。定义调用[CpmAntModel](/docs/transformers/v4.37.2/en/model_doc/cpmant#transformers.CpmAntModel)时传递的`input`中可以表示的不同标记数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 4096) — Dimension of the encoder
    layers.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *可选*, 默认为4096) — 编码器层的维度。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 32) — Number of attention
    heads in the Transformer encoder.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *可选*, 默认为32) — Transformer编码器中的注意力头数。'
- en: '`dim_head` (`int`, *optional*, defaults to 128) — Dimension of attention heads
    for each attention layer in the Transformer encoder.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim_head` (`int`, *可选*, 默认为128) — Transformer编码器中每个注意力层的注意力头维度。'
- en: '`dim_ff` (`int`, *optional*, defaults to 10240) — Dimension of the “intermediate”
    (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim_ff` (`int`, *可选*, 默认为10240) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 48) — Number of layers
    of the Transformer encoder.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *可选*, 默认为48) — Transformer编码器的层数。'
- en: '`dropout_p` (`float`, *optional*, defaults to 0.0) — The dropout probabilitiy
    for all fully connected layers in the embeddings, encoder.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout_p` (`float`, *可选*, 默认为0.0) — 嵌入层、编码器中所有全连接层的dropout概率。'
- en: '`position_bias_num_buckets` (`int`, *optional*, defaults to 512) — The number
    of position_bias buckets.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_bias_num_buckets` (`int`, *可选*, 默认为512) — 位置偏置桶的数量。'
- en: '`position_bias_max_distance` (`int`, *optional*, defaults to 2048) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_bias_max_distance` (`int`, *可选*, 默认为2048) — 该模型可能被使用的最大序列长度。通常设置为一个较大的值以防万一（例如512、1024或2048）。'
- en: '`eps` (`float`, *optional*, defaults to 1e-06) — The epsilon used by the layer
    normalization layers.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps` (`float`, *可选*, 默认为1e-06) — 层归一化层使用的epsilon。'
- en: '`init_std` (`float`, *optional*, defaults to 1.0) — Initialize parameters with
    std = init_std.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_std` (`float`, *可选*, 默认为1.0) — 使用std = init_std初始化参数。'
- en: '`prompt_types` (`int`, *optional*, defaults to 32) — The type of prompt.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_types` (`int`, *可选*, 默认为32) — 提示类型。'
- en: '`prompt_length` (`int`, *optional*, defaults to 32) — The length of prompt.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_length` (`int`, *可选*, 默认为32) — 提示的长度。'
- en: '`segment_types` (`int`, *optional*, defaults to 32) — The type of segment.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segment_types` (`int`, *可选*, 默认为32) — 分段类型。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether to use cache.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *可选*, 默认为`True`) — 是否使用缓存。'
- en: This is the configuration class to store the configuration of a [CpmAntModel](/docs/transformers/v4.37.2/en/model_doc/cpmant#transformers.CpmAntModel).
    It is used to instantiate an CPMAnt model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the CPMAnt [openbmb/cpm-ant-10b](https://huggingface.co/openbmb/cpm-ant-10b)
    architecture.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[CpmAntModel](/docs/transformers/v4.37.2/en/model_doc/cpmant#transformers.CpmAntModel)的配置。根据指定的参数实例化一个CPMAnt模型，定义模型架构。使用默认值实例化配置将产生类似于CPMAnt
    [openbmb/cpm-ant-10b](https://huggingface.co/openbmb/cpm-ant-10b)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: CpmAntTokenizer
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CpmAntTokenizer
- en: '### `class transformers.CpmAntTokenizer`'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CpmAntTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/tokenization_cpmant.py#L88)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/tokenization_cpmant.py#L88)'
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇文件的路径。'
- en: '`bod_token` (`str`, *optional*, defaults to `"<d>"`) — The beginning of document
    token.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bod_token`（`str`，*可选*，默认为`"<d>"`）— 文档起始标记。'
- en: '`eod_token` (`str`, *optional*, defaults to `"</d>"`) — The end of document
    token.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eod_token`（`str`，*可选*，默认为`"</d>"`）— 文档结束标记。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`，*可选*，默认为`"<s>"`）— 序列起始标记。'
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`（`str`，*可选*，默认为`"</s>"`）— 序列结束标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token`（`str`，*可选*，默认为`"<pad>"`）— 用于填充的标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*可选*，默认为`"<unk>"`）— 未知标记。'
- en: '`line_token` (`str`, *optional*, defaults to `"</n>"`) — The line token.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`line_token`（`str`，*可选*，默认为`"</n>"`）— 行标记。'
- en: '`space_token` (`str`, *optional*, defaults to `"</_>"`) — The space token.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`space_token`（`str`，*可选*，默认为`"</_>"`）— 空格标记。'
- en: Construct a CPMAnt tokenizer. Based on byte-level Byte-Pair-Encoding.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个CPMAnt分词器。基于字节级字节对编码。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/tokenization_cpmant.py#L236)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/tokenization_cpmant.py#L236)'
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — The first tokenized sequence that special tokens
    will be added.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）— 将添加特殊标记的第一个标记化序列。'
- en: '`token_ids_1` (`List[int]`) — The optional second tokenized sequence that special
    tokens will be added.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`）— 将添加特殊标记的可选第二个标记化序列。'
- en: Returns
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: The model input with special tokens.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 带有特殊标记的模型输入。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. A CPMAnt sequence has the following
    format:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记，从序列或序列对构建用于序列分类任务的模型输入。CPMAnt序列的格式如下：
- en: 'single sequence: `[BOS] Sequence`.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个序列：`[BOS] Sequence`。
- en: '#### `get_special_tokens_mask`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/tokenization_cpmant.py#L254)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/tokenization_cpmant.py#L254)'
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）— ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens`（`bool`，*可选*，默认为`False`）— 标记列表是否已经格式化为模型的特殊标记。'
- en: Returns
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一个整数列表，范围为[0, 1]：1表示特殊标记，0表示序列标记。
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从没有添加特殊标记的标记列表中检索序列ID。在使用分词器`prepare_for_model`方法添加特殊标记时调用此方法。
- en: CpmAntModel
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CpmAntModel
- en: '### `class transformers.CpmAntModel`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CpmAntModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/modeling_cpmant.py#L594)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/modeling_cpmant.py#L594)'
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The bare CPMAnt Model outputting raw hidden-states without any specific head
    on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 裸CPMAnt模型输出原始隐藏状态，没有特定的头部。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。
- en: 'Parameters config ([~CpmAntConfig](/docs/transformers/v4.37.2/en/model_doc/cpmant#transformers.CpmAntConfig)):
    Model configuration class with all the parameters of the Initializing with a config
    file does not load the weights associated with the model, only the configuration.
    Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 参数配置（[~CpmAntConfig](/docs/transformers/v4.37.2/en/model_doc/cpmant#transformers.CpmAntConfig)）：具有所有初始化参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。
- en: '#### `forward`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/modeling_cpmant.py#L636)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/modeling_cpmant.py#L636)'
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.Tensor` of shape `(batch_size, seq_len)`) — Indices of
    input sequence tokens in the vocabulary.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, seq_len)`的`torch.Tensor`）— 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using `CPMAntTokenizer`. See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用`CPMAntTokenizer`获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Contains pre-computed
    hidden-states (key and values in the self-attention blocks and in the cross-attention
    blocks) that can be used (see `past_key_values` input) to speed up sequential
    decoding.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递了`use_cache=True`或者当`config.use_cache=True`时返回）
    — 包含预计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*） — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*） — 是否返回所有注意力层的注意力张量。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*） — 是否返回所有层的隐藏状态。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*） — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast)
    or `tuple(torch.FloatTensor)`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.BaseModelOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast)或者`tuple(torch.FloatTensor)`
- en: A [transformers.modeling_outputs.BaseModelOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CpmAntConfig](/docs/transformers/v4.37.2/en/model_doc/cpmant#transformers.CpmAntConfig))
    and inputs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.BaseModelOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast)或者一个`torch.FloatTensor`的元组（如果传递了`return_dict=False`或者当`config.return_dict=False`时），包括各种元素，取决于配置（[CpmAntConfig](/docs/transformers/v4.37.2/en/model_doc/cpmant#transformers.CpmAntConfig)）和输入。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）
    — 模型最后一层的隐藏状态序列。'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果仅使用`past_key_values`，则输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and optionally if `config.is_encoder_decoder=True`
    2 additional tensors of shape `(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递了`use_cache=True`或者当`config.use_cache=True`时返回）
    — 长度为`config.n_layers`的元组`tuple(torch.FloatTensor)`，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量，并且如果`config.is_encoder_decoder=True`，还有2个额外的形状为`(batch_size,
    num_heads, encoder_sequence_length, embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and optionally if `config.is_encoder_decoder=True` in the cross-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预计算的隐藏状态（自注意力块中的键和值，以及如果`config.is_encoder_decoder=True`，则包含交叉注意力块中的键和值），可用于加速顺序解码。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递了`output_hidden_states=True`或者当`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层的输出隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递了`output_attentions=True`或者当`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重经过注意力softmax后，用于计算自注意力头中的加权平均值。
- en: The [CpmAntModel](/docs/transformers/v4.37.2/en/model_doc/cpmant#transformers.CpmAntModel)
    forward method, overrides the `__call__` special method.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[CpmAntModel](/docs/transformers/v4.37.2/en/model_doc/cpmant#transformers.CpmAntModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: CpmAntForCausalLM
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CpmAntForCausalLM
- en: '### `class transformers.CpmAntForCausalLM`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CpmAntForCausalLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/modeling_cpmant.py#L739)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/modeling_cpmant.py#L739)'
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The CPMAnt Model with a language modeling head on top (linear layer with weights
    tied to the input embeddings).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: CPMAnt模型在顶部带有语言建模头（线性层，其权重与输入嵌入绑定）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。
- en: 'Parameters config ([~CpmAntConfig](/docs/transformers/v4.37.2/en/model_doc/cpmant#transformers.CpmAntConfig)):
    Model configuration class with all the parameters of the Initializing with a config
    file does not load the weights associated with the model, only the configuration.
    Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 参数配置（[~CpmAntConfig](/docs/transformers/v4.37.2/en/model_doc/cpmant#transformers.CpmAntConfig)）：模型配置类，包含所有初始化参数。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。
- en: '#### `forward`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/modeling_cpmant.py#L758)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/modeling_cpmant.py#L758)'
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.Tensor` of shape `(batch_size, seq_len)`) — Indices of
    input sequence tokens in the vocabulary.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为（batch_size，seq_len）的`torch.Tensor`） - 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using `CPMAntTokenizer`. See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用`CPMAntTokenizer`获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Contains pre-computed
    hidden-states (key and values in the self-attention blocks and in the cross-attention
    blocks) that can be used (see `past_key_values` input) to speed up sequential
    decoding.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）
    - 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*） - 如果设置为`True`，将返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*） - 是否返回所有注意力层的注意力张量。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*） - 是否返回所有层的隐藏状态。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*） - 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: 'Args — input_ids (`torch.Tensor` of shape `(batch_size, seq_len)`): Indices
    of input sequence tokens in the vocabulary.'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数 - 输入ID（形状为（batch_size，seq_len）的`torch.Tensor`）：词汇表中输入序列标记的索引。
- en: Indices can be obtained using `CPMAntTokenizer`. See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用`CPMAntTokenizer`获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids) past_key_values (`tuple(tuple(torch.FloatTensor))`,
    *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
    Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding. use_cache (`bool`, *optional*): If set to `True`,
    `past_key_values` key value states are returned and can be used to speed up decoding
    (see `past_key_values`). output_attentions (`bool`, *optional*): Whether or not
    to return the attentions tensors of all attention layers. output_hidden_states
    (`bool`, *optional*): Whether or not to return the hidden states of all layers.
    labels (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
    Labels for computing the masked language modeling loss. return_dict (`bool`, *optional*):
    Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. attention_mask (`torch.Tensor` of shape `(batch_size,
    sequence_length)`, *optional*): CPMAnt will process attention mask automatically,
    this parameter is a dummy parameter for text-generation pipeline.'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids) `past_key_values` (`tuple(tuple(torch.FloatTensor))`,
    *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回：包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。
    `use_cache` (`bool`, *optional*): 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。
    `output_attentions` (`bool`, *optional*): 是否返回所有注意力层的注意力张量。 `output_hidden_states`
    (`bool`, *optional*): 是否返回所有层的隐藏状态。 `labels` (`torch.Tensor`，形状为`(batch_size,
    sequence_length)`，*optional*): 用于计算掩码语言建模损失的标签。 `return_dict` (`bool`，*optional*):
    是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。
    `attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*optional*):
    CPMAnt将自动处理注意力掩码，此参数是文本生成流水线的虚拟参数。'
- en: Example —
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例 —
- en: '`Text` Generation with CpmAntForCausalLM. —'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CpmAntForCausalLM进行文本生成。 —
- en: Returns
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)
    or `tuple(torch.FloatTensor)`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)或`torch.FloatTensor`元组。'
- en: A [transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CpmAntConfig](/docs/transformers/v4.37.2/en/model_doc/cpmant#transformers.CpmAntConfig))
    and inputs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[CpmAntConfig](/docs/transformers/v4.37.2/en/model_doc/cpmant#transformers.CpmAntConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回) — 语言建模损失（用于下一个标记预测）。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回
    — 长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [CpmAntForCausalLM](/docs/transformers/v4.37.2/en/model_doc/cpmant#transformers.CpmAntForCausalLM)
    forward method, overrides the `__call__` special method.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[CpmAntForCausalLM](/docs/transformers/v4.37.2/en/model_doc/cpmant#transformers.CpmAntForCausalLM)
    前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传播的配方需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者负责运行预处理和后处理步骤，而后者则会默默地忽略它们。
- en: 'Example:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
