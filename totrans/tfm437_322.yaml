- en: VITS
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VITS
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vits](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vits)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vits](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vits)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The VITS model was proposed in [Conditional Variational Autoencoder with Adversarial
    Learning for End-to-End Text-to-Speech](https://arxiv.org/abs/2106.06103) by Jaehyeon
    Kim, Jungil Kong, Juhee Son.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: VITS模型是由Jaehyeon Kim，Jungil Kong，Juhee Son在[端到端文本到语音的条件变分自动编码器与对抗学习](https://arxiv.org/abs/2106.06103)中提出的。
- en: VITS (**V**ariational **I**nference with adversarial learning for end-to-end
    **T**ext-to-**S**peech) is an end-to-end speech synthesis model that predicts
    a speech waveform conditional on an input text sequence. It is a conditional variational
    autoencoder (VAE) comprised of a posterior encoder, decoder, and conditional prior.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: VITS（端到端文本到语音的变分推断与对抗学习）是一种端到端语音合成模型，根据输入文本序列预测语音波形。它是由后验编码器、解码器和条件先验组成的条件变分自动编码器（VAE）。
- en: A set of spectrogram-based acoustic features are predicted by the flow-based
    module, which is formed of a Transformer-based text encoder and multiple coupling
    layers. The spectrogram is decoded using a stack of transposed convolutional layers,
    much in the same style as the HiFi-GAN vocoder. Motivated by the one-to-many nature
    of the TTS problem, where the same text input can be spoken in multiple ways,
    the model also includes a stochastic duration predictor, which allows the model
    to synthesise speech with different rhythms from the same input text.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 基于流的模块预测了一组基于频谱图的声学特征，该模块由基于Transformer的文本编码器和多个耦合层组成。使用一堆转置卷积层对频谱图进行解码，这与HiFi-GAN声码器的风格非常相似。受TTS问题的一对多性质的启发，其中相同的文本输入可以以多种方式发音，该模型还包括一个随机持续时间预测器，允许模型从相同的输入文本中合成具有不同节奏的语音。
- en: The model is trained end-to-end with a combination of losses derived from variational
    lower bound and adversarial training. To improve the expressiveness of the model,
    normalizing flows are applied to the conditional prior distribution. During inference,
    the text encodings are up-sampled based on the duration prediction module, and
    then mapped into the waveform using a cascade of the flow module and HiFi-GAN
    decoder. Due to the stochastic nature of the duration predictor, the model is
    non-deterministic, and thus requires a fixed seed to generate the same speech
    waveform.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型通过从变分下界和对抗训练导出的损失的组合进行端到端训练。为了提高模型的表现力，将归一化流应用于条件先验分布。在推断过程中，基于持续时间预测模块对文本编码进行上采样，然后使用一系列流模块和HiFi-GAN解码器将其映射到波形中。由于持续时间预测器的随机性质，该模型是非确定性的，因此需要一个固定的种子来生成相同的语音波形。
- en: 'The abstract from the paper is the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的摘要如下：
- en: '*Several recent end-to-end text-to-speech (TTS) models enabling single-stage
    training and parallel sampling have been proposed, but their sample quality does
    not match that of two-stage TTS systems. In this work, we present a parallel end-to-end
    TTS method that generates more natural sounding audio than current two-stage models.
    Our method adopts variational inference augmented with normalizing flows and an
    adversarial training process, which improves the expressive power of generative
    modeling. We also propose a stochastic duration predictor to synthesize speech
    with diverse rhythms from input text. With the uncertainty modeling over latent
    variables and the stochastic duration predictor, our method expresses the natural
    one-to-many relationship in which a text input can be spoken in multiple ways
    with different pitches and rhythms. A subjective human evaluation (mean opinion
    score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method
    outperforms the best publicly available TTS systems and achieves a MOS comparable
    to ground truth.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*最近提出了几种端到端文本到语音（TTS）模型，实现了单阶段训练和并行采样，但它们的样本质量不及两阶段TTS系统。在这项工作中，我们提出了一种并行端到端TTS方法，其生成的音频听起来比当前的两阶段模型更自然。我们的方法采用了变分推断，增加了归一化流和对抗训练过程，提高了生成建模的表现力。我们还提出了一个随机持续时间预测器，用于从输入文本中合成具有不同节奏的语音。通过对潜在变量进行不确定性建模和随机持续时间预测器，我们的方法表达了自然的一对多关系，即文本输入可以以不同的音高和节奏发音。对LJ
    Speech（单个说话者数据集）的主观人类评估（平均意见分数，或MOS）显示，我们的方法优于最佳公开可用的TTS系统，并实现了与地面真相相当的MOS。*'
- en: This model can also be used with TTS checkpoints from [Massively Multilingual
    Speech (MMS)](https://arxiv.org/abs/2305.13516) as these checkpoints use the same
    architecture and a slightly modified tokenizer.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也可以与[大规模多语言语音（MMS）](https://arxiv.org/abs/2305.13516)的TTS检查点一起使用，因为这些检查点使用相同的架构和稍作修改的分词器。
- en: This model was contributed by [Matthijs](https://huggingface.co/Matthijs) and
    [sanchit-gandhi](https://huggingface.co/sanchit-gandhi). The original code can
    be found [here](https://github.com/jaywalnut310/vits).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[Matthijs](https://huggingface.co/Matthijs)和[sanchit-gandhi](https://huggingface.co/sanchit-gandhi)贡献的。原始代码可以在[这里](https://github.com/jaywalnut310/vits)找到。
- en: Usage examples
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用法示例
- en: 'Both the VITS and MMS-TTS checkpoints can be used with the same API. Since
    the flow-based model is non-deterministic, it is good practice to set a seed to
    ensure reproducibility of the outputs. For languages with a Roman alphabet, such
    as English or French, the tokenizer can be used directly to pre-process the text
    inputs. The following code example runs a forward pass using the MMS-TTS English
    checkpoint:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: VITS和MMS-TTS检查点都可以使用相同的API。由于基于流的模型是非确定性的，最好设置一个种子以确保输出的可重现性。对于使用罗马字母表的语言，如英语或法语，可以直接使用分词器对文本输入进行预处理。以下代码示例运行了一个前向传递，使用了MMS-TTS英语检查点：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The resulting waveform can be saved as a `.wav` file:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的波形可以保存为`.wav`文件：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Or displayed in a Jupyter Notebook / Google Colab:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 或在Jupyter Notebook / Google Colab中显示：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For certain languages with a non-Roman alphabet, such as Arabic, Mandarin or
    Hindi, the [`uroman`](https://github.com/isi-nlp/uroman) perl package is required
    to pre-process the text inputs to the Roman alphabet.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些具有非罗马字母表的语言，如阿拉伯语、普通话或印地语，需要使用[`uroman`](https://github.com/isi-nlp/uroman)
    perl包对文本输入进行预处理，转换为罗马字母表。
- en: 'You can check whether you require the `uroman` package for your language by
    inspecting the `is_uroman` attribute of the pre-trained `tokenizer`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过检查预训练的`tokenizer`的`is_uroman`属性来确定您的语言是否需要`uroman`包。
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If required, you should apply the uroman package to your text inputs **prior**
    to passing them to the `VitsTokenizer`, since currently the tokenizer does not
    support performing the pre-processing itself.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，应在将文本输入传递给`VitsTokenizer`之前**先**对其应用uroman包，因为目前分词器不支持执行预处理。
- en: 'To do this, first clone the uroman repository to your local machine and set
    the bash variable `UROMAN` to the local path:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此操作，首先将uroman存储库克隆到本地计算机，并将bash变量`UROMAN`设置为本地路径：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can then pre-process the text input using the following code snippet. You
    can either rely on using the bash variable `UROMAN` to point to the uroman repository,
    or you can pass the uroman directory as an argument to the `uromaize` function:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用以下代码片段对文本输入进行预处理。您可以依赖使用bash变量`UROMAN`指向uroman存储库，或者将uroman目录作为参数传递给`uromaize`函数：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: VitsConfig
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VitsConfig
- en: '### `class transformers.VitsConfig`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.VitsConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/configuration_vits.py#L29)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/configuration_vits.py#L29)'
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 38) — Vocabulary size of the VITS
    model. Defines the number of different tokens that can be represented by the `inputs_ids`
    passed to the forward method of [VitsModel](/docs/transformers/v4.37.2/en/model_doc/vits#transformers.VitsModel).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 38) — VITS模型的词汇量。定义了可以由传递给[VitsModel](/docs/transformers/v4.37.2/en/model_doc/vits#transformers.VitsModel)的`inputs_ids`表示的不同令牌数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 192) — Dimensionality of the
    text encoder layers.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 192) — 文本编码器层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 6) — Number of hidden layers
    in the Transformer encoder.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 6) — Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 2) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 2) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`window_size` (`int`, *optional*, defaults to 4) — Window size for the relative
    positional embeddings in the attention layers of the Transformer encoder.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`window_size` (`int`, *optional*, defaults to 4) — Transformer编码器中注意力层的相对位置嵌入的窗口大小。'
- en: '`use_bias` (`bool`, *optional*, defaults to `True`) — Whether to use bias in
    the key, query, value projection layers in the Transformer encoder.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_bias` (`bool`, *optional*, defaults to `True`) — 是否在Transformer编码器中的键、查询、值投影层中使用偏置。'
- en: '`ffn_dim` (`int`, *optional*, defaults to 768) — Dimensionality of the “intermediate”
    (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ffn_dim` (`int`, *optional*, defaults to 768) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`layerdrop` (`float`, *optional*, defaults to 0.1) — The LayerDrop probability
    for the encoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layerdrop` (`float`, *optional*, defaults to 0.1) — 编码器的LayerDrop概率。有关更多详细信息，请参阅[LayerDrop
    paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))。'
- en: '`ffn_kernel_size` (`int`, *optional*, defaults to 3) — Kernel size of the 1D
    convolution layers used by the feed-forward network in the Transformer encoder.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ffn_kernel_size` (`int`, *optional*, defaults to 3) — Transformer编码器中前馈网络使用的1D卷积层的核大小。'
- en: '`flow_size` (`int`, *optional*, defaults to 192) — Dimensionality of the flow
    layers.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flow_size` (`int`, *optional*, defaults to 192) — 流层的维度。'
- en: '`spectrogram_bins` (`int`, *optional*, defaults to 513) — Number of frequency
    bins in the target spectrogram.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spectrogram_bins` (`int`, *optional*, defaults to 513) — 目标频谱图中的频率箱数。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"relu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"relu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。'
- en: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings and encoder.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) — 嵌入层和编码器中所有全连接层的dropout概率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for the attention probabilities.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力概率的dropout比率。'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for activations inside the fully connected layer.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — 全连接层内激活的dropout比率。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — 层归一化层使用的epsilon。'
- en: '`use_stochastic_duration_prediction` (`bool`, *optional*, defaults to `True`)
    — Whether to use the stochastic duration prediction module or the regular duration
    predictor.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_stochastic_duration_prediction` (`bool`, *optional*, defaults to `True`)
    — 是否使用随机持续时间预测模块或常规持续时间预测器。'
- en: '`num_speakers` (`int`, *optional*, defaults to 1) — Number of speakers if this
    is a multi-speaker model.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_speakers` (`int`, *optional*, defaults to 1) — 如果这是多说话者模型，则说话者数量。'
- en: '`speaker_embedding_size` (`int`, *optional*, defaults to 0) — Number of channels
    used by the speaker embeddings. Is zero for single-speaker models.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speaker_embedding_size` (`int`, *optional*, 默认为0) — 说话者嵌入使用的通道数量。对于单说话者模型，该值为零。'
- en: '`upsample_initial_channel` (`int`, *optional*, defaults to 512) — The number
    of input channels into the HiFi-GAN upsampling network.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`upsample_initial_channel` (`int`, *optional*, 默认为512) — HiFi-GAN上采样网络的输入通道数。'
- en: '`upsample_rates` (`Tuple[int]` or `List[int]`, *optional*, defaults to `[8,
    8, 2, 2]`) — A tuple of integers defining the stride of each 1D convolutional
    layer in the HiFi-GAN upsampling network. The length of `upsample_rates` defines
    the number of convolutional layers and has to match the length of `upsample_kernel_sizes`.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`upsample_rates` (`Tuple[int]` 或 `List[int]`, *optional*, 默认为`[8, 8, 2, 2]`)
    — 一个整数元组，定义HiFi-GAN上采样网络中每个1D卷积层的步幅。`upsample_rates`的长度定义了卷积层的数量，并且必须与`upsample_kernel_sizes`的长度匹配。'
- en: '`upsample_kernel_sizes` (`Tuple[int]` or `List[int]`, *optional*, defaults
    to `[16, 16, 4, 4]`) — A tuple of integers defining the kernel size of each 1D
    convolutional layer in the HiFi-GAN upsampling network. The length of `upsample_kernel_sizes`
    defines the number of convolutional layers and has to match the length of `upsample_rates`.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`upsample_kernel_sizes` (`Tuple[int]` 或 `List[int]`, *optional*, 默认为`[16, 16,
    4, 4]`) — 一个整数元组，定义HiFi-GAN上采样网络中每个1D卷积层的核大小。`upsample_kernel_sizes`的长度定义了卷积层的数量，并且必须与`upsample_rates`的长度匹配。'
- en: '`resblock_kernel_sizes` (`Tuple[int]` or `List[int]`, *optional*, defaults
    to `[3, 7, 11]`) — A tuple of integers defining the kernel sizes of the 1D convolutional
    layers in the HiFi-GAN multi-receptive field fusion (MRF) module.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resblock_kernel_sizes` (`Tuple[int]` 或 `List[int]`, *optional*, 默认为`[3, 7,
    11]`) — 一个整数元组，定义HiFi-GAN多接受域融合（MRF）模块中1D卷积层的核大小。'
- en: '`resblock_dilation_sizes` (`Tuple[Tuple[int]]` or `List[List[int]]`, *optional*,
    defaults to `[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`) — A nested tuple of integers
    defining the dilation rates of the dilated 1D convolutional layers in the HiFi-GAN
    multi-receptive field fusion (MRF) module.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resblock_dilation_sizes` (`Tuple[Tuple[int]]` 或 `List[List[int]]`, *optional*,
    默认为`[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`) — 一个嵌套的整数元组，定义HiFi-GAN多接受域融合（MRF）模块中扩张1D卷积层的扩张率。'
- en: '`leaky_relu_slope` (`float`, *optional*, defaults to 0.1) — The angle of the
    negative slope used by the leaky ReLU activation.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`leaky_relu_slope` (`float`, *optional*, 默认为0.1) — leaky ReLU激活中使用的负斜率的角度。'
- en: '`depth_separable_channels` (`int`, *optional*, defaults to 2) — Number of channels
    to use in each depth-separable block.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depth_separable_channels` (`int`, *optional*, 默认为2) — 每个深度可分离块中要使用的通道数。'
- en: '`depth_separable_num_layers` (`int`, *optional*, defaults to 3) — Number of
    convolutional layers to use in each depth-separable block.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depth_separable_num_layers` (`int`, *optional*, 默认为3) — 每个深度可分离块中要使用的卷积层数量。'
- en: '`duration_predictor_flow_bins` (`int`, *optional*, defaults to 10) — Number
    of channels to map using the unonstrained rational spline in the duration predictor
    model.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`duration_predictor_flow_bins` (`int`, *optional*, 默认为10) — 在持续时间预测模型中使用无约束有理样条映射的通道数量。'
- en: '`duration_predictor_tail_bound` (`float`, *optional*, defaults to 5.0) — Value
    of the tail bin boundary when computing the unconstrained rational spline in the
    duration predictor model.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`duration_predictor_tail_bound` (`float`, *optional*, 默认为5.0) — 计算持续时间预测模型中无约束有理样条时的尾部区间边界值。'
- en: '`duration_predictor_kernel_size` (`int`, *optional*, defaults to 3) — Kernel
    size of the 1D convolution layers used in the duration predictor model.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`duration_predictor_kernel_size` (`int`, *optional*, 默认为3) — 用于持续时间预测模型中的1D卷积层的核大小。'
- en: '`duration_predictor_dropout` (`float`, *optional*, defaults to 0.5) — The dropout
    ratio for the duration predictor model.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`duration_predictor_dropout` (`float`, *optional*, 默认为0.5) — 持续时间预测模型的丢失比率。'
- en: '`duration_predictor_num_flows` (`int`, *optional*, defaults to 4) — Number
    of flow stages used by the duration predictor model.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`duration_predictor_num_flows` (`int`, *optional*, 默认为4) — 持续时间预测模型使用的流阶段数量。'
- en: '`duration_predictor_filter_channels` (`int`, *optional*, defaults to 256) —
    Number of channels for the convolution layers used in the duration predictor model.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`duration_predictor_filter_channels` (`int`, *optional*, 默认为256) — 持续时间预测模型中使用的卷积层的通道数。'
- en: '`prior_encoder_num_flows` (`int`, *optional*, defaults to 4) — Number of flow
    stages used by the prior encoder flow model.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_encoder_num_flows` (`int`, *optional*, 默认为4) — 先验编码器流模型使用的流阶段数量。'
- en: '`prior_encoder_num_wavenet_layers` (`int`, *optional*, defaults to 4) — Number
    of WaveNet layers used by the prior encoder flow model.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_encoder_num_wavenet_layers` (`int`, *optional*, 默认为4) — 先验编码器流模型使用的WaveNet层数量。'
- en: '`posterior_encoder_num_wavenet_layers` (`int`, *optional*, defaults to 16)
    — Number of WaveNet layers used by the posterior encoder model.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`posterior_encoder_num_wavenet_layers` (`int`, *optional*, 默认为16) — 后验编码器模型使用的WaveNet层数量。'
- en: '`wavenet_kernel_size` (`int`, *optional*, defaults to 5) — Kernel size of the
    1D convolution layers used in the WaveNet model.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wavenet_kernel_size` (`int`, *optional*, 默认为5) — WaveNet模型中使用的1D卷积层的核大小。'
- en: '`wavenet_dilation_rate` (`int`, *optional*, defaults to 1) — Dilation rates
    of the dilated 1D convolutional layers used in the WaveNet model.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wavenet_dilation_rate` (`int`, *optional*, 默认为1) — WaveNet模型中使用的扩张1D卷积层的扩张率。'
- en: '`wavenet_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for the WaveNet layers.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wavenet_dropout` (`float`, *optional*, 默认为0.0) — WaveNet层的丢失比率。'
- en: '`speaking_rate` (`float`, *optional*, defaults to 1.0) — Speaking rate. Larger
    values give faster synthesised speech.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speaking_rate` (`float`, *optional*, 默认为1.0) — 说话速率。较大的值会导致合成语音速度更快。'
- en: '`noise_scale` (`float`, *optional*, defaults to 0.667) — How random the speech
    prediction is. Larger values create more variation in the predicted speech.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`noise_scale` (`float`, *optional*, 默认为0.667) — 语音预测的随机程度。较大的值会导致预测语音的变化更大。'
- en: '`noise_scale_duration` (`float`, *optional*, defaults to 0.8) — How random
    the duration prediction is. Larger values create more variation in the predicted
    durations.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`noise_scale_duration` (`float`, *optional*, 默认为0.8) — 持续时间预测的随机程度。较大的值会导致预测持续时间的变化更大。'
- en: '`sampling_rate` (`int`, *optional*, defaults to 16000) — The sampling rate
    at which the output audio waveform is digitalized expressed in hertz (Hz).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate` (`int`, *optional*, 默认为16000) — 输出音频波形的数字化采样率，以赫兹（Hz）表示。'
- en: This is the configuration class to store the configuration of a [VitsModel](/docs/transformers/v4.37.2/en/model_doc/vits#transformers.VitsModel).
    It is used to instantiate a VITS model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the VITS [facebook/mms-tts-eng](https://huggingface.co/facebook/mms-tts-eng)
    architecture.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[VitsModel](/docs/transformers/v4.37.2/en/model_doc/vits#transformers.VitsModel)配置的配置类。它用于根据指定的参数实例化VITS模型，定义模型架构。使用默认值实例化配置将产生类似于[VITS](https://huggingface.co/facebook/mms-tts-eng)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: VitsTokenizer
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VitsTokenizer
- en: '### `class transformers.VitsTokenizer`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.VitsTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/tokenization_vits.py#L57)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/tokenization_vits.py#L57)'
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇表文件的路径。'
- en: '`language` (`str`, *optional*) — Language identifier.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`language` (`str`, *optional*) — 语言标识符。'
- en: '`add_blank` (`bool`, *optional*, defaults to `True`) — Whether to insert token
    id 0 in between the other tokens.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_blank` (`bool`, *optional*, 默认为`True`) — 是否在其他标记之间插入标记id 0。'
- en: '`normalize` (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the input text by removing all casing and punctuation.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`normalize` (`bool`, *optional*, 默认为`True`) — 是否通过删除所有大小写和标点来规范化输入文本。'
- en: '`phonemize` (`bool`, *optional*, defaults to `True`) — Whether to convert the
    input text into phonemes.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`phonemize` (`bool`, *optional*, 默认为`True`) — 是否将输入文本转换为音素。'
- en: '`is_uroman` (`bool`, *optional*, defaults to `False`) — Whether the `uroman`
    Romanizer needs to be applied to the input text prior to tokenizing.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_uroman` (`bool`, *optional*, 默认为`False`) — 是否在分词之前需要将`uroman`罗马化器应用于输入文本。'
- en: Construct a VITS tokenizer. Also supports MMS-TTS.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个VITS分词器。还支持MMS-TTS。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `__call__`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence or
    batch of sequences to be encoded. Each sequence can be a string or a list of strings
    (pretokenized string). If the sequences are provided as list of strings (pretokenized),
    you must set `is_split_into_words=True` (to lift the ambiguity with a batch of
    sequences).'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批量序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批量序列的歧义）。'
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded. Each sequence can be a string or a list of
    strings (pretokenized string). If the sequences are provided as list of strings
    (pretokenized), you must set `is_split_into_words=True` (to lift the ambiguity
    with a batch of sequences).'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批量序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批量序列的歧义）。'
- en: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded as target texts. Each sequence can be a string
    or a list of strings (pretokenized string). If the sequences are provided as list
    of strings (pretokenized), you must set `is_split_into_words=True` (to lift the
    ambiguity with a batch of sequences).'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码为目标文本的序列或批量序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批量序列的歧义）。'
- en: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The
    sequence or batch of sequences to be encoded as target texts. Each sequence can
    be a string or a list of strings (pretokenized string). If the sequences are provided
    as list of strings (pretokenized), you must set `is_split_into_words=True` (to
    lift the ambiguity with a batch of sequences).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码为目标文本的序列或批量序列。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批量序列的歧义）。'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (`bool`, *optional*, 默认为`True`) — 在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了自动添加到输入id的标记。如果要自动添加`bos`或`eos`标记，则这很有用。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`（`bool`，`str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`False`）—
    激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`: 填充到批次中最长的序列（如果只提供单个序列，则不进行填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: 填充到指定的最大长度，该长度由参数`max_length`指定，或者如果未提供该参数，则填充到模型可接受的最大输入长度。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）：不进行填充（即，可以输出具有不同长度序列的批次）。'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`（`bool`，`str`或[TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)，*可选*，默认为`False`）—
    激活和控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest_first''`: 仅截断到指定的最大长度，该长度由参数`max_length`指定，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列），则将逐个标记截断，从一对序列中最长的序列中删除一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`: 仅截断到指定的最大长度，该长度由参数`max_length`指定，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列），则仅会截断第一序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: 仅截断到指定的最大长度，该长度由参数`max_length`指定，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列），则仅会截断第二序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_truncate''`（默认）：不截断（即，可以输出具有大于模型最大可接受输入大小的序列长度的批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`（`int`，*可选*）— 控制截断/填充参数使用的最大长度。'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置或设置为`None`，则将使用预定义的模型最大长度，如果截断/填充参数中的一个需要最大长度。如果模型没有特定的最大输入长度（如XLNet），则将禁用截断/填充到最大长度。
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride`（`int`，*可选*，默认为0）— 如果与`max_length`一起设置为一个数字，则当`return_overflowing_tokens=True`时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义重叠标记的数量。'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words`（`bool`，*可选*，默认为`False`）— 输入是否已经预分词（例如，已分割为单词）。如果设置为`True`，则分词器会假定输入已经分割为单词（例如，通过在空格上分割），然后对其进行分词。这对于NER或标记分类很有用。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`（`int`，*可选*）— 如果设置，将序列填充到提供的值的倍数。需要激活`padding`。这对于启用具有计算能力`>=
    7.5`（Volta）的NVIDIA硬件上的Tensor Cores特别有用。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`（`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*）—
    如果设置，将返回张量而不是Python整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回TensorFlow `tf.constant`对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回PyTorch `torch.Tensor`对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回Numpy `np.ndarray`对象。'
- en: '`return_token_type_ids` (`bool`, *optional*) — Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_token_type_ids`（`bool`，*可选*）— 是否返回标记类型ID。如果保持默认设置，将根据特定分词器的默认设置返回标记类型ID，由`return_outputs`属性定义。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask` (`bool`, *可选*) — 是否返回注意力掩码。如果保持默认设置，将根据特定标记化器的默认值返回注意力掩码，由
    `return_outputs` 属性定义。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_overflowing_tokens` (`bool`, *可选*, 默认为 `False`) — 是否返回溢出的标记序列。如果提供了一对输入
    ID 序列（或一批对）并且 `truncation_strategy = longest_first` 或 `True`，则会引发错误，而不是返回溢出的标记。'
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return special tokens mask information.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_special_tokens_mask` (`bool`, *可选*, 默认为 `False`) — 是否返回特殊标记掩码信息。'
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return `(char_start, char_end)` for each token.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_offsets_mapping` (`bool`, *可选*, 默认为 `False`) — 是否返回每个标记的 `(char_start,
    char_end)`。'
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Python’s tokenizer, this method will raise `NotImplementedError`.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这仅适用于继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)的快速标记化器，如果使用Python的标记化器，此方法将引发
    `NotImplementedError`。
- en: '`return_length` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the lengths of the encoded inputs.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_length` (`bool`, *可选*, 默认为 `False`) — 是否返回编码输入的长度。'
- en: '`verbose` (`bool`, *optional*, defaults to `True`) — Whether or not to print
    more information and warnings. **kwargs — passed to the `self.tokenize()` method'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose` (`bool`, *可选*, 默认为 `True`) — 是否打印更多信息和警告。**kwargs — 传递给 `self.tokenize()`
    方法'
- en: Returns
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：
- en: '`input_ids` — List of token ids to be fed to a model.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` — 要提供给模型的标记 ID 列表。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 IDs？](../glossary#input-ids)'
- en: '`token_type_ids` — List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *“token_type_ids”* is in `self.model_input_names`).'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` — 要提供给模型的标记类型 ID 列表（当 `return_token_type_ids=True` 或 *“token_type_ids”*
    在 `self.model_input_names` 中时）。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是token type IDs？](../glossary#token-type-ids)'
- en: '`attention_mask` — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names`).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` — 指定哪些标记应该被模型关注的索引列表（当 `return_attention_mask=True` 或 *“attention_mask”*
    在 `self.model_input_names` 中时）。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`overflowing_tokens` — List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overflowing_tokens` — 溢出标记序列的列表（当指定了 `max_length` 并且 `return_overflowing_tokens=True`
    时）。'
- en: '`num_truncated_tokens` — Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_truncated_tokens` — 截断的标记数量（当指定了 `max_length` 并且 `return_overflowing_tokens=True`
    时）。'
- en: '`special_tokens_mask` — List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_mask` — 由0和1组成的列表，其中1指定添加的特殊标记，0指定常规序列标记（当 `add_special_tokens=True`
    和 `return_special_tokens_mask=True` 时）。'
- en: '`length` — The length of the inputs (when `return_length=True`)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length` — 输入的长度（当 `return_length=True` 时）'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化和准备模型一个或多个序列或一个或多个序列对的主要方法。
- en: '#### `save_vocabulary`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/tokenization_vits.py#L238)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/tokenization_vits.py#L238)'
- en: '[PRE10]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: VitsModel
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VitsModel
- en: '### `class transformers.VitsModel`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.VitsModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/modeling_vits.py#L1326)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/modeling_vits.py#L1326)'
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([VitsConfig](/docs/transformers/v4.37.2/en/model_doc/vits#transformers.VitsConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([VitsConfig](/docs/transformers/v4.37.2/en/model_doc/vits#transformers.VitsConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The complete VITS model, for text-to-speech synthesis. This model inherits from
    [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 用于文本到语音合成的完整 VITS 模型。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/modeling_vits.py#L1360)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`speaker_id` (`int`, *optional*) — Which speaker embedding to use. Only used
    for multispeaker models.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.FloatTensor` of shape `(batch_size, config.spectrogram_bins,
    sequence_length)`, *optional*) — Float values of target spectrogram. Timesteps
    set to `-100.0` are ignored (masked) for the loss computation.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.vits.modeling_vits.VitsModelOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.vits.modeling_vits.VitsModelOutput` or a tuple of `torch.FloatTensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration ([VitsConfig](/docs/transformers/v4.37.2/en/model_doc/vits#transformers.VitsConfig))
    and inputs.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '`waveform` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) —
    The final audio waveform predicted by the model.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sequence_lengths` (`torch.FloatTensor` of shape `(batch_size,)`) — The length
    in samples of each element in the `waveform` batch.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spectrogram` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_bins)`) — The log-mel spectrogram predicted at the output of the flow model.
    This spectrogram is passed to the Hi-Fi GAN decoder model to obtain the final
    audio waveform.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [VitsModel](/docs/transformers/v4.37.2/en/model_doc/vits#transformers.VitsModel)
    forward method, overrides the `__call__` special method.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[VitsModel](/docs/transformers/v4.37.2/en/model_doc/vits#transformers.VitsModel)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则会默默地忽略它们。
- en: 'Example:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE13]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
