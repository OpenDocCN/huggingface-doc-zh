- en: VITS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vits](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vits)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The VITS model was proposed in [Conditional Variational Autoencoder with Adversarial
    Learning for End-to-End Text-to-Speech](https://arxiv.org/abs/2106.06103) by Jaehyeon
    Kim, Jungil Kong, Juhee Son.
  prefs: []
  type: TYPE_NORMAL
- en: VITS (**V**ariational **I**nference with adversarial learning for end-to-end
    **T**ext-to-**S**peech) is an end-to-end speech synthesis model that predicts
    a speech waveform conditional on an input text sequence. It is a conditional variational
    autoencoder (VAE) comprised of a posterior encoder, decoder, and conditional prior.
  prefs: []
  type: TYPE_NORMAL
- en: A set of spectrogram-based acoustic features are predicted by the flow-based
    module, which is formed of a Transformer-based text encoder and multiple coupling
    layers. The spectrogram is decoded using a stack of transposed convolutional layers,
    much in the same style as the HiFi-GAN vocoder. Motivated by the one-to-many nature
    of the TTS problem, where the same text input can be spoken in multiple ways,
    the model also includes a stochastic duration predictor, which allows the model
    to synthesise speech with different rhythms from the same input text.
  prefs: []
  type: TYPE_NORMAL
- en: The model is trained end-to-end with a combination of losses derived from variational
    lower bound and adversarial training. To improve the expressiveness of the model,
    normalizing flows are applied to the conditional prior distribution. During inference,
    the text encodings are up-sampled based on the duration prediction module, and
    then mapped into the waveform using a cascade of the flow module and HiFi-GAN
    decoder. Due to the stochastic nature of the duration predictor, the model is
    non-deterministic, and thus requires a fixed seed to generate the same speech
    waveform.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Several recent end-to-end text-to-speech (TTS) models enabling single-stage
    training and parallel sampling have been proposed, but their sample quality does
    not match that of two-stage TTS systems. In this work, we present a parallel end-to-end
    TTS method that generates more natural sounding audio than current two-stage models.
    Our method adopts variational inference augmented with normalizing flows and an
    adversarial training process, which improves the expressive power of generative
    modeling. We also propose a stochastic duration predictor to synthesize speech
    with diverse rhythms from input text. With the uncertainty modeling over latent
    variables and the stochastic duration predictor, our method expresses the natural
    one-to-many relationship in which a text input can be spoken in multiple ways
    with different pitches and rhythms. A subjective human evaluation (mean opinion
    score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method
    outperforms the best publicly available TTS systems and achieves a MOS comparable
    to ground truth.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model can also be used with TTS checkpoints from [Massively Multilingual
    Speech (MMS)](https://arxiv.org/abs/2305.13516) as these checkpoints use the same
    architecture and a slightly modified tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [Matthijs](https://huggingface.co/Matthijs) and
    [sanchit-gandhi](https://huggingface.co/sanchit-gandhi). The original code can
    be found [here](https://github.com/jaywalnut310/vits).
  prefs: []
  type: TYPE_NORMAL
- en: Usage examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Both the VITS and MMS-TTS checkpoints can be used with the same API. Since
    the flow-based model is non-deterministic, it is good practice to set a seed to
    ensure reproducibility of the outputs. For languages with a Roman alphabet, such
    as English or French, the tokenizer can be used directly to pre-process the text
    inputs. The following code example runs a forward pass using the MMS-TTS English
    checkpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting waveform can be saved as a `.wav` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Or displayed in a Jupyter Notebook / Google Colab:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For certain languages with a non-Roman alphabet, such as Arabic, Mandarin or
    Hindi, the [`uroman`](https://github.com/isi-nlp/uroman) perl package is required
    to pre-process the text inputs to the Roman alphabet.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check whether you require the `uroman` package for your language by
    inspecting the `is_uroman` attribute of the pre-trained `tokenizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If required, you should apply the uroman package to your text inputs **prior**
    to passing them to the `VitsTokenizer`, since currently the tokenizer does not
    support performing the pre-processing itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, first clone the uroman repository to your local machine and set
    the bash variable `UROMAN` to the local path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then pre-process the text input using the following code snippet. You
    can either rely on using the bash variable `UROMAN` to point to the uroman repository,
    or you can pass the uroman directory as an argument to the `uromaize` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: VitsConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.VitsConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/configuration_vits.py#L29)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 38) — Vocabulary size of the VITS
    model. Defines the number of different tokens that can be represented by the `inputs_ids`
    passed to the forward method of [VitsModel](/docs/transformers/v4.37.2/en/model_doc/vits#transformers.VitsModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 192) — Dimensionality of the
    text encoder layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 6) — Number of hidden layers
    in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 2) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window_size` (`int`, *optional*, defaults to 4) — Window size for the relative
    positional embeddings in the attention layers of the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_bias` (`bool`, *optional*, defaults to `True`) — Whether to use bias in
    the key, query, value projection layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ffn_dim` (`int`, *optional*, defaults to 768) — Dimensionality of the “intermediate”
    (i.e., feed-forward) layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layerdrop` (`float`, *optional*, defaults to 0.1) — The LayerDrop probability
    for the encoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ffn_kernel_size` (`int`, *optional*, defaults to 3) — Kernel size of the 1D
    convolution layers used by the feed-forward network in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flow_size` (`int`, *optional*, defaults to 192) — Dimensionality of the flow
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spectrogram_bins` (`int`, *optional*, defaults to 513) — Number of frequency
    bins in the target spectrogram.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"relu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings and encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for activations inside the fully connected layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_stochastic_duration_prediction` (`bool`, *optional*, defaults to `True`)
    — Whether to use the stochastic duration prediction module or the regular duration
    predictor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_speakers` (`int`, *optional*, defaults to 1) — Number of speakers if this
    is a multi-speaker model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speaker_embedding_size` (`int`, *optional*, defaults to 0) — Number of channels
    used by the speaker embeddings. Is zero for single-speaker models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`upsample_initial_channel` (`int`, *optional*, defaults to 512) — The number
    of input channels into the HiFi-GAN upsampling network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`upsample_rates` (`Tuple[int]` or `List[int]`, *optional*, defaults to `[8,
    8, 2, 2]`) — A tuple of integers defining the stride of each 1D convolutional
    layer in the HiFi-GAN upsampling network. The length of `upsample_rates` defines
    the number of convolutional layers and has to match the length of `upsample_kernel_sizes`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`upsample_kernel_sizes` (`Tuple[int]` or `List[int]`, *optional*, defaults
    to `[16, 16, 4, 4]`) — A tuple of integers defining the kernel size of each 1D
    convolutional layer in the HiFi-GAN upsampling network. The length of `upsample_kernel_sizes`
    defines the number of convolutional layers and has to match the length of `upsample_rates`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resblock_kernel_sizes` (`Tuple[int]` or `List[int]`, *optional*, defaults
    to `[3, 7, 11]`) — A tuple of integers defining the kernel sizes of the 1D convolutional
    layers in the HiFi-GAN multi-receptive field fusion (MRF) module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resblock_dilation_sizes` (`Tuple[Tuple[int]]` or `List[List[int]]`, *optional*,
    defaults to `[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`) — A nested tuple of integers
    defining the dilation rates of the dilated 1D convolutional layers in the HiFi-GAN
    multi-receptive field fusion (MRF) module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`leaky_relu_slope` (`float`, *optional*, defaults to 0.1) — The angle of the
    negative slope used by the leaky ReLU activation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`depth_separable_channels` (`int`, *optional*, defaults to 2) — Number of channels
    to use in each depth-separable block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`depth_separable_num_layers` (`int`, *optional*, defaults to 3) — Number of
    convolutional layers to use in each depth-separable block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`duration_predictor_flow_bins` (`int`, *optional*, defaults to 10) — Number
    of channels to map using the unonstrained rational spline in the duration predictor
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`duration_predictor_tail_bound` (`float`, *optional*, defaults to 5.0) — Value
    of the tail bin boundary when computing the unconstrained rational spline in the
    duration predictor model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`duration_predictor_kernel_size` (`int`, *optional*, defaults to 3) — Kernel
    size of the 1D convolution layers used in the duration predictor model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`duration_predictor_dropout` (`float`, *optional*, defaults to 0.5) — The dropout
    ratio for the duration predictor model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`duration_predictor_num_flows` (`int`, *optional*, defaults to 4) — Number
    of flow stages used by the duration predictor model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`duration_predictor_filter_channels` (`int`, *optional*, defaults to 256) —
    Number of channels for the convolution layers used in the duration predictor model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_encoder_num_flows` (`int`, *optional*, defaults to 4) — Number of flow
    stages used by the prior encoder flow model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_encoder_num_wavenet_layers` (`int`, *optional*, defaults to 4) — Number
    of WaveNet layers used by the prior encoder flow model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`posterior_encoder_num_wavenet_layers` (`int`, *optional*, defaults to 16)
    — Number of WaveNet layers used by the posterior encoder model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wavenet_kernel_size` (`int`, *optional*, defaults to 5) — Kernel size of the
    1D convolution layers used in the WaveNet model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wavenet_dilation_rate` (`int`, *optional*, defaults to 1) — Dilation rates
    of the dilated 1D convolutional layers used in the WaveNet model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wavenet_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for the WaveNet layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speaking_rate` (`float`, *optional*, defaults to 1.0) — Speaking rate. Larger
    values give faster synthesised speech.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`noise_scale` (`float`, *optional*, defaults to 0.667) — How random the speech
    prediction is. Larger values create more variation in the predicted speech.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`noise_scale_duration` (`float`, *optional*, defaults to 0.8) — How random
    the duration prediction is. Larger values create more variation in the predicted
    durations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*, defaults to 16000) — The sampling rate
    at which the output audio waveform is digitalized expressed in hertz (Hz).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [VitsModel](/docs/transformers/v4.37.2/en/model_doc/vits#transformers.VitsModel).
    It is used to instantiate a VITS model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the VITS [facebook/mms-tts-eng](https://huggingface.co/facebook/mms-tts-eng)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: VitsTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.VitsTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/tokenization_vits.py#L57)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`language` (`str`, *optional*) — Language identifier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_blank` (`bool`, *optional*, defaults to `True`) — Whether to insert token
    id 0 in between the other tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalize` (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the input text by removing all casing and punctuation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`phonemize` (`bool`, *optional*, defaults to `True`) — Whether to convert the
    input text into phonemes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_uroman` (`bool`, *optional*, defaults to `False`) — Whether the `uroman`
    Romanizer needs to be applied to the input text prior to tokenizing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a VITS tokenizer. Also supports MMS-TTS.
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence or
    batch of sequences to be encoded. Each sequence can be a string or a list of strings
    (pretokenized string). If the sequences are provided as list of strings (pretokenized),
    you must set `is_split_into_words=True` (to lift the ambiguity with a batch of
    sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded. Each sequence can be a string or a list of
    strings (pretokenized string). If the sequences are provided as list of strings
    (pretokenized), you must set `is_split_into_words=True` (to lift the ambiguity
    with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded as target texts. Each sequence can be a string
    or a list of strings (pretokenized string). If the sequences are provided as list
    of strings (pretokenized), you must set `is_split_into_words=True` (to lift the
    ambiguity with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The
    sequence or batch of sequences to be encoded as target texts. Each sequence can
    be a string or a list of strings (pretokenized string). If the sequences are provided
    as list of strings (pretokenized), you must set `is_split_into_words=True` (to
    lift the ambiguity with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_token_type_ids` (`bool`, *optional*) — Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizer’s default, defined by the `return_outputs` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizer’s default, defined by the `return_outputs` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return special tokens mask information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return `(char_start, char_end)` for each token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Python’s tokenizer, this method will raise `NotImplementedError`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_length` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the lengths of the encoded inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose` (`bool`, *optional*, defaults to `True`) — Whether or not to print
    more information and warnings. **kwargs — passed to the `self.tokenize()` method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` — List of token ids to be fed to a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` — List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *“token_type_ids”* is in `self.model_input_names`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`overflowing_tokens` — List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_truncated_tokens` — Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`special_tokens_mask` — List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length` — The length of the inputs (when `return_length=True`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_vocabulary`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/tokenization_vits.py#L238)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: VitsModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.VitsModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/modeling_vits.py#L1326)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([VitsConfig](/docs/transformers/v4.37.2/en/model_doc/vits#transformers.VitsConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complete VITS model, for text-to-speech synthesis. This model inherits from
    [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vits/modeling_vits.py#L1360)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`speaker_id` (`int`, *optional*) — Which speaker embedding to use. Only used
    for multispeaker models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.FloatTensor` of shape `(batch_size, config.spectrogram_bins,
    sequence_length)`, *optional*) — Float values of target spectrogram. Timesteps
    set to `-100.0` are ignored (masked) for the loss computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.vits.modeling_vits.VitsModelOutput` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.vits.modeling_vits.VitsModelOutput` or a tuple of `torch.FloatTensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration ([VitsConfig](/docs/transformers/v4.37.2/en/model_doc/vits#transformers.VitsConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`waveform` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) —
    The final audio waveform predicted by the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sequence_lengths` (`torch.FloatTensor` of shape `(batch_size,)`) — The length
    in samples of each element in the `waveform` batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spectrogram` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_bins)`) — The log-mel spectrogram predicted at the output of the flow model.
    This spectrogram is passed to the Hi-Fi GAN decoder model to obtain the final
    audio waveform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [VitsModel](/docs/transformers/v4.37.2/en/model_doc/vits#transformers.VitsModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
