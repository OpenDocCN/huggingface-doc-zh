["```py\n>>> from transformers import AutoProcessor, SeamlessM4Tv2Model\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n>>> model = SeamlessM4Tv2Model.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n```", "```py\n>>> # let's load an audio sample from an Arabic speech corpus\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"arabic_speech_corpus\", split=\"test\", streaming=True)\n>>> audio_sample = next(iter(dataset))[\"audio\"]\n\n>>> # now, process it\n>>> audio_inputs = processor(audios=audio_sample[\"array\"], return_tensors=\"pt\")\n\n>>> # now, process some English text as well\n>>> text_inputs = processor(text = \"Hello, my dog is cute\", src_lang=\"eng\", return_tensors=\"pt\")\n```", "```py\n>>> audio_array_from_text = model.generate(**text_inputs, tgt_lang=\"rus\")[0].cpu().numpy().squeeze()\n>>> audio_array_from_audio = model.generate(**audio_inputs, tgt_lang=\"rus\")[0].cpu().numpy().squeeze()\n```", "```py\n>>> # from audio\n>>> output_tokens = model.generate(**audio_inputs, tgt_lang=\"fra\", generate_speech=False)\n>>> translated_text_from_audio = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n\n>>> # from text\n>>> output_tokens = model.generate(**text_inputs, tgt_lang=\"fra\", generate_speech=False)\n>>> translated_text_from_text = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n```", "```py\n>>> from transformers import SeamlessM4Tv2ForSpeechToSpeech\n>>> model = SeamlessM4Tv2ForSpeechToSpeech.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n```", "```py\n>>> from transformers import SeamlessM4Tv2ForTextToText\n>>> model = SeamlessM4Tv2ForTextToText.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n```", "```py\n( config current_modality = 'text' )\n```", "```py\n( input_ids: Optional = None input_features: Optional = None return_intermediate_token_ids: Optional = None tgt_lang: Optional = None speaker_id: Optional = 0 generate_speech: Optional = True **kwargs ) \u2192 export const metadata = 'undefined';Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor], ModelOutput]\n```", "```py\n( config: SeamlessM4Tv2Config )\n```", "```py\n( input_ids: Optional = None return_intermediate_token_ids: Optional = None tgt_lang: Optional = None speaker_id: Optional = 0 **kwargs ) \u2192 export const metadata = 'undefined';Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor]]\n```", "```py\n( config )\n```", "```py\n( input_features: Optional = None return_intermediate_token_ids: Optional = None tgt_lang: Optional = None speaker_id: Optional = 0 **kwargs ) \u2192 export const metadata = 'undefined';Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor]]\n```", "```py\n( config: SeamlessM4Tv2Config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs )\n```", "```py\n( input_ids = None tgt_lang = None generation_config = None logits_processor = None stopping_criteria = None prefix_allowed_tokens_fn = None synced_gpus = False **kwargs ) \u2192 export const metadata = 'undefined';ModelOutput or torch.LongTensor\n```", "```py\n( config: SeamlessM4Tv2Config )\n```", "```py\n( input_features: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs )\n```", "```py\n( input_features = None tgt_lang = None generation_config = None logits_processor = None stopping_criteria = None prefix_allowed_tokens_fn = None synced_gpus = False **kwargs ) \u2192 export const metadata = 'undefined';ModelOutput or torch.LongTensor\n```", "```py\n( vocab_size = 256102 t2u_vocab_size = 10082 char_vocab_size = 10943 hidden_size = 1024 initializer_range = 0.02 layer_norm_eps = 1e-05 use_cache = True max_position_embeddings = 4096 is_encoder_decoder = True encoder_layerdrop = 0.05 decoder_layerdrop = 0.05 activation_function = 'relu' dropout = 0.1 attention_dropout = 0.1 activation_dropout = 0.0 scale_embedding = True encoder_layers = 24 encoder_ffn_dim = 8192 encoder_attention_heads = 16 decoder_layers = 24 decoder_ffn_dim = 8192 decoder_attention_heads = 16 decoder_start_token_id = 3 max_new_tokens = 256 pad_token_id = 0 bos_token_id = 2 eos_token_id = 3 speech_encoder_layers = 24 speech_encoder_attention_heads = 16 speech_encoder_intermediate_size = 4096 speech_encoder_hidden_act = 'swish' speech_encoder_dropout = 0.0 add_adapter = True speech_encoder_layerdrop = 0.1 feature_projection_input_dim = 160 adaptor_kernel_size = 8 adaptor_stride = 8 adaptor_dropout = 0.1 num_adapter_layers = 1 position_embeddings_type = 'relative_key' conv_depthwise_kernel_size = 31 left_max_position_embeddings = 64 right_max_position_embeddings = 8 speech_encoder_chunk_size = 20000 speech_encoder_left_chunk_num = 128 t2u_bos_token_id = 0 t2u_pad_token_id = 1 t2u_eos_token_id = 2 t2u_encoder_layers = 6 t2u_encoder_ffn_dim = 8192 t2u_encoder_attention_heads = 16 t2u_decoder_layers = 6 t2u_decoder_ffn_dim = 8192 t2u_decoder_attention_heads = 16 t2u_max_position_embeddings = 4096 t2u_variance_predictor_embed_dim = 1024 t2u_variance_predictor_hidden_dim = 256 t2u_variance_predictor_kernel_size = 3 t2u_variance_pred_dropout = 0.5 sampling_rate = 16000 upsample_initial_channel = 512 upsample_rates = [5, 4, 4, 2, 2] upsample_kernel_sizes = [11, 8, 8, 4, 4] resblock_kernel_sizes = [3, 7, 11] resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]] leaky_relu_slope = 0.1 unit_hifi_gan_vocab_size = 10000 unit_embed_dim = 1280 lang_embed_dim = 256 spkr_embed_dim = 256 vocoder_num_langs = 36 vocoder_num_spkrs = 200 variance_predictor_kernel_size = 3 var_pred_dropout = 0.5 vocoder_offset = 4 **kwargs )\n```", "```py\n>>> from transformers import SeamlessM4Tv2Model, SeamlessM4Tv2Config\n\n>>> # Initializing a SeamlessM4Tv2 \"\" style configuration\n>>> configuration = SeamlessM4Tv2Config()\n\n>>> # Initializing a model from the \"\" style configuration\n>>> model = SeamlessM4Tv2Model(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```"]