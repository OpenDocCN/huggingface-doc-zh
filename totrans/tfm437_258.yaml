- en: Convolutional Vision Transformer (CvT)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cvt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cvt)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/100.4289259c.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Markdown.fef84341.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/stores.c16bc1a5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/PipelineTag.44585822.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The CvT model was proposed in [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808)
    by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan and Lei
    Zhang. The Convolutional vision Transformer (CvT) improves the [Vision Transformer
    (ViT)](vit) in performance and efficiency by introducing convolutions into ViT
    to yield the best of both designs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We present in this paper a new architecture, named Convolutional vision Transformer
    (CvT), that improves Vision Transformer (ViT) in performance and efficiency by
    introducing convolutions into ViT to yield the best of both designs. This is accomplished
    through two primary modifications: a hierarchy of Transformers containing a new
    convolutional token embedding, and a convolutional Transformer block leveraging
    a convolutional projection. These changes introduce desirable properties of convolutional
    neural networks (CNNs) to the ViT architecture (\ie shift, scale, and distortion
    invariance) while maintaining the merits of Transformers (\ie dynamic attention,
    global context, and better generalization). We validate CvT by conducting extensive
    experiments, showing that this approach achieves state-of-the-art performance
    over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters
    and lower FLOPs. In addition, performance gains are maintained when pretrained
    on larger datasets (\eg ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained
    on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7\% on the ImageNet-1k
    val set. Finally, our results show that the positional encoding, a crucial component
    in existing Vision Transformers, can be safely removed in our model, simplifying
    the design for higher resolution vision tasks.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [anugunj](https://huggingface.co/anugunj). The
    original code can be found [here](https://github.com/microsoft/CvT).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CvT models are regular Vision Transformers, but trained with convolutions. They
    outperform the [original model (ViT)](vit) when fine-tuned on ImageNet-1K and
    CIFAR-100.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can check out demo notebooks regarding inference as well as fine-tuning
    on custom data [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)
    (you can just replace [ViTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTFeatureExtractor)
    by [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    and [ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)
    by [CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The available checkpoints are either (1) pre-trained on [ImageNet-22k](http://www.image-net.org/)
    (a collection of 14 million images and 22k classes) only, (2) also fine-tuned
    on ImageNet-22k or (3) also fine-tuned on [ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/)
    (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000
    classes).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with CvT.
  prefs: []
  type: TYPE_NORMAL
- en: Image Classification
  prefs: []
  type: TYPE_NORMAL
- en: '[CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we’ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  prefs: []
  type: TYPE_NORMAL
- en: CvtConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.CvtConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/configuration_cvt.py#L29)'
  prefs: []
  type: TYPE_NORMAL
- en: ( num_channels = 3 patch_sizes = [7, 3, 3] patch_stride = [4, 2, 2] patch_padding
    = [2, 1, 1] embed_dim = [64, 192, 384] num_heads = [1, 3, 6] depth = [1, 2, 10]
    mlp_ratio = [4.0, 4.0, 4.0] attention_drop_rate = [0.0, 0.0, 0.0] drop_rate =
    [0.0, 0.0, 0.0] drop_path_rate = [0.0, 0.0, 0.1] qkv_bias = [True, True, True]
    cls_token = [False, False, True] qkv_projection_method = ['dw_bn', 'dw_bn', 'dw_bn']
    kernel_qkv = [3, 3, 3] padding_kv = [1, 1, 1] stride_kv = [2, 2, 2] padding_q
    = [1, 1, 1] stride_q = [1, 1, 1] initializer_range = 0.02 layer_norm_eps = 1e-12
    **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**num_channels** (`int`, *optional*, defaults to 3) — The number of input channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**patch_sizes** (`List[int]`, *optional*, defaults to `[7, 3, 3]`) — The kernel
    size of each encoder’s patch embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**patch_stride** (`List[int]`, *optional*, defaults to `[4, 2, 2]`) — The stride
    size of each encoder’s patch embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**patch_padding** (`List[int]`, *optional*, defaults to `[2, 1, 1]`) — The
    padding size of each encoder’s patch embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**embed_dim** (`List[int]`, *optional*, defaults to `[64, 192, 384]`) — Dimension
    of each of the encoder blocks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_heads** (`List[int]`, *optional*, defaults to `[1, 3, 6]`) — Number of
    attention heads for each attention layer in each block of the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**depth** (`List[int]`, *optional*, defaults to `[1, 2, 10]`) — The number
    of layers in each encoder block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mlp_ratios** (`List[float]`, *optional*, defaults to `[4.0, 4.0, 4.0, 4.0]`)
    — Ratio of the size of the hidden layer compared to the size of the input layer
    of the Mix FFNs in the encoder blocks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_drop_rate** (`List[float]`, *optional*, defaults to `[0.0, 0.0,
    0.0]`) — The dropout ratio for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**drop_rate** (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`) —
    The dropout ratio for the patch embeddings probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**drop_path_rate** (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.1]`)
    — The dropout probability for stochastic depth, used in the blocks of the Transformer
    encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**qkv_bias** (`List[bool]`, *optional*, defaults to `[True, True, True]`) —
    The bias bool for query, key and value in attentions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_token** (`List[bool]`, *optional*, defaults to `[False, False, True]`)
    — Whether or not to add a classification token to the output of each of the last
    3 stages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**qkv_projection_method** (`List[string]`, *optional*, defaults to [“dw_bn”,
    “dw_bn”, “dw_bn”]`) — The projection method for query, key and value Default is
    depth-wise convolutions with batch norm. For Linear projection use “avg”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kernel_qkv** (`List[int]`, *optional*, defaults to `[3, 3, 3]`) — The kernel
    size for query, key and value in attention layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**padding_kv** (`List[int]`, *optional*, defaults to `[1, 1, 1]`) — The padding
    size for key and value in attention layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stride_kv** (`List[int]`, *optional*, defaults to `[2, 2, 2]`) — The stride
    size for key and value in attention layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**padding_q** (`List[int]`, *optional*, defaults to `[1, 1, 1]`) — The padding
    size for query in attention layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stride_q** (`List[int]`, *optional*, defaults to `[1, 1, 1]`) — The stride
    size for query in attention layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initializer_range** (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**layer_norm_eps** (`float`, *optional*, defaults to 1e-6) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [CvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtModel).
    It is used to instantiate a CvT model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the CvT [microsoft/cvt-13](https://huggingface.co/microsoft/cvt-13)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: CvtModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.CvtModel'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L586)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config add_pooling_layer = True )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare Cvt Model transformer outputting raw hidden-states without any specific
    head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L605)'
  prefs: []
  type: TYPE_NORMAL
- en: '( pixel_values: Optional = None output_hidden_states: Optional = None return_dict:
    Optional = None ) → `transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken`
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**pixel_values** (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `CvtImageProcessor.__call__` for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken` or a tuple
    of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_token_value** (`torch.FloatTensor` of shape `(batch_size, 1, hidden_size)`)
    — Classification token at the output of the last layer of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [CvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: CvtForImageClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.CvtForImageClassification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L644)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cvt Model transformer with an image classification head on top (a linear layer
    on top of the final hidden state of the [CLS] token) e.g. for ImageNet.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L666)'
  prefs: []
  type: TYPE_NORMAL
- en: '( pixel_values: Optional = None labels: Optional = None output_hidden_states:
    Optional = None return_dict: Optional = None ) → [transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**pixel_values** (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `CvtImageProcessor.__call__` for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`)
    — Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each stage) of shape `(batch_size, num_channels, height,
    width)`. Hidden-states (also called feature maps) of the model at the output of
    each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: TFCvtModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.TFCvtModel'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L926)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: CvtConfig *inputs **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare Cvt Model transformer outputting raw hidden-states without any specific
    head on top.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'TF 2.0 models accepts two formats as inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional arguments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This second option is useful when using `tf.keras.Model.fit` method which currently
    requires having all the tensors in the first argument of the model call function:
    `model(inputs)`.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### call'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L936)'
  prefs: []
  type: TYPE_NORMAL
- en: '( pixel_values: tf.Tensor | None = None output_hidden_states: Optional[bool]
    = None return_dict: Optional[bool] = None training: Optional[bool] = False ) →
    `transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**pixel_values** (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str,
    tf.Tensor]` or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) — Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `CvtImageProcessor.__call__` for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**training** (`bool`, *optional*, defaults to `False“) — Whether or not to
    use the model in training mode (some modules like dropout modules have different
    behaviors between training and evaluation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` or
    `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` or
    a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**last_hidden_state** (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_token_value** (`tf.Tensor` of shape `(batch_size, 1, hidden_size)`) —
    Classification token at the output of the last layer of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [TFCvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: TFCvtForImageClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.TFCvtForImageClassification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L995)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: CvtConfig *inputs **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cvt Model transformer with an image classification head on top (a linear layer
    on top of the final hidden state of the [CLS] token) e.g. for ImageNet.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'TF 2.0 models accepts two formats as inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional arguments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This second option is useful when using `tf.keras.Model.fit` method which currently
    requires having all the tensors in the first argument of the model call function:
    `model(inputs)`.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### call'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L1021)'
  prefs: []
  type: TYPE_NORMAL
- en: '( pixel_values: tf.Tensor | None = None labels: tf.Tensor | None = None output_hidden_states:
    Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool]
    = False ) → `transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**pixel_values** (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str,
    tf.Tensor]` or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) — Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `CvtImageProcessor.__call__` for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**training** (`bool`, *optional*, defaults to `False“) — Whether or not to
    use the model in training mode (some modules like dropout modules have different
    behaviors between training and evaluation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*)
    — Labels for computing the image classification/regression loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression
    loss is computed (Mean-Square loss), If `config.num_labels > 1` a classification
    loss is computed (Cross-Entropy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention` or
    `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings, if the model has an embedding layer, + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the model at the output of each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [TFCvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtForImageClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
