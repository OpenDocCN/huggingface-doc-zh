- en: Convolutional Vision Transformer (CvT)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cvt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cvt)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The CvT model was proposed in [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808)
    by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan and Lei
    Zhang. The Convolutional vision Transformer (CvT) improves the [Vision Transformer
    (ViT)](vit) in performance and efficiency by introducing convolutions into ViT
    to yield the best of both designs.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '*We present in this paper a new architecture, named Convolutional vision Transformer
    (CvT), that improves Vision Transformer (ViT) in performance and efficiency by
    introducing convolutions into ViT to yield the best of both designs. This is accomplished
    through two primary modifications: a hierarchy of Transformers containing a new
    convolutional token embedding, and a convolutional Transformer block leveraging
    a convolutional projection. These changes introduce desirable properties of convolutional
    neural networks (CNNs) to the ViT architecture (\ie shift, scale, and distortion
    invariance) while maintaining the merits of Transformers (\ie dynamic attention,
    global context, and better generalization). We validate CvT by conducting extensive
    experiments, showing that this approach achieves state-of-the-art performance
    over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters
    and lower FLOPs. In addition, performance gains are maintained when pretrained
    on larger datasets (\eg ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained
    on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7\% on the ImageNet-1k
    val set. Finally, our results show that the positional encoding, a crucial component
    in existing Vision Transformers, can be safely removed in our model, simplifying
    the design for higher resolution vision tasks.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [anugunj](https://huggingface.co/anugunj). The
    original code can be found [here](https://github.com/microsoft/CvT).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CvT models are regular Vision Transformers, but trained with convolutions. They
    outperform the [original model (ViT)](vit) when fine-tuned on ImageNet-1K and
    CIFAR-100.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can check out demo notebooks regarding inference as well as fine-tuning
    on custom data [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)
    (you can just replace [ViTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTFeatureExtractor)
    by [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    and [ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)
    by [CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The available checkpoints are either (1) pre-trained on [ImageNet-22k](http://www.image-net.org/)
    (a collection of 14 million images and 22k classes) only, (2) also fine-tuned
    on ImageNet-22k or (3) also fine-tuned on [ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/)
    (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000
    classes).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A list of official Hugging Face and community (indicated by üåé) resources to
    help you get started with CvT.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Image Classification
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you‚Äôre interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we‚Äôll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: CvtConfig
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.CvtConfig`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/configuration_cvt.py#L29)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '`num_channels` (`int`, *optional*, defaults to 3) ‚Äî The number of input channels.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_sizes` (`List[int]`, *optional*, defaults to `[7, 3, 3]`) ‚Äî The kernel
    size of each encoder‚Äôs patch embedding.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_stride` (`List[int]`, *optional*, defaults to `[4, 2, 2]`) ‚Äî The stride
    size of each encoder‚Äôs patch embedding.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_padding` (`List[int]`, *optional*, defaults to `[2, 1, 1]`) ‚Äî The padding
    size of each encoder‚Äôs patch embedding.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embed_dim` (`List[int]`, *optional*, defaults to `[64, 192, 384]`) ‚Äî Dimension
    of each of the encoder blocks.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_heads` (`List[int]`, *optional*, defaults to `[1, 3, 6]`) ‚Äî Number of
    attention heads for each attention layer in each block of the Transformer encoder.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`depth` (`List[int]`, *optional*, defaults to `[1, 2, 10]`) ‚Äî The number of
    layers in each encoder block.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlp_ratios` (`List[float]`, *optional*, defaults to `[4.0, 4.0, 4.0, 4.0]`)
    ‚Äî Ratio of the size of the hidden layer compared to the size of the input layer
    of the Mix FFNs in the encoder blocks.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_drop_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`)
    ‚Äî The dropout ratio for the attention probabilities.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`) ‚Äî The
    dropout ratio for the patch embeddings probabilities.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_path_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.1]`)
    ‚Äî The dropout probability for stochastic depth, used in the blocks of the Transformer
    encoder.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qkv_bias` (`List[bool]`, *optional*, defaults to `[True, True, True]`) ‚Äî The
    bias bool for query, key and value in attentions'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls_token` (`List[bool]`, *optional*, defaults to `[False, False, True]`)
    ‚Äî Whether or not to add a classification token to the output of each of the last
    3 stages.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qkv_projection_method` (`List[string]`, *optional*, defaults to [‚Äúdw_bn‚Äù,
    ‚Äúdw_bn‚Äù, ‚Äúdw_bn‚Äù]`) ‚Äî The projection method for query, key and value Default is
    depth-wise convolutions with batch norm. For Linear projection use ‚Äúavg‚Äù.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel_qkv` (`List[int]`, *optional*, defaults to `[3, 3, 3]`) ‚Äî The kernel
    size for query, key and value in attention layer'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding_kv` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) ‚Äî The padding
    size for key and value in attention layer'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stride_kv` (`List[int]`, *optional*, defaults to `[2, 2, 2]`) ‚Äî The stride
    size for key and value in attention layer'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding_q` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) ‚Äî The padding
    size for query in attention layer'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stride_q` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) ‚Äî The stride
    size for query in attention layer'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) ‚Äî The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-6) ‚Äî The epsilon used
    by the layer normalization layers.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [CvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtModel).
    It is used to instantiate a CvT model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the CvT [microsoft/cvt-13](https://huggingface.co/microsoft/cvt-13)
    architecture.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: PytorchHide Pytorch content
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: CvtModel
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.CvtModel`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L586)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    ‚Äî Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare Cvt Model transformer outputting raw hidden-states without any specific
    head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L605)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) ‚Äî Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `CvtImageProcessor.__call__` for details.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) ‚Äî Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) ‚Äî Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken` or `tuple(torch.FloatTensor)`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken` or a tuple
    of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    and inputs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) ‚Äî Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls_token_value` (`torch.FloatTensor` of shape `(batch_size, 1, hidden_size)`)
    ‚Äî Classification token at the output of the last layer of the model.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [CvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtModel)
    forward method, overrides the `__call__` special method.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: CvtForImageClassification
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.CvtForImageClassification`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L644)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    ‚Äî Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cvt Model transformer with an image classification head on top (a linear layer
    on top of the final hidden state of the [CLS] token) e.g. for ImageNet.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L666)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) ‚Äî Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `CvtImageProcessor.__call__` for details.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) ‚Äî Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) ‚Äî Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) ‚Äî Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or `tuple(torch.FloatTensor)`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    and inputs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) ‚Äî Classification (or regression if config.num_labels==1) loss.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) ‚Äî
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each stage) of shape `(batch_size, num_channels, height,
    width)`. Hidden-states (also called feature maps) of the model at the output of
    each stage.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: TensorFlowHide TensorFlow content
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: TFCvtModel
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFCvtModel`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L926)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    ‚Äî Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare Cvt Model transformer outputting raw hidden-states without any specific
    head on top.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'TF 2.0 models accepts two formats as inputs:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional arguments.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This second option is useful when using `tf.keras.Model.fit` method which currently
    requires having all the tensors in the first argument of the model call function:
    `model(inputs)`.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L936)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) ‚Äî Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `CvtImageProcessor.__call__` for details.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) ‚Äî Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) ‚Äî Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False‚Äú) ‚Äî Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` or
    `tuple(tf.Tensor)`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` or
    a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    and inputs.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    ‚Äî Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls_token_value` (`tf.Tensor` of shape `(batch_size, 1, hidden_size)`) ‚Äî Classification
    token at the output of the last layer of the model.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) ‚Äî Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [TFCvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtModel)
    forward method, overrides the `__call__` special method.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: TFCvtForImageClassification
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFCvtForImageClassification`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L995)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    ‚Äî Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cvt Model transformer with an image classification head on top (a linear layer
    on top of the final hidden state of the [CLS] token) e.g. for ImageNet.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'TF 2.0 models accepts two formats as inputs:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional arguments.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This second option is useful when using `tf.keras.Model.fit` method which currently
    requires having all the tensors in the first argument of the model call function:
    `model(inputs)`.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L1021)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) ‚Äî Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `CvtImageProcessor.__call__` for details.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) ‚Äî Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) ‚Äî Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False‚Äú) ‚Äî Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*)
    ‚Äî Labels for computing the image classification/regression loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression
    loss is computed (Mean-Square loss), If `config.num_labels > 1` a classification
    loss is computed (Cross-Entropy).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention` or
    `tuple(tf.Tensor)`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    and inputs.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention` ÊàñËÄÖ‰∏Ä‰∏™
    `tf.Tensor` ÂÖÉÁªÑÔºàÂ¶ÇÊûú‰º†ÂÖ•‰∫Ü `return_dict=False` ÊàñËÄÖ `config.return_dict=False`ÔºâÂåÖÂê´‰∏çÂêåÁöÑÂÖÉÁ¥†ÔºåÂèñÂÜ≥‰∫éÈÖçÁΩÆÔºà[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)ÔºâÂíåËæìÂÖ•„ÄÇ'
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) ‚Äî Classification (or regression if config.num_labels==1) loss.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ÔºàÂΩ¢Áä∂‰∏∫ `(1,)` ÁöÑ `tf.Tensor`Ôºå*ÂèØÈÄâ*ÔºåÂΩìÊèê‰æõ‰∫Ü `labels` Êó∂ËøîÂõûÔºâ‚Äî ÂàÜÁ±ªÔºàÂ¶ÇÊûú `config.num_labels==1`
    Âàô‰∏∫ÂõûÂΩíÔºâÊçüÂ§±„ÄÇ'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) ‚Äî Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ÔºàÂΩ¢Áä∂‰∏∫ `(batch_size, config.num_labels)` ÁöÑ `tf.Tensor`Ôºâ‚Äî ÂàÜÁ±ªÔºàÂ¶ÇÊûú `config.num_labels==1`
    Âàô‰∏∫ÂõûÂΩíÔºâÂæóÂàÜÔºàSoftMax ‰πãÂâçÔºâ„ÄÇ'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) ‚Äî Tuple of `tf.Tensor` (one
    for the output of the embeddings, if the model has an embedding layer, + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the model at the output of each stage.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`Ôºà`tuple(tf.Tensor)`Ôºå*ÂèØÈÄâ*ÔºåÂΩì‰º†ÂÖ• `output_hidden_states=True` ÊàñËÄÖ
    `config.output_hidden_states=True` Êó∂ËøîÂõûÔºâ‚Äî ÂΩ¢Áä∂‰∏∫ `(batch_size, num_channels, height,
    width)` ÁöÑ `tf.Tensor` ÂÖÉÁªÑ„ÄÇÊ®°ÂûãÂú®ÊØè‰∏™Èò∂ÊÆµËæìÂá∫ÁöÑÈöêËóèÁä∂ÊÄÅÔºà‰πüÁß∞‰∏∫ÁâπÂæÅÂõæÔºâ„ÄÇ'
- en: The [TFCvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFCvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtForImageClassification)
    ÁöÑÂâçÂêëÊñπÊ≥ïÔºåÈáçÂÜô‰∫Ü `__call__` ÁâπÊÆäÊñπÊ≥ï„ÄÇ'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ËôΩÁÑ∂ÂâçÂêë‰º†ÈÄíÁöÑÊñπÊ≥ïÈúÄË¶ÅÂú®Ê≠§ÂáΩÊï∞ÂÜÖÂÆö‰πâÔºå‰ΩÜÂ∫îËØ•Âú®Ê≠§‰πãÂêéË∞ÉÁî® `Module` ÂÆû‰æãÔºåËÄå‰∏çÊòØËøô‰∏™ÂáΩÊï∞ÔºåÂõ†‰∏∫ÂâçËÄÖ‰ºöË¥üË¥£ËøêË°åÈ¢ÑÂ§ÑÁêÜÂíåÂêéÂ§ÑÁêÜÊ≠•È™§ÔºåËÄåÂêéËÄÖ‰ºöÈªòÈªòÂú∞ÂøΩÁï•ÂÆÉ‰ª¨„ÄÇ
- en: 'Examples:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Á§∫‰æãÔºö
- en: '[PRE13]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
