- en: Convolutional Vision Transformer (CvT)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积视觉Transformer（CvT）
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cvt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cvt)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cvt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cvt)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The CvT model was proposed in [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808)
    by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan and Lei
    Zhang. The Convolutional vision Transformer (CvT) improves the [Vision Transformer
    (ViT)](vit) in performance and efficiency by introducing convolutions into ViT
    to yield the best of both designs.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'CvT模型由吴海平、肖斌、诺尔·科代拉、刘梦晨、戴希扬、袁璐和张磊在[CvT: Introducing Convolutions to Vision
    Transformers](https://arxiv.org/abs/2103.15808)中提出。卷积视觉Transformer（CvT）通过将卷积引入ViT中，提高了[视觉Transformer（ViT）](vit)的性能和效率，以获得这两种设计的最佳效果。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*We present in this paper a new architecture, named Convolutional vision Transformer
    (CvT), that improves Vision Transformer (ViT) in performance and efficiency by
    introducing convolutions into ViT to yield the best of both designs. This is accomplished
    through two primary modifications: a hierarchy of Transformers containing a new
    convolutional token embedding, and a convolutional Transformer block leveraging
    a convolutional projection. These changes introduce desirable properties of convolutional
    neural networks (CNNs) to the ViT architecture (\ie shift, scale, and distortion
    invariance) while maintaining the merits of Transformers (\ie dynamic attention,
    global context, and better generalization). We validate CvT by conducting extensive
    experiments, showing that this approach achieves state-of-the-art performance
    over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters
    and lower FLOPs. In addition, performance gains are maintained when pretrained
    on larger datasets (\eg ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained
    on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7\% on the ImageNet-1k
    val set. Finally, our results show that the positional encoding, a crucial component
    in existing Vision Transformers, can be safely removed in our model, simplifying
    the design for higher resolution vision tasks.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们在本文中提出了一种名为卷积视觉Transformer（CvT）的新架构，通过将卷积引入ViT中，提高了ViT的性能和效率，以获得这两种设计的最佳效果。这通过两个主要修改实现：包含新的卷积标记嵌入的Transformer层次结构，以及利用卷积投影的卷积Transformer块。这些改变将卷积神经网络（CNN）的理想特性引入ViT架构（即平移、缩放和失真不变性），同时保持Transformer的优点（即动态注意力、全局上下文和更好的泛化）。我们通过进行广泛实验验证了CvT，显示这种方法在ImageNet-1k上实现了其他视觉Transformer和ResNet的最新性能，参数更少，FLOPs更低。此外，当在更大的数据集（例如ImageNet-22k）上进行预训练并微调到下游任务时，性能增益得以保持。在ImageNet-22k上预训练，我们的CvT-W24在ImageNet-1k验证集上获得了87.7\%的top-1准确率。最后，我们的结果表明，位置编码，现有视觉Transformer中的关键组件，可以在我们的模型中安全地移除，简化了更高分辨率视觉任务的设计。*'
- en: This model was contributed by [anugunj](https://huggingface.co/anugunj). The
    original code can be found [here](https://github.com/microsoft/CvT).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[anugunj](https://huggingface.co/anugunj)贡献。原始代码可以在[这里](https://github.com/microsoft/CvT)找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: CvT models are regular Vision Transformers, but trained with convolutions. They
    outperform the [original model (ViT)](vit) when fine-tuned on ImageNet-1K and
    CIFAR-100.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CvT模型是常规的视觉Transformer，但是经过卷积训练。当在ImageNet-1K和CIFAR-100上进行微调时，它们的性能优于[原始模型（ViT）](vit)。
- en: You can check out demo notebooks regarding inference as well as fine-tuning
    on custom data [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)
    (you can just replace [ViTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTFeatureExtractor)
    by [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    and [ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)
    by [CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以查看关于推理以及在自定义数据上进行微调的演示笔记本[这里](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)（您只需将[ViTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTFeatureExtractor)替换为[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)，将[ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)替换为[CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)）。
- en: The available checkpoints are either (1) pre-trained on [ImageNet-22k](http://www.image-net.org/)
    (a collection of 14 million images and 22k classes) only, (2) also fine-tuned
    on ImageNet-22k or (3) also fine-tuned on [ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/)
    (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000
    classes).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用的检查点要么（1）仅在[ImageNet-22k](http://www.image-net.org/)（包含1400万图像和22k类别）上进行预训练，要么（2）在ImageNet-22k上进行微调，要么（3）在[ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/)（也称为ILSVRC
    2012，包含130万图像和1000类别）上进行微调。
- en: Resources
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with CvT.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一份官方Hugging Face和社区（由🌎表示）资源列表，帮助您开始使用CvT。
- en: Image Classification
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类
- en: '[CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)由这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)支持。'
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另请参阅：[图像分类任务指南](../tasks/image_classification)
- en: If you’re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we’ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将进行审查！资源应该展示一些新内容，而不是重复现有资源。
- en: CvtConfig
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CvtConfig
- en: '### `class transformers.CvtConfig`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CvtConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/configuration_cvt.py#L29)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/configuration_cvt.py#L29)'
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *optional*, defaults to 3) — 输入通道数'
- en: '`patch_sizes` (`List[int]`, *optional*, defaults to `[7, 3, 3]`) — The kernel
    size of each encoder’s patch embedding.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_sizes` (`List[int]`, *optional*, defaults to `[7, 3, 3]`) — 每个编码器块的补丁嵌入的内核大小'
- en: '`patch_stride` (`List[int]`, *optional*, defaults to `[4, 2, 2]`) — The stride
    size of each encoder’s patch embedding.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_stride` (`List[int]`, *optional*, defaults to `[4, 2, 2]`) — 每个编码器块的补丁嵌入的步幅大小'
- en: '`patch_padding` (`List[int]`, *optional*, defaults to `[2, 1, 1]`) — The padding
    size of each encoder’s patch embedding.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_padding` (`List[int]`, *optional*, defaults to `[2, 1, 1]`) — 每个编码器块的补丁嵌入的填充大小'
- en: '`embed_dim` (`List[int]`, *optional*, defaults to `[64, 192, 384]`) — Dimension
    of each of the encoder blocks.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embed_dim` (`List[int]`, *optional*, defaults to `[64, 192, 384]`) — 每个编码器块的维度'
- en: '`num_heads` (`List[int]`, *optional*, defaults to `[1, 3, 6]`) — Number of
    attention heads for each attention layer in each block of the Transformer encoder.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_heads` (`List[int]`, *optional*, defaults to `[1, 3, 6]`) — 每个Transformer编码器块中每个注意力层的注意力头数。'
- en: '`depth` (`List[int]`, *optional*, defaults to `[1, 2, 10]`) — The number of
    layers in each encoder block.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depth` (`List[int]`, *optional*, defaults to `[1, 2, 10]`) — 每个编码器块中的层数'
- en: '`mlp_ratios` (`List[float]`, *optional*, defaults to `[4.0, 4.0, 4.0, 4.0]`)
    — Ratio of the size of the hidden layer compared to the size of the input layer
    of the Mix FFNs in the encoder blocks.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp_ratios` (`List[float]`, *optional*, defaults to `[4.0, 4.0, 4.0, 4.0]`)
    — 在编码器块中Mix FFN的隐藏层大小与输入层大小的比率'
- en: '`attention_drop_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`)
    — The dropout ratio for the attention probabilities.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_drop_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`)
    — 注意力概率的丢弃比率'
- en: '`drop_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`) — The
    dropout ratio for the patch embeddings probabilities.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`) — 补丁嵌入概率的丢弃比率'
- en: '`drop_path_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.1]`)
    — The dropout probability for stochastic depth, used in the blocks of the Transformer
    encoder.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_path_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.1]`)
    — 随机深度的丢弃概率，用于Transformer编码器块中'
- en: '`qkv_bias` (`List[bool]`, *optional*, defaults to `[True, True, True]`) — The
    bias bool for query, key and value in attentions'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_bias` (`List[bool]`, *optional*, defaults to `[True, True, True]`) — 查询、键和值的注意力中的偏置布尔值'
- en: '`cls_token` (`List[bool]`, *optional*, defaults to `[False, False, True]`)
    — Whether or not to add a classification token to the output of each of the last
    3 stages.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`List[bool]`, *optional*, defaults to `[False, False, True]`)
    — 是否向每个最后3个阶段的输出添加分类令牌'
- en: '`qkv_projection_method` (`List[string]`, *optional*, defaults to [“dw_bn”,
    “dw_bn”, “dw_bn”]`) — The projection method for query, key and value Default is
    depth-wise convolutions with batch norm. For Linear projection use “avg”.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_projection_method` (`List[string]`, *optional*, defaults to [“dw_bn”,
    “dw_bn”, “dw_bn”]`) — 查询、键和值的投影方法，默认为深度卷积和批量归一化。使用“avg”进行线性投影。'
- en: '`kernel_qkv` (`List[int]`, *optional*, defaults to `[3, 3, 3]`) — The kernel
    size for query, key and value in attention layer'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel_qkv` (`List[int]`, *optional*, defaults to `[3, 3, 3]`) — 注意力层中查询、键和值的内核大小'
- en: '`padding_kv` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) — The padding
    size for key and value in attention layer'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_kv` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) — 注意力层中键和值的填充大小'
- en: '`stride_kv` (`List[int]`, *optional*, defaults to `[2, 2, 2]`) — The stride
    size for key and value in attention layer'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride_kv` (`List[int]`, *optional*, defaults to `[2, 2, 2]`) — 注意力层中键和值的步幅大小'
- en: '`padding_q` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) — The padding
    size for query in attention layer'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_q` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) — 注意力层中查询的填充大小'
- en: '`stride_q` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) — The stride
    size for query in attention layer'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride_q` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) — 查询在注意力层中的步幅大小'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-6) — The epsilon used
    by the layer normalization layers.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-6) — 层归一化层使用的epsilon'
- en: This is the configuration class to store the configuration of a [CvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtModel).
    It is used to instantiate a CvT model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the CvT [microsoft/cvt-13](https://huggingface.co/microsoft/cvt-13)
    architecture.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[CvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtModel)配置的配置类。它用于根据指定的参数实例化CvT模型，定义模型架构。使用默认值实例化配置将产生类似于CvT
    [microsoft/cvt-13](https://huggingface.co/microsoft/cvt-13)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: PytorchHide Pytorch content
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch内容
- en: CvtModel
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CvtModel
- en: '### `class transformers.CvtModel`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CvtModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L586)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L586)'
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare Cvt Model transformer outputting raw hidden-states without any specific
    head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 裸Cvt模型变压器输出原始隐藏状态，没有特定的头部。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。
- en: '#### `forward`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L605)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L605)'
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `CvtImageProcessor.__call__` for details.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）-
    像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅`CvtImageProcessor.__call__`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken` or `tuple(torch.FloatTensor)`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken` or a tuple
    of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    and inputs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)）和输入的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）-
    模型最后一层的隐藏状态序列。'
- en: '`cls_token_value` (`torch.FloatTensor` of shape `(batch_size, 1, hidden_size)`)
    — Classification token at the output of the last layer of the model.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token_value`（形状为`(batch_size, 1, hidden_size)`的`torch.FloatTensor`）- 模型最后一层的分类令牌。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。模型在每一层的输出的隐藏状态
    + 初始嵌入输出。'
- en: The [CvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtModel)
    forward method, overrides the `__call__` special method.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[CvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtModel)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: CvtForImageClassification
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CvtForImageClassification
- en: '### `class transformers.CvtForImageClassification`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CvtForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L644)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L644)'
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([CvtConfig](/docs/transformers/v4.37.2/zh/model_doc/cvt#transformers.CvtConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/zh/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: Cvt Model transformer with an image classification head on top (a linear layer
    on top of the final hidden state of the [CLS] token) e.g. for ImageNet.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 带有图像分类头部的 Cvt 模型变压器（在 [CLS] 标记的最终隐藏状态之上的线性层），例如用于 ImageNet。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `前进`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L666)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[< 源代码 >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L666)'
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `CvtImageProcessor.__call__` for details.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height,
    width)`） — 像素值。可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/zh/model_doc/auto#transformers.AutoImageProcessor)
    获取像素值。有关详细信息，请参阅 `CvtImageProcessor.__call__`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为 `(batch_size,)`，*可选*) — 用于计算图像分类/回归损失的标签。索引应在
    `[0, ..., config.num_labels - 1]` 范围内。如果 `config.num_labels == 1`，则计算回归损失（均方损失），如果
    `config.num_labels > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 返回结果
- en: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or `tuple(torch.FloatTensor)`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    and inputs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False`
    时）包含根据配置（[CvtConfig](/docs/transformers/v4.37.2/zh/model_doc/cvt#transformers.CvtConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为 `(1,)`，*可选*，当提供 `labels` 时返回) — 分类（如果 config.num_labels==1
    则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为 `(batch_size, config.num_labels)`） — 分类（如果
    config.num_labels==1 则为回归）分数（SoftMax 之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each stage) of shape `(batch_size, num_channels, height,
    width)`. Hidden-states (also called feature maps) of the model at the output of
    each stage.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递了 `output_hidden_states=True`
    或当 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, num_channels, height,
    width)` 的 `torch.FloatTensor` 元组。模型在每个阶段输出的隐藏状态（也称为特征图）。'
- en: The [CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[CvtForImageClassification](/docs/transformers/v4.37.2/zh/model_doc/cvt#transformers.CvtForImageClassification)
    前进方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: TensorFlowHide TensorFlow content
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 隐藏 TensorFlow 内容
- en: TFCvtModel
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFCvtModel
- en: '### `class transformers.TFCvtModel`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '### `类 transformers.TFCvtModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L926)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L926)'
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The bare Cvt Model transformer outputting raw hidden-states without any specific
    head on top.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 裸 Cvt 模型变压器输出原始隐藏状态，没有特定的顶部头。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个 [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    子类。将其用作常规的 TF 2.0 Keras 模型，并参考 TF 2.0 文档以获取有关一般用法和行为的所有相关信息。
- en: 'TF 2.0 models accepts two formats as inputs:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: TF 2.0 模型接受两种格式的输入：
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（如 PyTorch 模型），或
- en: having all inputs as a list, tuple or dict in the first positional arguments.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典放在第一个位置参数中。
- en: 'This second option is useful when using `tf.keras.Model.fit` method which currently
    requires having all the tensors in the first argument of the model call function:
    `model(inputs)`.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 `tf.keras.Model.fit` 方法时，第二个选项很有用，该方法当前要求在模型调用函数的第一个参数中具有所有张量：`model(inputs)`。
- en: '#### `call`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L936)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L936)'
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) — Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `CvtImageProcessor.__call__` for details.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（`np.ndarray`、`tf.Tensor`、`List[tf.Tensor]`、`Dict[str, tf.Tensor]`
    或 `Dict[str, np.ndarray]`，每个示例的形状必须为`(batch_size, num_channels, height, width)`）—
    像素值。像素值可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    获取。有关详细信息，请参阅 `CvtImageProcessor.__call__`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下可用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是一个普通元组。此参数可以在急切模式下使用，在图模式下该值将始终设置为 True。'
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training`（`bool`，*可选*，默认为`False`）— 是否在训练模式下使用模型（某些模块如丢弃模块在训练和评估之间有不同的行为）。'
- en: Returns
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` or
    `tuple(tf.Tensor)`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` 或 `tuple(tf.Tensor)`'
- en: A `transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` or
    a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    and inputs.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` 或一个
    `tf.Tensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)）和输入的各种元素。
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`）—
    模型最后一层的隐藏状态序列。'
- en: '`cls_token_value` (`tf.Tensor` of shape `(batch_size, 1, hidden_size)`) — Classification
    token at the output of the last layer of the model.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token_value`（形状为`(batch_size, 1, hidden_size)`的`tf.Tensor`）— 模型最后一层的分类标记。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递 `output_hidden_states=True` 或当
    `config.output_hidden_states=True` 时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的
    `tf.Tensor` 元组（一个用于嵌入的输出 + 一个用于每个层的输出）。模型在每一层的输出隐藏状态加上初始嵌入输出。'
- en: The [TFCvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtModel)
    forward method, overrides the `__call__` special method.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFCvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: TFCvtForImageClassification
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFCvtForImageClassification
- en: '### `class transformers.TFCvtForImageClassification`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFCvtForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L995)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L995)'
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Cvt Model transformer with an image classification head on top (a linear layer
    on top of the final hidden state of the [CLS] token) e.g. for ImageNet.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Cvt模型变压器，顶部带有图像分类头（在[CLS]标记的最终隐藏状态之上的线性层），例如用于ImageNet。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为其所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有事项。
- en: 'TF 2.0 models accepts two formats as inputs:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: TF 2.0模型接受两种格式的输入：
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或
- en: having all inputs as a list, tuple or dict in the first positional arguments.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典放在第一个位置参数中。
- en: 'This second option is useful when using `tf.keras.Model.fit` method which currently
    requires having all the tensors in the first argument of the model call function:
    `model(inputs)`.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`tf.keras.Model.fit`方法时，第二个选项很有用，该方法当前要求在模型调用函数的第一个参数中具有所有张量：`model(inputs)`。
- en: '#### `call`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L1021)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L1021)'
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) — Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `CvtImageProcessor.__call__` for details.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（`np.ndarray`、`tf.Tensor`、`List[tf.Tensor]`、`Dict[str, tf.Tensor]`或`Dict[str,
    np.ndarray]`，每个示例的形状必须为`(batch_size, num_channels, height, width)`）— 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参见`CvtImageProcessor.__call__`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。此参数仅在急切模式下可用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为True。'
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training`（`bool`，*可选*，默认为`False`）— 是否在训练模式下使用模型（某些模块如dropout模块在训练和评估之间具有不同的行为）。'
- en: '`labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*)
    — Labels for computing the image classification/regression loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression
    loss is computed (Mean-Square loss), If `config.num_labels > 1` a classification
    loss is computed (Cross-Entropy).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`tf.Tensor`或`np.ndarray`，*可选*）— 用于计算图像分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention` or
    `tuple(tf.Tensor)`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`或`tuple(tf.Tensor)`'
- en: A `transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig))
    and inputs.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention` 或者一个
    `tf.Tensor` 元组（如果传入了 `return_dict=False` 或者 `config.return_dict=False`）包含不同的元素，取决于配置（[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)）和输入。'
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为 `(1,)` 的 `tf.Tensor`，*可选*，当提供了 `labels` 时返回）— 分类（如果 `config.num_labels==1`
    则为回归）损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为 `(batch_size, config.num_labels)` 的 `tf.Tensor`）— 分类（如果 `config.num_labels==1`
    则为回归）得分（SoftMax 之前）。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings, if the model has an embedding layer, + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the model at the output of each stage.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传入 `output_hidden_states=True` 或者
    `config.output_hidden_states=True` 时返回）— 形状为 `(batch_size, num_channels, height,
    width)` 的 `tf.Tensor` 元组。模型在每个阶段输出的隐藏状态（也称为特征图）。'
- en: The [TFCvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFCvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtForImageClassification)
    的前向方法，重写了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是这个函数，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE13]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
