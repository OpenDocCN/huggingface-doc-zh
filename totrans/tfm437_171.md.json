["```py\n>>> from transformers import FSMTConfig, FSMTModel\n\n>>> # Initializing a FSMT facebook/wmt19-en-ru style configuration\n>>> config = FSMTConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = FSMTModel(config)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence |\n```", "```py\n>>> from transformers import AutoTokenizer, FSMTModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/wmt19-ru-en\")\n>>> model = FSMTModel.from_pretrained(\"facebook/wmt19-ru-en\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, FSMTForConditionalGeneration\n\n>>> mname = \"facebook/wmt19-ru-en\"\n>>> model = FSMTForConditionalGeneration.from_pretrained(mname)\n>>> tokenizer = AutoTokenizer.from_pretrained(mname)\n\n>>> src_text = \"\u041c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 - \u044d\u0442\u043e \u0437\u0434\u043e\u0440\u043e\u0432\u043e, \u043d\u0435 \u0442\u0430\u043a \u043b\u0438?\"\n>>> input_ids = tokenizer(src_text, return_tensors=\"pt\").input_ids\n>>> outputs = model.generate(input_ids, num_beams=5, num_return_sequences=3)\n>>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n\"Machine learning is great, isn't it?\"\n```"]