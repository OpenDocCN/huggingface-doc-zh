- en: InstructBLIP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/instructblip](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/instructblip)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The InstructBLIP model was proposed in [InstructBLIP: Towards General-purpose
    Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500)
    by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng
    Wang, Boyang Li, Pascale Fung, Steven Hoi. InstructBLIP leverages the [BLIP-2](blip2)
    architecture for visual instruction tuning.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '*General-purpose language models that can solve various language-domain tasks
    have emerged driven by the pre-training and instruction-tuning pipeline. However,
    building general-purpose vision-language models is challenging due to the increased
    task discrepancy introduced by the additional visual input. Although vision-language
    pre-training has been widely studied, vision-language instruction tuning remains
    relatively less explored. In this paper, we conduct a systematic and comprehensive
    study on vision-language instruction tuning based on the pre-trained BLIP-2 models.
    We gather a wide variety of 26 publicly available datasets, transform them into
    instruction tuning format and categorize them into two clusters for held-in instruction
    tuning and held-out zero-shot evaluation. Additionally, we introduce instruction-aware
    visual feature extraction, a crucial method that enables the model to extract
    informative features tailored to the given instruction. The resulting InstructBLIP
    models achieve state-of-the-art zero-shot performance across all 13 held-out datasets,
    substantially outperforming BLIP-2 and the larger Flamingo. Our models also lead
    to state-of-the-art performance when finetuned on individual downstream tasks
    (e.g., 90.7% accuracy on ScienceQA IMG). Furthermore, we qualitatively demonstrate
    the advantages of InstructBLIP over concurrent multimodal models.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![drawing](../Images/d7b3694f8c1b3e7e48722829f18a9625.png) InstructBLIP architecture.
    Taken from the [original paper.](https://arxiv.org/abs/2305.06500)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code can be found [here](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'InstructBLIP uses the same architecture as [BLIP-2](blip2) with a tiny but
    important difference: it also feeds the text prompt (instruction) to the Q-Former.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: InstructBlipConfig
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.InstructBlipConfig`'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/configuration_instructblip.py#L253)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '`vision_config` (`dict`, *optional*) — Dictionary of configuration options
    used to initialize [InstructBlipVisionConfig](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipVisionConfig).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qformer_config` (`dict`, *optional*) — Dictionary of configuration options
    used to initialize [InstructBlipQFormerConfig](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipQFormerConfig).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_config` (`dict`, *optional*) — Dictionary of configuration options used
    to initialize any [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_query_tokens` (`int`, *optional*, defaults to 32) — The number of query
    tokens passed through the Transformer.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (*optional*) — Dictionary of keyword arguments.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[InstructBlipConfig](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipConfig)
    is the configuration class to store the configuration of a [InstructBlipForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipForConditionalGeneration).
    It is used to instantiate a InstructBLIP model according to the specified arguments,
    defining the vision model, Q-Former model and language model configs. Instantiating
    a configuration with the defaults will yield a similar configuration to that of
    the InstructBLIP [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5)
    architecture.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#### `from_vision_qformer_text_configs`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/configuration_instructblip.py#L338)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Returns
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[InstructBlipConfig](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipConfig)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: An instance of a configuration object
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a [InstructBlipConfig](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipConfig)
    (or a derived class) from a InstructBLIP vision model, Q-Former and language model
    configurations.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: InstructBlipVisionConfig
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.InstructBlipVisionConfig`'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/configuration_instructblip.py#L33)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 1408) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intermediate_size` (`int`, *optional*, defaults to 6144) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 39) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 16) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_size` (`int`, *optional*, defaults to 224) — The size (resolution) of
    each image.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_size` (`int`, *optional*, defaults to 14) — The size (resolution) of
    each patch.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` ``"gelu"` are supported.
    to 1e-5): The epsilon used by the layer normalization layers.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-06) — The epsilon used
    by the layer normalization layers.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for the attention probabilities.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 1e-10) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — Whether to add a bias
    to the queries and values in the self-attention layers.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [InstructBlipVisionModel](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipVisionModel).
    It is used to instantiate a InstructBLIP vision encoder according to the specified
    arguments, defining the model architecture. Instantiating a configuration defaults
    will yield a similar configuration to that of the InstructBLIP [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5)
    architecture.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: InstructBlipQFormerConfig
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InstructBlipQFormerConfig
- en: '### `class transformers.InstructBlipQFormerConfig`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.InstructBlipQFormerConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/configuration_instructblip.py#L134)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/configuration_instructblip.py#L134)'
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) — Vocabulary size of the
    Q-Former model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling the model.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, 默认为30522) — Q-Former模型的词汇量。定义了在调用模型时可以表示的不同标记数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, 默认为768) — 编码器层和池化层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, 默认为12) — Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, 默认为12) — Transformer编码器中每个注意力层的注意力头数量。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (often named feed-forward) layer in the Transformer encoder.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, 默认为3072) — Transformer编码器中“中间”（通常称为前馈）层的维度。'
- en: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`或`Callable`, *optional*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, 默认为0.1) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — The
    dropout ratio for the attention probabilities.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, 默认为0.1) — 注意力概率的dropout比率。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, 默认为512) — 此模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如512、1024或2048）。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, 默认为1e-12) — 层归一化层使用的epsilon。'
- en: '`position_embedding_type` (`str`, *optional*, defaults to `"absolute"`) — Type
    of position embedding. Choose one of `"absolute"`, `"relative_key"`, `"relative_key_query"`.
    For positional embeddings use `"absolute"`. For more information on `"relative_key"`,
    please refer to [Self-Attention with Relative Position Representations (Shaw et
    al.)](https://arxiv.org/abs/1803.02155). For more information on `"relative_key_query"`,
    please refer to *Method 4* in [Improve Transformer Models with Better Relative
    Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_embedding_type` (`str`, *optional*, 默认为`"absolute"`) — 位置嵌入的类型。选择`"absolute"`、`"relative_key"`或`"relative_key_query"`中的一个。对于位置嵌入，请使用`"absolute"`。有关`"relative_key"`的更多信息，请参考[Self-Attention
    with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)。有关`"relative_key_query"`的更多信息，请参考[Improve
    Transformer Models with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658)中的*Method
    4*。'
- en: '`cross_attention_frequency` (`int`, *optional*, defaults to 2) — The frequency
    of adding cross-attention to the Transformer layers.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_frequency` (`int`, *optional*, 默认为2) — 向Transformer层添加交叉注意力的频率。'
- en: '`encoder_hidden_size` (`int`, *optional*, defaults to 1408) — The hidden size
    of the hidden states for cross-attention.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_size` (`int`, *optional*, 默认为1408) — 用于交叉注意力的隐藏状态的隐藏大小。'
- en: This is the configuration class to store the configuration of a [InstructBlipQFormerModel](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipQFormerModel).
    It is used to instantiate a InstructBLIP Querying Transformer (Q-Former) model
    according to the specified arguments, defining the model architecture. Instantiating
    a configuration with the defaults will yield a similar configuration to that of
    the InstructBLIP [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5)
    architecture. Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[InstructBlipQFormerModel](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipQFormerModel)的配置。它用于根据指定的参数实例化一个InstructBLIP
    Querying Transformer (Q-Former)模型，定义模型架构。使用默认值实例化配置将产生类似于InstructBLIP [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5)架构的配置。配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: Note that [InstructBlipQFormerModel](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipQFormerModel)
    is very similar to [BertLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertLMHeadModel)
    with interleaved cross-attention.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[InstructBlipQFormerModel](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipQFormerModel)与[BertLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertLMHeadModel)非常相似，具有交错的交叉注意力。
- en: 'Examples:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: InstructBlipProcessor
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InstructBlipProcessor
- en: '### `class transformers.InstructBlipProcessor`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.InstructBlipProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/processing_instructblip.py#L30)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/processing_instructblip.py#L30)'
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image_processor` (`BlipImageProcessor`) — An instance of [BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor).
    The image processor is a required input.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor` (`BlipImageProcessor`) — [BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)的一个实例。图像处理器是必需的输入。'
- en: '`tokenizer` (`AutoTokenizer`) — An instance of [‘PreTrainedTokenizer`]. The
    tokenizer is a required input.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`AutoTokenizer`) — [‘PreTrainedTokenizer`]的一个实例。分词器是必需的输入。'
- en: '`qformer_tokenizer` (`AutoTokenizer`) — An instance of [‘PreTrainedTokenizer`].
    The Q-Former tokenizer is a required input.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qformer_tokenizer` (`AutoTokenizer`) — [‘PreTrainedTokenizer`]的一个实例。Q-Former
    tokenizer是必需的输入。'
- en: Constructs an InstructBLIP processor which wraps a BLIP image processor and
    a LLaMa/T5 tokenizer into a single processor.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个InstructBLIP处理器，将BLIP图像处理器和LLaMa/T5分词器包装成一个单一处理器。
- en: '[InstructBlipProcessor](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipProcessor)
    offers all the functionalities of [BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)
    and [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See the docstring of `__call__()` and [decode()](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipProcessor.decode)
    for more information.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[InstructBlipProcessor](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipProcessor)提供了[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)和[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)的所有功能。查看`__call__()`和[decode()](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipProcessor.decode)的文档字符串以获取更多信息。'
- en: '#### `batch_decode`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/processing_instructblip.py#L136)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/processing_instructblip.py#L136)'
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This method forwards all its arguments to PreTrainedTokenizer’s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将其所有参数转发给PreTrainedTokenizer的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。有关更多信息，请参阅此方法的文档字符串。
- en: '#### `decode`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/processing_instructblip.py#L144)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/processing_instructblip.py#L144)'
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This method forwards all its arguments to PreTrainedTokenizer’s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将其所有参数转发给PreTrainedTokenizer的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。有关更多信息，请参阅此方法的文档字符串。
- en: InstructBlipVisionModel
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InstructBlipVisionModel
- en: '### `class transformers.InstructBlipVisionModel`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.InstructBlipVisionModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L490)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L490)'
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#### `forward`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L505)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L505)'
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [InstructBlipProcessor](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipProcessor).
    See `InstructBlipProcessor.__call__()` for details.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height,
    width)`) — 像素值。像素值可以使用[InstructBlipProcessor](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipProcessor)获取。有关详细信息，请参阅`InstructBlipProcessor.__call__()`。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: Returns
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.instructblip.configuration_instructblip.InstructBlipVisionConfig'>`)
    and inputs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含各种元素，这取决于配置（`<class
    'transformers.models.instructblip.configuration_instructblip.InstructBlipVisionConfig'>`）和输入。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`) — 经过用于辅助预训练任务的层进一步处理后，序列中第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出和每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [InstructBlipVisionModel](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipVisionModel)
    forward method, overrides the `__call__` special method.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[InstructBlipVisionModel](/docs/transformers/v4.37.2/zh/model_doc/instructblip#transformers.InstructBlipVisionModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: InstructBlipQFormerModel
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InstructBlipQFormerModel
- en: '### `class transformers.InstructBlipQFormerModel`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.InstructBlipQFormerModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L1035)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L1035)'
- en: '[PRE12]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Querying Transformer (Q-Former), used in InstructBLIP. Slightly modified from
    BLIP-2 as it also takes the instruction as input.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 查询变换器（Q-Former），用于InstructBLIP。与BLIP-2略有修改，因为它还将指令作为输入。
- en: '#### `forward`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L1108)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L1108)'
- en: '[PRE13]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*): Sequence of hidden-states at the output of the last
    layer of the encoder. Used in the cross-attention if the model is configured as
    a decoder. encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size,
    sequence_length)`, *optional*): Mask to avoid performing attention on the padding
    token indices of the encoder input. This mask is used in the cross-attention if
    the model is configured as a decoder. Mask values selected in `[0, 1]`:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: encoder_hidden_states（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）：编码器最后一层的隐藏状态序列。如果模型配置为解码器，则在交叉注意力中使用。encoder_attention_mask（形状为`(batch_size,
    sequence_length)`的`torch.FloatTensor`，*可选*）：避免对编码器输入的填充标记索引执行注意力的掩码。如果模型配置为解码器，则在交叉注意力中使用。选择的掩码值为`[0,
    1]`：
- en: 1 for tokens that are `not masked`,
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记，值为1。
- en: '0 for tokens that are `masked`. past_key_values (`tuple(tuple(torch.FloatTensor))`
    of length `config.n_layers` with each tuple having 4 tensors of: shape `(batch_size,
    num_heads, sequence_length - 1, embed_size_per_head)`): Contains precomputed key
    and value hidden states of the attention blocks. Can be used to speed up decoding.
    If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
    use_cache (`bool`, *optional*): If set to `True`, `past_key_values` key value
    states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记，值为0。past_key_values（长度为`config.n_layers`的`tuple(tuple(torch.FloatTensor))`，每个元组有4个张量，形状为`(batch_size,
    num_heads, sequence_length - 1, embed_size_per_head)`）：包含注意力块的预计算键和值隐藏状态。可用于加速解码。如果使用`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（那些没有将其过去的键值状态提供给此模型的）的形状为`(batch_size,
    1)`，而不是所有形状为`(batch_size, sequence_length)`的`decoder_input_ids`。use_cache（`bool`，*可选*）：如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。
- en: InstructBlipForConditionalGeneration
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InstructBlipForConditionalGeneration
- en: '### `class transformers.InstructBlipForConditionalGeneration`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.InstructBlipForConditionalGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L1231)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L1231)'
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([InstructBlipConfig](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[InstructBlipConfig](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipConfig)）—模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: InstructBLIP Model for generating text given an image and an optional text prompt.
    The model consists of a vision encoder, Querying Transformer (Q-Former) and a
    language model.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: InstructBLIP模型用于根据图像和可选文本提示生成文本。该模型由视觉编码器、查询变换器（Q-Former）和语言模型组成。
- en: One can optionally pass `input_ids` to the model, which serve as a text prompt,
    to make the language model continue the prompt. Otherwise, the language model
    starts generating text from the [BOS] (beginning-of-sequence) token.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 可以选择向模型传递`input_ids`，作为文本提示，以使语言模型继续提示。否则，语言模型将从[BOS]（序列开始）标记开始生成文本。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L1314)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L1314)'
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [InstructBlipProcessor](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipProcessor).
    See `InstructBlipProcessor.__call__()` for details.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—像素值。像素值可以使用[InstructBlipProcessor](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipProcessor)获取。有关详细信息，请参阅`InstructBlipProcessor.__call__()`。'
- en: '`qformer_input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of input sequence tokens in the vocabulary of the Q-Former.
    Input tokens can optionally be provided to serve as text prompt, which the Q-Former
    model will encode.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qformer_input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—Q-Former词汇表中输入序列标记的索引。可以选择提供输入标记作为文本提示，Q-Former模型将对其进行编码。'
- en: Indices can be obtained using [InstructBlipProcessor](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipProcessor).
    See `InstructBlipProcessor.__call__()` for details.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[InstructBlipProcessor](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipProcessor)获取索引。有关详细信息，请参阅`InstructBlipProcessor.__call__()`。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`qformer_attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qformer_attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of input sequence tokens in the vocabulary of the language model. Input
    tokens can optionally be provided to serve as text prompt, which the language
    model can continue.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — 输入序列标记在语言模型词汇中的索引。输入标记可以选择作为文本提示提供，语言模型可以继续。'
- en: Indices can be obtained using [InstructBlipProcessor](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipProcessor).
    See `InstructBlipProcessor.__call__()` for details.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[InstructBlipProcessor](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipProcessor)获取索引。有关详细信息，请参阅`InstructBlipProcessor.__call__()`。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary of the
    language model. Only relevant in case an encoder-decoder language model (like
    T5) is used.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — 解码器输入序列标记在语言模型词汇中的索引。仅在使用编码器-解码器语言模型（如T5）时相关。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are decoder input IDs?](../glossary#decoder-input-ids)
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。[什么是解码器输入ID？](../glossary#decoder-input-ids)
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。'
- en: Only relevant in case an encoder-decoder language model (like T5) is used.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 仅在使用编码器-解码器语言模型（如T5）时相关。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多细节，请参阅返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多细节，请参阅返回的张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the language modeling loss. Indices should be in `[-100, 0, ...,
    config.vocab_size - 1]`. All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — 用于计算语言建模损失的标签。索引应在`[-100,
    0, ..., config.vocab_size - 1]`中。所有设置为`-100`的标签将被忽略（被`masked`），损失仅计算标签在`[0, ...,
    config.vocab_size]`中的标签'
- en: Returns
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.instructblip.modeling_instructblip.InstructBlipForConditionalGenerationModelOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.instructblip.modeling_instructblip.InstructBlipForConditionalGenerationModelOutput`
    或 `tuple(torch.FloatTensor)`'
- en: A `transformers.models.instructblip.modeling_instructblip.InstructBlipForConditionalGenerationModelOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.instructblip.configuration_instructblip.InstructBlipVisionConfig'>`)
    and inputs.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.instructblip.modeling_instructblip.InstructBlipForConditionalGenerationModelOutput`
    或 `torch.FloatTensor` 元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class
    ''transformers.models.instructblip.configuration_instructblip.InstructBlipVisionConfig''>`）和输入的各种元素。'
- en: '`loss` (`torch.FloatTensor`, *optional*, returned when `labels` is provided,
    `torch.FloatTensor` of shape `(1,)`) — Language modeling loss from the language
    model.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`, *optional*, 当提供`labels`时返回，形状为`(1,)`) — 语言建模损失来自语言模型。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head of the language model.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — 语言模型的语言建模头的预测分数。'
- en: '`vision_outputs` (`BaseModelOutputWithPooling`) — Outputs of the vision encoder.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vision_outputs` (`BaseModelOutputWithPooling`) — 视觉编码器的输出。'
- en: '`qformer_outputs` (`BaseModelOutputWithPoolingAndCrossAttentions`) — Outputs
    of the Q-Former (Querying Transformer).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qformer_outputs` (`BaseModelOutputWithPoolingAndCrossAttentions`) — Q-Former（查询变换器）的输出。'
- en: '`language_model_outputs` (`CausalLMOutputWithPast` or `Seq2SeqLMOutput`) —
    Outputs of the language model.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`language_model_outputs` (`CausalLMOutputWithPast`或`Seq2SeqLMOutput`) — 语言模型的输出。'
- en: The [InstructBlipForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[InstructBlipForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/instructblip#transformers.InstructBlipForConditionalGeneration)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE16]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#### `generate`'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L1469)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/instructblip/modeling_instructblip.py#L1469)'
- en: '[PRE17]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape (batch_size, num_channels, height,
    width)) — Input images to be processed.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor` of shape (batch_size, num_channels, height,
    width)) — 要处理的输入图像。'
- en: '`qformer_input_ids` (`torch.LongTensor` of shape (batch_size, sequence_length),
    *optional*) — The sequence used as a prompt to be fed to the Q-Former module.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qformer_input_ids` (`torch.LongTensor` of shape (batch_size, sequence_length),
    *optional*) — 用作输入到Q-Former模块的提示序列。'
- en: '`qformer_attention_mask` (`torch.LongTensor` of shape (batch_size, sequence_length),
    *optional*) — Mask to avoid performing attention on padding token indices.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qformer_attention_mask` (`torch.LongTensor` of shape (batch_size, sequence_length),
    *optional*) — 用于避免在填充标记索引上执行注意力的掩码。'
- en: '`input_ids` (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*)
    — The sequence used as a prompt for the generation.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*)
    — 用作生成提示的序列。'
- en: '`attention_mask` (`torch.LongTensor` of shape (batch_size, sequence_length),
    *optional*) — Mask to avoid performing attention on padding token indices.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor` of shape (batch_size, sequence_length),
    *optional*) — 用于避免在填充标记索引上执行注意力的掩码。'
- en: Returns
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: captions (list)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 字幕（列表）
- en: A list of strings of length batch_size * num_captions.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一个长度为batch_size * num_captions的字符串列表。
- en: Overrides `generate` function to be able to use the model as a conditional generator.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖`generate`函数以能够将模型用作有条件生成器。
