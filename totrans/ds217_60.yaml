- en: Main classes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要类
- en: 'Original text: [https://huggingface.co/docs/datasets/package_reference/main_classes](https://huggingface.co/docs/datasets/package_reference/main_classes)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/datasets/package_reference/main_classes](https://huggingface.co/docs/datasets/package_reference/main_classes)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: DatasetInfo
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DatasetInfo
- en: '### `class datasets.DatasetInfo`'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class datasets.DatasetInfo`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L92)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L92)'
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`description` (`str`) — A description of the dataset.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`description` (`str`) — 数据集的描述。'
- en: '`citation` (`str`) — A BibTeX citation of the dataset.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`citation` (`str`) — 数据集的BibTeX引用。'
- en: '`homepage` (`str`) — A URL to the official homepage for the dataset.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`homepage` (`str`) — 数据集官方主页的URL。'
- en: '`license` (`str`) — The dataset’s license. It can be the name of the license
    or a paragraph containing the terms of the license.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`license` (`str`) — 数据集的许可证。可以是许可证的名称或包含许可证条款的段落。'
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — The features used to specify the dataset’s column types.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — 用于指定数据集列类型的特征。'
- en: '`post_processed` (`PostProcessedInfo`, *optional*) — Information regarding
    the resources of a possible post-processing of a dataset. For example, it can
    contain the information of an index.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`post_processed` (`PostProcessedInfo`, *optional*) — 有关数据集可能的后处理资源的信息。例如，它可以包含索引的信息。'
- en: '`supervised_keys` (`SupervisedKeysData`, *optional*) — Specifies the input
    feature and the label for supervised learning if applicable for the dataset (legacy
    from TFDS).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`supervised_keys` (`SupervisedKeysData`, *optional*) — 如果适用于数据集，则指定监督学习的输入特征和标签（来自TFDS的遗留）。'
- en: '`builder_name` (`str`, *optional*) — The name of the `GeneratorBasedBuilder`
    subclass used to create the dataset. Usually matched to the corresponding script
    name. It is also the snake_case version of the dataset builder class name.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`builder_name` (`str`, *optional*) — 用于创建数据集的`GeneratorBasedBuilder`子类的名称。通常与相应的脚本名称匹配。它也是数据集构建器类名称的蛇形命名版本。'
- en: '`config_name` (`str`, *optional*) — The name of the configuration derived from
    [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config_name` (`str`, *optional*) — 派生自[BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)的配置名称。'
- en: '`version` (`str` or [Version](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.Version),
    *optional*) — The version of the dataset.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`version` (`str` or [Version](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.Version),
    *optional*) — 数据集的版本。'
- en: '`splits` (`dict`, *optional*) — The mapping between split name and metadata.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`splits` (`dict`, *optional*) — 拆分名称和元数据之间的映射。'
- en: '`download_checksums` (`dict`, *optional*) — The mapping between the URL to
    download the dataset’s checksums and corresponding metadata.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`download_checksums` (`dict`, *optional*) — 下载数据集校验和及相应元数据之间的映射。'
- en: '`download_size` (`int`, *optional*) — The size of the files to download to
    generate the dataset, in bytes.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`download_size` (`int`, *optional*) — 下载文件以生成数据集的大小，以字节为单位。'
- en: '`post_processing_size` (`int`, *optional*) — Size of the dataset in bytes after
    post-processing, if any.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`post_processing_size` (`int`, *optional*) — 经过后处理后数据集的大小，以字节为单位（如果有的话）。'
- en: '`dataset_size` (`int`, *optional*) — The combined size in bytes of the Arrow
    tables for all splits.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_size` (`int`, *optional*) — 所有拆分的Arrow表的组合大小（以字节为单位）。'
- en: '`size_in_bytes` (`int`, *optional*) — The combined size in bytes of all files
    associated with the dataset (downloaded files + Arrow files).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size_in_bytes` (`int`, *optional*) — 与数据集相关的所有文件的组合大小（下载的文件+Arrow文件）。'
- en: '`task_templates` (`List[TaskTemplate]`, *optional*) — The task templates to
    prepare the dataset for during training and evaluation. Each template casts the
    dataset’s [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    to standardized column names and types as detailed in `datasets.tasks`.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_templates` (`List[TaskTemplate]`, *optional*) — 在训练和评估期间为数据集准备任务模板。每个模板将数据集的[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)转换为`datasets.tasks`中详细说明的标准化列名和类型。'
- en: '*`*config_kwargs` (additional keyword arguments) — Keyword arguments to be
    passed to the [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)
    and used in the [DatasetBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*config_kwargs`（额外的关键字参数） — 要传递给[BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)并在[DatasetBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder)中使用的关键字参数。'
- en: Information about a dataset.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有关数据集的信息。
- en: '`DatasetInfo` documents datasets, including its name, version, and features.
    See the constructor arguments and properties for a full list.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`DatasetInfo` 文档数据集，包括其名称、版本和特征。查看构造函数参数和属性以获取完整列表。'
- en: Not all fields are known on construction and may be updated later.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有字段在构建时都已知，可能稍后更新。
- en: '#### `from_directory`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_directory`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L304)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L304)'
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`dataset_info_dir` (`str`) — The directory containing the metadata file. This
    should be the root directory of a specific dataset version.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_info_dir` (`str`) — 包含元数据文件的目录。这应该是特定数据集版本的根目录。'
- en: '`fs` (`fsspec.spec.AbstractFileSystem`, *optional*) — Instance of the remote
    filesystem used to download the files from.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fs` (`fsspec.spec.AbstractFileSystem`, *optional*) — 用于从中下载文件的远程文件系统的实例。'
- en: Deprecated in 2.9.0
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.9.0中弃用
- en: '`fs` was deprecated in version 2.9.0 and will be removed in 3.0.0. Please use
    `storage_options` instead, e.g. `storage_options=fs.storage_options`.'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`fs` 在版本2.9.0中已弃用，并将在3.0.0中删除。请改用`storage_options`，例如`storage_options=fs.storage_options`。'
- en: '`storage_options` (`dict`, *optional*) — Key/value pairs to be passed on to
    the file-system backend, if any.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`storage_options` (`dict`, *optional*) — 要传递给文件系统后端的键/值对（如果有的话）。'
- en: Added in 2.9.0
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2.9.0中添加
- en: Create [DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)
    from the JSON file in `dataset_info_dir`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从`dataset_info_dir`中的JSON文件创建[DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)。
- en: This function updates all the dynamically generated fields (num_examples, hash,
    time of creation,…) of the [DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数更新[DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)的所有动态生成字段（num_examples、hash、创建时间等）。
- en: This will overwrite all previous metadata.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这将覆盖所有先前的元数据。
- en: 'Example:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `write_to_directory`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `write_to_directory`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L212)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L212)'
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`dataset_info_dir` (`str`) — Destination directory.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_info_dir` (`str`) — 目标目录。'
- en: '`pretty_print` (`bool`, defaults to `False`) — If `True`, the JSON will be
    pretty-printed with the indent level of 4.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretty_print` (`bool`, 默认为`False`) — 如果为`True`，JSON将以缩进级别4进行漂亮打印。'
- en: '`fs` (`fsspec.spec.AbstractFileSystem`, *optional*) — Instance of the remote
    filesystem used to download the files from.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fs` (`fsspec.spec.AbstractFileSystem`, *可选*) — 用于从远程文件系统下载文件的实例。'
- en: Deprecated in 2.9.0
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在2.9.0中弃用
- en: '`fs` was deprecated in version 2.9.0 and will be removed in 3.0.0. Please use
    `storage_options` instead, e.g. `storage_options=fs.storage_options`.'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`fs`在版本2.9.0中已弃用，并将在3.0.0中移除。请改用`storage_options`，例如`storage_options=fs.storage_options`。'
- en: '`storage_options` (`dict`, *optional*) — Key/value pairs to be passed on to
    the file-system backend, if any.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`storage_options` (`dict`, *可选*) — 要传递给文件系统后端的键/值对。'
- en: Added in 2.9.0
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 添加于2.9.0
- en: Write `DatasetInfo` and license (if present) as JSON files to `dataset_info_dir`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 将`DatasetInfo`和许可证（如果存在）写入`dataset_info_dir`作为JSON文件。
- en: 'Example:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Dataset
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: The base class [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    implements a Dataset backed by an Apache Arrow table.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 基类[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)实现了由Apache
    Arrow表支持的数据集。
- en: '### `class datasets.Dataset`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class datasets.Dataset`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L659)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L659)'
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: A Dataset backed by an Arrow table.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由Arrow表支持的数据集。
- en: '#### `add_column`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_column`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5602)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5602)'
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`name` (`str`) — Column name.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name` (`str`) — 列名。'
- en: '`column` (`list` or `np.array`) — Column data to be added.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`column` (`list` 或 `np.array`) — 要添加的列数据。'
- en: Add column to Dataset.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 向数据集添加列。
- en: Added in 1.7
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 添加于1.7
- en: 'Example:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#### `add_item`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_item`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5849)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5849)'
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`item` (`dict`) — Item data to be added.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`item` (`dict`) — 要添加的项目数据。'
- en: Add item to Dataset.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 向数据集添加项目。
- en: Added in 1.7
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 添加于1.7
- en: 'Example:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#### `from_file`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_file`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L737)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L737)'
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`filename` (`str`) — File name of the dataset.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filename` (`str`) — 数据集的文件名。'
- en: '`info` (`DatasetInfo`, *optional*) — Dataset information, like description,
    citation, etc.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info` (`DatasetInfo`, *可选*) — 数据集信息，如描述、引用等。'
- en: '`split` (`NamedSplit`, *optional*) — Name of the dataset split.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split` (`NamedSplit`, *可选*) — 数据集拆分的名称。'
- en: '`indices_filename` (`str`, *optional*) — File names of the indices.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`indices_filename` (`str`, *可选*) — 索引文件名。'
- en: '`in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`in_memory` (`bool`, 默认为`False`) — 是否将数据复制到内存中。'
- en: Instantiate a Dataset backed by an Arrow table at filename.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件名处实例化由Arrow表支持的数据集。
- en: '#### `from_buffer`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_buffer`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L777)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L777)'
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`buffer` (`pyarrow.Buffer`) — Arrow buffer.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`buffer` (`pyarrow.Buffer`) — Arrow缓冲区。'
- en: '`info` (`DatasetInfo`, *optional*) — Dataset information, like description,
    citation, etc.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info` (`DatasetInfo`, *可选*) — 数据集信息，如描述、引用等。'
- en: '`split` (`NamedSplit`, *optional*) — Name of the dataset split.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split` (`NamedSplit`, *可选*) — 数据集拆分的名称。'
- en: '`indices_buffer` (`pyarrow.Buffer`, *optional*) — Indices Arrow buffer.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`indices_buffer` (`pyarrow.Buffer`, *可选*) — 索引Arrow缓冲区。'
- en: Instantiate a Dataset backed by an Arrow buffer.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化由Arrow缓冲区支持的数据集。
- en: '#### `from_pandas`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_pandas`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L809)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L809)'
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`df` (`pandas.DataFrame`) — Dataframe that contains the dataset.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`df` (`pandas.DataFrame`) — 包含数据集的数据框。'
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *可选*) — 数据集特征。'
- en: '`info` (`DatasetInfo`, *optional*) — Dataset information, like description,
    citation, etc.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info` (`DatasetInfo`, *可选*) — 数据集信息，如描述、引用等。'
- en: '`split` (`NamedSplit`, *optional*) — Name of the dataset split.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split` (`NamedSplit`, *可选*) — 数据集拆分的名称。'
- en: '`preserve_index` (`bool`, *optional*) — Whether to store the index as an additional
    column in the resulting Dataset. The default of `None` will store the index as
    a column, except for `RangeIndex` which is stored as metadata only. Use `preserve_index=True`
    to force it to be stored as a column.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preserve_index` (`bool`, *可选*) — 是否将索引存储为结果数据集中的附加列。`None`的默认值将索引存储为列，除了`RangeIndex`，它仅存储为元数据。使用`preserve_index=True`强制将其存储为列。'
- en: Convert `pandas.DataFrame` to a `pyarrow.Table` to create a [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 将`pandas.DataFrame`转换为`pyarrow.Table`以创建[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)。
- en: The column types in the resulting Arrow Table are inferred from the dtypes of
    the `pandas.Series` in the DataFrame. In the case of non-object Series, the NumPy
    dtype is translated to its Arrow equivalent. In the case of `object`, we need
    to guess the datatype by looking at the Python objects in this Series.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that Series of the `object` dtype don’t carry enough information to
    always lead to a meaningful Arrow type. In the case that we cannot infer a type,
    e.g. because the DataFrame is of length 0 or the Series only contains `None/nan`
    objects, the type is set to `null`. This behavior can be avoided by constructing
    explicit features and passing it to this function.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#### `from_dict`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L871)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '`mapping` (`Mapping`) — Mapping of strings to Arrays or Python lists.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info` (`DatasetInfo`, *optional*) — Dataset information, like description,
    citation, etc.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`NamedSplit`, *optional*) — Name of the dataset split.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert `dict` to a `pyarrow.Table` to create a [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_generator`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1007)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '`generator` ( —`Callable`): A generator function that `yields` examples.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gen_kwargs(dict,` *optional*) — Keyword arguments to be passed to the `generator`
    callable. You can define a sharded dataset by passing the list of shards in `gen_kwargs`
    and setting `num_proc` greater than 1.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*, defaults to `None`) — Number of processes when
    downloading and generating the dataset locally. This is helpful if the dataset
    is made of multiple files. Multiprocessing is disabled by default. If `num_proc`
    is greater than one, then all list values in `gen_kwargs` must be the same length.
    These values will be split between calls to the generator. The number of shards
    will be the minimum of the shortest list in `gen_kwargs` and `num_proc`.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.7.0
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to :`GeneratorConfig`.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a Dataset from a generator.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#### `data`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1739)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The Apache Arrow table backing the dataset.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#### `cache_files`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1759)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The cache files containing the Apache Arrow table backing the dataset.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#### `num_columns`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1777)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Number of columns in the dataset.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '#### `num_rows`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1792)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Number of rows in the dataset (same as [Dataset.**len**()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.__len__)).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#### `column_names`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1809)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Names of the columns in the dataset.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '#### `shape`'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1824)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Shape of the dataset (number of columns, number of rows).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '#### `unique`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1841)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — Column name (list all the column names with [column_names](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.column_names)).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`column` (`str`) — 列名（使用 [column_names](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.column_names)
    列出所有列名）。'
- en: Returns
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`list`'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`list`'
- en: List of unique elements in the given column.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 给定列中唯一元素的列表。
- en: Return a list of the unique elements in a column.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 返回列中唯一元素的列表。
- en: This is implemented in the low-level backend and as such, very fast.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这在底层后端中实现，因此非常快。
- en: 'Example:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE31]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '#### `flatten`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `flatten`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1947)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1947)'
- en: '[PRE32]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`new_fingerprint` (`str`, *optional*) — The new fingerprint of the dataset
    after transform. If `None`, the new fingerprint is computed using a hash of the
    previous fingerprint, and the transform arguments.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_fingerprint` (`str`, *可选*) — 转换后数据集的新指纹。如果为 `None`，则使用先前指纹和转换参数的哈希值计算新指纹。'
- en: Returns
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
- en: A copy of the dataset with flattened columns.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 具有展平列的数据集的副本。
- en: Flatten the table. Each column with a struct type is flattened into one column
    per struct field. Other columns are left unchanged.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 展平表格。具有结构类型的每列都展平为每个结构字段的一列。其他列保持不变。
- en: 'Example:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE33]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '#### `cast`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `cast`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1992)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1992)'
- en: '[PRE34]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Parameters
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features))
    — New features to cast the dataset to. The name of the fields in the features
    must match the current column names. The type of the data must also be convertible
    from one type to the other. For non-trivial conversion, e.g. `str` <-> `ClassLabel`
    you should use [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    to update the Dataset.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features))
    — 将数据集转换为的新特征。特征中字段的名称必须与当前列名匹配。数据的类型也必须可以从一种类型转换为另一种类型。对于非平凡的转换，例如 `str` <->
    `ClassLabel`，您应该使用 [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    来更新数据集。'
- en: '`batch_size` (`int`, defaults to `1000`) — Number of examples per batch provided
    to cast. If `batch_size <= 0` or `batch_size == None` then provide the full dataset
    as a single batch to cast.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, 默认为 `1000`) — 提供给转换的每个批次的示例数。如果 `batch_size <= 0` 或 `batch_size
    == None`，则将整个数据集作为一个批次提供给转换。'
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_in_memory` (`bool`, 默认为 `False`) — 是否将数据复制到内存中。'
- en: '`load_from_cache_file` (`bool`, defaults to `True` if caching is enabled) —
    If a cache file storing the current computation from `function` can be identified,
    use it instead of recomputing.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_from_cache_file` (`bool`, 默认为 `True` 如果启用缓存) — 如果可以识别到存储当前计算结果的缓存文件，就使用它而不是重新计算。'
- en: '`cache_file_name` (`str`, *optional*, defaults to `None`) — Provide the name
    of a path for the cache file. It is used to store the results of the computation
    instead of the automatically generated cache file name.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache_file_name` (`str`, *可选*, 默认为 `None`) — 提供缓存文件的路径名称。用于存储计算结果而不是自动生成的缓存文件名称。'
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`writer_batch_size` (`int`, 默认为 `1000`) — 每个缓存文件写操作的行数。这个值在处理过程中内存使用和处理速度之间取得了很好的平衡。较高的值使处理过程中查找次数减少，较低的值在运行时消耗较少的临时内存
    [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)。'
- en: '`num_proc` (`int`, *optional*, defaults to `None`) — Number of processes for
    multiprocessing. By default it doesn’t use multiprocessing.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_proc` (`int`, *可选*, 默认为 `None`) — 用于多进程的进程数。默认情况下不使用多进程。'
- en: Returns
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
- en: A copy of the dataset with casted features.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 具有转换特征的数据集的副本。
- en: Cast the dataset to a new set of features.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集转换为一组新特征。
- en: 'Example:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE35]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '#### `cast_column`'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `cast_column`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2075)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2075)'
- en: '[PRE36]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Parameters
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`column` (`str`) — Column name.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`column` (`str`) — 列名。'
- en: '`feature` (`FeatureType`) — Target feature.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature` (`FeatureType`) — 目标特征。'
- en: '`new_fingerprint` (`str`, *optional*) — The new fingerprint of the dataset
    after transform. If `None`, the new fingerprint is computed using a hash of the
    previous fingerprint, and the transform arguments.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_fingerprint` (`str`, *可选*) — 转换后数据集的新指纹。如果为 `None`，则使用先前指纹和转换参数的哈希值计算新指纹。'
- en: Cast column to feature for decoding.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 将列转换为特征以进行解码。
- en: 'Example:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE37]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '#### `remove_columns`'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `remove_columns`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2117)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2117)'
- en: '[PRE38]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`column_names` (`Union[str, List[str]]`) — Name of the column(s) to remove.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`column_names` (`Union[str, List[str]]`) — 要移除的列的名称。'
- en: '`new_fingerprint` (`str`, *optional*) — The new fingerprint of the dataset
    after transform. If `None`, the new fingerprint is computed using a hash of the
    previous fingerprint, and the transform arguments.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_fingerprint` (`str`, *可选*) — 转换后数据集的新指纹。如果为 `None`，则使用先前指纹和转换参数的哈希值计算新指纹。'
- en: Returns
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
- en: A copy of the dataset object without the columns to remove.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 具有要移除的列的数据集对象的副本。
- en: Remove one or several column(s) in the dataset and the features associated to
    them.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 删除数据集中的一个或多个列以及与它们关联的特征。
- en: You can also remove a column using [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    with `remove_columns` but the present method is in-place (doesn’t copy the data
    to a new dataset) and is thus faster.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)和`remove_columns`来删除列，但是当前方法是原地操作（不会将数据复制到新数据集），因此更快。
- en: 'Example:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE39]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '#### `rename_column`'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `rename_column`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2173)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2173)'
- en: '[PRE40]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`original_column_name` (`str`) — Name of the column to rename.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`original_column_name` (`str`) — 要重命名的列的名称。'
- en: '`new_column_name` (`str`) — New name for the column.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_column_name` (`str`) — 列的新名称。'
- en: '`new_fingerprint` (`str`, *optional*) — The new fingerprint of the dataset
    after transform. If `None`, the new fingerprint is computed using a hash of the
    previous fingerprint, and the transform arguments.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_fingerprint` (`str`, *可选*) — 转换后数据集的新指纹。如果为`None`，则使用先前指纹和转换参数的哈希计算新指纹。'
- en: Returns
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
- en: A copy of the dataset with a renamed column.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 具有重命名列的数据集的副本。
- en: Rename a column in the dataset, and move the features associated to the original
    column under the new column name.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 重命名数据集中的列，并将与原始列关联的特征移动到新列名下。
- en: 'Example:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE41]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '#### `rename_columns`'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `rename_columns`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2240)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2240)'
- en: '[PRE42]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Parameters
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`column_mapping` (`Dict[str, str]`) — A mapping of columns to rename to their
    new names'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`column_mapping` (`Dict[str, str]`) — 要重命名为其新名称的列的映射'
- en: '`new_fingerprint` (`str`, *optional*) — The new fingerprint of the dataset
    after transform. If `None`, the new fingerprint is computed using a hash of the
    previous fingerprint, and the transform arguments.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_fingerprint` (`str`, *可选*) — 转换后数据集的新指纹。如果为`None`，则使用先前指纹和转换参数的哈希计算新指纹。'
- en: Returns
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
- en: A copy of the dataset with renamed columns
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 具有重命名列的数据集的副本
- en: Rename several columns in the dataset, and move the features associated to the
    original columns under the new column names.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 重命名数据集中的多个列，并将与原始列关联的特征移动到新列名下。
- en: 'Example:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE43]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '#### `select_columns`'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `select_columns`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2308)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2308)'
- en: '[PRE44]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Parameters
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`column_names` (`Union[str, List[str]]`) — Name of the column(s) to keep.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`column_names` (`Union[str, List[str]]`) — 要保留的列的名称。'
- en: '`new_fingerprint` (`str`, *optional*) — The new fingerprint of the dataset
    after transform. If `None`, the new fingerprint is computed using a hash of the
    previous fingerprint, and the transform arguments.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_fingerprint` (`str`, *可选*) — 转换后数据集的新指纹。如果为`None`，则使用先前指纹和转换参数的哈希计算新指纹。'
- en: Returns
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
- en: A copy of the dataset object which only consists of selected columns.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集对象的副本，仅包含选定的列。
- en: Select one or several column(s) in the dataset and the features associated to
    them.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 选择数据集中的一个或多个列以及与它们关联的特征。
- en: 'Example:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE45]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '#### `class_encode_column`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `class_encode_column`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1872)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1872)'
- en: '[PRE46]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Parameters
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`column` (`str`) — The name of the column to cast (list all the column names
    with [column_names](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.column_names))'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`column` (`str`) — 要转换的列的名称（使用[column_names](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.column_names)列出所有列名）'
- en: '`include_nulls` (`bool`, defaults to `False`) — Whether to include null values
    in the class labels. If `True`, the null values will be encoded as the `"None"`
    class label.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_nulls` (`bool`, 默认为 `False`) — 是否在类标签中包含空值。如果为`True`，则空值将被编码为`"None"`类标签。'
- en: Added in 1.14.2
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 添加于1.14.2
- en: Casts the given column as [ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)
    and updates the table.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 将给定的列转换为[ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)并更新表格。
- en: 'Example:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE47]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '#### `__len__`'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__len__`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2357)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2357)'
- en: '[PRE48]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Number of rows in the dataset.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的行数。
- en: 'Example:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE49]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '#### `__iter__`'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__iter__`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2374)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2374)'
- en: '[PRE50]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Iterate through the examples.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历示例。
- en: If a formatting is set with [Dataset.set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format)
    rows will be returned with the selected format.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用[Dataset.set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format)设置了格式，则将以所选格式返回行。
- en: '#### `iter`'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `iter`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2403)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2403)'
- en: '[PRE51]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Parameters
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`batch_size` (`int`) — size of each batch to yield.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`) — 每个批次的大小。'
- en: '`drop_last_batch` (`bool`, default *False*) — Whether a last batch smaller
    than the batch_size should be dropped'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_last_batch` (`bool`, 默认 *False*) — 是否应删除小于batch_size的最后一个批次'
- en: Iterate through the batches of size *batch_size*.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历大小为*batch_size*的批次。
- en: If a formatting is set with [*~datasets.Dataset.set_format*] rows will be returned
    with the selected format.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '#### `formatted_as`'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2447)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Parameters
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '`type` (`str`, *optional*) — Output type selected in `[None, ''numpy'', ''torch'',
    ''tensorflow'', ''pandas'', ''arrow'', ''jax'']`. `None` means `**getitem**“ returns
    python objects (default).'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — Columns to format in the output. `None`
    means `__getitem__` returns all columns (default).'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_all_columns` (`bool`, defaults to `False`) — Keep un-formatted columns
    as well in the output (as python objects).'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*format_kwargs` (additional keyword arguments) — Keywords arguments passed
    to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To be used in a `with` statement. Set `__getitem__` return format (type and
    columns).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_format`'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2479)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Parameters
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '`type` (`str`, *optional*) — Either output type selected in `[None, ''numpy'',
    ''torch'', ''tensorflow'', ''pandas'', ''arrow'', ''jax'']`. `None` means `__getitem__`
    returns python objects (default).'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — Columns to format in the output. `None`
    means `__getitem__` returns all columns (default).'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_all_columns` (`bool`, defaults to `False`) — Keep un-formatted columns
    as well in the output (as python objects).'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*format_kwargs` (additional keyword arguments) — Keywords arguments passed
    to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set `__getitem__` return format (type and columns). The data formatting is applied
    on-the-fly. The format `type` (for example “numpy”) is used to format batches
    when using `__getitem__`. It’s also possible to use custom transforms for formatting
    using [set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to call [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    after calling `set_format`. Since `map` may add new columns, then the list of
    formatted columns
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'gets updated. In this case, if you apply `map` on a dataset to add a new column,
    then this column will be formatted as:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Example:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '#### `set_transform`'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2587)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Parameters
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '`transform` (`Callable`, *optional*) — User-defined formatting transform, replaces
    the format defined by [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format).
    A formatting function is a callable that takes a batch (as a `dict`) as input
    and returns a batch. This function is applied right before returning the objects
    in `__getitem__`.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — Columns to format in the output. If specified,
    then the input batch of the transform only contains those columns.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_all_columns` (`bool`, defaults to `False`) — Keep un-formatted columns
    as well in the output (as python objects). If set to True, then the other un-formatted
    columns are kept with the output of the transform.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set `__getitem__` return format using this transform. The transform is applied
    on-the-fly on batches when `__getitem__` is called. As [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format),
    this can be reset using [reset_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.reset_format).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '#### `reset_format`'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2558)'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Reset `__getitem__` return format to python objects and all columns.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Same as `self.set_format()`
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '#### `with_format`'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2630)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Parameters
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '`type` (`str`, *optional*) — Either output type selected in `[None, ''numpy'',
    ''torch'', ''tensorflow'', ''pandas'', ''arrow'', ''jax'']`. `None` means `__getitem__`
    returns python objects (default).'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type` (`str`, *可选*) — 选择在 `[None, ''numpy'', ''torch'', ''tensorflow'', ''pandas'',
    ''arrow'', ''jax'']` 中的输出类型。`None` 表示 `__getitem__` 返回python对象（默认）。'
- en: '`columns` (`List[str]`, *optional*) — Columns to format in the output. `None`
    means `__getitem__` returns all columns (default).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`columns` (`List[str]`, *可选*) — 要在输出中格式化的列。`None` 表示 `__getitem__` 返回所有列（默认）。'
- en: '`output_all_columns` (`bool`, defaults to `False`) — Keep un-formatted columns
    as well in the output (as python objects).'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_all_columns` (`bool`, 默认为 `False`) — 在输出中保留未格式化的列（作为python对象）。'
- en: '*`*format_kwargs` (additional keyword arguments) — Keywords arguments passed
    to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*format_kwargs`（额外的关键字参数） — 传递给转换函数的关键字参数，如 `np.array`、`torch.tensor` 或 `tensorflow.ragged.constant`。'
- en: Set `__getitem__` return format (type and columns). The data formatting is applied
    on-the-fly. The format `type` (for example “numpy”) is used to format batches
    when using `__getitem__`.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 `__getitem__` 返回格式（类型和列）。数据格式化是实时应用的。当使用 `__getitem__` 时，格式 `type`（例如“numpy”）用于格式化批处理。
- en: It’s also possible to use custom transforms for formatting using [with_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.with_transform).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用自定义转换来进行格式化，使用 [with_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.with_transform)。
- en: Contrary to [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format),
    `with_format` returns a new [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    object.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 与 [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format)
    不同，`with_format` 返回一个新的 [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    对象。
- en: 'Example:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE61]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '#### `with_transform`'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `with_transform`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2681)'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2681)'
- en: '[PRE62]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Parameters
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`transform` (`Callable`, `optional`) — User-defined formatting transform, replaces
    the format defined by [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format).
    A formatting function is a callable that takes a batch (as a `dict`) as input
    and returns a batch. This function is applied right before returning the objects
    in `__getitem__`.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transform` (`Callable`, `可选`) — 用户定义的格式化转换，替换 [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format)
    定义的格式。格式化函数是一个可调用对象，接受批处理（作为 `dict`）作为输入并返回一个批处理。此函数在返回 `__getitem__` 中的对象之前应用。'
- en: '`columns` (`List[str]`, `optional`) — Columns to format in the output. If specified,
    then the input batch of the transform only contains those columns.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`columns` (`List[str]`, `可选`) — 要在输出中格式化的列。如果指定，则转换的输入批处理仅包含这些列。'
- en: '`output_all_columns` (`bool`, defaults to `False`) — Keep un-formatted columns
    as well in the output (as python objects). If set to `True`, then the other un-formatted
    columns are kept with the output of the transform.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_all_columns` (`bool`, 默认为 `False`) — 在输出中保留未格式化的列（作为python对象）。如果设置为
    `True`，则其他未格式化的列将与转换的输出一起保留。'
- en: Set `__getitem__` return format using this transform. The transform is applied
    on-the-fly on batches when `__getitem__` is called.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此转换设置 `__getitem__` 返回格式。当调用 `__getitem__` 时，转换会在批处理时实时应用。
- en: As [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format),
    this can be reset using [reset_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.reset_format).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 与 [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format)
    不同，可以使用 [reset_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.reset_format)
    进行重置。
- en: Contrary to [set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform),
    `with_transform` returns a new [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    object.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 与 [set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform)
    不同，`with_transform` 返回一个新的 [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    对象。
- en: 'Example:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE63]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '#### `__getitem__`'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__getitem__`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2808)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2808)'
- en: '[PRE64]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Can be used to index columns (by string names) or rows (by integer index or
    iterable of indices or bools).
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 可用于按字符串名称索引列或按整数索引或索引的可迭代对象或布尔值索引行。
- en: '#### `cleanup_cache_files`'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `cleanup_cache_files`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2818)'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2818)'
- en: '[PRE65]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Returns
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`int`'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Number of removed files.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 删除的文件数量。
- en: Clean up all cache files in the dataset cache directory, excepted the currently
    used cache file if there is one.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 清理数据集缓存目录中的所有缓存文件，除非当前正在使用的缓存文件。
- en: Be careful when running this command that no other process is currently using
    other cache files.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令时请注意，当前没有其他进程正在使用其他缓存文件。
- en: 'Example:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE66]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '#### `map`'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `map`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2865)'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2865)'
- en: '[PRE67]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Parameters
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`function` (`Callable`) — Function with one of the following signatures:'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function` (`Callable`) — 具有以下签名之一的函数：'
- en: '`function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and
    `with_indices=False` and `with_rank=False`'
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function(example: Dict[str, Any]) -> Dict[str, Any]` 如果 `batched=False` 且
    `with_indices=False` 且 `with_rank=False`'
- en: '`function(example: Dict[str, Any], *extra_args) -> Dict[str, Any]` if `batched=False`
    and `with_indices=True` and/or `with_rank=True` (one extra arg for each)'
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function(example: Dict[str, Any], *extra_args) -> Dict[str, Any]` 如果 `batched=False`
    且 `with_indices=True` 和/或 `with_rank=True`（每个额外参数一个）'
- en: '`function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and
    `with_indices=False` and `with_rank=False`'
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function(batch: Dict[str, List]) -> Dict[str, List]` 如果 `batched=True` 且 `with_indices=False`
    且 `with_rank=False`'
- en: '`function(batch: Dict[str, List], *extra_args) -> Dict[str, List]` if `batched=True`
    and `with_indices=True` and/or `with_rank=True` (one extra arg for each)'
  id: totrans-393
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function(batch: Dict[str, List], *extra_args) -> Dict[str, List]` 如果 `batched=True`
    并且 `with_indices=True` 和/或 `with_rank=True`（每个额外参数一个）'
- en: 'For advanced usage, the function can also return a `pyarrow.Table`. Moreover
    if your function returns nothing (`None`), then `map` will run your function and
    return the dataset unchanged. If no function is provided, default to identity
    function: `lambda x: x`.'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '对于高级用法，函数还可以返回 `pyarrow.Table`。此外，如果您的函数返回空（`None`），则 `map` 将运行您的函数并返回数据集。如果未提供函数，则默认为恒等函数：`lambda
    x: x`。'
- en: '`with_indices` (`bool`, defaults to `False`) — Provide example indices to `function`.
    Note that in this case the signature of `function` should be `def function(example,
    idx[, rank]): ...`.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`with_indices` (`bool`, 默认为 `False`) — 向 `function` 提供示例索引。请注意，在这种情况下，`function`
    的签名应为 `def function(example, idx[, rank]): ...`。'
- en: '`with_rank` (`bool`, defaults to `False`) — Provide process rank to `function`.
    Note that in this case the signature of `function` should be `def function(example[,
    idx], rank): ...`.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`with_rank` (`bool`, 默认为 `False`) — 向 `function` 提供进程排名。请注意，在这种情况下，`function`
    的签名应为 `def function(example[, idx], rank): ...`。'
- en: '`input_columns` (`Optional[Union[str, List[str]]]`, defaults to `None`) — The
    columns to be passed into `function` as positional arguments. If `None`, a `dict`
    mapping to all formatted columns is passed as one argument.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_columns` (`Optional[Union[str, List[str]]]`, 默认为 `None`) — 要作为位置参数传递给
    `function` 的列。如果为 `None`，则传递一个映射到所有格式化列的 `dict` 作为一个参数。'
- en: '`batched` (`bool`, defaults to `False`) — Provide batch of examples to `function`.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batched` (`bool`, 默认为 `False`) — 向 `function` 提供示例的批次。'
- en: '`batch_size` (`int`, *optional*, defaults to `1000`) — Number of examples per
    batch provided to `function` if `batched=True`. If `batch_size <= 0` or `batch_size
    == None`, provide the full dataset as a single batch to `function`.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *optional*, 默认为 `1000`) — 如果 `batched=True`，则提供给 `function`
    的每个批次的示例数。如果 `batch_size <= 0` 或 `batch_size == None`，则将整个数据集作为单个批次提供给 `function`。'
- en: '`drop_last_batch` (`bool`, defaults to `False`) — Whether a last batch smaller
    than the batch_size should be dropped instead of being processed by the function.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_last_batch` (`bool`, 默认为 `False`) — 是否应删除小于 `batch_size` 的最后一个批次，而不是由函数处理。'
- en: '`remove_columns` (`Optional[Union[str, List[str]]]`, defaults to `None`) —
    Remove a selection of columns while doing the mapping. Columns will be removed
    before updating the examples with the output of `function`, i.e. if `function`
    is adding columns with names in `remove_columns`, these columns will be kept.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`remove_columns` (`Optional[Union[str, List[str]]]`, 默认为 `None`) — 在执行映射时删除一些列。在使用
    `function` 的输出更新示例之前将删除列，即如果 `function` 添加具有 `remove_columns` 中名称的列，则这些列将被保留。'
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the dataset in memory
    instead of writing it to a cache file.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_in_memory` (`bool`, 默认为 `False`) — 将数据集保留在内存中，而不是将其写入缓存文件。'
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is
    enabled) — If a cache file storing the current computation from `function` can
    be identified, use it instead of recomputing.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_from_cache_file` (`Optional[bool]`, 默认为 `True` 如果启用了缓存) — 如果可以识别存储来自
    `function` 的当前计算的缓存文件，则使用它而不是重新计算。'
- en: '`cache_file_name` (`str`, *optional*, defaults to `None`) — Provide the name
    of a path for the cache file. It is used to store the results of the computation
    instead of the automatically generated cache file name.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache_file_name` (`str`, *optional*, 默认为 `None`) — 提供缓存文件的路径名称。用于存储计算结果，而不是自动生成的缓存文件名称。'
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`writer_batch_size` (`int`, 默认为 `1000`) — 缓存文件写入器的每次写入操作的行数。此值在处理期间的内存使用和处理速度之间取得良好的折衷。较高的值使处理执行更少的查找，较低的值在运行
    `map` 时消耗更少的临时内存。'
- en: '`features` (`Optional[datasets.Features]`, defaults to `None`) — Use a specific
    Features to store the cache file instead of the automatically generated one.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`features` (`Optional[datasets.Features]`, 默认为 `None`) — 使用特定的 Features 来存储缓存文件，而不是自动生成的文件。'
- en: '`disable_nullable` (`bool`, defaults to `False`) — Disallow null values in
    the table.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`disable_nullable` (`bool`, 默认为 `False`) — 在表中禁止空值。'
- en: '`fn_kwargs` (`Dict`, *optional*, defaults to `None`) — Keyword arguments to
    be passed to `function`.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fn_kwargs` (`Dict`, *optional*, 默认为 `None`) — 要传递给 `function` 的关键字参数。'
- en: '`num_proc` (`int`, *optional*, defaults to `None`) — Max number of processes
    when generating cache. Already cached shards are loaded sequentially.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_proc` (`int`, *optional*, 默认为 `None`) — 生成缓存时的最大进程数。已缓存的分片将按顺序加载。'
- en: '`suffix_template` (`str`) — If `cache_file_name` is specified, then this suffix
    will be added at the end of the base name of each. Defaults to `"_{rank:05d}_of_{num_proc:05d}"`.
    For example, if `cache_file_name` is “processed.arrow”, then for `rank=1` and
    `num_proc=4`, the resulting file would be `"processed_00001_of_00004.arrow"` for
    the default suffix.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`suffix_template` (`str`) — 如果指定了 `cache_file_name`，则此后缀将添加到每个基本名称的末尾。默认为 `"_{rank:05d}_of_{num_proc:05d}"`。例如，如果
    `cache_file_name` 是“processed.arrow”，那么对于 `rank=1` 和 `num_proc=4`，生成的文件将是 `"processed_00001_of_00004.arrow"`。'
- en: '`new_fingerprint` (`str`, *optional*, defaults to `None`) — The new fingerprint
    of the dataset after transform. If `None`, the new fingerprint is computed using
    a hash of the previous fingerprint, and the transform arguments.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_fingerprint` (`str`, *optional*, 默认为 `None`) — 转换后数据集的新指纹。如果为 `None`，则使用先前指纹的哈希和转换参数计算新指纹。'
- en: '`desc` (`str`, *optional*, defaults to `None`) — Meaningful description to
    be displayed alongside with the progress bar while mapping examples.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`desc` (`str`, *optional*, 默认为 `None`) — 映射示例时显示在进度条旁边的有意义的描述。'
- en: Apply a function to all the examples in the table (individually or in batches)
    and update the table. If your function returns a column that already exists, then
    it overwrites it.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify whether the function should be batched or not with the `batched`
    parameter:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'If batched is `False`, then the function takes 1 example in and should return
    1 example. An example is a dictionary, e.g. `{"text": "Hello there !"}`.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If batched is `True` and `batch_size` is 1, then the function takes a batch
    of 1 example as input and can return a batch with 1 or more examples. A batch
    is a dictionary, e.g. a batch of 1 example is `{"text": ["Hello there !"]}`.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If batched is `True` and `batch_size` is `n > 1`, then the function takes a
    batch of `n` examples as input and can return a batch with `n` examples, or with
    an arbitrary number of examples. Note that the last batch may have less than `n`
    examples. A batch is a dictionary, e.g. a batch of `n` examples is `{"text": ["Hello
    there !"] * n}`.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '#### `filter`'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L3540)'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Parameters
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '`function` (`Callable`) — Callable with one of the following signatures:'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any]) -> bool` if `batched=False` and `with_indices=False`
    and `with_rank=False`'
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any], *extra_args) -> bool` if `batched=False`
    and `with_indices=True` and/or `with_rank=True` (one extra arg for each)'
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List]) -> List[bool]` if `batched=True` and `with_indices=False`
    and `with_rank=False`'
  id: totrans-427
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List], *extra_args) -> List[bool]` if `batched=True`
    and `with_indices=True` and/or `with_rank=True` (one extra arg for each)'
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If no function is provided, defaults to an always `True` function: `lambda
    x: True`.'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`with_indices` (`bool`, defaults to `False`) — Provide example indices to `function`.
    Note that in this case the signature of `function` should be `def function(example,
    idx[, rank]): ...`.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`with_rank` (`bool`, defaults to `False`) — Provide process rank to `function`.
    Note that in this case the signature of `function` should be `def function(example[,
    idx], rank): ...`.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_columns` (`str` or `List[str]`, *optional*) — The columns to be passed
    into `function` as positional arguments. If `None`, a `dict` mapping to all formatted
    columns is passed as one argument.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batched` (`bool`, defaults to `False`) — Provide batch of examples to `function`.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to `1000`) — Number of examples per
    batch provided to `function` if `batched = True`. If `batched = False`, one example
    per batch is passed to `function`. If `batch_size <= 0` or `batch_size == None`,
    provide the full dataset as a single batch to `function`.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the dataset in memory
    instead of writing it to a cache file.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is
    enabled) — If a cache file storing the current computation from `function` can
    be identified, use it instead of recomputing.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_file_name` (`str`, *optional*) — Provide the name of a path for the
    cache file. It is used to store the results of the computation instead of the
    automatically generated cache file name.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fn_kwargs` (`dict`, *optional*) — Keyword arguments to be passed to `function`.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*) — Number of processes for multiprocessing. By
    default it doesn’t use multiprocessing.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`suffix_template` (`str`) — If `cache_file_name` is specified, then this suffix
    will be added at the end of the base name of each. For example, if `cache_file_name`
    is `"processed.arrow"`, then for `rank = 1` and `num_proc = 4`, the resulting
    file would be `"processed_00001_of_00004.arrow"` for the default suffix (default
    `_{rank:05d}_of_{num_proc:05d}`).'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`suffix_template` (`str`) — 如果指定了 `cache_file_name`，则此后缀将添加到每个基本名称的末尾。例如，如果
    `cache_file_name` 为 `"processed.arrow"`，那么对于 `rank = 1` 和 `num_proc = 4`，生成的文件将是
    `"processed_00001_of_00004.arrow"`，对于默认后缀（默认为 `_{rank:05d}_of_{num_proc:05d}`）。'
- en: '`new_fingerprint` (`str`, *optional*) — The new fingerprint of the dataset
    after transform. If `None`, the new fingerprint is computed using a hash of the
    previous fingerprint, and the transform arguments.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_fingerprint` (`str`, *optional*) — 转换后数据集的新指纹。如果为 `None`，则使用先前指纹的哈希和转换参数计算新指纹。'
- en: '`desc` (`str`, *optional*, defaults to `None`) — Meaningful description to
    be displayed alongside with the progress bar while filtering examples.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`desc` (`str`, *optional*, 默认为 `None`) — 在过滤示例时显示在进度条旁边的有意义的描述。'
- en: Apply a filter function to all the elements in the table in batches and update
    the table so that the dataset only includes examples according to the filter function.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 对表中的所有元素应用过滤函数，并更新表，使数据集仅包含根据过滤函数选择的示例。
- en: 'Example:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE70]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '#### `select`'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `select`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L3751)'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L3751)'
- en: '[PRE71]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Parameters
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`indices` (`range`, `list`, `iterable`, `ndarray` or `Series`) — Range, list
    or 1D-array of integer indices for indexing. If the indices correspond to a contiguous
    range, the Arrow table is simply sliced. However passing a list of indices that
    are not contiguous creates indices mapping, which is much less efficient, but
    still faster than recreating an Arrow table made of the requested rows.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`indices` (`range`, `list`, `iterable`, `ndarray` 或 `Series`) — 用于索引的整数索引的范围、列表或
    1D 数组。如果索引对应于连续范围，则 Arrow 表将简单地被切片。然而，传递一个不连续的索引列表会创建索引映射，这样做效率要低得多，但仍然比重新创建由请求的行组成的
    Arrow 表要快。'
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the indices mapping in
    memory instead of writing it to a cache file.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_in_memory` (`bool`, 默认为 `False`) — 将索引映射保留在内存中，而不是写入缓存文件。'
- en: '`indices_cache_file_name` (`str`, *optional*, defaults to `None`) — Provide
    the name of a path for the cache file. It is used to store the indices mapping
    instead of the automatically generated cache file name.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`indices_cache_file_name` (`str`, *optional*, 默认为 `None`) — 提供缓存文件的路径名称。它用于存储索引映射，而不是自动生成的缓存文件名称。'
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`writer_batch_size` (`int`, 默认为 `1000`) — 每次写入操作的行数，用于缓存文件写入器。这个值在处理过程中内存使用和处理速度之间取得了很好的平衡。较高的值使处理过程中查找次数减少，较低的值在运行
    `map` 时消耗更少的临时内存。'
- en: '`new_fingerprint` (`str`, *optional*, defaults to `None`) — The new fingerprint
    of the dataset after transform. If `None`, the new fingerprint is computed using
    a hash of the previous fingerprint, and the transform arguments.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_fingerprint` (`str`, *optional*, 默认为 `None`) — 转换后数据集的新指纹。如果为 `None`，则使用先前指纹的哈希和转换参数计算新指纹。'
- en: Create a new dataset with rows selected following the list/array of indices.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 使用选择的列表/数组中的索引创建新数据集。
- en: 'Example:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE72]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '#### `sort`'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `sort`'
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4001)'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4001)'
- en: '[PRE73]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Parameters
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`column_names` (`Union[str, Sequence[str]]`) — Column name(s) to sort by.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`column_names` (`Union[str, Sequence[str]]`) — 要排序的列名。'
- en: '`reverse` (`Union[bool, Sequence[bool]]`, defaults to `False`) — If `True`,
    sort by descending order rather than ascending. If a single bool is provided,
    the value is applied to the sorting of all column names. Otherwise a list of bools
    with the same length and order as column_names must be provided.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reverse` (`Union[bool, Sequence[bool]]`, 默认为 `False`) — 如果为 `True`，则按降序排序而不是升序。如果提供单个布尔值，则该值将应用于所有列名的排序。否则，必须提供与
    `column_names` 长度和顺序相同的布尔值列表。'
- en: '`kind` (`str`, *optional*) — Pandas algorithm for sorting selected in `{quicksort,
    mergesort, heapsort, stable}`, The default is `quicksort`. Note that both `stable`
    and `mergesort` use `timsort` under the covers and, in general, the actual implementation
    will vary with data type. The `mergesort` option is retained for backwards compatibility.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kind` (`str`, *optional*) — 选择排序的 Pandas 算法，可选值为 `{quicksort, mergesort, heapsort,
    stable}`，默认为 `quicksort`。请注意，`stable` 和 `mergesort` 都在内部使用 `timsort`，通常实际实现会随数据类型而变化。`mergesort`
    选项保留了向后兼容性。'
- en: Deprecated in 2.8.0
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 2.8.0 版中弃用
- en: '`kind` was deprecated in version 2.10.0 and will be removed in 3.0.0.'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`kind` 在 2.10.0 版中已弃用，并将在 3.0.0 版中移除。'
- en: '`null_placement` (`str`, defaults to `at_end`) — Put `None` values at the beginning
    if `at_start` or `first` or at the end if `at_end` or `last`'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`null_placement` (`str`, 默认为 `at_end`) — 如果为 `at_start`、`first`，则将 `None` 值放在开头；如果为
    `at_end`、`last`，则将 `None` 值放在末尾。'
- en: Added in 1.14.2
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 1.14.2 版中添加
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the sorted indices in
    memory instead of writing it to a cache file.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_in_memory` (`bool`, 默认为 `False`) — 将排序后的索引保留在内存中，而不是写入缓存文件。'
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is
    enabled) — If a cache file storing the sorted indices can be identified, use it
    instead of recomputing.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_from_cache_file` (`Optional[bool]`, 默认为 `True`，如果启用缓存，则使用存储排序后的索引的缓存文件，而不是重新计算。'
- en: '`indices_cache_file_name` (`str`, *optional*, defaults to `None`) — Provide
    the name of a path for the cache file. It is used to store the sorted indices
    instead of the automatically generated cache file name.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`indices_cache_file_name` (`str`, *optional*, 默认为 `None`) — 提供缓存文件的路径名称。它用于存储排序后的索引，而不是自动生成的缓存文件名称。'
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. Higher value gives smaller cache files, lower
    value consume less temporary memory.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_fingerprint` (`str`, *optional*, defaults to `None`) — The new fingerprint
    of the dataset after transform. If `None`, the new fingerprint is computed using
    a hash of the previous fingerprint, and the transform arguments'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new dataset sorted according to a single or multiple columns.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '#### `shuffle`'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4146)'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Parameters
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '`seed` (`int`, *optional*) — A seed to initialize the default BitGenerator
    if `generator=None`. If `None`, then fresh, unpredictable entropy will be pulled
    from the OS. If an `int` or `array_like[ints]` is passed, then it will be passed
    to SeedSequence to derive the initial BitGenerator state.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`numpy.random.Generator`, *optional*) — Numpy random Generator
    to use to compute the permutation of the dataset rows. If `generator=None` (default),
    uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, default `False`) — Keep the shuffled indices in memory
    instead of writing it to a cache file.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is
    enabled) — If a cache file storing the shuffled indices can be identified, use
    it instead of recomputing.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indices_cache_file_name` (`str`, *optional*) — Provide the name of a path
    for the cache file. It is used to store the shuffled indices instead of the automatically
    generated cache file name.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_fingerprint` (`str`, *optional*, defaults to `None`) — The new fingerprint
    of the dataset after transform. If `None`, the new fingerprint is computed using
    a hash of the previous fingerprint, and the transform arguments.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new Dataset where the rows are shuffled.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: Currently shuffling uses numpy random generators. You can either supply a NumPy
    BitGenerator to use, or a seed to initiate NumPy’s default random generator (PCG64).
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: Shuffling takes the list of indices `[0:len(my_dataset)]` and shuffles it to
    create an indices mapping. However as soon as your [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    has an indices mapping, the speed can become 10x slower. This is because there
    is an extra step to get the row index to read using the indices mapping, and most
    importantly, you aren’t reading contiguous chunks of data anymore. To restore
    the speed, you’d need to rewrite the entire dataset on your disk again using [Dataset.flatten_indices()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.flatten_indices),
    which removes the indices mapping.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: 'This may take a lot of time depending of the size of your dataset though:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: In this case, we recommend switching to an [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    and leveraging its fast approximate shuffling method [IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle).
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: 'It only shuffles the shards order and adds a shuffle buffer to your dataset,
    which keeps the speed of your dataset optimal:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Example:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '#### `train_test_split`'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4278)'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Parameters
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: '`test_size` (`numpy.random.Generator`, *optional*) — Size of the test split
    If `float`, should be between `0.0` and `1.0` and represent the proportion of
    the dataset to include in the test split. If `int`, represents the absolute number
    of test samples. If `None`, the value is set to the complement of the train size.
    If `train_size` is also `None`, it will be set to `0.25`.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_size` (`numpy.random.Generator`, *optional*) — Size of the train split
    If `float`, should be between `0.0` and `1.0` and represent the proportion of
    the dataset to include in the train split. If `int`, represents the absolute number
    of train samples. If `None`, the value is automatically set to the complement
    of the test size.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shuffle` (`bool`, *optional*, defaults to `True`) — Whether or not to shuffle
    the data before splitting.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stratify_by_column` (`str`, *optional*, defaults to `None`) — The column name
    of labels to be used to perform stratified split of data.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` (`int`, *optional*) — A seed to initialize the default BitGenerator
    if `generator=None`. If `None`, then fresh, unpredictable entropy will be pulled
    from the OS. If an `int` or `array_like[ints]` is passed, then it will be passed
    to SeedSequence to derive the initial BitGenerator state.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`numpy.random.Generator`, *optional*) — Numpy random Generator
    to use to compute the permutation of the dataset rows. If `generator=None` (default),
    uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the splits indices in
    memory instead of writing it to a cache file.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is
    enabled) — If a cache file storing the splits indices can be identified, use it
    instead of recomputing.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_cache_file_name` (`str`, *optional*) — Provide the name of a path for
    the cache file. It is used to store the train split indices instead of the automatically
    generated cache file name.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_cache_file_name` (`str`, *optional*) — Provide the name of a path for
    the cache file. It is used to store the test split indices instead of the automatically
    generated cache file name.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_new_fingerprint` (`str`, *optional*, defaults to `None`) — The new fingerprint
    of the train set after transform. If `None`, the new fingerprint is computed using
    a hash of the previous fingerprint, and the transform arguments'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_new_fingerprint` (`str`, *optional*, defaults to `None`) — The new fingerprint
    of the test set after transform. If `None`, the new fingerprint is computed using
    a hash of the previous fingerprint, and the transform arguments'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return a dictionary ([datasets.DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict))
    with two random train and test subsets (`train` and `test` `Dataset` splits).
    Splits are created from the dataset according to `test_size`, `train_size` and
    `shuffle`.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: This method is similar to scikit-learn `train_test_split`.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '#### `shard`'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4561)'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Parameters
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: '`num_shards` (`int`) — How many shards to split the dataset into.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index` (`int`) — Which shard to select and return. contiguous — (`bool`, defaults
    to `False`): Whether to select contiguous blocks of indices for shards.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the dataset in memory
    instead of writing it to a cache file.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indices_cache_file_name` (`str`, *optional*) — Provide the name of a path
    for the cache file. It is used to store the indices of each shard instead of the
    automatically generated cache file name.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return the `index`-nth shard from dataset split into `num_shards` pieces.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: This shards deterministically. `dset.shard(n, i)` will contain all elements
    of dset whose index mod `n = i`.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: '`dset.shard(n, i, contiguous=True)` will instead split dset into contiguous
    chunks, so it can be easily concatenated back together after processing. If `n
    % i == l`, then the first `l` shards will have length `(n // i) + 1`, and the
    remaining shards will have length `(n // i)`. `datasets.concatenate([dset.shard(n,
    i, contiguous=True) for i in range(n)])` will return a dataset with the same order
    as the original.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to shard before using any randomizing operator (such as `shuffle`).
    It is best if the shard operator is used early in the dataset pipeline.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-534
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '#### `to_tf_dataset`'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L327)'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Parameters
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*) — Size of batches to load from the dataset.
    Defaults to `None`, which implies that the dataset won’t be batched, but the returned
    dataset can be batched later with `tf_dataset.batch(batch_size)`.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]` or `str`, *optional*) — Dataset column(s) to load in
    the `tf.data.Dataset`. Column names that are created by the `collate_fn` and that
    do not exist in the original dataset can be used.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shuffle(bool,` defaults to `False`) — Shuffle the dataset order when loading.
    Recommended `True` for training, `False` for validation/evaluation.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_remainder(bool,` defaults to `False`) — Drop the last incomplete batch
    when loading. Ensures that all batches yielded by the dataset will have the same
    length on the batch dimension.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`collate_fn(Callable,` *optional*) — A function or callable object (such as
    a `DataCollator`) that will collate lists of samples into a batch.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`collate_fn_args` (`Dict`, *optional*) — An optional `dict` of keyword arguments
    to be passed to the `collate_fn`.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_cols` (`List[str]` or `str`, defaults to `None`) — Dataset column(s)
    to load as labels. Note that many models compute loss internally rather than letting
    Keras do it, in which case passing the labels here is optional, as long as they’re
    in the input `columns`.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefetch` (`bool`, defaults to `True`) — Whether to run the dataloader in
    a separate thread and maintain a small buffer of batches for training. Improves
    performance by allowing data to be loaded in the background while the model is
    training.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, defaults to `0`) — Number of workers to use for loading
    the dataset. Only supported on Python versions >= 3.8.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_test_batches` (`int`, defaults to `20`) — Number of batches to use to
    infer the output signature of the dataset. The higher this number, the more accurate
    the signature will be, but the longer it will take to create the dataset.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a `tf.data.Dataset` from the underlying Dataset. This `tf.data.Dataset`
    will load and collate batches from the Dataset, and is suitable for passing to
    methods like `model.fit()` or `model.predict()`. The dataset will yield `dicts`
    for both inputs and labels unless the `dict` would contain only a single key,
    in which case a raw `tf.Tensor` is yielded instead.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '#### `push_to_hub`'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5248)'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-554
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Parameters
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: '`repo_id` (`str`) — The ID of the repository to push to in the following format:
    `<user>/<dataset_name>` or `<org>/<dataset_name>`. Also accepts `<dataset_name>`,
    which will default to the namespace of the logged-in user.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config_name` (`str`, defaults to “default”) — The configuration name (or subset)
    of a dataset. Defaults to “default”.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`set_default` (`bool`, *optional*) — Whether to set this configuration as the
    default one. Otherwise, the default configuration is the one named “default”.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`str`, *optional*) — The name of the split that will be given to that
    dataset. Defaults to `self.split`.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_dir` (`str`, *optional*) — Directory name that will contain the uploaded
    data files. Defaults to the `config_name` if different from “default”, else “data”.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.17.0
  id: totrans-561
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`commit_message` (`str`, *optional*) — Message to commit while pushing. Will
    default to `"Upload dataset"`.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`commit_description` (`str`, *optional*) — Description of the commit that will
    be created. Additionally, description of the PR if a PR is created (`create_pr`
    is True).'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.16.0
  id: totrans-564
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`private` (`bool`, *optional*, defaults to `False`) — Whether the dataset repository
    should be set to private or not. Only affects repository creation: a repository
    that already exists will not be affected by that parameter.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str`, *optional*) — An optional authentication token for the Hugging
    Face Hub. If no token is passed, will default to the token saved locally when
    logging in with `huggingface-cli login`. Will raise an error if no token is passed
    and the user is not logged-in.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*) — Branch to push the uploaded files to. Defaults
    to the `"main"` branch.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.15.0
  id: totrans-568
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`branch` (`str`, *optional*) — The git branch on which to push the dataset.
    This defaults to the default branch as specified in your repository, which defaults
    to `"main"`.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.15.0
  id: totrans-570
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`branch` was deprecated in favor of `revision` in version 2.15.0 and will be
    removed in 3.0.0.'
  id: totrans-571
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) — Whether to create a
    PR with the uploaded files or directly commit.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.15.0
  id: totrans-573
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"500MB"`) — The
    maximum size of the dataset shards to be uploaded to the hub. If expressed as
    a string, needs to be digits followed by a unit (like `"5MB"`).'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_shards` (`int`, *optional*) — Number of shards to write. By default, the
    number of shards depends on `max_shard_size`.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  id: totrans-576
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`embed_external_files` (`bool`, defaults to `True`) — Whether to embed file
    bytes in the shards. In particular, this will do the following before the push
    for the fields of type:'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    and [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image):
    remove local path information and embed file content in the Parquet files.'
  id: totrans-578
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Pushes the dataset to the hub as a Parquet dataset. The dataset is pushed using
    HTTP requests and does not need to have neither git or git-lfs installed.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: The resulting Parquet files are self-contained by default. If your dataset contains
    [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    or [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    data, the Parquet files will store the bytes of your images or audio files. You
    can disable this by setting `embed_external_files` to `False`.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'If your dataset has multiple splits (e.g. train/validation/test):'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-584
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'If you want to add a new configuration (or subset) to a dataset (e.g. if the
    dataset has multiple tasks/versions/languages):'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-586
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '#### `save_to_disk`'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1383)'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-589
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Parameters
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset_path` (`str`) — Path (e.g. `dataset/train`) or remote URI (e.g. `s3://my-bucket/dataset/train`)
    of the dataset directory where the dataset will be saved to.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs` (`fsspec.spec.AbstractFileSystem`, *optional*) — Instance of the remote
    filesystem where the dataset will be saved to.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.8.0
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`fs` was deprecated in version 2.8.0 and will be removed in 3.0.0. Please use
    `storage_options` instead, e.g. `storage_options=fs.storage_options`'
  id: totrans-594
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"500MB"`) — The
    maximum size of the dataset shards to be uploaded to the hub. If expressed as
    a string, needs to be digits followed by a unit (like `"50MB"`).'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_shards` (`int`, *optional*) — Number of shards to write. By default the
    number of shards depends on `max_shard_size` and `num_proc`.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  id: totrans-597
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*) — Number of processes when downloading and generating
    the dataset locally. Multiprocessing is disabled by default.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  id: totrans-599
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`storage_options` (`dict`, *optional*) — Key/value pairs to be passed on to
    the file-system backend, if any.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Saves a dataset to a dataset directory, or in a filesystem using any implementation
    of `fsspec.spec.AbstractFileSystem`.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: 'For [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    and [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    data:'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: All the Image() and Audio() data are stored in the arrow files. If you want
    to store paths or urls, please use the Value(“string”) type.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-606
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '#### `load_from_disk`'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1599)'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-609
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Parameters
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset_path` (`str`) — Path (e.g. `"dataset/train"`) or remote URI (e.g.
    `"s3//my-bucket/dataset/train"`) of the dataset directory where the dataset will
    be loaded from.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs` (`fsspec.spec.AbstractFileSystem`, *optional*) — Instance of the remote
    filesystem where the dataset will be saved to.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.8.0
  id: totrans-613
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`fs` was deprecated in version 2.8.0 and will be removed in 3.0.0. Please use
    `storage_options` instead, e.g. `storage_options=fs.storage_options`'
  id: totrans-614
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `None`) — Whether to copy the dataset
    in-memory. If `None`, the dataset will not be copied in-memory unless explicitly
    enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to nonzero. See more details
    in the [improve performance](../cache#improve-performance) section.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storage_options` (`dict`, *optional*) — Key/value pairs to be passed on to
    the file-system backend, if any.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  id: totrans-617
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    or [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: If `dataset_path` is a path of a dataset directory, the dataset requested.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `dataset_path` is a path of a dataset dict directory, a `datasets.DatasetDict`
    with each split.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loads a dataset that was previously saved using `save_to_disk` from a dataset
    directory, or from a filesystem using any implementation of `fsspec.spec.AbstractFileSystem`.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-624
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '#### `flatten_indices`'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L3672)'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Parameters
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the dataset in memory
    instead of writing it to a cache file.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_file_name` (`str`, *optional*, default `None`) — Provide the name of
    a path for the cache file. It is used to store the results of the computation
    instead of the automatically generated cache file name.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` (`Optional[datasets.Features]`, defaults to `None`) — Use a specific
    [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    to store the cache file instead of the automatically generated one.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`disable_nullable` (`bool`, defaults to `False`) — Allow null values in the
    table.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, optional, default `None`) — Max number of processes when
    generating cache. Already cached shards are loaded sequentially'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_fingerprint` (`str`, *optional*, defaults to `None`) — The new fingerprint
    of the dataset after transform. If `None`, the new fingerprint is computed using
    a hash of the previous fingerprint, and the transform arguments'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create and cache a new Dataset by flattening the indices mapping.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
- en: '#### `to_csv`'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4727)'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-639
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Parameters
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf` (`PathLike` or `FileOrBuffer`) — Either a path to a file or a
    BinaryIO.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*) — Size of the batch to load in memory and
    write at once. Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*) — Number of processes for multiprocessing. By
    default it doesn’t use multiprocessing. `batch_size` in this case defaults to
    `datasets.config.DEFAULT_MAX_BATCH_SIZE` but feel free to make it 5x or 10x of
    the default value if you have sufficient compute power.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*to_csv_kwargs` (additional keyword arguments) — Parameters to pass to pandas’s
    [`pandas.DataFrame.to_csv`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html).'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changed in 2.10.0
  id: totrans-645
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, `index` defaults to `False` if not specified.
  id: totrans-646
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you would like to write the index, pass `index=True` and also set a name
    for the index column by passing `index_label`.
  id: totrans-647
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: '`int`'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: The number of characters or bytes written.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
- en: Exports the dataset to csv
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-653
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '#### `to_pandas`'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4887)'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-656
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Parameters
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: '`batched` (`bool`) — Set to `True` to return a generator that yields the dataset
    as batches of `batch_size` rows. Defaults to `False` (returns the whole datasets
    once).'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*) — The size (number of rows) of the batches
    if `batched` is `True`. Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns the dataset as a `pandas.DataFrame`. Can also return a generator for
    large datasets.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-662
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '#### `to_dict`'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4773)'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  id: totrans-665
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: Parameters
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
- en: '`batched` (`bool`) — Set to `True` to return a generator that yields the dataset
    as batches of `batch_size` rows. Defaults to `False` (returns the whole datasets
    once).'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.11.0
  id: totrans-668
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use `.iter(batch_size=batch_size)` followed by `.to_dict()` on the individual
    batches instead.
  id: totrans-669
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*) — The size (number of rows) of the batches
    if `batched` is `True`. Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns the dataset as a Python dict. Can also return a generator for large
    datasets.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-673
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '#### `to_json`'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4842)'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  id: totrans-676
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: Parameters
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf` (`PathLike` or `FileOrBuffer`) — Either a path to a file or a
    BinaryIO.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*) — Size of the batch to load in memory and
    write at once. Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*) — Number of processes for multiprocessing. By
    default it doesn’t use multiprocessing. `batch_size` in this case defaults to
    `datasets.config.DEFAULT_MAX_BATCH_SIZE` but feel free to make it 5x or 10x of
    the default value if you have sufficient compute power.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*to_json_kwargs` (additional keyword arguments) — Parameters to pass to pandas’s
    [`pandas.DataFrame.to_json`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html).'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changed in 2.11.0
  id: totrans-682
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, `index` defaults to `False` if `orient` is `"split"` or `"table"`.
  id: totrans-683
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you would like to write the index, pass `index=True`.
  id: totrans-684
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
- en: '`int`'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
- en: The number of characters or bytes written.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: Export the dataset to JSON Lines or JSON.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-690
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '#### `to_parquet`'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4926)'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  id: totrans-693
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: Parameters
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf` (`PathLike` or `FileOrBuffer`) — Either a path to a file or a
    BinaryIO.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*) — Size of the batch to load in memory and
    write at once. Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*parquet_writer_kwargs` (additional keyword arguments) — Parameters to pass
    to PyArrow’s `pyarrow.parquet.ParquetWriter`.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
- en: '`int`'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
- en: The number of characters or bytes written.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
- en: Exports the dataset to parquet
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  id: totrans-703
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '#### `to_sql`'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4957)'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  id: totrans-706
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: Parameters
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
- en: '`name` (`str`) — Name of SQL table.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`con` (`str` or `sqlite3.Connection` or `sqlalchemy.engine.Connection` or `sqlalchemy.engine.Connection`)
    — A [URI string](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls)
    or a SQLite3/SQLAlchemy connection object used to write to a database.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*) — Size of the batch to load in memory and
    write at once. Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*sql_writer_kwargs` (additional keyword arguments) — Parameters to pass to
    pandas’s [`pandas.DataFrame.to_sql`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html).'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changed in 2.11.0
  id: totrans-712
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, `index` defaults to `False` if not specified.
  id: totrans-713
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you would like to write the index, pass `index=True` and also set a name
    for the index column by passing `index_label`.
  id: totrans-714
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
- en: '`int`'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
- en: The number of records written.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
- en: Exports the dataset to a SQL database.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  id: totrans-720
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '#### `to_iterable_dataset`'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5049)'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  id: totrans-723
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: Parameters
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
- en: '`num_shards` (`int`, default to `1`) — Number of shards to define when instantiating
    the iterable dataset. This is especially useful for big datasets to be able to
    shuffle properly, and also to enable fast parallel loading using a PyTorch DataLoader
    or in distributed setups for example. Shards are defined using [datasets.Dataset.shard()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.shard):
    it simply slices the data without writing anything on disk.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get an [datasets.IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    from a map-style [datasets.Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset).
    This is equivalent to loading a dataset in streaming mode with [datasets.load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset),
    but much faster since the data is streamed from local files.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to map-style datasets, iterable datasets are lazy and can only be iterated
    over (e.g. using a for loop). Since they are read sequentially in training loops,
    iterable datasets are much faster than map-style datasets. All the transformations
    applied to iterable datasets like filtering or processing are done on-the-fly
    when you start iterating over the dataset.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
- en: Still, it is possible to shuffle an iterable dataset using [datasets.IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle).
    This is a fast approximate shuffling that works best if you have multiple shards
    and if you specify a buffer size that is big enough.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
- en: To get the best speed performance, make sure your dataset doesn’t have an indices
    mapping. If this is the case, the data are not read contiguously, which can be
    slow sometimes. You can use `ds = ds.flatten_indices()` to write your dataset
    in contiguous chunks of data and have optimal speed before switching to an iterable
    dataset.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic usage:'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  id: totrans-732
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'With lazy filtering and processing:'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  id: totrans-734
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'With sharding to enable efficient shuffling:'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  id: totrans-736
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'With a PyTorch DataLoader:'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  id: totrans-738
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'With a PyTorch DataLoader and shuffling:'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  id: totrans-740
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: In a distributed setup like PyTorch DDP with a PyTorch DataLoader and shuffling
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  id: totrans-742
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'With shuffling and multiple epochs:'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  id: totrans-744
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'Feel free to also use `IterableDataset.set_epoch()` when using a PyTorch DataLoader
    or in distributed setups. #### `add_faiss_index`'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5642)'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  id: totrans-747
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: Parameters
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — The column of the vectors to add to the index.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_name` (`str`, *optional*) — The `index_name`/identifier of the index.
    This is the `index_name` that is used to call [get_nearest_examples()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.get_nearest_examples)
    or [search()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.search).
    By default it corresponds to `column`.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`Union[int, List[int]]`, *optional*) — If positive integer, this
    is the index of the GPU to use. If negative integer, use all GPUs. If a list of
    positive integers is passed in, run only on those GPUs. By default it uses the
    CPU.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`string_factory` (`str`, *optional*) — This is passed to the index factory
    of Faiss to create the index. Default index class is `IndexFlat`.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metric_type` (`int`, *optional*) — Type of metric. Ex: `faiss.METRIC_INNER_PRODUCT`
    or `faiss.METRIC_L2`.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`custom_index` (`faiss.Index`, *optional*) — Custom Faiss index that you already
    have instantiated and configured for your needs.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`) — Size of the batch to use while adding vectors to the
    `FaissIndex`. Default value is `1000`.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.4.0
  id: totrans-756
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`train_size` (`int`, *optional*) — If the index needs a training step, specifies
    how many vectors will be used to train the index.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`faiss_verbose` (`bool`, defaults to `False`) — Enable the verbosity of the
    Faiss index.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`data-type`) — The dtype of the numpy arrays that are indexed. Default
    is `np.float32`.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add a dense index using Faiss for fast retrieval. By default the index is done
    over the vectors of the specified column. You can specify `device` if you want
    to run it on GPU (`device` must be the GPU index). You can find more information
    about Faiss here:'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
- en: For [string factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory)
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  id: totrans-763
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '#### `add_faiss_index_from_external_arrays`'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5722)'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  id: totrans-766
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: Parameters
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
- en: '`external_arrays` (`np.array`) — If you want to use arrays from outside the
    lib for the index, you can set `external_arrays`. It will use `external_arrays`
    to create the Faiss index instead of the arrays in the given `column`.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The `index_name`/identifier of the index. This is the
    `index_name` that is used to call [get_nearest_examples()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.get_nearest_examples)
    or [search()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.search).'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (Optional `Union[int, List[int]]`, *optional*) — If positive integer,
    this is the index of the GPU to use. If negative integer, use all GPUs. If a list
    of positive integers is passed in, run only on those GPUs. By default it uses
    the CPU.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`string_factory` (`str`, *optional*) — This is passed to the index factory
    of Faiss to create the index. Default index class is `IndexFlat`.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metric_type` (`int`, *optional*) — Type of metric. Ex: `faiss.faiss.METRIC_INNER_PRODUCT`
    or `faiss.METRIC_L2`.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`custom_index` (`faiss.Index`, *optional*) — Custom Faiss index that you already
    have instantiated and configured for your needs.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*) — Size of the batch to use while adding vectors
    to the FaissIndex. Default value is 1000.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.4.0
  id: totrans-775
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`train_size` (`int`, *optional*) — If the index needs a training step, specifies
    how many vectors will be used to train the index.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`faiss_verbose` (`bool`, defaults to False) — Enable the verbosity of the Faiss
    index.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`numpy.dtype`) — The dtype of the numpy arrays that are indexed. Default
    is np.float32.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add a dense index using Faiss for fast retrieval. The index is created using
    the vectors of `external_arrays`. You can specify `device` if you want to run
    it on GPU (`device` must be the GPU index). You can find more information about
    Faiss here:'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
- en: For [string factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory)
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `save_faiss_index`'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L529)'
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  id: totrans-783
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: Parameters
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The index_name/identifier of the index. This is the
    index_name that is used to call `.get_nearest` or `.search`.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`file` (`str`) — The path to the serialized faiss index on disk or remote URI
    (e.g. `"s3://my-bucket/index.faiss"`).'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storage_options` (`dict`, *optional*) — Key/value pairs to be passed on to
    the file-system backend, if any.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.11.0
  id: totrans-788
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Save a FaissIndex on disk.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
- en: '#### `load_faiss_index`'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L547)'
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  id: totrans-792
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: Parameters
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The index_name/identifier of the index. This is the
    index_name that is used to call `.get_nearest` or `.search`.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`file` (`str`) — The path to the serialized faiss index on disk or remote URI
    (e.g. `"s3://my-bucket/index.faiss"`).'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (Optional `Union[int, List[int]]`) — If positive integer, this is
    the index of the GPU to use. If negative integer, use all GPUs. If a list of positive
    integers is passed in, run only on those GPUs. By default it uses the CPU.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storage_options` (`dict`, *optional*) — Key/value pairs to be passed on to
    the file-system backend, if any.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.11.0
  id: totrans-798
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Load a FaissIndex from disk.
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
- en: If you want to do additional configurations, you can have access to the faiss
    index object by doing `.get_index(index_name).faiss_index` to make it fit your
    needs.
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
- en: '#### `add_elasticsearch_index`'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5781)'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  id: totrans-803
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: Parameters
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — The column of the documents to add to the index.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_name` (`str`, *optional*) — The `index_name`/identifier of the index.
    This is the index name that is used to call [get_nearest_examples()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.get_nearest_examples)
    or [Dataset.search()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.search).
    By default it corresponds to `column`.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host` (`str`, *optional*, defaults to `localhost`) — Host of where ElasticSearch
    is running.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`port` (`str`, *optional*, defaults to `9200`) — Port of where ElasticSearch
    is running.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`es_client` (`elasticsearch.Elasticsearch`, *optional*) — The elasticsearch
    client used to create the index if host and port are `None`.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`es_index_name` (`str`, *optional*) — The elasticsearch index name used to
    create the index.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`es_index_config` (`dict`, *optional*) — The configuration of the elasticsearch
    index. Default config is:'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a text index using ElasticSearch for fast retrieval. This is done in-place.
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  id: totrans-814
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '#### `load_elasticsearch_index`'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L631)'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  id: totrans-817
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: Parameters
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The `index_name`/identifier of the index. This is the
    index name that is used to call `get_nearest` or `search`.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`es_index_name` (`str`) — The name of elasticsearch index to load.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host` (`str`, *optional*, defaults to `localhost`) — Host of where ElasticSearch
    is running.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`port` (`str`, *optional*, defaults to `9200`) — Port of where ElasticSearch
    is running.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`es_client` (`elasticsearch.Elasticsearch`, *optional*) — The elasticsearch
    client used to create the index if host and port are `None`.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`es_index_config` (`dict`, *optional*) — The configuration of the elasticsearch
    index. Default config is:'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load an existing text index using ElasticSearch for fast retrieval.
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
- en: '#### `list_indexes`'
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L432)'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  id: totrans-828
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: List the `colindex_nameumns`/identifiers of all the attached indexes.
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_index`'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L436)'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  id: totrans-832
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: Parameters
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — Index name.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List the `index_name`/identifiers of all the attached indexes.
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
- en: '#### `drop_index`'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L678)'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  id: totrans-838
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: Parameters
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The `index_name`/identifier of the index.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drop the index with the specified column.
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
- en: '#### `search`'
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L687)'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  id: totrans-844
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: Parameters
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The name/identifier of the index.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query` (`Union[str, np.ndarray]`) — The query as a string if `index_name`
    is a text index or as a numpy array if `index_name` is a vector index.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`k` (`int`) — The number of examples to retrieve.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
- en: '`(scores, indices)`'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
- en: 'A tuple of `(scores, indices)` where:'
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
- en: '`scores` (`List[List[float]`): the retrieval scores from either FAISS (`IndexFlatL2`
    by default) or ElasticSearch of the retrieved examples'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indices` (`List[List[int]]`): the indices of the retrieved examples'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the nearest examples indices in the dataset to the query.
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
- en: '#### `search_batch`'
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L707)'
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  id: totrans-857
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: Parameters
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The `index_name`/identifier of the index.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`queries` (`Union[List[str], np.ndarray]`) — The queries as a list of strings
    if `index_name` is a text index or as a numpy array if `index_name` is a vector
    index.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`k` (`int`) — The number of examples to retrieve per query.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
- en: '`(total_scores, total_indices)`'
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
- en: 'A tuple of `(total_scores, total_indices)` where:'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
- en: '`total_scores` (`List[List[float]`): the retrieval scores from either FAISS
    (`IndexFlatL2` by default) or ElasticSearch of the retrieved examples per query'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total_indices` (`List[List[int]]`): the indices of the retrieved examples
    per query'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the nearest examples indices in the dataset to the query.
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_nearest_examples`'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L729)'
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  id: totrans-870
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: Parameters
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The index_name/identifier of the index.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query` (`Union[str, np.ndarray]`) — The query as a string if `index_name`
    is a text index or as a numpy array if `index_name` is a vector index.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`k` (`int`) — The number of examples to retrieve.'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
- en: '`(scores, examples)`'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
- en: 'A tuple of `(scores, examples)` where:'
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
- en: '`scores` (`List[float]`): the retrieval scores from either FAISS (`IndexFlatL2`
    by default) or ElasticSearch of the retrieved examples'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`examples` (`dict`): the retrieved examples'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the nearest examples in the dataset to the query.
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_nearest_examples_batch`'
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L753)'
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  id: totrans-883
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: Parameters
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The `index_name`/identifier of the index.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`queries` (`Union[List[str], np.ndarray]`) — The queries as a list of strings
    if `index_name` is a text index or as a numpy array if `index_name` is a vector
    index.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`k` (`int`) — The number of examples to retrieve per query.'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
- en: '`(total_scores, total_examples)`'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
- en: 'A tuple of `(total_scores, total_examples)` where:'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
- en: '`total_scores` (`List[List[float]`): the retrieval scores from either FAISS
    (`IndexFlatL2` by default) or ElasticSearch of the retrieved examples per query'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total_examples` (`List[dict]`): the retrieved examples per query'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the nearest examples in the dataset to the query.
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
- en: '#### `info`'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L159)'
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  id: totrans-896
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)
    object containing all the metadata in the dataset.'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
- en: '#### `split`'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L164)'
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  id: totrans-900
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[NamedSplit](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.NamedSplit)
    object corresponding to a named dataset split.'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
- en: '#### `builder_name`'
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L169)'
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  id: totrans-904
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '#### `citation`'
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L173)'
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  id: totrans-907
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '#### `config_name`'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L177)'
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  id: totrans-910
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '#### `dataset_size`'
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L181)'
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  id: totrans-913
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '#### `description`'
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L185)'
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  id: totrans-916
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '#### `download_checksums`'
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L189)'
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  id: totrans-919
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: '#### `download_size`'
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L193)'
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  id: totrans-922
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '#### `features`'
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L730)'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  id: totrans-925
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '#### `homepage`'
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L201)'
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  id: totrans-928
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '#### `license`'
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L205)'
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  id: totrans-931
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '#### `size_in_bytes`'
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L209)'
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  id: totrans-934
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '#### `supervised_keys`'
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L213)'
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  id: totrans-937
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '#### `version`'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L221)'
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  id: totrans-940
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '#### `from_csv`'
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L954)'
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  id: totrans-943
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: Parameters
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_paths` (`path-like` or list of `path-like`) — Path(s) of the CSV file(s).'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` ([NamedSplit](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.NamedSplit),
    *optional*) — Split name to be assigned to the dataset.'
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*, defaults to `None`) — Number of processes when
    downloading and generating the dataset locally. This is helpful if the dataset
    is made of multiple files. Multiprocessing is disabled by default.'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  id: totrans-951
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `pandas.read_csv`.'
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create Dataset from CSV file(s).
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  id: totrans-955
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '#### `from_json`'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1076)'
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  id: totrans-958
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: Parameters
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_paths` (`path-like` or list of `path-like`) — Path(s) of the JSON
    or JSON Lines file(s).'
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` ([NamedSplit](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.NamedSplit),
    *optional*) — Split name to be assigned to the dataset.'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`field` (`str`, *optional*) — Field name of the JSON file where the dataset
    is contained in.'
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional* defaults to `None`) — Number of processes when
    downloading and generating the dataset locally. This is helpful if the dataset
    is made of multiple files. Multiprocessing is disabled by default.'
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  id: totrans-967
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `JsonConfig`.'
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create Dataset from JSON or JSON Lines file(s).
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  id: totrans-971
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '#### `from_parquet`'
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1133)'
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  id: totrans-974
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: Parameters
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_paths` (`path-like` or list of `path-like`) — Path(s) of the Parquet
    file(s).'
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`NamedSplit`, *optional*) — Split name to be assigned to the dataset.'
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` (`Features`, *optional*) — Dataset features.'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — If not `None`, only these columns will
    be read from the file. A column name may be a prefix of a nested field, e.g. ‘a’
    will select ‘a.b’, ‘a.c’, and ‘a.d.e’.'
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*, defaults to `None`) — Number of processes when
    downloading and generating the dataset locally. This is helpful if the dataset
    is made of multiple files. Multiprocessing is disabled by default.'
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  id: totrans-983
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `ParquetConfig`.'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create Dataset from Parquet file(s).
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  id: totrans-987
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '#### `from_text`'
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1192)'
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  id: totrans-990
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: Parameters
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_paths` (`path-like` or list of `path-like`) — Path(s) of the text
    file(s).'
  id: totrans-992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`NamedSplit`, *optional*) — Split name to be assigned to the dataset.'
  id: totrans-993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` (`Features`, *optional*) — Dataset features.'
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  id: totrans-996
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*, defaults to `None`) — Number of processes when
    downloading and generating the dataset locally. This is helpful if the dataset
    is made of multiple files. Multiprocessing is disabled by default.'
  id: totrans-997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  id: totrans-998
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `TextConfig`.'
  id: totrans-999
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create Dataset from text file(s).
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  id: totrans-1002
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '#### `from_sql`'
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1307)'
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  id: totrans-1005
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: Parameters
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
- en: '`sql` (`str` or `sqlalchemy.sql.Selectable`) — SQL query to be executed or
    a table name.'
  id: totrans-1007
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`con` (`str` or `sqlite3.Connection` or `sqlalchemy.engine.Connection` or `sqlalchemy.engine.Connection`)
    — A [URI string](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls)
    used to instantiate a database connection or a SQLite3/SQLAlchemy connection object.'
  id: totrans-1008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `SqlConfig`.'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create Dataset from SQL query or database table.
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  id: totrans-1015
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: The returned dataset can only be cached if `con` is specified as URI string.
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
- en: '#### `prepare_for_task`'
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2729)'
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE154]'
  id: totrans-1019
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: Parameters
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
- en: '`task` (`Union[str, TaskTemplate]`) — The task to prepare the dataset for during
    training and evaluation. If `str`, supported tasks include:'
  id: totrans-1021
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"text-classification"`'
  id: totrans-1022
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"question-answering"`'
  id: totrans-1023
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If `TaskTemplate`, must be one of the task templates in [`datasets.tasks`](./task_templates).
  id: totrans-1024
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`id` (`int`, defaults to `0`) — The id required to unambiguously identify the
    task template when multiple task templates of the same type are supported.'
  id: totrans-1025
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare a dataset for the given task by casting the dataset’s [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    to standardized column names and types as detailed in [`datasets.tasks`](./task_templates).
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
- en: Casts `datasets.DatasetInfo.features` according to a task-specific schema. Intended
    for single-use only, so all task templates are removed from `datasets.DatasetInfo.task_templates`
    after casting.
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
- en: '#### `align_labels_with_mapping`'
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5903)'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  id: totrans-1030
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: Parameters
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
- en: '`label2id` (`dict`) — The label name to ID mapping to align the dataset with.'
  id: totrans-1032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_column` (`str`) — The column name of labels to align on.'
  id: totrans-1033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Align the dataset’s label ID and label name mapping to match an input `label2id`
    mapping. This is useful when you want to ensure that a model’s predicted labels
    are aligned with the dataset. The alignment in done using the lowercase label
    names.
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  id: totrans-1036
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '#### `datasets.concatenate_datasets`'
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/combine.py#L158)'
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  id: totrans-1039
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: Parameters
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
- en: '`dsets` (`List[datasets.Dataset]`) — List of Datasets to concatenate.'
  id: totrans-1041
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info` (`DatasetInfo`, *optional*) — Dataset information, like description,
    citation, etc.'
  id: totrans-1042
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`NamedSplit`, *optional*) — Name of the dataset split.'
  id: totrans-1043
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`axis` (`{0, 1}`, defaults to `0`) — Axis to concatenate over, where `0` means
    over rows (vertically) and `1` means over columns (horizontally).'
  id: totrans-1044
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 1.6.0
  id: totrans-1045
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Converts a list of [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    with the same schema into a single [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset).
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  id: totrans-1048
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '#### `datasets.interleave_datasets`'
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/combine.py#L18)'
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE159]'
  id: totrans-1051
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: Parameters
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
- en: '`datasets` (`List[Dataset]` or `List[IterableDataset]`) — List of datasets
    to interleave.'
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`probabilities` (`List[float]`, *optional*, defaults to `None`) — If specified,
    the new dataset is constructed by sampling examples from one source at a time
    according to these probabilities.'
  id: totrans-1054
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` (`int`, *optional*, defaults to `None`) — The random seed used to choose
    a source for each example.'
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info` ([DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo),
    *optional*) — Dataset information, like description, citation, etc.'
  id: totrans-1056
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.4.0
  id: totrans-1057
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`split` ([NamedSplit](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.NamedSplit),
    *optional*) — Name of the dataset split.'
  id: totrans-1058
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.4.0
  id: totrans-1059
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`stopping_strategy` (`str`, defaults to `first_exhausted`) — Two strategies
    are proposed right now, `first_exhausted` and `all_exhausted`. By default, `first_exhausted`
    is an undersampling strategy, i.e the dataset construction is stopped as soon
    as one dataset has ran out of samples. If the strategy is `all_exhausted`, we
    use an oversampling strategy, i.e the dataset construction is stopped as soon
    as every samples of every dataset has been added at least once. Note that if the
    strategy is `all_exhausted`, the interleaved dataset size can get enormous:'
  id: totrans-1060
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: with no probabilities, the resulting dataset will have `max_length_datasets*nb_dataset`
    samples.
  id: totrans-1061
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: with given probabilities, the resulting dataset will have more samples if some
    datasets have really low probability of visiting.
  id: totrans-1062
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    or [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)'
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
- en: Return type depends on the input `datasets` parameter. `Dataset` if the input
    is a list of `Dataset`, `IterableDataset` if the input is a list of `IterableDataset`.
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
- en: Interleave several datasets (sources) into a single dataset. The new dataset
    is constructed by alternating between the sources to get the examples.
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
- en: You can use this function on a list of [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    objects, or on a list of [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    objects.
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
- en: If `probabilities` is `None` (default) the new dataset is constructed by cycling
    between each source to get the examples.
  id: totrans-1068
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `probabilities` is not `None`, the new dataset is constructed by getting
    examples from a random source at a time according to the provided probabilities.
  id: totrans-1069
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The resulting dataset ends when one of the source datasets runs out of examples
    except when `oversampling` is `True`, in which case, the resulting dataset ends
    when all datasets have ran out of examples at least one time.
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
- en: 'Note for iterable datasets:'
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
- en: In a distributed setup or in PyTorch DataLoader workers, the stopping strategy
    is applied per process. Therefore the “first_exhausted” strategy on an sharded
    iterable dataset can generate less samples in total (up to 1 missing sample per
    subdataset per worker).
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
- en: 'For regular datasets (map-style):'
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  id: totrans-1075
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '#### `datasets.distributed.split_dataset_by_node`'
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/distributed.py#L10)'
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE161]'
  id: totrans-1078
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: Parameters
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset` ([Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    or [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset))
    — The dataset to split by node.'
  id: totrans-1080
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank` (`int`) — Rank of the current node.'
  id: totrans-1081
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`world_size` (`int`) — Total number of nodes.'
  id: totrans-1082
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    or [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)'
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
- en: The dataset to be used on the node at rank `rank`.
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
- en: Split a dataset for the node at rank `rank` in a pool of nodes of size `world_size`.
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
- en: 'For map-style datasets:'
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
- en: Each node is assigned a chunk of data, e.g. rank 0 is given the first chunk
    of the dataset. To maximize data loading throughput, chunks are made of contiguous
    data on disk if possible.
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
- en: 'For iterable datasets:'
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
- en: If the dataset has a number of shards that is a factor of `world_size` (i.e.
    if `dataset.n_shards % world_size == 0`), then the shards are evenly assigned
    across the nodes, which is the most optimized. Otherwise, each node keeps 1 example
    out of `world_size`, skipping the other examples.
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
- en: '#### `datasets.enable_caching`'
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/fingerprint.py#L95)'
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE162]'
  id: totrans-1093
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: When applying transforms on a dataset, the data are stored in cache files. The
    caching mechanism allows to reload an existing cache file if it’s already been
    computed.
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
- en: Reloading a dataset is possible since the cache files are named using the dataset
    fingerprint, which is updated after each transform.
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
- en: 'If disabled, the library will no longer reload cached datasets files when applying
    transforms to the datasets. More precisely, if the caching is disabled:'
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
- en: cache files are always recreated
  id: totrans-1097
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cache files are written to a temporary directory that is deleted when session
    closes
  id: totrans-1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cache files are named using a random hash instead of the dataset fingerprint
  id: totrans-1099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use [save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk)
    to save a transformed dataset or it will be deleted when session closes
  id: totrans-1100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: caching doesn’t affect [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset).
    If you want to regenerate a dataset from scratch you should use the `download_mode`
    parameter in [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset).
  id: totrans-1101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `datasets.disable_caching`'
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/fingerprint.py#L116)'
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE163]'
  id: totrans-1104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: When applying transforms on a dataset, the data are stored in cache files. The
    caching mechanism allows to reload an existing cache file if it’s already been
    computed.
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
- en: Reloading a dataset is possible since the cache files are named using the dataset
    fingerprint, which is updated after each transform.
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
- en: 'If disabled, the library will no longer reload cached datasets files when applying
    transforms to the datasets. More precisely, if the caching is disabled:'
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
- en: cache files are always recreated
  id: totrans-1108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cache files are written to a temporary directory that is deleted when session
    closes
  id: totrans-1109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cache files are named using a random hash instead of the dataset fingerprint
  id: totrans-1110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use [save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk)
    to save a transformed dataset or it will be deleted when session closes
  id: totrans-1111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: caching doesn’t affect [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset).
    If you want to regenerate a dataset from scratch you should use the `download_mode`
    parameter in [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset).
  id: totrans-1112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `datasets.is_caching_enabled`'
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/fingerprint.py#L161)'
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE164]'
  id: totrans-1115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: When applying transforms on a dataset, the data are stored in cache files. The
    caching mechanism allows to reload an existing cache file if it’s already been
    computed.
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
- en: Reloading a dataset is possible since the cache files are named using the dataset
    fingerprint, which is updated after each transform.
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
- en: 'If disabled, the library will no longer reload cached datasets files when applying
    transforms to the datasets. More precisely, if the caching is disabled:'
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
- en: cache files are always recreated
  id: totrans-1119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cache files are written to a temporary directory that is deleted when session
    closes
  id: totrans-1120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cache files are named using a random hash instead of the dataset fingerprint
  id: totrans-1121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use [save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk)]
    to save a transformed dataset or it will be deleted when session closes
  id: totrans-1122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: caching doesn’t affect [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset).
    If you want to regenerate a dataset from scratch you should use the `download_mode`
    parameter in [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset).
  id: totrans-1123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DatasetDict
  id: totrans-1124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dictionary with split names as keys (‘train’, ‘test’ for example), and `Dataset`
    objects as values. It also has dataset transform methods like map or filter, to
    process all the splits at once.
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
- en: '### `class datasets.DatasetDict`'
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L45)'
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE165]'
  id: totrans-1128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: 'A dictionary (dict of str: datasets.Dataset) with dataset transforms methods
    (map, filter, etc.)'
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
- en: '#### `data`'
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L86)'
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE166]'
  id: totrans-1132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: The Apache Arrow tables backing each split.
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE167]'
  id: totrans-1135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '#### `cache_files`'
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L101)'
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE168]'
  id: totrans-1138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: The cache files containing the Apache Arrow table backing each split.
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE169]'
  id: totrans-1141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: '#### `num_columns`'
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L119)'
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  id: totrans-1144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: Number of columns in each split of the dataset.
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE171]'
  id: totrans-1147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '#### `num_rows`'
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L135)'
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE172]'
  id: totrans-1150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: Number of rows in each split of the dataset (same as [datasets.Dataset.**len**()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.__len__)).
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  id: totrans-1153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '#### `column_names`'
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L151)'
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE174]'
  id: totrans-1156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: Names of the columns in each split of the dataset.
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE175]'
  id: totrans-1159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '#### `shape`'
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L169)'
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE176]'
  id: totrans-1162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: Shape of each split of the dataset (number of columns, number of rows).
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE177]'
  id: totrans-1165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: '#### `unique`'
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L217)'
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE178]'
  id: totrans-1168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: Parameters
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — column name (list all the column names with [column_names](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.column_names))'
  id: totrans-1170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
- en: Dict[`str`, `list`]
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary of unique elements in the given column.
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
- en: Return a list of the unique elements in a column for each split.
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
- en: This is implemented in the low-level backend and as such, very fast.
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE179]'
  id: totrans-1177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '#### `cleanup_cache_files`'
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L241)'
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE180]'
  id: totrans-1180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: Clean up all cache files in the dataset cache directory, excepted the currently
    used cache file if there is one. Be careful when running this command that no
    other process is currently using other cache files.
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE181]'
  id: totrans-1183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: '#### `map`'
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L764)'
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE182]'
  id: totrans-1186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: Parameters
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
- en: '`function` (`callable`) — with one of the following signature:'
  id: totrans-1188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and
    `with_indices=False`'
  id: totrans-1189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any], indices: int) -> Dict[str, Any]` if `batched=False`
    and `with_indices=True`'
  id: totrans-1190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and
    `with_indices=False`'
  id: totrans-1191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List]` if
    `batched=True` and `with_indices=True`'
  id: totrans-1192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For advanced usage, the function can also return a `pyarrow.Table`. Moreover
    if your function returns nothing (`None`), then `map` will run your function and
    return the dataset unchanged.
  id: totrans-1193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`with_indices` (`bool`, defaults to `False`) — Provide example indices to `function`.
    Note that in this case the signature of `function` should be `def function(example,
    idx): ...`.'
  id: totrans-1194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`with_rank` (`bool`, defaults to `False`) — Provide process rank to `function`.
    Note that in this case the signature of `function` should be `def function(example[,
    idx], rank): ...`.'
  id: totrans-1195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_columns` (`[Union[str, List[str]]]`, *optional*, defaults to `None`)
    — The columns to be passed into `function` as positional arguments. If `None`,
    a dict mapping to all formatted columns is passed as one argument.'
  id: totrans-1196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batched` (`bool`, defaults to `False`) — Provide batch of examples to `function`.'
  id: totrans-1197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to `1000`) — Number of examples per
    batch provided to `function` if `batched=True`, `batch_size <= 0` or `batch_size
    == None` then provide the full dataset as a single batch to `function`.'
  id: totrans-1198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_last_batch` (`bool`, defaults to `False`) — Whether a last batch smaller
    than the batch_size should be dropped instead of being processed by the function.'
  id: totrans-1199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`remove_columns` (`[Union[str, List[str]]]`, *optional*, defaults to `None`)
    — Remove a selection of columns while doing the mapping. Columns will be removed
    before updating the examples with the output of `function`, i.e. if `function`
    is adding columns with names in `remove_columns`, these columns will be kept.'
  id: totrans-1200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the dataset in memory
    instead of writing it to a cache file.'
  id: totrans-1201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is
    enabled) — If a cache file storing the current computation from `function` can
    be identified, use it instead of recomputing.'
  id: totrans-1202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_file_names` (`[Dict[str, str]]`, *optional*, defaults to `None`) — Provide
    the name of a path for the cache file. It is used to store the results of the
    computation instead of the automatically generated cache file name. You have to
    provide one `cache_file_name` per dataset in the dataset dictionary.'
  id: totrans-1203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, default `1000`) — Number of rows per write operation
    for the cache file writer. This value is a good trade-off between memory usage
    during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  id: totrans-1204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` (`[datasets.Features]`, *optional*, defaults to `None`) — Use a
    specific [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    to store the cache file instead of the automatically generated one.'
  id: totrans-1205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`disable_nullable` (`bool`, defaults to `False`) — Disallow null values in
    the table.'
  id: totrans-1206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fn_kwargs` (`Dict`, *optional*, defaults to `None`) — Keyword arguments to
    be passed to `function`'
  id: totrans-1207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*, defaults to `None`) — Number of processes for
    multiprocessing. By default it doesn’t use multiprocessing.'
  id: totrans-1208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`desc` (`str`, *optional*, defaults to `None`) — Meaningful description to
    be displayed alongside with the progress bar while mapping examples.'
  id: totrans-1209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a function to all the elements in the table (individually or in batches)
    and update the table (if function does updated examples). The transformation is
    applied to all the datasets of the dataset dictionary.
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE183]'
  id: totrans-1212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: '#### `filter`'
  id: totrans-1213
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L892)'
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE184]'
  id: totrans-1215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: Parameters
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
- en: '`function` (`Callable`) — Callable with one of the following signatures:'
  id: totrans-1217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any]) -> bool` if `batched=False` and `with_indices=False`
    and `with_rank=False`'
  id: totrans-1218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any], *extra_args) -> bool` if `batched=False`
    and `with_indices=True` and/or `with_rank=True` (one extra arg for each)'
  id: totrans-1219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List]) -> List[bool]` if `batched=True` and `with_indices=False`
    and `with_rank=False`'
  id: totrans-1220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List], *extra_args) -> List[bool]` if `batched=True`
    and `with_indices=True` and/or `with_rank=True` (one extra arg for each)'
  id: totrans-1221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If no function is provided, defaults to an always `True` function: `lambda
    x: True`.'
  id: totrans-1222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`with_indices` (`bool`, defaults to `False`) — Provide example indices to `function`.
    Note that in this case the signature of `function` should be `def function(example,
    idx[, rank]): ...`.'
  id: totrans-1223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`with_rank` (`bool`, defaults to `False`) — Provide process rank to `function`.
    Note that in this case the signature of `function` should be `def function(example[,
    idx], rank): ...`.'
  id: totrans-1224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_columns` (`[Union[str, List[str]]]`, *optional*, defaults to `None`)
    — The columns to be passed into `function` as positional arguments. If `None`,
    a dict mapping to all formatted columns is passed as one argument.'
  id: totrans-1225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batched` (`bool`, defaults to `False`) — Provide batch of examples to `function`.'
  id: totrans-1226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to `1000`) — Number of examples per
    batch provided to `function` if `batched=True` `batch_size <= 0` or `batch_size
    == None` then provide the full dataset as a single batch to `function`.'
  id: totrans-1227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the dataset in memory
    instead of writing it to a cache file.'
  id: totrans-1228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if chaching is
    enabled) — If a cache file storing the current computation from `function` can
    be identified, use it instead of recomputing.'
  id: totrans-1229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_file_names` (`[Dict[str, str]]`, *optional*, defaults to `None`) — Provide
    the name of a path for the cache file. It is used to store the results of the
    computation instead of the automatically generated cache file name. You have to
    provide one `cache_file_name` per dataset in the dataset dictionary.'
  id: totrans-1230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  id: totrans-1231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fn_kwargs` (`Dict`, *optional*, defaults to `None`) — Keyword arguments to
    be passed to `function`'
  id: totrans-1232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*, defaults to `None`) — Number of processes for
    multiprocessing. By default it doesn’t use multiprocessing.'
  id: totrans-1233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`desc` (`str`, *optional*, defaults to `None`) — Meaningful description to
    be displayed alongside with the progress bar while filtering examples.'
  id: totrans-1234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a filter function to all the elements in the table in batches and update
    the table so that the dataset only includes examples according to the filter function.
    The transformation is applied to all the datasets of the dataset dictionary.
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE185]'
  id: totrans-1237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: '#### `sort`'
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1054)'
  id: totrans-1239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE186]'
  id: totrans-1240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: Parameters
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, Sequence[str]]`) — Column name(s) to sort by.'
  id: totrans-1242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reverse` (`Union[bool, Sequence[bool]]`, defaults to `False`) — If `True`,
    sort by descending order rather than ascending. If a single bool is provided,
    the value is applied to the sorting of all column names. Otherwise a list of bools
    with the same length and order as column_names must be provided.'
  id: totrans-1243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kind` (`str`, *optional*) — Pandas algorithm for sorting selected in `{quicksort,
    mergesort, heapsort, stable}`, The default is `quicksort`. Note that both `stable`
    and `mergesort` use timsort under the covers and, in general, the actual implementation
    will vary with data type. The `mergesort` option is retained for backwards compatibility.'
  id: totrans-1244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.8.0
  id: totrans-1245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`kind` was deprecated in version 2.10.0 and will be removed in 3.0.0.'
  id: totrans-1246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`null_placement` (`str`, defaults to `at_end`) — Put `None` values at the beginning
    if `at_start` or `first` or at the end if `at_end` or `last`'
  id: totrans-1247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the sorted indices in
    memory instead of writing it to a cache file.'
  id: totrans-1248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is
    enabled) — If a cache file storing the sorted indices can be identified, use it
    instead of recomputing.'
  id: totrans-1249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indices_cache_file_names` (`[Dict[str, str]]`, *optional*, defaults to `None`)
    — Provide the name of a path for the cache file. It is used to store the indices
    mapping instead of the automatically generated cache file name. You have to provide
    one `cache_file_name` per dataset in the dataset dictionary.'
  id: totrans-1250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. Higher value gives smaller cache files, lower
    value consume less temporary memory.'
  id: totrans-1251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new dataset sorted according to a single or multiple columns.
  id: totrans-1252
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE187]'
  id: totrans-1254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: '#### `shuffle`'
  id: totrans-1255
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1132)'
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE188]'
  id: totrans-1257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: Parameters
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
- en: '`seeds` (`Dict[str, int]` or `int`, *optional*) — A seed to initialize the
    default BitGenerator if `generator=None`. If `None`, then fresh, unpredictable
    entropy will be pulled from the OS. If an `int` or `array_like[ints]` is passed,
    then it will be passed to SeedSequence to derive the initial BitGenerator state.
    You can provide one `seed` per dataset in the dataset dictionary.'
  id: totrans-1259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` (`int`, *optional*) — A seed to initialize the default BitGenerator
    if `generator=None`. Alias for seeds (a `ValueError` is raised if both are provided).'
  id: totrans-1260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generators` (`Dict[str, *optional*, np.random.Generator]`) — Numpy random
    Generator to use to compute the permutation of the dataset rows. If `generator=None`
    (default), uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).
    You have to provide one `generator` per dataset in the dataset dictionary.'
  id: totrans-1261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the dataset in memory
    instead of writing it to a cache file.'
  id: totrans-1262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is
    enabled) — If a cache file storing the current computation from `function` can
    be identified, use it instead of recomputing.'
  id: totrans-1263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indices_cache_file_names` (`Dict[str, str]`, *optional*) — Provide the name
    of a path for the cache file. It is used to store the indices mappings instead
    of the automatically generated cache file name. You have to provide one `cache_file_name`
    per dataset in the dataset dictionary.'
  id: totrans-1264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  id: totrans-1265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new Dataset where the rows are shuffled.
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
- en: The transformation is applied to all the datasets of the dataset dictionary.
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
- en: Currently shuffling uses numpy random generators. You can either supply a NumPy
    BitGenerator to use, or a seed to initiate NumPy’s default random generator (PCG64).
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE189]'
  id: totrans-1270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: '#### `set_format`'
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L556)'
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE190]'
  id: totrans-1273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE190]'
- en: Parameters
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
- en: '`type` (`str`, *optional*) — Output type selected in `[None, ''numpy'', ''torch'',
    ''tensorflow'', ''pandas'', ''arrow'', ''jax'']`. `None` means `__getitem__` returns
    python objects (default).'
  id: totrans-1275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — Columns to format in the output. `None`
    means `__getitem__` returns all columns (default).'
  id: totrans-1276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_all_columns` (`bool`, defaults to False) — Keep un-formatted columns
    as well in the output (as python objects),'
  id: totrans-1277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*format_kwargs` (additional keyword arguments) — Keywords arguments passed
    to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.'
  id: totrans-1278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set `__getitem__` return format (type and columns). The format is set for every
    dataset in the dataset dictionary.
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to call `map` after calling `set_format`. Since `map` may add
    new columns, then the list of formatted columns gets updated. In this case, if
    you apply `map` on a dataset to add a new column, then this column will be formatted:'
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
- en: '`new formatted columns = (all columns - previously unformatted columns)`'
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE191]'
  id: totrans-1283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: '#### `reset_format`'
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L602)'
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE192]'
  id: totrans-1286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE192]'
- en: Reset `__getitem__` return format to python objects and all columns. The transformation
    is applied to all the datasets of the dataset dictionary.
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
- en: Same as `self.set_format()`
  id: totrans-1288
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE193]'
  id: totrans-1290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '#### `formatted_as`'
  id: totrans-1291
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L519)'
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE194]'
  id: totrans-1293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE194]'
- en: Parameters
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
- en: '`type` (`str`, *optional*) — Output type selected in `[None, ''numpy'', ''torch'',
    ''tensorflow'', ''pandas'', ''arrow'', ''jax'']`. `None` means `__getitem__` returns
    python objects (default).'
  id: totrans-1295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — Columns to format in the output. `None`
    means `__getitem__` returns all columns (default).'
  id: totrans-1296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_all_columns` (`bool`, defaults to False) — Keep un-formatted columns
    as well in the output (as python objects).'
  id: totrans-1297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*format_kwargs` (additional keyword arguments) — Keywords arguments passed
    to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.'
  id: totrans-1298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To be used in a `with` statement. Set `__getitem__` return format (type and
    columns). The transformation is applied to all the datasets of the dataset dictionary.
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
- en: '#### `with_format`'
  id: totrans-1300
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L658)'
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE195]'
  id: totrans-1302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE195]'
- en: Parameters
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
- en: '`type` (`str`, *optional*) — Output type selected in `[None, ''numpy'', ''torch'',
    ''tensorflow'', ''pandas'', ''arrow'', ''jax'']`. `None` means `__getitem__` returns
    python objects (default).'
  id: totrans-1304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — Columns to format in the output. `None`
    means `__getitem__` returns all columns (default).'
  id: totrans-1305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_all_columns` (`bool`, defaults to `False`) — Keep un-formatted columns
    as well in the output (as python objects).'
  id: totrans-1306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*format_kwargs` (additional keyword arguments) — Keywords arguments passed
    to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.'
  id: totrans-1307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set `__getitem__` return format (type and columns). The data formatting is applied
    on-the-fly. The format `type` (for example “numpy”) is used to format batches
    when using `__getitem__`. The format is set for every dataset in the dataset dictionary.
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
- en: It’s also possible to use custom transforms for formatting using [with_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.with_transform).
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict.set_format),
    `with_format` returns a new [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    object with new [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    objects.
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE196]'
  id: totrans-1312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE196]'
- en: '#### `with_transform`'
  id: totrans-1313
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L710)'
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE197]'
  id: totrans-1315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE197]'
- en: Parameters
  id: totrans-1316
  prefs: []
  type: TYPE_NORMAL
- en: '`transform` (`Callable`, *optional*) — User-defined formatting transform, replaces
    the format defined by [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format).
    A formatting function is a callable that takes a batch (as a dict) as input and
    returns a batch. This function is applied right before returning the objects in
    `__getitem__`.'
  id: totrans-1317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — Columns to format in the output. If specified,
    then the input batch of the transform only contains those columns.'
  id: totrans-1318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_all_columns` (`bool`, defaults to False) — Keep un-formatted columns
    as well in the output (as python objects). If set to `True`, then the other un-formatted
    columns are kept with the output of the transform.'
  id: totrans-1319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set `__getitem__` return format using this transform. The transform is applied
    on-the-fly on batches when `__getitem__` is called. The transform is set for every
    dataset in the dataset dictionary
  id: totrans-1320
  prefs: []
  type: TYPE_NORMAL
- en: As [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format),
    this can be reset using [reset_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.reset_format).
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to `set_transform()`, `with_transform` returns a new [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    object with new [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    objects.
  id: totrans-1322
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE198]'
  id: totrans-1324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE198]'
- en: '#### `flatten`'
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L185)'
  id: totrans-1326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE199]'
  id: totrans-1327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE199]'
- en: Flatten the Apache Arrow Table of each split (nested features are flatten).
    Each column with a struct type is flattened into one column per struct field.
    Other columns are left unchanged.
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE200]'
  id: totrans-1330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE200]'
- en: '#### `cast`'
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L265)'
  id: totrans-1332
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE201]'
  id: totrans-1333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE201]'
- en: Parameters
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features))
    — New features to cast the dataset to. The name and order of the fields in the
    features must match the current column names. The type of the data must also be
    convertible from one type to the other. For non-trivial conversion, e.g. `string`
    <-> `ClassLabel` you should use [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    to update the Dataset.'
  id: totrans-1335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cast the dataset to a new set of features. The transformation is applied to
    all the datasets of the dataset dictionary.
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
- en: You can also remove a column using [Dataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    with `feature` but `cast` is in-place (doesn’t copy the data to a new dataset)
    and is thus faster.
  id: totrans-1337
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE202]'
  id: totrans-1339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '#### `cast_column`'
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L300)'
  id: totrans-1341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE203]'
  id: totrans-1342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE203]'
- en: Parameters
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — Column name.'
  id: totrans-1344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature` (`Feature`) — Target feature.'
  id: totrans-1345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cast column to feature for decoding.
  id: totrans-1346
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE204]'
  id: totrans-1348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE204]'
- en: '#### `remove_columns`'
  id: totrans-1349
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L329)'
  id: totrans-1350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE205]'
  id: totrans-1351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE205]'
- en: Parameters
  id: totrans-1352
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, List[str]]`) — Name of the column(s) to remove.'
  id: totrans-1353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove one or several column(s) from each split in the dataset and the features
    associated to the column(s).
  id: totrans-1354
  prefs: []
  type: TYPE_NORMAL
- en: The transformation is applied to all the splits of the dataset dictionary.
  id: totrans-1355
  prefs: []
  type: TYPE_NORMAL
- en: You can also remove a column using [Dataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    with `remove_columns` but the present method is in-place (doesn’t copy the data
    to a new dataset) and is thus faster.
  id: totrans-1356
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE206]'
  id: totrans-1358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE206]'
- en: '#### `rename_column`'
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L368)'
  id: totrans-1360
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE207]'
  id: totrans-1361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE207]'
- en: Parameters
  id: totrans-1362
  prefs: []
  type: TYPE_NORMAL
- en: '`original_column_name` (`str`) — Name of the column to rename.'
  id: totrans-1363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_column_name` (`str`) — New name for the column.'
  id: totrans-1364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rename a column in the dataset and move the features associated to the original
    column under the new column name. The transformation is applied to all the datasets
    of the dataset dictionary.
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also rename a column using [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    with `remove_columns` but the present method:'
  id: totrans-1366
  prefs: []
  type: TYPE_NORMAL
- en: takes care of moving the original features under the new column name.
  id: totrans-1367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: doesn’t copy the data to a new dataset and is thus much faster.
  id: totrans-1368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE208]'
  id: totrans-1370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE208]'
- en: '#### `rename_columns`'
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L413)'
  id: totrans-1372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE209]'
  id: totrans-1373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE209]'
- en: Parameters
  id: totrans-1374
  prefs: []
  type: TYPE_NORMAL
- en: '`column_mapping` (`Dict[str, str]`) — A mapping of columns to rename to their
    new names.'
  id: totrans-1375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
- en: '[DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)'
  id: totrans-1377
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset with renamed columns.
  id: totrans-1378
  prefs: []
  type: TYPE_NORMAL
- en: Rename several columns in the dataset, and move the features associated to the
    original columns under the new column names. The transformation is applied to
    all the datasets of the dataset dictionary.
  id: totrans-1379
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1380
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE210]'
  id: totrans-1381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE210]'
- en: '#### `select_columns`'
  id: totrans-1382
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L451)'
  id: totrans-1383
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE211]'
  id: totrans-1384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE211]'
- en: Parameters
  id: totrans-1385
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, List[str]]`) — Name of the column(s) to keep.'
  id: totrans-1386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select one or several column(s) from each split in the dataset and the features
    associated to the column(s).
  id: totrans-1387
  prefs: []
  type: TYPE_NORMAL
- en: The transformation is applied to all the splits of the dataset dictionary.
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1389
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE212]'
  id: totrans-1390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE212]'
- en: '#### `class_encode_column`'
  id: totrans-1391
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L487)'
  id: totrans-1392
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE213]'
  id: totrans-1393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE213]'
- en: Parameters
  id: totrans-1394
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — The name of the column to cast.'
  id: totrans-1395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`include_nulls` (`bool`, defaults to `False`) — Whether to include null values
    in the class labels. If `True`, the null values will be encoded as the `"None"`
    class label.'
  id: totrans-1396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 1.14.2
  id: totrans-1397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Casts the given column as [ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)
    and updates the tables.
  id: totrans-1398
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1399
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE214]'
  id: totrans-1400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE214]'
- en: '#### `push_to_hub`'
  id: totrans-1401
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1564)'
  id: totrans-1402
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE215]'
  id: totrans-1403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE215]'
- en: Parameters
  id: totrans-1404
  prefs: []
  type: TYPE_NORMAL
- en: '`repo_id` (`str`) — The ID of the repository to push to in the following format:
    `<user>/<dataset_name>` or `<org>/<dataset_name>`. Also accepts `<dataset_name>`,
    which will default to the namespace of the logged-in user.'
  id: totrans-1405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config_name` (`str`) — Configuration name of a dataset. Defaults to “default”.'
  id: totrans-1406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`set_default` (`bool`, *optional*) — Whether to set this configuration as the
    default one. Otherwise, the default configuration is the one named “default”.'
  id: totrans-1407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_dir` (`str`, *optional*) — Directory name that will contain the uploaded
    data files. Defaults to the `config_name` if different from “default”, else “data”.'
  id: totrans-1408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.17.0
  id: totrans-1409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`commit_message` (`str`, *optional*) — Message to commit while pushing. Will
    default to `"Upload dataset"`.'
  id: totrans-1410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`commit_description` (`str`, *optional*) — Description of the commit that will
    be created. Additionally, description of the PR if a PR is created (`create_pr`
    is True).'
  id: totrans-1411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.16.0
  id: totrans-1412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`private` (`bool`, *optional*) — Whether the dataset repository should be set
    to private or not. Only affects repository creation: a repository that already
    exists will not be affected by that parameter.'
  id: totrans-1413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str`, *optional*) — An optional authentication token for the Hugging
    Face Hub. If no token is passed, will default to the token saved locally when
    logging in with `huggingface-cli login`. Will raise an error if no token is passed
    and the user is not logged-in.'
  id: totrans-1414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*) — Branch to push the uploaded files to. Defaults
    to the `"main"` branch.'
  id: totrans-1415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.15.0
  id: totrans-1416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`branch` (`str`, *optional*) — The git branch on which to push the dataset.
    This defaults to the default branch as specified in your repository, which defaults
    to `"main"`.'
  id: totrans-1417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.15.0
  id: totrans-1418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`branch` was deprecated in favor of `revision` in version 2.15.0 and will be
    removed in 3.0.0.'
  id: totrans-1419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) — Whether to create a
    PR with the uploaded files or directly commit.'
  id: totrans-1420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.15.0
  id: totrans-1421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"500MB"`) — The
    maximum size of the dataset shards to be uploaded to the hub. If expressed as
    a string, needs to be digits followed by a unit (like `"500MB"` or `"1GB"`).'
  id: totrans-1422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_shards` (`Dict[str, int]`, *optional*) — Number of shards to write. By
    default, the number of shards depends on `max_shard_size`. Use a dictionary to
    define a different num_shards for each split.'
  id: totrans-1423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  id: totrans-1424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`embed_external_files` (`bool`, defaults to `True`) — Whether to embed file
    bytes in the shards. In particular, this will do the following before the push
    for the fields of type:'
  id: totrans-1425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    and [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    removes local path information and embed file content in the Parquet files.'
  id: totrans-1426
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Pushes the [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    to the hub as a Parquet dataset. The [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    is pushed using HTTP requests and does not need to have neither git or git-lfs
    installed.
  id: totrans-1427
  prefs: []
  type: TYPE_NORMAL
- en: Each dataset split will be pushed independently. The pushed dataset will keep
    the original split names.
  id: totrans-1428
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting Parquet files are self-contained by default: if your dataset
    contains [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    or [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    data, the Parquet files will store the bytes of your images or audio files. You
    can disable this by setting `embed_external_files` to False.'
  id: totrans-1429
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1430
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE216]'
  id: totrans-1431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE216]'
- en: 'If you want to add a new configuration (or subset) to a dataset (e.g. if the
    dataset has multiple tasks/versions/languages):'
  id: totrans-1432
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE217]'
  id: totrans-1433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE217]'
- en: '#### `save_to_disk`'
  id: totrans-1434
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1215)'
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE218]'
  id: totrans-1436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE218]'
- en: Parameters
  id: totrans-1437
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset_dict_path` (`str`) — Path (e.g. `dataset/train`) or remote URI (e.g.
    `s3://my-bucket/dataset/train`) of the dataset dict directory where the dataset
    dict will be saved to.'
  id: totrans-1438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs` (`fsspec.spec.AbstractFileSystem`, *optional*) — Instance of the remote
    filesystem where the dataset will be saved to.'
  id: totrans-1439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.8.0
  id: totrans-1440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`fs` was deprecated in version 2.8.0 and will be removed in 3.0.0. Please use
    `storage_options` instead, e.g. `storage_options=fs.storage_options`'
  id: totrans-1441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"500MB"`) — The
    maximum size of the dataset shards to be uploaded to the hub. If expressed as
    a string, needs to be digits followed by a unit (like `"50MB"`).'
  id: totrans-1442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_shards` (`Dict[str, int]`, *optional*) — Number of shards to write. By
    default the number of shards depends on `max_shard_size` and `num_proc`. You need
    to provide the number of shards for each dataset in the dataset dictionary. Use
    a dictionary to define a different num_shards for each split.'
  id: totrans-1443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  id: totrans-1444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*, default `None`) — Number of processes when downloading
    and generating the dataset locally. Multiprocessing is disabled by default.'
  id: totrans-1445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  id: totrans-1446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`storage_options` (`dict`, *optional*) — Key/value pairs to be passed on to
    the file-system backend, if any.'
  id: totrans-1447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  id: totrans-1448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Saves a dataset dict to a filesystem using `fsspec.spec.AbstractFileSystem`.
  id: totrans-1449
  prefs: []
  type: TYPE_NORMAL
- en: 'For [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    and [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    data:'
  id: totrans-1450
  prefs: []
  type: TYPE_NORMAL
- en: All the Image() and Audio() data are stored in the arrow files. If you want
    to store paths or urls, please use the Value(“string”) type.
  id: totrans-1451
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1452
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE219]'
  id: totrans-1453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE219]'
- en: '#### `load_from_disk`'
  id: totrans-1454
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1305)'
  id: totrans-1455
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE220]'
  id: totrans-1456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE220]'
- en: Parameters
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset_dict_path` (`str`) — Path (e.g. `"dataset/train"`) or remote URI (e.g.
    `"s3//my-bucket/dataset/train"`) of the dataset dict directory where the dataset
    dict will be loaded from.'
  id: totrans-1458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs` (`fsspec.spec.AbstractFileSystem`, *optional*) — Instance of the remote
    filesystem where the dataset will be saved to.'
  id: totrans-1459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.8.0
  id: totrans-1460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`fs` was deprecated in version 2.8.0 and will be removed in 3.0.0. Please use
    `storage_options` instead, e.g. `storage_options=fs.storage_options`'
  id: totrans-1461
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `None`) — Whether to copy the dataset
    in-memory. If `None`, the dataset will not be copied in-memory unless explicitly
    enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to nonzero. See more details
    in the [improve performance](../cache#improve-performance) section.'
  id: totrans-1462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storage_options` (`dict`, *optional*) — Key/value pairs to be passed on to
    the file-system backend, if any.'
  id: totrans-1463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  id: totrans-1464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Load a dataset that was previously saved using `save_to_disk` from a filesystem
    using `fsspec.spec.AbstractFileSystem`.
  id: totrans-1465
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1466
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE221]'
  id: totrans-1467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE221]'
- en: '#### `from_csv`'
  id: totrans-1468
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1382)'
  id: totrans-1469
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE222]'
  id: totrans-1470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE222]'
- en: Parameters
  id: totrans-1471
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_paths` (`dict` of path-like) — Path(s) of the CSV file(s).'
  id: totrans-1472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  id: totrans-1473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (str, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  id: totrans-1474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  id: totrans-1475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `pandas.read_csv`.'
  id: totrans-1476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    from CSV file(s).
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1478
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE223]'
  id: totrans-1479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE223]'
- en: '#### `from_json`'
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1421)'
  id: totrans-1481
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE224]'
  id: totrans-1482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE224]'
- en: Parameters
  id: totrans-1483
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_paths` (`path-like` or list of `path-like`) — Path(s) of the JSON
    Lines file(s).'
  id: totrans-1484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  id: totrans-1485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (str, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  id: totrans-1486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  id: totrans-1487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `JsonConfig`.'
  id: totrans-1488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    from JSON Lines file(s).
  id: totrans-1489
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1490
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE225]'
  id: totrans-1491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE225]'
- en: '#### `from_parquet`'
  id: totrans-1492
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1460)'
  id: totrans-1493
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE226]'
  id: totrans-1494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE226]'
- en: Parameters
  id: totrans-1495
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_paths` (`dict` of path-like) — Path(s) of the CSV file(s).'
  id: totrans-1496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  id: totrans-1497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  id: totrans-1498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  id: totrans-1499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — If not `None`, only these columns will
    be read from the file. A column name may be a prefix of a nested field, e.g. ‘a’
    will select ‘a.b’, ‘a.c’, and ‘a.d.e’.'
  id: totrans-1500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `ParquetConfig`.'
  id: totrans-1501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    from Parquet file(s).
  id: totrans-1502
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1503
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE227]'
  id: totrans-1504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE227]'
- en: '#### `from_text`'
  id: totrans-1505
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1509)'
  id: totrans-1506
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE228]'
  id: totrans-1507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE228]'
- en: Parameters
  id: totrans-1508
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_paths` (`dict` of path-like) — Path(s) of the text file(s).'
  id: totrans-1509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  id: totrans-1510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  id: totrans-1511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  id: totrans-1512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `TextConfig`.'
  id: totrans-1513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    from text file(s).
  id: totrans-1514
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1515
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE229]'
  id: totrans-1516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE229]'
- en: '#### `prepare_for_task`'
  id: totrans-1517
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1548)'
  id: totrans-1518
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE230]'
  id: totrans-1519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE230]'
- en: Parameters
  id: totrans-1520
  prefs: []
  type: TYPE_NORMAL
- en: '`task` (`Union[str, TaskTemplate]`) — The task to prepare the dataset for during
    training and evaluation. If `str`, supported tasks include:'
  id: totrans-1521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"text-classification"`'
  id: totrans-1522
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"question-answering"`'
  id: totrans-1523
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If `TaskTemplate`, must be one of the task templates in [`datasets.tasks`](./task_templates).
  id: totrans-1524
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`id` (`int`, defaults to `0`) — The id required to unambiguously identify the
    task template when multiple task templates of the same type are supported.'
  id: totrans-1525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare a dataset for the given task by casting the dataset’s [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    to standardized column names and types as detailed in [`datasets.tasks`](./task_templates).
  id: totrans-1526
  prefs: []
  type: TYPE_NORMAL
- en: Casts `datasets.DatasetInfo.features` according to a task-specific schema. Intended
    for single-use only, so all task templates are removed from `datasets.DatasetInfo.task_templates`
    after casting.
  id: totrans-1527
  prefs: []
  type: TYPE_NORMAL
- en: IterableDataset
  id: totrans-1528
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The base class [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    implements an iterable Dataset backed by python generators.
  id: totrans-1529
  prefs: []
  type: TYPE_NORMAL
- en: '### `class datasets.IterableDataset`'
  id: totrans-1530
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1191)'
  id: totrans-1531
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE231]'
  id: totrans-1532
  prefs: []
  type: TYPE_PRE
  zh: '[PRE231]'
- en: A Dataset backed by an iterable.
  id: totrans-1533
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_generator`'
  id: totrans-1534
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1439)'
  id: totrans-1535
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE232]'
  id: totrans-1536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE232]'
- en: Parameters
  id: totrans-1537
  prefs: []
  type: TYPE_NORMAL
- en: '`generator` (`Callable`) — A generator function that `yields` examples.'
  id: totrans-1538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` (`Features`, *optional*) — Dataset features.'
  id: totrans-1539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gen_kwargs(dict,` *optional*) — Keyword arguments to be passed to the `generator`
    callable. You can define a sharded iterable dataset by passing the list of shards
    in `gen_kwargs`. This can be used to improve shuffling and when iterating over
    the dataset with multiple workers.'
  id: totrans-1540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1541
  prefs: []
  type: TYPE_NORMAL
- en: '`IterableDataset`'
  id: totrans-1542
  prefs: []
  type: TYPE_NORMAL
- en: Create an Iterable Dataset from a generator.
  id: totrans-1543
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1544
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE233]'
  id: totrans-1545
  prefs: []
  type: TYPE_PRE
  zh: '[PRE233]'
- en: '[PRE234]'
  id: totrans-1546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE234]'
- en: '#### `remove_columns`'
  id: totrans-1547
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1999)'
  id: totrans-1548
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE235]'
  id: totrans-1549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE235]'
- en: Parameters
  id: totrans-1550
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, List[str]]`) — Name of the column(s) to remove.'
  id: totrans-1551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1552
  prefs: []
  type: TYPE_NORMAL
- en: '`IterableDataset`'
  id: totrans-1553
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset object without the columns to remove.
  id: totrans-1554
  prefs: []
  type: TYPE_NORMAL
- en: Remove one or several column(s) in the dataset and the features associated to
    them. The removal is done on-the-fly on the examples when iterating over the dataset.
  id: totrans-1555
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1556
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE236]'
  id: totrans-1557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE236]'
- en: '#### `select_columns`'
  id: totrans-1558
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L2039)'
  id: totrans-1559
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE237]'
  id: totrans-1560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE237]'
- en: Parameters
  id: totrans-1561
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, List[str]]`) — Name of the column(s) to select.'
  id: totrans-1562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1563
  prefs: []
  type: TYPE_NORMAL
- en: '`IterableDataset`'
  id: totrans-1564
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset object with selected columns.
  id: totrans-1565
  prefs: []
  type: TYPE_NORMAL
- en: Select one or several column(s) in the dataset and the features associated to
    them. The selection is done on-the-fly on the examples when iterating over the
    dataset.
  id: totrans-1566
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1567
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE238]'
  id: totrans-1568
  prefs: []
  type: TYPE_PRE
  zh: '[PRE238]'
- en: '#### `cast_column`'
  id: totrans-1569
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L2095)'
  id: totrans-1570
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE239]'
  id: totrans-1571
  prefs: []
  type: TYPE_PRE
  zh: '[PRE239]'
- en: Parameters
  id: totrans-1572
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — Column name.'
  id: totrans-1573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature` (`Feature`) — Target feature.'
  id: totrans-1574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1575
  prefs: []
  type: TYPE_NORMAL
- en: '`IterableDataset`'
  id: totrans-1576
  prefs: []
  type: TYPE_NORMAL
- en: Cast column to feature for decoding.
  id: totrans-1577
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1578
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE240]'
  id: totrans-1579
  prefs: []
  type: TYPE_PRE
  zh: '[PRE240]'
- en: '#### `cast`'
  id: totrans-1580
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L2146)'
  id: totrans-1581
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE241]'
  id: totrans-1582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE241]'
- en: Parameters
  id: totrans-1583
  prefs: []
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features))
    — New features to cast the dataset to. The name of the fields in the features
    must match the current column names. The type of the data must also be convertible
    from one type to the other. For non-trivial conversion, e.g. `string` <-> `ClassLabel`
    you should use [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    to update the Dataset.'
  id: totrans-1584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1585
  prefs: []
  type: TYPE_NORMAL
- en: '`IterableDataset`'
  id: totrans-1586
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset with casted features.
  id: totrans-1587
  prefs: []
  type: TYPE_NORMAL
- en: Cast the dataset to a new set of features.
  id: totrans-1588
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1589
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE242]'
  id: totrans-1590
  prefs: []
  type: TYPE_PRE
  zh: '[PRE242]'
- en: '#### `__iter__`'
  id: totrans-1591
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1360)'
  id: totrans-1592
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE243]'
  id: totrans-1593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE243]'
- en: '#### `iter`'
  id: totrans-1594
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1397)'
  id: totrans-1595
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE244]'
  id: totrans-1596
  prefs: []
  type: TYPE_PRE
  zh: '[PRE244]'
- en: Parameters
  id: totrans-1597
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size` (`int`) — size of each batch to yield.'
  id: totrans-1598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_last_batch` (`bool`, default *False*) — Whether a last batch smaller
    than the batch_size should be dropped'
  id: totrans-1599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterate through the batches of size *batch_size*.
  id: totrans-1600
  prefs: []
  type: TYPE_NORMAL
- en: '#### `map`'
  id: totrans-1601
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1580)'
  id: totrans-1602
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE245]'
  id: totrans-1603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE245]'
- en: Parameters
  id: totrans-1604
  prefs: []
  type: TYPE_NORMAL
- en: '`function` (`Callable`, *optional*, defaults to `None`) — Function applied
    on-the-fly on the examples when you iterate on the dataset. It must have one of
    the following signatures:'
  id: totrans-1605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and
    `with_indices=False`'
  id: totrans-1606
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any], idx: int) -> Dict[str, Any]` if `batched=False`
    and `with_indices=True`'
  id: totrans-1607
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and
    `with_indices=False`'
  id: totrans-1608
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List]` if
    `batched=True` and `with_indices=True`'
  id: totrans-1609
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For advanced usage, the function can also return a `pyarrow.Table`. Moreover
    if your function returns nothing (`None`), then `map` will run your function and
    return the dataset unchanged. If no function is provided, default to identity
    function: `lambda x: x`.'
  id: totrans-1610
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`with_indices` (`bool`, defaults to `False`) — Provide example indices to `function`.
    Note that in this case the signature of `function` should be `def function(example,
    idx[, rank]): ...`.'
  id: totrans-1611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_columns` (`Optional[Union[str, List[str]]]`, defaults to `None`) — The
    columns to be passed into `function` as positional arguments. If `None`, a dict
    mapping to all formatted columns is passed as one argument.'
  id: totrans-1612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batched` (`bool`, defaults to `False`) — Provide batch of examples to `function`.'
  id: totrans-1613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to `1000`) — Number of examples per
    batch provided to `function` if `batched=True`. `batch_size <= 0` or `batch_size
    == None` then provide the full dataset as a single batch to `function`.'
  id: totrans-1614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_last_batch` (`bool`, defaults to `False`) — Whether a last batch smaller
    than the batch_size should be dropped instead of being processed by the function.'
  id: totrans-1615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`remove_columns` (`[List[str]]`, *optional*, defaults to `None`) — Remove a
    selection of columns while doing the mapping. Columns will be removed before updating
    the examples with the output of `function`, i.e. if `function` is adding columns
    with names in `remove_columns`, these columns will be kept.'
  id: totrans-1616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` (`[Features]`, *optional*, defaults to `None`) — Feature types of
    the resulting dataset.'
  id: totrans-1617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fn_kwargs` (`Dict`, *optional*, default `None`) — Keyword arguments to be
    passed to `function`.'
  id: totrans-1618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a function to all the examples in the iterable dataset (individually or
    in batches) and update them. If your function returns a column that already exists,
    then it overwrites it. The function is applied on-the-fly on the examples when
    iterating over the dataset.
  id: totrans-1619
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify whether the function should be batched or not with the `batched`
    parameter:'
  id: totrans-1620
  prefs: []
  type: TYPE_NORMAL
- en: 'If batched is `False`, then the function takes 1 example in and should return
    1 example. An example is a dictionary, e.g. `{"text": "Hello there !"}`.'
  id: totrans-1621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If batched is `True` and `batch_size` is 1, then the function takes a batch
    of 1 example as input and can return a batch with 1 or more examples. A batch
    is a dictionary, e.g. a batch of 1 example is {“text”: [“Hello there !”]}.'
  id: totrans-1622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If batched is `True` and `batch_size` is `n` > 1, then the function takes a
    batch of `n` examples as input and can return a batch with `n` examples, or with
    an arbitrary number of examples. Note that the last batch may have less than `n`
    examples. A batch is a dictionary, e.g. a batch of `n` examples is `{"text": ["Hello
    there !"] * n}`.'
  id: totrans-1623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1624
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE246]'
  id: totrans-1625
  prefs: []
  type: TYPE_PRE
  zh: '[PRE246]'
- en: '#### `rename_column`'
  id: totrans-1626
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1939)'
  id: totrans-1627
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE247]'
  id: totrans-1628
  prefs: []
  type: TYPE_PRE
  zh: '[PRE247]'
- en: Parameters
  id: totrans-1629
  prefs: []
  type: TYPE_NORMAL
- en: '`original_column_name` (`str`) — Name of the column to rename.'
  id: totrans-1630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_column_name` (`str`) — New name for the column.'
  id: totrans-1631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1632
  prefs: []
  type: TYPE_NORMAL
- en: '`IterableDataset`'
  id: totrans-1633
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset with a renamed column.
  id: totrans-1634
  prefs: []
  type: TYPE_NORMAL
- en: Rename a column in the dataset, and move the features associated to the original
    column under the new column name.
  id: totrans-1635
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1636
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE248]'
  id: totrans-1637
  prefs: []
  type: TYPE_PRE
  zh: '[PRE248]'
- en: '#### `filter`'
  id: totrans-1638
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1693)'
  id: totrans-1639
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE249]'
  id: totrans-1640
  prefs: []
  type: TYPE_PRE
  zh: '[PRE249]'
- en: Parameters
  id: totrans-1641
  prefs: []
  type: TYPE_NORMAL
- en: '`function` (`Callable`) — Callable with one of the following signatures:'
  id: totrans-1642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any]) -> bool` if `with_indices=False, batched=False`'
  id: totrans-1643
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any], indices: int) -> bool` if `with_indices=True,
    batched=False`'
  id: totrans-1644
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, List]) -> List[bool]` if `with_indices=False,
    batched=True`'
  id: totrans-1645
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, List], indices: List[int]) -> List[bool]` if `with_indices=True,
    batched=True`'
  id: totrans-1646
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If no function is provided, defaults to an always True function: `lambda x:
    True`.'
  id: totrans-1647
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`with_indices` (`bool`, defaults to `False`) — Provide example indices to `function`.
    Note that in this case the signature of `function` should be `def function(example,
    idx): ...`.'
  id: totrans-1648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_columns` (`str` or `List[str]`, *optional*) — The columns to be passed
    into `function` as positional arguments. If `None`, a dict mapping to all formatted
    columns is passed as one argument.'
  id: totrans-1649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batched` (`bool`, defaults to `False`) — Provide batch of examples to `function`.'
  id: totrans-1650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, default `1000`) — Number of examples per batch
    provided to `function` if `batched=True`.'
  id: totrans-1651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fn_kwargs` (`Dict`, *optional*, default `None`) — Keyword arguments to be
    passed to `function`.'
  id: totrans-1652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a filter function to all the elements so that the dataset only includes
    examples according to the filter function. The filtering is done on-the-fly when
    iterating over the dataset.
  id: totrans-1653
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1654
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE250]'
  id: totrans-1655
  prefs: []
  type: TYPE_PRE
  zh: '[PRE250]'
- en: '#### `shuffle`'
  id: totrans-1656
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1771)'
  id: totrans-1657
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE251]'
  id: totrans-1658
  prefs: []
  type: TYPE_PRE
  zh: '[PRE251]'
- en: Parameters
  id: totrans-1659
  prefs: []
  type: TYPE_NORMAL
- en: '`seed` (`int`, *optional*, defaults to `None`) — Random seed that will be used
    to shuffle the dataset. It is used to sample from the shuffle buffer and also
    to shuffle the data shards.'
  id: totrans-1660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`numpy.random.Generator`, *optional*) — Numpy random Generator
    to use to compute the permutation of the dataset rows. If `generator=None` (default),
    uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).'
  id: totrans-1661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`buffer_size` (`int`, defaults to `1000`) — Size of the buffer.'
  id: totrans-1662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly shuffles the elements of this dataset.
  id: totrans-1663
  prefs: []
  type: TYPE_NORMAL
- en: This dataset fills a buffer with `buffer_size` elements, then randomly samples
    elements from this buffer, replacing the selected elements with new elements.
    For perfect shuffling, a buffer size greater than or equal to the full size of
    the dataset is required.
  id: totrans-1664
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if your dataset contains 10,000 elements but `buffer_size` is
    set to 1000, then `shuffle` will initially select a random element from only the
    first 1000 elements in the buffer. Once an element is selected, its space in the
    buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1000 element
    buffer.
  id: totrans-1665
  prefs: []
  type: TYPE_NORMAL
- en: If the dataset is made of several shards, it also does shuffle the order of
    the shards. However if the order has been fixed by using [skip()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.skip)
    or [take()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.take)
    then the order of the shards is kept unchanged.
  id: totrans-1666
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1667
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE252]'
  id: totrans-1668
  prefs: []
  type: TYPE_PRE
  zh: '[PRE252]'
- en: '#### `skip`'
  id: totrans-1669
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1841)'
  id: totrans-1670
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE253]'
  id: totrans-1671
  prefs: []
  type: TYPE_PRE
  zh: '[PRE253]'
- en: Parameters
  id: totrans-1672
  prefs: []
  type: TYPE_NORMAL
- en: '`n` (`int`) — Number of elements to skip.'
  id: totrans-1673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    that skips the first `n` elements.
  id: totrans-1674
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1675
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE254]'
  id: totrans-1676
  prefs: []
  type: TYPE_PRE
  zh: '[PRE254]'
- en: '#### `take`'
  id: totrans-1677
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1880)'
  id: totrans-1678
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE255]'
  id: totrans-1679
  prefs: []
  type: TYPE_PRE
  zh: '[PRE255]'
- en: Parameters
  id: totrans-1680
  prefs: []
  type: TYPE_NORMAL
- en: '`n` (`int`) — Number of elements to take.'
  id: totrans-1681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    with only the first `n` elements.
  id: totrans-1682
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1683
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE256]'
  id: totrans-1684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE256]'
- en: '#### `info`'
  id: totrans-1685
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L159)'
  id: totrans-1686
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE257]'
  id: totrans-1687
  prefs: []
  type: TYPE_PRE
  zh: '[PRE257]'
- en: '[DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)
    object containing all the metadata in the dataset.'
  id: totrans-1688
  prefs: []
  type: TYPE_NORMAL
- en: '#### `split`'
  id: totrans-1689
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L164)'
  id: totrans-1690
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE258]'
  id: totrans-1691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE258]'
- en: '[NamedSplit](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.NamedSplit)
    object corresponding to a named dataset split.'
  id: totrans-1692
  prefs: []
  type: TYPE_NORMAL
- en: '#### `builder_name`'
  id: totrans-1693
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L169)'
  id: totrans-1694
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE259]'
  id: totrans-1695
  prefs: []
  type: TYPE_PRE
  zh: '[PRE259]'
- en: '#### `citation`'
  id: totrans-1696
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L173)'
  id: totrans-1697
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE260]'
  id: totrans-1698
  prefs: []
  type: TYPE_PRE
  zh: '[PRE260]'
- en: '#### `config_name`'
  id: totrans-1699
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L177)'
  id: totrans-1700
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE261]'
  id: totrans-1701
  prefs: []
  type: TYPE_PRE
  zh: '[PRE261]'
- en: '#### `dataset_size`'
  id: totrans-1702
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L181)'
  id: totrans-1703
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE262]'
  id: totrans-1704
  prefs: []
  type: TYPE_PRE
  zh: '[PRE262]'
- en: '#### `description`'
  id: totrans-1705
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L185)'
  id: totrans-1706
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE263]'
  id: totrans-1707
  prefs: []
  type: TYPE_PRE
  zh: '[PRE263]'
- en: '#### `download_checksums`'
  id: totrans-1708
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L189)'
  id: totrans-1709
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE264]'
  id: totrans-1710
  prefs: []
  type: TYPE_PRE
  zh: '[PRE264]'
- en: '#### `download_size`'
  id: totrans-1711
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L193)'
  id: totrans-1712
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE265]'
  id: totrans-1713
  prefs: []
  type: TYPE_PRE
  zh: '[PRE265]'
- en: '#### `features`'
  id: totrans-1714
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L197)'
  id: totrans-1715
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE266]'
  id: totrans-1716
  prefs: []
  type: TYPE_PRE
  zh: '[PRE266]'
- en: '#### `homepage`'
  id: totrans-1717
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L201)'
  id: totrans-1718
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE267]'
  id: totrans-1719
  prefs: []
  type: TYPE_PRE
  zh: '[PRE267]'
- en: '#### `license`'
  id: totrans-1720
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L205)'
  id: totrans-1721
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE268]'
  id: totrans-1722
  prefs: []
  type: TYPE_PRE
  zh: '[PRE268]'
- en: '#### `size_in_bytes`'
  id: totrans-1723
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L209)'
  id: totrans-1724
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE269]'
  id: totrans-1725
  prefs: []
  type: TYPE_PRE
  zh: '[PRE269]'
- en: '#### `supervised_keys`'
  id: totrans-1726
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L213)'
  id: totrans-1727
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE270]'
  id: totrans-1728
  prefs: []
  type: TYPE_PRE
  zh: '[PRE270]'
- en: '#### `version`'
  id: totrans-1729
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L221)'
  id: totrans-1730
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE271]'
  id: totrans-1731
  prefs: []
  type: TYPE_PRE
  zh: '[PRE271]'
- en: IterableDatasetDict
  id: totrans-1732
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dictionary with split names as keys (‘train’, ‘test’ for example), and `IterableDataset`
    objects as values.
  id: totrans-1733
  prefs: []
  type: TYPE_NORMAL
- en: '### `class datasets.IterableDatasetDict`'
  id: totrans-1734
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1864)'
  id: totrans-1735
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE272]'
  id: totrans-1736
  prefs: []
  type: TYPE_PRE
  zh: '[PRE272]'
- en: '#### `map`'
  id: totrans-1737
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1899)'
  id: totrans-1738
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE273]'
  id: totrans-1739
  prefs: []
  type: TYPE_PRE
  zh: '[PRE273]'
- en: Parameters
  id: totrans-1740
  prefs: []
  type: TYPE_NORMAL
- en: '`function` (`Callable`, *optional*, defaults to `None`) — Function applied
    on-the-fly on the examples when you iterate on the dataset. It must have one of
    the following signatures:'
  id: totrans-1741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and
    `with_indices=False`'
  id: totrans-1742
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any], idx: int) -> Dict[str, Any]` if `batched=False`
    and `with_indices=True`'
  id: totrans-1743
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and
    `with_indices=False`'
  id: totrans-1744
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List]` if
    `batched=True` and `with_indices=True`'
  id: totrans-1745
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For advanced usage, the function can also return a `pyarrow.Table`. Moreover
    if your function returns nothing (`None`), then `map` will run your function and
    return the dataset unchanged. If no function is provided, default to identity
    function: `lambda x: x`.'
  id: totrans-1746
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`with_indices` (`bool`, defaults to `False`) — Provide example indices to `function`.
    Note that in this case the signature of `function` should be `def function(example,
    idx[, rank]): ...`.'
  id: totrans-1747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_columns` (`[Union[str, List[str]]]`, *optional*, defaults to `None`)
    — The columns to be passed into `function` as positional arguments. If `None`,
    a dict mapping to all formatted columns is passed as one argument.'
  id: totrans-1748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batched` (`bool`, defaults to `False`) — Provide batch of examples to `function`.'
  id: totrans-1749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to `1000`) — Number of examples per
    batch provided to `function` if `batched=True`.'
  id: totrans-1750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_last_batch` (`bool`, defaults to `False`) — Whether a last batch smaller
    than the `batch_size` should be dropped instead of being processed by the function.'
  id: totrans-1751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`remove_columns` (`[List[str]]`, *optional*, defaults to `None`) — Remove a
    selection of columns while doing the mapping. Columns will be removed before updating
    the examples with the output of `function`, i.e. if `function` is adding columns
    with names in `remove_columns`, these columns will be kept.'
  id: totrans-1752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fn_kwargs` (`Dict`, *optional*, defaults to `None`) — Keyword arguments to
    be passed to `function`'
  id: totrans-1753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a function to all the examples in the iterable dataset (individually or
    in batches) and update them. If your function returns a column that already exists,
    then it overwrites it. The function is applied on-the-fly on the examples when
    iterating over the dataset. The transformation is applied to all the datasets
    of the dataset dictionary.
  id: totrans-1754
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify whether the function should be batched or not with the `batched`
    parameter:'
  id: totrans-1755
  prefs: []
  type: TYPE_NORMAL
- en: 'If batched is `False`, then the function takes 1 example in and should return
    1 example. An example is a dictionary, e.g. `{"text": "Hello there !"}`.'
  id: totrans-1756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If batched is `True` and `batch_size` is 1, then the function takes a batch
    of 1 example as input and can return a batch with 1 or more examples. A batch
    is a dictionary, e.g. a batch of 1 example is `{"text": ["Hello there !"]}`.'
  id: totrans-1757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If batched is `True` and `batch_size` is `n` > 1, then the function takes a
    batch of `n` examples as input and can return a batch with `n` examples, or with
    an arbitrary number of examples. Note that the last batch may have less than `n`
    examples. A batch is a dictionary, e.g. a batch of `n` examples is `{"text": ["Hello
    there !"] * n}`.'
  id: totrans-1758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1759
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE274]'
  id: totrans-1760
  prefs: []
  type: TYPE_PRE
  zh: '[PRE274]'
- en: '#### `filter`'
  id: totrans-1761
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1988)'
  id: totrans-1762
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE275]'
  id: totrans-1763
  prefs: []
  type: TYPE_PRE
  zh: '[PRE275]'
- en: Parameters
  id: totrans-1764
  prefs: []
  type: TYPE_NORMAL
- en: '`function` (`Callable`) — Callable with one of the following signatures:'
  id: totrans-1765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any]) -> bool` if `with_indices=False, batched=False`'
  id: totrans-1766
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any], indices: int) -> bool` if `with_indices=True,
    batched=False`'
  id: totrans-1767
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, List]) -> List[bool]` if `with_indices=False,
    batched=True`'
  id: totrans-1768
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, List], indices: List[int]) -> List[bool]` if `with_indices=True,
    batched=True`'
  id: totrans-1769
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If no function is provided, defaults to an always True function: `lambda x:
    True`.'
  id: totrans-1770
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`with_indices` (`bool`, defaults to `False`) — Provide example indices to `function`.
    Note that in this case the signature of `function` should be `def function(example,
    idx): ...`.'
  id: totrans-1771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_columns` (`str` or `List[str]`, *optional*) — The columns to be passed
    into `function` as positional arguments. If `None`, a dict mapping to all formatted
    columns is passed as one argument.'
  id: totrans-1772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batched` (`bool`, defaults to `False`) — Provide batch of examples to `function`'
  id: totrans-1773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to `1000`) — Number of examples per
    batch provided to `function` if `batched=True`.'
  id: totrans-1774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fn_kwargs` (`Dict`, *optional*, defaults to `None`) — Keyword arguments to
    be passed to `function`'
  id: totrans-1775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a filter function to all the elements so that the dataset only includes
    examples according to the filter function. The filtering is done on-the-fly when
    iterating over the dataset. The filtering is applied to all the datasets of the
    dataset dictionary.
  id: totrans-1776
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1777
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE276]'
  id: totrans-1778
  prefs: []
  type: TYPE_PRE
  zh: '[PRE276]'
- en: '#### `shuffle`'
  id: totrans-1779
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2051)'
  id: totrans-1780
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE277]'
  id: totrans-1781
  prefs: []
  type: TYPE_PRE
  zh: '[PRE277]'
- en: Parameters
  id: totrans-1782
  prefs: []
  type: TYPE_NORMAL
- en: '`seed` (`int`, *optional*, defaults to `None`) — Random seed that will be used
    to shuffle the dataset. It is used to sample from the shuffle buffer and also
    to shuffle the data shards.'
  id: totrans-1783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`numpy.random.Generator`, *optional*) — Numpy random Generator
    to use to compute the permutation of the dataset rows. If `generator=None` (default),
    uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).'
  id: totrans-1784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`buffer_size` (`int`, defaults to `1000`) — Size of the buffer.'
  id: totrans-1785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly shuffles the elements of this dataset. The shuffling is applied to
    all the datasets of the dataset dictionary.
  id: totrans-1786
  prefs: []
  type: TYPE_NORMAL
- en: This dataset fills a buffer with buffer_size elements, then randomly samples
    elements from this buffer, replacing the selected elements with new elements.
    For perfect shuffling, a buffer size greater than or equal to the full size of
    the dataset is required.
  id: totrans-1787
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if your dataset contains 10,000 elements but `buffer_size` is
    set to 1000, then `shuffle` will initially select a random element from only the
    first 1000 elements in the buffer. Once an element is selected, its space in the
    buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1000 element
    buffer.
  id: totrans-1788
  prefs: []
  type: TYPE_NORMAL
- en: If the dataset is made of several shards, it also does `shuffle` the order of
    the shards. However if the order has been fixed by using [skip()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.skip)
    or [take()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.take)
    then the order of the shards is kept unchanged.
  id: totrans-1789
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1790
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE278]'
  id: totrans-1791
  prefs: []
  type: TYPE_PRE
  zh: '[PRE278]'
- en: '#### `with_format`'
  id: totrans-1792
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1870)'
  id: totrans-1793
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE279]'
  id: totrans-1794
  prefs: []
  type: TYPE_PRE
  zh: '[PRE279]'
- en: Parameters
  id: totrans-1795
  prefs: []
  type: TYPE_NORMAL
- en: '`type` (`str`, *optional*, defaults to `None`) — If set to “torch”, the returned
    dataset will be a subclass of `torch.utils.data.IterableDataset` to be used in
    a `DataLoader`.'
  id: totrans-1796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return a dataset with the specified format. This method only supports the “torch”
    format for now. The format is set to all the datasets of the dataset dictionary.
  id: totrans-1797
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1798
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE280]'
  id: totrans-1799
  prefs: []
  type: TYPE_PRE
  zh: '[PRE280]'
- en: '#### `cast`'
  id: totrans-1800
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2253)'
  id: totrans-1801
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE281]'
  id: totrans-1802
  prefs: []
  type: TYPE_PRE
  zh: '[PRE281]'
- en: Parameters
  id: totrans-1803
  prefs: []
  type: TYPE_NORMAL
- en: '`features` (`Features`) — New features to cast the dataset to. The name of
    the fields in the features must match the current column names. The type of the
    data must also be convertible from one type to the other. For non-trivial conversion,
    e.g. `string` <-> `ClassLabel` you should use `map` to update the Dataset.'
  id: totrans-1804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1805
  prefs: []
  type: TYPE_NORMAL
- en: '[IterableDatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDatasetDict)'
  id: totrans-1806
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset with casted features.
  id: totrans-1807
  prefs: []
  type: TYPE_NORMAL
- en: Cast the dataset to a new set of features. The type casting is applied to all
    the datasets of the dataset dictionary.
  id: totrans-1808
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1809
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE282]'
  id: totrans-1810
  prefs: []
  type: TYPE_PRE
  zh: '[PRE282]'
- en: '#### `cast_column`'
  id: totrans-1811
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2222)'
  id: totrans-1812
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE283]'
  id: totrans-1813
  prefs: []
  type: TYPE_PRE
  zh: '[PRE283]'
- en: Parameters
  id: totrans-1814
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — Column name.'
  id: totrans-1815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature` (`Feature`) — Target feature.'
  id: totrans-1816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cast column to feature for decoding. The type casting is applied to all the
    datasets of the dataset dictionary.
  id: totrans-1817
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1818
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE284]'
  id: totrans-1819
  prefs: []
  type: TYPE_PRE
  zh: '[PRE284]'
- en: '#### `remove_columns`'
  id: totrans-1820
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2170)'
  id: totrans-1821
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE285]'
  id: totrans-1822
  prefs: []
  type: TYPE_PRE
  zh: '[PRE285]'
- en: Parameters
  id: totrans-1823
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, List[str]]`) — Name of the column(s) to remove.'
  id: totrans-1824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1825
  prefs: []
  type: TYPE_NORMAL
- en: '[IterableDatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDatasetDict)'
  id: totrans-1826
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset object without the columns to remove.
  id: totrans-1827
  prefs: []
  type: TYPE_NORMAL
- en: Remove one or several column(s) in the dataset and the features associated to
    them. The removal is done on-the-fly on the examples when iterating over the dataset.
    The removal is applied to all the datasets of the dataset dictionary.
  id: totrans-1828
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1829
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE286]'
  id: totrans-1830
  prefs: []
  type: TYPE_PRE
  zh: '[PRE286]'
- en: '#### `rename_column`'
  id: totrans-1831
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2109)'
  id: totrans-1832
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE287]'
  id: totrans-1833
  prefs: []
  type: TYPE_PRE
  zh: '[PRE287]'
- en: Parameters
  id: totrans-1834
  prefs: []
  type: TYPE_NORMAL
- en: '`original_column_name` (`str`) — Name of the column to rename.'
  id: totrans-1835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_column_name` (`str`) — New name for the column.'
  id: totrans-1836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1837
  prefs: []
  type: TYPE_NORMAL
- en: '[IterableDatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDatasetDict)'
  id: totrans-1838
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset with a renamed column.
  id: totrans-1839
  prefs: []
  type: TYPE_NORMAL
- en: Rename a column in the dataset, and move the features associated to the original
    column under the new column name. The renaming is applied to all the datasets
    of the dataset dictionary.
  id: totrans-1840
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1841
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE288]'
  id: totrans-1842
  prefs: []
  type: TYPE_PRE
  zh: '[PRE288]'
- en: '#### `rename_columns`'
  id: totrans-1843
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2142)'
  id: totrans-1844
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE289]'
  id: totrans-1845
  prefs: []
  type: TYPE_PRE
  zh: '[PRE289]'
- en: Parameters
  id: totrans-1846
  prefs: []
  type: TYPE_NORMAL
- en: '`column_mapping` (`Dict[str, str]`) — A mapping of columns to rename to their
    new names.'
  id: totrans-1847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1848
  prefs: []
  type: TYPE_NORMAL
- en: '[IterableDatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDatasetDict)'
  id: totrans-1849
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset with renamed columns
  id: totrans-1850
  prefs: []
  type: TYPE_NORMAL
- en: Rename several columns in the dataset, and move the features associated to the
    original columns under the new column names. The renaming is applied to all the
    datasets of the dataset dictionary.
  id: totrans-1851
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1852
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE290]'
  id: totrans-1853
  prefs: []
  type: TYPE_PRE
  zh: '[PRE290]'
- en: '#### `select_columns`'
  id: totrans-1854
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2196)'
  id: totrans-1855
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE291]'
  id: totrans-1856
  prefs: []
  type: TYPE_PRE
  zh: '[PRE291]'
- en: Parameters
  id: totrans-1857
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, List[str]]`) — Name of the column(s) to keep.'
  id: totrans-1858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1859
  prefs: []
  type: TYPE_NORMAL
- en: '[IterableDatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDatasetDict)'
  id: totrans-1860
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset object with only selected columns.
  id: totrans-1861
  prefs: []
  type: TYPE_NORMAL
- en: Select one or several column(s) in the dataset and the features associated to
    them. The selection is done on-the-fly on the examples when iterating over the
    dataset. The selection is applied to all the datasets of the dataset dictionary.
  id: totrans-1862
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1863
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE292]'
  id: totrans-1864
  prefs: []
  type: TYPE_PRE
  zh: '[PRE292]'
- en: Features
  id: totrans-1865
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class datasets.Features`'
  id: totrans-1866
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1582)'
  id: totrans-1867
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE293]'
  id: totrans-1868
  prefs: []
  type: TYPE_PRE
  zh: '[PRE293]'
- en: A special dictionary that defines the internal structure of a dataset.
  id: totrans-1869
  prefs: []
  type: TYPE_NORMAL
- en: Instantiated with a dictionary of type `dict[str, FieldType]`, where keys are
    the desired column names, and values are the type of that column.
  id: totrans-1870
  prefs: []
  type: TYPE_NORMAL
- en: '`FieldType` can be one of the following:'
  id: totrans-1871
  prefs: []
  type: TYPE_NORMAL
- en: a [Value](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Value)
    feature specifies a single typed value, e.g. `int64` or `string`.
  id: totrans-1872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a [ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)
    feature specifies a field with a predefined set of classes which can have labels
    associated to them and will be stored as integers in the dataset.
  id: totrans-1873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a python `dict` which specifies that the field is a nested field containing
    a mapping of sub-fields to sub-fields features. It’s possible to have nested fields
    of nested fields in an arbitrary manner.
  id: totrans-1874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a python `list` or a [Sequence](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Sequence)
    specifies that the field contains a list of objects. The python `list` or [Sequence](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Sequence)
    should be provided with a single sub-feature as an example of the feature type
    hosted in this list.
  id: totrans-1875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [Sequence](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Sequence)
    with a internal dictionary feature will be automatically converted into a dictionary
    of lists. This behavior is implemented to have a compatilbity layer with the TensorFlow
    Datasets library but may be un-wanted in some cases. If you don’t want this behavior,
    you can use a python `list` instead of the [Sequence](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Sequence).
  id: totrans-1876
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a [Array2D](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Array2D),
    [Array3D](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Array3D),
    [Array4D](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Array4D)
    or [Array5D](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Array5D)
    feature for multidimensional arrays.
  id: totrans-1877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    feature to store the absolute path to an audio file or a dictionary with the relative
    path to an audio file (“path” key) and its bytes content (“bytes” key). This feature
    extracts the audio data.
  id: totrans-1878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    feature to store the absolute path to an image file, an `np.ndarray` object, a
    `PIL.Image.Image` object or a dictionary with the relative path to an image file
    (“path” key) and its bytes content (“bytes” key). This feature extracts the image
    data.
  id: totrans-1879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Translation](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Translation)
    and [TranslationVariableLanguages](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.TranslationVariableLanguages),
    the two features specific to Machine Translation.'
  id: totrans-1880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `copy`'
  id: totrans-1881
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1993)'
  id: totrans-1882
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE294]'
  id: totrans-1883
  prefs: []
  type: TYPE_PRE
  zh: '[PRE294]'
- en: Make a deep copy of [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features).
  id: totrans-1884
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1885
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE295]'
  id: totrans-1886
  prefs: []
  type: TYPE_PRE
  zh: '[PRE295]'
- en: '#### `decode_batch`'
  id: totrans-1887
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1966)'
  id: totrans-1888
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE296]'
  id: totrans-1889
  prefs: []
  type: TYPE_PRE
  zh: '[PRE296]'
- en: Parameters
  id: totrans-1890
  prefs: []
  type: TYPE_NORMAL
- en: '`batch` (`dict[str, list[Any]]`) — Dataset batch data.'
  id: totrans-1891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_per_repo_id` (`dict`, *optional*) — To access and decode audio or image
    files from private repositories on the Hub, you can pass a dictionary repo_id
    (str) -> token (bool or str)'
  id: totrans-1892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decode batch with custom feature decoding.
  id: totrans-1893
  prefs: []
  type: TYPE_NORMAL
- en: '#### `decode_column`'
  id: totrans-1894
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1948)'
  id: totrans-1895
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE297]'
  id: totrans-1896
  prefs: []
  type: TYPE_PRE
  zh: '[PRE297]'
- en: Parameters
  id: totrans-1897
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`list[Any]`) — Dataset column data.'
  id: totrans-1898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`column_name` (`str`) — Dataset column name.'
  id: totrans-1899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decode column with custom feature decoding.
  id: totrans-1900
  prefs: []
  type: TYPE_NORMAL
- en: '#### `decode_example`'
  id: totrans-1901
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1925)'
  id: totrans-1902
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE298]'
  id: totrans-1903
  prefs: []
  type: TYPE_PRE
  zh: '[PRE298]'
- en: Parameters
  id: totrans-1904
  prefs: []
  type: TYPE_NORMAL
- en: '`example` (`dict[str, Any]`) — Dataset row data.'
  id: totrans-1905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_per_repo_id` (`dict`, *optional*) — To access and decode audio or image
    files from private repositories on the Hub, you can pass a dictionary `repo_id
    (str) -> token (bool or str)`.'
  id: totrans-1906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decode example with custom feature decoding.
  id: totrans-1907
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_batch`'
  id: totrans-1908
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1906)'
  id: totrans-1909
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE299]'
  id: totrans-1910
  prefs: []
  type: TYPE_PRE
  zh: '[PRE299]'
- en: Parameters
  id: totrans-1911
  prefs: []
  type: TYPE_NORMAL
- en: '`batch` (`dict[str, list[Any]]`) — Data in a Dataset batch.'
  id: totrans-1912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encode batch into a format for Arrow.
  id: totrans-1913
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_column`'
  id: totrans-1914
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1890)'
  id: totrans-1915
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE300]'
  id: totrans-1916
  prefs: []
  type: TYPE_PRE
  zh: '[PRE300]'
- en: Parameters
  id: totrans-1917
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`list[Any]`) — Data in a Dataset column.'
  id: totrans-1918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`column_name` (`str`) — Dataset column name.'
  id: totrans-1919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encode column into a format for Arrow.
  id: totrans-1920
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_example`'
  id: totrans-1921
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1876)'
  id: totrans-1922
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE301]'
  id: totrans-1923
  prefs: []
  type: TYPE_PRE
  zh: '[PRE301]'
- en: Parameters
  id: totrans-1924
  prefs: []
  type: TYPE_NORMAL
- en: '`example` (`dict[str, Any]`) — Data in a Dataset row.'
  id: totrans-1925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encode example into a format for Arrow.
  id: totrans-1926
  prefs: []
  type: TYPE_NORMAL
- en: '#### `flatten`'
  id: totrans-1927
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L2080)'
  id: totrans-1928
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE302]'
  id: totrans-1929
  prefs: []
  type: TYPE_PRE
  zh: '[PRE302]'
- en: Returns
  id: totrans-1930
  prefs: []
  type: TYPE_NORMAL
- en: '[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)'
  id: totrans-1931
  prefs: []
  type: TYPE_NORMAL
- en: The flattened features.
  id: totrans-1932
  prefs: []
  type: TYPE_NORMAL
- en: 'Flatten the features. Every dictionary column is removed and is replaced by
    all the subfields it contains. The new fields are named by concatenating the name
    of the original column and the subfield name like this: `<original>.<subfield>`.'
  id: totrans-1933
  prefs: []
  type: TYPE_NORMAL
- en: 'If a column contains nested dictionaries, then all the lower-level subfields
    names are also concatenated to form new columns: `<original>.<subfield>.<subsubfield>`,
    etc.'
  id: totrans-1934
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1935
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE303]'
  id: totrans-1936
  prefs: []
  type: TYPE_PRE
  zh: '[PRE303]'
- en: '#### `from_arrow_schema`'
  id: totrans-1937
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1657)'
  id: totrans-1938
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE304]'
  id: totrans-1939
  prefs: []
  type: TYPE_PRE
  zh: '[PRE304]'
- en: Parameters
  id: totrans-1940
  prefs: []
  type: TYPE_NORMAL
- en: '`pa_schema` (`pyarrow.Schema`) — Arrow Schema.'
  id: totrans-1941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    from Arrow Schema. It also checks the schema metadata for Hugging Face Datasets
    features. Non-nullable fields are not supported and set to nullable.
  id: totrans-1942
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_dict`'
  id: totrans-1943
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1688)'
  id: totrans-1944
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE305]'
  id: totrans-1945
  prefs: []
  type: TYPE_PRE
  zh: '[PRE305]'
- en: Parameters
  id: totrans-1946
  prefs: []
  type: TYPE_NORMAL
- en: '`dic` (*dict[str, Any]*) — Python dictionary.'
  id: totrans-1947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1948
  prefs: []
  type: TYPE_NORMAL
- en: '*Features*'
  id: totrans-1949
  prefs: []
  type: TYPE_NORMAL
- en: Construct [*Features*] from dict.
  id: totrans-1950
  prefs: []
  type: TYPE_NORMAL
- en: Regenerate the nested feature object from a deserialized dict. We use the *_type*
    key to infer the dataclass name of the feature *FieldType*.
  id: totrans-1951
  prefs: []
  type: TYPE_NORMAL
- en: It allows for a convenient constructor syntax to define features from deserialized
    JSON dictionaries. This function is used in particular when deserializing a [*DatasetInfo*]
    that was dumped to a JSON object. This acts as an analogue to [*Features.from_arrow_schema*]
    and handles the recursive field-by-field instantiation, but doesn’t require any
    mapping to/from pyarrow, except for the fact that it takes advantage of the mapping
    of pyarrow primitive dtypes that [*Value*] automatically performs.
  id: totrans-1952
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1953
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE306]'
  id: totrans-1954
  prefs: []
  type: TYPE_PRE
  zh: '[PRE306]'
- en: '#### `reorder_fields_as`'
  id: totrans-1955
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L2013)'
  id: totrans-1956
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE307]'
  id: totrans-1957
  prefs: []
  type: TYPE_PRE
  zh: '[PRE307]'
- en: Parameters
  id: totrans-1958
  prefs: []
  type: TYPE_NORMAL
- en: '`other` ([*Features*]) — The other [*Features*] to align with.'
  id: totrans-1959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reorder Features fields to match the field order of other [*Features*].
  id: totrans-1960
  prefs: []
  type: TYPE_NORMAL
- en: The order of the fields is important since it matters for the underlying arrow
    data. Re-ordering the fields allows to make the underlying arrow data type match.
  id: totrans-1961
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1962
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE308]'
  id: totrans-1963
  prefs: []
  type: TYPE_PRE
  zh: '[PRE308]'
- en: '### `class datasets.Sequence`'
  id: totrans-1964
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1128)'
  id: totrans-1965
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE309]'
  id: totrans-1966
  prefs: []
  type: TYPE_PRE
  zh: '[PRE309]'
- en: Parameters
  id: totrans-1967
  prefs: []
  type: TYPE_NORMAL
- en: '`length` (`int`) — Length of the sequence.'
  id: totrans-1968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a list of feature from a single type or a dict of types. Mostly here
    for compatiblity with tfds.
  id: totrans-1969
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1970
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE310]'
  id: totrans-1971
  prefs: []
  type: TYPE_PRE
  zh: '[PRE310]'
- en: '### `class datasets.ClassLabel`'
  id: totrans-1972
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L930)'
  id: totrans-1973
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE311]'
  id: totrans-1974
  prefs: []
  type: TYPE_PRE
  zh: '[PRE311]'
- en: Parameters
  id: totrans-1975
  prefs: []
  type: TYPE_NORMAL
- en: '`num_classes` (`int`, *optional*) — Number of classes. All labels must be <
    `num_classes`.'
  id: totrans-1976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`names` (`list` of `str`, *optional*) — String names for the integer classes.
    The order in which the names are provided is kept.'
  id: totrans-1977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`names_file` (`str`, *optional*) — Path to a file with names for the integer
    classes, one per line.'
  id: totrans-1978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature type for integer class labels.
  id: totrans-1979
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 3 ways to define a `ClassLabel`, which correspond to the 3 arguments:'
  id: totrans-1980
  prefs: []
  type: TYPE_NORMAL
- en: '`num_classes`: Create 0 to (num_classes-1) labels.'
  id: totrans-1981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`names`: List of label strings.'
  id: totrans-1982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`names_file`: File containing the list of labels.'
  id: totrans-1983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the hood the labels are stored as integers. You can use negative integers
    to represent unknown/missing labels.
  id: totrans-1984
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1985
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE312]'
  id: totrans-1986
  prefs: []
  type: TYPE_PRE
  zh: '[PRE312]'
- en: '#### `cast_storage`'
  id: totrans-1987
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1096)'
  id: totrans-1988
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE313]'
  id: totrans-1989
  prefs: []
  type: TYPE_PRE
  zh: '[PRE313]'
- en: Parameters
  id: totrans-1990
  prefs: []
  type: TYPE_NORMAL
- en: '`storage` (`Union[pa.StringArray, pa.IntegerArray]`) — PyArrow array to cast.'
  id: totrans-1991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1992
  prefs: []
  type: TYPE_NORMAL
- en: '`pa.Int64Array`'
  id: totrans-1993
  prefs: []
  type: TYPE_NORMAL
- en: Array in the `ClassLabel` arrow storage type.
  id: totrans-1994
  prefs: []
  type: TYPE_NORMAL
- en: 'Cast an Arrow array to the `ClassLabel` arrow storage type. The Arrow types
    that can be converted to the `ClassLabel` pyarrow storage type are:'
  id: totrans-1995
  prefs: []
  type: TYPE_NORMAL
- en: '`pa.string()`'
  id: totrans-1996
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.int()`'
  id: totrans-1997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `int2str`'
  id: totrans-1998
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1050)'
  id: totrans-1999
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE314]'
  id: totrans-2000
  prefs: []
  type: TYPE_PRE
  zh: '[PRE314]'
- en: Conversion `integer` => class name `string`.
  id: totrans-2001
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding unknown/missing labels: passing negative integers raises `ValueError`.'
  id: totrans-2002
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-2003
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE315]'
  id: totrans-2004
  prefs: []
  type: TYPE_PRE
  zh: '[PRE315]'
- en: '#### `str2int`'
  id: totrans-2005
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1005)'
  id: totrans-2006
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE316]'
  id: totrans-2007
  prefs: []
  type: TYPE_PRE
  zh: '[PRE316]'
- en: Conversion class name `string` => `integer`.
  id: totrans-2008
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-2009
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE317]'
  id: totrans-2010
  prefs: []
  type: TYPE_PRE
  zh: '[PRE317]'
- en: '### `class datasets.Value`'
  id: totrans-2011
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L452)'
  id: totrans-2012
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE318]'
  id: totrans-2013
  prefs: []
  type: TYPE_PRE
  zh: '[PRE318]'
- en: 'The `Value` dtypes are as follows:'
  id: totrans-2014
  prefs: []
  type: TYPE_NORMAL
- en: '`null`'
  id: totrans-2015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bool`'
  id: totrans-2016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`int8`'
  id: totrans-2017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`int16`'
  id: totrans-2018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`int32`'
  id: totrans-2019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`int64`'
  id: totrans-2020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`uint8`'
  id: totrans-2021
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`uint16`'
  id: totrans-2022
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`uint32`'
  id: totrans-2023
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`uint64`'
  id: totrans-2024
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float16`'
  id: totrans-2025
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float32` (alias float)'
  id: totrans-2026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float64` (alias double)'
  id: totrans-2027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time32[(s|ms)]`'
  id: totrans-2028
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time64[(us|ns)]`'
  id: totrans-2029
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timestamp[(s|ms|us|ns)]`'
  id: totrans-2030
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timestamp[(s|ms|us|ns), tz=(tzstring)]`'
  id: totrans-2031
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date32`'
  id: totrans-2032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date64`'
  id: totrans-2033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`duration[(s|ms|us|ns)]`'
  id: totrans-2034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decimal128(precision, scale)`'
  id: totrans-2035
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decimal256(precision, scale)`'
  id: totrans-2036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary`'
  id: totrans-2037
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`large_binary`'
  id: totrans-2038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`string`'
  id: totrans-2039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`large_string`'
  id: totrans-2040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-2041
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE319]'
  id: totrans-2042
  prefs: []
  type: TYPE_PRE
  zh: '[PRE319]'
- en: '### `class datasets.Translation`'
  id: totrans-2043
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/translation.py#L11)'
  id: totrans-2044
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE320]'
  id: totrans-2045
  prefs: []
  type: TYPE_PRE
  zh: '[PRE320]'
- en: Parameters
  id: totrans-2046
  prefs: []
  type: TYPE_NORMAL
- en: '`languages` (`dict`) — A dictionary for each example mapping string language
    codes to string translations.'
  id: totrans-2047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FeatureConnector` for translations with fixed languages per example. Here
    for compatiblity with tfds.'
  id: totrans-2048
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-2049
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE321]'
  id: totrans-2050
  prefs: []
  type: TYPE_PRE
  zh: '[PRE321]'
- en: '#### `flatten`'
  id: totrans-2051
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/translation.py#L44)'
  id: totrans-2052
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE322]'
  id: totrans-2053
  prefs: []
  type: TYPE_PRE
  zh: '[PRE322]'
- en: Flatten the Translation feature into a dictionary.
  id: totrans-2054
  prefs: []
  type: TYPE_NORMAL
- en: '### `class datasets.TranslationVariableLanguages`'
  id: totrans-2055
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/translation.py#L51)'
  id: totrans-2056
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE323]'
  id: totrans-2057
  prefs: []
  type: TYPE_PRE
  zh: '[PRE323]'
- en: Parameters
  id: totrans-2058
  prefs: []
  type: TYPE_NORMAL
- en: '`languages` (`dict`) — A dictionary for each example mapping string language
    codes to one or more string translations. The languages present may vary from
    example to example.'
  id: totrans-2059
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-2060
  prefs: []
  type: TYPE_NORMAL
- en: '`language` or `translation` (variable-length 1D `tf.Tensor` of `tf.string`)'
  id: totrans-2061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language codes sorted in ascending order or plain text translations, sorted
    to align with language codes.
  id: totrans-2062
  prefs: []
  type: TYPE_NORMAL
- en: '`FeatureConnector` for translations with variable languages per example. Here
    for compatiblity with tfds.'
  id: totrans-2063
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-2064
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE324]'
  id: totrans-2065
  prefs: []
  type: TYPE_PRE
  zh: '[PRE324]'
- en: '#### `flatten`'
  id: totrans-2066
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/translation.py#L122)'
  id: totrans-2067
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE325]'
  id: totrans-2068
  prefs: []
  type: TYPE_PRE
  zh: '[PRE325]'
- en: Flatten the TranslationVariableLanguages feature into a dictionary.
  id: totrans-2069
  prefs: []
  type: TYPE_NORMAL
- en: '### `class datasets.Array2D`'
  id: totrans-2070
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L535)'
  id: totrans-2071
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE326]'
  id: totrans-2072
  prefs: []
  type: TYPE_PRE
  zh: '[PRE326]'
- en: Parameters
  id: totrans-2073
  prefs: []
  type: TYPE_NORMAL
- en: '`shape` (`tuple`) — The size of each dimension.'
  id: totrans-2074
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`str`) — The value of the data type.'
  id: totrans-2075
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a two-dimensional array.
  id: totrans-2076
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-2077
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE327]'
  id: totrans-2078
  prefs: []
  type: TYPE_PRE
  zh: '[PRE327]'
- en: '### `class datasets.Array3D`'
  id: totrans-2079
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L560)'
  id: totrans-2080
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE328]'
  id: totrans-2081
  prefs: []
  type: TYPE_PRE
  zh: '[PRE328]'
- en: Parameters
  id: totrans-2082
  prefs: []
  type: TYPE_NORMAL
- en: '`shape` (`tuple`) — The size of each dimension.'
  id: totrans-2083
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`str`) — The value of the data type.'
  id: totrans-2084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a three-dimensional array.
  id: totrans-2085
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-2086
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE329]'
  id: totrans-2087
  prefs: []
  type: TYPE_PRE
  zh: '[PRE329]'
- en: '### `class datasets.Array4D`'
  id: totrans-2088
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L585)'
  id: totrans-2089
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE330]'
  id: totrans-2090
  prefs: []
  type: TYPE_PRE
  zh: '[PRE330]'
- en: Parameters
  id: totrans-2091
  prefs: []
  type: TYPE_NORMAL
- en: '`shape` (`tuple`) — The size of each dimension.'
  id: totrans-2092
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`str`) — The value of the data type.'
  id: totrans-2093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a four-dimensional array.
  id: totrans-2094
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-2095
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE331]'
  id: totrans-2096
  prefs: []
  type: TYPE_PRE
  zh: '[PRE331]'
- en: '### `class datasets.Array5D`'
  id: totrans-2097
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L610)'
  id: totrans-2098
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE332]'
  id: totrans-2099
  prefs: []
  type: TYPE_PRE
  zh: '[PRE332]'
- en: Parameters
  id: totrans-2100
  prefs: []
  type: TYPE_NORMAL
- en: '`shape` (`tuple`) — The size of each dimension.'
  id: totrans-2101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`str`) — The value of the data type.'
  id: totrans-2102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a five-dimensional array.
  id: totrans-2103
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-2104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE333]'
  id: totrans-2105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE333]'
- en: '### `class datasets.Audio`'
  id: totrans-2106
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L20)'
  id: totrans-2107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE334]'
  id: totrans-2108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE334]'
- en: Parameters
  id: totrans-2109
  prefs: []
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*) — Target sampling rate. If `None`, the
    native sampling rate is used.'
  id: totrans-2110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mono` (`bool`, defaults to `True`) — Whether to convert the audio signal to
    mono by averaging samples across channels.'
  id: totrans-2111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decode` (`bool`, defaults to `True`) — Whether to decode the audio data. If
    `False`, returns the underlying dictionary in the format `{"path": audio_path,
    "bytes": audio_bytes}`.'
  id: totrans-2112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio `Feature` to extract audio data from an audio file.
  id: totrans-2113
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: The Audio feature accepts as input:'
  id: totrans-2114
  prefs: []
  type: TYPE_NORMAL
- en: 'A `str`: Absolute path to the audio file (i.e. random access is allowed).'
  id: totrans-2115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `dict` with the keys:'
  id: totrans-2116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path`: String with relative path of the audio file to the archive file.'
  id: totrans-2117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bytes`: Bytes content of the audio file.'
  id: totrans-2118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This is useful for archived files with sequential access.
  id: totrans-2119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A `dict` with the keys:'
  id: totrans-2120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path`: String with relative path of the audio file to the archive file.'
  id: totrans-2121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`array`: Array containing the audio sample'
  id: totrans-2122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate`: Integer corresponding to the sampling rate of the audio sample.'
  id: totrans-2123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This is useful for archived files with sequential access.
  id: totrans-2124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-2125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE335]'
  id: totrans-2126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE335]'
- en: '#### `cast_storage`'
  id: totrans-2127
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L209)'
  id: totrans-2128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE336]'
  id: totrans-2129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE336]'
- en: Parameters
  id: totrans-2130
  prefs: []
  type: TYPE_NORMAL
- en: '`storage` (`Union[pa.StringArray, pa.StructArray]`) — PyArrow array to cast.'
  id: totrans-2131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-2132
  prefs: []
  type: TYPE_NORMAL
- en: '`pa.StructArray`'
  id: totrans-2133
  prefs: []
  type: TYPE_NORMAL
- en: 'Array in the Audio arrow storage type, that is `pa.struct({"bytes": pa.binary(),
    "path": pa.string()})`'
  id: totrans-2134
  prefs: []
  type: TYPE_NORMAL
- en: 'Cast an Arrow array to the Audio arrow storage type. The Arrow types that can
    be converted to the Audio pyarrow storage type are:'
  id: totrans-2135
  prefs: []
  type: TYPE_NORMAL
- en: '`pa.string()` - it must contain the “path” data'
  id: totrans-2136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.binary()` - it must contain the audio bytes'
  id: totrans-2137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.struct({"bytes": pa.binary()})`'
  id: totrans-2138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.struct({"path": pa.string()})`'
  id: totrans-2139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.struct({"bytes": pa.binary(), "path": pa.string()})` - order doesn’t matter'
  id: totrans-2140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `decode_example`'
  id: totrans-2141
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L126)'
  id: totrans-2142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE337]'
  id: totrans-2143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE337]'
- en: Parameters
  id: totrans-2144
  prefs: []
  type: TYPE_NORMAL
- en: '`value` (`dict`) — A dictionary with keys:'
  id: totrans-2145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path`: String with relative audio file path.'
  id: totrans-2146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bytes`: Bytes of the audio file.'
  id: totrans-2147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_per_repo_id` (`dict`, *optional*) — To access and decode audio files
    from private repositories on the Hub, you can pass a dictionary repo_id (`str`)
    -> token (`bool` or `str`)'
  id: totrans-2148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-2149
  prefs: []
  type: TYPE_NORMAL
- en: '`dict`'
  id: totrans-2150
  prefs: []
  type: TYPE_NORMAL
- en: Decode example audio file into audio data.
  id: totrans-2151
  prefs: []
  type: TYPE_NORMAL
- en: '#### `embed_storage`'
  id: totrans-2152
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L247)'
  id: totrans-2153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE338]'
  id: totrans-2154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE338]'
- en: Parameters
  id: totrans-2155
  prefs: []
  type: TYPE_NORMAL
- en: '`storage` (`pa.StructArray`) — PyArrow array to embed.'
  id: totrans-2156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-2157
  prefs: []
  type: TYPE_NORMAL
- en: '`pa.StructArray`'
  id: totrans-2158
  prefs: []
  type: TYPE_NORMAL
- en: 'Array in the Audio arrow storage type, that is `pa.struct({"bytes": pa.binary(),
    "path": pa.string()})`.'
  id: totrans-2159
  prefs: []
  type: TYPE_NORMAL
- en: Embed audio files into the Arrow array.
  id: totrans-2160
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_example`'
  id: totrans-2161
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L77)'
  id: totrans-2162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE339]'
  id: totrans-2163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE339]'
- en: Parameters
  id: totrans-2164
  prefs: []
  type: TYPE_NORMAL
- en: '`value` (`str` or `dict`) — Data passed as input to Audio feature.'
  id: totrans-2165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-2166
  prefs: []
  type: TYPE_NORMAL
- en: '`dict`'
  id: totrans-2167
  prefs: []
  type: TYPE_NORMAL
- en: Encode example into a format for Arrow.
  id: totrans-2168
  prefs: []
  type: TYPE_NORMAL
- en: '#### `flatten`'
  id: totrans-2169
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L198)'
  id: totrans-2170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE340]'
  id: totrans-2171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE340]'
- en: If in the decodable state, raise an error, otherwise flatten the feature into
    a dictionary.
  id: totrans-2172
  prefs: []
  type: TYPE_NORMAL
- en: '### `class datasets.Image`'
  id: totrans-2173
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L46)'
  id: totrans-2174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE341]'
  id: totrans-2175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE341]'
- en: Parameters
  id: totrans-2176
  prefs: []
  type: TYPE_NORMAL
- en: '`decode` (`bool`, defaults to `True`) — Whether to decode the image data. If
    `False`, returns the underlying dictionary in the format `{"path": image_path,
    "bytes": image_bytes}`.'
  id: totrans-2177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image `Feature` to read image data from an image file.
  id: totrans-2178
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: The Image feature accepts as input:'
  id: totrans-2179
  prefs: []
  type: TYPE_NORMAL
- en: 'A `str`: Absolute path to the image file (i.e. random access is allowed).'
  id: totrans-2180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `dict` with the keys:'
  id: totrans-2181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path`: String with relative path of the image file to the archive file.'
  id: totrans-2182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bytes`: Bytes of the image file.'
  id: totrans-2183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This is useful for archived files with sequential access.
  id: totrans-2184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'An `np.ndarray`: NumPy array representing an image.'
  id: totrans-2185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `PIL.Image.Image`: PIL image object.'
  id: totrans-2186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-2187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE342]'
  id: totrans-2188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE342]'
- en: '#### `cast_storage`'
  id: totrans-2189
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L201)'
  id: totrans-2190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE343]'
  id: totrans-2191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE343]'
- en: Parameters
  id: totrans-2192
  prefs: []
  type: TYPE_NORMAL
- en: '`storage` (`Union[pa.StringArray, pa.StructArray, pa.ListArray]`) — PyArrow
    array to cast.'
  id: totrans-2193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-2194
  prefs: []
  type: TYPE_NORMAL
- en: '`pa.StructArray`'
  id: totrans-2195
  prefs: []
  type: TYPE_NORMAL
- en: 'Array in the Image arrow storage type, that is `pa.struct({"bytes": pa.binary(),
    "path": pa.string()})`.'
  id: totrans-2196
  prefs: []
  type: TYPE_NORMAL
- en: 'Cast an Arrow array to the Image arrow storage type. The Arrow types that can
    be converted to the Image pyarrow storage type are:'
  id: totrans-2197
  prefs: []
  type: TYPE_NORMAL
- en: '`pa.string()` - it must contain the “path” data'
  id: totrans-2198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.binary()` - it must contain the image bytes'
  id: totrans-2199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.struct({"bytes": pa.binary()})`'
  id: totrans-2200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.struct({"path": pa.string()})`'
  id: totrans-2201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.struct({"bytes": pa.binary(), "path": pa.string()})` - order doesn’t matter'
  id: totrans-2202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.list(*)` - it must contain the image array data'
  id: totrans-2203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `decode_example`'
  id: totrans-2204
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L131)'
  id: totrans-2205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE344]'
  id: totrans-2206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE344]'
- en: Parameters
  id: totrans-2207
  prefs: []
  type: TYPE_NORMAL
- en: '`value` (`str` or `dict`) — A string with the absolute image file path, a dictionary
    with keys:'
  id: totrans-2208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path`: String with absolute or relative image file path.'
  id: totrans-2209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bytes`: The bytes of the image file.'
  id: totrans-2210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_per_repo_id` (`dict`, *optional*) — To access and decode image files
    from private repositories on the Hub, you can pass a dictionary repo_id (`str`)
    -> token (`bool` or `str`).'
  id: totrans-2211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decode example image file into image data.
  id: totrans-2212
  prefs: []
  type: TYPE_NORMAL
- en: '#### `embed_storage`'
  id: totrans-2213
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L247)'
  id: totrans-2214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE345]'
  id: totrans-2215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE345]'
- en: Parameters
  id: totrans-2216
  prefs: []
  type: TYPE_NORMAL
- en: '`storage` (`pa.StructArray`) — PyArrow array to embed.'
  id: totrans-2217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-2218
  prefs: []
  type: TYPE_NORMAL
- en: '`pa.StructArray`'
  id: totrans-2219
  prefs: []
  type: TYPE_NORMAL
- en: 'Array in the Image arrow storage type, that is `pa.struct({"bytes": pa.binary(),
    "path": pa.string()})`.'
  id: totrans-2220
  prefs: []
  type: TYPE_NORMAL
- en: Embed image files into the Arrow array.
  id: totrans-2221
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_example`'
  id: totrans-2222
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L92)'
  id: totrans-2223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE346]'
  id: totrans-2224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE346]'
- en: Parameters
  id: totrans-2225
  prefs: []
  type: TYPE_NORMAL
- en: '`value` (`str`, `np.ndarray`, `PIL.Image.Image` or `dict`) — Data passed as
    input to Image feature.'
  id: totrans-2226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encode example into a format for Arrow.
  id: totrans-2227
  prefs: []
  type: TYPE_NORMAL
- en: '#### `flatten`'
  id: totrans-2228
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L188)'
  id: totrans-2229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE347]'
  id: totrans-2230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE347]'
- en: If in the decodable state, return the feature itself, otherwise flatten the
    feature into a dictionary.
  id: totrans-2231
  prefs: []
  type: TYPE_NORMAL
- en: MetricInfo
  id: totrans-2232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class datasets.MetricInfo`'
  id: totrans-2233
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L510)'
  id: totrans-2234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE348]'
  id: totrans-2235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE348]'
- en: Information about a metric.
  id: totrans-2236
  prefs: []
  type: TYPE_NORMAL
- en: '`MetricInfo` documents a metric, including its name, version, and features.
    See the constructor arguments and properties for a full list.'
  id: totrans-2237
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Not all fields are known on construction and may be updated later.'
  id: totrans-2238
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_directory`'
  id: totrans-2239
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L566)'
  id: totrans-2240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE349]'
  id: totrans-2241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE349]'
- en: Create MetricInfo from the JSON file in `metric_info_dir`.
  id: totrans-2242
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-2243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE350]'
  id: totrans-2244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE350]'
- en: '#### `write_to_directory`'
  id: totrans-2245
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L546)'
  id: totrans-2246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE351]'
  id: totrans-2247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE351]'
- en: Write `MetricInfo` as JSON to `metric_info_dir`. Also save the license separately
    in LICENCE. If `pretty_print` is True, the JSON will be pretty-printed with the
    indent level of 4.
  id: totrans-2248
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-2249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE352]'
  id: totrans-2250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE352]'
- en: Metric
  id: totrans-2251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The base class `Metric` implements a Metric backed by one or several [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset).
  id: totrans-2252
  prefs: []
  type: TYPE_NORMAL
- en: '### `class datasets.Metric`'
  id: totrans-2253
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/metric.py#L147)'
  id: totrans-2254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE353]'
  id: totrans-2255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE353]'
- en: Parameters
  id: totrans-2256
  prefs: []
  type: TYPE_NORMAL
- en: '`config_name` (`str`) — This is used to define a hash specific to a metrics
    computation script and prevents the metric’s data to be overridden when the metric
    loading script is modified.'
  id: totrans-2257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`) — keep all predictions and references in memory.
    Not possible in distributed settings.'
  id: totrans-2258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`) — Path to a directory in which temporary prediction/references
    data will be stored. The data directory should be located on a shared file-system
    in distributed setups.'
  id: totrans-2259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_process` (`int`) — specify the total number of nodes in a distributed
    settings. This is useful to compute metrics in distributed setups (in particular
    non-additive metrics like F1).'
  id: totrans-2260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`process_id` (`int`) — specify the id of the current process in a distributed
    setup (between 0 and num_process-1) This is useful to compute metrics in distributed
    setups (in particular non-additive metrics like F1).'
  id: totrans-2261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` (`int`, optional) — If specified, this will temporarily set numpy’s
    random seed when [datasets.Metric.compute()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric.compute)
    is run.'
  id: totrans-2262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`experiment_id` (`str`) — A specific experiment id. This is used if several
    distributed evaluations share the same file system. This is useful to compute
    metrics in distributed setups (in particular non-additive metrics like F1).'
  id: totrans-2263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_concurrent_cache_files` (`int`) — Max number of concurrent metrics cache
    files (default 10000).'
  id: totrans-2264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (`Union[int, float]`) — Timeout in second for distributed setting
    synchronization.'
  id: totrans-2265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Metric is the base class and common API for all metrics.
  id: totrans-2266
  prefs: []
  type: TYPE_NORMAL
- en: Deprecated in 2.5.0
  id: totrans-2267
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the new library 🤗 Evaluate instead: [https://huggingface.co/docs/evaluate](https://huggingface.co/docs/evaluate)'
  id: totrans-2268
  prefs: []
  type: TYPE_NORMAL
- en: '#### `add`'
  id: totrans-2269
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/metric.py#L522)'
  id: totrans-2270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE354]'
  id: totrans-2271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE354]'
- en: Parameters
  id: totrans-2272
  prefs: []
  type: TYPE_NORMAL
- en: '`prediction` (list/array/tensor, optional) — Predictions.'
  id: totrans-2273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reference` (list/array/tensor, optional) — References.'
  id: totrans-2274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add one prediction and reference for the metric’s stack.
  id: totrans-2275
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-2276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE355]'
  id: totrans-2277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE355]'
- en: '#### `add_batch`'
  id: totrans-2278
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/metric.py#L475)'
  id: totrans-2279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE356]'
  id: totrans-2280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE356]'
- en: Parameters
  id: totrans-2281
  prefs: []
  type: TYPE_NORMAL
- en: '`predictions` (list/array/tensor, optional) — Predictions.'
  id: totrans-2282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`references` (list/array/tensor, optional) — References.'
  id: totrans-2283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a batch of predictions and references for the metric’s stack.
  id: totrans-2284
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-2285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE357]'
  id: totrans-2286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE357]'
- en: '#### `compute`'
  id: totrans-2287
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/metric.py#L404)'
  id: totrans-2288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE358]'
  id: totrans-2289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE358]'
- en: Parameters
  id: totrans-2290
  prefs: []
  type: TYPE_NORMAL
- en: '`predictions` (list/array/tensor, optional) — Predictions.'
  id: totrans-2291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`references` (list/array/tensor, optional) — References.'
  id: totrans-2292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (optional) — Keyword arguments that will be forwarded to the metrics
    `_compute` method (see details in the docstring).'
  id: totrans-2293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the metrics.
  id: totrans-2294
  prefs: []
  type: TYPE_NORMAL
- en: Usage of positional arguments is not allowed to prevent mistakes.
  id: totrans-2295
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-2296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE359]'
  id: totrans-2297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE359]'
- en: '#### `download_and_prepare`'
  id: totrans-2298
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/metric.py#L605)'
  id: totrans-2299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE360]'
  id: totrans-2300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE360]'
- en: Parameters
  id: totrans-2301
  prefs: []
  type: TYPE_NORMAL
- en: '`download_config` ([DownloadConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadConfig),
    optional) — Specific download configuration parameters.'
  id: totrans-2302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dl_manager` ([DownloadManager](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager),
    optional) — Specific download manager to use.'
  id: totrans-2303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloads and prepares dataset for reading.
  id: totrans-2304
  prefs: []
  type: TYPE_NORMAL
- en: Filesystems
  id: totrans-2305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class datasets.filesystems.S3FileSystem`'
  id: totrans-2306
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/filesystems/s3filesystem.py#L6)'
  id: totrans-2307
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE361]'
  id: totrans-2308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE361]'
- en: Parameters
  id: totrans-2309
  prefs: []
  type: TYPE_NORMAL
- en: '`anon` (`bool`, default to `False`) — Whether to use anonymous connection (public
    buckets only). If `False`, uses the key/secret given, or boto’s credential resolver
    (client_kwargs, environment, variables, config files, EC2 IAM server, in that
    order).'
  id: totrans-2310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`key` (`str`) — If not anonymous, use this access key ID, if specified.'
  id: totrans-2311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`secret` (`str`) — If not anonymous, use this secret access key, if specified.'
  id: totrans-2312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str`) — If not anonymous, use this security token, if specified.'
  id: totrans-2313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_ssl` (`bool`, defaults to `True`) — Whether to use SSL in connections
    to S3; may be faster without, but insecure. If `use_ssl` is also set in `client_kwargs`,
    the value set in `client_kwargs` will take priority.'
  id: totrans-2314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s3_additional_kwargs` (`dict`) — Parameters that are used when calling S3
    API methods. Typically used for things like ServerSideEncryption.'
  id: totrans-2315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`client_kwargs` (`dict`) — Parameters for the botocore client.'
  id: totrans-2316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requester_pays` (`bool`, defaults to `False`) — Whether `RequesterPays` buckets
    are supported.'
  id: totrans-2317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default_block_size` (`int`) — If given, the default block size value used
    for `open()`, if no specific value is given at all time. The built-in default
    is 5MB.'
  id: totrans-2318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default_fill_cache` (`bool`, defaults to `True`) — Whether to use cache filling
    with open by default. Refer to `S3File.open`.'
  id: totrans-2319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default_cache_type` (`str`, defaults to `bytes`) — If given, the default `cache_type`
    value used for `open()`. Set to `none` if no caching is desired. See fsspec’s
    documentation for other available `cache_type` values.'
  id: totrans-2320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`version_aware` (`bool`, defaults to `False`) — Whether to support bucket versioning.
    If enable this will require the user to have the necessary IAM permissions for
    dealing with versioned objects.'
  id: totrans-2321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_regions` (`bool`, defaults to `False`) — Whether to cache bucket regions.
    Whenever a new bucket is used, it will first find out which region it belongs
    to and then use the client for that region.'
  id: totrans-2322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`asynchronous` (`bool`, defaults to `False`) — Whether this instance is to
    be used from inside coroutines.'
  id: totrans-2323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config_kwargs` (`dict`) — Parameters passed to `botocore.client.Config`. **kwargs
    — Other parameters for core session.'
  id: totrans-2324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`session` (`aiobotocore.session.AioSession`) — Session to be used for all connections.
    This session will be used inplace of creating a new session inside S3FileSystem.
    For example: `aiobotocore.session.AioSession(profile=''test_user'')`.'
  id: totrans-2325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_instance_cache` (`bool`) — Control reuse of instances. Passed on to `fsspec`.'
  id: totrans-2326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_listings_cache` (`bool`) — Control reuse of directory listings. Passed
    on to `fsspec`.'
  id: totrans-2327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`listings_expiry_time` (`int` or `float`) — Control reuse of directory listings.
    Passed on to `fsspec`.'
  id: totrans-2328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_paths` (`int`) — Control reuse of directory listings. Passed on to `fsspec`.'
  id: totrans-2329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`datasets.filesystems.S3FileSystem` is a subclass of [`s3fs.S3FileSystem`](https://s3fs.readthedocs.io/en/latest/api.html).'
  id: totrans-2330
  prefs: []
  type: TYPE_NORMAL
- en: Users can use this class to access S3 as if it were a file system. It exposes
    a filesystem-like API (ls, cp, open, etc.) on top of S3 storage. Provide credentials
    either explicitly (`key=`, `secret=`) or with boto’s credential methods. See botocore
    documentation for more information. If no credentials are available, use `anon=True`.
  id: totrans-2331
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-2332
  prefs: []
  type: TYPE_NORMAL
- en: Listing files from public S3 bucket.
  id: totrans-2333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE362]'
  id: totrans-2334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE362]'
- en: Listing files from private S3 bucket using `aws_access_key_id` and `aws_secret_access_key`.
  id: totrans-2335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE363]'
  id: totrans-2336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE363]'
- en: Using `S3Filesystem` with `botocore.session.Session` and custom `aws_profile`.
  id: totrans-2337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE364]'
  id: totrans-2338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE364]'
- en: Loading dataset from S3 using `S3Filesystem` and [load_from_disk()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_from_disk).
  id: totrans-2339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE365]'
  id: totrans-2340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE365]'
- en: Saving dataset to S3 using `S3Filesystem` and [Dataset.save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk).
  id: totrans-2341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE366]'
  id: totrans-2342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE366]'
- en: '#### `datasets.filesystems.extract_path_from_uri`'
  id: totrans-2343
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/filesystems.py#L35)'
  id: totrans-2344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE367]'
  id: totrans-2345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE367]'
- en: Parameters
  id: totrans-2346
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset_path` (`str`) — Path (e.g. `dataset/train`) or remote uri (e.g. `s3://my-bucket/dataset/train`)
    of the dataset directory.'
  id: totrans-2347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocesses `dataset_path` and removes remote filesystem (e.g. removing `s3://`).
  id: totrans-2348
  prefs: []
  type: TYPE_NORMAL
- en: '#### `datasets.filesystems.is_remote_filesystem`'
  id: totrans-2349
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/filesystems/__init__.py#L51)'
  id: totrans-2350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE368]'
  id: totrans-2351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE368]'
- en: Parameters
  id: totrans-2352
  prefs: []
  type: TYPE_NORMAL
- en: '`fs` (`fsspec.spec.AbstractFileSystem`) — An abstract super-class for pythonic
    file-systems, e.g. `fsspec.filesystem(''file'')` or [datasets.filesystems.S3FileSystem](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.filesystems.S3FileSystem).'
  id: totrans-2353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checks if `fs` is a remote filesystem.
  id: totrans-2354
  prefs: []
  type: TYPE_NORMAL
- en: Fingerprint
  id: totrans-2355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class datasets.fingerprint.Hasher`'
  id: totrans-2356
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/fingerprint.py#L205)'
  id: totrans-2357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE369]'
  id: totrans-2358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE369]'
- en: Hasher that accepts python objects as inputs.
  id: totrans-2359
  prefs: []
  type: TYPE_NORMAL
