- en: Main classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/datasets/package_reference/main_classes](https://huggingface.co/docs/datasets/package_reference/main_classes)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: DatasetInfo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class datasets.DatasetInfo`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L92)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`description` (`str`) — A description of the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`citation` (`str`) — A BibTeX citation of the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`homepage` (`str`) — A URL to the official homepage for the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`license` (`str`) — The dataset’s license. It can be the name of the license
    or a paragraph containing the terms of the license.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — The features used to specify the dataset’s column types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`post_processed` (`PostProcessedInfo`, *optional*) — Information regarding
    the resources of a possible post-processing of a dataset. For example, it can
    contain the information of an index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`supervised_keys` (`SupervisedKeysData`, *optional*) — Specifies the input
    feature and the label for supervised learning if applicable for the dataset (legacy
    from TFDS).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`builder_name` (`str`, *optional*) — The name of the `GeneratorBasedBuilder`
    subclass used to create the dataset. Usually matched to the corresponding script
    name. It is also the snake_case version of the dataset builder class name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config_name` (`str`, *optional*) — The name of the configuration derived from
    [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`version` (`str` or [Version](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.Version),
    *optional*) — The version of the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`splits` (`dict`, *optional*) — The mapping between split name and metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`download_checksums` (`dict`, *optional*) — The mapping between the URL to
    download the dataset’s checksums and corresponding metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`download_size` (`int`, *optional*) — The size of the files to download to
    generate the dataset, in bytes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`post_processing_size` (`int`, *optional*) — Size of the dataset in bytes after
    post-processing, if any.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataset_size` (`int`, *optional*) — The combined size in bytes of the Arrow
    tables for all splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size_in_bytes` (`int`, *optional*) — The combined size in bytes of all files
    associated with the dataset (downloaded files + Arrow files).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`task_templates` (`List[TaskTemplate]`, *optional*) — The task templates to
    prepare the dataset for during training and evaluation. Each template casts the
    dataset’s [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    to standardized column names and types as detailed in `datasets.tasks`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*config_kwargs` (additional keyword arguments) — Keyword arguments to be
    passed to the [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)
    and used in the [DatasetBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information about a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '`DatasetInfo` documents datasets, including its name, version, and features.
    See the constructor arguments and properties for a full list.'
  prefs: []
  type: TYPE_NORMAL
- en: Not all fields are known on construction and may be updated later.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_directory`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L304)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset_info_dir` (`str`) — The directory containing the metadata file. This
    should be the root directory of a specific dataset version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs` (`fsspec.spec.AbstractFileSystem`, *optional*) — Instance of the remote
    filesystem used to download the files from.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.9.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`fs` was deprecated in version 2.9.0 and will be removed in 3.0.0. Please use
    `storage_options` instead, e.g. `storage_options=fs.storage_options`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`storage_options` (`dict`, *optional*) — Key/value pairs to be passed on to
    the file-system backend, if any.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.9.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Create [DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)
    from the JSON file in `dataset_info_dir`.
  prefs: []
  type: TYPE_NORMAL
- en: This function updates all the dynamically generated fields (num_examples, hash,
    time of creation,…) of the [DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo).
  prefs: []
  type: TYPE_NORMAL
- en: This will overwrite all previous metadata.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#### `write_to_directory`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L212)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset_info_dir` (`str`) — Destination directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pretty_print` (`bool`, defaults to `False`) — If `True`, the JSON will be
    pretty-printed with the indent level of 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs` (`fsspec.spec.AbstractFileSystem`, *optional*) — Instance of the remote
    filesystem used to download the files from.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.9.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`fs` was deprecated in version 2.9.0 and will be removed in 3.0.0. Please use
    `storage_options` instead, e.g. `storage_options=fs.storage_options`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`storage_options` (`dict`, *optional*) — Key/value pairs to be passed on to
    the file-system backend, if any.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.9.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Write `DatasetInfo` and license (if present) as JSON files to `dataset_info_dir`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The base class [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    implements a Dataset backed by an Apache Arrow table.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class datasets.Dataset`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L659)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: A Dataset backed by an Arrow table.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `add_column`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5602)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`name` (`str`) — Column name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`column` (`list` or `np.array`) — Column data to be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add column to Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Added in 1.7
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#### `add_item`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5849)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`item` (`dict`) — Item data to be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add item to Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Added in 1.7
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_file`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L737)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`filename` (`str`) — File name of the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info` (`DatasetInfo`, *optional*) — Dataset information, like description,
    citation, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`NamedSplit`, *optional*) — Name of the dataset split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indices_filename` (`str`, *optional*) — File names of the indices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate a Dataset backed by an Arrow table at filename.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_buffer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L777)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`buffer` (`pyarrow.Buffer`) — Arrow buffer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info` (`DatasetInfo`, *optional*) — Dataset information, like description,
    citation, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`NamedSplit`, *optional*) — Name of the dataset split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indices_buffer` (`pyarrow.Buffer`, *optional*) — Indices Arrow buffer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate a Dataset backed by an Arrow buffer.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_pandas`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L809)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`df` (`pandas.DataFrame`) — Dataframe that contains the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info` (`DatasetInfo`, *optional*) — Dataset information, like description,
    citation, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`NamedSplit`, *optional*) — Name of the dataset split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`preserve_index` (`bool`, *optional*) — Whether to store the index as an additional
    column in the resulting Dataset. The default of `None` will store the index as
    a column, except for `RangeIndex` which is stored as metadata only. Use `preserve_index=True`
    to force it to be stored as a column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert `pandas.DataFrame` to a `pyarrow.Table` to create a [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset).
  prefs: []
  type: TYPE_NORMAL
- en: The column types in the resulting Arrow Table are inferred from the dtypes of
    the `pandas.Series` in the DataFrame. In the case of non-object Series, the NumPy
    dtype is translated to its Arrow equivalent. In the case of `object`, we need
    to guess the datatype by looking at the Python objects in this Series.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that Series of the `object` dtype don’t carry enough information to
    always lead to a meaningful Arrow type. In the case that we cannot infer a type,
    e.g. because the DataFrame is of length 0 or the Series only contains `None/nan`
    objects, the type is set to `null`. This behavior can be avoided by constructing
    explicit features and passing it to this function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_dict`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L871)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`mapping` (`Mapping`) — Mapping of strings to Arrays or Python lists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info` (`DatasetInfo`, *optional*) — Dataset information, like description,
    citation, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`NamedSplit`, *optional*) — Name of the dataset split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert `dict` to a `pyarrow.Table` to create a [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_generator`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1007)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`generator` ( —`Callable`): A generator function that `yields` examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gen_kwargs(dict,` *optional*) — Keyword arguments to be passed to the `generator`
    callable. You can define a sharded dataset by passing the list of shards in `gen_kwargs`
    and setting `num_proc` greater than 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*, defaults to `None`) — Number of processes when
    downloading and generating the dataset locally. This is helpful if the dataset
    is made of multiple files. Multiprocessing is disabled by default. If `num_proc`
    is greater than one, then all list values in `gen_kwargs` must be the same length.
    These values will be split between calls to the generator. The number of shards
    will be the minimum of the shortest list in `gen_kwargs` and `num_proc`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.7.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to :`GeneratorConfig`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a Dataset from a generator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#### `data`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1739)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The Apache Arrow table backing the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#### `cache_files`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1759)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The cache files containing the Apache Arrow table backing the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '#### `num_columns`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1777)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Number of columns in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '#### `num_rows`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1792)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Number of rows in the dataset (same as [Dataset.**len**()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.__len__)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '#### `column_names`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1809)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Names of the columns in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '#### `shape`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1824)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Shape of the dataset (number of columns, number of rows).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '#### `unique`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1841)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — Column name (list all the column names with [column_names](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.column_names)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`list`'
  prefs: []
  type: TYPE_NORMAL
- en: List of unique elements in the given column.
  prefs: []
  type: TYPE_NORMAL
- en: Return a list of the unique elements in a column.
  prefs: []
  type: TYPE_NORMAL
- en: This is implemented in the low-level backend and as such, very fast.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '#### `flatten`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1947)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`new_fingerprint` (`str`, *optional*) — The new fingerprint of the dataset
    after transform. If `None`, the new fingerprint is computed using a hash of the
    previous fingerprint, and the transform arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset with flattened columns.
  prefs: []
  type: TYPE_NORMAL
- en: Flatten the table. Each column with a struct type is flattened into one column
    per struct field. Other columns are left unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '#### `cast`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1992)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features))
    — New features to cast the dataset to. The name of the fields in the features
    must match the current column names. The type of the data must also be convertible
    from one type to the other. For non-trivial conversion, e.g. `str` <-> `ClassLabel`
    you should use [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    to update the Dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, defaults to `1000`) — Number of examples per batch provided
    to cast. If `batch_size <= 0` or `batch_size == None` then provide the full dataset
    as a single batch to cast.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`bool`, defaults to `True` if caching is enabled) —
    If a cache file storing the current computation from `function` can be identified,
    use it instead of recomputing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_file_name` (`str`, *optional*, defaults to `None`) — Provide the name
    of a path for the cache file. It is used to store the results of the computation
    instead of the automatically generated cache file name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*, defaults to `None`) — Number of processes for
    multiprocessing. By default it doesn’t use multiprocessing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset with casted features.
  prefs: []
  type: TYPE_NORMAL
- en: Cast the dataset to a new set of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '#### `cast_column`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2075)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — Column name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature` (`FeatureType`) — Target feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_fingerprint` (`str`, *optional*) — The new fingerprint of the dataset
    after transform. If `None`, the new fingerprint is computed using a hash of the
    previous fingerprint, and the transform arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cast column to feature for decoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '#### `remove_columns`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2117)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, List[str]]`) — Name of the column(s) to remove.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_fingerprint` (`str`, *optional*) — The new fingerprint of the dataset
    after transform. If `None`, the new fingerprint is computed using a hash of the
    previous fingerprint, and the transform arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset object without the columns to remove.
  prefs: []
  type: TYPE_NORMAL
- en: Remove one or several column(s) in the dataset and the features associated to
    them.
  prefs: []
  type: TYPE_NORMAL
- en: You can also remove a column using [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    with `remove_columns` but the present method is in-place (doesn’t copy the data
    to a new dataset) and is thus faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '#### `rename_column`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2173)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`original_column_name` (`str`) — Name of the column to rename.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_column_name` (`str`) — New name for the column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_fingerprint` (`str`, *optional*) — The new fingerprint of the dataset
    after transform. If `None`, the new fingerprint is computed using a hash of the
    previous fingerprint, and the transform arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset with a renamed column.
  prefs: []
  type: TYPE_NORMAL
- en: Rename a column in the dataset, and move the features associated to the original
    column under the new column name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '#### `rename_columns`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2240)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column_mapping` (`Dict[str, str]`) — A mapping of columns to rename to their
    new names'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_fingerprint` (`str`, *optional*) — The new fingerprint of the dataset
    after transform. If `None`, the new fingerprint is computed using a hash of the
    previous fingerprint, and the transform arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset with renamed columns
  prefs: []
  type: TYPE_NORMAL
- en: Rename several columns in the dataset, and move the features associated to the
    original columns under the new column names.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '#### `select_columns`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2308)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, List[str]]`) — Name of the column(s) to keep.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_fingerprint` (`str`, *optional*) — The new fingerprint of the dataset
    after transform. If `None`, the new fingerprint is computed using a hash of the
    previous fingerprint, and the transform arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset object which only consists of selected columns.
  prefs: []
  type: TYPE_NORMAL
- en: Select one or several column(s) in the dataset and the features associated to
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '#### `class_encode_column`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1872)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — The name of the column to cast (list all the column names
    with [column_names](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.column_names))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`include_nulls` (`bool`, defaults to `False`) — Whether to include null values
    in the class labels. If `True`, the null values will be encoded as the `"None"`
    class label.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 1.14.2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Casts the given column as [ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)
    and updates the table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__len__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2357)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Number of rows in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__iter__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2374)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Iterate through the examples.
  prefs: []
  type: TYPE_NORMAL
- en: If a formatting is set with [Dataset.set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format)
    rows will be returned with the selected format.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `iter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2403)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size` (`int`) — size of each batch to yield.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_last_batch` (`bool`, default *False*) — Whether a last batch smaller
    than the batch_size should be dropped'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterate through the batches of size *batch_size*.
  prefs: []
  type: TYPE_NORMAL
- en: If a formatting is set with [*~datasets.Dataset.set_format*] rows will be returned
    with the selected format.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `formatted_as`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2447)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`type` (`str`, *optional*) — Output type selected in `[None, ''numpy'', ''torch'',
    ''tensorflow'', ''pandas'', ''arrow'', ''jax'']`. `None` means `**getitem**“ returns
    python objects (default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — Columns to format in the output. `None`
    means `__getitem__` returns all columns (default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_all_columns` (`bool`, defaults to `False`) — Keep un-formatted columns
    as well in the output (as python objects).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*format_kwargs` (additional keyword arguments) — Keywords arguments passed
    to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To be used in a `with` statement. Set `__getitem__` return format (type and
    columns).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_format`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2479)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`type` (`str`, *optional*) — Either output type selected in `[None, ''numpy'',
    ''torch'', ''tensorflow'', ''pandas'', ''arrow'', ''jax'']`. `None` means `__getitem__`
    returns python objects (default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — Columns to format in the output. `None`
    means `__getitem__` returns all columns (default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_all_columns` (`bool`, defaults to `False`) — Keep un-formatted columns
    as well in the output (as python objects).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*format_kwargs` (additional keyword arguments) — Keywords arguments passed
    to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set `__getitem__` return format (type and columns). The data formatting is applied
    on-the-fly. The format `type` (for example “numpy”) is used to format batches
    when using `__getitem__`. It’s also possible to use custom transforms for formatting
    using [set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform).
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to call [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    after calling `set_format`. Since `map` may add new columns, then the list of
    formatted columns
  prefs: []
  type: TYPE_NORMAL
- en: 'gets updated. In this case, if you apply `map` on a dataset to add a new column,
    then this column will be formatted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '#### `set_transform`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2587)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`transform` (`Callable`, *optional*) — User-defined formatting transform, replaces
    the format defined by [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format).
    A formatting function is a callable that takes a batch (as a `dict`) as input
    and returns a batch. This function is applied right before returning the objects
    in `__getitem__`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — Columns to format in the output. If specified,
    then the input batch of the transform only contains those columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_all_columns` (`bool`, defaults to `False`) — Keep un-formatted columns
    as well in the output (as python objects). If set to True, then the other un-formatted
    columns are kept with the output of the transform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set `__getitem__` return format using this transform. The transform is applied
    on-the-fly on batches when `__getitem__` is called. As [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format),
    this can be reset using [reset_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.reset_format).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '#### `reset_format`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2558)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Reset `__getitem__` return format to python objects and all columns.
  prefs: []
  type: TYPE_NORMAL
- en: Same as `self.set_format()`
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '#### `with_format`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2630)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`type` (`str`, *optional*) — Either output type selected in `[None, ''numpy'',
    ''torch'', ''tensorflow'', ''pandas'', ''arrow'', ''jax'']`. `None` means `__getitem__`
    returns python objects (default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — Columns to format in the output. `None`
    means `__getitem__` returns all columns (default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_all_columns` (`bool`, defaults to `False`) — Keep un-formatted columns
    as well in the output (as python objects).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*format_kwargs` (additional keyword arguments) — Keywords arguments passed
    to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set `__getitem__` return format (type and columns). The data formatting is applied
    on-the-fly. The format `type` (for example “numpy”) is used to format batches
    when using `__getitem__`.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also possible to use custom transforms for formatting using [with_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.with_transform).
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format),
    `with_format` returns a new [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '#### `with_transform`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2681)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`transform` (`Callable`, `optional`) — User-defined formatting transform, replaces
    the format defined by [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format).
    A formatting function is a callable that takes a batch (as a `dict`) as input
    and returns a batch. This function is applied right before returning the objects
    in `__getitem__`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, `optional`) — Columns to format in the output. If specified,
    then the input batch of the transform only contains those columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_all_columns` (`bool`, defaults to `False`) — Keep un-formatted columns
    as well in the output (as python objects). If set to `True`, then the other un-formatted
    columns are kept with the output of the transform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set `__getitem__` return format using this transform. The transform is applied
    on-the-fly on batches when `__getitem__` is called.
  prefs: []
  type: TYPE_NORMAL
- en: As [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format),
    this can be reset using [reset_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.reset_format).
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to [set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform),
    `with_transform` returns a new [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__getitem__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2808)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Can be used to index columns (by string names) or rows (by integer index or
    iterable of indices or bools).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `cleanup_cache_files`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2818)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`int`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of removed files.
  prefs: []
  type: TYPE_NORMAL
- en: Clean up all cache files in the dataset cache directory, excepted the currently
    used cache file if there is one.
  prefs: []
  type: TYPE_NORMAL
- en: Be careful when running this command that no other process is currently using
    other cache files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '#### `map`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2865)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`function` (`Callable`) — Function with one of the following signatures:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and
    `with_indices=False` and `with_rank=False`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any], *extra_args) -> Dict[str, Any]` if `batched=False`
    and `with_indices=True` and/or `with_rank=True` (one extra arg for each)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and
    `with_indices=False` and `with_rank=False`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List], *extra_args) -> Dict[str, List]` if `batched=True`
    and `with_indices=True` and/or `with_rank=True` (one extra arg for each)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For advanced usage, the function can also return a `pyarrow.Table`. Moreover
    if your function returns nothing (`None`), then `map` will run your function and
    return the dataset unchanged. If no function is provided, default to identity
    function: `lambda x: x`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`with_indices` (`bool`, defaults to `False`) — Provide example indices to `function`.
    Note that in this case the signature of `function` should be `def function(example,
    idx[, rank]): ...`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`with_rank` (`bool`, defaults to `False`) — Provide process rank to `function`.
    Note that in this case the signature of `function` should be `def function(example[,
    idx], rank): ...`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_columns` (`Optional[Union[str, List[str]]]`, defaults to `None`) — The
    columns to be passed into `function` as positional arguments. If `None`, a `dict`
    mapping to all formatted columns is passed as one argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batched` (`bool`, defaults to `False`) — Provide batch of examples to `function`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to `1000`) — Number of examples per
    batch provided to `function` if `batched=True`. If `batch_size <= 0` or `batch_size
    == None`, provide the full dataset as a single batch to `function`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_last_batch` (`bool`, defaults to `False`) — Whether a last batch smaller
    than the batch_size should be dropped instead of being processed by the function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`remove_columns` (`Optional[Union[str, List[str]]]`, defaults to `None`) —
    Remove a selection of columns while doing the mapping. Columns will be removed
    before updating the examples with the output of `function`, i.e. if `function`
    is adding columns with names in `remove_columns`, these columns will be kept.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the dataset in memory
    instead of writing it to a cache file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is
    enabled) — If a cache file storing the current computation from `function` can
    be identified, use it instead of recomputing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_file_name` (`str`, *optional*, defaults to `None`) — Provide the name
    of a path for the cache file. It is used to store the results of the computation
    instead of the automatically generated cache file name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` (`Optional[datasets.Features]`, defaults to `None`) — Use a specific
    Features to store the cache file instead of the automatically generated one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`disable_nullable` (`bool`, defaults to `False`) — Disallow null values in
    the table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fn_kwargs` (`Dict`, *optional*, defaults to `None`) — Keyword arguments to
    be passed to `function`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*, defaults to `None`) — Max number of processes
    when generating cache. Already cached shards are loaded sequentially.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`suffix_template` (`str`) — If `cache_file_name` is specified, then this suffix
    will be added at the end of the base name of each. Defaults to `"_{rank:05d}_of_{num_proc:05d}"`.
    For example, if `cache_file_name` is “processed.arrow”, then for `rank=1` and
    `num_proc=4`, the resulting file would be `"processed_00001_of_00004.arrow"` for
    the default suffix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_fingerprint` (`str`, *optional*, defaults to `None`) — The new fingerprint
    of the dataset after transform. If `None`, the new fingerprint is computed using
    a hash of the previous fingerprint, and the transform arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`desc` (`str`, *optional*, defaults to `None`) — Meaningful description to
    be displayed alongside with the progress bar while mapping examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a function to all the examples in the table (individually or in batches)
    and update the table. If your function returns a column that already exists, then
    it overwrites it.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify whether the function should be batched or not with the `batched`
    parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If batched is `False`, then the function takes 1 example in and should return
    1 example. An example is a dictionary, e.g. `{"text": "Hello there !"}`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If batched is `True` and `batch_size` is 1, then the function takes a batch
    of 1 example as input and can return a batch with 1 or more examples. A batch
    is a dictionary, e.g. a batch of 1 example is `{"text": ["Hello there !"]}`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If batched is `True` and `batch_size` is `n > 1`, then the function takes a
    batch of `n` examples as input and can return a batch with `n` examples, or with
    an arbitrary number of examples. Note that the last batch may have less than `n`
    examples. A batch is a dictionary, e.g. a batch of `n` examples is `{"text": ["Hello
    there !"] * n}`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '#### `filter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L3540)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`function` (`Callable`) — Callable with one of the following signatures:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any]) -> bool` if `batched=False` and `with_indices=False`
    and `with_rank=False`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any], *extra_args) -> bool` if `batched=False`
    and `with_indices=True` and/or `with_rank=True` (one extra arg for each)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List]) -> List[bool]` if `batched=True` and `with_indices=False`
    and `with_rank=False`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List], *extra_args) -> List[bool]` if `batched=True`
    and `with_indices=True` and/or `with_rank=True` (one extra arg for each)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If no function is provided, defaults to an always `True` function: `lambda
    x: True`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`with_indices` (`bool`, defaults to `False`) — Provide example indices to `function`.
    Note that in this case the signature of `function` should be `def function(example,
    idx[, rank]): ...`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`with_rank` (`bool`, defaults to `False`) — Provide process rank to `function`.
    Note that in this case the signature of `function` should be `def function(example[,
    idx], rank): ...`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_columns` (`str` or `List[str]`, *optional*) — The columns to be passed
    into `function` as positional arguments. If `None`, a `dict` mapping to all formatted
    columns is passed as one argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batched` (`bool`, defaults to `False`) — Provide batch of examples to `function`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to `1000`) — Number of examples per
    batch provided to `function` if `batched = True`. If `batched = False`, one example
    per batch is passed to `function`. If `batch_size <= 0` or `batch_size == None`,
    provide the full dataset as a single batch to `function`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the dataset in memory
    instead of writing it to a cache file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is
    enabled) — If a cache file storing the current computation from `function` can
    be identified, use it instead of recomputing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_file_name` (`str`, *optional*) — Provide the name of a path for the
    cache file. It is used to store the results of the computation instead of the
    automatically generated cache file name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fn_kwargs` (`dict`, *optional*) — Keyword arguments to be passed to `function`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*) — Number of processes for multiprocessing. By
    default it doesn’t use multiprocessing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`suffix_template` (`str`) — If `cache_file_name` is specified, then this suffix
    will be added at the end of the base name of each. For example, if `cache_file_name`
    is `"processed.arrow"`, then for `rank = 1` and `num_proc = 4`, the resulting
    file would be `"processed_00001_of_00004.arrow"` for the default suffix (default
    `_{rank:05d}_of_{num_proc:05d}`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_fingerprint` (`str`, *optional*) — The new fingerprint of the dataset
    after transform. If `None`, the new fingerprint is computed using a hash of the
    previous fingerprint, and the transform arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`desc` (`str`, *optional*, defaults to `None`) — Meaningful description to
    be displayed alongside with the progress bar while filtering examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a filter function to all the elements in the table in batches and update
    the table so that the dataset only includes examples according to the filter function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '#### `select`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L3751)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`indices` (`range`, `list`, `iterable`, `ndarray` or `Series`) — Range, list
    or 1D-array of integer indices for indexing. If the indices correspond to a contiguous
    range, the Arrow table is simply sliced. However passing a list of indices that
    are not contiguous creates indices mapping, which is much less efficient, but
    still faster than recreating an Arrow table made of the requested rows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the indices mapping in
    memory instead of writing it to a cache file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indices_cache_file_name` (`str`, *optional*, defaults to `None`) — Provide
    the name of a path for the cache file. It is used to store the indices mapping
    instead of the automatically generated cache file name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_fingerprint` (`str`, *optional*, defaults to `None`) — The new fingerprint
    of the dataset after transform. If `None`, the new fingerprint is computed using
    a hash of the previous fingerprint, and the transform arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new dataset with rows selected following the list/array of indices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '#### `sort`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4001)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, Sequence[str]]`) — Column name(s) to sort by.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reverse` (`Union[bool, Sequence[bool]]`, defaults to `False`) — If `True`,
    sort by descending order rather than ascending. If a single bool is provided,
    the value is applied to the sorting of all column names. Otherwise a list of bools
    with the same length and order as column_names must be provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kind` (`str`, *optional*) — Pandas algorithm for sorting selected in `{quicksort,
    mergesort, heapsort, stable}`, The default is `quicksort`. Note that both `stable`
    and `mergesort` use `timsort` under the covers and, in general, the actual implementation
    will vary with data type. The `mergesort` option is retained for backwards compatibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`kind` was deprecated in version 2.10.0 and will be removed in 3.0.0.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`null_placement` (`str`, defaults to `at_end`) — Put `None` values at the beginning
    if `at_start` or `first` or at the end if `at_end` or `last`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 1.14.2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the sorted indices in
    memory instead of writing it to a cache file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is
    enabled) — If a cache file storing the sorted indices can be identified, use it
    instead of recomputing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indices_cache_file_name` (`str`, *optional*, defaults to `None`) — Provide
    the name of a path for the cache file. It is used to store the sorted indices
    instead of the automatically generated cache file name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. Higher value gives smaller cache files, lower
    value consume less temporary memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_fingerprint` (`str`, *optional*, defaults to `None`) — The new fingerprint
    of the dataset after transform. If `None`, the new fingerprint is computed using
    a hash of the previous fingerprint, and the transform arguments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new dataset sorted according to a single or multiple columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '#### `shuffle`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4146)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`seed` (`int`, *optional*) — A seed to initialize the default BitGenerator
    if `generator=None`. If `None`, then fresh, unpredictable entropy will be pulled
    from the OS. If an `int` or `array_like[ints]` is passed, then it will be passed
    to SeedSequence to derive the initial BitGenerator state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`numpy.random.Generator`, *optional*) — Numpy random Generator
    to use to compute the permutation of the dataset rows. If `generator=None` (default),
    uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, default `False`) — Keep the shuffled indices in memory
    instead of writing it to a cache file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is
    enabled) — If a cache file storing the shuffled indices can be identified, use
    it instead of recomputing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indices_cache_file_name` (`str`, *optional*) — Provide the name of a path
    for the cache file. It is used to store the shuffled indices instead of the automatically
    generated cache file name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_fingerprint` (`str`, *optional*, defaults to `None`) — The new fingerprint
    of the dataset after transform. If `None`, the new fingerprint is computed using
    a hash of the previous fingerprint, and the transform arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new Dataset where the rows are shuffled.
  prefs: []
  type: TYPE_NORMAL
- en: Currently shuffling uses numpy random generators. You can either supply a NumPy
    BitGenerator to use, or a seed to initiate NumPy’s default random generator (PCG64).
  prefs: []
  type: TYPE_NORMAL
- en: Shuffling takes the list of indices `[0:len(my_dataset)]` and shuffles it to
    create an indices mapping. However as soon as your [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    has an indices mapping, the speed can become 10x slower. This is because there
    is an extra step to get the row index to read using the indices mapping, and most
    importantly, you aren’t reading contiguous chunks of data anymore. To restore
    the speed, you’d need to rewrite the entire dataset on your disk again using [Dataset.flatten_indices()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.flatten_indices),
    which removes the indices mapping.
  prefs: []
  type: TYPE_NORMAL
- en: 'This may take a lot of time depending of the size of your dataset though:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we recommend switching to an [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    and leveraging its fast approximate shuffling method [IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle).
  prefs: []
  type: TYPE_NORMAL
- en: 'It only shuffles the shards order and adds a shuffle buffer to your dataset,
    which keeps the speed of your dataset optimal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '#### `train_test_split`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4278)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`test_size` (`numpy.random.Generator`, *optional*) — Size of the test split
    If `float`, should be between `0.0` and `1.0` and represent the proportion of
    the dataset to include in the test split. If `int`, represents the absolute number
    of test samples. If `None`, the value is set to the complement of the train size.
    If `train_size` is also `None`, it will be set to `0.25`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_size` (`numpy.random.Generator`, *optional*) — Size of the train split
    If `float`, should be between `0.0` and `1.0` and represent the proportion of
    the dataset to include in the train split. If `int`, represents the absolute number
    of train samples. If `None`, the value is automatically set to the complement
    of the test size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shuffle` (`bool`, *optional*, defaults to `True`) — Whether or not to shuffle
    the data before splitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stratify_by_column` (`str`, *optional*, defaults to `None`) — The column name
    of labels to be used to perform stratified split of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` (`int`, *optional*) — A seed to initialize the default BitGenerator
    if `generator=None`. If `None`, then fresh, unpredictable entropy will be pulled
    from the OS. If an `int` or `array_like[ints]` is passed, then it will be passed
    to SeedSequence to derive the initial BitGenerator state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`numpy.random.Generator`, *optional*) — Numpy random Generator
    to use to compute the permutation of the dataset rows. If `generator=None` (default),
    uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the splits indices in
    memory instead of writing it to a cache file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is
    enabled) — If a cache file storing the splits indices can be identified, use it
    instead of recomputing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_cache_file_name` (`str`, *optional*) — Provide the name of a path for
    the cache file. It is used to store the train split indices instead of the automatically
    generated cache file name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_cache_file_name` (`str`, *optional*) — Provide the name of a path for
    the cache file. It is used to store the test split indices instead of the automatically
    generated cache file name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_new_fingerprint` (`str`, *optional*, defaults to `None`) — The new fingerprint
    of the train set after transform. If `None`, the new fingerprint is computed using
    a hash of the previous fingerprint, and the transform arguments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_new_fingerprint` (`str`, *optional*, defaults to `None`) — The new fingerprint
    of the test set after transform. If `None`, the new fingerprint is computed using
    a hash of the previous fingerprint, and the transform arguments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return a dictionary ([datasets.DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict))
    with two random train and test subsets (`train` and `test` `Dataset` splits).
    Splits are created from the dataset according to `test_size`, `train_size` and
    `shuffle`.
  prefs: []
  type: TYPE_NORMAL
- en: This method is similar to scikit-learn `train_test_split`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '#### `shard`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4561)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`num_shards` (`int`) — How many shards to split the dataset into.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index` (`int`) — Which shard to select and return. contiguous — (`bool`, defaults
    to `False`): Whether to select contiguous blocks of indices for shards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the dataset in memory
    instead of writing it to a cache file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indices_cache_file_name` (`str`, *optional*) — Provide the name of a path
    for the cache file. It is used to store the indices of each shard instead of the
    automatically generated cache file name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return the `index`-nth shard from dataset split into `num_shards` pieces.
  prefs: []
  type: TYPE_NORMAL
- en: This shards deterministically. `dset.shard(n, i)` will contain all elements
    of dset whose index mod `n = i`.
  prefs: []
  type: TYPE_NORMAL
- en: '`dset.shard(n, i, contiguous=True)` will instead split dset into contiguous
    chunks, so it can be easily concatenated back together after processing. If `n
    % i == l`, then the first `l` shards will have length `(n // i) + 1`, and the
    remaining shards will have length `(n // i)`. `datasets.concatenate([dset.shard(n,
    i, contiguous=True) for i in range(n)])` will return a dataset with the same order
    as the original.'
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to shard before using any randomizing operator (such as `shuffle`).
    It is best if the shard operator is used early in the dataset pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '#### `to_tf_dataset`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L327)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*) — Size of batches to load from the dataset.
    Defaults to `None`, which implies that the dataset won’t be batched, but the returned
    dataset can be batched later with `tf_dataset.batch(batch_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]` or `str`, *optional*) — Dataset column(s) to load in
    the `tf.data.Dataset`. Column names that are created by the `collate_fn` and that
    do not exist in the original dataset can be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shuffle(bool,` defaults to `False`) — Shuffle the dataset order when loading.
    Recommended `True` for training, `False` for validation/evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_remainder(bool,` defaults to `False`) — Drop the last incomplete batch
    when loading. Ensures that all batches yielded by the dataset will have the same
    length on the batch dimension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`collate_fn(Callable,` *optional*) — A function or callable object (such as
    a `DataCollator`) that will collate lists of samples into a batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`collate_fn_args` (`Dict`, *optional*) — An optional `dict` of keyword arguments
    to be passed to the `collate_fn`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_cols` (`List[str]` or `str`, defaults to `None`) — Dataset column(s)
    to load as labels. Note that many models compute loss internally rather than letting
    Keras do it, in which case passing the labels here is optional, as long as they’re
    in the input `columns`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefetch` (`bool`, defaults to `True`) — Whether to run the dataloader in
    a separate thread and maintain a small buffer of batches for training. Improves
    performance by allowing data to be loaded in the background while the model is
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, defaults to `0`) — Number of workers to use for loading
    the dataset. Only supported on Python versions >= 3.8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_test_batches` (`int`, defaults to `20`) — Number of batches to use to
    infer the output signature of the dataset. The higher this number, the more accurate
    the signature will be, but the longer it will take to create the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a `tf.data.Dataset` from the underlying Dataset. This `tf.data.Dataset`
    will load and collate batches from the Dataset, and is suitable for passing to
    methods like `model.fit()` or `model.predict()`. The dataset will yield `dicts`
    for both inputs and labels unless the `dict` would contain only a single key,
    in which case a raw `tf.Tensor` is yielded instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '#### `push_to_hub`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5248)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`repo_id` (`str`) — The ID of the repository to push to in the following format:
    `<user>/<dataset_name>` or `<org>/<dataset_name>`. Also accepts `<dataset_name>`,
    which will default to the namespace of the logged-in user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config_name` (`str`, defaults to “default”) — The configuration name (or subset)
    of a dataset. Defaults to “default”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`set_default` (`bool`, *optional*) — Whether to set this configuration as the
    default one. Otherwise, the default configuration is the one named “default”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`str`, *optional*) — The name of the split that will be given to that
    dataset. Defaults to `self.split`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_dir` (`str`, *optional*) — Directory name that will contain the uploaded
    data files. Defaults to the `config_name` if different from “default”, else “data”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.17.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`commit_message` (`str`, *optional*) — Message to commit while pushing. Will
    default to `"Upload dataset"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`commit_description` (`str`, *optional*) — Description of the commit that will
    be created. Additionally, description of the PR if a PR is created (`create_pr`
    is True).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.16.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`private` (`bool`, *optional*, defaults to `False`) — Whether the dataset repository
    should be set to private or not. Only affects repository creation: a repository
    that already exists will not be affected by that parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str`, *optional*) — An optional authentication token for the Hugging
    Face Hub. If no token is passed, will default to the token saved locally when
    logging in with `huggingface-cli login`. Will raise an error if no token is passed
    and the user is not logged-in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*) — Branch to push the uploaded files to. Defaults
    to the `"main"` branch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.15.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`branch` (`str`, *optional*) — The git branch on which to push the dataset.
    This defaults to the default branch as specified in your repository, which defaults
    to `"main"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.15.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`branch` was deprecated in favor of `revision` in version 2.15.0 and will be
    removed in 3.0.0.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) — Whether to create a
    PR with the uploaded files or directly commit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.15.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"500MB"`) — The
    maximum size of the dataset shards to be uploaded to the hub. If expressed as
    a string, needs to be digits followed by a unit (like `"5MB"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_shards` (`int`, *optional*) — Number of shards to write. By default, the
    number of shards depends on `max_shard_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`embed_external_files` (`bool`, defaults to `True`) — Whether to embed file
    bytes in the shards. In particular, this will do the following before the push
    for the fields of type:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    and [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image):
    remove local path information and embed file content in the Parquet files.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Pushes the dataset to the hub as a Parquet dataset. The dataset is pushed using
    HTTP requests and does not need to have neither git or git-lfs installed.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting Parquet files are self-contained by default. If your dataset contains
    [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    or [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    data, the Parquet files will store the bytes of your images or audio files. You
    can disable this by setting `embed_external_files` to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'If your dataset has multiple splits (e.g. train/validation/test):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to add a new configuration (or subset) to a dataset (e.g. if the
    dataset has multiple tasks/versions/languages):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '#### `save_to_disk`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1383)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset_path` (`str`) — Path (e.g. `dataset/train`) or remote URI (e.g. `s3://my-bucket/dataset/train`)
    of the dataset directory where the dataset will be saved to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs` (`fsspec.spec.AbstractFileSystem`, *optional*) — Instance of the remote
    filesystem where the dataset will be saved to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`fs` was deprecated in version 2.8.0 and will be removed in 3.0.0. Please use
    `storage_options` instead, e.g. `storage_options=fs.storage_options`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"500MB"`) — The
    maximum size of the dataset shards to be uploaded to the hub. If expressed as
    a string, needs to be digits followed by a unit (like `"50MB"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_shards` (`int`, *optional*) — Number of shards to write. By default the
    number of shards depends on `max_shard_size` and `num_proc`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*) — Number of processes when downloading and generating
    the dataset locally. Multiprocessing is disabled by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`storage_options` (`dict`, *optional*) — Key/value pairs to be passed on to
    the file-system backend, if any.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Saves a dataset to a dataset directory, or in a filesystem using any implementation
    of `fsspec.spec.AbstractFileSystem`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    and [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: All the Image() and Audio() data are stored in the arrow files. If you want
    to store paths or urls, please use the Value(“string”) type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '#### `load_from_disk`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1599)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset_path` (`str`) — Path (e.g. `"dataset/train"`) or remote URI (e.g.
    `"s3//my-bucket/dataset/train"`) of the dataset directory where the dataset will
    be loaded from.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs` (`fsspec.spec.AbstractFileSystem`, *optional*) — Instance of the remote
    filesystem where the dataset will be saved to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`fs` was deprecated in version 2.8.0 and will be removed in 3.0.0. Please use
    `storage_options` instead, e.g. `storage_options=fs.storage_options`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `None`) — Whether to copy the dataset
    in-memory. If `None`, the dataset will not be copied in-memory unless explicitly
    enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to nonzero. See more details
    in the [improve performance](../cache#improve-performance) section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storage_options` (`dict`, *optional*) — Key/value pairs to be passed on to
    the file-system backend, if any.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    or [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)'
  prefs: []
  type: TYPE_NORMAL
- en: If `dataset_path` is a path of a dataset directory, the dataset requested.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `dataset_path` is a path of a dataset dict directory, a `datasets.DatasetDict`
    with each split.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loads a dataset that was previously saved using `save_to_disk` from a dataset
    directory, or from a filesystem using any implementation of `fsspec.spec.AbstractFileSystem`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '#### `flatten_indices`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L3672)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the dataset in memory
    instead of writing it to a cache file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_file_name` (`str`, *optional*, default `None`) — Provide the name of
    a path for the cache file. It is used to store the results of the computation
    instead of the automatically generated cache file name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` (`Optional[datasets.Features]`, defaults to `None`) — Use a specific
    [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    to store the cache file instead of the automatically generated one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`disable_nullable` (`bool`, defaults to `False`) — Allow null values in the
    table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, optional, default `None`) — Max number of processes when
    generating cache. Already cached shards are loaded sequentially'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_fingerprint` (`str`, *optional*, defaults to `None`) — The new fingerprint
    of the dataset after transform. If `None`, the new fingerprint is computed using
    a hash of the previous fingerprint, and the transform arguments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create and cache a new Dataset by flattening the indices mapping.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `to_csv`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4727)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf` (`PathLike` or `FileOrBuffer`) — Either a path to a file or a
    BinaryIO.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*) — Size of the batch to load in memory and
    write at once. Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*) — Number of processes for multiprocessing. By
    default it doesn’t use multiprocessing. `batch_size` in this case defaults to
    `datasets.config.DEFAULT_MAX_BATCH_SIZE` but feel free to make it 5x or 10x of
    the default value if you have sufficient compute power.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*to_csv_kwargs` (additional keyword arguments) — Parameters to pass to pandas’s
    [`pandas.DataFrame.to_csv`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changed in 2.10.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, `index` defaults to `False` if not specified.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you would like to write the index, pass `index=True` and also set a name
    for the index column by passing `index_label`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`int`'
  prefs: []
  type: TYPE_NORMAL
- en: The number of characters or bytes written.
  prefs: []
  type: TYPE_NORMAL
- en: Exports the dataset to csv
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '#### `to_pandas`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4887)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`batched` (`bool`) — Set to `True` to return a generator that yields the dataset
    as batches of `batch_size` rows. Defaults to `False` (returns the whole datasets
    once).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*) — The size (number of rows) of the batches
    if `batched` is `True`. Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns the dataset as a `pandas.DataFrame`. Can also return a generator for
    large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '#### `to_dict`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4773)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`batched` (`bool`) — Set to `True` to return a generator that yields the dataset
    as batches of `batch_size` rows. Defaults to `False` (returns the whole datasets
    once).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.11.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use `.iter(batch_size=batch_size)` followed by `.to_dict()` on the individual
    batches instead.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*) — The size (number of rows) of the batches
    if `batched` is `True`. Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns the dataset as a Python dict. Can also return a generator for large
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '#### `to_json`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4842)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf` (`PathLike` or `FileOrBuffer`) — Either a path to a file or a
    BinaryIO.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*) — Size of the batch to load in memory and
    write at once. Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*) — Number of processes for multiprocessing. By
    default it doesn’t use multiprocessing. `batch_size` in this case defaults to
    `datasets.config.DEFAULT_MAX_BATCH_SIZE` but feel free to make it 5x or 10x of
    the default value if you have sufficient compute power.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*to_json_kwargs` (additional keyword arguments) — Parameters to pass to pandas’s
    [`pandas.DataFrame.to_json`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changed in 2.11.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, `index` defaults to `False` if `orient` is `"split"` or `"table"`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you would like to write the index, pass `index=True`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`int`'
  prefs: []
  type: TYPE_NORMAL
- en: The number of characters or bytes written.
  prefs: []
  type: TYPE_NORMAL
- en: Export the dataset to JSON Lines or JSON.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '#### `to_parquet`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4926)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf` (`PathLike` or `FileOrBuffer`) — Either a path to a file or a
    BinaryIO.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*) — Size of the batch to load in memory and
    write at once. Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*parquet_writer_kwargs` (additional keyword arguments) — Parameters to pass
    to PyArrow’s `pyarrow.parquet.ParquetWriter`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`int`'
  prefs: []
  type: TYPE_NORMAL
- en: The number of characters or bytes written.
  prefs: []
  type: TYPE_NORMAL
- en: Exports the dataset to parquet
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '#### `to_sql`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4957)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`name` (`str`) — Name of SQL table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`con` (`str` or `sqlite3.Connection` or `sqlalchemy.engine.Connection` or `sqlalchemy.engine.Connection`)
    — A [URI string](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls)
    or a SQLite3/SQLAlchemy connection object used to write to a database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*) — Size of the batch to load in memory and
    write at once. Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*sql_writer_kwargs` (additional keyword arguments) — Parameters to pass to
    pandas’s [`pandas.DataFrame.to_sql`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changed in 2.11.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, `index` defaults to `False` if not specified.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you would like to write the index, pass `index=True` and also set a name
    for the index column by passing `index_label`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`int`'
  prefs: []
  type: TYPE_NORMAL
- en: The number of records written.
  prefs: []
  type: TYPE_NORMAL
- en: Exports the dataset to a SQL database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '#### `to_iterable_dataset`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5049)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`num_shards` (`int`, default to `1`) — Number of shards to define when instantiating
    the iterable dataset. This is especially useful for big datasets to be able to
    shuffle properly, and also to enable fast parallel loading using a PyTorch DataLoader
    or in distributed setups for example. Shards are defined using [datasets.Dataset.shard()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.shard):
    it simply slices the data without writing anything on disk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get an [datasets.IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    from a map-style [datasets.Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset).
    This is equivalent to loading a dataset in streaming mode with [datasets.load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset),
    but much faster since the data is streamed from local files.
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to map-style datasets, iterable datasets are lazy and can only be iterated
    over (e.g. using a for loop). Since they are read sequentially in training loops,
    iterable datasets are much faster than map-style datasets. All the transformations
    applied to iterable datasets like filtering or processing are done on-the-fly
    when you start iterating over the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Still, it is possible to shuffle an iterable dataset using [datasets.IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle).
    This is a fast approximate shuffling that works best if you have multiple shards
    and if you specify a buffer size that is big enough.
  prefs: []
  type: TYPE_NORMAL
- en: To get the best speed performance, make sure your dataset doesn’t have an indices
    mapping. If this is the case, the data are not read contiguously, which can be
    slow sometimes. You can use `ds = ds.flatten_indices()` to write your dataset
    in contiguous chunks of data and have optimal speed before switching to an iterable
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'With lazy filtering and processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'With sharding to enable efficient shuffling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'With a PyTorch DataLoader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'With a PyTorch DataLoader and shuffling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: In a distributed setup like PyTorch DDP with a PyTorch DataLoader and shuffling
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'With shuffling and multiple epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'Feel free to also use `IterableDataset.set_epoch()` when using a PyTorch DataLoader
    or in distributed setups. #### `add_faiss_index`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5642)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — The column of the vectors to add to the index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_name` (`str`, *optional*) — The `index_name`/identifier of the index.
    This is the `index_name` that is used to call [get_nearest_examples()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.get_nearest_examples)
    or [search()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.search).
    By default it corresponds to `column`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`Union[int, List[int]]`, *optional*) — If positive integer, this
    is the index of the GPU to use. If negative integer, use all GPUs. If a list of
    positive integers is passed in, run only on those GPUs. By default it uses the
    CPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`string_factory` (`str`, *optional*) — This is passed to the index factory
    of Faiss to create the index. Default index class is `IndexFlat`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metric_type` (`int`, *optional*) — Type of metric. Ex: `faiss.METRIC_INNER_PRODUCT`
    or `faiss.METRIC_L2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`custom_index` (`faiss.Index`, *optional*) — Custom Faiss index that you already
    have instantiated and configured for your needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`) — Size of the batch to use while adding vectors to the
    `FaissIndex`. Default value is `1000`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.4.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`train_size` (`int`, *optional*) — If the index needs a training step, specifies
    how many vectors will be used to train the index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`faiss_verbose` (`bool`, defaults to `False`) — Enable the verbosity of the
    Faiss index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`data-type`) — The dtype of the numpy arrays that are indexed. Default
    is `np.float32`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add a dense index using Faiss for fast retrieval. By default the index is done
    over the vectors of the specified column. You can specify `device` if you want
    to run it on GPU (`device` must be the GPU index). You can find more information
    about Faiss here:'
  prefs: []
  type: TYPE_NORMAL
- en: For [string factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '#### `add_faiss_index_from_external_arrays`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5722)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`external_arrays` (`np.array`) — If you want to use arrays from outside the
    lib for the index, you can set `external_arrays`. It will use `external_arrays`
    to create the Faiss index instead of the arrays in the given `column`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The `index_name`/identifier of the index. This is the
    `index_name` that is used to call [get_nearest_examples()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.get_nearest_examples)
    or [search()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (Optional `Union[int, List[int]]`, *optional*) — If positive integer,
    this is the index of the GPU to use. If negative integer, use all GPUs. If a list
    of positive integers is passed in, run only on those GPUs. By default it uses
    the CPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`string_factory` (`str`, *optional*) — This is passed to the index factory
    of Faiss to create the index. Default index class is `IndexFlat`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metric_type` (`int`, *optional*) — Type of metric. Ex: `faiss.faiss.METRIC_INNER_PRODUCT`
    or `faiss.METRIC_L2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`custom_index` (`faiss.Index`, *optional*) — Custom Faiss index that you already
    have instantiated and configured for your needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*) — Size of the batch to use while adding vectors
    to the FaissIndex. Default value is 1000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.4.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`train_size` (`int`, *optional*) — If the index needs a training step, specifies
    how many vectors will be used to train the index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`faiss_verbose` (`bool`, defaults to False) — Enable the verbosity of the Faiss
    index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`numpy.dtype`) — The dtype of the numpy arrays that are indexed. Default
    is np.float32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add a dense index using Faiss for fast retrieval. The index is created using
    the vectors of `external_arrays`. You can specify `device` if you want to run
    it on GPU (`device` must be the GPU index). You can find more information about
    Faiss here:'
  prefs: []
  type: TYPE_NORMAL
- en: For [string factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `save_faiss_index`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L529)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The index_name/identifier of the index. This is the
    index_name that is used to call `.get_nearest` or `.search`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`file` (`str`) — The path to the serialized faiss index on disk or remote URI
    (e.g. `"s3://my-bucket/index.faiss"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storage_options` (`dict`, *optional*) — Key/value pairs to be passed on to
    the file-system backend, if any.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.11.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Save a FaissIndex on disk.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `load_faiss_index`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L547)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The index_name/identifier of the index. This is the
    index_name that is used to call `.get_nearest` or `.search`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`file` (`str`) — The path to the serialized faiss index on disk or remote URI
    (e.g. `"s3://my-bucket/index.faiss"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (Optional `Union[int, List[int]]`) — If positive integer, this is
    the index of the GPU to use. If negative integer, use all GPUs. If a list of positive
    integers is passed in, run only on those GPUs. By default it uses the CPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storage_options` (`dict`, *optional*) — Key/value pairs to be passed on to
    the file-system backend, if any.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.11.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Load a FaissIndex from disk.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to do additional configurations, you can have access to the faiss
    index object by doing `.get_index(index_name).faiss_index` to make it fit your
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `add_elasticsearch_index`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5781)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — The column of the documents to add to the index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_name` (`str`, *optional*) — The `index_name`/identifier of the index.
    This is the index name that is used to call [get_nearest_examples()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.get_nearest_examples)
    or [Dataset.search()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.search).
    By default it corresponds to `column`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host` (`str`, *optional*, defaults to `localhost`) — Host of where ElasticSearch
    is running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`port` (`str`, *optional*, defaults to `9200`) — Port of where ElasticSearch
    is running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`es_client` (`elasticsearch.Elasticsearch`, *optional*) — The elasticsearch
    client used to create the index if host and port are `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`es_index_name` (`str`, *optional*) — The elasticsearch index name used to
    create the index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`es_index_config` (`dict`, *optional*) — The configuration of the elasticsearch
    index. Default config is:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a text index using ElasticSearch for fast retrieval. This is done in-place.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '#### `load_elasticsearch_index`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L631)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The `index_name`/identifier of the index. This is the
    index name that is used to call `get_nearest` or `search`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`es_index_name` (`str`) — The name of elasticsearch index to load.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host` (`str`, *optional*, defaults to `localhost`) — Host of where ElasticSearch
    is running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`port` (`str`, *optional*, defaults to `9200`) — Port of where ElasticSearch
    is running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`es_client` (`elasticsearch.Elasticsearch`, *optional*) — The elasticsearch
    client used to create the index if host and port are `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`es_index_config` (`dict`, *optional*) — The configuration of the elasticsearch
    index. Default config is:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load an existing text index using ElasticSearch for fast retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `list_indexes`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L432)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: List the `colindex_nameumns`/identifiers of all the attached indexes.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_index`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L436)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — Index name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List the `index_name`/identifiers of all the attached indexes.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `drop_index`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L678)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The `index_name`/identifier of the index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drop the index with the specified column.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `search`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L687)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The name/identifier of the index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query` (`Union[str, np.ndarray]`) — The query as a string if `index_name`
    is a text index or as a numpy array if `index_name` is a vector index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`k` (`int`) — The number of examples to retrieve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`(scores, indices)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A tuple of `(scores, indices)` where:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scores` (`List[List[float]`): the retrieval scores from either FAISS (`IndexFlatL2`
    by default) or ElasticSearch of the retrieved examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indices` (`List[List[int]]`): the indices of the retrieved examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the nearest examples indices in the dataset to the query.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `search_batch`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L707)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The `index_name`/identifier of the index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`queries` (`Union[List[str], np.ndarray]`) — The queries as a list of strings
    if `index_name` is a text index or as a numpy array if `index_name` is a vector
    index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`k` (`int`) — The number of examples to retrieve per query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`(total_scores, total_indices)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A tuple of `(total_scores, total_indices)` where:'
  prefs: []
  type: TYPE_NORMAL
- en: '`total_scores` (`List[List[float]`): the retrieval scores from either FAISS
    (`IndexFlatL2` by default) or ElasticSearch of the retrieved examples per query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total_indices` (`List[List[int]]`): the indices of the retrieved examples
    per query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the nearest examples indices in the dataset to the query.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_nearest_examples`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L729)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The index_name/identifier of the index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query` (`Union[str, np.ndarray]`) — The query as a string if `index_name`
    is a text index or as a numpy array if `index_name` is a vector index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`k` (`int`) — The number of examples to retrieve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`(scores, examples)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A tuple of `(scores, examples)` where:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scores` (`List[float]`): the retrieval scores from either FAISS (`IndexFlatL2`
    by default) or ElasticSearch of the retrieved examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`examples` (`dict`): the retrieved examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the nearest examples in the dataset to the query.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_nearest_examples_batch`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L753)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name` (`str`) — The `index_name`/identifier of the index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`queries` (`Union[List[str], np.ndarray]`) — The queries as a list of strings
    if `index_name` is a text index or as a numpy array if `index_name` is a vector
    index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`k` (`int`) — The number of examples to retrieve per query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`(total_scores, total_examples)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A tuple of `(total_scores, total_examples)` where:'
  prefs: []
  type: TYPE_NORMAL
- en: '`total_scores` (`List[List[float]`): the retrieval scores from either FAISS
    (`IndexFlatL2` by default) or ElasticSearch of the retrieved examples per query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total_examples` (`List[dict]`): the retrieved examples per query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the nearest examples in the dataset to the query.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `info`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L159)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '[DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)
    object containing all the metadata in the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `split`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L164)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[NamedSplit](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.NamedSplit)
    object corresponding to a named dataset split.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `builder_name`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L169)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: '#### `citation`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L173)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '#### `config_name`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L177)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: '#### `dataset_size`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L181)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: '#### `description`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L185)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '#### `download_checksums`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L189)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: '#### `download_size`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L193)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: '#### `features`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L730)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: '#### `homepage`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L201)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '#### `license`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L205)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: '#### `size_in_bytes`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L209)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: '#### `supervised_keys`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L213)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: '#### `version`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L221)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_csv`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L954)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_paths` (`path-like` or list of `path-like`) — Path(s) of the CSV file(s).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` ([NamedSplit](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.NamedSplit),
    *optional*) — Split name to be assigned to the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*, defaults to `None`) — Number of processes when
    downloading and generating the dataset locally. This is helpful if the dataset
    is made of multiple files. Multiprocessing is disabled by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `pandas.read_csv`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create Dataset from CSV file(s).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_json`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1076)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_paths` (`path-like` or list of `path-like`) — Path(s) of the JSON
    or JSON Lines file(s).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` ([NamedSplit](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.NamedSplit),
    *optional*) — Split name to be assigned to the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`field` (`str`, *optional*) — Field name of the JSON file where the dataset
    is contained in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional* defaults to `None`) — Number of processes when
    downloading and generating the dataset locally. This is helpful if the dataset
    is made of multiple files. Multiprocessing is disabled by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `JsonConfig`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create Dataset from JSON or JSON Lines file(s).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_parquet`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1133)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_paths` (`path-like` or list of `path-like`) — Path(s) of the Parquet
    file(s).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`NamedSplit`, *optional*) — Split name to be assigned to the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` (`Features`, *optional*) — Dataset features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — If not `None`, only these columns will
    be read from the file. A column name may be a prefix of a nested field, e.g. ‘a’
    will select ‘a.b’, ‘a.c’, and ‘a.d.e’.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*, defaults to `None`) — Number of processes when
    downloading and generating the dataset locally. This is helpful if the dataset
    is made of multiple files. Multiprocessing is disabled by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `ParquetConfig`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create Dataset from Parquet file(s).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_text`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1192)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_paths` (`path-like` or list of `path-like`) — Path(s) of the text
    file(s).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`NamedSplit`, *optional*) — Split name to be assigned to the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` (`Features`, *optional*) — Dataset features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*, defaults to `None`) — Number of processes when
    downloading and generating the dataset locally. This is helpful if the dataset
    is made of multiple files. Multiprocessing is disabled by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `TextConfig`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create Dataset from text file(s).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_sql`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1307)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sql` (`str` or `sqlalchemy.sql.Selectable`) — SQL query to be executed or
    a table name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`con` (`str` or `sqlite3.Connection` or `sqlalchemy.engine.Connection` or `sqlalchemy.engine.Connection`)
    — A [URI string](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls)
    used to instantiate a database connection or a SQLite3/SQLAlchemy connection object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `SqlConfig`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create Dataset from SQL query or database table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: The returned dataset can only be cached if `con` is specified as URI string.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `prepare_for_task`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2729)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`task` (`Union[str, TaskTemplate]`) — The task to prepare the dataset for during
    training and evaluation. If `str`, supported tasks include:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"text-classification"`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"question-answering"`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If `TaskTemplate`, must be one of the task templates in [`datasets.tasks`](./task_templates).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`id` (`int`, defaults to `0`) — The id required to unambiguously identify the
    task template when multiple task templates of the same type are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare a dataset for the given task by casting the dataset’s [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    to standardized column names and types as detailed in [`datasets.tasks`](./task_templates).
  prefs: []
  type: TYPE_NORMAL
- en: Casts `datasets.DatasetInfo.features` according to a task-specific schema. Intended
    for single-use only, so all task templates are removed from `datasets.DatasetInfo.task_templates`
    after casting.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `align_labels_with_mapping`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5903)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`label2id` (`dict`) — The label name to ID mapping to align the dataset with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_column` (`str`) — The column name of labels to align on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Align the dataset’s label ID and label name mapping to match an input `label2id`
    mapping. This is useful when you want to ensure that a model’s predicted labels
    are aligned with the dataset. The alignment in done using the lowercase label
    names.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: '#### `datasets.concatenate_datasets`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/combine.py#L158)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dsets` (`List[datasets.Dataset]`) — List of Datasets to concatenate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info` (`DatasetInfo`, *optional*) — Dataset information, like description,
    citation, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` (`NamedSplit`, *optional*) — Name of the dataset split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`axis` (`{0, 1}`, defaults to `0`) — Axis to concatenate over, where `0` means
    over rows (vertically) and `1` means over columns (horizontally).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 1.6.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Converts a list of [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    with the same schema into a single [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: '#### `datasets.interleave_datasets`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/combine.py#L18)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`datasets` (`List[Dataset]` or `List[IterableDataset]`) — List of datasets
    to interleave.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`probabilities` (`List[float]`, *optional*, defaults to `None`) — If specified,
    the new dataset is constructed by sampling examples from one source at a time
    according to these probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` (`int`, *optional*, defaults to `None`) — The random seed used to choose
    a source for each example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info` ([DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo),
    *optional*) — Dataset information, like description, citation, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.4.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`split` ([NamedSplit](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.NamedSplit),
    *optional*) — Name of the dataset split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.4.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`stopping_strategy` (`str`, defaults to `first_exhausted`) — Two strategies
    are proposed right now, `first_exhausted` and `all_exhausted`. By default, `first_exhausted`
    is an undersampling strategy, i.e the dataset construction is stopped as soon
    as one dataset has ran out of samples. If the strategy is `all_exhausted`, we
    use an oversampling strategy, i.e the dataset construction is stopped as soon
    as every samples of every dataset has been added at least once. Note that if the
    strategy is `all_exhausted`, the interleaved dataset size can get enormous:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: with no probabilities, the resulting dataset will have `max_length_datasets*nb_dataset`
    samples.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: with given probabilities, the resulting dataset will have more samples if some
    datasets have really low probability of visiting.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    or [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)'
  prefs: []
  type: TYPE_NORMAL
- en: Return type depends on the input `datasets` parameter. `Dataset` if the input
    is a list of `Dataset`, `IterableDataset` if the input is a list of `IterableDataset`.
  prefs: []
  type: TYPE_NORMAL
- en: Interleave several datasets (sources) into a single dataset. The new dataset
    is constructed by alternating between the sources to get the examples.
  prefs: []
  type: TYPE_NORMAL
- en: You can use this function on a list of [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    objects, or on a list of [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: If `probabilities` is `None` (default) the new dataset is constructed by cycling
    between each source to get the examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `probabilities` is not `None`, the new dataset is constructed by getting
    examples from a random source at a time according to the provided probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The resulting dataset ends when one of the source datasets runs out of examples
    except when `oversampling` is `True`, in which case, the resulting dataset ends
    when all datasets have ran out of examples at least one time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note for iterable datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: In a distributed setup or in PyTorch DataLoader workers, the stopping strategy
    is applied per process. Therefore the “first_exhausted” strategy on an sharded
    iterable dataset can generate less samples in total (up to 1 missing sample per
    subdataset per worker).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For regular datasets (map-style):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: '#### `datasets.distributed.split_dataset_by_node`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/distributed.py#L10)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset` ([Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    or [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset))
    — The dataset to split by node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank` (`int`) — Rank of the current node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`world_size` (`int`) — Total number of nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    or [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset to be used on the node at rank `rank`.
  prefs: []
  type: TYPE_NORMAL
- en: Split a dataset for the node at rank `rank` in a pool of nodes of size `world_size`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For map-style datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: Each node is assigned a chunk of data, e.g. rank 0 is given the first chunk
    of the dataset. To maximize data loading throughput, chunks are made of contiguous
    data on disk if possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'For iterable datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: If the dataset has a number of shards that is a factor of `world_size` (i.e.
    if `dataset.n_shards % world_size == 0`), then the shards are evenly assigned
    across the nodes, which is the most optimized. Otherwise, each node keeps 1 example
    out of `world_size`, skipping the other examples.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `datasets.enable_caching`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/fingerprint.py#L95)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: When applying transforms on a dataset, the data are stored in cache files. The
    caching mechanism allows to reload an existing cache file if it’s already been
    computed.
  prefs: []
  type: TYPE_NORMAL
- en: Reloading a dataset is possible since the cache files are named using the dataset
    fingerprint, which is updated after each transform.
  prefs: []
  type: TYPE_NORMAL
- en: 'If disabled, the library will no longer reload cached datasets files when applying
    transforms to the datasets. More precisely, if the caching is disabled:'
  prefs: []
  type: TYPE_NORMAL
- en: cache files are always recreated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cache files are written to a temporary directory that is deleted when session
    closes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cache files are named using a random hash instead of the dataset fingerprint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use [save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk)
    to save a transformed dataset or it will be deleted when session closes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: caching doesn’t affect [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset).
    If you want to regenerate a dataset from scratch you should use the `download_mode`
    parameter in [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `datasets.disable_caching`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/fingerprint.py#L116)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: When applying transforms on a dataset, the data are stored in cache files. The
    caching mechanism allows to reload an existing cache file if it’s already been
    computed.
  prefs: []
  type: TYPE_NORMAL
- en: Reloading a dataset is possible since the cache files are named using the dataset
    fingerprint, which is updated after each transform.
  prefs: []
  type: TYPE_NORMAL
- en: 'If disabled, the library will no longer reload cached datasets files when applying
    transforms to the datasets. More precisely, if the caching is disabled:'
  prefs: []
  type: TYPE_NORMAL
- en: cache files are always recreated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cache files are written to a temporary directory that is deleted when session
    closes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cache files are named using a random hash instead of the dataset fingerprint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use [save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk)
    to save a transformed dataset or it will be deleted when session closes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: caching doesn’t affect [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset).
    If you want to regenerate a dataset from scratch you should use the `download_mode`
    parameter in [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `datasets.is_caching_enabled`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/fingerprint.py#L161)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: When applying transforms on a dataset, the data are stored in cache files. The
    caching mechanism allows to reload an existing cache file if it’s already been
    computed.
  prefs: []
  type: TYPE_NORMAL
- en: Reloading a dataset is possible since the cache files are named using the dataset
    fingerprint, which is updated after each transform.
  prefs: []
  type: TYPE_NORMAL
- en: 'If disabled, the library will no longer reload cached datasets files when applying
    transforms to the datasets. More precisely, if the caching is disabled:'
  prefs: []
  type: TYPE_NORMAL
- en: cache files are always recreated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cache files are written to a temporary directory that is deleted when session
    closes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cache files are named using a random hash instead of the dataset fingerprint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use [save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk)]
    to save a transformed dataset or it will be deleted when session closes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: caching doesn’t affect [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset).
    If you want to regenerate a dataset from scratch you should use the `download_mode`
    parameter in [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DatasetDict
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dictionary with split names as keys (‘train’, ‘test’ for example), and `Dataset`
    objects as values. It also has dataset transform methods like map or filter, to
    process all the splits at once.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class datasets.DatasetDict`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L45)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: 'A dictionary (dict of str: datasets.Dataset) with dataset transforms methods
    (map, filter, etc.)'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `data`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L86)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: The Apache Arrow tables backing each split.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: '#### `cache_files`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L101)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: The cache files containing the Apache Arrow table backing each split.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: '#### `num_columns`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L119)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: Number of columns in each split of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: '#### `num_rows`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L135)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: Number of rows in each split of the dataset (same as [datasets.Dataset.**len**()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.__len__)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: '#### `column_names`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L151)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: Names of the columns in each split of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: '#### `shape`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L169)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: Shape of each split of the dataset (number of columns, number of rows).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: '#### `unique`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L217)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — column name (list all the column names with [column_names](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.column_names))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: Dict[`str`, `list`]
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary of unique elements in the given column.
  prefs: []
  type: TYPE_NORMAL
- en: Return a list of the unique elements in a column for each split.
  prefs: []
  type: TYPE_NORMAL
- en: This is implemented in the low-level backend and as such, very fast.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: '#### `cleanup_cache_files`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L241)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: Clean up all cache files in the dataset cache directory, excepted the currently
    used cache file if there is one. Be careful when running this command that no
    other process is currently using other cache files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: '#### `map`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L764)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`function` (`callable`) — with one of the following signature:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and
    `with_indices=False`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any], indices: int) -> Dict[str, Any]` if `batched=False`
    and `with_indices=True`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and
    `with_indices=False`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List]` if
    `batched=True` and `with_indices=True`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For advanced usage, the function can also return a `pyarrow.Table`. Moreover
    if your function returns nothing (`None`), then `map` will run your function and
    return the dataset unchanged.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`with_indices` (`bool`, defaults to `False`) — Provide example indices to `function`.
    Note that in this case the signature of `function` should be `def function(example,
    idx): ...`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`with_rank` (`bool`, defaults to `False`) — Provide process rank to `function`.
    Note that in this case the signature of `function` should be `def function(example[,
    idx], rank): ...`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_columns` (`[Union[str, List[str]]]`, *optional*, defaults to `None`)
    — The columns to be passed into `function` as positional arguments. If `None`,
    a dict mapping to all formatted columns is passed as one argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batched` (`bool`, defaults to `False`) — Provide batch of examples to `function`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to `1000`) — Number of examples per
    batch provided to `function` if `batched=True`, `batch_size <= 0` or `batch_size
    == None` then provide the full dataset as a single batch to `function`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_last_batch` (`bool`, defaults to `False`) — Whether a last batch smaller
    than the batch_size should be dropped instead of being processed by the function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`remove_columns` (`[Union[str, List[str]]]`, *optional*, defaults to `None`)
    — Remove a selection of columns while doing the mapping. Columns will be removed
    before updating the examples with the output of `function`, i.e. if `function`
    is adding columns with names in `remove_columns`, these columns will be kept.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the dataset in memory
    instead of writing it to a cache file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is
    enabled) — If a cache file storing the current computation from `function` can
    be identified, use it instead of recomputing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_file_names` (`[Dict[str, str]]`, *optional*, defaults to `None`) — Provide
    the name of a path for the cache file. It is used to store the results of the
    computation instead of the automatically generated cache file name. You have to
    provide one `cache_file_name` per dataset in the dataset dictionary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, default `1000`) — Number of rows per write operation
    for the cache file writer. This value is a good trade-off between memory usage
    during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` (`[datasets.Features]`, *optional*, defaults to `None`) — Use a
    specific [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    to store the cache file instead of the automatically generated one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`disable_nullable` (`bool`, defaults to `False`) — Disallow null values in
    the table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fn_kwargs` (`Dict`, *optional*, defaults to `None`) — Keyword arguments to
    be passed to `function`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*, defaults to `None`) — Number of processes for
    multiprocessing. By default it doesn’t use multiprocessing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`desc` (`str`, *optional*, defaults to `None`) — Meaningful description to
    be displayed alongside with the progress bar while mapping examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a function to all the elements in the table (individually or in batches)
    and update the table (if function does updated examples). The transformation is
    applied to all the datasets of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: '#### `filter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L892)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`function` (`Callable`) — Callable with one of the following signatures:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any]) -> bool` if `batched=False` and `with_indices=False`
    and `with_rank=False`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any], *extra_args) -> bool` if `batched=False`
    and `with_indices=True` and/or `with_rank=True` (one extra arg for each)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List]) -> List[bool]` if `batched=True` and `with_indices=False`
    and `with_rank=False`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List], *extra_args) -> List[bool]` if `batched=True`
    and `with_indices=True` and/or `with_rank=True` (one extra arg for each)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If no function is provided, defaults to an always `True` function: `lambda
    x: True`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`with_indices` (`bool`, defaults to `False`) — Provide example indices to `function`.
    Note that in this case the signature of `function` should be `def function(example,
    idx[, rank]): ...`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`with_rank` (`bool`, defaults to `False`) — Provide process rank to `function`.
    Note that in this case the signature of `function` should be `def function(example[,
    idx], rank): ...`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_columns` (`[Union[str, List[str]]]`, *optional*, defaults to `None`)
    — The columns to be passed into `function` as positional arguments. If `None`,
    a dict mapping to all formatted columns is passed as one argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batched` (`bool`, defaults to `False`) — Provide batch of examples to `function`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to `1000`) — Number of examples per
    batch provided to `function` if `batched=True` `batch_size <= 0` or `batch_size
    == None` then provide the full dataset as a single batch to `function`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the dataset in memory
    instead of writing it to a cache file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if chaching is
    enabled) — If a cache file storing the current computation from `function` can
    be identified, use it instead of recomputing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_file_names` (`[Dict[str, str]]`, *optional*, defaults to `None`) — Provide
    the name of a path for the cache file. It is used to store the results of the
    computation instead of the automatically generated cache file name. You have to
    provide one `cache_file_name` per dataset in the dataset dictionary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fn_kwargs` (`Dict`, *optional*, defaults to `None`) — Keyword arguments to
    be passed to `function`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*, defaults to `None`) — Number of processes for
    multiprocessing. By default it doesn’t use multiprocessing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`desc` (`str`, *optional*, defaults to `None`) — Meaningful description to
    be displayed alongside with the progress bar while filtering examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a filter function to all the elements in the table in batches and update
    the table so that the dataset only includes examples according to the filter function.
    The transformation is applied to all the datasets of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: '#### `sort`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1054)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, Sequence[str]]`) — Column name(s) to sort by.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reverse` (`Union[bool, Sequence[bool]]`, defaults to `False`) — If `True`,
    sort by descending order rather than ascending. If a single bool is provided,
    the value is applied to the sorting of all column names. Otherwise a list of bools
    with the same length and order as column_names must be provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kind` (`str`, *optional*) — Pandas algorithm for sorting selected in `{quicksort,
    mergesort, heapsort, stable}`, The default is `quicksort`. Note that both `stable`
    and `mergesort` use timsort under the covers and, in general, the actual implementation
    will vary with data type. The `mergesort` option is retained for backwards compatibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`kind` was deprecated in version 2.10.0 and will be removed in 3.0.0.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`null_placement` (`str`, defaults to `at_end`) — Put `None` values at the beginning
    if `at_start` or `first` or at the end if `at_end` or `last`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the sorted indices in
    memory instead of writing it to a cache file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is
    enabled) — If a cache file storing the sorted indices can be identified, use it
    instead of recomputing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indices_cache_file_names` (`[Dict[str, str]]`, *optional*, defaults to `None`)
    — Provide the name of a path for the cache file. It is used to store the indices
    mapping instead of the automatically generated cache file name. You have to provide
    one `cache_file_name` per dataset in the dataset dictionary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. Higher value gives smaller cache files, lower
    value consume less temporary memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new dataset sorted according to a single or multiple columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: '#### `shuffle`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1132)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`seeds` (`Dict[str, int]` or `int`, *optional*) — A seed to initialize the
    default BitGenerator if `generator=None`. If `None`, then fresh, unpredictable
    entropy will be pulled from the OS. If an `int` or `array_like[ints]` is passed,
    then it will be passed to SeedSequence to derive the initial BitGenerator state.
    You can provide one `seed` per dataset in the dataset dictionary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` (`int`, *optional*) — A seed to initialize the default BitGenerator
    if `generator=None`. Alias for seeds (a `ValueError` is raised if both are provided).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generators` (`Dict[str, *optional*, np.random.Generator]`) — Numpy random
    Generator to use to compute the permutation of the dataset rows. If `generator=None`
    (default), uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).
    You have to provide one `generator` per dataset in the dataset dictionary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Keep the dataset in memory
    instead of writing it to a cache file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is
    enabled) — If a cache file storing the current computation from `function` can
    be identified, use it instead of recomputing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indices_cache_file_names` (`Dict[str, str]`, *optional*) — Provide the name
    of a path for the cache file. It is used to store the indices mappings instead
    of the automatically generated cache file name. You have to provide one `cache_file_name`
    per dataset in the dataset dictionary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writer_batch_size` (`int`, defaults to `1000`) — Number of rows per write
    operation for the cache file writer. This value is a good trade-off between memory
    usage during the processing, and processing speed. Higher value makes the processing
    do fewer lookups, lower value consume less temporary memory while running `map`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new Dataset where the rows are shuffled.
  prefs: []
  type: TYPE_NORMAL
- en: The transformation is applied to all the datasets of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: Currently shuffling uses numpy random generators. You can either supply a NumPy
    BitGenerator to use, or a seed to initiate NumPy’s default random generator (PCG64).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: '#### `set_format`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L556)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE190]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`type` (`str`, *optional*) — Output type selected in `[None, ''numpy'', ''torch'',
    ''tensorflow'', ''pandas'', ''arrow'', ''jax'']`. `None` means `__getitem__` returns
    python objects (default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — Columns to format in the output. `None`
    means `__getitem__` returns all columns (default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_all_columns` (`bool`, defaults to False) — Keep un-formatted columns
    as well in the output (as python objects),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*format_kwargs` (additional keyword arguments) — Keywords arguments passed
    to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set `__getitem__` return format (type and columns). The format is set for every
    dataset in the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to call `map` after calling `set_format`. Since `map` may add
    new columns, then the list of formatted columns gets updated. In this case, if
    you apply `map` on a dataset to add a new column, then this column will be formatted:'
  prefs: []
  type: TYPE_NORMAL
- en: '`new formatted columns = (all columns - previously unformatted columns)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: '#### `reset_format`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L602)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE192]'
  prefs: []
  type: TYPE_PRE
- en: Reset `__getitem__` return format to python objects and all columns. The transformation
    is applied to all the datasets of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: Same as `self.set_format()`
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE193]'
  prefs: []
  type: TYPE_PRE
- en: '#### `formatted_as`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L519)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE194]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`type` (`str`, *optional*) — Output type selected in `[None, ''numpy'', ''torch'',
    ''tensorflow'', ''pandas'', ''arrow'', ''jax'']`. `None` means `__getitem__` returns
    python objects (default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — Columns to format in the output. `None`
    means `__getitem__` returns all columns (default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_all_columns` (`bool`, defaults to False) — Keep un-formatted columns
    as well in the output (as python objects).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*format_kwargs` (additional keyword arguments) — Keywords arguments passed
    to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To be used in a `with` statement. Set `__getitem__` return format (type and
    columns). The transformation is applied to all the datasets of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `with_format`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L658)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE195]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`type` (`str`, *optional*) — Output type selected in `[None, ''numpy'', ''torch'',
    ''tensorflow'', ''pandas'', ''arrow'', ''jax'']`. `None` means `__getitem__` returns
    python objects (default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — Columns to format in the output. `None`
    means `__getitem__` returns all columns (default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_all_columns` (`bool`, defaults to `False`) — Keep un-formatted columns
    as well in the output (as python objects).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*format_kwargs` (additional keyword arguments) — Keywords arguments passed
    to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set `__getitem__` return format (type and columns). The data formatting is applied
    on-the-fly. The format `type` (for example “numpy”) is used to format batches
    when using `__getitem__`. The format is set for every dataset in the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also possible to use custom transforms for formatting using [with_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.with_transform).
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict.set_format),
    `with_format` returns a new [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    object with new [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE196]'
  prefs: []
  type: TYPE_PRE
- en: '#### `with_transform`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L710)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE197]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`transform` (`Callable`, *optional*) — User-defined formatting transform, replaces
    the format defined by [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format).
    A formatting function is a callable that takes a batch (as a dict) as input and
    returns a batch. This function is applied right before returning the objects in
    `__getitem__`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — Columns to format in the output. If specified,
    then the input batch of the transform only contains those columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_all_columns` (`bool`, defaults to False) — Keep un-formatted columns
    as well in the output (as python objects). If set to `True`, then the other un-formatted
    columns are kept with the output of the transform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set `__getitem__` return format using this transform. The transform is applied
    on-the-fly on batches when `__getitem__` is called. The transform is set for every
    dataset in the dataset dictionary
  prefs: []
  type: TYPE_NORMAL
- en: As [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format),
    this can be reset using [reset_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.reset_format).
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to `set_transform()`, `with_transform` returns a new [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    object with new [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE198]'
  prefs: []
  type: TYPE_PRE
- en: '#### `flatten`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L185)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE199]'
  prefs: []
  type: TYPE_PRE
- en: Flatten the Apache Arrow Table of each split (nested features are flatten).
    Each column with a struct type is flattened into one column per struct field.
    Other columns are left unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE200]'
  prefs: []
  type: TYPE_PRE
- en: '#### `cast`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L265)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE201]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features))
    — New features to cast the dataset to. The name and order of the fields in the
    features must match the current column names. The type of the data must also be
    convertible from one type to the other. For non-trivial conversion, e.g. `string`
    <-> `ClassLabel` you should use [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    to update the Dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cast the dataset to a new set of features. The transformation is applied to
    all the datasets of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: You can also remove a column using [Dataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    with `feature` but `cast` is in-place (doesn’t copy the data to a new dataset)
    and is thus faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE202]'
  prefs: []
  type: TYPE_PRE
- en: '#### `cast_column`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L300)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE203]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — Column name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature` (`Feature`) — Target feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cast column to feature for decoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE204]'
  prefs: []
  type: TYPE_PRE
- en: '#### `remove_columns`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L329)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE205]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, List[str]]`) — Name of the column(s) to remove.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove one or several column(s) from each split in the dataset and the features
    associated to the column(s).
  prefs: []
  type: TYPE_NORMAL
- en: The transformation is applied to all the splits of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: You can also remove a column using [Dataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    with `remove_columns` but the present method is in-place (doesn’t copy the data
    to a new dataset) and is thus faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE206]'
  prefs: []
  type: TYPE_PRE
- en: '#### `rename_column`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L368)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE207]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`original_column_name` (`str`) — Name of the column to rename.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_column_name` (`str`) — New name for the column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rename a column in the dataset and move the features associated to the original
    column under the new column name. The transformation is applied to all the datasets
    of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also rename a column using [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    with `remove_columns` but the present method:'
  prefs: []
  type: TYPE_NORMAL
- en: takes care of moving the original features under the new column name.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: doesn’t copy the data to a new dataset and is thus much faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE208]'
  prefs: []
  type: TYPE_PRE
- en: '#### `rename_columns`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L413)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE209]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column_mapping` (`Dict[str, str]`) — A mapping of columns to rename to their
    new names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset with renamed columns.
  prefs: []
  type: TYPE_NORMAL
- en: Rename several columns in the dataset, and move the features associated to the
    original columns under the new column names. The transformation is applied to
    all the datasets of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE210]'
  prefs: []
  type: TYPE_PRE
- en: '#### `select_columns`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L451)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE211]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, List[str]]`) — Name of the column(s) to keep.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select one or several column(s) from each split in the dataset and the features
    associated to the column(s).
  prefs: []
  type: TYPE_NORMAL
- en: The transformation is applied to all the splits of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE212]'
  prefs: []
  type: TYPE_PRE
- en: '#### `class_encode_column`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L487)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE213]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — The name of the column to cast.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`include_nulls` (`bool`, defaults to `False`) — Whether to include null values
    in the class labels. If `True`, the null values will be encoded as the `"None"`
    class label.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 1.14.2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Casts the given column as [ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)
    and updates the tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE214]'
  prefs: []
  type: TYPE_PRE
- en: '#### `push_to_hub`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1564)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE215]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`repo_id` (`str`) — The ID of the repository to push to in the following format:
    `<user>/<dataset_name>` or `<org>/<dataset_name>`. Also accepts `<dataset_name>`,
    which will default to the namespace of the logged-in user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config_name` (`str`) — Configuration name of a dataset. Defaults to “default”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`set_default` (`bool`, *optional*) — Whether to set this configuration as the
    default one. Otherwise, the default configuration is the one named “default”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_dir` (`str`, *optional*) — Directory name that will contain the uploaded
    data files. Defaults to the `config_name` if different from “default”, else “data”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.17.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`commit_message` (`str`, *optional*) — Message to commit while pushing. Will
    default to `"Upload dataset"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`commit_description` (`str`, *optional*) — Description of the commit that will
    be created. Additionally, description of the PR if a PR is created (`create_pr`
    is True).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.16.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`private` (`bool`, *optional*) — Whether the dataset repository should be set
    to private or not. Only affects repository creation: a repository that already
    exists will not be affected by that parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str`, *optional*) — An optional authentication token for the Hugging
    Face Hub. If no token is passed, will default to the token saved locally when
    logging in with `huggingface-cli login`. Will raise an error if no token is passed
    and the user is not logged-in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*) — Branch to push the uploaded files to. Defaults
    to the `"main"` branch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.15.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`branch` (`str`, *optional*) — The git branch on which to push the dataset.
    This defaults to the default branch as specified in your repository, which defaults
    to `"main"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.15.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`branch` was deprecated in favor of `revision` in version 2.15.0 and will be
    removed in 3.0.0.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) — Whether to create a
    PR with the uploaded files or directly commit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.15.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"500MB"`) — The
    maximum size of the dataset shards to be uploaded to the hub. If expressed as
    a string, needs to be digits followed by a unit (like `"500MB"` or `"1GB"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_shards` (`Dict[str, int]`, *optional*) — Number of shards to write. By
    default, the number of shards depends on `max_shard_size`. Use a dictionary to
    define a different num_shards for each split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`embed_external_files` (`bool`, defaults to `True`) — Whether to embed file
    bytes in the shards. In particular, this will do the following before the push
    for the fields of type:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    and [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    removes local path information and embed file content in the Parquet files.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Pushes the [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    to the hub as a Parquet dataset. The [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    is pushed using HTTP requests and does not need to have neither git or git-lfs
    installed.
  prefs: []
  type: TYPE_NORMAL
- en: Each dataset split will be pushed independently. The pushed dataset will keep
    the original split names.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting Parquet files are self-contained by default: if your dataset
    contains [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    or [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    data, the Parquet files will store the bytes of your images or audio files. You
    can disable this by setting `embed_external_files` to False.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE216]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to add a new configuration (or subset) to a dataset (e.g. if the
    dataset has multiple tasks/versions/languages):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE217]'
  prefs: []
  type: TYPE_PRE
- en: '#### `save_to_disk`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1215)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE218]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset_dict_path` (`str`) — Path (e.g. `dataset/train`) or remote URI (e.g.
    `s3://my-bucket/dataset/train`) of the dataset dict directory where the dataset
    dict will be saved to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs` (`fsspec.spec.AbstractFileSystem`, *optional*) — Instance of the remote
    filesystem where the dataset will be saved to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`fs` was deprecated in version 2.8.0 and will be removed in 3.0.0. Please use
    `storage_options` instead, e.g. `storage_options=fs.storage_options`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"500MB"`) — The
    maximum size of the dataset shards to be uploaded to the hub. If expressed as
    a string, needs to be digits followed by a unit (like `"50MB"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_shards` (`Dict[str, int]`, *optional*) — Number of shards to write. By
    default the number of shards depends on `max_shard_size` and `num_proc`. You need
    to provide the number of shards for each dataset in the dataset dictionary. Use
    a dictionary to define a different num_shards for each split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`num_proc` (`int`, *optional*, default `None`) — Number of processes when downloading
    and generating the dataset locally. Multiprocessing is disabled by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`storage_options` (`dict`, *optional*) — Key/value pairs to be passed on to
    the file-system backend, if any.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Saves a dataset dict to a filesystem using `fsspec.spec.AbstractFileSystem`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    and [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: All the Image() and Audio() data are stored in the arrow files. If you want
    to store paths or urls, please use the Value(“string”) type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE219]'
  prefs: []
  type: TYPE_PRE
- en: '#### `load_from_disk`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1305)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE220]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset_dict_path` (`str`) — Path (e.g. `"dataset/train"`) or remote URI (e.g.
    `"s3//my-bucket/dataset/train"`) of the dataset dict directory where the dataset
    dict will be loaded from.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs` (`fsspec.spec.AbstractFileSystem`, *optional*) — Instance of the remote
    filesystem where the dataset will be saved to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`fs` was deprecated in version 2.8.0 and will be removed in 3.0.0. Please use
    `storage_options` instead, e.g. `storage_options=fs.storage_options`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `None`) — Whether to copy the dataset
    in-memory. If `None`, the dataset will not be copied in-memory unless explicitly
    enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to nonzero. See more details
    in the [improve performance](../cache#improve-performance) section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storage_options` (`dict`, *optional*) — Key/value pairs to be passed on to
    the file-system backend, if any.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added in 2.8.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Load a dataset that was previously saved using `save_to_disk` from a filesystem
    using `fsspec.spec.AbstractFileSystem`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE221]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_csv`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1382)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE222]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_paths` (`dict` of path-like) — Path(s) of the CSV file(s).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (str, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `pandas.read_csv`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    from CSV file(s).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE223]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_json`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1421)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE224]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_paths` (`path-like` or list of `path-like`) — Path(s) of the JSON
    Lines file(s).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (str, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `JsonConfig`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    from JSON Lines file(s).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE225]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_parquet`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1460)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE226]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_paths` (`dict` of path-like) — Path(s) of the CSV file(s).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` (`List[str]`, *optional*) — If not `None`, only these columns will
    be read from the file. A column name may be a prefix of a nested field, e.g. ‘a’
    will select ‘a.b’, ‘a.c’, and ‘a.d.e’.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `ParquetConfig`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    from Parquet file(s).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE227]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_text`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1509)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE228]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_paths` (`dict` of path-like) — Path(s) of the text file(s).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features),
    *optional*) — Dataset features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*, defaults to `"~/.cache/huggingface/datasets"`)
    — Directory to cache data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`, defaults to `False`) — Whether to copy the data in-memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (additional keyword arguments) — Keyword arguments to be passed
    to `TextConfig`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create [DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)
    from text file(s).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE229]'
  prefs: []
  type: TYPE_PRE
- en: '#### `prepare_for_task`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1548)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE230]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`task` (`Union[str, TaskTemplate]`) — The task to prepare the dataset for during
    training and evaluation. If `str`, supported tasks include:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"text-classification"`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"question-answering"`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If `TaskTemplate`, must be one of the task templates in [`datasets.tasks`](./task_templates).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`id` (`int`, defaults to `0`) — The id required to unambiguously identify the
    task template when multiple task templates of the same type are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare a dataset for the given task by casting the dataset’s [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    to standardized column names and types as detailed in [`datasets.tasks`](./task_templates).
  prefs: []
  type: TYPE_NORMAL
- en: Casts `datasets.DatasetInfo.features` according to a task-specific schema. Intended
    for single-use only, so all task templates are removed from `datasets.DatasetInfo.task_templates`
    after casting.
  prefs: []
  type: TYPE_NORMAL
- en: IterableDataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The base class [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    implements an iterable Dataset backed by python generators.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class datasets.IterableDataset`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1191)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE231]'
  prefs: []
  type: TYPE_PRE
- en: A Dataset backed by an iterable.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_generator`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1439)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE232]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`generator` (`Callable`) — A generator function that `yields` examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` (`Features`, *optional*) — Dataset features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gen_kwargs(dict,` *optional*) — Keyword arguments to be passed to the `generator`
    callable. You can define a sharded iterable dataset by passing the list of shards
    in `gen_kwargs`. This can be used to improve shuffling and when iterating over
    the dataset with multiple workers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`IterableDataset`'
  prefs: []
  type: TYPE_NORMAL
- en: Create an Iterable Dataset from a generator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE233]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE234]'
  prefs: []
  type: TYPE_PRE
- en: '#### `remove_columns`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1999)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE235]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, List[str]]`) — Name of the column(s) to remove.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`IterableDataset`'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset object without the columns to remove.
  prefs: []
  type: TYPE_NORMAL
- en: Remove one or several column(s) in the dataset and the features associated to
    them. The removal is done on-the-fly on the examples when iterating over the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE236]'
  prefs: []
  type: TYPE_PRE
- en: '#### `select_columns`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L2039)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE237]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, List[str]]`) — Name of the column(s) to select.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`IterableDataset`'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset object with selected columns.
  prefs: []
  type: TYPE_NORMAL
- en: Select one or several column(s) in the dataset and the features associated to
    them. The selection is done on-the-fly on the examples when iterating over the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE238]'
  prefs: []
  type: TYPE_PRE
- en: '#### `cast_column`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L2095)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE239]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — Column name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature` (`Feature`) — Target feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`IterableDataset`'
  prefs: []
  type: TYPE_NORMAL
- en: Cast column to feature for decoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE240]'
  prefs: []
  type: TYPE_PRE
- en: '#### `cast`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L2146)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE241]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features))
    — New features to cast the dataset to. The name of the fields in the features
    must match the current column names. The type of the data must also be convertible
    from one type to the other. For non-trivial conversion, e.g. `string` <-> `ClassLabel`
    you should use [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)
    to update the Dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`IterableDataset`'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset with casted features.
  prefs: []
  type: TYPE_NORMAL
- en: Cast the dataset to a new set of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE242]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__iter__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1360)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE243]'
  prefs: []
  type: TYPE_PRE
- en: '#### `iter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1397)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE244]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size` (`int`) — size of each batch to yield.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_last_batch` (`bool`, default *False*) — Whether a last batch smaller
    than the batch_size should be dropped'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterate through the batches of size *batch_size*.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `map`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1580)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE245]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`function` (`Callable`, *optional*, defaults to `None`) — Function applied
    on-the-fly on the examples when you iterate on the dataset. It must have one of
    the following signatures:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and
    `with_indices=False`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any], idx: int) -> Dict[str, Any]` if `batched=False`
    and `with_indices=True`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and
    `with_indices=False`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List]` if
    `batched=True` and `with_indices=True`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For advanced usage, the function can also return a `pyarrow.Table`. Moreover
    if your function returns nothing (`None`), then `map` will run your function and
    return the dataset unchanged. If no function is provided, default to identity
    function: `lambda x: x`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`with_indices` (`bool`, defaults to `False`) — Provide example indices to `function`.
    Note that in this case the signature of `function` should be `def function(example,
    idx[, rank]): ...`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_columns` (`Optional[Union[str, List[str]]]`, defaults to `None`) — The
    columns to be passed into `function` as positional arguments. If `None`, a dict
    mapping to all formatted columns is passed as one argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batched` (`bool`, defaults to `False`) — Provide batch of examples to `function`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to `1000`) — Number of examples per
    batch provided to `function` if `batched=True`. `batch_size <= 0` or `batch_size
    == None` then provide the full dataset as a single batch to `function`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_last_batch` (`bool`, defaults to `False`) — Whether a last batch smaller
    than the batch_size should be dropped instead of being processed by the function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`remove_columns` (`[List[str]]`, *optional*, defaults to `None`) — Remove a
    selection of columns while doing the mapping. Columns will be removed before updating
    the examples with the output of `function`, i.e. if `function` is adding columns
    with names in `remove_columns`, these columns will be kept.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features` (`[Features]`, *optional*, defaults to `None`) — Feature types of
    the resulting dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fn_kwargs` (`Dict`, *optional*, default `None`) — Keyword arguments to be
    passed to `function`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a function to all the examples in the iterable dataset (individually or
    in batches) and update them. If your function returns a column that already exists,
    then it overwrites it. The function is applied on-the-fly on the examples when
    iterating over the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify whether the function should be batched or not with the `batched`
    parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If batched is `False`, then the function takes 1 example in and should return
    1 example. An example is a dictionary, e.g. `{"text": "Hello there !"}`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If batched is `True` and `batch_size` is 1, then the function takes a batch
    of 1 example as input and can return a batch with 1 or more examples. A batch
    is a dictionary, e.g. a batch of 1 example is {“text”: [“Hello there !”]}.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If batched is `True` and `batch_size` is `n` > 1, then the function takes a
    batch of `n` examples as input and can return a batch with `n` examples, or with
    an arbitrary number of examples. Note that the last batch may have less than `n`
    examples. A batch is a dictionary, e.g. a batch of `n` examples is `{"text": ["Hello
    there !"] * n}`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE246]'
  prefs: []
  type: TYPE_PRE
- en: '#### `rename_column`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1939)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE247]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`original_column_name` (`str`) — Name of the column to rename.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_column_name` (`str`) — New name for the column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`IterableDataset`'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset with a renamed column.
  prefs: []
  type: TYPE_NORMAL
- en: Rename a column in the dataset, and move the features associated to the original
    column under the new column name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE248]'
  prefs: []
  type: TYPE_PRE
- en: '#### `filter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1693)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE249]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`function` (`Callable`) — Callable with one of the following signatures:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any]) -> bool` if `with_indices=False, batched=False`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any], indices: int) -> bool` if `with_indices=True,
    batched=False`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, List]) -> List[bool]` if `with_indices=False,
    batched=True`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, List], indices: List[int]) -> List[bool]` if `with_indices=True,
    batched=True`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If no function is provided, defaults to an always True function: `lambda x:
    True`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`with_indices` (`bool`, defaults to `False`) — Provide example indices to `function`.
    Note that in this case the signature of `function` should be `def function(example,
    idx): ...`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_columns` (`str` or `List[str]`, *optional*) — The columns to be passed
    into `function` as positional arguments. If `None`, a dict mapping to all formatted
    columns is passed as one argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batched` (`bool`, defaults to `False`) — Provide batch of examples to `function`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, default `1000`) — Number of examples per batch
    provided to `function` if `batched=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fn_kwargs` (`Dict`, *optional*, default `None`) — Keyword arguments to be
    passed to `function`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a filter function to all the elements so that the dataset only includes
    examples according to the filter function. The filtering is done on-the-fly when
    iterating over the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE250]'
  prefs: []
  type: TYPE_PRE
- en: '#### `shuffle`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1771)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE251]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`seed` (`int`, *optional*, defaults to `None`) — Random seed that will be used
    to shuffle the dataset. It is used to sample from the shuffle buffer and also
    to shuffle the data shards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`numpy.random.Generator`, *optional*) — Numpy random Generator
    to use to compute the permutation of the dataset rows. If `generator=None` (default),
    uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`buffer_size` (`int`, defaults to `1000`) — Size of the buffer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly shuffles the elements of this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset fills a buffer with `buffer_size` elements, then randomly samples
    elements from this buffer, replacing the selected elements with new elements.
    For perfect shuffling, a buffer size greater than or equal to the full size of
    the dataset is required.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if your dataset contains 10,000 elements but `buffer_size` is
    set to 1000, then `shuffle` will initially select a random element from only the
    first 1000 elements in the buffer. Once an element is selected, its space in the
    buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1000 element
    buffer.
  prefs: []
  type: TYPE_NORMAL
- en: If the dataset is made of several shards, it also does shuffle the order of
    the shards. However if the order has been fixed by using [skip()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.skip)
    or [take()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.take)
    then the order of the shards is kept unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE252]'
  prefs: []
  type: TYPE_PRE
- en: '#### `skip`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1841)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE253]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`n` (`int`) — Number of elements to skip.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    that skips the first `n` elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE254]'
  prefs: []
  type: TYPE_PRE
- en: '#### `take`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1880)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE255]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`n` (`int`) — Number of elements to take.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)
    with only the first `n` elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE256]'
  prefs: []
  type: TYPE_PRE
- en: '#### `info`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L159)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE257]'
  prefs: []
  type: TYPE_PRE
- en: '[DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)
    object containing all the metadata in the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `split`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L164)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE258]'
  prefs: []
  type: TYPE_PRE
- en: '[NamedSplit](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.NamedSplit)
    object corresponding to a named dataset split.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `builder_name`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L169)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE259]'
  prefs: []
  type: TYPE_PRE
- en: '#### `citation`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L173)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE260]'
  prefs: []
  type: TYPE_PRE
- en: '#### `config_name`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L177)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE261]'
  prefs: []
  type: TYPE_PRE
- en: '#### `dataset_size`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L181)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE262]'
  prefs: []
  type: TYPE_PRE
- en: '#### `description`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L185)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE263]'
  prefs: []
  type: TYPE_PRE
- en: '#### `download_checksums`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L189)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE264]'
  prefs: []
  type: TYPE_PRE
- en: '#### `download_size`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L193)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE265]'
  prefs: []
  type: TYPE_PRE
- en: '#### `features`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L197)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE266]'
  prefs: []
  type: TYPE_PRE
- en: '#### `homepage`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L201)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE267]'
  prefs: []
  type: TYPE_PRE
- en: '#### `license`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L205)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE268]'
  prefs: []
  type: TYPE_PRE
- en: '#### `size_in_bytes`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L209)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE269]'
  prefs: []
  type: TYPE_PRE
- en: '#### `supervised_keys`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L213)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE270]'
  prefs: []
  type: TYPE_PRE
- en: '#### `version`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L221)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE271]'
  prefs: []
  type: TYPE_PRE
- en: IterableDatasetDict
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dictionary with split names as keys (‘train’, ‘test’ for example), and `IterableDataset`
    objects as values.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class datasets.IterableDatasetDict`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1864)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE272]'
  prefs: []
  type: TYPE_PRE
- en: '#### `map`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1899)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE273]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`function` (`Callable`, *optional*, defaults to `None`) — Function applied
    on-the-fly on the examples when you iterate on the dataset. It must have one of
    the following signatures:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and
    `with_indices=False`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any], idx: int) -> Dict[str, Any]` if `batched=False`
    and `with_indices=True`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and
    `with_indices=False`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List]` if
    `batched=True` and `with_indices=True`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For advanced usage, the function can also return a `pyarrow.Table`. Moreover
    if your function returns nothing (`None`), then `map` will run your function and
    return the dataset unchanged. If no function is provided, default to identity
    function: `lambda x: x`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`with_indices` (`bool`, defaults to `False`) — Provide example indices to `function`.
    Note that in this case the signature of `function` should be `def function(example,
    idx[, rank]): ...`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_columns` (`[Union[str, List[str]]]`, *optional*, defaults to `None`)
    — The columns to be passed into `function` as positional arguments. If `None`,
    a dict mapping to all formatted columns is passed as one argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batched` (`bool`, defaults to `False`) — Provide batch of examples to `function`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to `1000`) — Number of examples per
    batch provided to `function` if `batched=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_last_batch` (`bool`, defaults to `False`) — Whether a last batch smaller
    than the `batch_size` should be dropped instead of being processed by the function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`remove_columns` (`[List[str]]`, *optional*, defaults to `None`) — Remove a
    selection of columns while doing the mapping. Columns will be removed before updating
    the examples with the output of `function`, i.e. if `function` is adding columns
    with names in `remove_columns`, these columns will be kept.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fn_kwargs` (`Dict`, *optional*, defaults to `None`) — Keyword arguments to
    be passed to `function`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a function to all the examples in the iterable dataset (individually or
    in batches) and update them. If your function returns a column that already exists,
    then it overwrites it. The function is applied on-the-fly on the examples when
    iterating over the dataset. The transformation is applied to all the datasets
    of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify whether the function should be batched or not with the `batched`
    parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If batched is `False`, then the function takes 1 example in and should return
    1 example. An example is a dictionary, e.g. `{"text": "Hello there !"}`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If batched is `True` and `batch_size` is 1, then the function takes a batch
    of 1 example as input and can return a batch with 1 or more examples. A batch
    is a dictionary, e.g. a batch of 1 example is `{"text": ["Hello there !"]}`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If batched is `True` and `batch_size` is `n` > 1, then the function takes a
    batch of `n` examples as input and can return a batch with `n` examples, or with
    an arbitrary number of examples. Note that the last batch may have less than `n`
    examples. A batch is a dictionary, e.g. a batch of `n` examples is `{"text": ["Hello
    there !"] * n}`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE274]'
  prefs: []
  type: TYPE_PRE
- en: '#### `filter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1988)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE275]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`function` (`Callable`) — Callable with one of the following signatures:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any]) -> bool` if `with_indices=False, batched=False`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, Any], indices: int) -> bool` if `with_indices=True,
    batched=False`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, List]) -> List[bool]` if `with_indices=False,
    batched=True`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function(example: Dict[str, List], indices: List[int]) -> List[bool]` if `with_indices=True,
    batched=True`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If no function is provided, defaults to an always True function: `lambda x:
    True`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`with_indices` (`bool`, defaults to `False`) — Provide example indices to `function`.
    Note that in this case the signature of `function` should be `def function(example,
    idx): ...`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_columns` (`str` or `List[str]`, *optional*) — The columns to be passed
    into `function` as positional arguments. If `None`, a dict mapping to all formatted
    columns is passed as one argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batched` (`bool`, defaults to `False`) — Provide batch of examples to `function`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to `1000`) — Number of examples per
    batch provided to `function` if `batched=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fn_kwargs` (`Dict`, *optional*, defaults to `None`) — Keyword arguments to
    be passed to `function`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a filter function to all the elements so that the dataset only includes
    examples according to the filter function. The filtering is done on-the-fly when
    iterating over the dataset. The filtering is applied to all the datasets of the
    dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE276]'
  prefs: []
  type: TYPE_PRE
- en: '#### `shuffle`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2051)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE277]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`seed` (`int`, *optional*, defaults to `None`) — Random seed that will be used
    to shuffle the dataset. It is used to sample from the shuffle buffer and also
    to shuffle the data shards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`numpy.random.Generator`, *optional*) — Numpy random Generator
    to use to compute the permutation of the dataset rows. If `generator=None` (default),
    uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`buffer_size` (`int`, defaults to `1000`) — Size of the buffer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly shuffles the elements of this dataset. The shuffling is applied to
    all the datasets of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset fills a buffer with buffer_size elements, then randomly samples
    elements from this buffer, replacing the selected elements with new elements.
    For perfect shuffling, a buffer size greater than or equal to the full size of
    the dataset is required.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if your dataset contains 10,000 elements but `buffer_size` is
    set to 1000, then `shuffle` will initially select a random element from only the
    first 1000 elements in the buffer. Once an element is selected, its space in the
    buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1000 element
    buffer.
  prefs: []
  type: TYPE_NORMAL
- en: If the dataset is made of several shards, it also does `shuffle` the order of
    the shards. However if the order has been fixed by using [skip()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.skip)
    or [take()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.take)
    then the order of the shards is kept unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE278]'
  prefs: []
  type: TYPE_PRE
- en: '#### `with_format`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1870)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE279]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`type` (`str`, *optional*, defaults to `None`) — If set to “torch”, the returned
    dataset will be a subclass of `torch.utils.data.IterableDataset` to be used in
    a `DataLoader`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return a dataset with the specified format. This method only supports the “torch”
    format for now. The format is set to all the datasets of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE280]'
  prefs: []
  type: TYPE_PRE
- en: '#### `cast`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2253)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE281]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`features` (`Features`) — New features to cast the dataset to. The name of
    the fields in the features must match the current column names. The type of the
    data must also be convertible from one type to the other. For non-trivial conversion,
    e.g. `string` <-> `ClassLabel` you should use `map` to update the Dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[IterableDatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDatasetDict)'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset with casted features.
  prefs: []
  type: TYPE_NORMAL
- en: Cast the dataset to a new set of features. The type casting is applied to all
    the datasets of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE282]'
  prefs: []
  type: TYPE_PRE
- en: '#### `cast_column`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2222)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE283]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`str`) — Column name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature` (`Feature`) — Target feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cast column to feature for decoding. The type casting is applied to all the
    datasets of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE284]'
  prefs: []
  type: TYPE_PRE
- en: '#### `remove_columns`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2170)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE285]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, List[str]]`) — Name of the column(s) to remove.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[IterableDatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDatasetDict)'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset object without the columns to remove.
  prefs: []
  type: TYPE_NORMAL
- en: Remove one or several column(s) in the dataset and the features associated to
    them. The removal is done on-the-fly on the examples when iterating over the dataset.
    The removal is applied to all the datasets of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE286]'
  prefs: []
  type: TYPE_PRE
- en: '#### `rename_column`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2109)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE287]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`original_column_name` (`str`) — Name of the column to rename.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new_column_name` (`str`) — New name for the column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[IterableDatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDatasetDict)'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset with a renamed column.
  prefs: []
  type: TYPE_NORMAL
- en: Rename a column in the dataset, and move the features associated to the original
    column under the new column name. The renaming is applied to all the datasets
    of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE288]'
  prefs: []
  type: TYPE_PRE
- en: '#### `rename_columns`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2142)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE289]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column_mapping` (`Dict[str, str]`) — A mapping of columns to rename to their
    new names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[IterableDatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDatasetDict)'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset with renamed columns
  prefs: []
  type: TYPE_NORMAL
- en: Rename several columns in the dataset, and move the features associated to the
    original columns under the new column names. The renaming is applied to all the
    datasets of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE290]'
  prefs: []
  type: TYPE_PRE
- en: '#### `select_columns`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2196)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE291]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column_names` (`Union[str, List[str]]`) — Name of the column(s) to keep.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[IterableDatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDatasetDict)'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset object with only selected columns.
  prefs: []
  type: TYPE_NORMAL
- en: Select one or several column(s) in the dataset and the features associated to
    them. The selection is done on-the-fly on the examples when iterating over the
    dataset. The selection is applied to all the datasets of the dataset dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE292]'
  prefs: []
  type: TYPE_PRE
- en: Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class datasets.Features`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1582)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE293]'
  prefs: []
  type: TYPE_PRE
- en: A special dictionary that defines the internal structure of a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Instantiated with a dictionary of type `dict[str, FieldType]`, where keys are
    the desired column names, and values are the type of that column.
  prefs: []
  type: TYPE_NORMAL
- en: '`FieldType` can be one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: a [Value](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Value)
    feature specifies a single typed value, e.g. `int64` or `string`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a [ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)
    feature specifies a field with a predefined set of classes which can have labels
    associated to them and will be stored as integers in the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a python `dict` which specifies that the field is a nested field containing
    a mapping of sub-fields to sub-fields features. It’s possible to have nested fields
    of nested fields in an arbitrary manner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a python `list` or a [Sequence](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Sequence)
    specifies that the field contains a list of objects. The python `list` or [Sequence](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Sequence)
    should be provided with a single sub-feature as an example of the feature type
    hosted in this list.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [Sequence](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Sequence)
    with a internal dictionary feature will be automatically converted into a dictionary
    of lists. This behavior is implemented to have a compatilbity layer with the TensorFlow
    Datasets library but may be un-wanted in some cases. If you don’t want this behavior,
    you can use a python `list` instead of the [Sequence](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Sequence).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a [Array2D](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Array2D),
    [Array3D](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Array3D),
    [Array4D](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Array4D)
    or [Array5D](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Array5D)
    feature for multidimensional arrays.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)
    feature to store the absolute path to an audio file or a dictionary with the relative
    path to an audio file (“path” key) and its bytes content (“bytes” key). This feature
    extracts the audio data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)
    feature to store the absolute path to an image file, an `np.ndarray` object, a
    `PIL.Image.Image` object or a dictionary with the relative path to an image file
    (“path” key) and its bytes content (“bytes” key). This feature extracts the image
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Translation](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Translation)
    and [TranslationVariableLanguages](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.TranslationVariableLanguages),
    the two features specific to Machine Translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `copy`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1993)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE294]'
  prefs: []
  type: TYPE_PRE
- en: Make a deep copy of [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE295]'
  prefs: []
  type: TYPE_PRE
- en: '#### `decode_batch`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1966)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE296]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`batch` (`dict[str, list[Any]]`) — Dataset batch data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_per_repo_id` (`dict`, *optional*) — To access and decode audio or image
    files from private repositories on the Hub, you can pass a dictionary repo_id
    (str) -> token (bool or str)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decode batch with custom feature decoding.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `decode_column`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1948)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE297]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`list[Any]`) — Dataset column data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`column_name` (`str`) — Dataset column name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decode column with custom feature decoding.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `decode_example`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1925)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE298]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`example` (`dict[str, Any]`) — Dataset row data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_per_repo_id` (`dict`, *optional*) — To access and decode audio or image
    files from private repositories on the Hub, you can pass a dictionary `repo_id
    (str) -> token (bool or str)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decode example with custom feature decoding.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_batch`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1906)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE299]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`batch` (`dict[str, list[Any]]`) — Data in a Dataset batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encode batch into a format for Arrow.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_column`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1890)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE300]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`column` (`list[Any]`) — Data in a Dataset column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`column_name` (`str`) — Dataset column name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encode column into a format for Arrow.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_example`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1876)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE301]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`example` (`dict[str, Any]`) — Data in a Dataset row.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encode example into a format for Arrow.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `flatten`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L2080)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE302]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)'
  prefs: []
  type: TYPE_NORMAL
- en: The flattened features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Flatten the features. Every dictionary column is removed and is replaced by
    all the subfields it contains. The new fields are named by concatenating the name
    of the original column and the subfield name like this: `<original>.<subfield>`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If a column contains nested dictionaries, then all the lower-level subfields
    names are also concatenated to form new columns: `<original>.<subfield>.<subsubfield>`,
    etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE303]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_arrow_schema`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1657)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE304]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pa_schema` (`pyarrow.Schema`) — Arrow Schema.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    from Arrow Schema. It also checks the schema metadata for Hugging Face Datasets
    features. Non-nullable fields are not supported and set to nullable.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_dict`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1688)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE305]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dic` (*dict[str, Any]*) — Python dictionary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '*Features*'
  prefs: []
  type: TYPE_NORMAL
- en: Construct [*Features*] from dict.
  prefs: []
  type: TYPE_NORMAL
- en: Regenerate the nested feature object from a deserialized dict. We use the *_type*
    key to infer the dataclass name of the feature *FieldType*.
  prefs: []
  type: TYPE_NORMAL
- en: It allows for a convenient constructor syntax to define features from deserialized
    JSON dictionaries. This function is used in particular when deserializing a [*DatasetInfo*]
    that was dumped to a JSON object. This acts as an analogue to [*Features.from_arrow_schema*]
    and handles the recursive field-by-field instantiation, but doesn’t require any
    mapping to/from pyarrow, except for the fact that it takes advantage of the mapping
    of pyarrow primitive dtypes that [*Value*] automatically performs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE306]'
  prefs: []
  type: TYPE_PRE
- en: '#### `reorder_fields_as`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L2013)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE307]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`other` ([*Features*]) — The other [*Features*] to align with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reorder Features fields to match the field order of other [*Features*].
  prefs: []
  type: TYPE_NORMAL
- en: The order of the fields is important since it matters for the underlying arrow
    data. Re-ordering the fields allows to make the underlying arrow data type match.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE308]'
  prefs: []
  type: TYPE_PRE
- en: '### `class datasets.Sequence`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1128)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE309]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`length` (`int`) — Length of the sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a list of feature from a single type or a dict of types. Mostly here
    for compatiblity with tfds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE310]'
  prefs: []
  type: TYPE_PRE
- en: '### `class datasets.ClassLabel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L930)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE311]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`num_classes` (`int`, *optional*) — Number of classes. All labels must be <
    `num_classes`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`names` (`list` of `str`, *optional*) — String names for the integer classes.
    The order in which the names are provided is kept.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`names_file` (`str`, *optional*) — Path to a file with names for the integer
    classes, one per line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature type for integer class labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 3 ways to define a `ClassLabel`, which correspond to the 3 arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`num_classes`: Create 0 to (num_classes-1) labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`names`: List of label strings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`names_file`: File containing the list of labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the hood the labels are stored as integers. You can use negative integers
    to represent unknown/missing labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE312]'
  prefs: []
  type: TYPE_PRE
- en: '#### `cast_storage`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1096)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE313]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`storage` (`Union[pa.StringArray, pa.IntegerArray]`) — PyArrow array to cast.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`pa.Int64Array`'
  prefs: []
  type: TYPE_NORMAL
- en: Array in the `ClassLabel` arrow storage type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cast an Arrow array to the `ClassLabel` arrow storage type. The Arrow types
    that can be converted to the `ClassLabel` pyarrow storage type are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pa.string()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.int()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `int2str`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1050)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE314]'
  prefs: []
  type: TYPE_PRE
- en: Conversion `integer` => class name `string`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding unknown/missing labels: passing negative integers raises `ValueError`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE315]'
  prefs: []
  type: TYPE_PRE
- en: '#### `str2int`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1005)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE316]'
  prefs: []
  type: TYPE_PRE
- en: Conversion class name `string` => `integer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE317]'
  prefs: []
  type: TYPE_PRE
- en: '### `class datasets.Value`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L452)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE318]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Value` dtypes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`null`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bool`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`int8`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`int16`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`int32`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`int64`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`uint8`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`uint16`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`uint32`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`uint64`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float16`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float32` (alias float)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float64` (alias double)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time32[(s|ms)]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time64[(us|ns)]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timestamp[(s|ms|us|ns)]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timestamp[(s|ms|us|ns), tz=(tzstring)]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date32`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date64`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`duration[(s|ms|us|ns)]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decimal128(precision, scale)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decimal256(precision, scale)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`large_binary`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`string`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`large_string`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE319]'
  prefs: []
  type: TYPE_PRE
- en: '### `class datasets.Translation`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/translation.py#L11)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE320]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`languages` (`dict`) — A dictionary for each example mapping string language
    codes to string translations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FeatureConnector` for translations with fixed languages per example. Here
    for compatiblity with tfds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE321]'
  prefs: []
  type: TYPE_PRE
- en: '#### `flatten`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/translation.py#L44)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE322]'
  prefs: []
  type: TYPE_PRE
- en: Flatten the Translation feature into a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class datasets.TranslationVariableLanguages`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/translation.py#L51)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE323]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`languages` (`dict`) — A dictionary for each example mapping string language
    codes to one or more string translations. The languages present may vary from
    example to example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`language` or `translation` (variable-length 1D `tf.Tensor` of `tf.string`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language codes sorted in ascending order or plain text translations, sorted
    to align with language codes.
  prefs: []
  type: TYPE_NORMAL
- en: '`FeatureConnector` for translations with variable languages per example. Here
    for compatiblity with tfds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE324]'
  prefs: []
  type: TYPE_PRE
- en: '#### `flatten`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/translation.py#L122)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE325]'
  prefs: []
  type: TYPE_PRE
- en: Flatten the TranslationVariableLanguages feature into a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class datasets.Array2D`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L535)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE326]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`shape` (`tuple`) — The size of each dimension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`str`) — The value of the data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a two-dimensional array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE327]'
  prefs: []
  type: TYPE_PRE
- en: '### `class datasets.Array3D`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L560)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE328]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`shape` (`tuple`) — The size of each dimension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`str`) — The value of the data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a three-dimensional array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE329]'
  prefs: []
  type: TYPE_PRE
- en: '### `class datasets.Array4D`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L585)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE330]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`shape` (`tuple`) — The size of each dimension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`str`) — The value of the data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a four-dimensional array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE331]'
  prefs: []
  type: TYPE_PRE
- en: '### `class datasets.Array5D`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L610)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE332]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`shape` (`tuple`) — The size of each dimension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`str`) — The value of the data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a five-dimensional array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE333]'
  prefs: []
  type: TYPE_PRE
- en: '### `class datasets.Audio`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L20)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE334]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*) — Target sampling rate. If `None`, the
    native sampling rate is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mono` (`bool`, defaults to `True`) — Whether to convert the audio signal to
    mono by averaging samples across channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decode` (`bool`, defaults to `True`) — Whether to decode the audio data. If
    `False`, returns the underlying dictionary in the format `{"path": audio_path,
    "bytes": audio_bytes}`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio `Feature` to extract audio data from an audio file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: The Audio feature accepts as input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A `str`: Absolute path to the audio file (i.e. random access is allowed).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `dict` with the keys:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path`: String with relative path of the audio file to the archive file.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bytes`: Bytes content of the audio file.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This is useful for archived files with sequential access.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A `dict` with the keys:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path`: String with relative path of the audio file to the archive file.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`array`: Array containing the audio sample'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate`: Integer corresponding to the sampling rate of the audio sample.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This is useful for archived files with sequential access.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE335]'
  prefs: []
  type: TYPE_PRE
- en: '#### `cast_storage`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L209)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE336]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`storage` (`Union[pa.StringArray, pa.StructArray]`) — PyArrow array to cast.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`pa.StructArray`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Array in the Audio arrow storage type, that is `pa.struct({"bytes": pa.binary(),
    "path": pa.string()})`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cast an Arrow array to the Audio arrow storage type. The Arrow types that can
    be converted to the Audio pyarrow storage type are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pa.string()` - it must contain the “path” data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.binary()` - it must contain the audio bytes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.struct({"bytes": pa.binary()})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.struct({"path": pa.string()})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.struct({"bytes": pa.binary(), "path": pa.string()})` - order doesn’t matter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `decode_example`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L126)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE337]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`value` (`dict`) — A dictionary with keys:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path`: String with relative audio file path.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bytes`: Bytes of the audio file.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_per_repo_id` (`dict`, *optional*) — To access and decode audio files
    from private repositories on the Hub, you can pass a dictionary repo_id (`str`)
    -> token (`bool` or `str`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`dict`'
  prefs: []
  type: TYPE_NORMAL
- en: Decode example audio file into audio data.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `embed_storage`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L247)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE338]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`storage` (`pa.StructArray`) — PyArrow array to embed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`pa.StructArray`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Array in the Audio arrow storage type, that is `pa.struct({"bytes": pa.binary(),
    "path": pa.string()})`.'
  prefs: []
  type: TYPE_NORMAL
- en: Embed audio files into the Arrow array.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_example`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L77)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE339]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`value` (`str` or `dict`) — Data passed as input to Audio feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`dict`'
  prefs: []
  type: TYPE_NORMAL
- en: Encode example into a format for Arrow.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `flatten`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L198)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE340]'
  prefs: []
  type: TYPE_PRE
- en: If in the decodable state, raise an error, otherwise flatten the feature into
    a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class datasets.Image`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L46)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE341]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`decode` (`bool`, defaults to `True`) — Whether to decode the image data. If
    `False`, returns the underlying dictionary in the format `{"path": image_path,
    "bytes": image_bytes}`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image `Feature` to read image data from an image file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: The Image feature accepts as input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A `str`: Absolute path to the image file (i.e. random access is allowed).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `dict` with the keys:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path`: String with relative path of the image file to the archive file.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bytes`: Bytes of the image file.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This is useful for archived files with sequential access.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'An `np.ndarray`: NumPy array representing an image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `PIL.Image.Image`: PIL image object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE342]'
  prefs: []
  type: TYPE_PRE
- en: '#### `cast_storage`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L201)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE343]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`storage` (`Union[pa.StringArray, pa.StructArray, pa.ListArray]`) — PyArrow
    array to cast.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`pa.StructArray`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Array in the Image arrow storage type, that is `pa.struct({"bytes": pa.binary(),
    "path": pa.string()})`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cast an Arrow array to the Image arrow storage type. The Arrow types that can
    be converted to the Image pyarrow storage type are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pa.string()` - it must contain the “path” data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.binary()` - it must contain the image bytes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.struct({"bytes": pa.binary()})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.struct({"path": pa.string()})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.struct({"bytes": pa.binary(), "path": pa.string()})` - order doesn’t matter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pa.list(*)` - it must contain the image array data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `decode_example`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L131)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE344]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`value` (`str` or `dict`) — A string with the absolute image file path, a dictionary
    with keys:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path`: String with absolute or relative image file path.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bytes`: The bytes of the image file.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_per_repo_id` (`dict`, *optional*) — To access and decode image files
    from private repositories on the Hub, you can pass a dictionary repo_id (`str`)
    -> token (`bool` or `str`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decode example image file into image data.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `embed_storage`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L247)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE345]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`storage` (`pa.StructArray`) — PyArrow array to embed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`pa.StructArray`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Array in the Image arrow storage type, that is `pa.struct({"bytes": pa.binary(),
    "path": pa.string()})`.'
  prefs: []
  type: TYPE_NORMAL
- en: Embed image files into the Arrow array.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_example`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L92)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE346]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`value` (`str`, `np.ndarray`, `PIL.Image.Image` or `dict`) — Data passed as
    input to Image feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encode example into a format for Arrow.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `flatten`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L188)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE347]'
  prefs: []
  type: TYPE_PRE
- en: If in the decodable state, return the feature itself, otherwise flatten the
    feature into a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: MetricInfo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class datasets.MetricInfo`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L510)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE348]'
  prefs: []
  type: TYPE_PRE
- en: Information about a metric.
  prefs: []
  type: TYPE_NORMAL
- en: '`MetricInfo` documents a metric, including its name, version, and features.
    See the constructor arguments and properties for a full list.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Not all fields are known on construction and may be updated later.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_directory`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L566)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE349]'
  prefs: []
  type: TYPE_PRE
- en: Create MetricInfo from the JSON file in `metric_info_dir`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE350]'
  prefs: []
  type: TYPE_PRE
- en: '#### `write_to_directory`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L546)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE351]'
  prefs: []
  type: TYPE_PRE
- en: Write `MetricInfo` as JSON to `metric_info_dir`. Also save the license separately
    in LICENCE. If `pretty_print` is True, the JSON will be pretty-printed with the
    indent level of 4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE352]'
  prefs: []
  type: TYPE_PRE
- en: Metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The base class `Metric` implements a Metric backed by one or several [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset).
  prefs: []
  type: TYPE_NORMAL
- en: '### `class datasets.Metric`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/metric.py#L147)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE353]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config_name` (`str`) — This is used to define a hash specific to a metrics
    computation script and prevents the metric’s data to be overridden when the metric
    loading script is modified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`) — keep all predictions and references in memory.
    Not possible in distributed settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`) — Path to a directory in which temporary prediction/references
    data will be stored. The data directory should be located on a shared file-system
    in distributed setups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_process` (`int`) — specify the total number of nodes in a distributed
    settings. This is useful to compute metrics in distributed setups (in particular
    non-additive metrics like F1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`process_id` (`int`) — specify the id of the current process in a distributed
    setup (between 0 and num_process-1) This is useful to compute metrics in distributed
    setups (in particular non-additive metrics like F1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` (`int`, optional) — If specified, this will temporarily set numpy’s
    random seed when [datasets.Metric.compute()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric.compute)
    is run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`experiment_id` (`str`) — A specific experiment id. This is used if several
    distributed evaluations share the same file system. This is useful to compute
    metrics in distributed setups (in particular non-additive metrics like F1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_concurrent_cache_files` (`int`) — Max number of concurrent metrics cache
    files (default 10000).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (`Union[int, float]`) — Timeout in second for distributed setting
    synchronization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Metric is the base class and common API for all metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Deprecated in 2.5.0
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the new library 🤗 Evaluate instead: [https://huggingface.co/docs/evaluate](https://huggingface.co/docs/evaluate)'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `add`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/metric.py#L522)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE354]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prediction` (list/array/tensor, optional) — Predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reference` (list/array/tensor, optional) — References.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add one prediction and reference for the metric’s stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE355]'
  prefs: []
  type: TYPE_PRE
- en: '#### `add_batch`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/metric.py#L475)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE356]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`predictions` (list/array/tensor, optional) — Predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`references` (list/array/tensor, optional) — References.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a batch of predictions and references for the metric’s stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE357]'
  prefs: []
  type: TYPE_PRE
- en: '#### `compute`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/metric.py#L404)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE358]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`predictions` (list/array/tensor, optional) — Predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`references` (list/array/tensor, optional) — References.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (optional) — Keyword arguments that will be forwarded to the metrics
    `_compute` method (see details in the docstring).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Usage of positional arguments is not allowed to prevent mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE359]'
  prefs: []
  type: TYPE_PRE
- en: '#### `download_and_prepare`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/metric.py#L605)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE360]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`download_config` ([DownloadConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadConfig),
    optional) — Specific download configuration parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dl_manager` ([DownloadManager](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager),
    optional) — Specific download manager to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloads and prepares dataset for reading.
  prefs: []
  type: TYPE_NORMAL
- en: Filesystems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class datasets.filesystems.S3FileSystem`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/filesystems/s3filesystem.py#L6)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE361]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`anon` (`bool`, default to `False`) — Whether to use anonymous connection (public
    buckets only). If `False`, uses the key/secret given, or boto’s credential resolver
    (client_kwargs, environment, variables, config files, EC2 IAM server, in that
    order).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`key` (`str`) — If not anonymous, use this access key ID, if specified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`secret` (`str`) — If not anonymous, use this secret access key, if specified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str`) — If not anonymous, use this security token, if specified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_ssl` (`bool`, defaults to `True`) — Whether to use SSL in connections
    to S3; may be faster without, but insecure. If `use_ssl` is also set in `client_kwargs`,
    the value set in `client_kwargs` will take priority.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s3_additional_kwargs` (`dict`) — Parameters that are used when calling S3
    API methods. Typically used for things like ServerSideEncryption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`client_kwargs` (`dict`) — Parameters for the botocore client.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requester_pays` (`bool`, defaults to `False`) — Whether `RequesterPays` buckets
    are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default_block_size` (`int`) — If given, the default block size value used
    for `open()`, if no specific value is given at all time. The built-in default
    is 5MB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default_fill_cache` (`bool`, defaults to `True`) — Whether to use cache filling
    with open by default. Refer to `S3File.open`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default_cache_type` (`str`, defaults to `bytes`) — If given, the default `cache_type`
    value used for `open()`. Set to `none` if no caching is desired. See fsspec’s
    documentation for other available `cache_type` values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`version_aware` (`bool`, defaults to `False`) — Whether to support bucket versioning.
    If enable this will require the user to have the necessary IAM permissions for
    dealing with versioned objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_regions` (`bool`, defaults to `False`) — Whether to cache bucket regions.
    Whenever a new bucket is used, it will first find out which region it belongs
    to and then use the client for that region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`asynchronous` (`bool`, defaults to `False`) — Whether this instance is to
    be used from inside coroutines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config_kwargs` (`dict`) — Parameters passed to `botocore.client.Config`. **kwargs
    — Other parameters for core session.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`session` (`aiobotocore.session.AioSession`) — Session to be used for all connections.
    This session will be used inplace of creating a new session inside S3FileSystem.
    For example: `aiobotocore.session.AioSession(profile=''test_user'')`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_instance_cache` (`bool`) — Control reuse of instances. Passed on to `fsspec`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_listings_cache` (`bool`) — Control reuse of directory listings. Passed
    on to `fsspec`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`listings_expiry_time` (`int` or `float`) — Control reuse of directory listings.
    Passed on to `fsspec`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_paths` (`int`) — Control reuse of directory listings. Passed on to `fsspec`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`datasets.filesystems.S3FileSystem` is a subclass of [`s3fs.S3FileSystem`](https://s3fs.readthedocs.io/en/latest/api.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Users can use this class to access S3 as if it were a file system. It exposes
    a filesystem-like API (ls, cp, open, etc.) on top of S3 storage. Provide credentials
    either explicitly (`key=`, `secret=`) or with boto’s credential methods. See botocore
    documentation for more information. If no credentials are available, use `anon=True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing files from public S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE362]'
  prefs: []
  type: TYPE_PRE
- en: Listing files from private S3 bucket using `aws_access_key_id` and `aws_secret_access_key`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE363]'
  prefs: []
  type: TYPE_PRE
- en: Using `S3Filesystem` with `botocore.session.Session` and custom `aws_profile`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE364]'
  prefs: []
  type: TYPE_PRE
- en: Loading dataset from S3 using `S3Filesystem` and [load_from_disk()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_from_disk).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE365]'
  prefs: []
  type: TYPE_PRE
- en: Saving dataset to S3 using `S3Filesystem` and [Dataset.save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE366]'
  prefs: []
  type: TYPE_PRE
- en: '#### `datasets.filesystems.extract_path_from_uri`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/filesystems.py#L35)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE367]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset_path` (`str`) — Path (e.g. `dataset/train`) or remote uri (e.g. `s3://my-bucket/dataset/train`)
    of the dataset directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocesses `dataset_path` and removes remote filesystem (e.g. removing `s3://`).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `datasets.filesystems.is_remote_filesystem`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/filesystems/__init__.py#L51)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE368]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`fs` (`fsspec.spec.AbstractFileSystem`) — An abstract super-class for pythonic
    file-systems, e.g. `fsspec.filesystem(''file'')` or [datasets.filesystems.S3FileSystem](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.filesystems.S3FileSystem).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checks if `fs` is a remote filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: Fingerprint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class datasets.fingerprint.Hasher`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/fingerprint.py#L205)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE369]'
  prefs: []
  type: TYPE_PRE
- en: Hasher that accepts python objects as inputs.
  prefs: []
  type: TYPE_NORMAL
