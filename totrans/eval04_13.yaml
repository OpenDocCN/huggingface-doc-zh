- en: 🤗 Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🤗 Transformers
- en: 'Original text: [https://huggingface.co/docs/evaluate/transformers_integrations](https://huggingface.co/docs/evaluate/transformers_integrations)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/evaluate/transformers_integrations](https://huggingface.co/docs/evaluate/transformers_integrations)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the 🤗 Transformers examples make sure you have installed the following
    libraries:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行🤗 Transformers示例，请确保已安装以下库：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Trainer
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Trainer
- en: The metrics in `evaluate` can be easily integrated with the [Trainer](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/trainer#transformers.Trainer).
    The `Trainer` accepts a `compute_metrics` keyword argument that passes a function
    to compute metrics. One can specify the evaluation interval with `evaluation_strategy`
    in the `TrainerArguments`, and based on that, the model is evaluated accordingly,
    and the predictions and labels passed to `compute_metrics`.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate`中的指标可以轻松与[Trainer](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/trainer#transformers.Trainer)集成。`Trainer`接受一个`compute_metrics`关键字参数，传递一个计算指标的函数。可以在`TrainerArguments`中使用`evaluation_strategy`指定评估间隔，根据此间隔，相应地评估模型，并将预测和标签传递给`compute_metrics`。'
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Seq2SeqTrainer
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Seq2SeqTrainer
- en: We can use the [Seq2SeqTrainer](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/trainer#transformers.Seq2SeqTrainer)
    for sequence-to-sequence tasks such as translation or summarization. For such
    generative tasks usually metrics such as ROUGE or BLEU are evaluated. However,
    these metrics require that we generate some text with the model rather than a
    single forward pass as with e.g. classification. The `Seq2SeqTrainer` allows for
    the use of the generate method when setting `predict_with_generate=True` which
    will generate text for each sample in the evaluation set. That means we evaluate
    generated text within the `compute_metric` function. We just need to decode the
    predictions and labels first.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在序列到序列任务（如翻译或摘要）中使用[Seq2SeqTrainer](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/trainer#transformers.Seq2SeqTrainer)。对于这种生成式任务，通常会评估ROUGE或BLEU等指标。然而，这些指标要求我们使用模型生成一些文本，而不是像分类那样进行单次前向传递。当设置`predict_with_generate=True`时，`Seq2SeqTrainer`允许使用生成方法，该方法将为评估集中的每个样本生成文本。这意味着我们在`compute_metric`函数中评估生成的文本。我们只需要首先解码预测和标签。
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can use any `evaluate` metric with the `Trainer` and `Seq2SeqTrainer` as
    long as they are compatible with the task and predictions. In case you don’t want
    to train a model but just evaluate an existing model you can replace `trainer.train()`
    with `trainer.evaluate()` in the above scripts.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在`Trainer`和`Seq2SeqTrainer`中使用任何与任务和预测兼容的`evaluate`指标。如果您不想训练模型，而只是评估现有模型，可以在上述脚本中用`trainer.evaluate()`替换`trainer.train()`。
