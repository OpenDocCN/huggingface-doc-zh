- en: ðŸ¤— Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/evaluate/transformers_integrations](https://huggingface.co/docs/evaluate/transformers_integrations)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/assets/pages/__layout.svelte-hf-doc-builder.css">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/start-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/vendor-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/paths-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/pages/__layout.svelte-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/pages/transformers_integrations.mdx-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/IconCopyLink-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/CodeBlock-hf-doc-builder.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the ðŸ¤— Transformers examples make sure you have installed the following
    libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Trainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The metrics in `evaluate` can be easily integrated with the [Trainer](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/trainer#transformers.Trainer).
    The `Trainer` accepts a `compute_metrics` keyword argument that passes a function
    to compute metrics. One can specify the evaluation interval with `evaluation_strategy`
    in the `TrainerArguments`, and based on that, the model is evaluated accordingly,
    and the predictions and labels passed to `compute_metrics`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Seq2SeqTrainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use the [Seq2SeqTrainer](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/trainer#transformers.Seq2SeqTrainer)
    for sequence-to-sequence tasks such as translation or summarization. For such
    generative tasks usually metrics such as ROUGE or BLEU are evaluated. However,
    these metrics require that we generate some text with the model rather than a
    single forward pass as with e.g. classification. The `Seq2SeqTrainer` allows for
    the use of the generate method when setting `predict_with_generate=True` which
    will generate text for each sample in the evaluation set. That means we evaluate
    generated text within the `compute_metric` function. We just need to decode the
    predictions and labels first.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can use any `evaluate` metric with the `Trainer` and `Seq2SeqTrainer` as
    long as they are compatible with the task and predictions. In case you donâ€™t want
    to train a model but just evaluate an existing model you can replace `trainer.train()`
    with `trainer.evaluate()` in the above scripts.
  prefs: []
  type: TYPE_NORMAL
