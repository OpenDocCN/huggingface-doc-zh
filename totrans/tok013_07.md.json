["```py\nfrom tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers\ntokenizer = Tokenizer(models.Unigram())\ntokenizer.normalizer = normalizers.NFKC()\ntokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\ntokenizer.decoder = decoders.ByteLevel()\ntrainer = trainers.UnigramTrainer(\n    vocab_size=20000,\n    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n    special_tokens=[\"<PAD>\", \"<BOS>\", \"<EOS>\"],\n)\n```", "```py\n# First few lines of the \"Zen of Python\" https://www.python.org/dev/peps/pep-0020/\ndata = [\n    \"Beautiful is better than ugly.\"\n    \"Explicit is better than implicit.\"\n    \"Simple is better than complex.\"\n    \"Complex is better than complicated.\"\n    \"Flat is better than nested.\"\n    \"Sparse is better than dense.\"\n    \"Readability counts.\"\n]\ntokenizer.train_from_iterator(data, trainer=trainer)\n```", "```py\nimport datasets\ndataset = datasets.load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train+test+validation\")\n```", "```py\ndef batch_iterator(batch_size=1000):\n    for i in range(0, len(dataset), batch_size):\n        yield dataset[i : i + batch_size][\"text\"]\n```", "```py\ntokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))\n```", "```py\nimport gzip\nwith gzip.open(\"data/my-file.0.gz\", \"rt\") as f:\n    tokenizer.train_from_iterator(f, trainer=trainer)\n```", "```py\nfiles = [\"data/my-file.0.gz\", \"data/my-file.1.gz\", \"data/my-file.2.gz\"]\ndef gzip_iterator():\n    for path in files:\n        with gzip.open(path, \"rt\") as f:\n            for line in f:\n                yield line\ntokenizer.train_from_iterator(gzip_iterator(), trainer=trainer)\n```"]