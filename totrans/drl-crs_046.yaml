- en: The Deep Q-Network (DQN)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/learn/deep-rl-course/unit3/deep-q-network](https://huggingface.co/learn/deep-rl-course/unit3/deep-q-network)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/39.c9061f89.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the architecture of our Deep Q-Learning network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep Q Network](../Images/ae1d15faad114a179990b8f1f33104d4.png)'
  prefs: []
  type: TYPE_IMG
- en: As input, we take a **stack of 4 frames** passed through the network as a state
    and output a **vector of Q-values for each possible action at that state**. Then,
    like with Q-Learning, we just need to use our epsilon-greedy policy to select
    which action to take.
  prefs: []
  type: TYPE_NORMAL
- en: When the Neural Network is initialized, **the Q-value estimation is terrible**.
    But during training, our Deep Q-Network agent will associate a situation with
    the appropriate action and **learn to play the game well**.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the input and temporal limitation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to **preprocess the input**. It’s an essential step since we want to
    **reduce the complexity of our state to reduce the computation time needed for
    training**.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, we **reduce the state space to 84x84 and grayscale it**. We
    can do this since the colors in Atari environments don’t add important information.
    This is a big improvement since we **reduce our three color channels (RGB) to
    1**.
  prefs: []
  type: TYPE_NORMAL
- en: We can also **crop a part of the screen in some games** if it does not contain
    important information. Then we stack four frames together.
  prefs: []
  type: TYPE_NORMAL
- en: '![Preprocessing](../Images/30478bfb5d8bf377a5c066b55e0d3f0d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Why do we stack four frames together?** We stack frames together because
    it helps us **handle the problem of temporal limitation**. Let’s take an example
    with the game of Pong. When you see this frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Temporal Limitation](../Images/916225d18ad696514245f8c4e88a5a56.png)'
  prefs: []
  type: TYPE_IMG
- en: Can you tell me where the ball is going? No, because one frame is not enough
    to have a sense of motion! But what if I add three more frames? **Here you can
    see that the ball is going to the right**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Temporal Limitation](../Images/e35a3e3cfeefe6f7a16b681ab91dfa7b.png) That’s
    why, to capture temporal information, we stack four frames together.'
  prefs: []
  type: TYPE_NORMAL
- en: Then the stacked frames are processed by three convolutional layers. These layers
    **allow us to capture and exploit spatial relationships in images**. But also,
    because the frames are stacked together, **we can exploit some temporal properties
    across those frames**.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t know what convolutional layers are, don’t worry. You can check
    out [Lesson 4 of this free Deep Learning Course by Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188)
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have a couple of fully connected layers that output a Q-value for
    each possible action at that state.
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep Q Network](../Images/ae1d15faad114a179990b8f1f33104d4.png)'
  prefs: []
  type: TYPE_IMG
- en: So, we see that Deep Q-Learning uses a neural network to approximate, given
    a state, the different Q-values for each possible action at that state. Now let’s
    study the Deep Q-Learning algorithm.
  prefs: []
  type: TYPE_NORMAL
