- en: LoHa
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/peft/package_reference/loha](https://huggingface.co/docs/peft/package_reference/loha)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Low-Rank Hadamard Product ([LoHa](https://huggingface.co/papers/2108.06098)),
    is similar to LoRA except it approximates the large weight matrix with more low-rank
    matrices and combines them with the Hadamard product. This method is even more
    parameter-efficient than LoRA and achieves comparable performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*In this work, we propose a communication-efficient parameterization, FedPara,
    for federated learning (FL) to overcome the burdens on frequent model uploads
    and downloads. Our method re-parameterizes weight parameters of layers using low-rank
    weights followed by the Hadamard product. Compared to the conventional low-rank
    parameterization, our FedPara method is not restricted to low-rank constraints,
    and thereby it has a far larger capacity. This property enables to achieve comparable
    performance while requiring 3 to 10 times lower communication costs than the model
    with the original layers, which is not achievable by the traditional low-rank
    methods. The efficiency of our method can be further improved by combining with
    other efficient FL optimizers. In addition, we extend our method to a personalized
    FL application, pFedPara, which separates parameters into global and local ones.
    We show that pFedPara outperforms competing personalized FL methods with more
    than three times fewer parameters*.'
  prefs: []
  type: TYPE_NORMAL
- en: LoHaConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.LoHaConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/loha/config.py#L22)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`r` (`int`) — LoHa rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha` (`int`) — The alpha parameter for LoHa scaling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank_dropout` (`float`) — The dropout probability for rank dimension during
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`module_dropout` (`float`) — The dropout probability for disabling LoHa modules
    during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_effective_conv2d` (`bool`) — Use parameter effective decomposition for
    Conv2d with ksize > 1 (“Proposition 3” from FedPara paper).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_modules` (`Optional[Union[List[str], str]]`) — The names of the modules
    to apply the adapter to. If this is specified, only the modules with the specified
    names will be replaced. When passing a string, a regex match will be performed.
    When passing a list of strings, either an exact match will be performed or it
    is checked if the name of the module ends with any of the passed strings. If this
    is specified as ‘all-linear’, then all linear/Conv1D modules are chosen, excluding
    the output layer. If this is not specified, modules will be chosen according to
    the model architecture. If the architecture is not known, an error will be raised
    — in this case, you should specify the target modules manually.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_weights` (`bool`) — Whether to perform initialization of adapter weights.
    This defaults to `True`, passing `False` is discouraged.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layers_to_transform` (`Union[List[int], int]`) — The layer indices to transform.
    If a list of ints is passed, it will apply the adapter to the layer indices that
    are specified in this list. If a single integer is passed, it will apply the transformations
    on the layer at this index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layers_pattern` (`str`) — The layer pattern name, used only if `layers_to_transform`
    is different from `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank_pattern` (`dict`) — The mapping from layer names or regexp expression
    to ranks which are different from the default rank specified by `r`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha_pattern` (`dict`) — The mapping from layer names or regexp expression
    to alphas which are different from the default alpha specified by `alpha`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modules_to_save` (`Optional[List[str]]`) — List of modules apart from adapter
    layers to be set as trainable and saved in the final checkpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [LoHaModel](/docs/peft/v0.8.2/en/package_reference/loha#peft.LoHaModel).
  prefs: []
  type: TYPE_NORMAL
- en: LoHaModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.LoHaModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/loha/model.py#L27)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model to which the adapter tuner layers will
    be attached.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config` ([LoHaConfig](/docs/peft/v0.8.2/en/package_reference/loha#peft.LoHaConfig))
    — The configuration of the LoHa model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`) — The name of the adapter, defaults to `"default"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.nn.Module`'
  prefs: []
  type: TYPE_NORMAL
- en: The LoHa model.
  prefs: []
  type: TYPE_NORMAL
- en: Creates Low-Rank Hadamard Product model from a pretrained model. The method
    is partially described in [https://arxiv.org/abs/2108.06098](https://arxiv.org/abs/2108.06098)
    Current implementation heavily borrows from [https://github.com/KohakuBlueleaf/LyCORIS/blob/eb460098187f752a5d66406d3affade6f0a07ece/lycoris/modules/loha.py](https://github.com/KohakuBlueleaf/LyCORIS/blob/eb460098187f752a5d66406d3affade6f0a07ece/lycoris/modules/loha.py)
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Attributes**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`~torch.nn.Module`) — The model to be adapted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` ([LoHaConfig](/docs/peft/v0.8.2/en/package_reference/loha#peft.LoHaConfig)):
    The configuration of the LoHa model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
