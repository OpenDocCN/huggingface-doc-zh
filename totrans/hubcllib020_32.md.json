["```py\n( model: Optional = None token: Union = None timeout: Optional = None headers: Optional = None cookies: Optional = None )\n```", "```py\n( audio: Union model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.audio_classification(\"audio.flac\")\n[{'score': 0.4976358711719513, 'label': 'hap'}, {'score': 0.3677836060523987, 'label': 'neu'},...]\n```", "```py\n( audio: Union model: Optional = None ) \u2192 export const metadata = 'undefined';str\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.automatic_speech_recognition(\"hello_world.flac\")\n\"hello world\"\n```", "```py\n( text: str generated_responses: Optional = None past_user_inputs: Optional = None parameters: Optional = None model: Optional = None ) \u2192 export const metadata = 'undefined';Dict\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> output = client.conversational(\"Hi, who are you?\")\n>>> output\n{'generated_text': 'I am the one who knocks.', 'conversation': {'generated_responses': ['I am the one who knocks.'], 'past_user_inputs': ['Hi, who are you?']}, 'warnings': ['Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.']}\n>>> client.conversational(\n...     \"Wow, that's scary!\",\n...     generated_responses=output[\"conversation\"][\"generated_responses\"],\n...     past_user_inputs=output[\"conversation\"][\"past_user_inputs\"],\n... )\n```", "```py\n( image: Union question: str model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.document_question_answering(image=\"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\", question=\"What is the invoice number?\")\n[{'score': 0.42515629529953003, 'answer': 'us-001', 'start': 16, 'end': 16}]\n```", "```py\n( text: str model: Optional = None ) \u2192 export const metadata = 'undefined';np.ndarray\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.feature_extraction(\"Hi, who are you?\")\narray([[ 2.424802  ,  2.93384   ,  1.1750331 , ...,  1.240499, -0.13776633, -0.7889173 ],\n[-0.42943227, -0.6364878 , -1.693462  , ...,  0.41978157, -2.4336355 ,  0.6162071 ],\n...,\n[ 0.28552425, -0.928395  , -1.2077185 , ...,  0.76810825, -2.1069427 ,  0.6236161 ]], dtype=float32)\n```", "```py\n( text: str model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.fill_mask(\"The goal of life is <mask>.\")\n[{'score': 0.06897063553333282,\n'token': 11098,\n'token_str': ' happiness',\n'sequence': 'The goal of life is happiness.'},\n{'score': 0.06554922461509705,\n'token': 45075,\n'token_str': ' immortality',\n'sequence': 'The goal of life is immortality.'}]\n```", "```py\n( model: Optional = None ) \u2192 export const metadata = 'undefined';ModelStatus\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.get_model_status(\"bigcode/starcoder\")\nModelStatus(loaded=True, state='Loaded', compute_type='gpu', framework='text-generation-inference')\n```", "```py\n( task: str ) \u2192 export const metadata = 'undefined';str\n```", "```py\n( image: Union model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.image_classification(\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg\")\n[{'score': 0.9779096841812134, 'label': 'Blenheim spaniel'}, ...]\n```", "```py\n( image: Union model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.image_segmentation(\"cat.jpg\"):\n[{'score': 0.989008, 'label': 'LABEL_184', 'mask': <PIL.PngImagePlugin.PngImageFile image mode=L size=400x300 at 0x7FDD2B129CC0>}, ...]\n```", "```py\n( image: Union prompt: Optional = None negative_prompt: Optional = None height: Optional = None width: Optional = None num_inference_steps: Optional = None guidance_scale: Optional = None model: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';Image\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> image = client.image_to_image(\"cat.jpg\", prompt=\"turn the cat into a tiger\")\n>>> image.save(\"tiger.jpg\")\n```", "```py\n( image: Union model: Optional = None ) \u2192 export const metadata = 'undefined';str\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.image_to_text(\"cat.jpg\")\n'a cat standing in a grassy field '\n>>> client.image_to_text(\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg\")\n'a dog laying on the grass next to a flower pot '\n```", "```py\n( frameworks: Union = None ) \u2192 export const metadata = 'undefined';Dict[str, List[str]]\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n\n# Discover zero-shot-classification models currently deployed\n>>> models = client.list_deployed_models()\n>>> models[\"zero-shot-classification\"]\n['Narsil/deberta-large-mnli-zero-cls', 'facebook/bart-large-mnli', ...]\n\n# List from only 1 framework\n>>> client.list_deployed_models(\"text-generation-inference\")\n{'text-generation': ['bigcode/starcoder', 'meta-llama/Llama-2-70b-chat-hf', ...], ...}\n```", "```py\n( image: Union model: Optional = None ) \u2192 export const metadata = 'undefined';List[ObjectDetectionOutput]\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.object_detection(\"people.jpg\"):\n[{\"score\":0.9486683011054993,\"label\":\"person\",\"box\":{\"xmin\":59,\"ymin\":39,\"xmax\":420,\"ymax\":510}}, ... ]\n```", "```py\n( json: Union = None data: Union = None model: Optional = None task: Optional = None stream: bool = False ) \u2192 export const metadata = 'undefined';bytes\n```", "```py\n( question: str context: str model: Optional = None ) \u2192 export const metadata = 'undefined';Dict\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.question_answering(question=\"What's my name?\", context=\"My name is Clara and I live in Berkeley.\")\n{'score': 0.9326562285423279, 'start': 11, 'end': 16, 'answer': 'Clara'}\n```", "```py\n( sentence: str other_sentences: List model: Optional = None ) \u2192 export const metadata = 'undefined';List[float]\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.sentence_similarity(\n...     \"Machine learning is so easy.\",\n...     other_sentences=[\n...         \"Deep learning is so straightforward.\",\n...         \"This is so difficult, like rocket science.\",\n...         \"I can't believe how much I struggled with this.\",\n...     ],\n... )\n[0.7785726189613342, 0.45876261591911316, 0.2906220555305481]\n```", "```py\n( text: str parameters: Optional = None model: Optional = None ) \u2192 export const metadata = 'undefined';str\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.summarization(\"The Eiffel tower...\")\n'The Eiffel tower is one of the most famous landmarks in the world....'\n```", "```py\n( table: Dict query: str model: Optional = None ) \u2192 export const metadata = 'undefined';Dict\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> query = \"How many stars does the transformers repository have?\"\n>>> table = {\"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"], \"Stars\": [\"36542\", \"4512\", \"3934\"]}\n>>> client.table_question_answering(table, query, model=\"google/tapas-base-finetuned-wtq\")\n{'answer': 'AVERAGE > 36542', 'coordinates': [[0, 1]], 'cells': ['36542'], 'aggregator': 'AVERAGE'}\n```", "```py\n( table: Dict model: str ) \u2192 export const metadata = 'undefined';List\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> table = {\n...     \"fixed_acidity\": [\"7.4\", \"7.8\", \"10.3\"],\n...     \"volatile_acidity\": [\"0.7\", \"0.88\", \"0.32\"],\n...     \"citric_acid\": [\"0\", \"0\", \"0.45\"],\n...     \"residual_sugar\": [\"1.9\", \"2.6\", \"6.4\"],\n...     \"chlorides\": [\"0.076\", \"0.098\", \"0.073\"],\n...     \"free_sulfur_dioxide\": [\"11\", \"25\", \"5\"],\n...     \"total_sulfur_dioxide\": [\"34\", \"67\", \"13\"],\n...     \"density\": [\"0.9978\", \"0.9968\", \"0.9976\"],\n...     \"pH\": [\"3.51\", \"3.2\", \"3.23\"],\n...     \"sulphates\": [\"0.56\", \"0.68\", \"0.82\"],\n...     \"alcohol\": [\"9.4\", \"9.8\", \"12.6\"],\n... }\n>>> client.tabular_classification(table=table, model=\"julien-c/wine-quality\")\n[\"5\", \"5\", \"5\"]\n```", "```py\n( table: Dict model: str ) \u2192 export const metadata = 'undefined';List\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> table = {\n...     \"Height\": [\"11.52\", \"12.48\", \"12.3778\"],\n...     \"Length1\": [\"23.2\", \"24\", \"23.9\"],\n...     \"Length2\": [\"25.4\", \"26.3\", \"26.5\"],\n...     \"Length3\": [\"30\", \"31.2\", \"31.1\"],\n...     \"Species\": [\"Bream\", \"Bream\", \"Bream\"],\n...     \"Width\": [\"4.02\", \"4.3056\", \"4.6961\"],\n... }\n>>> client.tabular_regression(table, model=\"scikit-learn/Fish-Weight\")\n[110, 120, 130]\n```", "```py\n( text: str model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.text_classification(\"I like you\")\n[{'label': 'POSITIVE', 'score': 0.9998695850372314}, {'label': 'NEGATIVE', 'score': 0.0001304351753788069}]\n```", "```py\n( prompt: str details: bool = False stream: bool = False model: Optional = None do_sample: bool = False max_new_tokens: int = 20 best_of: Optional = None repetition_penalty: Optional = None return_full_text: bool = False seed: Optional = None stop_sequences: Optional = None temperature: Optional = None top_k: Optional = None top_p: Optional = None truncate: Optional = None typical_p: Optional = None watermark: bool = False decoder_input_details: bool = False ) \u2192 export const metadata = 'undefined';Union[str, TextGenerationResponse, Iterable[str], Iterable[TextGenerationStreamResponse]]\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n\n# Case 1: generate text\n>>> client.text_generation(\"The huggingface_hub library is \", max_new_tokens=12)\n'100% open source and built to be easy to use.'\n\n# Case 2: iterate over the generated tokens. Useful for large generation.\n>>> for token in client.text_generation(\"The huggingface_hub library is \", max_new_tokens=12, stream=True):\n...     print(token)\n100\n%\nopen\nsource\nand\nbuilt\nto\nbe\neasy\nto\nuse\n.\n\n# Case 3: get more details about the generation process.\n>>> client.text_generation(\"The huggingface_hub library is \", max_new_tokens=12, details=True)\nTextGenerationResponse(\n    generated_text='100% open source and built to be easy to use.',\n    details=Details(\n        finish_reason=<FinishReason.Length: 'length'>,\n        generated_tokens=12,\n        seed=None,\n        prefill=[\n            InputToken(id=487, text='The', logprob=None),\n            InputToken(id=53789, text=' hugging', logprob=-13.171875),\n            (...)\n            InputToken(id=204, text=' ', logprob=-7.0390625)\n        ],\n        tokens=[\n            Token(id=1425, text='100', logprob=-1.0175781, special=False),\n            Token(id=16, text='%', logprob=-0.0463562, special=False),\n            (...)\n            Token(id=25, text='.', logprob=-0.5703125, special=False)\n        ],\n        best_of_sequences=None\n    )\n)\n\n# Case 4: iterate over the generated tokens with more details.\n# Last object is more complete, containing the full generated text and the finish reason.\n>>> for details in client.text_generation(\"The huggingface_hub library is \", max_new_tokens=12, details=True, stream=True):\n...     print(details)\n...\nTextGenerationStreamResponse(token=Token(id=1425, text='100', logprob=-1.0175781, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=16, text='%', logprob=-0.0463562, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=1314, text=' open', logprob=-1.3359375, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=3178, text=' source', logprob=-0.28100586, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=273, text=' and', logprob=-0.5961914, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=3426, text=' built', logprob=-1.9423828, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=271, text=' to', logprob=-1.4121094, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=314, text=' be', logprob=-1.5224609, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=1833, text=' easy', logprob=-2.1132812, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=271, text=' to', logprob=-0.08520508, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=745, text=' use', logprob=-0.39453125, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(\n    id=25,\n    text='.',\n    logprob=-0.5703125,\n    special=False),\n    generated_text='100% open source and built to be easy to use.',\n    details=StreamDetails(finish_reason=<FinishReason.Length: 'length'>, generated_tokens=12, seed=None)\n)\n```", "```py\n( prompt: str negative_prompt: Optional = None height: Optional = None width: Optional = None num_inference_steps: Optional = None guidance_scale: Optional = None model: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';Image\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n\n>>> image = client.text_to_image(\"An astronaut riding a horse on the moon.\")\n>>> image.save(\"astronaut.png\")\n\n>>> image = client.text_to_image(\n...     \"An astronaut riding a horse on the moon.\",\n...     negative_prompt=\"low resolution, blurry\",\n...     model=\"stabilityai/stable-diffusion-2-1\",\n... )\n>>> image.save(\"better_astronaut.png\")\n```", "```py\n( text: str model: Optional = None ) \u2192 export const metadata = 'undefined';bytes\n```", "```py\n>>> from pathlib import Path\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n\n>>> audio = client.text_to_speech(\"Hello world\")\n>>> Path(\"hello_world.flac\").write_bytes(audio)\n```", "```py\n( text: str model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.token_classification(\"My name is Sarah Jessica Parker but you can call me Jessica\")\n[{'entity_group': 'PER',\n'score': 0.9971321225166321,\n'word': 'Sarah Jessica Parker',\n'start': 11,\n'end': 31},\n{'entity_group': 'PER',\n'score': 0.9773476123809814,\n'word': 'Jessica',\n'start': 52,\n'end': 59}]\n```", "```py\n( text: str model: Optional = None src_lang: Optional = None tgt_lang: Optional = None ) \u2192 export const metadata = 'undefined';str\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.translation(\"My name is Wolfgang and I live in Berlin\")\n'Mein Name ist Wolfgang und ich lebe in Berlin.'\n>>> client.translation(\"My name is Wolfgang and I live in Berlin\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n\"Je m'appelle Wolfgang et je vis \u00e0 Berlin.\"\n```", "```py\n>>> client.translation(\"My name is Sarah Jessica Parker but you can call me Jessica\", model=\"facebook/mbart-large-50-many-to-many-mmt\", src_lang=\"en_XX\", tgt_lang=\"fr_XX\")\n\"Mon nom est Sarah Jessica Parker mais vous pouvez m'appeler Jessica\"\n```", "```py\n( image: Union question: str model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.visual_question_answering(\n...     image=\"https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\",\n...     question=\"What is the animal doing?\"\n... )\n[{'score': 0.778609573841095, 'answer': 'laying down'},{'score': 0.6957435607910156, 'answer': 'sitting'}, ...]\n```", "```py\n( text: str labels: List multi_label: bool = False model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> text = (\n...     \"A new model offers an explanation for how the Galilean satellites formed around the solar system's\"\n...     \"largest world. Konstantin Batygin did not set out to solve one of the solar system's most puzzling\"\n...     \" mysteries when he went for a run up a hill in Nice, France.\"\n... )\n>>> labels = [\"space & cosmos\", \"scientific discovery\", \"microbiology\", \"robots\", \"archeology\"]\n>>> client.zero_shot_classification(text, labels)\n[\n    {\"label\": \"scientific discovery\", \"score\": 0.7961668968200684},\n    {\"label\": \"space & cosmos\", \"score\": 0.18570658564567566},\n    {\"label\": \"microbiology\", \"score\": 0.00730885099619627},\n    {\"label\": \"archeology\", \"score\": 0.006258360575884581},\n    {\"label\": \"robots\", \"score\": 0.004559356719255447},\n]\n>>> client.zero_shot_classification(text, labels, multi_label=True)\n[\n    {\"label\": \"scientific discovery\", \"score\": 0.9829297661781311},\n    {\"label\": \"space & cosmos\", \"score\": 0.755190908908844},\n    {\"label\": \"microbiology\", \"score\": 0.0005462635890580714},\n    {\"label\": \"archeology\", \"score\": 0.00047131875180639327},\n    {\"label\": \"robots\", \"score\": 0.00030448526376858354},\n]\n```", "```py\n( image: Union labels: List model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n\n>>> client.zero_shot_image_classification(\n...     \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg\",\n...     labels=[\"dog\", \"cat\", \"horse\"],\n... )\n[{\"label\": \"dog\", \"score\": 0.956}, ...]\n```", "```py\npip install --upgrade huggingface_hub[inference]\n# or\n# pip install aiohttp\n```", "```py\n( model: Optional = None token: Union = None timeout: Optional = None headers: Optional = None cookies: Optional = None )\n```", "```py\n( audio: Union model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.audio_classification(\"audio.flac\")\n[{'score': 0.4976358711719513, 'label': 'hap'}, {'score': 0.3677836060523987, 'label': 'neu'},...]\n```", "```py\n( audio: Union model: Optional = None ) \u2192 export const metadata = 'undefined';str\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.automatic_speech_recognition(\"hello_world.flac\")\n\"hello world\"\n```", "```py\n( text: str generated_responses: Optional = None past_user_inputs: Optional = None parameters: Optional = None model: Optional = None ) \u2192 export const metadata = 'undefined';Dict\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> output = await client.conversational(\"Hi, who are you?\")\n>>> output\n{'generated_text': 'I am the one who knocks.', 'conversation': {'generated_responses': ['I am the one who knocks.'], 'past_user_inputs': ['Hi, who are you?']}, 'warnings': ['Setting `pad_token_id` to `eos_token_id`:50256 async for open-end generation.']}\n>>> await client.conversational(\n...     \"Wow, that's scary!\",\n...     generated_responses=output[\"conversation\"][\"generated_responses\"],\n...     past_user_inputs=output[\"conversation\"][\"past_user_inputs\"],\n... )\n```", "```py\n( image: Union question: str model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.document_question_answering(image=\"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\", question=\"What is the invoice number?\")\n[{'score': 0.42515629529953003, 'answer': 'us-001', 'start': 16, 'end': 16}]\n```", "```py\n( text: str model: Optional = None ) \u2192 export const metadata = 'undefined';np.ndarray\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.feature_extraction(\"Hi, who are you?\")\narray([[ 2.424802  ,  2.93384   ,  1.1750331 , ...,  1.240499, -0.13776633, -0.7889173 ],\n[-0.42943227, -0.6364878 , -1.693462  , ...,  0.41978157, -2.4336355 ,  0.6162071 ],\n...,\n[ 0.28552425, -0.928395  , -1.2077185 , ...,  0.76810825, -2.1069427 ,  0.6236161 ]], dtype=float32)\n```", "```py\n( text: str model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.fill_mask(\"The goal of life is <mask>.\")\n[{'score': 0.06897063553333282,\n'token': 11098,\n'token_str': ' happiness',\n'sequence': 'The goal of life is happiness.'},\n{'score': 0.06554922461509705,\n'token': 45075,\n'token_str': ' immortality',\n'sequence': 'The goal of life is immortality.'}]\n```", "```py\n( model: Optional = None ) \u2192 export const metadata = 'undefined';ModelStatus\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.get_model_status(\"bigcode/starcoder\")\nModelStatus(loaded=True, state='Loaded', compute_type='gpu', framework='text-generation-inference')\n```", "```py\n( task: str ) \u2192 export const metadata = 'undefined';str\n```", "```py\n( image: Union model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.image_classification(\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg\")\n[{'score': 0.9779096841812134, 'label': 'Blenheim spaniel'}, ...]\n```", "```py\n( image: Union model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.image_segmentation(\"cat.jpg\"):\n[{'score': 0.989008, 'label': 'LABEL_184', 'mask': <PIL.PngImagePlugin.PngImageFile image mode=L size=400x300 at 0x7FDD2B129CC0>}, ...]\n```", "```py\n( image: Union prompt: Optional = None negative_prompt: Optional = None height: Optional = None width: Optional = None num_inference_steps: Optional = None guidance_scale: Optional = None model: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';Image\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> image = await client.image_to_image(\"cat.jpg\", prompt=\"turn the cat into a tiger\")\n>>> image.save(\"tiger.jpg\")\n```", "```py\n( image: Union model: Optional = None ) \u2192 export const metadata = 'undefined';str\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.image_to_text(\"cat.jpg\")\n'a cat standing in a grassy field '\n>>> await client.image_to_text(\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg\")\n'a dog laying on the grass next to a flower pot '\n```", "```py\n( frameworks: Union = None ) \u2192 export const metadata = 'undefined';Dict[str, List[str]]\n```", "```py\n# Must be run in an async contextthon\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n\n# Discover zero-shot-classification models currently deployed\n>>> models = await client.list_deployed_models()\n>>> models[\"zero-shot-classification\"]\n['Narsil/deberta-large-mnli-zero-cls', 'facebook/bart-large-mnli', ...]\n\n# List from only 1 framework\n>>> await client.list_deployed_models(\"text-generation-inference\")\n{'text-generation': ['bigcode/starcoder', 'meta-llama/Llama-2-70b-chat-hf', ...], ...}\n```", "```py\n( image: Union model: Optional = None ) \u2192 export const metadata = 'undefined';List[ObjectDetectionOutput]\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.object_detection(\"people.jpg\"):\n[{\"score\":0.9486683011054993,\"label\":\"person\",\"box\":{\"xmin\":59,\"ymin\":39,\"xmax\":420,\"ymax\":510}}, ... ]\n```", "```py\n( json: Union = None data: Union = None model: Optional = None task: Optional = None stream: bool = False ) \u2192 export const metadata = 'undefined';bytes\n```", "```py\n( question: str context: str model: Optional = None ) \u2192 export const metadata = 'undefined';Dict\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.question_answering(question=\"What's my name?\", context=\"My name is Clara and I live in Berkeley.\")\n{'score': 0.9326562285423279, 'start': 11, 'end': 16, 'answer': 'Clara'}\n```", "```py\n( sentence: str other_sentences: List model: Optional = None ) \u2192 export const metadata = 'undefined';List[float]\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.sentence_similarity(\n...     \"Machine learning is so easy.\",\n...     other_sentences=[\n...         \"Deep learning is so straightforward.\",\n...         \"This is so difficult, like rocket science.\",\n...         \"I can't believe how much I struggled with this.\",\n...     ],\n... )\n[0.7785726189613342, 0.45876261591911316, 0.2906220555305481]\n```", "```py\n( text: str parameters: Optional = None model: Optional = None ) \u2192 export const metadata = 'undefined';str\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.summarization(\"The Eiffel tower...\")\n'The Eiffel tower is one of the most famous landmarks in the world....'\n```", "```py\n( table: Dict query: str model: Optional = None ) \u2192 export const metadata = 'undefined';Dict\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> query = \"How many stars does the transformers repository have?\"\n>>> table = {\"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"], \"Stars\": [\"36542\", \"4512\", \"3934\"]}\n>>> await client.table_question_answering(table, query, model=\"google/tapas-base-finetuned-wtq\")\n{'answer': 'AVERAGE > 36542', 'coordinates': [[0, 1]], 'cells': ['36542'], 'aggregator': 'AVERAGE'}\n```", "```py\n( table: Dict model: str ) \u2192 export const metadata = 'undefined';List\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> table = {\n...     \"fixed_acidity\": [\"7.4\", \"7.8\", \"10.3\"],\n...     \"volatile_acidity\": [\"0.7\", \"0.88\", \"0.32\"],\n...     \"citric_acid\": [\"0\", \"0\", \"0.45\"],\n...     \"residual_sugar\": [\"1.9\", \"2.6\", \"6.4\"],\n...     \"chlorides\": [\"0.076\", \"0.098\", \"0.073\"],\n...     \"free_sulfur_dioxide\": [\"11\", \"25\", \"5\"],\n...     \"total_sulfur_dioxide\": [\"34\", \"67\", \"13\"],\n...     \"density\": [\"0.9978\", \"0.9968\", \"0.9976\"],\n...     \"pH\": [\"3.51\", \"3.2\", \"3.23\"],\n...     \"sulphates\": [\"0.56\", \"0.68\", \"0.82\"],\n...     \"alcohol\": [\"9.4\", \"9.8\", \"12.6\"],\n... }\n>>> await client.tabular_classification(table=table, model=\"julien-c/wine-quality\")\n[\"5\", \"5\", \"5\"]\n```", "```py\n( table: Dict model: str ) \u2192 export const metadata = 'undefined';List\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> table = {\n...     \"Height\": [\"11.52\", \"12.48\", \"12.3778\"],\n...     \"Length1\": [\"23.2\", \"24\", \"23.9\"],\n...     \"Length2\": [\"25.4\", \"26.3\", \"26.5\"],\n...     \"Length3\": [\"30\", \"31.2\", \"31.1\"],\n...     \"Species\": [\"Bream\", \"Bream\", \"Bream\"],\n...     \"Width\": [\"4.02\", \"4.3056\", \"4.6961\"],\n... }\n>>> await client.tabular_regression(table, model=\"scikit-learn/Fish-Weight\")\n[110, 120, 130]\n```", "```py\n( text: str model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.text_classification(\"I like you\")\n[{'label': 'POSITIVE', 'score': 0.9998695850372314}, {'label': 'NEGATIVE', 'score': 0.0001304351753788069}]\n```", "```py\n( prompt: str details: bool = False stream: bool = False model: Optional = None do_sample: bool = False max_new_tokens: int = 20 best_of: Optional = None repetition_penalty: Optional = None return_full_text: bool = False seed: Optional = None stop_sequences: Optional = None temperature: Optional = None top_k: Optional = None top_p: Optional = None truncate: Optional = None typical_p: Optional = None watermark: bool = False decoder_input_details: bool = False ) \u2192 export const metadata = 'undefined';Union[str, TextGenerationResponse, Iterable[str], Iterable[TextGenerationStreamResponse]]\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n\n# Case 1: generate text\n>>> await client.text_generation(\"The huggingface_hub library is \", max_new_tokens=12)\n'100% open source and built to be easy to use.'\n\n# Case 2: iterate over the generated tokens. Useful async for large generation.\n>>> async for token in await client.text_generation(\"The huggingface_hub library is \", max_new_tokens=12, stream=True):\n...     print(token)\n100\n%\nopen\nsource\nand\nbuilt\nto\nbe\neasy\nto\nuse\n.\n\n# Case 3: get more details about the generation process.\n>>> await client.text_generation(\"The huggingface_hub library is \", max_new_tokens=12, details=True)\nTextGenerationResponse(\n    generated_text='100% open source and built to be easy to use.',\n    details=Details(\n        finish_reason=<FinishReason.Length: 'length'>,\n        generated_tokens=12,\n        seed=None,\n        prefill=[\n            InputToken(id=487, text='The', logprob=None),\n            InputToken(id=53789, text=' hugging', logprob=-13.171875),\n            (...)\n            InputToken(id=204, text=' ', logprob=-7.0390625)\n        ],\n        tokens=[\n            Token(id=1425, text='100', logprob=-1.0175781, special=False),\n            Token(id=16, text='%', logprob=-0.0463562, special=False),\n            (...)\n            Token(id=25, text='.', logprob=-0.5703125, special=False)\n        ],\n        best_of_sequences=None\n    )\n)\n\n# Case 4: iterate over the generated tokens with more details.\n# Last object is more complete, containing the full generated text and the finish reason.\n>>> async for details in await client.text_generation(\"The huggingface_hub library is \", max_new_tokens=12, details=True, stream=True):\n...     print(details)\n...\nTextGenerationStreamResponse(token=Token(id=1425, text='100', logprob=-1.0175781, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=16, text='%', logprob=-0.0463562, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=1314, text=' open', logprob=-1.3359375, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=3178, text=' source', logprob=-0.28100586, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=273, text=' and', logprob=-0.5961914, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=3426, text=' built', logprob=-1.9423828, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=271, text=' to', logprob=-1.4121094, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=314, text=' be', logprob=-1.5224609, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=1833, text=' easy', logprob=-2.1132812, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=271, text=' to', logprob=-0.08520508, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(id=745, text=' use', logprob=-0.39453125, special=False), generated_text=None, details=None)\nTextGenerationStreamResponse(token=Token(\n    id=25,\n    text='.',\n    logprob=-0.5703125,\n    special=False),\n    generated_text='100% open source and built to be easy to use.',\n    details=StreamDetails(finish_reason=<FinishReason.Length: 'length'>, generated_tokens=12, seed=None)\n)\n```", "```py\n( prompt: str negative_prompt: Optional = None height: Optional = None width: Optional = None num_inference_steps: Optional = None guidance_scale: Optional = None model: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';Image\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n\n>>> image = await client.text_to_image(\"An astronaut riding a horse on the moon.\")\n>>> image.save(\"astronaut.png\")\n\n>>> image = await client.text_to_image(\n...     \"An astronaut riding a horse on the moon.\",\n...     negative_prompt=\"low resolution, blurry\",\n...     model=\"stabilityai/stable-diffusion-2-1\",\n... )\n>>> image.save(\"better_astronaut.png\")\n```", "```py\n( text: str model: Optional = None ) \u2192 export const metadata = 'undefined';bytes\n```", "```py\n# Must be run in an async context\n>>> from pathlib import Path\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n\n>>> audio = await client.text_to_speech(\"Hello world\")\n>>> Path(\"hello_world.flac\").write_bytes(audio)\n```", "```py\n( text: str model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.token_classification(\"My name is Sarah Jessica Parker but you can call me Jessica\")\n[{'entity_group': 'PER',\n'score': 0.9971321225166321,\n'word': 'Sarah Jessica Parker',\n'start': 11,\n'end': 31},\n{'entity_group': 'PER',\n'score': 0.9773476123809814,\n'word': 'Jessica',\n'start': 52,\n'end': 59}]\n```", "```py\n( text: str model: Optional = None src_lang: Optional = None tgt_lang: Optional = None ) \u2192 export const metadata = 'undefined';str\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.translation(\"My name is Wolfgang and I live in Berlin\")\n'Mein Name ist Wolfgang und ich lebe in Berlin.'\n>>> await client.translation(\"My name is Wolfgang and I live in Berlin\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n\"Je m'appelle Wolfgang et je vis \u00e0 Berlin.\"\n```", "```py\n>>> client.translation(\"My name is Sarah Jessica Parker but you can call me Jessica\", model=\"facebook/mbart-large-50-many-to-many-mmt\", src_lang=\"en_XX\", tgt_lang=\"fr_XX\")\n\"Mon nom est Sarah Jessica Parker mais vous pouvez m'appeler Jessica\"\n```", "```py\n( image: Union question: str model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> await client.visual_question_answering(\n...     image=\"https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\",\n...     question=\"What is the animal doing?\"\n... )\n[{'score': 0.778609573841095, 'answer': 'laying down'},{'score': 0.6957435607910156, 'answer': 'sitting'}, ...]\n```", "```py\n( text: str labels: List multi_label: bool = False model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n>>> text = (\n...     \"A new model offers an explanation async for how the Galilean satellites formed around the solar system's\"\n...     \"largest world. Konstantin Batygin did not set out to solve one of the solar system's most puzzling\"\n...     \" mysteries when he went async for a run up a hill in Nice, France.\"\n... )\n>>> labels = [\"space & cosmos\", \"scientific discovery\", \"microbiology\", \"robots\", \"archeology\"]\n>>> await client.zero_shot_classification(text, labels)\n[\n    {\"label\": \"scientific discovery\", \"score\": 0.7961668968200684},\n    {\"label\": \"space & cosmos\", \"score\": 0.18570658564567566},\n    {\"label\": \"microbiology\", \"score\": 0.00730885099619627},\n    {\"label\": \"archeology\", \"score\": 0.006258360575884581},\n    {\"label\": \"robots\", \"score\": 0.004559356719255447},\n]\n>>> await client.zero_shot_classification(text, labels, multi_label=True)\n[\n    {\"label\": \"scientific discovery\", \"score\": 0.9829297661781311},\n    {\"label\": \"space & cosmos\", \"score\": 0.755190908908844},\n    {\"label\": \"microbiology\", \"score\": 0.0005462635890580714},\n    {\"label\": \"archeology\", \"score\": 0.00047131875180639327},\n    {\"label\": \"robots\", \"score\": 0.00030448526376858354},\n]\n```", "```py\n( image: Union labels: List model: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n# Must be run in an async context\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n\n>>> await client.zero_shot_image_classification(\n...     \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg\",\n...     labels=[\"dog\", \"cat\", \"horse\"],\n... )\n[{\"label\": \"dog\", \"score\": 0.956}, ...]\n```", "```py\n( *args **kwargs )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( loaded: bool state: str compute_type: str framework: str )\n```", "```py\n( )\n```", "```py\n( do_sample: bool = False max_new_tokens: int = 20 repetition_penalty: Optional = None return_full_text: bool = False stop: List = <factory> seed: Optional = None temperature: Optional = None top_k: Optional = None top_p: Optional = None truncate: Optional = None typical_p: Optional = None best_of: Optional = None watermark: bool = False details: bool = False decoder_input_details: bool = False )\n```", "```py\n( generated_text: str details: Optional = None )\n```", "```py\n( token: Token generated_text: Optional = None details: Optional = None )\n```", "```py\n( id: int text: str logprob: Optional = None )\n```", "```py\n( id: int text: str logprob: float special: bool )\n```", "```py\n( value names = None module = None qualname = None type = None start = 1 )\n```", "```py\n( generated_text: str finish_reason: FinishReason generated_tokens: int seed: Optional = None prefill: List = <factory> tokens: List = <factory> )\n```", "```py\n( finish_reason: FinishReason generated_tokens: int seed: Optional = None prefill: List = <factory> tokens: List = <factory> best_of_sequences: Optional = None )\n```", "```py\n( finish_reason: FinishReason generated_tokens: int seed: Optional = None )\n```", "```py\n( repo_id: str task: Optional = None token: Optional = None gpu: bool = False )\n```", "```py\n>>> from huggingface_hub.inference_api import InferenceApi\n\n>>> # Mask-fill example\n>>> inference = InferenceApi(\"bert-base-uncased\")\n>>> inference(inputs=\"The goal of life is [MASK].\")\n[{'sequence': 'the goal of life is life.', 'score': 0.10933292657136917, 'token': 2166, 'token_str': 'life'}]\n\n>>> # Question Answering example\n>>> inference = InferenceApi(\"deepset/roberta-base-squad2\")\n>>> inputs = {\n...     \"question\": \"What's my name?\",\n...     \"context\": \"My name is Clara and I live in Berkeley.\",\n... }\n>>> inference(inputs)\n{'score': 0.9326569437980652, 'start': 11, 'end': 16, 'answer': 'Clara'}\n\n>>> # Zero-shot example\n>>> inference = InferenceApi(\"typeform/distilbert-base-uncased-mnli\")\n>>> inputs = \"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!\"\n>>> params = {\"candidate_labels\": [\"refund\", \"legal\", \"faq\"]}\n>>> inference(inputs, params)\n{'sequence': 'Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!', 'labels': ['refund', 'faq', 'legal'], 'scores': [0.9378499388694763, 0.04914155602455139, 0.013008488342165947]}\n\n>>> # Overriding configured task\n>>> inference = InferenceApi(\"bert-base-uncased\", task=\"feature-extraction\")\n\n>>> # Text-to-image\n>>> inference = InferenceApi(\"stabilityai/stable-diffusion-2-1\")\n>>> inference(\"cat\")\n<PIL.PngImagePlugin.PngImageFile image (...)>\n\n>>> # Return as raw response to parse the output yourself\n>>> inference = InferenceApi(\"mio/amadeus\")\n>>> response = inference(\"hello world\", raw_response=True)\n>>> response.headers\n{\"Content-Type\": \"audio/flac\", ...}\n>>> response.content # raw bytes from server\nb'(...)'\n```", "```py\n( repo_id: str task: Optional = None token: Optional = None gpu: bool = False )\n```", "```py\n( inputs: Union = None params: Optional = None data: Optional = None raw_response: bool = False )\n```"]