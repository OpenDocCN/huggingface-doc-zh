["```py\npip install pretty-midi==0.2.9 essentia==2.1b6.dev1034 librosa scipy\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor\n\n>>> model = Pop2PianoForConditionalGeneration.from_pretrained(\"sweetcocoa/pop2piano\")\n>>> processor = Pop2PianoProcessor.from_pretrained(\"sweetcocoa/pop2piano\")\n>>> ds = load_dataset(\"sweetcocoa/pop2piano_ci\", split=\"test\")\n\n>>> inputs = processor(\n...     audio=ds[\"audio\"][0][\"array\"], sampling_rate=ds[\"audio\"][0][\"sampling_rate\"], return_tensors=\"pt\"\n... )\n>>> model_output = model.generate(input_features=inputs[\"input_features\"], composer=\"composer1\")\n>>> tokenizer_output = processor.batch_decode(\n...     token_ids=model_output, feature_extractor_output=inputs\n... )[\"pretty_midi_objects\"][0]\n>>> tokenizer_output.write(\"./Outputs/midi_output.mid\")\n```", "```py\n>>> import librosa\n>>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor\n\n>>> audio, sr = librosa.load(\"<your_audio_file_here>\", sr=44100)  # feel free to change the sr to a suitable value.\n>>> model = Pop2PianoForConditionalGeneration.from_pretrained(\"sweetcocoa/pop2piano\")\n>>> processor = Pop2PianoProcessor.from_pretrained(\"sweetcocoa/pop2piano\")\n\n>>> inputs = processor(audio=audio, sampling_rate=sr, return_tensors=\"pt\")\n>>> model_output = model.generate(input_features=inputs[\"input_features\"], composer=\"composer1\")\n>>> tokenizer_output = processor.batch_decode(\n...     token_ids=model_output, feature_extractor_output=inputs\n... )[\"pretty_midi_objects\"][0]\n>>> tokenizer_output.write(\"./Outputs/midi_output.mid\")\n```", "```py\n>>> import librosa\n>>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor\n\n>>> # feel free to change the sr to a suitable value.\n>>> audio1, sr1 = librosa.load(\"<your_first_audio_file_here>\", sr=44100)  \n>>> audio2, sr2 = librosa.load(\"<your_second_audio_file_here>\", sr=44100)\n>>> model = Pop2PianoForConditionalGeneration.from_pretrained(\"sweetcocoa/pop2piano\")\n>>> processor = Pop2PianoProcessor.from_pretrained(\"sweetcocoa/pop2piano\")\n\n>>> inputs = processor(audio=[audio1, audio2], sampling_rate=[sr1, sr2], return_attention_mask=True, return_tensors=\"pt\")\n>>> # Since we now generating in batch(2 audios) we must pass the attention_mask\n>>> model_output = model.generate(\n...     input_features=inputs[\"input_features\"],\n...     attention_mask=inputs[\"attention_mask\"],\n...     composer=\"composer1\",\n... )\n>>> tokenizer_output = processor.batch_decode(\n...     token_ids=model_output, feature_extractor_output=inputs\n... )[\"pretty_midi_objects\"]\n\n>>> # Since we now have 2 generated MIDI files\n>>> tokenizer_output[0].write(\"./Outputs/midi_output1.mid\")\n>>> tokenizer_output[1].write(\"./Outputs/midi_output2.mid\")\n```", "```py\n>>> import librosa\n>>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoFeatureExtractor, Pop2PianoTokenizer\n\n>>> # feel free to change the sr to a suitable value.\n>>> audio1, sr1 = librosa.load(\"<your_first_audio_file_here>\", sr=44100)  \n>>> audio2, sr2 = librosa.load(\"<your_second_audio_file_here>\", sr=44100)\n>>> model = Pop2PianoForConditionalGeneration.from_pretrained(\"sweetcocoa/pop2piano\")\n>>> feature_extractor = Pop2PianoFeatureExtractor.from_pretrained(\"sweetcocoa/pop2piano\")\n>>> tokenizer = Pop2PianoTokenizer.from_pretrained(\"sweetcocoa/pop2piano\")\n\n>>> inputs = feature_extractor(\n...     audio=[audio1, audio2], \n...     sampling_rate=[sr1, sr2], \n...     return_attention_mask=True, \n...     return_tensors=\"pt\",\n... )\n>>> # Since we now generating in batch(2 audios) we must pass the attention_mask\n>>> model_output = model.generate(\n...     input_features=inputs[\"input_features\"],\n...     attention_mask=inputs[\"attention_mask\"],\n...     composer=\"composer1\",\n... )\n>>> tokenizer_output = tokenizer.batch_decode(\n...     token_ids=model_output, feature_extractor_output=inputs\n... )[\"pretty_midi_objects\"]\n\n>>> # Since we now have 2 generated MIDI files\n>>> tokenizer_output[0].write(\"./Outputs/midi_output1.mid\")\n>>> tokenizer_output[1].write(\"./Outputs/midi_output2.mid\")\n```", "```py\n( vocab_size = 2400 composer_vocab_size = 21 d_model = 512 d_kv = 64 d_ff = 2048 num_layers = 6 num_decoder_layers = None num_heads = 8 relative_attention_num_buckets = 32 relative_attention_max_distance = 128 dropout_rate = 0.1 layer_norm_epsilon = 1e-06 initializer_factor = 1.0 feed_forward_proj = 'gated-gelu' is_encoder_decoder = True use_cache = True pad_token_id = 0 eos_token_id = 1 dense_act_fn = 'relu' **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config: Pop2PianoConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None input_features: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n( input_features attention_mask = None composer = 'composer1' generation_config = None **kwargs ) \u2192 export const metadata = 'undefined';ModelOutput or torch.LongTensor\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```"]