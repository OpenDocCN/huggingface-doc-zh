- en: Pix2Struct
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pix2Struct
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pix2struct](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pix2struct)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '原始文本: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pix2struct](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pix2struct)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The Pix2Struct model was proposed in [Pix2Struct: Screenshot Parsing as Pretraining
    for Visual Language Understanding](https://arxiv.org/abs/2210.03347) by Kenton
    Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi
    Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pix2Struct模型是由Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu,
    Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova在[《Pix2Struct:
    Screenshot Parsing as Pretraining for Visual Language Understanding》](https://arxiv.org/abs/2210.03347)中提出的。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '论文摘要如下:'
- en: 'Visually-situated language is ubiquitous — sources range from textbooks with
    diagrams to web pages with images and tables, to mobile apps with buttons and
    forms. Perhaps due to this diversity, previous work has typically relied on domain-specific
    recipes with limited sharing of the underlying data, model architectures, and
    objectives. We present Pix2Struct, a pretrained image-to-text model for purely
    visual language understanding, which can be finetuned on tasks containing visually-situated
    language. Pix2Struct is pretrained by learning to parse masked screenshots of
    web pages into simplified HTML. The web, with its richness of visual elements
    cleanly reflected in the HTML structure, provides a large source of pretraining
    data well suited to the diversity of downstream tasks. Intuitively, this objective
    subsumes common pretraining signals such as OCR, language modeling, image captioning.
    In addition to the novel pretraining strategy, we introduce a variable-resolution
    input representation and a more flexible integration of language and vision inputs,
    where language prompts such as questions are rendered directly on top of the input
    image. For the first time, we show that a single pretrained model can achieve
    state-of-the-art results in six out of nine tasks across four domains: documents,
    illustrations, user interfaces, and natural images.'
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '视觉定位语言是无处不在的 — 源头包括带有图表的教科书、带有图像和表格的网页，以及带有按钮和表单的移动应用程序。也许正是由于这种多样性，先前的工作通常依赖于具有限制性的领域特定配方，对底层数据、模型架构和目标的共享有限。我们提出了Pix2Struct，这是一个预训练的图像到文本模型，用于纯粹的视觉语言理解，可以在包含视觉定位语言的任务上进行微调。Pix2Struct通过学习将屏幕截图的掩码解析为简化的HTML来进行预训练。网络，其丰富的视觉元素清晰地反映在HTML结构中，为下游任务的多样性提供了大量的预训练数据。直观地说，这个目标包含了常见的预训练信号，如OCR、语言建模、图像字幕。除了新颖的预训练策略，我们还引入了可变分辨率的输入表示和更灵活的语言和视觉输入集成，其中语言提示（如问题）直接呈现在输入图像的顶部。我们首次展示，单个预训练模型可以在四个领域的九项任务中的六项中取得最先进的结果:
    文档、插图、用户界面和自然图像。'
- en: 'Tips:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '提示:'
- en: Pix2Struct has been fine tuned on a variety of tasks and datasets, ranging from
    image captioning, visual question answering (VQA) over different inputs (books,
    charts, science diagrams), captioning UI components etc. The full list can be
    found in Table 1 of the paper. We therefore advise you to use these models for
    the tasks they have been fine tuned on. For instance, if you want to use Pix2Struct
    for UI captioning, you should use the model fine tuned on the UI dataset. If you
    want to use Pix2Struct for image captioning, you should use the model fine tuned
    on the natural images captioning dataset and so on.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Pix2Struct已经在各种任务和数据集上进行了微调，包括图像字幕、视觉问答（VQA）以及不同输入（书籍、图表、科学图表）、字幕UI组件等。完整列表可以在论文的表1中找到。因此，我们建议您将这些模型用于它们已经进行微调的任务。例如，如果您想要将Pix2Struct用于UI字幕，您应该使用在UI数据集上进行微调的模型。如果您想要将Pix2Struct用于图像字幕，您应该使用在自然图像字幕数据集上进行微调的模型，依此类推。
- en: If you want to use the model to perform conditional text captioning, make sure
    to use the processor with `add_special_tokens=False`.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要使用模型执行有条件的文本字幕，确保使用`add_special_tokens=False`的处理器。
- en: This model was contributed by [ybelkada](https://huggingface.co/ybelkada). The
    original code can be found [here](https://github.com/google-research/pix2struct).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[ybelkada](https://huggingface.co/ybelkada)贡献。原始代码可以在[这里](https://github.com/google-research/pix2struct)找到。
- en: Resources
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Fine-tuning Notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[微调笔记本](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb)'
- en: '[All models](https://huggingface.co/models?search=pix2struct)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[所有模型](https://huggingface.co/models?search=pix2struct)'
- en: Pix2StructConfig
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pix2StructConfig
- en: '### `class transformers.Pix2StructConfig`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Pix2StructConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/configuration_pix2struct.py#L292)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/configuration_pix2struct.py#L292)'
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text_config` (`dict`, *optional*) — Dictionary of configuration options used
    to initialize [Pix2StructTextConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructTextConfig).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_config` (`dict`, *可选*) — 用于初始化[Pix2StructTextConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructTextConfig)的配置选项字典。'
- en: '`vision_config` (`dict`, *optional*) — Dictionary of configuration options
    used to initialize [Pix2StructVisionConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructVisionConfig).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vision_config` (`dict`, *可选*) — 用于初始化[Pix2StructVisionConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructVisionConfig)的配置选项字典。'
- en: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — Factor to multiply
    the initialization range with.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_factor` (`float`, *可选*, 默认为1.0) — 用于乘以初始化范围的因子。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *可选*, 默认为0.02) — 用于初始化所有权重矩阵的truncated_normal_initializer的标准差。'
- en: '`is_vqa` (`bool`, *optional*, defaults to `False`) — Whether the model has
    been fine-tuned for VQA or not.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_vqa` (`bool`, *可选*, 默认为`False`) — 模型是否已经为VQA进行了微调。'
- en: '`kwargs` (*optional*) — Dictionary of keyword arguments.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*optional*) — 关键字参数的字典。'
- en: '[Pix2StructConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructConfig)
    is the configuration class to store the configuration of a [Pix2StructForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration).
    It is used to instantiate a Pix2Struct model according to the specified arguments,
    defining the text model and vision model configs. Instantiating a configuration
    with the defaults will yield a similar configuration to that of the Pix2Struct-base
    [google/pix2struct-base](https://huggingface.co/google/pix2struct-base) architecture.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[Pix2StructConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructConfig)
    是用于存储 [Pix2StructForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration)
    配置的配置类。根据指定的参数实例化 Pix2Struct 模型，定义文本模型和视觉模型配置。使用默认值实例化配置将产生类似于 Pix2Struct-base
    [google/pix2struct-base](https://huggingface.co/google/pix2struct-base) 架构的配置。'
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    的文档以获取更多信息。
- en: 'Example:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#### `from_text_vision_configs`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_text_vision_configs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/configuration_pix2struct.py#L378)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/configuration_pix2struct.py#L378)'
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Returns
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[Pix2StructConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructConfig)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[Pix2StructConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructConfig)'
- en: An instance of a configuration object
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象的一个实例
- en: Instantiate a [Pix2StructConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructConfig)
    (or a derived class) from pix2struct text model configuration and pix2struct vision
    model configuration.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从 pix2struct 文本模型配置和 pix2struct 视觉模型配置实例化一个 [Pix2StructConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructConfig)（或派生类）。
- en: Pix2StructTextConfig
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pix2StructTextConfig
- en: '### `class transformers.Pix2StructTextConfig`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Pix2StructTextConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/configuration_pix2struct.py#L33)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/configuration_pix2struct.py#L33)'
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 50244) — Vocabulary size of the
    `Pix2Struct` text model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [Pix2StructTextModel](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructTextModel).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 50244) — `Pix2Struct` 文本模型的词汇量。定义了在调用
    [Pix2StructTextModel](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructTextModel)
    时可以表示的不同标记数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化层的维度。'
- en: '`d_kv` (`int`, *optional*, defaults to 64) — Dimensionality of the key, query,
    value projections in each attention head.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_kv` (`int`, *optional*, defaults to 64) — 每个注意力头中键、查询、值投影的维度。'
- en: '`d_ff` (`int`, *optional*, defaults to 2048) — Dimensionality of the “intermediate”
    (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_ff` (`int`, *optional*, defaults to 2048) — Transformer 编码器中“中间”（即前馈）层的维度。'
- en: '`num_layers` (`int`, *optional*, defaults to 12) — Number of hidden layers
    in the Transformer encoder.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers` (`int`, *optional*, defaults to 12) — Transformer 编码器中的隐藏层数量。'
- en: '`num_heads` (`int`, *optional*, defaults to 12) — Number of attention heads
    for each attention layer in the Transformer encoder.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_heads` (`int`, *optional*, defaults to 12) — Transformer 编码器中每个注意力层的注意力头数量。'
- en: '`relative_attention_num_buckets` (`int`, *optional*, defaults to 32) — The
    number of buckets to use for each attention layer.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`relative_attention_num_buckets` (`int`, *optional*, defaults to 32) — 每个注意力层使用的桶数量。'
- en: '`relative_attention_max_distance` (`int`, *optional*, defaults to 128) — The
    maximum distance of the longer sequences for the bucket separation.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`relative_attention_max_distance` (`int`, *optional*, defaults to 128) — 用于桶分离的较长序列的最大距离。'
- en: '`dropout_rate` (`float`, *optional*, defaults to 0.1) — The dropout probabilitiy
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout_rate` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的丢弃概率。'
- en: '`layer_norm_epsilon` (`float`, *optional*, defaults to 1e-6) — The epsilon
    used by the layer normalization layers.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_epsilon` (`float`, *optional*, defaults to 1e-6) — 层归一化层使用的 epsilon。'
- en: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — A factor for
    initializing all weight matrices (should be kept to 1, used internally for initialization
    testing).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — 初始化所有权重矩阵的因子（应保持为
    1，用于内部初始化测试）。'
- en: '`dense_act_fn` (`Union[Callable, str]`, *optional*, defaults to `"gelu_new"`)
    — The non-linear activation function (function or string).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dense_act_fn` (`Union[Callable, str]`, *optional*, defaults to `"gelu_new"`)
    — 非线性激活函数（函数或字符串）。'
- en: '`decoder_start_token_id` (`int`, *optional*, defaults to 0) — The id of the
    `decoder_start_token_id` token.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_start_token_id` (`int`, *optional*, defaults to 0) — `decoder_start_token_id`
    标记的 id。'
- en: '`use_cache` (`bool`, *optional*, defaults to `False`) — Whether or not the
    model should return the last key/values attentions (not used by all models).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, defaults to `False`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。'
- en: '`pad_token_id` (`int`, *optional*, defaults to 0) — The id of the `padding`
    token.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *optional*, defaults to 0) — `padding` 标记的 id。'
- en: '`eos_token_id` (`int`, *optional*, defaults to 1) — The id of the `end-of-sequence`
    token.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`, *optional*, defaults to 1) — `end-of-sequence` 标记的 id。'
- en: This is the configuration class to store the configuration of a [Pix2StructTextModel](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructTextModel).
    It is used to instantiate a Pix2Struct text model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Pix2Struct text decoder used
    by the [google/pix2struct-base](https://huggingface.co/google/pix2struct-base)
    architecture.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[Pix2StructTextModel](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructTextModel)配置的配置类。根据指定的参数实例化Pix2Struct文本模型，定义模型架构。使用默认实例化配置将产生类似于Pix2Struct文本解码器的配置，该解码器由[google/pix2struct-base](https://huggingface.co/google/pix2struct-base)架构使用。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Pix2StructVisionConfig
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pix2StructVisionConfig
- en: '### `class transformers.Pix2StructVisionConfig`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Pix2StructVisionConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/configuration_pix2struct.py#L173)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/configuration_pix2struct.py#L173)'
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, 默认为768) — 编码器层和池化器层的维度。'
- en: '`patch_embed_hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality
    of the input patch_embedding layer in the Transformer encoder.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_embed_hidden_size` (`int`, *optional*, 默认为768) — Transformer编码器中输入patch_embedding层的维度。'
- en: '`d_ff` (`int`, *optional*, defaults to 2048) — Dimensionality of the “intermediate”
    (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_ff` (`int`, *optional*, 默认为2048) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`d_kv` (`int`, *optional*, defaults to 64) — Dimensionality of the key, query,
    value projections per attention head.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_kv` (`int`, *optional*, 默认为64) — 每个注意力头部的键、查询、值投影的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, 默认为12) — Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, 默认为12) — Transformer编码器中每个注意力层的注意力头数量。'
- en: '`dense_act_fn` (`str` or `function`, *optional*, defaults to `"gelu_new"`)
    — The non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` ``"gelu"` are supported.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dense_act_fn` (`str`或`function`, *optional*, 默认为`"gelu_new"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`
    `"gelu"`。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-06) — The epsilon used
    by the layer normalization layers.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, 默认为1e-06) — 层归一化层使用的epsilon。'
- en: '`dropout_rate` (`float`, *optional*, defaults to 0.0) — The dropout probabilitiy
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout_rate` (`float`, *optional*, 默认为0.0) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for the attention probabilities.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, 默认为0.0) — 注意力概率的dropout比率。'
- en: '`initializer_range` (`float`, *optional*, defaults to 1e-10) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, 默认为1e-10) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — A factor for
    initializing all weight matrices (should be kept to 1, used internally for initialization
    testing).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_factor` (`float`, *optional*, 默认为1.0) — 用于初始化所有权重矩阵的因子（应保持为1，用于内部初始化测试）。'
- en: '`seq_len` (`int`, *optional*, defaults to 4096) — Maximum sequence length (here
    number of patches) supported by the model.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seq_len` (`int`, *optional*, 默认为4096) — 模型支持的最大序列长度（这里是patch的数量）。'
- en: '`relative_attention_num_buckets` (`int`, *optional*, defaults to 32) — The
    number of buckets to use for each attention layer.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`relative_attention_num_buckets` (`int`, *optional*, 默认为32) — 每个注意力层使用的桶数量。'
- en: '`relative_attention_max_distance` (`int`, *optional*, defaults to 128) — The
    maximum distance (in tokens) to use for each attention layer.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`relative_attention_max_distance` (`int`, *optional*, 默认为128) — 每个注意力层使用的最大距离（以标记为单位）。'
- en: This is the configuration class to store the configuration of a [Pix2StructVisionModel](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructVisionModel).
    It is used to instantiate a Pix2Struct vision model according to the specified
    arguments, defining the model architecture. Instantiating a configuration defaults
    will yield a similar configuration to that of the Pix2Struct-base [google/pix2struct-base](https://huggingface.co/google/pix2struct-base)
    architecture.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[Pix2StructVisionModel](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructVisionModel)配置的配置类。根据指定的参数实例化Pix2Struct视觉模型，定义模型架构。使用默认实例化配置将产生类似于Pix2Struct-base
    [google/pix2struct-base](https://huggingface.co/google/pix2struct-base)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Pix2StructProcessor
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pix2StructProcessor
- en: '### `class transformers.Pix2StructProcessor`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Pix2StructProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/processing_pix2struct.py#L26)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/processing_pix2struct.py#L26)'
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image_processor` (`Pix2StructImageProcessor`) — An instance of [Pix2StructImageProcessor](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructImageProcessor).
    The image processor is a required input.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor` (`Pix2StructImageProcessor`) — 一个[Pix2StructImageProcessor](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructImageProcessor)的实例。图像处理器是必需的输入。'
- en: '`tokenizer` (Union[`T5TokenizerFast`, `T5Tokenizer`]) — An instance of [‘T5TokenizerFast`]
    or [‘T5Tokenizer`]. The tokenizer is a required input.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (Union[`T5TokenizerFast`, `T5Tokenizer`]) — 一个[‘T5TokenizerFast`]或[‘T5Tokenizer`]的实例。Tokenizer是必需的输入。'
- en: Constructs a PIX2STRUCT processor which wraps a BERT tokenizer and PIX2STRUCT
    image processor into a single processor.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个PIX2STRUCT处理器，将BERT tokenizer和PIX2STRUCT图像处理器封装成一个处理器。
- en: '[Pix2StructProcessor](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructProcessor)
    offers all the functionalities of [Pix2StructImageProcessor](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructImageProcessor)
    and [T5TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/mt5#transformers.T5TokenizerFast).
    See the docstring of `__call__()` and [decode()](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructProcessor.decode)
    for more information.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[Pix2StructProcessor](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructProcessor)提供了[Pix2StructImageProcessor](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructImageProcessor)和[T5TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/mt5#transformers.T5TokenizerFast)的所有功能。查看`__call__()`和[decode()](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructProcessor.decode)的文档字符串以获取更多信息。'
- en: '#### `batch_decode`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/processing_pix2struct.py#L145)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/processing_pix2struct.py#L145)'
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This method forwards all its arguments to Pix2StructTokenizerFast’s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法将所有参数转发给Pix2StructTokenizerFast的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。请参考此方法的文档字符串以获取更多信息。
- en: '#### `decode`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/processing_pix2struct.py#L152)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/processing_pix2struct.py#L152)'
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This method forwards all its arguments to Pix2StructTokenizerFast’s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法将所有参数转发给Pix2StructTokenizerFast的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。请参考此方法的文档字符串以获取更多信息。
- en: Pix2StructImageProcessor
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pix2StructImageProcessor
- en: '### `class transformers.Pix2StructImageProcessor`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Pix2StructImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/image_processing_pix2struct.py#L188)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/image_processing_pix2struct.py#L188)'
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`do_convert_rgb` (`bool`, *optional*, defaults to `True`) — Whether to convert
    the image to RGB.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_convert_rgb` (`bool`, *可选*, 默认为 `True`) — 是否将图像转换为RGB格式。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the image. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method. According to Pix2Struct paper and code, the image is normalized with its
    own mean and standard deviation.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *可选*, 默认为 `True`) — 是否对图像进行归一化。可以被`preprocess`方法中的`do_normalize`参数覆盖。根据Pix2Struct论文和代码，图像使用自己的均值和标准差进行归一化。'
- en: '`patch_size` (`Dict[str, int]`, *optional*, defaults to `{"height" -- 16, "width":
    16}`): The patch size to use for the image. According to Pix2Struct paper and
    code, the patch size is 16x16.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`Dict[str, int]`, *可选*, 默认为 `{"height" -- 16, "width": 16}`):
    用于图像的补丁大小。根据Pix2Struct论文和代码，补丁大小为16x16。'
- en: '`max_patches` (`int`, *optional*, defaults to 2048) — The maximum number of
    patches to extract from the image as per the [Pix2Struct paper](https://arxiv.org/pdf/2210.03347.pdf).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_patches` (`int`, *可选*, 默认为 2048) — 从图像中提取的最大补丁数，根据[Pix2Struct论文](https://arxiv.org/pdf/2210.03347.pdf)。'
- en: '`is_vqa` (`bool`, *optional*, defaults to `False`) — Whether or not the image
    processor is for the VQA task. If `True` and `header_text` is passed in, text
    is rendered onto the input images.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_vqa` (`bool`, *可选*, 默认为 `False`) — 图像处理器是否用于VQA任务。如果为`True`并且传入了`header_text`，则文本将呈现在输入图像上。'
- en: Constructs a Pix2Struct image processor.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个Pix2Struct图像处理器。
- en: '#### `preprocess`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/image_processing_pix2struct.py#L347)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/image_processing_pix2struct.py#L347)'
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`ImageInput`) — Image to preprocess. Expects a single or batch of
    images.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) — 要预处理的图像。期望单个图像或图像批次。'
- en: '`header_text` (`Union[List[str], str]`, *optional*) — Text to render as a header.
    Only has an effect if `image_processor.is_vqa` is `True`.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`header_text` (`Union[List[str], str]`, *可选*) — 要呈现为标题的文本。仅当`image_processor.is_vqa`为`True`时才有效。'
- en: '`do_convert_rgb` (`bool`, *optional*, defaults to `self.do_convert_rgb`) —
    Whether to convert the image to RGB.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_convert_rgb`（`bool`，*可选*，默认为 `self.do_convert_rgb`）— 是否将图像转换为 RGB。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — Whether
    to normalize the image.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize`（`bool`，*可选*，默认为 `self.do_normalize`）— 是否对图像进行标准化。'
- en: '`max_patches` (`int`, *optional*, defaults to `self.max_patches`) — Maximum
    number of patches to extract.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_patches`（`int`，*可选*，默认为 `self.max_patches`）— 要提取的最大补丁数。'
- en: '`patch_size` (`dict`, *optional*, defaults to `self.patch_size`) — Dictionary
    containing the patch height and width.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size`（`dict`，*可选*，默认为 `self.patch_size`）— 包含补丁高度和宽度的字典。'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`（`str` 或 `TensorType`，*可选*）— 要返回的张量类型。可以是以下之一：'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未设置：返回一个 `np.ndarray` 列表。
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW` 或 `''tf''`：返回类型为 `tf.Tensor` 的批次。'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH` 或 `''pt''`：返回类型为 `torch.Tensor` 的批次。'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY` 或 `''np''`：返回类型为 `np.ndarray` 的批次。'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX` 或 `''jax''`：返回类型为 `jax.numpy.ndarray` 的批次。'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format`（`ChannelDimension` 或 `str`，*可选*，默认为 `ChannelDimension.FIRST`）—
    输出图像的通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` 或 `ChannelDimension.FIRST`：图像以 (num_channels, height, width)
    格式表示。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` 或 `ChannelDimension.LAST`：图像以 (height, width, num_channels)
    格式表示。'
- en: 'Unset: Use the channel dimension format of the input image.'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未设置：使用输入图像的通道维度格式。
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format`（`ChannelDimension` 或 `str`，*可选*）— 输入图像的通道维度格式。如果未设置，将从输入图像中推断通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` 或 `ChannelDimension.FIRST`：图像以 (num_channels, height, width)
    格式表示。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` 或 `ChannelDimension.LAST`：图像以 (height, width, num_channels)
    格式表示。'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"` 或 `ChannelDimension.NONE`：图像以 (height, width) 格式表示。'
- en: Preprocess an image or batch of images. The processor first computes the maximum
    possible number of aspect-ratio preserving patches of size `patch_size` that can
    be extracted from the image. It then pads the image with zeros to make the image
    respect the constraint of `max_patches`. Before extracting the patches the images
    are standardized following the tensorflow implementation of `per_image_standardization`
    ([https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization](https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization)).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理图像或图像批次。处理器首先计算可以从图像中提取的保持纵横比的大小为 `patch_size` 的最大可能数量的补丁。然后，它使用零填充图像，使图像遵守
    `max_patches` 的约束。在提取补丁之前，图像将按照 tensorflow 的 `per_image_standardization` 实现进行标准化（[https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization](https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization)）。
- en: Pix2StructTextModel
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pix2StructTextModel
- en: '### `class transformers.Pix2StructTextModel`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Pix2StructTextModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L1302)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L1302)'
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` (Union[`Pix2StructConfig`, `Pix2StructTextConfig`]) — Model configuration
    class with all the parameters of the model. Initializing with a config file does
    not load the weights associated with the model, only the configuration. Check
    out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（Union[`Pix2StructConfig`, `Pix2StructTextConfig`]）— 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看
    [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The standalone text decoder of Pix2Struct
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Pix2Struct 的独立文本解码器
- en: 'The Pix2Struct model was proposed in [Pix2Struct: Screenshot Parsing as Pretraining
    for Visual Language Understanding](https://arxiv.org/abs/2210.03347) by Kenton
    Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi
    Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova. It’s an encoder decoder
    transformer pre-trained in a image-to-text setting.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pix2Struct 模型由 Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu,
    Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova
    在 [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://arxiv.org/abs/2210.03347)
    中提出。它是一个在图像到文本设置下进行预训练的编码器解码器 transformer 模型。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以了解所有与一般用法和行为相关的事项。
- en: '#### `forward`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L1371)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L1371)'
- en: '[PRE13]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Pix2StructText is a model
    with relative position embeddings so you should be able to pad the inputs on both
    the right and the left.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    词汇表中输入序列标记的索引。Pix2StructText 是一个带有相对位置嵌入的模型，因此您应该能够在右侧和左侧填充输入。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for detail.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: To know more on how to prepare `input_ids` for pretraining take a look a [Pix2StructText
    Training](./t5#training).
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要了解有关如何为预训练准备`input_ids`的更多信息，请查看[Pix2StructText Training](./t5#training)。
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示未被`屏蔽`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被`屏蔽`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — 词汇表中解码器输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是解码器输入ID？](../glossary#decoder-input-ids)'
- en: Pix2StructText uses the `pad_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Pix2StructText 使用`pad_token_id`作为`decoder_input_ids`生成的起始标记。如果使用`past_key_values`，可选择仅输入最后的`decoder_input_ids`（参见`past_key_values`）。
- en: To know more on how to prepare `decoder_input_ids` for pretraining take a look
    at [Pix2StructText Training](./t5#training).
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要了解有关如何为预训练准备`decoder_input_ids`的更多信息，请查看[Pix2StructText Training](./t5#training)。
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules in
    the encoder. Mask values selected in `[0, 1]`:'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 编码器中自注意力模块中选择性屏蔽头部的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被`屏蔽`，
- en: 0 indicates the head is `masked`.
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被`屏蔽`。
- en: '`decoder_head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules in the decoder. Mask values selected in `[0, 1]`:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — 解码器中自注意力模块中选择性屏蔽头部的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被`屏蔽`。
- en: 0 indicates the head is `masked`.
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被`屏蔽`。
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the cross-attention
    modules in the decoder. Mask values selected in `[0, 1]`:'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask` (`torch.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — 解码器中交叉注意力模块中选择性屏蔽头部的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被`屏蔽`。
- en: 0 indicates the head is `masked`.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被`屏蔽`。
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, `optional`: *hidden_states*, `optional`: *attentions*)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` is a
    sequence of hidden states at the output of the last layer of the encoder. Used
    in the cross-attention of the decoder.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组包括（`last_hidden_state`，可选：*hidden_states*，可选：*attentions*）`last_hidden_state`
    的形状为 `(batch_size, sequence_length, hidden_size)`，是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden states
    of the attention layers. Can be used to speed up decoding.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`，长度为`config.n_layers`，每个元组有4个形状为`(batch_size,
    num_heads, sequence_length - 1, embed_size_per_head)`的张量） — 包含注意力层的预计算键和值隐藏状态。可用于加速解码。'
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，用户可以选择仅输入形状为`(batch_size, 1)`的最后一个`decoder_input_ids`（那些没有将它们的过去键值状态提供给此模型的）而不是形状为`(batch_size,
    sequence_length)`的所有`decoder_input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*)
    — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, target_sequence_length,
    hidden_size)`，*可选*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用`past_key_values`，可以选择仅输入最后一个`decoder_inputs_embeds`（请参阅`past_key_values`）。如果您想要更多控制如何将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds`
    takes the value of `inputs_embeds`.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（请参阅`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)
    or `tuple(torch.FloatTensor)`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Pix2StructConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructConfig))
    and inputs.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`，或者当`config.return_dict=False`时）包含根据配置（[Pix2StructConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 语言建模损失（用于下一个标记预测）。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层的输出，则为一个
    + 每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层模型的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Cross attentions weights after the attention softmax, used to compute the weighted
    average in the cross-attention heads.
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自注意力模块中注意力softmax后的交叉注意力权重，用于计算交叉注意力头中的加权平均值。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `torch.FloatTensor`
    tuples of length `config.n_layers`, with each tuple containing the cached key,
    value states of the self-attention and the cross-attention layers if model is
    used in encoder-decoder setting. Only relevant if `config.is_decoder = True`.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回
    — 长度为`config.n_layers`的`torch.FloatTensor`元组，每个元组包含自注意力和交叉注意力层的缓存键、值状态，如果模型在编码器-解码器设置中使用，则相关。仅在`config.is_decoder
    = True`时相关。'
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（注意力块中的键和值），可用于加速顺序解码（请参阅`past_key_values`输入）。
- en: The [Pix2StructTextModel](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructTextModel)
    forward method, overrides the `__call__` special method.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[Pix2StructTextModel](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructTextModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Pix2StructVisionModel
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pix2StructVisionModel
- en: '### `class transformers.Pix2StructVisionModel`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Pix2StructVisionModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L537)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L537)'
- en: '[PRE15]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Pix2StructConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[Pix2StructConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructConfig)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare Pix2StructVision Model transformer outputting raw hidden-states without
    any specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的Pix2StructVision Model变压器输出原始隐藏状态，没有特定的头部。这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L570)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L570)'
- en: '[PRE16]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`flattened_patches` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_channels x patch_height x patch_width)`) — Flattened and padded pixel values.
    These values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `Pix2StructVisionImageProcessor.__call__` for details. Check the [original
    paper](https://arxiv.org/abs/2210.03347) (figure 5) for more details.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flattened_patches` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    num_channels x patch_height x patch_width)`） — 扁平化和填充的像素值。这些值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获得。有关详细信息，请参阅`Pix2StructVisionImageProcessor.__call__`。查看[原始论文](https://arxiv.org/abs/2210.03347)（图5）以获取更多详细信息。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding pixel values. Mask
    values selected in `[0, 1]`:'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*optional*)
    — 用于避免在填充像素值上执行注意力的掩码。掩码值选择在`[0, 1]`之间：'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*)
    — 用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部是`not masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部是`masked`。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）-是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Pix2StructConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructConfig))
    and inputs.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（[Pix2StructConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructConfig)）和输入的不同元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）-模型最后一层输出的隐藏状态序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）-序列第一个标记（分类标记）的最后一层隐藏状态，在通过用于辅助预训练任务的层进一步处理后。例如，对于BERT系列模型，这将返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-形状为`(batch_size,
    sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-形状为`(batch_size,
    num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [Pix2StructVisionModel](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructVisionModel)
    forward method, overrides the `__call__` special method.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[Pix2StructVisionModel](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructVisionModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE17]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Pix2StructForConditionalGeneration
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pix2StructForConditionalGeneration
- en: '### `class transformers.Pix2StructForConditionalGeneration`'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Pix2StructForConditionalGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L1574)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L1574)'
- en: '[PRE18]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` (Union[`Pix2StructConfig`, `Pix2StructTextConfig`]) — Model configuration
    class with all the parameters of the model. Initializing with a config file does
    not load the weights associated with the model, only the configuration. Check
    out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（Union[`Pix2StructConfig`，`Pix2StructTextConfig`]）-模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: A conditional generation model with a language modeling head. Can be used for
    sequence generation tasks.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 具有语言建模头的条件生成模型。可用于序列生成任务。
- en: 'The Pix2Struct model was proposed in [Pix2Struct: Screenshot Parsing as Pretraining
    for Visual Language Understanding](https://arxiv.org/abs/2210.03347) by Kenton
    Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi
    Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova. It’s an encoder decoder
    transformer pre-trained in a image-to-text setting.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pix2Struct模型是由Kenton Lee，Mandar Joshi，Iulia Turc，Hexiang Hu，Fangyu Liu，Julian
    Eisenschlos，Urvashi Khandelwal，Peter Shaw，Ming-Wei Chang，Kristina Toutanova在[Pix2Struct:
    Screenshot Parsing as Pretraining for Visual Language Understanding](https://arxiv.org/abs/2210.03347)中提出的。它是在图像到文本设置中预训练的编码器解码器变换器。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L1620)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pix2struct/modeling_pix2struct.py#L1620)'
- en: '[PRE19]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`flattened_patches` (`torch.FloatTensor` of shape `(batch_size, seq_length,
    hidden_size)`) — Flattened pixel patches. the `hidden_size` is obtained by the
    following formula: `hidden_size` = `num_channels` *`patch_size`* `patch_size`'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flattened_patches`（形状为`(batch_size, seq_length, hidden_size)`的`torch.FloatTensor`）—
    扁平化的像素块。`hidden_size`通过以下公式获得：`hidden_size` = `num_channels` * `patch_size` *
    `patch_size`'
- en: The process of flattening the pixel patches is done by `Pix2StructProcessor`.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 扁平化像素块的过程由`Pix2StructProcessor`完成。
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。掩码值选定在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示“未被掩盖”的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被“掩盖”的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）—
    词汇表中解码器输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器输入ID是什么？
- en: Pix2StructText uses the `pad_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Pix2StructText使用`pad_token_id`作为`decoder_input_ids`生成的起始标记。如果使用了`past_key_values`，则可以选择仅输入最后的`decoder_input_ids`（请参阅`past_key_values`）。
- en: To know more on how to prepare `decoder_input_ids` for pretraining take a look
    at [Pix2StructText Training](./t5#training).
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要了解有关如何为预训练准备`decoder_input_ids`的更多信息，请查看[Pix2StructText Training](./t5#training)。
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）—
    默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules in
    the encoder. Mask values selected in `[0, 1]`:'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）—
    用于将编码器中自注意力模块的选定头部置零的掩码。掩码值选定在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被“掩盖”，
- en: 0 indicates the head is `masked`.
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被“掩盖”。
- en: '`decoder_head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules in the decoder. Mask values selected in `[0, 1]`:'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）—
    用于将解码器中自注意力模块的选定头部置零的掩码。掩码值选定在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被“掩盖”，
- en: 0 indicates the head is `masked`.
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被“掩盖”。
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the cross-attention
    modules in the decoder. Mask values selected in `[0, 1]`:'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.Tensor`，*可选*）—
    用于将解码器中交叉注意力模块的选定头部置零的掩码。掩码值选定在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被“掩盖”，
- en: 0 indicates the head is `masked`.
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被“掩盖”。
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, `optional`: *hidden_states*, `optional`: *attentions*)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` is a
    sequence of hidden states at the output of the last layer of the encoder. Used
    in the cross-attention of the decoder.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）— 元组包括（`last_hidden_state`，*可选*：*hidden_states*，*可选*：*attentions*）`last_hidden_state`的形状为`(batch_size,
    sequence_length, hidden_size)`，是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden states
    of the attention layers. Can be used to speed up decoding.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（长度为`config.n_layers`的元组（元组（torch.FloatTensor）））- 包含注意力层的预计算键和值隐藏状态。可用于加速解码。'
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（那些没有将其过去的键值状态提供给此模型的）的形状为`(batch_size,
    1)`，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds`（`torch.FloatTensor`，形状为`(batch_size, target_sequence_length,
    hidden_size)`，*可选*）- 可选地，您可以选择直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用了`past_key_values`，则可以选择仅输入最后的`decoder_inputs_embeds`（参见`past_key_values`）。如果您想要更多控制如何将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds`
    takes the value of `inputs_embeds`.
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss for the decoder.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）- 用于计算解码器的掩码语言建模损失的标签。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*）- 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（请参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: Returns
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.Seq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Seq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.Seq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Pix2StructConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructConfig))
    and inputs.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.Seq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（[Pix2StructConfig](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructConfig)）和输入的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the decoder of the model.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`）-
    模型解码器最后一层的隐藏状态序列。'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，则输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）-
    长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（请参见`past_key_values`输入）。
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    `torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）的形状为`(batch_size, sequence_length,
    hidden_size)`。'
- en: Hidden-states of the decoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+
    一个用于每一层的输出）。'
- en: Hidden-states of the encoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [Pix2StructForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[Pix2StructForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传播的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: 'Inference:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 推理：
- en: '[PRE20]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Training:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 训练：
- en: '[PRE21]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
