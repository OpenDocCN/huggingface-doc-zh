["```py\n>>> from transformers import AutoModelForCausalLM\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n>>> model.generation_config\nGenerationConfig {\n    \"bos_token_id\": 50256,\n    \"eos_token_id\": 50256,\n}\n```", "```py\n>>> my_model.generate(**inputs, num_beams=4, do_sample=True)\n```", "```py\n>>> from transformers import AutoModelForCausalLM, GenerationConfig\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"my_account/my_model\")\n>>> generation_config = GenerationConfig(\n...     max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id\n... )\n>>> generation_config.save_pretrained(\"my_account/my_model\", push_to_hub=True)\n```", "```py\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n\n>>> translation_generation_config = GenerationConfig(\n...     num_beams=4,\n...     early_stopping=True,\n...     decoder_start_token_id=0,\n...     eos_token_id=model.config.eos_token_id,\n...     pad_token=model.config.pad_token_id,\n... )\n\n>>> # Tip: add `push_to_hub=True` to push to the Hub\n>>> translation_generation_config.save_pretrained(\"/tmp\", \"translation_generation_config.json\")\n\n>>> # You could then use the named generation config file to parameterize generation\n>>> generation_config = GenerationConfig.from_pretrained(\"/tmp\", \"translation_generation_config.json\")\n>>> inputs = tokenizer(\"translate English to French: Configuration files are easy to use!\", return_tensors=\"pt\")\n>>> outputs = model.generate(**inputs, generation_config=generation_config)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['Les fichiers de configuration sont faciles \u00e0 utiliser!']\n```", "```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n\n>>> tok = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n>>> inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n>>> streamer = TextStreamer(tok)\n\n>>> # Despite returning the usual output, the streamer will also print the generated text to stdout.\n>>> _ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)\nAn increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,\n```", "```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> prompt = \"I look forward to\"\n>>> checkpoint = \"distilgpt2\"\n\n>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n>>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n>>> outputs = model.generate(**inputs)\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n['I look forward to seeing you all again!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n']\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM\n\n>>> checkpoint = \"gpt2-large\"\n>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n\n>>> prompt = \"Hugging Face Company is\"\n>>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> outputs = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_new_tokens=100)\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n['Hugging Face Company is a family owned and operated business. We pride ourselves on being the best\nin the business and our customer service is second to none.\\n\\nIf you have any questions about our\nproducts or services, feel free to contact us at any time. We look forward to hearing from you!']\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n>>> set_seed(0)  # For reproducibility\n\n>>> checkpoint = \"gpt2-large\"\n>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n\n>>> prompt = \"Today was an amazing day because\"\n>>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n['Today was an amazing day because when you go to the World Cup and you don\\'t, or when you don\\'t get invited,\nthat\\'s a terrible feeling.\"']\n```", "```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> prompt = \"It is astonishing how one can\"\n>>> checkpoint = \"gpt2-medium\"\n\n>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n>>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n\n>>> outputs = model.generate(**inputs, num_beams=5, max_new_tokens=50)\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n['It is astonishing how one can have such a profound impact on the lives of so many people in such a short period of\ntime.\"\\n\\nHe added: \"I am very proud of the work I have been able to do in the last few years.\\n\\n\"I have']\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, set_seed\n>>> set_seed(0)  # For reproducibility\n\n>>> prompt = \"translate English to German: The house is wonderful.\"\n>>> checkpoint = \"t5-small\"\n\n>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n>>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n\n>>> outputs = model.generate(**inputs, num_beams=5, do_sample=True)\n>>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n'Das Haus ist wunderbar.'\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n>>> checkpoint = \"google/pegasus-xsum\"\n>>> prompt = (\n...     \"The Permaculture Design Principles are a set of universal design principles \"\n...     \"that can be applied to any location, climate and culture, and they allow us to design \"\n...     \"the most efficient and sustainable human habitation and food production systems. \"\n...     \"Permaculture is a design system that encompasses a wide variety of disciplines, such \"\n...     \"as ecology, landscape design, environmental science and energy conservation, and the \"\n...     \"Permaculture design principles are drawn from these various disciplines. Each individual \"\n...     \"design principle itself embodies a complete conceptual framework based on sound \"\n...     \"scientific principles. When we bring all these separate  principles together, we can \"\n...     \"create a design system that both looks at whole systems, the parts that these systems \"\n...     \"consist of, and how those parts interact with each other to create a complex, dynamic, \"\n...     \"living system. Each design principle serves as a tool that allows us to integrate all \"\n...     \"the separate parts of a design, referred to as elements, into a functional, synergistic, \"\n...     \"whole system, where the elements harmoniously interact and work together in the most \"\n...     \"efficient way possible.\"\n... )\n\n>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n>>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n\n>>> outputs = model.generate(**inputs, num_beams=5, num_beam_groups=5, max_new_tokens=30, diversity_penalty=1.0)\n>>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n'The Design Principles are a set of universal design principles that can be applied to any location, climate and\nculture, and they allow us to design the'\n```", "```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> prompt = \"Alice and Bob\"\n>>> checkpoint = \"EleutherAI/pythia-1.4b-deduped\"\n>>> assistant_checkpoint = \"EleutherAI/pythia-160m-deduped\"\n\n>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n>>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n>>> assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)\n>>> outputs = model.generate(**inputs, assistant_model=assistant_model)\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']\n```", "```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n>>> set_seed(42)  # For reproducibility\n\n>>> prompt = \"Alice and Bob\"\n>>> checkpoint = \"EleutherAI/pythia-1.4b-deduped\"\n>>> assistant_checkpoint = \"EleutherAI/pythia-160m-deduped\"\n\n>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n>>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n>>> assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)\n>>> outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.5)\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n['Alice and Bob are going to the same party. It is a small party, in a small']\n```"]