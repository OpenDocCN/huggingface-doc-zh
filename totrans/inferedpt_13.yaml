- en: Send Requests to Endpoints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/inference-endpoints/guides/test_endpoint](https://huggingface.co/docs/inference-endpoints/guides/test_endpoint)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: You can send requests to Inference Endpoints using the UI leveraging the Inference
    Widget or programmatically, e.g. with cURL, `@huggingface/inference`, `huggingface_hub`
    or any REST client. The Endpoint overview not only provides a interactive widget
    for you to test the Endpoint, but also generates code for `python`, `javascript`
    and `curl`. You can use this code to quickly get started with your Endpoint in
    your favorite programming language.
  prefs: []
  type: TYPE_NORMAL
- en: Below are also examples on how to use the `@huggingface/inference` library to
    call an inference endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Use the UI to send requests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Endpoint overview provides access to the Inference Widget which can be used
    to send requests (see step 6 of [Create an Endpoint](/docs/inference-endpoints/guides/create_endpoint)).
    This allows you to quickly test your Endpoint with different inputs and share
    it with team members.
  prefs: []
  type: TYPE_NORMAL
- en: Use cURL to send requests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The cURL command for the request above should look like this. Youâ€™ll need to
    provide your user token which can be found in your Hugging Face [account settings](https://huggingface.co/settings/tokens):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example Request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The Endpoints API offers the same API definitions as the [Inference API](https://huggingface.co/docs/api-inference/detailed_parameters)
    and the [SageMaker Inference Toolkit](https://huggingface.co/docs/sagemaker/reference#inference-toolkit-api).
    All the request payloads are documented in the [Supported Tasks](/docs/inference-endpoints/supported_tasks)
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means for an NLP task, the payload is represented as the `inputs` key
    and additional pipeline parameters are included in the `parameters` key. You can
    provide any of the supported `kwargs` from [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)
    as parameters. For image or audio tasks, you should send the data as a binary
    request with the corresponding mime type. Below is an example cURL for an audio
    payload:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To use your cURL command as code, use the [cURL Converter](https://curlconverter.com/)
    tool to quickly get started with the programming language of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: Use javascript library @huggingface/inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use the javascript library to call an inference endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Custom handler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`@huggingface/inference` supports tasks from [https://huggingface.co/tasks](https://huggingface.co/tasks),
    and is typed accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If your model has additional inputs, or even custom inputs / outputs you can
    use the more generic `.request` / `streamingRequest`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
