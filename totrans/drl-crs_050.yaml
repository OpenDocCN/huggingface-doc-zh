- en: Quiz
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测验
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit3/quiz](https://huggingface.co/learn/deep-rl-course/unit3/quiz)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/learn/deep-rl-course/unit3/quiz](https://huggingface.co/learn/deep-rl-course/unit3/quiz)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf)
    **is to test yourself.** This will help you to find **where you need to reinforce
    your knowledge**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 学习和[避免自以为是的错觉](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf)的最佳方法**是测试自己**。这将帮助您找到**需要加强知识的地方**。
- en: 'Q1: We mentioned Q Learning is a tabular method. What are tabular methods?'
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q1: 我们提到 Q 学习是一种表格方法。什么是表格方法？'
- en: <details data-svelte-h="svelte-gsebop"><summary>Solution</summary>
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-gsebop"><summary>解决方案</summary>
- en: '*Tabular methods* is a type of problem in which the state and actions spaces
    are small enough to approximate value functions to be **represented as arrays
    and tables**. For instance, **Q-Learning is a tabular method** since we use a
    table to represent the state, and action value pairs.</details>'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*表格方法*是一种问题类型，其中状态和动作空间足够小，可以将值函数近似为**数组和表**。例如，**Q 学习是一种表格方法**，因为我们使用表来表示状态和动作值对。</details>'
- en: 'Q2: Why can’t we use a classical Q-Learning to solve an Atari Game?'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q2: 为什么我们不能使用经典的 Q 学习来解决 Atari 游戏？'
- en: 'Q3: Why do we stack four frames together when we use frames as input in Deep
    Q-Learning?'
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q3: 当我们在深度 Q 学习中将四个帧堆叠在一起时，为什么我们要这样做？'
- en: <details data-svelte-h="svelte-nzbq54"><summary>Solution</summary>
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-nzbq54"><summary>解决方案</summary>
- en: 'We stack frames together because it helps us **handle the problem of temporal
    limitation**: one frame is not enough to capture temporal information. For instance,
    in pong, our agent **will be unable to know the ball direction if it gets only
    one frame**.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将帧堆叠在一起，因为这有助于我们**处理时间限制的问题**：一个帧不足以捕捉时间信息。例如，在乒乓球中，如果我们只有一个帧，我们的代理**将无法知道球的方向**。
- en: '![Temporal limitation](../Images/916225d18ad696514245f8c4e88a5a56.png) ![Temporal
    limitation](../Images/e35a3e3cfeefe6f7a16b681ab91dfa7b.png)</details>'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![时间限制](../Images/916225d18ad696514245f8c4e88a5a56.png) ![时间限制](../Images/e35a3e3cfeefe6f7a16b681ab91dfa7b.png)</details>'
- en: 'Q4: What are the two phases of Deep Q-Learning?'
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q4: 深度 Q 学习的两个阶段是什么？'
- en: 'Q5: Why do we create a replay memory in Deep Q-Learning?'
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q5: 为什么我们在深度 Q 学习中创建重放内存？'
- en: <details data-svelte-h="svelte-10rxkt3"><summary>Solution</summary>
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-10rxkt3"><summary>解决方案</summary>
- en: '**1\. Make more efficient use of the experiences during the training**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 在训练期间更有效地利用经验**'
- en: Usually, in online reinforcement learning, the agent interacts in the environment,
    gets experiences (state, action, reward, and next state), learns from them (updates
    the neural network), and discards them. This is not efficient. But, with experience
    replay, **we create a replay buffer that saves experience samples that we can
    reuse during the training**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在在线强化学习中，代理与环境互动，获得经验（状态、动作、奖励和下一个状态），从中学习（更新神经网络），然后丢弃它们。这并不高效。但是，通过经验重放，**我们创建一个保存经验样本的重放缓冲区，可以在训练期间重复使用**。
- en: '**2\. Avoid forgetting previous experiences and reduce the correlation between
    experiences**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 避免忘记先前的经验并减少经验之间的相关性**'
- en: The problem we get if we give sequential samples of experiences to our neural
    network is that it **tends to forget the previous experiences as it overwrites
    new experiences**. For instance, if we are in the first level and then the second,
    which is different, our agent can forget how to behave and play in the first level.</details>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将经验的顺序样本提供给我们的神经网络，问题就会出现，因为它**倾向于忘记先前的经验，因为它会覆盖新的经验**。例如，如果我们在第一关然后是不同的第二关，我们的代理可能会忘记如何在第一关中行为和玩耍。</details>
- en: 'Q6: How do we use Double Deep Q-Learning?'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Q6: 我们如何使用双重深度 Q 学习？'
- en: <details data-svelte-h="svelte-1u03gxl"><summary>Solution</summary>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: <details data-svelte-h="svelte-1u03gxl"><summary>解决方案</summary>
- en: 'When we compute the Q target, we use two networks to decouple the action selection
    from the target Q value generation. We:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们计算 Q 目标时，我们使用两个网络来解耦动作选择和目标 Q 值生成。我们：
- en: Use our *DQN network* to **select the best action to take for the next state**
    (the action with the highest Q value).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用我们的*DQN 网络*来**选择下一个状态要采取的最佳行动**（具有最高 Q 值的行动）。
- en: Use our *Target network* to calculate **the target Q value of taking that action
    at the next state**.</details>
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用我们的*目标网络*来计算**在下一个状态采取该动作的目标 Q 值**。</details>
- en: Congrats on finishing this Quiz 🥳, if you missed some elements, take time to
    read again the chapter to reinforce (😏) your knowledge.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜您完成了这个测验🥳，如果您错过了一些元素，请花时间再次阅读章节，以加强（😏）您的知识。
