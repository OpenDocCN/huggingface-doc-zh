["```py\n( prior: PriorTransformer image_encoder: CLIPVisionModelWithProjection text_encoder: CLIPTextModelWithProjection tokenizer: CLIPTokenizer scheduler: UnCLIPScheduler image_processor: CLIPImageProcessor )\n```", "```py\n( prompt: Union negative_prompt: Union = None num_images_per_prompt: int = 1 num_inference_steps: int = 25 generator: Union = None latents: Optional = None guidance_scale: float = 4.0 output_type: Optional = 'pt' return_dict: bool = True ) \u2192 export const metadata = 'undefined';KandinskyPriorPipelineOutput or tuple\n```", "```py\n>>> from diffusers import KandinskyPipeline, KandinskyPriorPipeline\n>>> import torch\n\n>>> pipe_prior = KandinskyPriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1-prior\")\n>>> pipe_prior.to(\"cuda\")\n\n>>> prompt = \"red cat, 4k photo\"\n>>> out = pipe_prior(prompt)\n>>> image_emb = out.image_embeds\n>>> negative_image_emb = out.negative_image_embeds\n\n>>> pipe = KandinskyPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\")\n>>> pipe.to(\"cuda\")\n\n>>> image = pipe(\n...     prompt,\n...     image_embeds=image_emb,\n...     negative_image_embeds=negative_image_emb,\n...     height=768,\n...     width=768,\n...     num_inference_steps=100,\n... ).images\n\n>>> image[0].save(\"cat.png\")\n```", "```py\n( images_and_prompts: List weights: List num_images_per_prompt: int = 1 num_inference_steps: int = 25 generator: Union = None latents: Optional = None negative_prior_prompt: Optional = None negative_prompt: str = '' guidance_scale: float = 4.0 device = None ) \u2192 export const metadata = 'undefined';KandinskyPriorPipelineOutput or tuple\n```", "```py\n>>> from diffusers import KandinskyPriorPipeline, KandinskyPipeline\n>>> from diffusers.utils import load_image\n>>> import PIL\n\n>>> import torch\n>>> from torchvision import transforms\n\n>>> pipe_prior = KandinskyPriorPipeline.from_pretrained(\n...     \"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16\n... )\n>>> pipe_prior.to(\"cuda\")\n\n>>> img1 = load_image(\n...     \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n...     \"/kandinsky/cat.png\"\n... )\n\n>>> img2 = load_image(\n...     \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n...     \"/kandinsky/starry_night.jpeg\"\n... )\n\n>>> images_texts = [\"a cat\", img1, img2]\n>>> weights = [0.3, 0.3, 0.4]\n>>> image_emb, zero_image_emb = pipe_prior.interpolate(images_texts, weights)\n\n>>> pipe = KandinskyPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16)\n>>> pipe.to(\"cuda\")\n\n>>> image = pipe(\n...     \"\",\n...     image_embeds=image_emb,\n...     negative_image_embeds=zero_image_emb,\n...     height=768,\n...     width=768,\n...     num_inference_steps=150,\n... ).images[0]\n\n>>> image.save(\"starry_cat.png\")\n```", "```py\n( text_encoder: MultilingualCLIP tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: Union movq: VQModel )\n```", "```py\n( prompt: Union image_embeds: Union negative_image_embeds: Union negative_prompt: Union = None height: int = 512 width: int = 512 num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) \u2192 export const metadata = 'undefined';ImagePipelineOutput or tuple\n```", "```py\n>>> from diffusers import KandinskyPipeline, KandinskyPriorPipeline\n>>> import torch\n\n>>> pipe_prior = KandinskyPriorPipeline.from_pretrained(\"kandinsky-community/Kandinsky-2-1-prior\")\n>>> pipe_prior.to(\"cuda\")\n\n>>> prompt = \"red cat, 4k photo\"\n>>> out = pipe_prior(prompt)\n>>> image_emb = out.image_embeds\n>>> negative_image_emb = out.negative_image_embeds\n\n>>> pipe = KandinskyPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\")\n>>> pipe.to(\"cuda\")\n\n>>> image = pipe(\n...     prompt,\n...     image_embeds=image_emb,\n...     negative_image_embeds=negative_image_emb,\n...     height=768,\n...     width=768,\n...     num_inference_steps=100,\n... ).images\n\n>>> image[0].save(\"cat.png\")\n```", "```py\n( text_encoder: MultilingualCLIP tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: Union movq: VQModel prior_prior: PriorTransformer prior_image_encoder: CLIPVisionModelWithProjection prior_text_encoder: CLIPTextModelWithProjection prior_tokenizer: CLIPTokenizer prior_scheduler: UnCLIPScheduler prior_image_processor: CLIPImageProcessor )\n```", "```py\n( prompt: Union negative_prompt: Union = None num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 height: int = 512 width: int = 512 prior_guidance_scale: float = 4.0 prior_num_inference_steps: int = 25 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) \u2192 export const metadata = 'undefined';ImagePipelineOutput or tuple\n```", "```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained(\n    \"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16\n)\npipe.enable_model_cpu_offload()\n\nprompt = \"A lion in galaxies, spirals, nebulae, stars, smoke, iridescent, intricate detail, octane render, 8k\"\n\nimage = pipe(prompt=prompt, num_inference_steps=25).images[0]\n```", "```py\n( gpu_id = 0 )\n```", "```py\n( text_encoder: MultilingualCLIP movq: VQModel tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: DDIMScheduler )\n```", "```py\n( prompt: Union image: Union image_embeds: FloatTensor negative_image_embeds: FloatTensor negative_prompt: Union = None height: int = 512 width: int = 512 num_inference_steps: int = 100 strength: float = 0.3 guidance_scale: float = 7.0 num_images_per_prompt: int = 1 generator: Union = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) \u2192 export const metadata = 'undefined';ImagePipelineOutput or tuple\n```", "```py\n>>> from diffusers import KandinskyImg2ImgPipeline, KandinskyPriorPipeline\n>>> from diffusers.utils import load_image\n>>> import torch\n\n>>> pipe_prior = KandinskyPriorPipeline.from_pretrained(\n...     \"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16\n... )\n>>> pipe_prior.to(\"cuda\")\n\n>>> prompt = \"A red cartoon frog, 4k\"\n>>> image_emb, zero_image_emb = pipe_prior(prompt, return_dict=False)\n\n>>> pipe = KandinskyImg2ImgPipeline.from_pretrained(\n...     \"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16\n... )\n>>> pipe.to(\"cuda\")\n\n>>> init_image = load_image(\n...     \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n...     \"/kandinsky/frog.png\"\n... )\n\n>>> image = pipe(\n...     prompt,\n...     image=init_image,\n...     image_embeds=image_emb,\n...     negative_image_embeds=zero_image_emb,\n...     height=768,\n...     width=768,\n...     num_inference_steps=100,\n...     strength=0.2,\n... ).images\n\n>>> image[0].save(\"red_frog.png\")\n```", "```py\n( text_encoder: MultilingualCLIP tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: Union movq: VQModel prior_prior: PriorTransformer prior_image_encoder: CLIPVisionModelWithProjection prior_text_encoder: CLIPTextModelWithProjection prior_tokenizer: CLIPTokenizer prior_scheduler: UnCLIPScheduler prior_image_processor: CLIPImageProcessor )\n```", "```py\n( prompt: Union image: Union negative_prompt: Union = None num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 strength: float = 0.3 height: int = 512 width: int = 512 prior_guidance_scale: float = 4.0 prior_num_inference_steps: int = 25 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) \u2192 export const metadata = 'undefined';ImagePipelineOutput or tuple\n```", "```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\nimport requests\nfrom io import BytesIO\nfrom PIL import Image\nimport os\n\npipe = AutoPipelineForImage2Image.from_pretrained(\n    \"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16\n)\npipe.enable_model_cpu_offload()\n\nprompt = \"A fantasy landscape, Cinematic lighting\"\nnegative_prompt = \"low quality, bad quality\"\n\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\"RGB\")\nimage.thumbnail((768, 768))\n\nimage = pipe(prompt=prompt, image=original_image, num_inference_steps=25).images[0]\n```", "```py\n( gpu_id = 0 )\n```", "```py\n( text_encoder: MultilingualCLIP movq: VQModel tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: DDIMScheduler )\n```", "```py\n( prompt: Union image: Union mask_image: Union image_embeds: FloatTensor negative_image_embeds: FloatTensor negative_prompt: Union = None height: int = 512 width: int = 512 num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) \u2192 export const metadata = 'undefined';ImagePipelineOutput or tuple\n```", "```py\n>>> from diffusers import KandinskyInpaintPipeline, KandinskyPriorPipeline\n>>> from diffusers.utils import load_image\n>>> import torch\n>>> import numpy as np\n\n>>> pipe_prior = KandinskyPriorPipeline.from_pretrained(\n...     \"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16\n... )\n>>> pipe_prior.to(\"cuda\")\n\n>>> prompt = \"a hat\"\n>>> image_emb, zero_image_emb = pipe_prior(prompt, return_dict=False)\n\n>>> pipe = KandinskyInpaintPipeline.from_pretrained(\n...     \"kandinsky-community/kandinsky-2-1-inpaint\", torch_dtype=torch.float16\n... )\n>>> pipe.to(\"cuda\")\n\n>>> init_image = load_image(\n...     \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"\n...     \"/kandinsky/cat.png\"\n... )\n\n>>> mask = np.zeros((768, 768), dtype=np.float32)\n>>> mask[:250, 250:-250] = 1\n\n>>> out = pipe(\n...     prompt,\n...     image=init_image,\n...     mask_image=mask,\n...     image_embeds=image_emb,\n...     negative_image_embeds=zero_image_emb,\n...     height=768,\n...     width=768,\n...     num_inference_steps=50,\n... )\n\n>>> image = out.images[0]\n>>> image.save(\"cat_with_hat.png\")\n```", "```py\n( text_encoder: MultilingualCLIP tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: Union movq: VQModel prior_prior: PriorTransformer prior_image_encoder: CLIPVisionModelWithProjection prior_text_encoder: CLIPTextModelWithProjection prior_tokenizer: CLIPTokenizer prior_scheduler: UnCLIPScheduler prior_image_processor: CLIPImageProcessor )\n```", "```py\n( prompt: Union image: Union mask_image: Union negative_prompt: Union = None num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 height: int = 512 width: int = 512 prior_guidance_scale: float = 4.0 prior_num_inference_steps: int = 25 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) \u2192 export const metadata = 'undefined';ImagePipelineOutput or tuple\n```", "```py\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\nimport torch\nimport numpy as np\n\npipe = AutoPipelineForInpainting.from_pretrained(\n    \"kandinsky-community/kandinsky-2-1-inpaint\", torch_dtype=torch.float16\n)\npipe.enable_model_cpu_offload()\n\nprompt = \"A fantasy landscape, Cinematic lighting\"\nnegative_prompt = \"low quality, bad quality\"\n\noriginal_image = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\" \"/kandinsky/cat.png\"\n)\n\nmask = np.zeros((768, 768), dtype=np.float32)\n# Let's mask out an area above the cat's head\nmask[:250, 250:-250] = 1\n\nimage = pipe(prompt=prompt, image=original_image, mask_image=mask, num_inference_steps=25).images[0]\n```", "```py\n( gpu_id = 0 )\n```"]