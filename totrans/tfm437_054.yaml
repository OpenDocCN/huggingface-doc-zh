- en: Templates for Chat Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/chat_templating](https://huggingface.co/docs/transformers/v4.37.2/en/chat_templating)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/11.218b5c49.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An increasingly common use case for LLMs is **chat**. In a chat context, rather
    than continuing a single string of text (as is the case with a standard language
    model), the model instead continues a conversation that consists of one or more
    **messages**, each of which includes a **role**, like “user” or “assistant”, as
    well as message text.
  prefs: []
  type: TYPE_NORMAL
- en: Much like tokenization, different models expect very different input formats
    for chat. This is the reason we added **chat templates** as a feature. Chat templates
    are part of the tokenizer. They specify how to convert conversations, represented
    as lists of messages, into a single tokenizable string in the format that the
    model expects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s make this concrete with a quick example using the `BlenderBot` model.
    BlenderBot has an extremely simple default template, which mostly just adds whitespace
    between rounds of dialogue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the entire chat is condensed into a single string. If we use `tokenize=True`,
    which is the default setting, that string will also be tokenized for us. To see
    a more complex template in action, though, let’s use the `mistralai/Mistral-7B-Instruct-v0.1`
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that this time, the tokenizer has added the control tokens [INST] and [/INST]
    to indicate the start and end of user messages (but not assistant messages!).
    Mistral-instruct was trained with these tokens, but BlenderBot was not.
  prefs: []
  type: TYPE_NORMAL
- en: How do I use chat templates?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can see in the example above, chat templates are easy to use. Simply
    build a list of messages, with `role` and `content` keys, and then pass it to
    the [apply_chat_template()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template)
    method. Once you do that, you’ll get output that’s ready to go! When using chat
    templates as input for model generation, it’s also a good idea to use `add_generation_prompt=True`
    to add a [generation prompt](#what-are-generation-prompts).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of preparing input for `model.generate()`, using the `Zephyr`
    assistant model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This will yield a string in the input format that Zephyr expects.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that our input is formatted correctly for Zephyr, we can use the model
    to generate a response to the user’s question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will yield:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Arr, ‘twas easy after all!
  prefs: []
  type: TYPE_NORMAL
- en: Is there an automated pipeline for chat?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Yes, there is: [ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline).
    This pipeline is designed to make it easy to use chat models. Let’s try the `Zephyr`
    example again, but this time using the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline)
    will take care of all the details of tokenization and calling `apply_chat_template`
    for you - once the model has a chat template, all you need to do is initialize
    the pipeline and pass it the list of messages!'
  prefs: []
  type: TYPE_NORMAL
- en: What are “generation prompts”?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You may have noticed that the `apply_chat_template` method has an `add_generation_prompt`
    argument. This argument tells the template to add tokens that indicate the start
    of a bot response. For example, consider the following chat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s what this will look like without a generation prompt, using the ChatML
    template we saw in the Zephyr example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'And here’s what it looks like **with** a generation prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that this time, we’ve added the tokens that indicate the start of a bot
    response. This ensures that when the model generates text it will write a bot
    response instead of doing something unexpected, like continuing the user’s message.
    Remember, chat models are still just language models - they’re trained to continue
    text, and chat is just a special kind of text to them! You need to guide them
    with the appropriate control tokens so they know what they’re supposed to be doing.
  prefs: []
  type: TYPE_NORMAL
- en: Not all models require generation prompts. Some models, like BlenderBot and
    LLaMA, don’t have any special tokens before bot responses. In these cases, the
    `add_generation_prompt` argument will have no effect. The exact effect that `add_generation_prompt`
    has will depend on the template being used.
  prefs: []
  type: TYPE_NORMAL
- en: Can I use chat templates in training?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Yes! We recommend that you apply the chat template as a preprocessing step
    for your dataset. After this, you can simply continue like any other language
    model training task. When training, you should usually set `add_generation_prompt=False`,
    because the added tokens to prompt an assistant response will not be helpful during
    training. Let’s see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'And we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: From here, just continue training like you would with a standard language modelling
    task, using the `formatted_chat` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Advanced: How do chat templates work?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The chat template for a model is stored on the `tokenizer.chat_template` attribute.
    If no chat template is set, the default template for that model class is used
    instead. Let’s take a look at the template for `BlenderBot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: That’s kind of intimidating. Let’s add some newlines and indentation to make
    it more readable. Note that the first newline after each block as well as any
    preceding whitespace before a block are ignored by default, using the Jinja `trim_blocks`
    and `lstrip_blocks` flags. However, be cautious - although leading whitespace
    on each line is stripped, spaces between blocks on the same line are not. We strongly
    recommend checking that your template isn’t printing extra spaces where it shouldn’t
    be!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If you’ve never seen one of these before, this is a [Jinja template](https://jinja.palletsprojects.com/en/3.1.x/templates/).
    Jinja is a templating language that allows you to write simple code that generates
    text. In many ways, the code and syntax resembles Python. In pure Python, this
    template would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Effectively, the template does three things:'
  prefs: []
  type: TYPE_NORMAL
- en: For each message, if the message is a user message, add a blank space before
    it, otherwise print nothing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the message content
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the message is not the last message, add two spaces after it. After the final
    message, print the EOS token.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is a pretty simple template - it doesn’t add any control tokens, and it
    doesn’t support “system” messages, which are a common way to give the model directives
    about how it should behave in the subsequent conversation. But Jinja gives you
    a lot of flexibility to do those things! Let’s see a Jinja template that can format
    inputs similarly to the way LLaMA formats them (note that the real LLaMA template
    includes handling for default system messages and slightly different system message
    handling in general - don’t use this one in your actual code!)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Hopefully if you stare at this for a little bit you can see what this template
    is doing - it adds specific tokens based on the “role” of each message, which
    represents who sent it. User, assistant and system messages are clearly distinguishable
    to the model because of the tokens they’re wrapped in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Advanced: Adding and editing chat templates'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do I create a chat template?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Simple, just write a jinja template and set `tokenizer.chat_template`. You
    may find it easier to start with an existing template from another model and simply
    edit it for your needs! For example, we could take the LLaMA template above and
    add ”[ASST]” and ”[/ASST]” to assistant messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, simply set the `tokenizer.chat_template` attribute. Next time you use [apply_chat_template()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template),
    it will use your new template! This attribute will be saved in the `tokenizer_config.json`
    file, so you can use [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    to upload your new template to the Hub and make sure everyone’s using the right
    template for your model!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The method [apply_chat_template()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template)
    which uses your chat template is called by the [ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline)
    class, so once you set the correct chat template, your model will automatically
    become compatible with [ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline).
  prefs: []
  type: TYPE_NORMAL
- en: What are “default” templates?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before the introduction of chat templates, chat handling was hardcoded at the
    model class level. For backwards compatibility, we have retained this class-specific
    handling as default templates, also set at the class level. If a model does not
    have a chat template set, but there is a default template for its model class,
    the `ConversationalPipeline` class and methods like `apply_chat_template` will
    use the class template instead. You can find out what the default template for
    your tokenizer is by checking the `tokenizer.default_chat_template` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: This is something we do purely for backward compatibility reasons, to avoid
    breaking any existing workflows. Even when the class template is appropriate for
    your model, we strongly recommend overriding the default template by setting the
    `chat_template` attribute explicitly to make it clear to users that your model
    has been correctly configured for chat, and to future-proof in case the default
    templates are ever altered or deprecated.
  prefs: []
  type: TYPE_NORMAL
- en: What template should I use?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When setting the template for a model that’s already been trained for chat,
    you should ensure that the template exactly matches the message formatting that
    the model saw during training, or else you will probably experience performance
    degradation. This is true even if you’re training the model further - you will
    probably get the best performance if you keep the chat tokens constant. This is
    very analogous to tokenization - you generally get the best performance for inference
    or fine-tuning when you precisely match the tokenization used during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re training a model from scratch, or fine-tuning a base language model
    for chat, on the other hand, you have a lot of freedom to choose an appropriate
    template! LLMs are smart enough to learn to handle lots of different input formats.
    Our default template for models that don’t have a class-specific template follows
    the [ChatML format](https://github.com/openai/openai-python/blob/main/chatml.md),
    and this is a good, flexible choice for many use-cases. It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If you like this one, here it is in one-liner form, ready to copy into your
    code. The one-liner also includes handy support for [generation prompts](#what-are-generation-prompts),
    but note that it doesn’t add BOS or EOS tokens! If your model expects those, they
    won’t be added automatically by `apply_chat_template` - in other words, the text
    will be tokenized with `add_special_tokens=False`. This is to avoid potential
    conflicts between the template and the `add_special_tokens` logic. If your model
    expects special tokens, make sure to add them to the template!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This template wraps each message in `<|im_start|>` and `<|im_end|>` tokens,
    and simply writes the role as a string, which allows for flexibility in the roles
    you train with. The output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The “user”, “system” and “assistant” roles are the standard for chat, and we
    recommend using them when it makes sense, particularly if you want your model
    to operate well with [ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline).
    However, you are not limited to these roles - templating is extremely flexible,
    and any string can be a role.
  prefs: []
  type: TYPE_NORMAL
- en: I want to add some chat templates! How should I get started?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have any chat models, you should set their `tokenizer.chat_template`
    attribute and test it using [apply_chat_template()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template),
    then push the updated tokenizer to the Hub. This applies even if you’re not the
    model owner - if you’re using a model with an empty chat template, or one that’s
    still using the default class template, please open a [pull request](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)
    to the model repository so that this attribute can be set properly!
  prefs: []
  type: TYPE_NORMAL
- en: Once the attribute is set, that’s it, you’re done! `tokenizer.apply_chat_template`
    will now work correctly for that model, which means it is also automatically supported
    in places like `ConversationalPipeline`!
  prefs: []
  type: TYPE_NORMAL
- en: By ensuring that models have this attribute, we can make sure that the whole
    community gets to use the full power of open-source models. Formatting mismatches
    have been haunting the field and silently harming performance for too long - it’s
    time to put an end to them!
  prefs: []
  type: TYPE_NORMAL
- en: 'Advanced: Template writing tips'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’re unfamiliar with Jinja, we generally find that the easiest way to write
    a chat template is to first write a short Python script that formats messages
    the way you want, and then convert that script into a template.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the template handler will receive the conversation history as
    a variable called `messages`. Each message is a dictionary with two keys, `role`
    and `content`. You will be able to access `messages` in your template just like
    you can in Python, which means you can loop over it with `{% for message in messages
    %}` or access individual messages with, for example, `{{ messages[0] }}`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use the following tips to convert your code to Jinja:'
  prefs: []
  type: TYPE_NORMAL
- en: For loops
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For loops in Jinja look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note that whatever’s inside the {{ expression block }} will be printed to the
    output. You can use operators like `+` to combine strings inside expression blocks.
  prefs: []
  type: TYPE_NORMAL
- en: If statements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If statements in Jinja look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note how where Python uses whitespace to mark the beginnings and ends of `for`
    and `if` blocks, Jinja requires you to explicitly end them with `{% endfor %}`
    and `{% endif %}`.
  prefs: []
  type: TYPE_NORMAL
- en: Special variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Inside your template, you will have access to the list of `messages`, but you
    can also access several other special variables. These include special tokens
    like `bos_token` and `eos_token`, as well as the `add_generation_prompt` variable
    that we discussed above. You can also use the `loop` variable to access information
    about the current loop iteration, for example using `{% if loop.last %}` to check
    if the current message is the last message in the conversation. Here’s an example
    that puts these ideas together to add a generation prompt at the end of the conversation
    if add_generation_prompt is `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Notes on whitespace
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As much as possible, we’ve tried to get Jinja to ignore whitespace outside of
    {{ expressions }}. However, be aware that Jinja is a general-purpose templating
    engine, and it may treat whitespace between blocks on the same line as significant
    and print it to the output. We **strongly** recommend checking that your template
    isn’t printing extra spaces where it shouldn’t be before you upload it!
  prefs: []
  type: TYPE_NORMAL
