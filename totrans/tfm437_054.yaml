- en: Templates for Chat Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/chat_templating](https://huggingface.co/docs/transformers/v4.37.2/en/chat_templating)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An increasingly common use case for LLMs is **chat**. In a chat context, rather
    than continuing a single string of text (as is the case with a standard language
    model), the model instead continues a conversation that consists of one or more
    **messages**, each of which includes a **role**, like “user” or “assistant”, as
    well as message text.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Much like tokenization, different models expect very different input formats
    for chat. This is the reason we added **chat templates** as a feature. Chat templates
    are part of the tokenizer. They specify how to convert conversations, represented
    as lists of messages, into a single tokenizable string in the format that the
    model expects.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s make this concrete with a quick example using the `BlenderBot` model.
    BlenderBot has an extremely simple default template, which mostly just adds whitespace
    between rounds of dialogue:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Notice how the entire chat is condensed into a single string. If we use `tokenize=True`,
    which is the default setting, that string will also be tokenized for us. To see
    a more complex template in action, though, let’s use the `mistralai/Mistral-7B-Instruct-v0.1`
    model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that this time, the tokenizer has added the control tokens [INST] and [/INST]
    to indicate the start and end of user messages (but not assistant messages!).
    Mistral-instruct was trained with these tokens, but BlenderBot was not.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: How do I use chat templates?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can see in the example above, chat templates are easy to use. Simply
    build a list of messages, with `role` and `content` keys, and then pass it to
    the [apply_chat_template()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template)
    method. Once you do that, you’ll get output that’s ready to go! When using chat
    templates as input for model generation, it’s also a good idea to use `add_generation_prompt=True`
    to add a [generation prompt](#what-are-generation-prompts).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of preparing input for `model.generate()`, using the `Zephyr`
    assistant model:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This will yield a string in the input format that Zephyr expects.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that our input is formatted correctly for Zephyr, we can use the model
    to generate a response to the user’s question:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will yield:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Arr, ‘twas easy after all!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Is there an automated pipeline for chat?
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Yes, there is: [ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline).
    This pipeline is designed to make it easy to use chat models. Let’s try the `Zephyr`
    example again, but this time using the pipeline:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline)
    will take care of all the details of tokenization and calling `apply_chat_template`
    for you - once the model has a chat template, all you need to do is initialize
    the pipeline and pass it the list of messages!'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: What are “generation prompts”?
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You may have noticed that the `apply_chat_template` method has an `add_generation_prompt`
    argument. This argument tells the template to add tokens that indicate the start
    of a bot response. For example, consider the following chat:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here’s what this will look like without a generation prompt, using the ChatML
    template we saw in the Zephyr example:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And here’s what it looks like **with** a generation prompt:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that this time, we’ve added the tokens that indicate the start of a bot
    response. This ensures that when the model generates text it will write a bot
    response instead of doing something unexpected, like continuing the user’s message.
    Remember, chat models are still just language models - they’re trained to continue
    text, and chat is just a special kind of text to them! You need to guide them
    with the appropriate control tokens so they know what they’re supposed to be doing.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这一次，我们添加了指示机器人响应开始的标记。这确保了当模型生成文本时，它将写入一个机器人响应，而不是做一些意外的事情，比如继续用户的消息。请记住，聊天模型仍然只是语言模型
    - 它们被训练来继续文本，而聊天只是对它们来说的一种特殊文本！您需要使用适当的控制标记来指导它们知道应该做什么。
- en: Not all models require generation prompts. Some models, like BlenderBot and
    LLaMA, don’t have any special tokens before bot responses. In these cases, the
    `add_generation_prompt` argument will have no effect. The exact effect that `add_generation_prompt`
    has will depend on the template being used.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有模型都需要生成提示。一些模型，如BlenderBot和LLaMA，在机器人响应之前没有任何特殊标记。在这些情况下，`add_generation_prompt`参数将不起作用。`add_generation_prompt`的确切效果将取决于所使用的模板。
- en: Can I use chat templates in training?
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我可以在训练中使用聊天模板吗？
- en: 'Yes! We recommend that you apply the chat template as a preprocessing step
    for your dataset. After this, you can simply continue like any other language
    model training task. When training, you should usually set `add_generation_prompt=False`,
    because the added tokens to prompt an assistant response will not be helpful during
    training. Let’s see an example:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 是的！我们建议您将聊天模板应用为数据集的预处理步骤。之后，您可以像处理任何其他语言模型训练任务一样继续。在训练时，通常应设置`add_generation_prompt=False`，因为在训练过程中，添加的提示助手响应的标记将不会有帮助。让我们看一个例子：
- en: '[PRE11]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And we get:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到：
- en: '[PRE12]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: From here, just continue training like you would with a standard language modelling
    task, using the `formatted_chat` column.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，就像处理标准语言建模任务一样继续训练，使用`formatted_chat`列。
- en: 'Advanced: How do chat templates work?'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级：聊天模板如何工作？
- en: 'The chat template for a model is stored on the `tokenizer.chat_template` attribute.
    If no chat template is set, the default template for that model class is used
    instead. Let’s take a look at the template for `BlenderBot`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的聊天模板存储在`tokenizer.chat_template`属性中。如果没有设置聊天模板，则将使用该模型类的默认模板。让我们看一下`BlenderBot`的模板：
- en: '[PRE13]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: That’s kind of intimidating. Let’s add some newlines and indentation to make
    it more readable. Note that the first newline after each block as well as any
    preceding whitespace before a block are ignored by default, using the Jinja `trim_blocks`
    and `lstrip_blocks` flags. However, be cautious - although leading whitespace
    on each line is stripped, spaces between blocks on the same line are not. We strongly
    recommend checking that your template isn’t printing extra spaces where it shouldn’t
    be!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点令人生畏。让我们添加一些换行和缩进，使其更易读。请注意，每个块后的第一个换行以及块之前的任何前导空格默认情况下会被忽略，使用Jinja的`trim_blocks`和`lstrip_blocks`标志。但是，请谨慎
    - 尽管每行的前导空格被剥离，但同一行上块之间的空格不会被剥离。我们强烈建议检查您的模板是否在不应该的地方打印额外的空格！
- en: '[PRE14]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If you’ve never seen one of these before, this is a [Jinja template](https://jinja.palletsprojects.com/en/3.1.x/templates/).
    Jinja is a templating language that allows you to write simple code that generates
    text. In many ways, the code and syntax resembles Python. In pure Python, this
    template would look something like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您以前从未见过这种模板，这是一个[Jinja模板](https://jinja.palletsprojects.com/en/3.1.x/templates/)。Jinja是一种模板语言，允许您编写生成文本的简单代码。在许多方面，代码和语法类似于Python。在纯Python中，这个模板看起来会像这样：
- en: '[PRE15]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Effectively, the template does three things:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，模板执行三件事：
- en: For each message, if the message is a user message, add a blank space before
    it, otherwise print nothing.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每条消息，如果消息是用户消息，则在其前添加一个空格，否则不打印任何内容。
- en: Add the message content
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加消息内容
- en: If the message is not the last message, add two spaces after it. After the final
    message, print the EOS token.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果消息不是最后一条消息，请在其后添加两个空格。在最后一条消息之后，打印EOS标记。
- en: This is a pretty simple template - it doesn’t add any control tokens, and it
    doesn’t support “system” messages, which are a common way to give the model directives
    about how it should behave in the subsequent conversation. But Jinja gives you
    a lot of flexibility to do those things! Let’s see a Jinja template that can format
    inputs similarly to the way LLaMA formats them (note that the real LLaMA template
    includes handling for default system messages and slightly different system message
    handling in general - don’t use this one in your actual code!)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的模板 - 它不添加任何控制标记，也不支持“系统”消息，这是一种常见的方式，用于向模型提供关于其在随后对话中应该如何行为的指令。但是Jinja为您提供了很大的灵活性来执行这些操作！让我们看一个Jinja模板，可以类似于LLaMA格式化输入（请注意，真正的LLaMA模板包括处理默认系统消息以及一般情况下稍有不同的系统消息处理
    - 不要在实际代码中使用这个！）
- en: '[PRE16]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Hopefully if you stare at this for a little bit you can see what this template
    is doing - it adds specific tokens based on the “role” of each message, which
    represents who sent it. User, assistant and system messages are clearly distinguishable
    to the model because of the tokens they’re wrapped in.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 希望如果您仔细看一下，您就能看出这个模板在做什么 - 它根据每条消息的“角色”添加特定的标记，这些标记代表发送者是谁。用户、助手和系统消息因为它们被包裹在其中的标记而清晰可辨。
- en: 'Advanced: Adding and editing chat templates'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级：添加和编辑聊天模板
- en: How do I create a chat template?
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何创建聊天模板？
- en: 'Simple, just write a jinja template and set `tokenizer.chat_template`. You
    may find it easier to start with an existing template from another model and simply
    edit it for your needs! For example, we could take the LLaMA template above and
    add ”[ASST]” and ”[/ASST]” to assistant messages:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 简单，只需编写一个Jinja模板并设置`tokenizer.chat_template`。您可能会发现，从另一个模型的现有模板开始，并为您的需求简单编辑它会更容易！例如，我们可以采用上面的LLaMA模板，并为助手消息添加"[ASST]"和"[/ASST]"：
- en: '[PRE17]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, simply set the `tokenizer.chat_template` attribute. Next time you use [apply_chat_template()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template),
    it will use your new template! This attribute will be saved in the `tokenizer_config.json`
    file, so you can use [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    to upload your new template to the Hub and make sure everyone’s using the right
    template for your model!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The method [apply_chat_template()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template)
    which uses your chat template is called by the [ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline)
    class, so once you set the correct chat template, your model will automatically
    become compatible with [ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: What are “default” templates?
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before the introduction of chat templates, chat handling was hardcoded at the
    model class level. For backwards compatibility, we have retained this class-specific
    handling as default templates, also set at the class level. If a model does not
    have a chat template set, but there is a default template for its model class,
    the `ConversationalPipeline` class and methods like `apply_chat_template` will
    use the class template instead. You can find out what the default template for
    your tokenizer is by checking the `tokenizer.default_chat_template` attribute.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: This is something we do purely for backward compatibility reasons, to avoid
    breaking any existing workflows. Even when the class template is appropriate for
    your model, we strongly recommend overriding the default template by setting the
    `chat_template` attribute explicitly to make it clear to users that your model
    has been correctly configured for chat, and to future-proof in case the default
    templates are ever altered or deprecated.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: What template should I use?
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When setting the template for a model that’s already been trained for chat,
    you should ensure that the template exactly matches the message formatting that
    the model saw during training, or else you will probably experience performance
    degradation. This is true even if you’re training the model further - you will
    probably get the best performance if you keep the chat tokens constant. This is
    very analogous to tokenization - you generally get the best performance for inference
    or fine-tuning when you precisely match the tokenization used during training.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re training a model from scratch, or fine-tuning a base language model
    for chat, on the other hand, you have a lot of freedom to choose an appropriate
    template! LLMs are smart enough to learn to handle lots of different input formats.
    Our default template for models that don’t have a class-specific template follows
    the [ChatML format](https://github.com/openai/openai-python/blob/main/chatml.md),
    and this is a good, flexible choice for many use-cases. It looks like this:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If you like this one, here it is in one-liner form, ready to copy into your
    code. The one-liner also includes handy support for [generation prompts](#what-are-generation-prompts),
    but note that it doesn’t add BOS or EOS tokens! If your model expects those, they
    won’t be added automatically by `apply_chat_template` - in other words, the text
    will be tokenized with `add_special_tokens=False`. This is to avoid potential
    conflicts between the template and the `add_special_tokens` logic. If your model
    expects special tokens, make sure to add them to the template!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This template wraps each message in `<|im_start|>` and `<|im_end|>` tokens,
    and simply writes the role as a string, which allows for flexibility in the roles
    you train with. The output looks like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The “user”, “system” and “assistant” roles are the standard for chat, and we
    recommend using them when it makes sense, particularly if you want your model
    to operate well with [ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline).
    However, you are not limited to these roles - templating is extremely flexible,
    and any string can be a role.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: “用户”、“系统”和“助手”角色是聊天的标准角色，我们建议在有意义的情况下使用它们，特别是如果您希望您的模型在[ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline)中运行良好。但是，您不限于这些角色
    - 模板非常灵活，任何字符串都可以是一个角色。
- en: I want to add some chat templates! How should I get started?
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我想添加一些聊天模板！我应该如何开始？
- en: If you have any chat models, you should set their `tokenizer.chat_template`
    attribute and test it using [apply_chat_template()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template),
    then push the updated tokenizer to the Hub. This applies even if you’re not the
    model owner - if you’re using a model with an empty chat template, or one that’s
    still using the default class template, please open a [pull request](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)
    to the model repository so that this attribute can be set properly!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有任何聊天模型，您应该设置它们的`tokenizer.chat_template`属性，并使用[apply_chat_template()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template)进行测试，然后将更新后的tokenizer推送到Hub。即使您不是模型所有者
    - 如果您使用的模型具有空白聊天模板，或者仍在使用默认类模板，请打开一个[拉取请求](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)到模型存储库，以便正确设置此属性！
- en: Once the attribute is set, that’s it, you’re done! `tokenizer.apply_chat_template`
    will now work correctly for that model, which means it is also automatically supported
    in places like `ConversationalPipeline`!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置了属性，就完成了！`tokenizer.apply_chat_template`现在将正确地为该模型工作，这意味着它也会自动支持像`ConversationalPipeline`这样的地方！
- en: By ensuring that models have this attribute, we can make sure that the whole
    community gets to use the full power of open-source models. Formatting mismatches
    have been haunting the field and silently harming performance for too long - it’s
    time to put an end to them!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通过确保模型具有此属性，我们可以确保整个社区都能够使用开源模型的全部功能。格式不匹配已经困扰该领域并悄悄损害了性能太久了 - 是时候结束它们了！
- en: 'Advanced: Template writing tips'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级：模板编写提示
- en: If you’re unfamiliar with Jinja, we generally find that the easiest way to write
    a chat template is to first write a short Python script that formats messages
    the way you want, and then convert that script into a template.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对Jinja不熟悉，我们通常发现编写聊天模板的最简单方法是首先编写一个格式化消息的Python脚本，然后将该脚本转换为模板。
- en: Remember that the template handler will receive the conversation history as
    a variable called `messages`. Each message is a dictionary with two keys, `role`
    and `content`. You will be able to access `messages` in your template just like
    you can in Python, which means you can loop over it with `{% for message in messages
    %}` or access individual messages with, for example, `{{ messages[0] }}`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 记住模板处理程序将接收对话历史作为名为`messages`的变量。每条消息都是一个带有两个键`role`和`content`的字典。您可以在模板中像在Python中一样访问`messages`，这意味着您可以使用`{%
    for message in messages %}`循环遍历它，或者例如使用`{{ messages[0] }}`访问单个消息。
- en: 'You can also use the following tips to convert your code to Jinja:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用以下提示将您的代码转换为Jinja：
- en: For loops
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对于循环
- en: 'For loops in Jinja look like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Jinja中的for循环如下所示：
- en: '[PRE22]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that whatever’s inside the {{ expression block }} will be printed to the
    output. You can use operators like `+` to combine strings inside expression blocks.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，无论{{表达式块}}中有什么都将打印到输出中。您可以在表达式块内使用`+`等运算符来组合字符串。
- en: If statements
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: if语句
- en: 'If statements in Jinja look like this:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Jinja中的if语句如下所示：
- en: '[PRE23]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note how where Python uses whitespace to mark the beginnings and ends of `for`
    and `if` blocks, Jinja requires you to explicitly end them with `{% endfor %}`
    and `{% endif %}`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Python使用空格来标记`for`和`if`块的开始和结束位置，而Jinja要求您使用`{% endfor %}`和`{% endif %}`显式结束它们。
- en: Special variables
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特殊变量
- en: 'Inside your template, you will have access to the list of `messages`, but you
    can also access several other special variables. These include special tokens
    like `bos_token` and `eos_token`, as well as the `add_generation_prompt` variable
    that we discussed above. You can also use the `loop` variable to access information
    about the current loop iteration, for example using `{% if loop.last %}` to check
    if the current message is the last message in the conversation. Here’s an example
    that puts these ideas together to add a generation prompt at the end of the conversation
    if add_generation_prompt is `True`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的模板中，您将可以访问`messages`列表，但也可以访问几个其他特殊变量。这些包括像`bos_token`和`eos_token`这样的特殊标记，以及我们上面讨论过的`add_generation_prompt`变量。您还可以使用`loop`变量来访问有关当前循环迭代的信息，例如使用`{%
    if loop.last %}`来检查当前消息是否是对话中的最后一条消息。以下是一个将这些想法结合在一起，在对话结束时添加生成提示的示例，如果`add_generation_prompt`为`True`：
- en: '[PRE24]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Notes on whitespace
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 空格注意事项
- en: As much as possible, we’ve tried to get Jinja to ignore whitespace outside of
    {{ expressions }}. However, be aware that Jinja is a general-purpose templating
    engine, and it may treat whitespace between blocks on the same line as significant
    and print it to the output. We **strongly** recommend checking that your template
    isn’t printing extra spaces where it shouldn’t be before you upload it!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能地，我们已经尝试让Jinja忽略{{表达式}}之外的空格。但是，请注意，Jinja是一个通用的模板引擎，它可能会将同一行上块之间的空格视为重要并将其打印到输出中。我们**强烈**建议在上传模板之前检查您的模板是否在不应该的地方打印额外的空格！
