# Text2Video-Zero

> 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/text_to_video_zero](https://huggingface.co/docs/diffusers/api/pipelines/text_to_video_zero)

[Text2Video-Zero：文本到图像扩散模型是零样本视频生成器](https://huggingface.co/papers/2303.13439) 作者是Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, [Zhangyang Wang](https://www.ece.utexas.edu/people/faculty/atlas-wang), Shant Navasardyan, [Humphrey Shi](https://www.humphreyshi.com)。

Text2Video-Zero使零样本视频生成成为可能：

1.  一个文本提示

1.  一个结合了姿势或边缘指导的提示

1.  视频指导-Pix2Pix（指导视频编辑）

结果在时间上保持一致，并且紧密遵循指导和文本提示。

![teaser-img](../Images/f8f3b9ddb58b874876214c2f2682292d.png)

论文摘要如下：

*最近的文本到视频生成方法依赖于计算密集型的训练，并需要大规模的视频数据集。在本文中，我们介绍了一种新的零样本文本到视频生成任务，并提出了一种低成本方法（无需任何训练或优化），通过利用现有文本到图像合成方法的能力（例如，稳定扩散），使其适用于视频领域。我们的关键修改包括（i）丰富生成帧的潜在代码，以保持全局场景和背景时间一致；和（ii）重新编程帧级自注意力，使用每帧对第一帧的新交叉帧注意力，以保留前景对象的上下文、外观和身份。实验表明，这导致低开销，但高质量且非常一致的视频生成。此外，我们的方法不仅限于文本到视频合成，还适用于其他任务，如有条件的和内容专门化的视频生成，以及视频指导-Pix2Pix，即指导视频编辑。正如实验所示，我们的方法在不经过额外视频数据训练的情况下表现出与最近方法相当甚至有时更好的性能。*

您可以在[项目页面](https://text2video-zero.github.io/)、[论文](https://arxiv.org/abs/2303.13439)和[原始代码库](https://github.com/Picsart-AI-Research/Text2Video-Zero)上找到有关Text2Video-Zero的其他信息。

## 用法示例

### 文本到视频

要从提示生成视频，请运行以下Python代码：

```py
import torch
from diffusers import TextToVideoZeroPipeline

model_id = "runwayml/stable-diffusion-v1-5"
pipe = TextToVideoZeroPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

prompt = "A panda is playing guitar on times square"
result = pipe(prompt=prompt).images
result = [(r * 255).astype("uint8") for r in result]
imageio.mimsave("video.mp4", result, fps=4)
```

您可以在管道调用中更改这些参数：

+   运动场强度（参见[论文](https://arxiv.org/abs/2303.13439)，第3.3.1节）：

    +   `motion_field_strength_x`和`motion_field_strength_y`。默认值：`motion_field_strength_x=12`，`motion_field_strength_y=12`

+   `T`和`T'`（参见[论文](https://arxiv.org/abs/2303.13439)，第3.3.1节）

    +   `t0`和`t1`在范围`{0, ..., num_inference_steps}`内。默认值：`t0=45`，`t1=48`

+   视频长度：

    +   `video_length`，要生成的帧数video_length。默认值：`video_length=8`

我们还可以通过分块处理来生成更长的视频：

```py
import torch
from diffusers import TextToVideoZeroPipeline
import numpy as np

model_id = "runwayml/stable-diffusion-v1-5"
pipe = TextToVideoZeroPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")
seed = 0
video_length = 24  #24 ÷ 4fps = 6 seconds
chunk_size = 8
prompt = "A panda is playing guitar on times square"

# Generate the video chunk-by-chunk
result = []
chunk_ids = np.arange(0, video_length, chunk_size - 1)
generator = torch.Generator(device="cuda")
for i in range(len(chunk_ids)):
    print(f"Processing chunk {i + 1} / {len(chunk_ids)}")
    ch_start = chunk_ids[i]
    ch_end = video_length if i == len(chunk_ids) - 1 else chunk_ids[i + 1]
    # Attach the first frame for Cross Frame Attention
    frame_ids = [0] + list(range(ch_start, ch_end))
    # Fix the seed for the temporal consistency
    generator.manual_seed(seed)
    output = pipe(prompt=prompt, video_length=len(frame_ids), generator=generator, frame_ids=frame_ids)
    result.append(output.images[1:])

# Concatenate chunks and save
result = np.concatenate(result)
result = [(r * 255).astype("uint8") for r in result]
imageio.mimsave("video.mp4", result, fps=4)
```

+   #### SDXL支持

    为了在从提示生成视频时使用SDXL模型，请使用`TextToVideoZeroSDXLPipeline`管道：

```py
import torch
from diffusers import TextToVideoZeroSDXLPipeline

model_id = "stabilityai/stable-diffusion-xl-base-1.0"
pipe = TextToVideoZeroSDXLPipeline.from_pretrained(
    model_id, torch_dtype=torch.float16, variant="fp16", use_safetensors=True
).to("cuda")
```

### 带姿势控制的文本到视频

要从提示生成带有额外姿势控制的视频

1.  下载演示视频

    ```py
    from huggingface_hub import hf_hub_download

    filename = "__assets__/poses_skeleton_gifs/dance1_corr.mp4"
    repo_id = "PAIR/Text2Video-Zero"
    video_path = hf_hub_download(repo_type="space", repo_id=repo_id, filename=filename)
    ```

1.  读取包含提取的姿势图像的视频

    ```py
    from PIL import Image
    import imageio

    reader = imageio.get_reader(video_path, "ffmpeg")
    frame_count = 8
    pose_images = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]
    ```

    要从实际视频中提取姿势，请阅读[ControlNet文档](controlnet)。

1.  运行`StableDiffusionControlNetPipeline`与我们的自定义注意力处理器

    ```py
    import torch
    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
    from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor

    model_id = "runwayml/stable-diffusion-v1-5"
    controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-openpose", torch_dtype=torch.float16)
    pipe = StableDiffusionControlNetPipeline.from_pretrained(
        model_id, controlnet=controlnet, torch_dtype=torch.float16
    ).to("cuda")

    # Set the attention processor
    pipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))
    pipe.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))

    # fix latents for all frames
    latents = torch.randn((1, 4, 64, 64), device="cuda", dtype=torch.float16).repeat(len(pose_images), 1, 1, 1)

    prompt = "Darth Vader dancing in a desert"
    result = pipe(prompt=[prompt] * len(pose_images), image=pose_images, latents=latents).images
    imageio.mimsave("video.mp4", result, fps=4)
    ```

+   #### SDXL支持

    由于我们的注意力处理器也适用于SDXL，它可以用于使用SDXL提供动力的ControlNet模型生成视频：

    ```py
    import torch
    from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel
    from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor

    controlnet_model_id = 'thibaud/controlnet-openpose-sdxl-1.0'
    model_id = 'stabilityai/stable-diffusion-xl-base-1.0'

    controlnet = ControlNetModel.from_pretrained(controlnet_model_id, torch_dtype=torch.float16)
    pipe = StableDiffusionControlNetPipeline.from_pretrained(
    	model_id, controlnet=controlnet, torch_dtype=torch.float16
    ).to('cuda')

    # Set the attention processor
    pipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))
    pipe.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))

    # fix latents for all frames
    latents = torch.randn((1, 4, 128, 128), device="cuda", dtype=torch.float16).repeat(len(pose_images), 1, 1, 1)

    prompt = "Darth Vader dancing in a desert"
    result = pipe(prompt=[prompt] * len(pose_images), image=pose_images, latents=latents).images
    imageio.mimsave("video.mp4", result, fps=4)
    ```

### 带边缘控制的文本到视频

要从提示生成带有额外Canny边缘控制的视频，请按照上述描述的相同步骤进行，使用[Canny边缘ControlNet模型](https://huggingface.co/lllyasviel/sd-controlnet-canny)进行姿势引导生成。

### 视频指导-Pix2Pix

执行文本引导的视频编辑（使用[InstructPix2Pix](pix2pix)）：

1.  下载演示视频

    ```py
    from huggingface_hub import hf_hub_download

    filename = "__assets__/pix2pix video/camel.mp4"
    repo_id = "PAIR/Text2Video-Zero"
    video_path = hf_hub_download(repo_type="space", repo_id=repo_id, filename=filename)
    ```

1.  从路径读取视频

    ```py
    from PIL import Image
    import imageio

    reader = imageio.get_reader(video_path, "ffmpeg")
    frame_count = 8
    video = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]
    ```

1.  使用我们的自定义注意力处理器运行`StableDiffusionInstructPix2PixPipeline`

    ```py
    import torch
    from diffusers import StableDiffusionInstructPix2PixPipeline
    from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor

    model_id = "timbrooks/instruct-pix2pix"
    pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")
    pipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=3))

    prompt = "make it Van Gogh Starry Night style"
    result = pipe(prompt=[prompt] * len(video), image=video).images
    imageio.mimsave("edited_video.mp4", result, fps=4)
    ```

### DreamBooth专业化

方法**文本到视频**、**文本到视频带姿势控制**和**文本到视频带边缘控制**可以使用自定义[DreamBooth](../../training/dreambooth)模型运行，如下所示为[Canny边缘ControlNet模型](https://huggingface.co/lllyasviel/sd-controlnet-canny)和[Avatar风格DreamBooth](https://huggingface.co/PAIR/text2video-zero-controlnet-canny-avatar)模型：

1.  下载演示视频

    ```py
    from huggingface_hub import hf_hub_download

    filename = "__assets__/canny_videos_mp4/girl_turning.mp4"
    repo_id = "PAIR/Text2Video-Zero"
    video_path = hf_hub_download(repo_type="space", repo_id=repo_id, filename=filename)
    ```

1.  从路径读取视频

    ```py
    from PIL import Image
    import imageio

    reader = imageio.get_reader(video_path, "ffmpeg")
    frame_count = 8
    canny_edges = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]
    ```

1.  使用自定义训练的DreamBooth模型运行`StableDiffusionControlNetPipeline`

    ```py
    import torch
    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
    from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor

    # set model id to custom model
    model_id = "PAIR/text2video-zero-controlnet-canny-avatar"
    controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16)
    pipe = StableDiffusionControlNetPipeline.from_pretrained(
        model_id, controlnet=controlnet, torch_dtype=torch.float16
    ).to("cuda")

    # Set the attention processor
    pipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))
    pipe.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))

    # fix latents for all frames
    latents = torch.randn((1, 4, 64, 64), device="cuda", dtype=torch.float16).repeat(len(canny_edges), 1, 1, 1)

    prompt = "oil painting of a beautiful girl avatar style"
    result = pipe(prompt=[prompt] * len(canny_edges), image=canny_edges, latents=latents).images
    imageio.mimsave("video.mp4", result, fps=4)
    ```

您可以使用[此链接](https://huggingface.co/models?search=dreambooth)筛选出一些可用的DreamBooth训练模型。

请务必查看调度器[指南](../../using-diffusers/schedulers)，以了解如何探索调度器速度和质量之间的权衡，并查看[跨流程重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分，以了解如何有效地将相同组件加载到多个流程中。

## TextToVideoZeroPipeline

### `class diffusers.TextToVideoZeroPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L284)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor requires_safety_checker: bool = True )
```

参数

+   `vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）- 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。

+   `text_encoder`（`CLIPTextModel`）- 冻结文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `tokenizer`（`CLIPTokenizer`）- 用于对文本进行标记化的[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)。

+   `unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)）- 用于去噪编码视频潜变量的[UNet3DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet3d-cond#diffusers.UNet3DConditionModel)。

+   `scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)）- 用于与`unet`结合使用以去噪编码图像潜变量的调度器。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。

+   `safety_checker`（`StableDiffusionSafetyChecker`）- 用于估计生成图像是否可能被视为具有冒犯性或有害的分类模块。请参考[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor`（`CLIPImageProcessor`）- 用于从生成的图像中提取特征的`CLIPImageProcessor`；作为`safety_checker`的输入。

使用Stable Diffusion进行零样本文本到视频生成的流程。

此模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以了解所有流程（下载、保存、在特定设备上运行等）实现的通用方法。

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L521)

```py
( prompt: Union video_length: Optional = 8 height: Optional = None width: Optional = None num_inference_steps: int = 50 guidance_scale: float = 7.5 negative_prompt: Union = None num_videos_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None motion_field_strength_x: float = 12 motion_field_strength_y: float = 12 output_type: Optional = 'tensor' return_dict: bool = True callback: Optional = None callback_steps: Optional = 1 t0: int = 44 t1: int = 47 frame_ids: Optional = None ) → export const metadata = 'undefined';TextToVideoPipelineOutput
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）- 用于引导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。

+   `video_length`（`int`，*可选*，默认为8）- 生成视频帧的数量。

+   `height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *optional*, defaults to 50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推断速度。

+   `guidance_scale` (`float`, *optional*, defaults to 7.5) — 更高的指导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用指导比例。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 指导视频生成中不包含的提示。如果未定义，则需要传递 `negative_prompt_embeds`。在不使用指导时（`guidance_scale < 1`）将被忽略。

+   `num_videos_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的视频数量。

+   `eta` (`float`, *optional*, defaults to 0.0) — 对应于 [DDIM](https://arxiv.org/abs/2010.02502) 论文中的参数 eta (η)。仅适用于 [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中将被忽略。

+   `generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中采样的预生成噪声潜变量，用作视频生成的输入。可用于使用不同提示调整相同生成。如果未提供，则通过使用提供的随机 `generator` 进行采样生成潜变量张量。

+   `output_type` (`str`, *optional*, defaults to `"numpy"`) — 生成视频的输出格式。选择 `"latent"` 和 `"numpy"` 之间。

+   `return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回 [TextToVideoPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video_zero#diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.TextToVideoPipelineOutput) 而不是普通元组。

+   `callback` (`Callable`, *optional*) — 在推断过程中每隔 `callback_steps` 步调用的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *optional*, defaults to 1) — 调用 `callback` 函数的频率。如果未指定，则在每一步调用回调。

+   `motion_field_strength_x` (`float`, *optional*, defaults to 12) — 生成视频沿 x 轴的运动强度。参见 [paper](https://arxiv.org/abs/2303.13439)，第 3.3.1 节。

+   `motion_field_strength_y` (`float`, *optional*, defaults to 12) — 生成视频沿 y 轴的运动强度。参见 [paper](https://arxiv.org/abs/2303.13439)，第 3.3.1 节。

+   `t0` (`int`, *optional*, defaults to 44) — 时间步 t0。应在范围 [0, num_inference_steps - 1] 内。参见 [paper](https://arxiv.org/abs/2303.13439)，第 3.3.1 节。

+   `t1` (`int`, *optional*, defaults to 47) — 时间步 t0。应在范围 [t0 + 1, num_inference_steps - 1] 内。参见 [paper](https://arxiv.org/abs/2303.13439)，第 3.3.1 节。

+   `frame_ids` (`List[int]`, *optional*) — 正在生成的帧的索引。在逐块生成较长视频时使用。

返回

[TextToVideoPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video_zero#diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.TextToVideoPipelineOutput)

当`output_type` != `"latent"`时，输出包含生成视频的`ndarray`，否则包含生成视频的潜在代码和一个`bool`列表，指示相应生成的视频是否包含“不安全内容”（nsfw）。

用于生成的管道的调用函数。

#### `backward_loop`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L375)

```py
( latents timesteps prompt_embeds guidance_scale callback callback_steps num_warmup_steps extra_step_kwargs cross_attention_kwargs = None ) → export const metadata = 'undefined';latents
```

参数

+   `callback`（`Callable`，*可选*）—在推断过程中每`callback_steps`步调用一次的函数。该函数使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps`（`int`，*可选*，默认为1）—调用`callback`函数的频率。如果未指定，将在每一步调用回调。extra_step_kwargs — 额外的步骤参数。cross_attention_kwargs — 如果指定，将传递给[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的`AttentionProcessor`的kwargs字典。num_warmup_steps — 预热步数。

返回

潜在代码

时间步长[-1]处的反向过程输出的潜在代码。

给定时间步长列表的反向过程。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L782)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）—要编码的提示设备 —（`torch.device`）：torch设备

+   `num_images_per_prompt`（`int`）—应该为每个提示生成的图像数量

+   `do_classifier_free_guidance`（`bool`）—是否使用分类器自由指导。

+   `negative_prompt`（`str`或`List[str]`，*可选*）—不用来指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即，如果`guidance_scale`小于`1`，则忽略）。

+   `prompt_embeds`（`torch.FloatTensor`，*可选*）—预生成的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds`（`torch.FloatTensor`，*可选*）—预生成的负文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数生成negative_prompt_embeds。

+   `lora_scale`（`float`，*可选*）—将应用于文本编码器的所有LoRA层的LoRA比例。

+   `clip_skip`（`int`，*可选*）—在计算提示嵌入时要跳过的CLIP层数。值为1意味着将使用预最终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

#### `forward_loop`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L351)

```py
( x_t0 t0 t1 generator ) → export const metadata = 'undefined';x_t1
```

参数

+   `generator`（`torch.Generator`或`List[torch.Generator]`，*可选*）—用于使生成具有确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

返回

x_t1

从时间t0到t1应用于x_t0的前向过程。

执行从时间t0到t1的DDPM前向过程。这与添加具有相应方差的噪声相同。

## 文本到视频零SDXL管道

### `class diffusers.TextToVideoZeroSDXLPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L328)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel text_encoder_2: CLIPTextModelWithProjection tokenizer: CLIPTokenizer tokenizer_2: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers image_encoder: CLIPVisionModelWithProjection = None feature_extractor: CLIPImageProcessor = None force_zeros_for_empty_prompt: bool = True add_watermarker: Optional = None )
```

参数

+   `vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）—变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。

+   `text_encoder`（`CLIPTextModel`）- 冻结的文本编码器。 Stable Diffusion XL 使用 [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel) 的文本部分，具体来说是 [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) 变体。

+   `text_encoder_2`（`CLIPTextModelWithProjection`）- 第二个冻结的文本编码器。 Stable Diffusion XL 使用 [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection) 的文本和池部分，具体来说是 [laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k) 变体。

+   `tokenizer`（`CLIPTokenizer`）- 类 [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer) 的分词器。

+   `tokenizer_2`（`CLIPTokenizer`）- 第二个分词器，类为 [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)。

+   `unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)）- 有条件的 U-Net 架构，用于去噪编码图像潜变量。

+   `scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)）- 用于与 `unet` 结合使用的调度器，以去噪编码图像潜变量。可以是 [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler) 或 [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler) 中的一个。

使用 Stable Diffusion XL 进行零样本文本到视频生成的流程。

此模型继承自 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看通用方法的超类文档，这些方法适用于所有流程（下载、保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L938)

```py
( prompt: Union prompt_2: Union = None video_length: Optional = 8 height: Optional = None width: Optional = None num_inference_steps: int = 50 denoising_end: Optional = None guidance_scale: float = 7.5 negative_prompt: Union = None negative_prompt_2: Union = None num_videos_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None frame_ids: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None latents: Optional = None motion_field_strength_x: float = 12 motion_field_strength_y: float = 12 output_type: Optional = 'tensor' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None guidance_rescale: float = 0.0 original_size: Optional = None crops_coords_top_left: Tuple = (0, 0) target_size: Optional = None t0: int = 44 t1: int = 47 )
```

参数

+   `prompt`（`str` 或 `List[str]`，*可选*）- 用于指导图像生成的提示。如果未定义，则必须传递 `prompt_embeds`。

+   `prompt_2`（`str` 或 `List[str]`，*可选*）- 要发送到 `tokenizer_2` 和 `text_encoder_2` 的提示。如果未定义，则在两个文本编码器中使用 `prompt`。

+   `video_length`（`int`，*可选*，默认为 8）- 生成的视频帧数。

+   `height`（`int`，*可选*，默认为 self.unet.config.sample_size * self.vae_scale_factor）- 生成图像的像素高度。

+   `width`（`int`，*可选*，默认为 self.unet.config.sample_size * self.vae_scale_factor）- 生成图像的像素宽度。

+   `num_inference_steps`（`int`，*可选*，默认为 50）- 去噪步骤的数量。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `denoising_end`（`float`，*可选*）- 当指定时，确定在故意提前终止之前完成的总去噪过程的比例（介于 0.0 和 1.0 之间）。因此，返回的样本仍将保留由调度器选择的离散时间步确定的大量噪声。当此流程作为“去噪混合”多流程设置的一部分时，应理想地利用 `denoising_end` 参数，如 [`Refining the Image Output`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output) 中所述。

+   `guidance_scale` (`float`, *optional*, 默认为 7.5) — 在 [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598) 中定义的指导比例。`guidance_scale` 被定义为 [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf) 中方程式 2 的 `w`。通过设置 `guidance_scale > 1` 来启用指导比例。更高的指导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。

+   `negative_prompt` (`str` 或 `List[str]`, *optional*) — 不指导图像生成的提示或提示。如果未定义，则必须传递 `negative_prompt_embeds`。如果不使用指导（即，如果 `guidance_scale` 小于 `1`，则将被忽略）。

+   `negative_prompt_2` (`str` 或 `List[str]`, *optional*) — 不指导图像生成的提示或提示，将发送到 `tokenizer_2` 和 `text_encoder_2`。如果未定义，则在两个文本编码器中使用 `negative_prompt`。

+   `num_videos_per_prompt` (`int`, *optional*, 默认为 1) — 每个提示生成的视频数量。

+   `eta` (`float`, *optional*, 默认为 0.0) — 对应于 DDIM 论文中的参数 eta (η)：[https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502)。仅适用于 [schedulers.DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，对其他情况将被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 一个或多个 [torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `frame_ids` (`List[int]`, *optional*) — 正在生成的帧的索引。在逐块生成较长视频时使用。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如 prompt 加权。如果未提供，将从 `prompt` 输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负面文本嵌入。可用于轻松调整文本输入，例如 prompt 加权。如果未提供，将从 `negative_prompt` 输入参数生成负面文本嵌入。

+   `pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的池化文本嵌入。可用于轻松调整文本输入，例如 prompt 加权。如果未提供，将从 `prompt` 输入参数生成池化文本嵌入。

+   `negative_pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负面池化文本嵌入。可用于轻松调整文本输入，例如 prompt 加权。如果未提供，将从 `negative_prompt` 输入参数生成负面池化文本嵌入。

+   `latents` (`torch.FloatTensor`, *optional*) — 预生成的噪声潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机 `generator` 进行采样生成潜变量张量。

+   `motion_field_strength_x` (`float`, *optional*, 默认为 12) — 生成视频中沿 x 轴的运动强度。参见 [paper](https://arxiv.org/abs/2303.13439)，第 3.3.1 节。

+   `motion_field_strength_y` (`float`, *optional*, 默认为 12) — 生成视频中沿 y 轴的运动强度。参见 [paper](https://arxiv.org/abs/2303.13439)，第 3.3.1 节。

+   `output_type` (`str`, *optional*, 默认为 `"pil"`) — 生成图像的输出格式。选择在 [PIL](https://pillow.readthedocs.io/en/stable/) 中的 `PIL.Image.Image` 或 `np.array` 之间。

+   `return_dict` (`bool`, *optional*, 默认为 `True`) — 是否返回 `~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput` 而不是普通元组。

+   `callback` (`Callable`, *optional*) — 在推断过程中每隔 `callback_steps` 步调用的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *optional*, 默认为 1) — `callback` 函数将被调用的频率。如果未指定，将在每一步调用回调。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给 `AttentionProcessor` 的 kwargs 字典，如在 [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py) 中定义。

+   `guidance_rescale` (`float`, *optional*, 默认为 0.7) — [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf) 提出的指导缩放因子 `guidance_scale` 在方程式 16 中定义为 `φ`。当使用零终端 SNR 时，指导缩放因子应该修复过度曝光。

+   `original_size` (`Tuple[int]`, *optional*, 默认为 (1024, 1024)) — 如果 `original_size` 与 `target_size` 不同，图像将呈现为缩小或放大。如果未指定，`original_size` 默认为 `(width, height)`。如在 [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952) 的第2.2节中所述，这是 SDXL 的微调节的一部分。

+   `crops_coords_top_left` (`Tuple[int]`, *optional*, 默认为 (0, 0)) — `crops_coords_top_left` 可用于生成一个看起来被从位置 `crops_coords_top_left` 向下“裁剪”的图像。通常通过将 `crops_coords_top_left` 设置为 (0, 0) 来实现有利的、居中的图像。如在 [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952) 的第2.2节中所述，这是 SDXL 的微调节的一部分。

+   `target_size` (`Tuple[int]`, *optional*, 默认为 (1024, 1024)) — 对于大多数情况，`target_size` 应设置为生成图像的期望高度和宽度。如果未指定，将默认为 `(width, height)`。如在 [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952) 的第2.2节中所述，这是 SDXL 的微调节的一部分。

+   `t0` (`int`, *optional*, 默认为 44) — 时间步 t0。应在范围 [0, num_inference_steps - 1] 内。参见 [paper](https://arxiv.org/abs/2303.13439)，第3.3.1节。

+   `t1` (`int`, *optional*, 默认为 47) — 时间步 t0。应在范围 [t0 + 1, num_inference_steps - 1] 内。参见 [paper](https://arxiv.org/abs/2303.13439)，第3.3.1节。

调用管道生成时调用的函数。

#### `backward_loop`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L853)

```py
( latents timesteps prompt_embeds guidance_scale callback callback_steps num_warmup_steps extra_step_kwargs add_text_embeds add_time_ids cross_attention_kwargs = None guidance_rescale: float = 0.0 ) → export const metadata = 'undefined';latents
```

参数

+   `callback` (`Callable`, *optional*) — 在推断过程中每隔 `callback_steps` 步调用的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *optional*, 默认为 1) — 调用 `callback` 函数的频率。如果未指定，将在每一步调用回调。extra_step_kwargs — 额外的步骤参数。cross_attention_kwargs — 如果指定，将传递给 `AttentionProcessor` 的 kwargs 字典，如在 [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) 中定义。num_warmup_steps — 预热步数。

返回

潜变量

反向过程在时间步 timesteps[-1] 输出的潜变量

给定时间步列表执行反向过程

#### `disable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L448)

```py
( )
```

禁用切片VAE解码。如果先前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L440)

```py
( )
```

启用切片VAE解码。启用此选项时，VAE将在几个步骤中将输入张量分割为片段以进行解码。这对节省一些内存并允许更大的批量大小很有用。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L594)

```py
( prompt: str prompt_2: Optional = None device: Optional = None num_images_per_prompt: int = 1 do_classifier_free_guidance: bool = True negative_prompt: Optional = None negative_prompt_2: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示

+   `prompt_2` (`str` 或 `List[str]`, *可选*) — 发送到`tokenizer_2`和`text_encoder_2`的提示或提示。如果未定义，则在两个文本编码器设备中使用`prompt` — (`torch.device`): torch设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用分类器自由指导

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。在不使用指导时被忽略（即，如果`guidance_scale`小于`1`，则被忽略）。

+   `negative_prompt_2` (`str` 或 `List[str]`, *可选*) — 不指导图像生成的提示或提示发送到`tokenizer_2`和`text_encoder_2`。如果未定义，则在两个文本编码器中使用`negative_prompt`

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成负negative_prompt_embeds。

+   `pooled_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成池化文本嵌入。

+   `negative_pooled_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成负池化的negative_prompt_embeds。

+   `lora_scale` (`float`, *可选*) — 如果加载了LoRA层，则将应用于文本编码器所有LoRA层的lora比例。

+   `clip_skip` (`int`, *可选*) — 从CLIP中跳过的层数，用于计算提示嵌入。值为1意味着将使用预最终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

#### `forward_loop`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero_sdxl.py#L829)

```py
( x_t0 t0 t1 generator ) → export const metadata = 'undefined';x_t1
```

参数

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

返回值

x_t1

从时间t0到t1应用于x_t0的前向过程。

从时间t0到t1执行DDPM前向过程。这与添加相应方差的噪声相同。

## 文本到视频管道输出

### `class diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.TextToVideoPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L182)

```py
( images: Union nsfw_content_detected: Optional )
```

参数

+   `images` (`[List[PIL.Image.Image]`, `np.ndarray`]) — 长度为 `batch_size` 的去噪 PIL 图像列表，或者形状为 `(batch_size, height, width, num_channels)` 的 NumPy 数组。

+   `nsfw_content_detected` (`[List[bool]]`) — 列表，指示相应生成的图像是否包含“不适宜工作场所”（nsfw）内容，如果无法执行安全检查，则为 `None`。

零样本文本到视频管道的输出类。
