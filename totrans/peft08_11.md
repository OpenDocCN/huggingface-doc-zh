# IA3

> 原文：[https://huggingface.co/docs/peft/task_guides/ia3](https://huggingface.co/docs/peft/task_guides/ia3)

[IA3](../conceptual_guides/ia3)将模型的激活（自注意力和编码器-解码器注意力块中的键和值，以及位置编码前馈网络的中间激活）乘以三个学习向量。这种PEFT方法引入的可训练参数数量比引入矩阵而不是向量的LoRA更少。原始模型的参数保持冻结，只更新这些向量。因此，对于新的下游任务进行微调更快、更便宜、更高效。

本指南将向您展示如何使用IA3训练一个序列到序列模型，以*生成情感*，给定一些财经新闻。

对序列到序列训练的一般过程有一定了解将非常有帮助，让您能够专注于如何应用IA3。如果您是新手，我们建议先从Transformers文档中查看[翻译](https://huggingface.co/docs/transformers/tasks/translation)和[摘要](https://huggingface.co/docs/transformers/tasks/summarization)指南。当您准备好时，回来看看将PEFT应用到您的训练中是多么容易！

## 数据集

您将使用[financial_phrasebank](https://huggingface.co/datasets/financial_phrasebank)数据集的sentences_allagree子集。该子集包含在情感标签上有100%注释者一致的财经新闻。查看[数据集查看器](https://huggingface.co/datasets/financial_phrasebank/viewer/sentences_allagree)以更好地了解您将要处理的数据和句子。

使用`load_dataset`函数加载数据集。数据集的子集仅包含训练集，因此使用`train_test_split`函数创建训练集和验证集。创建一个新的`text_label`列，以便更容易理解`label`值`0`、`1`和`2`的含义。

```py
from datasets import load_dataset

ds = load_dataset("financial_phrasebank", "sentences_allagree")
ds = ds["train"].train_test_split(test_size=0.1)
ds["validation"] = ds["test"]
del ds["test"]

classes = ds["train"].features["label"].names
ds = ds.map(
    lambda x: {"text_label": [classes[label] for label in x["label"]]},
    batched=True,
    num_proc=1,
)

ds["train"][0]
{'sentence': 'It will be operated by Nokia , and supported by its Nokia NetAct network and service management system .',
 'label': 1,
 'text_label': 'neutral'}
```

加载一个分词器并创建一个预处理函数，其中：

1.  对输入进行分词，填充和截断序列至`max_length`

1.  将相同的分词器应用于标签，但`max_length`较短，对应于标签的长度

1.  掩盖填充标记

```py
from transformers import AutoTokenizer

text_column = "sentence"
label_column = "text_label"
max_length = 128

tokenizer = AutoTokenizer.from_pretrained("bigscience/mt0-large")

def preprocess_function(examples):
    inputs = examples[text_column]
    targets = examples[label_column]
    model_inputs = tokenizer(inputs, max_length=max_length, padding="max_length", truncation=True, return_tensors="pt")
    labels = tokenizer(targets, max_length=3, padding="max_length", truncation=True, return_tensors="pt")
    labels = labels["input_ids"]
    labels[labels == tokenizer.pad_token_id] = -100
    model_inputs["labels"] = labels
    return model_inputs
```

使用[map](https://huggingface.co/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)函数将预处理函数应用于整个数据集。

```py
processed_ds = ds.map(
    preprocess_function,
    batched=True,
    num_proc=1,
    remove_columns=ds["train"].column_names,
    load_from_cache_file=False,
    desc="Running tokenizer on dataset",
)
```

创建一个训练和评估[`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)，并设置`pin_memory=True`以加快数据传输到GPU的速度，如果您的数据集样本在CPU上。

```py
from torch.utils.data import DataLoader
from transformers import default_data_collator

train_ds = processed_ds["train"]
eval_ds = processed_ds["validation"]

batch_size = 8

train_dataloader = DataLoader(
    train_ds, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True
)
eval_dataloader = DataLoader(eval_ds, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)
```

## 模型

现在您可以加载一个预训练模型作为IA3的基础模型。本指南使用[bigscience/mt0-large](https://huggingface.co/bigscience/mt0-large)模型，但您可以使用任何您喜欢的序列到序列模型。

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/mt0-large")
```

### PEFT配置和模型

所有PEFT方法都需要一个包含和指定PEFT方法应如何应用的所有参数的配置。创建一个包含任务类型并将推理模式设置为`False`的[IA3Config](/docs/peft/v0.8.2/en/package_reference/ia3#peft.IA3Config)。您可以在[API参考](../package_reference/ia3#ia3config)中找到此配置的其他参数。

调用[print_trainable_parameters()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.print_trainable_parameters)方法，比较[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)的可训练参数数量与基础模型的参数数量！

一旦配置设置完成，请将其传递给[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)函数，同时传递基础模型以创建可训练的[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)。

```py
from peft import IA3Config, get_peft_model

peft_config = IA3Config(task_type="SEQ_2_SEQ_LM")
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
"trainable params: 282,624 || all params: 1,229,863,936 || trainable%: 0.022980103060766553"
```

### 训练

设置优化器和学习率调度器。

```py
import torch
from transformers import get_linear_schedule_with_warmup

lr = 8e-3
num_epochs = 3

optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
lr_scheduler = get_linear_schedule_with_warmup(
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=(len(train_dataloader) * num_epochs),
)
```

将模型移至GPU，并创建一个训练循环，报告每个时期的损失和困惑度。

```py
from tqdm import tqdm

device = "cuda"
model = model.to(device)

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for step, batch in enumerate(tqdm(train_dataloader)):
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        total_loss += loss.detach().float()
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()

    model.eval()
    eval_loss = 0
    eval_preds = []
    for step, batch in enumerate(tqdm(eval_dataloader)):
        batch = {k: v.to(device) for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)
        loss = outputs.loss
        eval_loss += loss.detach().float()
        eval_preds.extend(
            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)
        )

    eval_epoch_loss = eval_loss / len(eval_dataloader)
    eval_ppl = torch.exp(eval_epoch_loss)
    train_epoch_loss = total_loss / len(train_dataloader)
    train_ppl = torch.exp(train_epoch_loss)
    print(f"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}")
```

## 分享您的模型

训练完成后，您可以使用[push_to_hub](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)方法将模型上传到Hub。您需要先登录到您的Hugging Face帐户，并在提示时输入您的令牌。

```py
from huggingface_hub import notebook_login

account = <your-hf-account-name>
peft_model_id = f"{account}/mt0-large-ia3"
model.push_to_hub(peft_model_id)
```

## 推理

要加载用于推理的模型，请使用[from_pretrained()](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel.from_pretrained)方法。还可以加载数据集中的一句金融新闻来生成情感。

```py
from peft import AutoPeftModelForSeq2SeqLM

model = AutoPeftModelForSeq2SeqLM.from_pretrained("<your-hf-account-name>/mt0-large-ia3").to("cuda")
tokenizer = AutoTokenizer.from_pretrained("bigscience/mt0-large")

i = 15
inputs = tokenizer(ds["validation"][text_column][i], return_tensors="pt")
print(ds["validation"][text_column][i])
"The robust growth was the result of the inclusion of clothing chain Lindex in the Group in December 2007 ."
```

调用[generate](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)方法生成预测的情感标签。

```py
with torch.no_grad():
    inputs = {k: v.to(device) for k, v in inputs.items()}
    outputs = model.generate(input_ids=inputs["input_ids"], max_new_tokens=10)
    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))
['positive']
```
