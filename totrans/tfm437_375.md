# 视觉编码器解码器模型

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder)

## 概述

[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)可用于使用任何预训练的基于Transformer的视觉模型作为编码器（例如[ViT](vit)、[BEiT](beit)、[DeiT](deit)、[Swin](swin)）和任何预训练语言模型作为解码器（例如[RoBERTa](roberta)、[GPT2](gpt2)、[BERT](bert)、[DistilBERT](distilbert)）初始化图像到文本模型。

使用预训练检查点初始化图像到文本序列模型的有效性已在Minghao Li、Tengchao Lv、Lei Cui、Yijuan Lu、Dinei Florencio、Cha Zhang、Zhoujun Li、Furu Wei的文章[TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282)中得到展示。

在训练/微调了这样一个[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)之后，它可以像其他模型一样保存/加载（有关更多信息，请参见下面的示例）。

一个示例应用是图像字幕，其中编码器用于对图像进行编码，之后自回归语言模型生成字幕。另一个示例是光学字符识别。请参考[TrOCR](trocr)，这是[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)的一个实例。

## 从模型配置随机初始化VisionEncoderDecoderModel。

[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)可以从编码器和解码器配置随机初始化。在以下示例中，我们展示了如何使用编码器的默认[ViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTModel)配置和解码器的默认`BertForCausalLM`配置来实现这一点。

```py
>>> from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel

>>> config_encoder = ViTConfig()
>>> config_decoder = BertConfig()

>>> config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)
>>> model = VisionEncoderDecoderModel(config=config)
```

## 从预训练的编码器和预训练的解码器初始化VisionEncoderDecoderModel。

[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)可以从预训练的编码器检查点和预训练的解码器检查点初始化。请注意，任何预训练的基于Transformer的视觉模型，例如[Swin](swin)，都可以作为编码器，而预训练的自编码模型，例如BERT，预训练的因果语言模型，例如GPT2，以及序列到序列模型的预训练解码器部分，例如BART的解码器，都可以作为解码器。根据您选择的解码器架构，交叉注意力层可能会被随机初始化。从预训练的编码器和解码器检查点初始化[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)需要对模型进行下游任务的微调，正如在*Warm-starting-encoder-decoder blog post*中所示。为此，`VisionEncoderDecoderModel`类提供了一个[VisionEncoderDecoderModel.from_encoder_decoder_pretrained()](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel.from_encoder_decoder_pretrained)方法。

```py
>>> from transformers import VisionEncoderDecoderModel

>>> model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "microsoft/swin-base-patch4-window7-224-in22k", "bert-base-uncased"
... )
```

## 加载现有的VisionEncoderDecoderModel检查点并执行推理。

要加载`VisionEncoderDecoderModel`类的微调检查点，[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)提供了`from_pretrained(...)`方法，就像Transformers中的任何其他模型架构一样。

要执行推断，可以使用 `generate` 方法，该方法允许自回归生成文本。此方法支持各种解码形式，如贪婪、束搜索和多项式采样。

```py
>>> import requests
>>> from PIL import Image

>>> from transformers import GPT2TokenizerFast, ViTImageProcessor, VisionEncoderDecoderModel

>>> # load a fine-tuned image captioning model and corresponding tokenizer and image processor
>>> model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
>>> tokenizer = GPT2TokenizerFast.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
>>> image_processor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

>>> # let's perform inference on an image
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> pixel_values = image_processor(image, return_tensors="pt").pixel_values

>>> # autoregressively generate caption (uses greedy decoding by default)
>>> generated_ids = model.generate(pixel_values)
>>> generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> print(generated_text)
a cat laying on a blanket next to a cat laying on a bed
```

## 将 PyTorch checkpoint 加载到 TFVisionEncoderDecoderModel 中。

[TFVisionEncoderDecoderModel.from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained) 目前不支持从 PyTorch checkpoint 初始化模型。将 `from_pt=True` 传递给此方法将引发异常。如果特定视觉编码器-解码器模型仅有 PyTorch checkpoints，可以使用以下解决方法：

```py
>>> from transformers import VisionEncoderDecoderModel, TFVisionEncoderDecoderModel

>>> _model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

>>> _model.encoder.save_pretrained("./encoder")
>>> _model.decoder.save_pretrained("./decoder")

>>> model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "./encoder", "./decoder", encoder_from_pt=True, decoder_from_pt=True
... )
>>> # This is only for copying some specific attributes of this particular model.
>>> model.config = _model.config
```

## 训练

创建模型后，可以类似于 BART、T5 或任何其他编码器-解码器模型在（图像，文本）对数据集上进行微调。正如您所看到的，为了计算损失，模型只需要 2 个输入：`pixel_values`（即图像）和 `labels`（即编码目标序列的 `input_ids`）。

```py
>>> from transformers import ViTImageProcessor, BertTokenizer, VisionEncoderDecoderModel
>>> from datasets import load_dataset

>>> image_processor = ViTImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")
>>> tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
>>> model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "google/vit-base-patch16-224-in21k", "bert-base-uncased"
... )

>>> model.config.decoder_start_token_id = tokenizer.cls_token_id
>>> model.config.pad_token_id = tokenizer.pad_token_id

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]
>>> pixel_values = image_processor(image, return_tensors="pt").pixel_values

>>> labels = tokenizer(
...     "an image of two cats chilling on a couch",
...     return_tensors="pt",
... ).input_ids

>>> # the forward function automatically creates the correct decoder_input_ids
>>> loss = model(pixel_values=pixel_values, labels=labels).loss
```

此模型由 [nielsr](https://github.com/nielsrogge) 贡献。此模型的 TensorFlow 和 Flax 版本由 [ydshieh](https://github.com/ydshieh) 贡献。

## VisionEncoderDecoderConfig

### `class transformers.VisionEncoderDecoderConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py#L33)

```py
( **kwargs )
```

参数

+   `kwargs`（*可选*）— 关键字参数字典。特别包括：

    +   `encoder`（[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，*可选*）— 定义编码器配置的配置对象实例。

    +   `decoder`（[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，*可选*）— 定义解码器配置的配置对象实例。

[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig) 是用于存储 [VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel) 配置的配置类。根据指定的参数实例化 Vision-Encoder-Text-Decoder 模型，定义编码器和解码器配置。

配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 的文档以获取更多信息。

示例：

```py
>>> from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel

>>> # Initializing a ViT & BERT style configuration
>>> config_encoder = ViTConfig()
>>> config_decoder = BertConfig()

>>> config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

>>> # Initializing a ViTBert model (with random weights) from a ViT & bert-base-uncased style configurations
>>> model = VisionEncoderDecoderModel(config=config)

>>> # Accessing the model configuration
>>> config_encoder = model.config.encoder
>>> config_decoder = model.config.decoder
>>> # set decoder config to causal lm
>>> config_decoder.is_decoder = True
>>> config_decoder.add_cross_attention = True

>>> # Saving the model, including its configuration
>>> model.save_pretrained("my-model")

>>> # loading model and config from pretrained folder
>>> encoder_decoder_config = VisionEncoderDecoderConfig.from_pretrained("my-model")
>>> model = VisionEncoderDecoderModel.from_pretrained("my-model", config=encoder_decoder_config)
```

#### `from_encoder_decoder_configs`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py#L100)

```py
( encoder_config: PretrainedConfig decoder_config: PretrainedConfig **kwargs ) → export const metadata = 'undefined';VisionEncoderDecoderConfig
```

返回

[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)

配置对象实例

从预训练编码器模型配置和解码器模型配置实例化一个 [VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)（或派生类）。

PytorchHide Pytorch content

## VisionEncoderDecoderModel

### `class transformers.VisionEncoderDecoderModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L150)

```py
( config: Optional = None encoder: Optional = None decoder: Optional = None )
```

参数

+   `config` ([VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)) — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

这个类可以用来初始化一个图像到文本序列模型，其中预训练的视觉自编码模型作为编码器，预训练的文本自回归模型作为解码器。编码器通过 [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained) 函数加载，解码器通过 [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained) 函数加载。交叉注意力层会自动添加到解码器，并应在下游生成任务（如图像字幕）上进行微调。

在 [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) 中，Sascha Rothe、Shashi Narayan、Aliaksei Severyn、Michael Matena、Yanqi Zhou、Wei Li、Peter J. Liu 展示了使用预训练检查点初始化序列到序列模型进行序列生成任务的有效性。

此外，在 [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) 中，展示了如何利用大型预训练视觉模型进行光学字符识别（OCR）可以显著提高性能。

训练/微调了这样一个视觉-编码器-文本-解码器模型后，它可以像其他模型一样保存/加载（有关更多信息，请参阅示例）。

这个模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库实现的所有模型的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel) 是一个通用的模型类，当使用 :meth*~transformers.AutoModel.from_pretrained* 类方法为编码器创建一个基础视觉模型类，并为解码器创建另一个基础视觉模型类时，将实例化为一个变压器架构。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L515)

```py
( pixel_values: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs ) → export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) — 像素值。像素值可以通过图像处理器获得（例如，如果您使用 ViT 作为编码器，应该使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)）。有关详细信息，请参阅 [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*) — 词汇表中解码器输入序列标记的索引。

    可以使用 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入 ID？](../glossary#input-ids)

    如果使用了 `past_key_values`，可选择仅输入最后的 `decoder_input_ids`（参见 `past_key_values`）。

    对于训练，`decoder_input_ids` 会被模型自动创建，通过将 `labels` 向右移动，用 `pad_token_id` 替换 -100，并在前面加上 `decoder_start_token_id`。

+   `decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*) — 默认行为：生成一个张量，忽略 `decoder_input_ids` 中的填充标记。因果掩码也将默认使用。

+   `encoder_outputs` (`tuple(torch.FloatTensor)`, *optional*) — 此元组必须包含 (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) 是编码器最后一层的隐藏状态张量。用于解码器的交叉注意力。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，长度为 `config.n_layers`，每个元组包含 4 个形状为 `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)` 的张量） — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。

    如果使用了 `past_key_values`，用户可以选择仅输入形状为 `(batch_size, 1)` 的最后的 `decoder_input_ids`（即没有将过去的键值状态提供给该模型的那些）而不是形状为 `(batch_size, sequence_length)` 的所有 `decoder_input_ids`。

+   `decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递 `decoder_input_ids`。如果您想要更多控制如何将 `decoder_input_ids` 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于计算解码器的掩码语言建模损失的标签。索引应在 `[-100, 0, ..., config.vocab_size]` 范围内（参见 `input_ids` 文档字符串）。索引设置为 `-100` 的标记将被忽略（掩码），损失仅计算具有标签在 `[0, ..., config.vocab_size]` 范围内的标记。

+   `use_cache` (`bool`, *optional*) — 如果设置为 `True`，将返回 `past_key_values` 键值状态，可用于加速解码（参见 `past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 如果设置为 `True`，模型将返回一个 `~utils.Seq2SeqLMOutput` 而不是一个普通元组。

+   `kwargs` (*optional*) — 剩余的关键字参数字典。关键字参数有两种类型：

    +   没有前缀，将作为 `**encoder_kwargs` 输入到编码器前向函数中。

    +   使用 *decoder_* 前缀，将作为 `**decoder_kwargs` 输入到解码器前向函数中。

返回

[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput) 或 `tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)）和输入而异的各种元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回) — 语言建模损失。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，*optional*，当传递`use_cache=True`或`config.use_cache=True`时返回) — 一个长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 一个元组，包含形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`（如果模型有嵌入层，则为嵌入层输出的一个+每层输出的一个）。

    解码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 一个元组，包含形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`（每层一个）。

    解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 一个元组，包含形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`（每层一个）。

    解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 一个元组，包含形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`（如果模型有嵌入层，则为嵌入层输出的一个+每层输出的一个）。

    编码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `encoder_attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 一个元组，包含形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`（每层一个）。

    编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

[VisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, VisionEncoderDecoderModel
>>> import requests
>>> from PIL import Image
>>> import torch

>>> processor = AutoProcessor.from_pretrained("microsoft/trocr-base-handwritten")
>>> model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")

>>> # load image from the IAM dataset
>>> url = "https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

>>> # training
>>> model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
>>> model.config.pad_token_id = processor.tokenizer.pad_token_id
>>> model.config.vocab_size = model.config.decoder.vocab_size

>>> pixel_values = processor(image, return_tensors="pt").pixel_values
>>> text = "hello world"
>>> labels = processor.tokenizer(text, return_tensors="pt").input_ids
>>> outputs = model(pixel_values=pixel_values, labels=labels)
>>> loss = outputs.loss

>>> # inference (generation)
>>> generated_ids = model.generate(pixel_values)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

#### `from_encoder_decoder_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L361)

```py
( encoder_pretrained_model_name_or_path: str = None decoder_pretrained_model_name_or_path: str = None *model_args **kwargs )
```

参数

+   `encoder_pretrained_model_name_or_path`（`str`，*可选*）- 启动图像编码器所需的信息。可以是：

    +   一个字符串，预训练模型的*模型ID*，托管在huggingface.co上的模型存储库内。一个示例是`google/vit-base-patch16-224-in21k`。

    +   一个*包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)保存的模型权重的目录*的路径，例如，`./my_model_directory/`。

    +   一个*指向tensorflow索引检查点文件*的路径或url（例如，`./tf_model/model.ckpt.index`）。在这种情况下，`from_tf`应设置为`True`，并且应将配置对象提供为`config`参数。使用此加载路径比使用提供的转换脚本将TensorFlow检查点转换为PyTorch模型并加载PyTorch模型要慢。

+   `decoder_pretrained_model_name_or_path`（`str`，*可选*，默认为`None`）- 启动文本解码器所需的信息。可以是：

    +   一个字符串，预训练模型的*模型ID*，托管在huggingface.co上的模型存储库内。有效的模型ID可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间，如`dbmdz/bert-base-german-cased`。

    +   一个*包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)保存的模型权重的目录*的路径，例如，`./my_model_directory/`。

    +   一个*指向tensorflow索引检查点文件*的路径或url（例如，`./tf_model/model.ckpt.index`）。在这种情况下，`from_tf`应设置为`True`，并且应将配置对象提供为`config`参数。使用此加载路径比使用提供的转换脚本将TensorFlow检查点转换为PyTorch模型并加载PyTorch模型要慢。

+   `model_args`（剩余的位置参数，*可选*）- 所有剩余的位置参数将传递给底层模型的`__init__`方法。

+   `kwargs`（剩余的关键字参数字典，*可选*）- 可用于更新配置对象（在加载后）并启动模型（例如，`output_attentions=True`）。

    +   要更新编码器配置，请为每个配置参数使用前缀*encoder_*。

    +   要更新解码器配置，请为每个配置参数使用前缀*decoder_*。

    +   要更新父模型配置，请不要为每个配置参数使用前缀。

    根据是否提供`config`而表现不同。

从预训练模型检查点中的一个或两个基类库中实例化一个编码器和一个解码器。

默认情况下，使用`model.eval()`将模型设置为评估模式（Dropout模块被停用）。要训练模型，您需要首先使用`model.train()`将其设置回训练模式。

示例：

```py
>>> from transformers import VisionEncoderDecoderModel

>>> # initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized
>>> model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "google/vit-base-patch16-224-in21k", "bert-base-uncased"
... )
>>> # saving model after fine-tuning
>>> model.save_pretrained("./vit-bert")
>>> # load fine-tuned model
>>> model = VisionEncoderDecoderModel.from_pretrained("./vit-bert")
```

TensorFlowHide TensorFlow内容

## TFVisionEncoderDecoderModel

### `class transformers.TFVisionEncoderDecoderModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py#L175)

```py
( config: Optional[PretrainedConfig] = None encoder: Optional[TFPreTrainedModel] = None decoder: Optional[TFPreTrainedModel] = None )
```

参数

+   `config`（[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)）— 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

这个类可用于使用任何预训练的视觉自编码模型作为编码器和任何预训练的文本自回归模型作为解码器来初始化一个图像到文本序列模型。编码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)函数加载，解码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)函数加载。交叉注意力层会自动添加到解码器，并应在下游生成任务（如图像字幕）上进行微调。

在[Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu的《利用预训练检查点进行序列生成任务》](https://arxiv.org/abs/1907.12461)中展示了使用预训练检查点初始化序列生成任务的序列到序列模型的有效性。

此外，在[TrOCR: 基于Transformer的预训练模型的光学字符识别](https://arxiv.org/abs/2109.10282)中展示了如何利用大型预训练的视觉模型进行光学字符识别（OCR）可以显著提高性能。

在训练/微调了这样一个Vision-Encoder-Text-Decoder模型之后，它可以像任何其他模型一样保存/加载（查看示例以获取更多信息）。

这个模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。

[TFVisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel)是一个通用模型类，当使用[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)类方法为编码器创建一个库中的基础视觉模型类，并为解码器创建另一个基础模型类时，将实例化为一个变压器架构。

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py#L458)

```py
( pixel_values: np.ndarray | tf.Tensor | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None labels: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False **kwargs ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqLMOutput or tuple(tf.Tensor)
```

参数

+   `pixel_values`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例的形状必须为`(batch_size, num_channels, height, width)`）— 像素值。像素值可以使用视觉模型的图像处理器获得。例如，使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)。有关详细信息，请参阅[ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `decoder_input_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, target_sequence_length)`, *optional*) — 词汇表中解码器输入序列标记的索引。

    可以使用 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入 ID？](../glossary#input-ids)

    如果使用了 `past_key_values`，可以选择仅输入最后的 `decoder_input_ids`（参见 `past_key_values`）。 

    为解码器提供序列到序列训练。可以使用 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

+   `decoder_attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, target_sequence_length)`, *optional*) — 默认行为：生成一个张量，忽略 `decoder_input_ids` 中的填充标记。因果掩码也将默认使用。

+   `encoder_outputs` (`tuple(tuple(tf.Tensor)`, *optional*) — 这个元组必须包含 (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`) 是编码器最后一层的隐藏状态张量。用于解码器的交叉注意力。

+   `past_key_values` (`tuple(tuple(tf.Tensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`) — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。

    如果使用了 `past_key_values`，用户可以选择仅输入最后的 `decoder_input_ids`（这些没有将它们的过去键值状态提供给此模型）的形状为 `(batch_size, 1)`，而不是所有形状为 `(batch_size, sequence_length)` 的 `decoder_input_ids`。

+   `decoder_inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*) — 可选地，您可以直接传递嵌入表示，而不是传递 `decoder_input_ids`。如果您想要更多控制权来将 `decoder_input_ids` 索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `labels` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于计算解码器的掩码语言建模损失的标签。索引应在 `[-100, 0, ..., config.vocab_size]`（参见 `input_ids` 文档字符串）。索引设置为 `-100` 的标记将被忽略（掩码），损失仅计算具有标签在 `[0, ..., config.vocab_size]` 中的标记。

+   `use_cache` (`bool`, *optional*) — 如果设置为 `True`，将返回 `past_key_values` 键值状态，并可用于加速解码（参见 `past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 如果设置为 `True`，模型将返回一个 `~utils.Seq2SeqLMOutput` 而不是一个普通元组。

+   `training` (`bool`, *optional*, 默认为`False`) — 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。

+   `kwargs` (*optional*) — 剩余的关键字参数字典。关键字参数有两种类型：

    +   没有前缀，将作为编码器前向函数的`**encoder_kwargs`输入。

    +   带有*decoder_*前缀，将作为解码器前向函数的`**decoder_kwargs`输入。

返回

[transformers.modeling_tf_outputs.TFSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput)或`tuple(tf.Tensor)`

一个[transformers.modeling_tf_outputs.TFSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)）和输入的各种元素。

+   `loss` (`tf.Tensor` of shape `(n,)`, *optional*, 其中n是未屏蔽标签的数量，在提供`labels`时返回) — 语言建模损失。

+   `logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `past_key_values` (`List[tf.Tensor]`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tf.Tensor`列表，每个张量的形状为`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`。

    包含解码器的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码。

+   `decoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组。

    解码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `decoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组。

    解码器的注意力权重，在注意力SoftMax之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组。

    解码器的交叉注意力层的注意力权重，在注意力SoftMax之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    编码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `encoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组。

    编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

[TFVisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel)的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, AutoTokenizer, TFVisionEncoderDecoderModel
>>> from PIL import Image
>>> import requests

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")
>>> decoder_tokenizer = AutoTokenizer.from_pretrained("gpt2")

>>> # initialize a bert2gpt2 from a pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized
>>> model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "google/vit-base-patch16-224-in21k", "gpt2"
... )

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> img = Image.open(requests.get(url, stream=True).raw)

>>> # forward
>>> pixel_values = image_processor(images=img, return_tensors="tf").pixel_values  # Batch size 1
>>> decoder_input_ids = decoder_tokenizer("Linda Davis", return_tensors="tf").input_ids  # Batch size 1
>>> outputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)

>>> # training
>>> outputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids, labels=decoder_input_ids)
>>> loss, logits = outputs.loss, outputs.logits

>>> # save and load from pretrained
>>> model.save_pretrained("vit-gpt2")
>>> model = TFVisionEncoderDecoderModel.from_pretrained("vit-gpt2")

>>> # generation
>>> generated = model.generate(pixel_values, decoder_start_token_id=model.config.decoder.bos_token_id)
```

#### `from_encoder_decoder_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py#L310)

```py
( encoder_pretrained_model_name_or_path: str = None decoder_pretrained_model_name_or_path: str = None *model_args **kwargs )
```

参数

+   `encoder_pretrained_model_name_or_path`（`str`，*可选*） — 初始化编码器所需的信息。可以是：

    +   预训练模型的*模型id*，托管在huggingface.co上的模型存储库中。例如，`google/vit-base-patch16-224-in21k`。

    +   指向使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained)保存的模型权重的*目录*的路径，例如，`./my_model_directory/`。

    +   指向*pytorch索引检查点文件*的路径或url（例如，`./pt_model/`）。在这种情况下，`encoder_from_pt`应设置为`True`。

+   `decoder_pretrained_model_name_or_path`（`str`，*可选*，默认为*None*） — 初始化解码器所需的信息。可以是：

    +   预训练模型的*模型id*，托管在huggingface.co上的模型存储库中。有效的模型id可以位于根级别，如`bert-base-uncased`，或在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。

    +   指向包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained)保存的模型权重的*目录*的路径，例如，`./my_model_directory/`。

    +   指向*pytorch检查点文件*的路径或url（例如，`./pt_model/`）。在这种情况下，`decoder_from_pt`应设置为`True`。

+   `model_args`（剩余的位置参数，*可选*） — 所有剩余的位置参数将传递给底层模型的`__init__`方法。

+   `kwargs`（剩余的关键字参数字典，*可选*） — 可用于更新配置对象（加载后）并初始化模型（例如，`output_attentions=True`）。

    +   更新编码器配置时，对每个配置参数使用前缀*encoder_*。

    +   更新解码器配置时，对每个配置参数使用前缀*decoder_*。

    +   要更新父模型配置，请不要对每个配置参数使用前缀。

    根据是否提供`config`或自动加载而表现不同。

从预训练模型检查点实例化一个编码器和一个解码器，可以是库中一个或两个基类的预训练模型检查点。

示例：

```py
>>> from transformers import TFVisionEncoderDecoderModel

>>> # initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized
>>> model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "google/vit-base-patch16-224-in21k", "bert-base-uncased"
... )
>>> # saving model after fine-tuning
>>> model.save_pretrained("./vit-bert")
>>> # load fine-tuned model
>>> model = TFVisionEncoderDecoderModel.from_pretrained("./vit-bert")
```

JAXHide JAX content

## FlaxVisionEncoderDecoderModel

### `class transformers.FlaxVisionEncoderDecoderModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L267)

```py
( config: VisionEncoderDecoderConfig input_shape: Optional = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

参数

+   `config`（[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)方法以加载模型权重。

+   `dtype`（`jax.numpy.dtype`，*可选*，默认为`jax.numpy.float32`） — 计算的数据类型。可以是`jax.numpy.float32`、`jax.numpy.float16`（在GPU上）和`jax.numpy.bfloat16`（在TPU上）之一。

    这可以用于在GPU或TPU上启用混合精度训练或半精度推断。如果指定了`dtype`，则所有计算将使用给定的`dtype`执行。

    `请注意，这仅指定计算的dtype，不影响模型参数的dtype。`

    如果您希望更改模型参数的dtype，请参阅[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)和[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)。

这个类可以用来初始化一个图像到文本序列模型，其中编码器是任何预训练的视觉自编码模型，解码器是任何预训练的文本自回归模型。编码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)函数加载，解码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)函数加载。交叉注意力层会自动添加到解码器上，并应该在下游生成任务（如图像字幕）上进行微调。

在[Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu](https://arxiv.org/abs/1907.12461)的研究中展示了使用预训练检查点初始化序列生成任务的序列到序列模型的有效性。

此外，在[TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282)中展示了如何利用大型预训练视觉模型进行光学字符识别（OCR）可以显著提高性能。

训练/微调了这样一个视觉-编码器-文本-解码器模型后，它可以像其他模型一样保存/加载（有关更多信息，请参阅示例）。

这个模型继承自[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)。查看超类文档以了解库实现的所有模型的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)子类。将其用作常规Flax模块，并参考Flax文档以了解与一般用法和行为相关的所有事项。

[FlaxVisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel)是一个通用的模型类，当使用:meth*~transformers.FlaxAutoModel.from_pretrained*类方法为编码器创建模块（flax.nn.Module）时，会实例化为一个transformer架构，库中的一个基本视觉模型类作为编码器模块，另一个作为解码器模块，并使用:meth*~transformers.FlaxAutoModelForCausalLM.from_pretrained*类方法为解码器创建模块。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L599)

```py
( pixel_values: Array decoder_input_ids: Optional = None decoder_attention_mask: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) → export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（`jnp.ndarray`，形状为`(batch_size, num_channels, height, width)`）— 像素值。像素值可以使用视觉模型的图像处理器获得。例如，使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)。有关详细信息，请参阅[ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `decoder_input_ids`（`jnp.ndarray`，形状为`(batch_size, target_sequence_length)`，*可选*）— 词汇表中解码器输入序列标记的索引。

    可以使用[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是解码器输入ID？](../glossary#decoder-input-ids)

+   `decoder_attention_mask` (`jnp.ndarray`，形状为`(batch_size, target_sequence_length)`，*可选*) — 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。

+   `decoder_position_ids` (`jnp.ndarray`，形状为`(batch_size, sequence_length)`，*可选*) — 每个解码器输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.decoder.max_position_embeddings - 1]`。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 如果设置为`True`，模型将返回一个`~utils.FlaxSeq2SeqLMOutput`而不是一个普通元组。

返回

[transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)）和输入而异的各种元素。

+   `logits` (`jnp.ndarray`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `past_key_values` (`tuple(tuple(jnp.ndarray))`, *可选*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tuple(jnp.ndarray)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入输出，一个用于每一层的输出）。

    解码器在每一层输出的隐藏状态以及初始嵌入输出。

+   `decoder_attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    解码器交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`jnp.ndarray`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出 + 一个用于每层的输出）。

    编码器在每层输出的隐藏状态加上初始嵌入输出。

+   `encoder_attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

[FlaxVisionEncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import FlaxVisionEncoderDecoderModel, AutoImageProcessor, AutoTokenizer
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")

>>> # load output tokenizer
>>> tokenizer_output = AutoTokenizer.from_pretrained("gpt2")

>>> # initialize a vit-gpt2 from pretrained ViT and GPT2 models. Note that the cross-attention layers will be randomly initialized
>>> model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "google/vit-base-patch16-224-in21k", "gpt2"
... )

>>> pixel_values = image_processor(images=image, return_tensors="np").pixel_values

>>> # use GPT2's eos_token as the pad as well as eos token
>>> model.config.eos_token_id = model.config.decoder.eos_token_id
>>> model.config.pad_token_id = model.config.eos_token_id

>>> # generation
>>> sequences = model.generate(pixel_values, num_beams=4, max_length=12).sequences

>>> captions = tokenizer_output.batch_decode(sequences, skip_special_tokens=True)
```

#### `from_encoder_decoder_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L724)

```py
( encoder_pretrained_model_name_or_path: Union = None decoder_pretrained_model_name_or_path: Union = None *model_args **kwargs )
```

参数

+   `encoder_pretrained_model_name_or_path` (`Union[str, os.PathLike]`, *可选*) — 初始化编码器所需的信息。可以是：

    +   一个字符串，托管在huggingface.co上的模型存储库中的预训练模型的*模型ID*。一个示例是`google/vit-base-patch16-224-in21k`。

    +   一个包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained)保存的模型权重的*目录*路径，例如，`./my_model_directory/`。

+   `decoder_pretrained_model_name_or_path` (`Union[str, os.PathLike]`, *可选*, 默认为`None`) — 初始化解码器所需的信息。可以是：

    +   一个字符串，预训练模型的*模型ID*，托管在huggingface.co上的模型存储库中。有效的模型ID可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。

    +   一个包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained)保存的模型权重的*目录*路径，例如，`./my_model_directory/`。

+   `model_args`（剩余的位置参数，*可选*） — 所有剩余的位置参数将传递给底层模型的`__init__`方法。

+   `kwargs`（剩余的关键字参数字典，*可选*） — 可用于更新配置对象（在加载后）并初始化模型（例如，`output_attentions=True`）。

    +   要更新编码器配置，请为每个配置参数使用前缀*encoder_*。

    +   要更新解码器配置，请为每个配置参数使用前缀*decoder_*。

    +   更新父模型配置时，不要为每个配置参数使用前缀。

    根据是否提供`config`或自动加载而表现不同。

从预训练模型检查点实例化一个编码器和一个解码器，可以是库中一个或两个基类。

示例：

```py
>>> from transformers import FlaxVisionEncoderDecoderModel

>>> # initialize a vit-gpt2 from a pretrained ViT and a pretrained GPT2 model. Note that the cross-attention layers will be randomly initialized
>>> model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "google/vit-base-patch16-224-in21k", "gpt2"
... )
>>> # saving model after fine-tuning
>>> model.save_pretrained("./vit-gpt2")
>>> # load fine-tuned model
>>> model = FlaxVisionEncoderDecoderModel.from_pretrained("./vit-gpt2")
```
