# 训练器

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer)

[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 类提供了一个用于在PyTorch中进行完整特征训练的API，并支持在多个GPU/TPU上进行分布式训练，支持[NVIDIA GPUs](https://nvidia.github.io/apex/)的混合精度，[AMD GPUs](https://rocm.docs.amd.com/en/latest/rocm.html)，以及PyTorch的[`torch.amp`](https://pytorch.org/docs/stable/amp.html)。[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 与[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)类相辅相成，后者提供了广泛的选项来自定义模型的训练方式。这两个类一起提供了一个完整的训练API。

[Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer) 和 [Seq2SeqTrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments) 继承自[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)和`TrainingArgument`类，它们适用于用于序列到序列任务（如摘要或翻译）的模型训练。

[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 类针对🤗 Transformers模型进行了优化，当与其他模型一起使用时可能会有一些意外行为。在使用自己的模型时，请确保：

+   您的模型始终返回元组或[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)的子类

+   如果提供了`labels`参数并且该损失作为元组的第一个元素返回（如果您的模型返回元组），则您的模型可以计算损失

+   您的模型可以接受多个标签参数（在[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中使用`label_names`指示它们的名称给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)），但它们中没有一个应该被命名为`"label"`

## 训练器

### `class transformers.Trainer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L236)

```py
( model: Union = None args: TrainingArguments = None data_collator: Optional = None train_dataset: Optional = None eval_dataset: Union = None tokenizer: Optional = None model_init: Optional = None compute_metrics: Optional = None callbacks: Optional = None optimizers: Tuple = (None, None) preprocess_logits_for_metrics: Optional = None )
```

参数

+   `model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel) 或 `torch.nn.Module`, *可选*) — 用于训练、评估或用于预测的模型。如果未提供，则必须传递一个`model_init`。

    [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 被优化为与库提供的[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)一起使用。只要您的模型与🤗 Transformers模型的工作方式相同，您仍然可以使用自己定义的`torch.nn.Module`模型。

+   `args` ([TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments), *可选*) — 用于调整训练的参数。如果未提供，将默认为一个基本的[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)实例，其中`output_dir`设置为当前目录中名为*tmp_trainer*的目录。

+   `data_collator` (`DataCollator`, *可选*) — 用于从`train_dataset`或`eval_dataset`的元素列表中形成批次的函数。如果未提供`tokenizer`，将默认为[default_data_collator()](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.default_data_collator)，否则将默认为[DataCollatorWithPadding](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding)的实例。

+   `train_dataset`（`torch.utils.data.Dataset`或`torch.utils.data.IterableDataset`，*可选*）— 用于训练的数据集。如果是[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，则不被`model.forward()`方法接受的列将自动删除。

    请注意，如果是带有一些随机化的`torch.utils.data.IterableDataset`，并且您正在以分布式方式进行训练，您的可迭代数据集应该使用一个内部属性`generator`，该属性是一个`torch.Generator`，用于在所有进程上保持相同的随机化（并且Trainer将在每个epoch手动设置此`generator`的种子），或者具有一个在内部设置用于随机数生成器的种子的`set_epoch()`方法。

+   `eval_dataset`（Union[`torch.utils.data.Dataset`，Dict[str，`torch.utils.data.Dataset`]），*可选*）— 用于评估的数据集。如果是[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，则不被`model.forward()`方法接受的列将自动删除。如果是字典，则将在每个数据集上评估，并在度量名称之前添加字典键。

+   `tokenizer`（[PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)，*可选*）— 用于预处理数据的分词器。如果提供，将用于在批处理输入时自动填充输入到最大长度，并将保存在模型中，以便更容易重新运行中断的训练或重用微调的模型。

+   `model_init`（`Callable[[], PreTrainedModel]`，*可选*）— 用于实例化要使用的模型的函数。如果提供，每次调用[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)都将从此函数给出的模型的新实例开始。

    该函数可能没有参数，或者包含一个参数，其中包含optuna/Ray Tune/SigOpt试验对象，以便根据超参数（例如层计数、内部层大小、丢失概率等）选择不同的架构。

+   `compute_metrics`（`Callable[[EvalPrediction], Dict]`，*可选*）— 用于在评估时计算指标的函数。必须接受[EvalPrediction](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.EvalPrediction)并返回一个字符串字典以表示指标值。

+   `callbacks`（[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)列表，*可选*）— 用于自定义训练循环的回调列表。将添加到[此处](callback)详细说明的默认回调列表中。

    如果要删除使用的默认回调之一，请使用[Trainer.remove_callback()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.remove_callback)方法。

+   `optimizers`（`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`，*可选*，默认为`(None, None)`）— 包含要使用的优化器和调度器的元组。将默认为您的模型上的[AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW)实例和由[get_linear_schedule_with_warmup()](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup)控制的调度器，由`args`指定。

+   `preprocess_logits_for_metrics`（`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`，*可选*）— 用于在每个评估步骤之前预处理logits的函数。必须接受两个张量，logits和标签，并根据需要返回处理后的logits。此函数所做的修改将反映在`compute_metrics`接收到的预测中。

    请注意，如果数据集没有标签，标签（第二个参数）将为`None`。

Trainer是一个简单但功能完备的PyTorch训练和评估循环，专为🤗 Transformers优化。

重要属性：

+   `model` - 始终指向核心模型。如果使用transformers模型，它将是一个[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)子类。

+   `model_wrapped` - 始终指向最外部的模型，以防一个或多个其他模块包装原始模型。这是应该用于前向传递的模型。例如，在`DeepSpeed`下，内部模型被包装在`DeepSpeed`中，然后再次包装在`torch.nn.DistributedDataParallel`中。如果内部模型没有被包装，那么`self.model_wrapped`与`self.model`相同。

+   `is_model_parallel` - 模型是否已切换到模型并行模式（与数据并行不同，这意味着一些模型层在不同的GPU上分割）。 

+   `place_model_on_device` - 是否自动将模型放置在设备上 - 如果使用模型并行或deepspeed，或者如果默认的`TrainingArguments.place_model_on_device`被覆盖为返回`False`，则将其设置为`False`。

+   `is_in_train` - 模型当前是否在运行`train`（例如，在`train`中调用`evaluate`时）

#### `add_callback`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L654)

```py
( callback )
```

参数

+   `callback`（`type`或[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)）- 一个[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)类或[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)的实例。在第一种情况下，将实例化该类的成员。

向当前的[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)列表中添加一个回调。

#### `autocast_smart_context_manager`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2734)

```py
( cache_enabled: Optional = True )
```

一个帮助器包装器，为`autocast`创建适当的上下文管理器，并根据情况提供所需的参数。

#### `compute_loss`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2785)

```py
( model inputs return_outputs = False )
```

Trainer如何计算损失。默认情况下，所有模型都在第一个元素中返回损失。

子类和覆盖以进行自定义行为。

#### `compute_loss_context_manager`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2728)

```py
( )
```

一个帮助器包装器，用于将上下文管理器组合在一起。

#### `create_model_card`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3564)

```py
( language: Optional = None license: Optional = None tags: Union = None model_name: Optional = None finetuned_from: Optional = None tasks: Union = None dataset_tags: Union = None dataset: Union = None dataset_args: Union = None )
```

参数

+   `language`（`str`，*可选*）- 模型的语言（如果适用）

+   `license`（`str`，*可选*）- 模型的许可证。如果原始模型给定给`Trainer`来自Hub上的repo，则将默认为使用的预训练模型的许可证。

+   `tags`（`str`或`List[str]`，*可选*）- 要包含在模型卡元数据中的一些标签。

+   `model_name`（`str`，*可选*）- 模型的名称。

+   `finetuned_from`（`str`，*可选*）- 用于微调此模型的模型的名称（如果适用）。如果原始模型给定给`Trainer`来自Hub，则将默认为原始模型的repo的名称。

+   `tasks`（`str`或`List[str]`，*可选*）- 一个或多个任务标识符，将包含在模型卡的元数据中。

+   `dataset_tags`（`str`或`List[str]`，*可选*）- 一个或多个数据集标签，将包含在模型卡的元数据中。

+   `dataset`（`str`或`List[str]`，*可选*）- 一个或多个数据集标识符，将包含在模型卡的元数据中。

+   `dataset_args` (`str` 或 `List[str]`, *可选*) — 一个或多个数据集参数，将包含在模型卡的元数据中。

使用`Trainer`可用的信息创建一个模型卡的草稿。

#### `create_optimizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L929)

```py
( )
```

设置优化器。

我们提供了一个合理的默认值，效果很好。如果您想使用其他内容，可以通过`optimizers`在Trainer的init中传递一个元组，或者在子类中重写此方法。

创建优化器和调度器

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L902)

```py
( num_training_steps: int )
```

设置优化器和学习率调度器。

我们提供了一个合理的默认值，效果很好。如果您想使用其他内容，可以通过`optimizers`在Trainer的init中传递一个元组，或者在子类中重写此方法（或`create_optimizer`和/或`create_scheduler`）。

#### `create_scheduler`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1109)

```py
( num_training_steps: int optimizer: Optimizer = None )
```

参数

+   `num_training_steps` (int) — 要执行的训练步骤数。

设置调度器。训练器的优化器必须在调用此方法之前设置好，或者作为参数传递。

#### `evaluate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3031)

```py
( eval_dataset: Union = None ignore_keys: Optional = None metric_key_prefix: str = 'eval' )
```

参数

+   `eval_dataset` (Union[`Dataset`, Dict[str, `Dataset`]), *可选*) — 如果要覆盖`self.eval_dataset`，请传递一个数据集。如果是一个[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，则`model.forward()`方法不接受的列将自动删除。如果是一个字典，它将对每个数据集进行评估，并在度量名称前加上字典键。数据集必须实现`__len__`方法。

    如果您传递一个以数据集名称为键、数据集为值的字典，评估将在每个数据集上单独运行。这对于监视训练如何影响其他数据集或仅仅获得更精细的评估很有用。当与`load_best_model_at_end`一起使用时，请确保`metric_for_best_model`引用确切地一个数据集。例如，如果为两个数据集`data1`和`data2`传递`{"data1": data1, "data2": data2}`，则可以指定`metric_for_best_model="eval_data1_loss"`来使用`data1`上的损失，以及`metric_for_best_model="eval_data1_loss"`来使用`data2`上的损失。

+   `ignore_keys` (`List[str]`, *可选*) — 在模型输出中应该被忽略的键的列表（如果它是一个字典）。

+   `metric_key_prefix` (`str`, *可选*, 默认为`"eval"`) — 用作指标键前缀的可选前缀。例如，如果前缀是`"eval"`（默认），则指标“bleu”将被命名为“eval_bleu”。

运行评估并返回指标。

调用脚本将负责提供计算指标的方法，因为它们是任务相关的（将其传递给`compute_metrics`参数进行初始化）。

您也可以子类化并重写此方法以注入自定义行为。

#### `evaluation_loop`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3191)

```py
( dataloader: DataLoader description: str prediction_loss_only: Optional = None ignore_keys: Optional = None metric_key_prefix: str = 'eval' )
```

预测/评估循环，由`Trainer.evaluate()`和`Trainer.predict()`共享。

可以使用带有或不带有标签的工作。

#### `floating_point_ops`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3529)

```py
( inputs: Dict ) → export const metadata = 'undefined';int
```

参数

+   `inputs` (`Dict[str, Union[torch.Tensor, Any]]`) — 模型的输入和目标。

返回

`int`

浮点运算的数量。

对于继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)的模型，使用该方法计算每次反向+前向传递的浮点运算次数。如果使用另一个模型，要么在模型中实现这样的方法，要么子类化并重写此方法。

#### `get_decay_parameter_names`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L918)

```py
( model )
```

获取将应用权重衰减的所有参数名称

请注意，一些模型实现了自己的layernorm而不是调用nn.LayerNorm，因此这个函数只过滤出nn.LayerNorm的实例

#### `get_eval_dataloader`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L834)

```py
( eval_dataset: Optional = None )
```

参数

+   `eval_dataset` (`torch.utils.data.Dataset`, *可选*) — 如果提供，将覆盖`self.eval_dataset`。如果它是一个[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，那些`model.forward()`方法不接受的列将被自动移除。它必须实现`__len__`。

返回评估`~torch.utils.data.DataLoader`。

如果您想要注入一些自定义行为，请子类化并重写此方法。

#### `get_optimizer_cls_and_kwargs`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L977)

```py
( args: TrainingArguments )
```

参数

+   `args` (`transformers.training_args.TrainingArguments`) — 训练会话的训练参数。

根据训练参数返回优化器类和优化器参数。

#### `get_test_dataloader`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L869)

```py
( test_dataset: Dataset )
```

参数

+   `test_dataset` (`torch.utils.data.Dataset`, *可选*) — 要使用的测试数据集。如果它是一个[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，那些`model.forward()`方法不接受的列将被自动移除。它必须实现`__len__`。

返回测试`~torch.utils.data.DataLoader`。

如果您想要注入一些自定义行为，请子类化并重写此方法。

#### `get_train_dataloader`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L778)

```py
( )
```

返回训练`~torch.utils.data.DataLoader`。

如果`train_dataset`不实现`__len__`，则不使用采样器，否则使用随机采样器（必要时适应分布式训练）。

如果您想要注入一些自定义行为，请子类化并重写此方法。

#### `hyperparameter_search`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2596)

```py
( hp_space: Optional = None compute_objective: Optional = None n_trials: int = 20 direction: Union = 'minimize' backend: Union = None hp_name: Optional = None **kwargs ) → export const metadata = 'undefined';[trainer_utils.BestRun or List[trainer_utils.BestRun]]
```

参数

+   `hp_space` (`Callable[["optuna.Trial"], Dict[str, float]]`, *可选*) — 定义超参数搜索空间的函数。将根据您的后端默认为`default_hp_space_optuna()`或`default_hp_space_ray()`或`default_hp_space_sigopt()`。

+   `compute_objective` (`Callable[[Dict[str, float]], float]`, *可选*) — 一个计算要最小化或最大化的目标的函数，从`evaluate`方法返回的指标中计算。将默认为`default_compute_objective()`。

+   `n_trials` (`int`, *可选*, 默认为100) — 测试运行的次数。

+   `direction` (`str` 或 `List[str]`, *可选*, 默认为`"minimize"`) — 如果是单目标优化，方向是`str`，可以是`"minimize"`或`"maximize"`，当优化验证损失时应选择`"minimize"`，当优化一个或多个指标时应选择`"maximize"`。如果是多目标优化，方向是`List[str]`，可以是`"minimize"`和`"maximize"`的列表，当优化验证损失时应选择`"minimize"`，当优化一个或多个指标时应选择`"maximize"`。

+   `backend`（`str`或`~training_utils.HPSearchBackend`，*可选*）—用于超参数搜索的后端。将默认为optuna、Ray Tune或SigOpt，取决于安装了哪个。如果所有都安装了，将默认为optuna。

+   `hp_name`（`Callable[["optuna.Trial"], str]`，*可选*）—定义试验/运行名称的函数。默认为None。

+   `kwargs`（`Dict[str, Any]`，*可选*）—传递给`optuna.create_study`或`ray.tune.run`的其他关键字参数。更多信息请参见：

    +   [optuna.create_study的文档](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html)

    +   [tune.run的文档](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run)

    +   [sigopt的文档](https://app.sigopt.com/docs/endpoints/experiments/create)

返回

[`trainer_utils.BestRun`或`List[trainer_utils.BestRun]`]

有关多目标优化的最佳运行或最佳运行的所有信息。实验摘要可以在Ray后端的`run_summary`属性中找到。

使用`optuna`、`Ray Tune`或`SigOpt`启动超参数搜索。优化的数量由`compute_objective`确定，默认情况下，当没有提供指标时返回评估损失的函数，否则返回所有指标的总和。

要使用这种方法，您需要在初始化[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)时提供一个`model_init`：我们需要在每次新运行时重新初始化模型。这与`optimizers`参数不兼容，因此您需要子类化[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)并重写方法[create_optimizer_and_scheduler()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_optimizer_and_scheduler)以获得自定义优化器/调度器。

#### `init_hf_repo`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3547)

```py
( )
```

在`self.args.hub_model_id`中初始化git存储库。

#### `is_local_process_zero`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2822)

```py
( )
```

此进程是否为本地（例如，在多台机器上以分布式方式进行训练时，如果是主要进程，则为一台机器上的进程）。

#### `is_world_process_zero`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2829)

```py
( )
```

此进程是否为全局主进程（在多台机器上以分布式方式进行训练时，只有一个进程会是`True`）。

#### `log`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2675)

```py
( logs: Dict )
```

参数

+   `logs`（`Dict[str, float]`）—要记录的值。

记录`logs`在观察训练的各种对象。

子类化并重写此方法以注入自定义行为。

#### `log_metrics`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L905)

```py
( split metrics )
```

参数

+   `split`（`str`）—模式/分割名称：`train`、`eval`、`test`之一

+   `metrics`（`Dict[str, float]`）—来自train/evaluate/predictmetrics的指标：指标字典

以特殊格式记录指标

在分布式环境下，这仅针对排名为0的进程执行。

关于内存报告的注意事项：

为了获得内存使用报告，您需要安装`psutil`。您可以使用`pip install psutil`来安装。

现在当运行此方法时，您将看到一个包含的报告：：

```py
init_mem_cpu_alloc_delta   =     1301MB
init_mem_cpu_peaked_delta  =      154MB
init_mem_gpu_alloc_delta   =      230MB
init_mem_gpu_peaked_delta  =        0MB
train_mem_cpu_alloc_delta  =     1345MB
train_mem_cpu_peaked_delta =        0MB
train_mem_gpu_alloc_delta  =      693MB
train_mem_gpu_peaked_delta =        7MB
```

**理解报告：**

+   例如，第一部分，例如`train__`，告诉您指标所属的阶段。以`init_`开头的报告将添加到运行的第一个阶段。因此，如果只运行评估，则将报告`__init__`的内存使用情况以及`eval_`指标。

+   第三部分，是`cpu`或`gpu`，告诉您它是通用RAM还是gpu0内存指标。

+   `*_alloc_delta` - 是阶段结束和开始时使用/分配内存计数器之间的差异 - 如果函数释放的内存多于分配的内存，则可能为负数。

+   `*_peaked_delta` - 是任何额外消耗然后释放的内存 - 相对于当前分配的内存计数器 - 它永远不会是负数。当您查看任何阶段的指标时，您将`alloc_delta` + `peaked_delta`相加，就知道完成该阶段需要多少内存。

仅对rank 0和gpu 0的进程进行报告（如果有gpu）。通常这已经足够了，因为主进程完成大部分工作，但如果使用模型并行，情况可能不太一样，其他GPU可能使用不同数量的gpu内存。在DataParallel下也不同，因为gpu0可能需要比其他GPU更多的内存，因为它存储了所有参与GPU的梯度和优化器状态。也许在未来，这些报告将发展到测量这些内容。

CPU RAM指标测量RSS（Resident Set Size），包括进程独有的内存和与其他进程共享的内存。重要的是要注意，它不包括被交换出的内存，因此报告可能不够精确。

CPU峰值内存是使用采样线程测量的。由于python的GIL，如果该线程在使用最高内存时没有运行的机会，它可能会错过一些峰值内存。因此，这份报告可能小于实际情况。使用`tracemalloc`将报告准确的峰值内存，但它不会报告python之外的内存分配。因此，如果某个C++ CUDA扩展分配了自己的内存，它将不会被报告。因此，它被放弃，以支持内存采样方法，该方法读取当前进程的内存使用情况。

GPU分配和峰值内存报告是通过`torch.cuda.memory_allocated()`和`torch.cuda.max_memory_allocated()`完成的。这个指标仅报告pytorch特定分配的“增量”，因为`torch.cuda`内存管理系统不跟踪pytorch之外分配的任何内存。例如，第一个cuda调用通常加载CUDA内核，可能占用0.5到2GB的GPU内存。

请注意，此跟踪器不考虑[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)的`__init__`、`train`、`evaluate`和`predict`调用之外的内存分配。

因为`evaluation`调用可能发生在`train`期间，我们无法处理嵌套调用，因为`torch.cuda.max_memory_allocated`是一个计数器，所以如果它被嵌套的eval调用重置，`train`的跟踪器将报告不正确的信息。如果这个[pytorch问题](https://github.com/pytorch/pytorch/issues/16266)得到解决，将有可能将这个类改为可重入。在那之前，我们只会跟踪`train`、`evaluate`和`predict`方法的外层级别。这意味着如果在`train`期间调用`eval`，后者将记录其内存使用情况以及前者的内存使用情况。

这也意味着如果在[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)调用期间使用任何其他工具`torch.cuda.reset_peak_memory_stats`，则gpu峰值内存统计数据可能无效。而且[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)将破坏任何依赖于自己调用`torch.cuda.reset_peak_memory_stats`的工具的正常行为。

为了获得最佳性能，您可能希望在生产运行中关闭内存分析。

#### `metrics_format`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L879)

```py
( metrics: Dict ) → export const metadata = 'undefined';metrics (Dict[str, float])
```

参数

+   `metrics` (`Dict[str, float]`) — 训练/评估/预测返回的指标

返回

metrics (`Dict[str, float]`)

重格式化的指标

将Trainer指标值重新格式化为人类可读的格式

#### `num_examples`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1128)

```py
( dataloader: DataLoader )
```

通过访问其数据集来获取`~torch.utils.data.DataLoader`中样本数的帮助程序。当数据加载器数据集不存在或没有长度时，尽可能估计

#### `num_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1142)

```py
( train_dl: DataLoader max_steps: Optional = None )
```

通过枚举数据加载器来获取`~torch.utils.data.DataLoader`中的标记数的帮助程序。

#### `pop_callback`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L665)

```py
( callback ) → export const metadata = 'undefined';TrainerCallback
```

参数

+   `callback`（`type`或[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)）- [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)类或[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)的实例。在第一种情况下，将弹出在回调列表中找到的该类的第一个成员。

返回

[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)

如果找到，将删除回调。

从当前的[TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)列表中删除回调并返回它。

如果未找到回调，则返回`None`（不会引发错误）。

#### `predict`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3129)

```py
( test_dataset: Dataset ignore_keys: Optional = None metric_key_prefix: str = 'test' )
```

参数

+   `test_dataset`（`Dataset`）- 要在其上运行预测的数据集。如果它是`datasets.Dataset`，则不被`model.forward()`方法接受的列将自动删除。必须实现方法`__len__`

+   `ignore_keys`（`List[str]`，*可选*）- 模型输出中应忽略的键列表（如果它是字典）。

+   `metric_key_prefix`（`str`，*可选*，默认为`"test"`）- 用作指标键前缀的可选前缀。例如，如果前缀是`"test"`（默认），则指标“bleu”将被命名为“test_bleu”。

运行预测并返回预测和潜在指标。

根据数据集和用例，您的测试数据集可能包含标签。在这种情况下，此方法还将返回指标，就像在`evaluate()`中一样。

如果您的预测或标签具有不同的序列长度（例如，因为您在标记分类任务中进行动态填充），则预测将被填充（在右侧），以允许连接到一个数组中。填充索引为-100。

返回：*NamedTuple* 具有以下键的命名元组：

+   预测（`np.ndarray`）：在`test_dataset`上的预测。

+   label_ids（`np.ndarray`，*可选*）：标签（如果数据集包含）。

+   指标（`Dict[str, float]`，*可选*）：潜在的指标字典（如果数据集包含标签）。

#### `prediction_loop`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3766)

```py
( dataloader: DataLoader description: str prediction_loss_only: Optional = None ignore_keys: Optional = None metric_key_prefix: str = 'eval' )
```

预测/评估循环，由`Trainer.evaluate()`和`Trainer.predict()`共享。

无论是否有标签，都可以使用。

#### `prediction_step`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3424)

```py
( model: Module inputs: Dict prediction_loss_only: bool ignore_keys: Optional = None ) → export const metadata = 'undefined';Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]
```

参数

+   `model`（`nn.Module`）- 要评估的模型。

+   `inputs`（`Dict[str, Union[torch.Tensor, Any]]`）- 模型的输入和目标。

    在馈送模型之前，字典将被解包。大多数模型希望目标在参数`labels`下。检查您模型的文档以获取所有接受的参数。

+   `prediction_loss_only`（`bool`）- 是否仅返回损失。

+   `ignore_keys`（`List[str]`，*可选*）- 模型输出中应忽略的键列表（如果它是字典）。

返回

Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]

一个包含损失、logits 和标签的元组（每个都是可选的）。

使用 `inputs` 在 `model` 上执行评估步骤。

子类和覆盖以注入自定义行为。

#### `propagate_args_to_deepspeed`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L4012)

```py
( auto_find_batch_size = False )
```

根据 Trainer 参数在 deepspeed 插件中设置值

#### `push_to_hub`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L3702)

```py
( commit_message: Optional = 'End of training' blocking: bool = True **kwargs )
```

参数

+   `commit_message` (`str`, *可选*, 默认为 `"End of training"`) — 推送时要提交的消息。

+   `blocking` (`bool`, *可选*, 默认为 `True`) — 函数是否应该在 `git push` 完成后才返回。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 传递给 [create_model_card()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.create_model_card) 的额外关键字参数。

将 `self.model` 和 `self.tokenizer` 上传到 🤗 模型中心，存储库为 `self.args.hub_model_id`。

#### `remove_callback`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L681)

```py
( callback )
```

参数

+   `回调` (`类型` 或 [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback)) — 一个 [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback) 类或一个 [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback) 的实例。在第一种情况下，将删除在回调列表中找到的该类的第一个成员。

从当前的 [TrainerCallback](/docs/transformers/v4.37.2/en/main_classes/callback#transformers.TrainerCallback) 列表中删除一个回调。

#### `save_metrics`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L995)

```py
( split metrics combined = True )
```

参数

+   `split` (`str`) — 模式/拆分名称：`train`, `eval`, `test`, `all` 中的一个

+   `metrics` (`Dict[str, float]`) — 从训练/评估/预测返回的指标

+   `combined` (`bool`, *可选*, 默认为 `True`) — 通过更新 `all_results.json` 创建组合指标，其中包括此调用的指标。

将指标保存到一个 json 文件中，例如 `train_results.json`。

在分布式环境下，这仅适用于秩为0的进程。

要了解指标，请阅读 [log_metrics()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.log_metrics) 的文档字符串。唯一的区别是原始未格式化的数字保存在当前方法中。

#### `save_model`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2841)

```py
( output_dir: Optional = None _internal_call: bool = False )
```

将保存模型，因此您可以使用 `from_pretrained()` 重新加载它。

仅从主进程保存。

#### `save_state`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L1033)

```py
( )
```

保存 Trainer 状态，因为 Trainer.save_model 仅保存了模型的 tokenizer

在分布式环境下，这仅适用于秩为0的进程。

#### `train`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L1438)

```py
( resume_from_checkpoint: Union = None trial: Union = None ignore_keys_for_eval: Optional = None **kwargs )
```

参数

+   `resume_from_checkpoint` (`str` 或 `bool`, *可选*) — 如果是 `str`，则是由之前的 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 实例保存的本地路径的检查点。如果是 `bool` 并且等于 `True`，则加载由之前的 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 实例保存在 *args.output_dir* 中的最后一个检查点。如果存在，训练将从此处加载的模型/优化器/调度器状态恢复。

+   `trial` (`optuna.Trial` 或 `Dict[str, Any]`, *可选*) — 运行试验或用于超参数搜索的超参数字典。

+   `ignore_keys_for_eval` (`List[str]`, *可选*) — 您的模型输出中应在评估期间忽略的键的列表（如果它是一个字典）。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 用于隐藏已弃用参数的附加关键字参数

主要训练入口。

#### `training_step`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer.py#L2746)

```py
( model: Module inputs: Dict ) → export const metadata = 'undefined';torch.Tensor
```

参数

+   `model` (`nn.Module`) — 要训练的模型。

+   `inputs` (`Dict[str, Union[torch.Tensor, Any]]`) — 模型的输入和目标。

    字典将在馈送到模型之前解包。大多数模型期望目标在参数`labels`下。检查您模型的文档以获取所有接受的参数。

返回

`torch.Tensor`

这批次的训练损失的张量。

对一批输入执行训练步骤。

子类和覆盖以注入自定义行为。

## Seq2SeqTrainer

### `class transformers.Seq2SeqTrainer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_seq2seq.py#L41)

```py
( model: Union = None args: TrainingArguments = None data_collator: Optional = None train_dataset: Optional = None eval_dataset: Union = None tokenizer: Optional = None model_init: Optional = None compute_metrics: Optional = None callbacks: Optional = None optimizers: Tuple = (None, None) preprocess_logits_for_metrics: Optional = None )
```

#### `evaluate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_seq2seq.py#L112)

```py
( eval_dataset: Optional = None ignore_keys: Optional = None metric_key_prefix: str = 'eval' **gen_kwargs )
```

参数

+   `eval_dataset` (`Dataset`, *可选*) — 如果要覆盖`self.eval_dataset`，请传递一个数据集。如果它是一个[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，则不被`model.forward()`方法接受的列将自动删除。它必须实现`__len__`方法。

+   `ignore_keys` (`List[str]`, *可选*) — 您的模型输出中应在收集预测时忽略的键的列表。

+   `metric_key_prefix` (`str`, *可选*, 默认为`"eval"`) — 用作指标键前缀的可选前缀。例如，如果前缀是`"eval"`（默认），则指标“bleu”将被命名为“eval_bleu”。

+   `max_length` (`int`, *可选*) — 在使用`generate`方法进行预测时使用的最大目标长度。

+   `num_beams` (`int`, *可选*) — 在使用`generate`方法进行预测时将用于束搜索的束数。1表示没有束搜索。gen_kwargs — 附加的`generate`特定kwargs。

运行评估并返回指标。

调用脚本将负责提供计算指标的方法，因为它们是任务相关的（将其传递给init `compute_metrics`参数）。

您还可以子类化并覆盖此方法以注入自定义行为。

#### `predict`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_seq2seq.py#L168)

```py
( test_dataset: Dataset ignore_keys: Optional = None metric_key_prefix: str = 'test' **gen_kwargs )
```

参数

+   `test_dataset` (`Dataset`) — 要在其上运行预测的数据集。如果它是一个[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)，则不被`model.forward()`方法接受的列将自动删除。必须实现`__len__`方法

+   `ignore_keys` (`List[str]`, *可选*) — 您的模型输出中应在收集预测时忽略的键的列表。

+   `metric_key_prefix` (`str`, *可选*, 默认为`"eval"`) — 用作指标键前缀的可选前缀。例如，如果前缀是`"eval"`（默认），则指标“bleu”将被命名为“eval_bleu”。

+   `max_length` (`int`, *可选*) — 在使用`generate`方法进行预测时使用的最大目标长度。

+   `num_beams` (`int`, *可选*) — 在使用`generate`方法进行预测时将用于束搜索的束数。1表示没有束搜索。gen_kwargs — 附加的`generate`特定kwargs。

运行预测并返回预测和潜在指标。

根据数据集和您的用例，您的测试数据集可能包含标签。在这种情况下，此方法还将返回指标，就像在`evaluate()`中一样。

如果您的预测或标签具有不同的序列长度（例如，因为您在标记分类任务中进行动态填充），则预测将被填充（在右侧）以允许连接成一个数组。填充索引为 -100。

返回: *NamedTuple* 具有以下键的命名元组:

+   predictions (`np.ndarray`): 在 `test_dataset` 上的预测。

+   label_ids (`np.ndarray`, *optional*): 标签（如果数据集包含标签）。

+   metrics (`Dict[str, float]`, *optional*): 潜在的指标字典（如果数据集包含标签）。

## TrainingArguments

### `class transformers.TrainingArguments`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L161)

```py
( output_dir: str overwrite_output_dir: bool = False do_train: bool = False do_eval: bool = False do_predict: bool = False evaluation_strategy: Union = 'no' prediction_loss_only: bool = False per_device_train_batch_size: int = 8 per_device_eval_batch_size: int = 8 per_gpu_train_batch_size: Optional = None per_gpu_eval_batch_size: Optional = None gradient_accumulation_steps: int = 1 eval_accumulation_steps: Optional = None eval_delay: Optional = 0 learning_rate: float = 5e-05 weight_decay: float = 0.0 adam_beta1: float = 0.9 adam_beta2: float = 0.999 adam_epsilon: float = 1e-08 max_grad_norm: float = 1.0 num_train_epochs: float = 3.0 max_steps: int = -1 lr_scheduler_type: Union = 'linear' lr_scheduler_kwargs: Optional = <factory> warmup_ratio: float = 0.0 warmup_steps: int = 0 log_level: Optional = 'passive' log_level_replica: Optional = 'warning' log_on_each_node: bool = True logging_dir: Optional = None logging_strategy: Union = 'steps' logging_first_step: bool = False logging_steps: float = 500 logging_nan_inf_filter: bool = True save_strategy: Union = 'steps' save_steps: float = 500 save_total_limit: Optional = None save_safetensors: Optional = True save_on_each_node: bool = False save_only_model: bool = False no_cuda: bool = False use_cpu: bool = False use_mps_device: bool = False seed: int = 42 data_seed: Optional = None jit_mode_eval: bool = False use_ipex: bool = False bf16: bool = False fp16: bool = False fp16_opt_level: str = 'O1' half_precision_backend: str = 'auto' bf16_full_eval: bool = False fp16_full_eval: bool = False tf32: Optional = None local_rank: int = -1 ddp_backend: Optional = None tpu_num_cores: Optional = None tpu_metrics_debug: bool = False debug: Union = '' dataloader_drop_last: bool = False eval_steps: Optional = None dataloader_num_workers: int = 0 past_index: int = -1 run_name: Optional = None disable_tqdm: Optional = None remove_unused_columns: Optional = True label_names: Optional = None load_best_model_at_end: Optional = False metric_for_best_model: Optional = None greater_is_better: Optional = None ignore_data_skip: bool = False fsdp: Union = '' fsdp_min_num_params: int = 0 fsdp_config: Optional = None fsdp_transformer_layer_cls_to_wrap: Optional = None deepspeed: Optional = None label_smoothing_factor: float = 0.0 optim: Union = 'adamw_torch' optim_args: Optional = None adafactor: bool = False group_by_length: bool = False length_column_name: Optional = 'length' report_to: Optional = None ddp_find_unused_parameters: Optional = None ddp_bucket_cap_mb: Optional = None ddp_broadcast_buffers: Optional = None dataloader_pin_memory: bool = True dataloader_persistent_workers: bool = False skip_memory_metrics: bool = True use_legacy_prediction_loop: bool = False push_to_hub: bool = False resume_from_checkpoint: Optional = None hub_model_id: Optional = None hub_strategy: Union = 'every_save' hub_token: Optional = None hub_private_repo: bool = False hub_always_push: bool = False gradient_checkpointing: bool = False gradient_checkpointing_kwargs: Optional = None include_inputs_for_metrics: bool = False fp16_backend: str = 'auto' push_to_hub_model_id: Optional = None push_to_hub_organization: Optional = None push_to_hub_token: Optional = None mp_parameters: str = '' auto_find_batch_size: bool = False full_determinism: bool = False torchdynamo: Optional = None ray_scope: Optional = 'last' ddp_timeout: Optional = 1800 torch_compile: bool = False torch_compile_backend: Optional = None torch_compile_mode: Optional = None dispatch_batches: Optional = None split_batches: Optional = False include_tokens_per_second: Optional = False include_num_input_tokens_seen: Optional = False neftune_noise_alpha: float = None )
```

参数

+   `output_dir` (`str`) — 模型预测和检查点将被写入的输出目录。

+   `overwrite_output_dir` (`bool`, *optional*, 默认为 `False`) — 如果为 `True`，则覆盖输出目录的内容。使用此选项继续训练，如果 `output_dir` 指向检查点目录。

+   `do_train` (`bool`, *optional*, 默认为 `False`) — 是否运行训练。此参数不会直接被 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 使用，而是用于您的训练/评估脚本。查看 [示例脚本](https://github.com/huggingface/transformers/tree/main/examples) 获取更多详细信息。

+   `do_eval` (`bool`, *optional*) — 是否在验证集上运行评估。如果 `evaluation_strategy` 与 `"no"` 不同，则将设置为 `True`。此参数不会直接被 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 使用，而是用于您的训练/评估脚本。查看 [示例脚本](https://github.com/huggingface/transformers/tree/main/examples) 获取更多详细信息。

+   `do_predict` (`bool`, *optional*, 默认为 `False`) — 是否在测试集上运行预测。此参数不会直接被 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 使用，而是用于您的训练/评估脚本。查看 [示例脚本](https://github.com/huggingface/transformers/tree/main/examples) 获取更多详细信息。

+   `evaluation_strategy` (`str` 或 [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy), *optional*, 默认为 `"no"`) — 训练期间采用的评估策略。可能的值为:

    +   `"no"`: 训练期间不进行评估。

    +   `"steps"`: 每 `eval_steps` 次进行评估（并记录日志）。

    +   `"epoch"`: 每个时期结束时进行评估。

+   `prediction_loss_only` (`bool`, *optional*, 默认为 `False`) — 在进行评估和生成预测时，仅返回损失。

+   `per_device_train_batch_size` (`int`, *optional*, 默认为 8) — 用于训练的每个 GPU/XPU/TPU/MPS/NPU 核心/CPU 的批处理大小。

+   `per_device_eval_batch_size` (`int`, *optional*, 默认为 8) — 用于评估的每个 GPU/XPU/TPU/MPS/NPU 核心/CPU 的批处理大小。

+   `gradient_accumulation_steps` (`int`, *optional*, 默认为 1) — 累积梯度的更新步数，然后执行反向/更新传递。

    使用梯度累积时，一个步骤被计为一个带有反向传递的步骤。因此，每 `gradient_accumulation_steps * xxx_step` 训练示例将进行日志记录、评估和保存。

+   `eval_accumulation_steps` (`int`, *optional*) — 在将结果移动到 CPU 之前，累积输出张量的预测步数。如果未设置，整个预测将在 GPU/NPU/TPU 上累积后再移动到 CPU（速度更快但需要更多内存）。

+   `eval_delay` (`float`, *optional*) — 在进行第一次评估之前等待的时期或步数，取决于 `evaluation_strategy`。

+   `learning_rate` (`float`, *optional*, defaults to 5e-5) — [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW) 优化器的初始学习率。

+   `weight_decay` (`float`, *optional*, defaults to 0) — 应用的权重衰减（如果不为零）到 [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW) 优化器中的所有层，除了所有偏置和 LayerNorm 权重。

+   `adam_beta1` (`float`, *optional*, defaults to 0.9) — [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW) 优化器的 beta1 超参数。

+   `adam_beta2` (`float`, *optional*, defaults to 0.999) — [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW) 优化器的 beta2 超参数。

+   `adam_epsilon` (`float`, *optional*, defaults to 1e-8) — [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW) 优化器的 epsilon 超参数。

+   `max_grad_norm` (`float`, *optional*, defaults to 1.0) — 最大梯度范数（用于梯度裁剪）。

+   `num_train_epochs(float,` *optional*, defaults to 3.0) — 执行的总训练时代数（如果不是整数，则在停止训练之前执行最后一个时代的小数部分百分比）。

+   `max_steps` (`int`, *optional*, defaults to -1) — 如果设置为正数，则执行的总训练步数。覆盖 `num_train_epochs`。对于有限的数据集，训练通过数据集（如果所有数据都用完）重复进行，直到达到 `max_steps`。

+   `lr_scheduler_type` (`str` or [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType), *optional*, defaults to `"linear"`) — 要使用的调度器类型。查看 [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType) 的文档以获取所有可能的值。

+   `lr_scheduler_kwargs` (‘dict’, *optional*, defaults to {}) — lr_scheduler 的额外参数。查看每个调度器的文档以获取可能的值。

+   `warmup_ratio` (`float`, *optional*, defaults to 0.0) — 用于从 0 线性预热到 `learning_rate` 的总训练步数的比率。

+   `warmup_steps` (`int`, *optional*, defaults to 0) — 用于从 0 线性预热到 `learning_rate` 的步骤数。覆盖 `warmup_ratio` 的任何效果。

+   `log_level` (`str`, *optional*, defaults to `passive`) — 主进程使用的记录器日志级别。可能的选择是字符串形式的日志级别：'debug'、'info'、'warning'、'error'和'critical'，以及一个'passive'级别，它不设置任何内容，并保持Transformers库的当前日志级别（默认为`"warning"`）。

+   `log_level_replica` (`str`, *optional*, defaults to `"warning"`) — 副本使用的记录器日志级别。与 `log_level` 相同的选择。

+   `log_on_each_node` (`bool`, *optional*, defaults to `True`) — 在多节点分布式训练中，是否每个节点使用 `log_level` 进行日志记录，还是仅在主节点上进行。

+   `logging_dir` (`str`, *optional*) — [TensorBoard](https://www.tensorflow.org/tensorboard) 日志目录。默认为 *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***。

+   `logging_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy), *optional*, defaults to `"steps"`) — 训练期间采用的日志记录策略。可能的值有：

    +   `"no"`: 训练期间不进行日志记录。

    +   `"epoch"`: 每个时代结束时进行日志记录。

    +   `"steps"`: 每 `logging_steps` 进行日志记录。

+   `logging_first_step` (`bool`, *optional*, defaults to `False`) — 是否记录和评估第一个 `global_step`。

+   `logging_steps` (`int` or `float`, *optional*, defaults to 500) — 如果 `logging_strategy="steps"`，则在两个日志之间的更新步骤数。应为整数或范围为 `[0,1)` 的浮点数。如果小于 1，将被解释为总训练步骤的比率。

+   `logging_nan_inf_filter` (`bool`, *optional*, defaults to `True`) — 是否过滤用于记录的 `nan` 和 `inf` 损失。如果设置为 `True`，则会过滤每个步骤的损失，如果为 `nan` 或 `inf`，则取当前日志窗口的平均损失。 

    `logging_nan_inf_filter` 仅影响损失值的记录，不会改变梯度的计算或应用于模型的行为。

+   `save_strategy` (`str` or [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy), *optional*, defaults to `"steps"`) — 训练期间采用的检查点保存策略。可能的值有：

    +   `"no"`: 训练期间不进行保存。

    +   `"epoch"`: 在每个时期结束时保存。

    +   `"steps"`: 每 `save_steps` 保存一次。

+   `save_steps` (`int` or `float`, *optional*, defaults to 500) — 如果 `save_strategy="steps"`，在两次检查点保存之前的更新步骤数。应为整数或范围为 `[0,1)` 的浮点数。如果小于 1，将被解释为总训练步骤的比率。

+   `save_total_limit` (`int`, *optional*) — 如果传递了一个值，将限制检查点的总量。删除 `output_dir` 中的旧检查点。当启用 `load_best_model_at_end` 时，根据 `metric_for_best_model` 的“最佳”检查点将始终保留在最近的检查点之外。例如，对于 `save_total_limit=5` 和 `load_best_model_at_end`，最后四个检查点将始终与最佳模型一起保留。当 `save_total_limit=1` 和 `load_best_model_at_end` 时，可能保存两个检查点：最后一个和最佳一个（如果它们不同）。

+   `save_safetensors` (`bool`, *optional*, defaults to `True`) — 使用 [safetensors](https://huggingface.co/docs/safetensors) 保存和加载状态字典，而不是默认的 `torch.load` 和 `torch.save`。

+   `save_on_each_node` (`bool`, *optional*, defaults to `False`) — 在进行多节点分布式训练时，是否在每个节点上保存模型和检查点，还是只在主节点上保存。

    当不同节点使用相同存储时，不应激活此选项，因为文件将以相同名称保存在每个节点上。

+   `save_only_model` (`bool`, *optional*, defaults to `False`) — 在进行检查点时，是否仅保存模型，还是同时保存优化器、调度器和 rng 状态。请注意，当此选项为 true 时，您将无法从检查点恢复训练。这样可以通过不存储优化器、调度器和 rng 状态来节省存储空间。您只能使用 `from_pretrained` 加载模型，并将此选项设置为 `True`。

+   `use_cpu` (`bool`, *optional*, defaults to `False`) — 是否使用 cpu。如果设置为 False，将使用 cuda 或 mps 设备（如果可用）。

+   `seed` (`int`, *optional*, defaults to 42) — 在训练开始时设置的随机种子。为了确保多次运行的可重现性，请使用 `~Trainer.model_init` 函数来实例化模型，如果模型具有一些随机初始化的参数。

+   `data_seed` (`int`, *optional*) — 用于数据采样器的随机种子。如果未设置，用于数据采样的随机生成器将使用与 `seed` 相同的种子。这可用于确保数据采样的可重现性，独立于模型种子。

+   `jit_mode_eval` (`bool`, *optional*, defaults to `False`) — 是否使用 PyTorch jit trace 进行推断。

+   `use_ipex` (`bool`, *optional*, defaults to `False`) — 在可用时使用 PyTorch 的 Intel 扩展。[IPEX 安装](https://github.com/intel/intel-extension-for-pytorch)。

+   `bf16` (`bool`, *optional*, defaults to `False`) — 是否使用bf16 16位（混合）精度训练，而不是32位训练。需要安培或更高的NVIDIA架构，或者使用CPU（use_cpu）或Ascend NPU。这是一个实验性API，可能会更改。

+   `fp16` (`bool`, *optional*, defaults to `False`) — 是否使用fp16 16位（混合）精度训练，而不是32位训练。

+   `fp16_opt_level` (`str`, *optional*, defaults to ‘O1’) — 对于`fp16`训练，选择在[‘O0’, ‘O1’, ‘O2’, 和 ‘O3’]中的Apex AMP优化级别。有关详细信息，请参阅[Apex文档](https://nvidia.github.io/apex/amp)。

+   `fp16_backend` (`str`, *optional*, defaults to `"auto"`) — 此参数已弃用。请改用`half_precision_backend`。

+   `half_precision_backend` (`str`, *optional*, defaults to `"auto"`) — 用于混合精度训练的后端。必须是`"auto", "apex", "cpu_amp"`之一。`"auto"`将根据检测到的PyTorch版本使用CPU/CUDA AMP或APEX，而其他选择将强制使用请求的后端。

+   `bf16_full_eval` (`bool`, *optional*, defaults to `False`) — 是否使用完整的bfloat16评估，而不是32位。这将更快，节省内存，但可能会损害指标值。这是一个实验性API，可能会更改。

+   `fp16_full_eval` (`bool`, *optional*, defaults to `False`) — 是否使用完整的float16评估，而不是32位。这将更快，节省内存，但可能会损害指标值。

+   `tf32` (`bool`, *optional*) — 是否启用TF32模式，适用于安培和更新的GPU架构。默认值取决于PyTorch版本的`torch.backends.cuda.matmul.allow_tf32`默认值。有关更多详细信息，请参阅[TF32](https://huggingface.co/docs/transformers/performance#tf32)文档。这是一个实验性API，可能会更改。

+   `local_rank` (`int`, *optional*, defaults to -1`) — 分布式训练过程中进程的排名。

+   `ddp_backend` (`str`, *optional*) — 用于分布式训练的后端。必须是`"nccl"`, `"mpi"`, `"ccl"`, `"gloo"`, `"hccl"`之一。

+   `tpu_num_cores` (`int`, *optional*) — 在TPU上训练时，TPU核心的数量（由启动脚本自动传递）。

+   `dataloader_drop_last` (`bool`, *optional*, defaults to `False`) — 是否丢弃最后一个不完整的批次（如果数据集的长度不是批次大小的整数倍）。

+   `eval_steps` (`int` or `float`, *optional*) — 如果`evaluation_strategy="steps"`，则两次评估之间的更新步数。如果未设置，将默认为与`logging_steps`相同的值。应为范围为`[0,1)`的整数或浮点数。如果小于1，则将解释为总训练步数的比率。

+   `dataloader_num_workers` (`int`, *optional*, defaults to 0) — 用于数据加载的子进程数（仅适用于PyTorch）。0表示数据将在主进程中加载。

+   `past_index` (`int`, *optional*, defaults to -1`) — 一些模型（如[TransformerXL](../model_doc/transformerxl)或[XLNet](../model_doc/xlnet)）可以利用过去的隐藏状态进行预测。如果将此参数设置为正整数，则`Trainer`将使用相应的输出（通常为索引2）作为过去状态，并在下一个训练步骤中将其作为关键字参数`mems`提供给模型。

+   `run_name` (`str`, *optional*) — 运行的描述符。通常用于[wandb](https://www.wandb.com/)和[mlflow](https://www.mlflow.org/)日志记录。

+   `disable_tqdm` (`bool`, *optional*) — 是否禁用Jupyter笔记本中由`~notebook.NotebookTrainingTracker`生成的tqdm进度条和指标表。如果日志级别设置为warn或更低（默认值），则默认为`True`，否则为`False`。

+   `remove_unused_columns` (`bool`, *optional*, defaults to `True`) — 是否自动删除模型前向方法未使用的列。

+   `label_names` (`List[str]`, *可选*) — 您的输入字典中与标签对应的键列表。

    最终将默认为模型接受的参数名称列表，其中包含单词“label”，除非使用的模型是 `XxxForQuestionAnswering` 之一，那么还将包括 `["start_positions", "end_positions"]` 键。

+   `load_best_model_at_end` (`bool`, *可选*, 默认为 `False`) — 是否在训练结束时加载找到的最佳模型。启用此选项时，将始终保存最佳检查点。有关更多信息，请参阅 [`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)。

    当设置为 `True` 时，参数 `save_strategy` 需要与 `evaluation_strategy` 相同，并且在其为 “steps” 的情况下，`save_steps` 必须是 `eval_steps` 的整数倍。

+   `metric_for_best_model` (`str`, *可选*) — 与 `load_best_model_at_end` 结合使用，指定用于比较两个不同模型的指标。必须是评估返回的指标的名称，带有或不带有前缀 `"eval_"`。如果未指定且 `load_best_model_at_end=True`，将默认为 `"loss"`（使用评估损失）。

    如果设置了此值，`greater_is_better` 将默认为 `True`。不要忘记，如果您的指标在较低时更好，则将其设置为 `False`。

+   `greater_is_better` (`bool`, *可选*) — 与 `load_best_model_at_end` 和 `metric_for_best_model` 结合使用，指定更好的模型是否应具有更大的指标。默认为：

    +   如果 `metric_for_best_model` 设置为不是 `"loss"` 或 `"eval_loss"` 的值，则为 `True`。

    +   如果未设置 `metric_for_best_model`，或设置为 `"loss"` 或 `"eval_loss"`，则为 `False`。

+   `ignore_data_skip` (`bool`, *可选*, 默认为 `False`) — 在恢复训练时，是否跳过批次以使数据加载与先前训练中的阶段相同。如果设置为 `True`，训练将更快开始（因为跳过步骤可能需要很长时间），但不会产生与中断训练相同的结果。

+   `fsdp` (`bool`, `str` 或 `FSDPOption` 列表, *可选*, 默认为 `''`) — 使用 PyTorch 分布式并行训练（仅在分布式训练中）。

    以下是一系列选项：

    +   `"full_shard"`: 分片参数、梯度和优化器状态。

    +   `"shard_grad_op"`: 分片优化器状态和梯度。

    +   `"hybrid_shard"`: 在节点内应用 `FULL_SHARD`，并在节点之间复制参数。

    +   `"hybrid_shard_zero2"`: 在节点内应用 `SHARD_GRAD_OP`，并在节点之间复制参数。

    +   `"offload"`: 将参数和梯度卸载到 CPU（仅与 `"full_shard"` 和 `"shard_grad_op"` 兼容）。

    +   `"auto_wrap"`: 使用 `default_auto_wrap_policy` 自动递归包装层与 FSDP。

+   `fsdp_config` (`str` 或 `dict`, *可选*) — 用于 fsdp（Pytorch 分布式并行训练）的配置。该值可以是 fsdp json 配置文件的位置（例如，`fsdp_config.json`）或已加载的 json 文件作为 `dict`。

    配置及其选项列表：

    +   min_num_params (`int`, *可选*, 默认为 `0`): FSDP 默认自动包装的参数最小数量。 （仅在传递 `fsdp` 字段时有用）。

    +   transformer_layer_cls_to_wrap (`List[str]`, *可选*): 要包装的 transformer 层类名称列表（区分大小写），例如，`BertLayer`、`GPTJBlock`、`T5Block` …（仅在传递 `fsdp` 标志时有用）。

    +   backward_prefetch (`str`, *可选*) FSDP 的后向预取模式。控制何时预取下一组参数（仅在传递 `fsdp` 字段时有用）。

        以下是一系列选项：

        +   `"backward_pre"` : 在当前参数的梯度计算之前，预取下一组参数。

        +   `"backward_post"` : 在当前参数的梯度计算之后，预取下一组参数。

    +   forward_prefetch（`bool`，*可选*，默认为`False`）FSDP的前向预取模式（仅在传递`fsdp`字段时有用）。如果为`"True"`，则FSDP在执行前向传递时明确预取下一个即将到来的全聚集。

    +   limit_all_gathers（`bool`，*可选*，默认为`False`）FSDP的limit_all_gathers（仅在传递`fsdp`字段时有用）。如果为`"True"`，FSDP明确同步CPU线程，以防止太多的in-flight all-gathers。

    +   use_orig_params（`bool`，*可选*，默认为`True`）如果为`"True"`，允许在初始化期间使用非均匀的`requires_grad`，这意味着支持交替冻结和可训练的参数。在参数高效微调等情况下很有用。请参考这个[博客]([https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019)

    +   sync_module_states（`bool`，*可选*，默认为`True`）如果为`"True"`，每个单独包装的FSDP单元将从rank 0广播模块参数，以确保它们在初始化后在所有rank中是相同的

    +   activation_checkpointing（`bool`，*可选*，默认为`False`）：如果为`"True"`，激活检查点是一种通过清除某些层的激活并在向后传递期间重新计算它们来减少内存使用的技术。实际上，这是以额外的计算时间换取减少内存使用。

    +   xla（`bool`，*可选*，默认为`False`）：是否使用PyTorch/XLA完全分片数据并行训练。这是一个实验性功能，其API可能会在未来发生变化。

    +   xla_fsdp_settings（`dict`，*可选*）该值是一个存储XLA FSDP包装参数的字典。

        有关完整的选项列表，请参见[这里](https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py)。

    +   xla_fsdp_grad_ckpt（`bool`，*可选*，默认为`False`）：将在每个嵌套的XLA FSDP包装层上使用梯度检查点。此设置仅在将xla标志设置为true并通过fsdp_min_num_params或fsdp_transformer_layer_cls_to_wrap指定自动包装策略时才能使用。

+   `deepspeed`（`str`或`dict`，*可选*）— 使用[Deepspeed](https://github.com/microsoft/deepspeed)。这是一个实验性功能，其API可能会在未来发生变化。该值可以是DeepSpeed json配置文件的位置（例如，`ds_config.json`）或已加载的json文件作为`dict`”

+   `label_smoothing_factor`（`float`，*可选*，默认为0.0）— 要使用的标签平滑因子。零表示不进行标签平滑，否则底层的onehot编码标签将从0和1更改为`label_smoothing_factor/num_labels`和`1 - label_smoothing_factor + label_smoothing_factor/num_labels`。

+   `debug`（`str`或`DebugOption`列表，*可选*，默认为`""`）— 启用一个或多个调试功能。这是一个实验性功能。

    可能的选项包括：

    +   `"underflow_overflow"`：检测模型输入/输出中的溢出并报告导致事件的最后帧

    +   `"tpu_metrics_debug"`：在TPU上打印调试指标

    选项应该用空格分隔。

+   `optim`（`str`或`training_args.OptimizerNames`，*可选*，默认为`"adamw_torch"`）— 要使用的优化器：adamw_hf、adamw_torch、adamw_torch_fused、adamw_apex_fused、adamw_anyprecision或adafactor。

+   `optim_args`（`str`，*可选*）— 供AnyPrecisionAdamW提供的可选参数。

+   `group_by_length`（`bool`，*可选*，默认为`False`）— 是否在训练数据集中将大致相同长度的样本分组在一起（以最小化应用的填充并提高效率）。仅在应用动态填充时有用。

+   `length_column_name` (`str`, *optional*, defaults to `"length"`) — 预先计算长度的列名。如果该列存在，按长度分组将使用这些值而不是在训练启动时计算它们。仅在 `group_by_length` 为 `True` 且数据集是 `Dataset` 的实例时才会被忽略。

+   `report_to` (`str` or `List[str]`, *optional*, defaults to `"all"`) — 报告结果和日志的集成列表。支持的平台有 `"azure_ml"`、`"clearml"`、`"codecarbon"`、`"comet_ml"`、`"dagshub"`、`"dvclive"`、`"flyte"`、`"mlflow"`、`"neptune"`、`"tensorboard"` 和 `"wandb"`。使用 `"all"` 报告到所有已安装的集成，使用 `"none"` 不报告到任何集成。

+   `ddp_find_unused_parameters` (`bool`, *optional*) — 在使用分布式训练时，传递给 `DistributedDataParallel` 的 `find_unused_parameters` 标志的值。如果使用了梯度检查点，则默认为 `False`，否则为 `True`。

+   `ddp_bucket_cap_mb` (`int`, *optional*) — 在使用分布式训练时，传递给 `DistributedDataParallel` 的 `bucket_cap_mb` 标志的值。

+   `ddp_broadcast_buffers` (`bool`, *optional*) — 在使用分布式训练时，传递给 `DistributedDataParallel` 的 `broadcast_buffers` 标志的值。如果使用了梯度检查点，则默认为 `False`，否则为 `True`。

+   `dataloader_pin_memory` (`bool`, *optional*, defaults to `True`) — 是否要在数据加载器中固定内存。默认为 `True`。

+   `dataloader_persistent_workers` (`bool`, *optional*, defaults to `False`) — 如果为 True，则数据加载器在数据集被消耗一次后不会关闭工作进程。这允许保持工作进程的数据集实例处于活动状态。可能会加快训练速度，但会增加内存使用量。默认为 `False`。

+   `skip_memory_metrics` (`bool`, *optional*, defaults to `True`) — 是否跳过将内存分析器报告添加到指标中。默认情况下会跳过这一步，因为它会减慢训练和评估速度。

+   `push_to_hub` (`bool`, *optional*, defaults to `False`) — 是否在每次保存模型时将模型推送到 Hub。如果激活了此选项，`output_dir` 将开始一个与仓库同步的 git 目录（由 `hub_model_id` 确定），并且每次触发保存时都会推送内容（取决于您的 `save_strategy`）。调用 [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model) 也会触发推送。

    如果 `output_dir` 存在，则它需要是将 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 将要推送到的仓库的本地克隆。

+   `resume_from_checkpoint` (`str`, *optional*) — 您的模型的有效检查点所在文件夹的路径。这个参数不会直接被 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 使用，而是打算由您的训练/评估脚本使用。查看 [示例脚本](https://github.com/huggingface/transformers/tree/main/examples) 以获取更多详细信息。

+   `hub_model_id` (`str`, *optional*) — 要与本地 *output_dir* 保持同步的仓库名称。它可以是一个简单的模型 ID，此时模型将被推送到您的命名空间。否则，它应该是整个仓库名称，例如 `"user_name/model"`，这样您就可以推送到您所属的组织，如 `"organization_name/model"`。默认为 `user_name/output_dir_name`，其中 *output_dir_name* 是 `output_dir` 的名称。

    默认为 `output_dir` 的名称。

+   `hub_strategy` (`str` or `HubStrategy`, *optional*, defaults to `"every_save"`) — 定义推送到 Hub 的范围和时间。可能的值有：

    +   `"end"`: 当调用 [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model) 方法时，会推送模型、其配置、分词器（如果传递给 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)）以及模型卡的草稿。

    +   `"every_save"`: 每次保存模型时，都会推送模型、其配置、分词器（如果传递给 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)）以及模型卡的草稿。推送是异步的，以避免阻塞训练，如果保存非常频繁，则只有在上一个推送完成后才会尝试新的推送。在训练结束时，会使用最终模型进行最后一次推送。

    +   `"checkpoint"`: 类似于 `"every_save"`，但最新的检查点也会被推送到名为 last-checkpoint 的子文件夹中，使您可以通过 `trainer.train(resume_from_checkpoint="last-checkpoint")` 轻松恢复训练。

    +   `"all_checkpoints"`: 类似于 `"checkpoint"`，但所有检查点都像它们出现在输出文件夹中一样被推送（因此您将在最终存储库中获得一个检查点文件夹）。

+   `hub_token` (`str`, *optional*) — 用于将模型推送到 Hub 的令牌。将默认使用通过 `huggingface-cli login` 获取的缓存文件夹中的令牌。

+   `hub_private_repo` (`bool`, *optional*, 默认为 `False`) — 如果为 True，则 Hub 存储库将设置为私有。

+   `hub_always_push` (`bool`, *optional*, 默认为 `False`) — 除非为 `True`，否则当上一个推送未完成时，`Trainer` 将跳过推送检查点。

+   `gradient_checkpointing` (`bool`, *optional*, 默认为 `False`) — 如果为 True，则使用梯度检查点来节省内存，但会导致反向传播速度变慢。

+   `gradient_checkpointing_kwargs` (`dict`, *optional*, 默认为 `None`) — 要传递给 `gradient_checkpointing_enable` 方法的关键字参数。

+   `include_inputs_for_metrics` (`bool`, *optional*, 默认为 `False`) — 是否将输入传递给 `compute_metrics` 函数。这适用于需要输入、预测和参考值进行评分计算的指标类。

+   `auto_find_batch_size` (`bool`, *optional*, 默认为 `False`) — 是否通过指数衰减自动找到适合内存的批处理大小，避免 CUDA 内存不足错误。需要安装 accelerate (`pip install accelerate`)。

+   `full_determinism` (`bool`, *optional*, 默认为 `False`) — 如果为 `True`，将调用 [enable_full_determinism()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.enable_full_determinism) 而不是 [set_seed()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.set_seed) 以确保在分布式训练中获得可重现的结果。重要提示：这将对性能产生负面影响，因此仅用于调试目的。

+   `torchdynamo` (`str`, *optional*) — 如果设置，TorchDynamo 的后端编译器。可能的选择包括 `"eager"`, `"aot_eager"`, `"inductor"`, `"nvfuser"`, `"aot_nvfuser"`, `"aot_cudagraphs"`, `"ofi"`, `"fx2trt"`, `"onnxrt"` 和 `"ipex"`。

+   `ray_scope` (`str`, *optional*, 默认为 `"last"`) — 在使用 Ray 进行超参数搜索时要使用的范围。默认情况下，将使用 `"last"`。然后，Ray 将使用所有试验的最后一个检查点，比较它们，并选择最佳的一个。但是，也有其他选项可用。有关更多选项，请参阅 [Ray 文档](https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial)。

+   `ddp_timeout` (`int`, *可选*, 默认为1800) — `torch.distributed.init_process_group`调用的超时时间，用于避免在分布式运行中执行缓慢操作时出现GPU套接字超时。请参考[PyTorch文档] ([https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)) 获取更多信息。

+   `use_mps_device` (`bool`, *可选*, 默认为 `False`) — 此参数已弃用。如果可用，将使用`mps`设备，类似于`cuda`设备。

+   `torch_compile` (`bool`, *可选*, 默认为 `False`) — 是否使用 PyTorch 2.0 [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/) 编译模型。

    这将使用[`torch.compile` API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile)的最佳默认值。您可以使用参数`torch_compile_backend`和`torch_compile_mode`自定义默认值，但我们不能保证它们中的任何一个会起作用，因为支持逐渐在PyTorch中推出。

    这个标志和整个编译API是实验性的，并可能在未来版本中发生变化。

+   `torch_compile_backend` (`str`, *可选*) — 在`torch.compile`中使用的后端。如果设置为任何值，`torch_compile`将设置为`True`。

    请参考PyTorch文档以获取可能的值，并注意它们可能会随着PyTorch版本的变化而改变。

    这个标志是实验性的，并可能在未来版本中发生变化。

+   `torch_compile_mode` (`str`, *可选*) — 在`torch.compile`中使用的模式。如果设置为任何值，`torch_compile`将设置为`True`。

    请参考PyTorch文档以获取可能的值，并注意它们可能会随着PyTorch版本的变化而改变。

    这个标志是实验性的，并可能在未来版本中发生变化。

+   `split_batches` (`bool`, *可选*) — 是否加速器在分布式训练期间应该将数据加载器产生的批次分配到设备上。如果

    设置为`True`，实际使用的批量大小将在任何类型的分布式进程上相同，但必须是

    将使用您正在使用的进程数量的倍数（例如GPU）进行四舍五入。

+   `include_tokens_per_second` (`bool`, *可选*) — 是否计算每个设备每秒的标记数以获取训练速度指标。

    这将在事先迭代整个训练数据加载器一次，

    并且会减慢整个过程。

+   `include_num_input_tokens_seen` (`bool`, *可选*) — 是否跟踪整个训练过程中看到的输入标记数。

    在分布式训练中可能会较慢，因为必须调用gather操作。

+   `neftune_noise_alpha` (`Optional[float]`) — 如果不是`None`，这将激活NEFTune噪声嵌入。这可以极大地提高指令微调的模型性能。查看[原始论文](https://arxiv.org/abs/2310.05914)和[原始代码](https://github.com/neelsjain/NEFTune)。支持transformers `PreTrainedModel`和`PeftModel`。

TrainingArguments是我们在示例脚本中使用的与训练循环本身相关的参数的子集。

使用[HfArgumentParser](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.HfArgumentParser)，我们可以将这个类转换为可以在命令行上指定的[argparse](https://docs.python.org/3/library/argparse#module-argparse)参数。

#### `get_process_log_level`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2028)

```py
( )
```

根据这个进程是节点0的主进程、非0节点的主进程还是非主进程，返回要使用的日志级别。

对于主进程，日志级别默认为设置的日志级别（如果您没有做任何操作，则为`logging.WARNING`），除非被`log_level`参数覆盖。

对于副本进程，默认的日志级别为`logging.WARNING`，除非被`log_level_replica`参数覆盖。

根据`should_log`的返回值来选择主进程和副本进程的设置。

#### `get_warmup_steps`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2117)

```py
( num_training_steps: int )
```

获取用于线性预热的步数。

#### `main_process_first`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2066)

```py
( local = True desc = 'work' )
```

参数

+   `local` (`bool`, *optional*, defaults to `True`) — 如果为`True`，则首先处理每个节点的排名为0的进程，如果为`False`，则首先处理排名为0的节点0的进程。在具有共享文件系统的多节点环境中，您很可能会想要使用`local=False`，以便只有第一个节点的主进程会进行处理。但是，如果文件系统不共享，则每个节点的主进程将需要进行处理，这是默认行为。

+   `desc` (`str`, *optional*, defaults to `"work"`) — 用于调试日志中的工作描述

用于torch分布式环境的上下文管理器，在主进程上需要执行某些操作，同时阻塞副本，完成后释放副本。

其中一种用法是用于`datasets`的`map`功能，为了有效率，应该在主进程上运行一次，完成后保存结果的缓存版本，然后副本会自动加载。

#### `set_dataloader`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2629)

```py
( train_batch_size: int = 8 eval_batch_size: int = 8 drop_last: bool = False num_workers: int = 0 pin_memory: bool = True persistent_workers: bool = False auto_find_batch_size: bool = False ignore_data_skip: bool = False sampler_seed: Optional = None )
```

参数

+   `drop_last` (`bool`, *optional*, defaults to `False`) — 是否丢弃最后一个不完整的批次（如果数据集的长度不可被批次大小整除）。

+   `num_workers` (`int`, *optional*, defaults to 0) — 用于数据加载的子进程数量（仅适用于PyTorch）。0表示数据将在主进程中加载。

+   `pin_memory` (`bool`, *optional*, defaults to `True`) — 是否要在数据加载器中固定内存。默认为`True`。

+   `persistent_workers` (`bool`, *optional*, defaults to `False`) — 如果为True，则数据加载器在数据集被消耗一次后不会关闭工作进程。这允许保持工作进程的数据集实例保持活动状态。可能会加快训练速度，但会增加内存使用量。默认为`False`。

+   `auto_find_batch_size` (`bool`, *optional*, defaults to `False`) — 是否通过指数衰减自动找到适合内存的批次大小，避免CUDA内存不足错误。需要安装accelerate（`pip install accelerate`）

+   `ignore_data_skip` (`bool`, *optional*, defaults to `False`) — 在恢复训练时，是否跳过批次和轮次以使数据加载处于与先前训练相同阶段。如果设置为`True`，训练将更快开始（因为跳过步骤可能需要很长时间），但不会产生与中断训练相同的结果。

+   `sampler_seed` (`int`, *optional*) — 用于数据采样器的随机种子。如果未设置，则数据采样的随机生成器将使用与`self.seed`相同的种子。这可用于确保数据采样的可重复性，独立于模型种子。

将所有与数据加载器创建相关的参数重新组合的方法。

示例：

```py
>>> from transformers import TrainingArguments

>>> args = TrainingArguments("working_dir")
>>> args = args.set_dataloader(train_batch_size=16, eval_batch_size=64)
>>> args.per_device_train_batch_size
16
```

#### `set_evaluate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2238)

```py
( strategy: Union = 'no' steps: int = 500 batch_size: int = 8 accumulation_steps: Optional = None delay: Optional = None loss_only: bool = False jit_mode: bool = False )
```

参数

+   `strategy` (`str`或[IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy), *optional*, defaults to `"no"`) — 训练过程中采用的评估策略。可能的值为：

    +   `"no"`: 训练过程中不进行评估。

    +   `"steps"`: 每`steps`进行评估（并记录日志）。

    +   `"epoch"`: 每个时代结束时进行评估。

    设置与`"no"`不同的`strategy`将`self.do_eval`设置为`True`。

+   `steps` (`int`, *optional*, 默认为500) — 如果`strategy="steps"`，两次评估之间的更新步数。

+   `batch_size` (`int` *optional*, 默认为8) — 用于评估的每个设备（GPU/TPU核心/CPU...）的批量大小。

+   `accumulation_steps` (`int`, *optional*) — 在将结果移动到CPU之前，累积输出张量的预测步数。如果未设置，整个预测将在GPU/TPU上累积后移至CPU（速度更快但需要更多内存）。

+   `delay` (`float`, *optional*) — 等待进行第一次评估的周期数或步数，取决于评估策略。

+   `loss_only` (`bool`, *optional*, 默认为`False`) — 仅忽略损失以外的所有输出。

+   `jit_mode` (`bool`, *optional*) — 是否使用PyTorch jit跟踪进行推断。

将所有与评估相关的参数分组的方法。

示例：

```py
>>> from transformers import TrainingArguments

>>> args = TrainingArguments("working_dir")
>>> args = args.set_evaluate(strategy="steps", steps=100)
>>> args.eval_steps
100
```

#### `set_logging`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2388)

```py
( strategy: Union = 'steps' steps: int = 500 report_to: Union = 'none' level: str = 'passive' first_step: bool = False nan_inf_filter: bool = False on_each_node: bool = False replica_level: str = 'passive' )
```

参数

+   `strategy` (`str` 或 [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy), *optional*, 默认为`"steps"`) — 训练期间采用的日志记录策略。可能的值有：

    +   `"no"`: 训练期间不保存。

    +   `"epoch"`: 在每个周期结束时保存。

    +   `"steps"`: 每`save_steps`保存一次。

+   `steps` (`int`, *optional*, 默认为500) — 如果`strategy="steps"`，两次日志记录之间的更新步数。

+   `level` (`str`, *optional*, 默认为`"passive"`) — 用于主进程的记录器日志级别。可能的选择是字符串形式的日志级别：`"debug"`、`"info"`、`"warning"`、`"error"`和`"critical"`，以及一个不设置任何内容并让应用程序设置级别的`"passive"`级别。

+   `report_to` (`str` 或 `List[str]`, *optional*, 默认为`"all"`) — 报告结果和日志的集成列表。支持的平台有`"azure_ml"`、`"clearml"`、`"codecarbon"`、`"comet_ml"`、`"dagshub"`、`"dvclive"`、`"flyte"`、`"mlflow"`、`"neptune"`、`"tensorboard"`和`"wandb"`。使用`"all"`报告所有已安装的集成，使用`"none"`不报告任何集成。

+   `first_step` (`bool`, *optional*, 默认为`False`) — 是否记录和评估第一个`global_step`。

+   `nan_inf_filter` (`bool`, *optional*, 默认为`True`) — 是否过滤用于日志记录的`nan`和`inf`损失。如果设置为`True`，则过滤每个步骤的`nan`或`inf`损失，并取代当前日志窗口的平均损失。

    `nan_inf_filter`仅影响损失值的日志记录，不会改变计算梯度或将梯度应用于模型的行为。

+   `on_each_node` (`bool`, *optional*, 默认为`True`) — 在多节点分布式训练中，是否每个节点仅使用`log_level`一次进行日志记录，或仅在主节点上进行日志记录。

+   `replica_level` (`str`, *optional*, 默认为`"passive"`) — 用于副本的记录器日志级别。与`log_level`相同的选择。

将所有与日志记录相关的参数分组的方法。

示例:

```py
>>> from transformers import TrainingArguments

>>> args = TrainingArguments("working_dir")
>>> args = args.set_logging(strategy="steps", steps=100)
>>> args.logging_steps
100
```

#### `set_lr_scheduler`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2584)

```py
( name: Union = 'linear' num_epochs: float = 3.0 max_steps: int = -1 warmup_ratio: float = 0 warmup_steps: int = 0 )
```

参数

+   `name` (`str` 或 [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType), *optional*, 默认为`"linear"`) — 要使用的调度程序类型。查看[SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType)的文档以获取所有可能的值。

+   `num_epochs(float,` *optional*, 默认为3.0) — 要执行的总训练周期数（如果不是整数，则在停止训练之前执行最后一个周期的小数部分百分比）。

+   `max_steps` (`int`, *可选*, 默认为 -1) — 如果设置为正数，则执行的总训练步数。覆盖`num_train_epochs`。对于有限的数据集，如果所有数据都用完，则通过数据集重复训练，直到达到`max_steps`。

+   `warmup_ratio` (`float`, *可选*, 默认为 0.0) — 用于从0到`learning_rate`进行线性预热的总训练步骤的比率。

+   `warmup_steps` (`int`, *可选*, 默认为 0) — 用于从0到`learning_rate`进行线性预热的步骤数。覆盖`warmup_ratio`的任何效果。

一个将所有与学习率调度器及其超参数相关联的参数重新分组的方法。

示例：

```py
>>> from transformers import TrainingArguments

>>> args = TrainingArguments("working_dir")
>>> args = args.set_lr_scheduler(name="cosine", warmup_ratio=0.05)
>>> args.warmup_ratio
0.05
```

#### `set_optimizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2533)

```py
( name: Union = 'adamw_torch' learning_rate: float = 5e-05 weight_decay: float = 0 beta1: float = 0.9 beta2: float = 0.999 epsilon: float = 1e-08 args: Optional = None )
```

参数

+   `name` (`str` 或 `training_args.OptimizerNames`, *可选*, 默认为 `"adamw_torch"`) — 要使用的优化器："adamw_hf"、"adamw_torch"、"adamw_torch_fused"、"adamw_apex_fused"、"adamw_anyprecision"或"adafactor"。

+   `learning_rate` (`float`, *可选*, 默认为 5e-5) — 初始学习率。

+   `weight_decay` (`float`, *可选*, 默认为 0) — 应用的权重衰减（如果不为零）到所有层，除了所有偏置和LayerNorm权重。

+   `beta1` (`float`, *可选*, 默认为 0.9) — Adam优化器或其变种的beta1超参数。

+   `beta2` (`float`, *可选*, 默认为 0.999) — Adam优化器或其变种的beta2超参数。

+   `epsilon` (`float`, *可选*, 默认为 1e-8) — Adam优化器或其变种的epsilon超参数。

+   `args` (`str`, *可选*) — 提供给AnyPrecisionAdamW的可选参数（仅在`optim="adamw_anyprecision"`时有用）。

一个将所有与优化器及其超参数相关联的参数重新分组的方法。

示例：

```py
>>> from transformers import TrainingArguments

>>> args = TrainingArguments("working_dir")
>>> args = args.set_optimizer(name="adamw_torch", beta1=0.8)
>>> args.optim
'adamw_torch'
```

#### `set_push_to_hub`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2463)

```py
( model_id: str strategy: Union = 'every_save' token: Optional = None private_repo: bool = False always_push: bool = False )
```

参数

+   `model_id` (`str`) — 与本地*output_dir*同步的存储库的名称。它可以是一个简单的模型ID，此时模型将被推送到您的命名空间。否则，它应该是整个存储库名称，例如`"user_name/model"`，这样您就可以将其推送到您是成员的组织中，例如`"organization_name/model"`。

+   `strategy` (`str` 或 `HubStrategy`, *可选*, 默认为 `"every_save"`) — 定义推送到Hub的范围和时间。可能的值为：

    +   `"end"`: 当调用[save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)方法时，推送模型、其配置、分词器（如果传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)）以及模型卡的草稿。

    +   `"every_save"`: 每次保存模型时，推送模型、其配置、分词器（如果传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)）以及模型卡的草稿。推送是异步的，以避免阻塞训练，如果保存非常频繁，则只有在上一个推送完成后才会尝试新的推送。在训练结束时，使用最终模型进行最后一次推送。

    +   `"checkpoint"`: 类似于`"every_save"`，但最新的检查点也被推送到名为last-checkpoint的子文件夹中，这样您可以轻松地使用`trainer.train(resume_from_checkpoint="last-checkpoint")`恢复训练。

    +   `"all_checkpoints"`: 类似于`"checkpoint"`，但所有检查点都像它们出现在输出文件夹中一样被推送（因此您将在最终存储库中的每个文件夹中获得一个检查点文件夹）。

+   `token` (`str`, *可选*) — 用于将模型推送到Hub的令牌。将默认使用通过`huggingface-cli login`获得的缓存文件夹中的令牌。

+   `private_repo` (`bool`, *可选*, 默认为 `False`) — 如果为True，则Hub存储库将设置为私有。

+   always_push（`bool`，*可选*，默认为`False`）— 除非为`True`，否则当上一次推送未完成时，`Trainer`将跳过推送检查点。

将所有与与Hub同步检查点相关的参数进行分组的方法。

调用此方法将设置`self.push_to_hub`为`True`，这意味着`output_dir`将开始一个与存储库同步的git目录（由`model_id`确定），并且每次触发保存时将推送内容（取决于`self.save_strategy`）。调用[save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model)也将触发推送。

示例：

```py
>>> from transformers import TrainingArguments

>>> args = TrainingArguments("working_dir")
>>> args = args.set_push_to_hub("me/awesome-model")
>>> args.hub_model_id
'me/awesome-model'
```

#### `set_save`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2339)

```py
( strategy: Union = 'steps' steps: int = 500 total_limit: Optional = None on_each_node: bool = False )
```

参数

+   strategy（`str`或[IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy)，*可选*，默认为`"steps"`）— 训练期间采用的检查点保存策略。可能的值为：

    +   `"no"`：在训练期间不进行保存。

    +   `"epoch"`：在每个时代结束时保存。

    +   `"steps"`：每`save_steps`保存一次。

+   `steps`（`int`，*可选*，默认为500）— 如果`strategy="steps"`，则在两个检查点保存之前的更新步骤数。

+   total_limit（`int`，*可选*）— 如果传递了一个值，将限制检查点的总量。删除`output_dir`中的旧检查点。

+   on_each_node（`bool`，*可选*，默认为`False`）— 在进行多节点分布式训练时，是否在每个节点上保存模型和检查点，还是仅在主节点上保存。

    当不同节点使用相同存储时，不应激活此选项，因为文件将以每个节点相同的名称保存。

将所有与检查点保存相关的参数进行分组的方法。

示例：

```py
>>> from transformers import TrainingArguments

>>> args = TrainingArguments("working_dir")
>>> args = args.set_save(strategy="steps", steps=100)
>>> args.save_steps
100
```

#### `set_testing`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2299)

```py
( batch_size: int = 8 loss_only: bool = False jit_mode: bool = False )
```

参数

+   batch_size（`int` *可选*，默认为8）— 用于测试的每个设备（GPU/TPU核心/CPU…）的批量大小。

+   loss_only（`bool`，*可选*，默认为`False`）— 除了损失之外，忽略所有输出。

+   jit_mode（`bool`，*可选*）— 是否使用PyTorch jit跟踪进行推断。

将所有与在保留数据集上进行测试相关的基本参数进行分组的方法。

调用此方法将自动将`self.do_predict`设置为`True`。

示例：

```py
>>> from transformers import TrainingArguments

>>> args = TrainingArguments("working_dir")
>>> args = args.set_testing(batch_size=32)
>>> args.per_device_eval_batch_size
32
```

#### `set_training`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2163)

```py
( learning_rate: float = 5e-05 batch_size: int = 8 weight_decay: float = 0 num_epochs: float = 3 max_steps: int = -1 gradient_accumulation_steps: int = 1 seed: int = 42 gradient_checkpointing: bool = False )
```

参数

+   learning_rate（`float`，*可选*，默认为5e-5）— 优化器的初始学习率。

+   batch_size（`int` *可选*，默认为8）— 用于训练的每个设备（GPU/TPU核心/CPU…）的批量大小。

+   weight_decay（`float`，*可选*，默认为0）— 应用的权重衰减（如果不为零）到优化器中除所有偏置和LayerNorm权重之外的所有层。

+   num_train_epochs（float，*可选*，默认为3.0）— 要执行的总训练时期数（如果不是整数，则在停止训练之前执行最后一个时期的小数部分百分比）。

+   max_steps（`int`，*可选*，默认为-1）— 如果设置为正数，则执行的总训练步数。覆盖`num_train_epochs`。对于有限的数据集，如果所有数据都用完，则通过数据集重复训练，直到达到`max_steps`。

+   gradient_accumulation_steps（`int`，*可选*，默认为1）— 在执行向后/更新传递之前，累积梯度的更新步骤数。

    在使用梯度累积时，一个步骤被计为一个带有向后传递的步骤。因此，每`gradient_accumulation_steps * xxx_step`个训练示例将进行日志记录、评估和保存。

+   `seed` (`int`, *optional*, 默认为 42) — 将在训练开始时设置的随机种子。为了确保在不同运行之间的可重现性，请使用 `~Trainer.model_init` 函数来实例化模型，如果模型有一些随机初始化的参数。

+   `gradient_checkpointing` (`bool`, *optional*, 默认为 `False`) — 如果为 True，则使用梯度检查点来节省内存，但会降低向后传递的速度。

将所有与训练相关的基本参数重新组合的方法。

调用此方法将自动将 `self.do_train` 设置为 `True`。

示例：

```py
>>> from transformers import TrainingArguments

>>> args = TrainingArguments("working_dir")
>>> args = args.set_training(learning_rate=1e-4, batch_size=32)
>>> args.learning_rate
1e-4
```

#### `to_dict`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2126)

```py
( )
```

将此实例序列化，同时用它们的值替换 `Enum`（用于 JSON 序列化支持）。通过删除它们的值来混淆令牌值。

#### `to_json_string`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2143)

```py
( )
```

将此实例序列化为 JSON 字符串。

#### `to_sanitized_dict`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L2149)

```py
( )
```

经过清理的序列化，可与 TensorBoard 的 hparams 一起使用

## Seq2SeqTrainingArguments

### `class transformers.Seq2SeqTrainingArguments`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args_seq2seq.py#L28)

```py
( output_dir: str overwrite_output_dir: bool = False do_train: bool = False do_eval: bool = False do_predict: bool = False evaluation_strategy: Union = 'no' prediction_loss_only: bool = False per_device_train_batch_size: int = 8 per_device_eval_batch_size: int = 8 per_gpu_train_batch_size: Optional = None per_gpu_eval_batch_size: Optional = None gradient_accumulation_steps: int = 1 eval_accumulation_steps: Optional = None eval_delay: Optional = 0 learning_rate: float = 5e-05 weight_decay: float = 0.0 adam_beta1: float = 0.9 adam_beta2: float = 0.999 adam_epsilon: float = 1e-08 max_grad_norm: float = 1.0 num_train_epochs: float = 3.0 max_steps: int = -1 lr_scheduler_type: Union = 'linear' lr_scheduler_kwargs: Optional = <factory> warmup_ratio: float = 0.0 warmup_steps: int = 0 log_level: Optional = 'passive' log_level_replica: Optional = 'warning' log_on_each_node: bool = True logging_dir: Optional = None logging_strategy: Union = 'steps' logging_first_step: bool = False logging_steps: float = 500 logging_nan_inf_filter: bool = True save_strategy: Union = 'steps' save_steps: float = 500 save_total_limit: Optional = None save_safetensors: Optional = True save_on_each_node: bool = False save_only_model: bool = False no_cuda: bool = False use_cpu: bool = False use_mps_device: bool = False seed: int = 42 data_seed: Optional = None jit_mode_eval: bool = False use_ipex: bool = False bf16: bool = False fp16: bool = False fp16_opt_level: str = 'O1' half_precision_backend: str = 'auto' bf16_full_eval: bool = False fp16_full_eval: bool = False tf32: Optional = None local_rank: int = -1 ddp_backend: Optional = None tpu_num_cores: Optional = None tpu_metrics_debug: bool = False debug: Union = '' dataloader_drop_last: bool = False eval_steps: Optional = None dataloader_num_workers: int = 0 past_index: int = -1 run_name: Optional = None disable_tqdm: Optional = None remove_unused_columns: Optional = True label_names: Optional = None load_best_model_at_end: Optional = False metric_for_best_model: Optional = None greater_is_better: Optional = None ignore_data_skip: bool = False fsdp: Union = '' fsdp_min_num_params: int = 0 fsdp_config: Optional = None fsdp_transformer_layer_cls_to_wrap: Optional = None deepspeed: Optional = None label_smoothing_factor: float = 0.0 optim: Union = 'adamw_torch' optim_args: Optional = None adafactor: bool = False group_by_length: bool = False length_column_name: Optional = 'length' report_to: Optional = None ddp_find_unused_parameters: Optional = None ddp_bucket_cap_mb: Optional = None ddp_broadcast_buffers: Optional = None dataloader_pin_memory: bool = True dataloader_persistent_workers: bool = False skip_memory_metrics: bool = True use_legacy_prediction_loop: bool = False push_to_hub: bool = False resume_from_checkpoint: Optional = None hub_model_id: Optional = None hub_strategy: Union = 'every_save' hub_token: Optional = None hub_private_repo: bool = False hub_always_push: bool = False gradient_checkpointing: bool = False gradient_checkpointing_kwargs: Optional = None include_inputs_for_metrics: bool = False fp16_backend: str = 'auto' push_to_hub_model_id: Optional = None push_to_hub_organization: Optional = None push_to_hub_token: Optional = None mp_parameters: str = '' auto_find_batch_size: bool = False full_determinism: bool = False torchdynamo: Optional = None ray_scope: Optional = 'last' ddp_timeout: Optional = 1800 torch_compile: bool = False torch_compile_backend: Optional = None torch_compile_mode: Optional = None dispatch_batches: Optional = None split_batches: Optional = False include_tokens_per_second: Optional = False include_num_input_tokens_seen: Optional = False neftune_noise_alpha: float = None sortish_sampler: bool = False predict_with_generate: bool = False generation_max_length: Optional = None generation_num_beams: Optional = None generation_config: Union = None )
```

参数

+   `output_dir` (`str`) — 模型预测和检查点将被写入的输出目录。

+   `overwrite_output_dir` (`bool`, *optional*, 默认为 `False`) — 如果为 `True`，则覆盖输出目录的内容。如果 `output_dir` 指向一个检查点目录，则使用此选项继续训练。

+   `do_train` (`bool`, *optional*, 默认为 `False`) — 是否运行训练。此参数不会直接被 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 使用，而是打算由您的训练/评估脚本使用。有关更多详细信息，请参阅[示例脚本](https://github.com/huggingface/transformers/tree/main/examples)。

+   `do_eval` (`bool`, *optional*) — 是否在验证集上运行评估。如果 `evaluation_strategy` 与 `"no"` 不同，则将设置为 `True`。此参数不会直接被 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 使用，而是打算由您的训练/评估脚本使用。有关更多详细信息，请参阅[示例脚本](https://github.com/huggingface/transformers/tree/main/examples)。

+   `do_predict` (`bool`, *optional*, 默认为 `False`) — 是否在测试集上运行预测。此参数不会直接被 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 使用，而是打算由您的训练/评估脚本使用。有关更多详细信息，请参阅[示例脚本](https://github.com/huggingface/transformers/tree/main/examples)。

+   `evaluation_strategy` (`str` 或 [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy), *optional*, 默认为 `"no"`) — 训练期间采用的评估策略。可能的值有：

    +   `"no"`: 在训练期间不进行评估。

    +   `"steps"`: 每 `eval_steps` 进行一次评估（并记录）。

    +   `"epoch"`: 在每个时代结束时进行评估。

+   `prediction_loss_only` (`bool`, *optional*, 默认为 `False`) — 在进行评估和生成预测时，仅返回损失。

+   `per_device_train_batch_size` (`int`, *optional*, 默认为 8) — 训练时每个 GPU/XPU/TPU/MPS/NPU 核心/CPU 的批量大小。

+   `per_device_eval_batch_size` (`int`, *optional*, 默认为 8) — 评估时每个 GPU/XPU/TPU/MPS/NPU 核心/CPU 的批量大小。

+   `gradient_accumulation_steps` (`int`, *optional*, 默认为 1) — 在执行向后/更新传递之前，累积梯度的更新步骤数。

    在使用梯度累积时，一个步骤被计为一个带有反向传播的步骤。因此，每 `gradient_accumulation_steps * xxx_step` 训练示例将进行记录、评估、保存。

+   `eval_accumulation_steps` (`int`, *可选*) — 在将结果移动到 CPU 之前，累积输出张量的预测步数。如果未设置，整个预测将在 GPU/NPU/TPU 上累积后再移动到 CPU（更快但需要更多内存）。

+   `eval_delay` (`float`, *可选*) — 在进行第一次评估之前等待的周期数或步数，具体取决于 evaluation_strategy。

+   `learning_rate` (`float`, *可选*, 默认为 5e-5) — [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW) 优化器的初始学习率。

+   `weight_decay` (`float`, *可选*, 默认为 0) — 要应用的权重衰减（如果不为零）到所有层，除了 [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW) 优化器中的所有偏置和 LayerNorm 权重。

+   `adam_beta1` (`float`, *可选*, 默认为 0.9) — [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW) 优化器的 beta1 超参数。

+   `adam_beta2` (`float`, *可选*, 默认为 0.999) — [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW) 优化器的 beta2 超参数。

+   `adam_epsilon` (`float`, *可选*, 默认为 1e-8) — [AdamW](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.AdamW) 优化器的 epsilon 超参数。

+   `max_grad_norm` (`float`, *可选*, 默认为 1.0) — 最大梯度范数（用于梯度裁剪）。

+   `num_train_epochs(float,` *可选*, 默认为 3.0) — 要执行的总训练周期数（如果不是整数，则在停止训练之前执行最后一个周期的小数部分百分比）。

+   `max_steps` (`int`, *可选*, 默认为 -1) — 如果设置为正数，则执行的总训练步数。覆盖 `num_train_epochs`。对于有限的数据集，如果所有数据都用完，则通过数据集重新进行训练，直到达到 `max_steps`。

+   `lr_scheduler_type` (`str` 或 [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType), *可选*, 默认为 `"linear"`) — 要使用的调度器类型。查看 [SchedulerType](/docs/transformers/v4.37.2/en/main_classes/optimizer_schedules#transformers.SchedulerType) 的文档以获取所有可能的值。

+   `lr_scheduler_kwargs`（‘dict’, *可选*, 默认为 {}） — lr_scheduler 的额外参数。查看每个调度器的文档以获取可能的值。

+   `warmup_ratio` (`float`, *可选*, 默认为 0.0) — 用于从 0 到 `learning_rate` 进行线性预热的总训练步数的比率。

+   `warmup_steps` (`int`, *可选*, 默认为 0) — 用于从 0 到 `learning_rate` 进行线性预热的步数。覆盖任何 `warmup_ratio` 的效果。

+   `log_level` (`str`, *可选*, 默认为 `passive`) — 要在主进程上使用的记录器日志级别。可能的选择是字符串形式的日志级别：‘debug’、‘info’、`"warning"`、‘error’ 和 ‘critical’，以及一个`passive`级别，它不设置任何内容并保持 Transformers 库的当前日志级别（默认为 `"warning"`）。

+   `log_level_replica` (`str`, *可选*, 默认为 `"warning"`) — 用于副本的记录器日志级别。与 `log_level` 相同的选择”

+   `log_on_each_node` (`bool`, *可选*, 默认为 `True`) — 在多节点分布式训练中，是否每个节点使用 `log_level` 进行记录，或仅在主节点上进行记录。

+   `logging_dir` (`str`, *可选*) — [TensorBoard](https://www.tensorflow.org/tensorboard) 日志目录。将默认为 *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***。

+   `logging_strategy` (`str` 或 [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy)，*可选*，默认为 `"steps"`) — 训练过程中采用的日志记录策略。可能的取值有：

    +   `"no"`: 训练过程中不进行日志记录。

    +   `"epoch"`: 每个时代结束时进行日志记录。

    +   `"steps"`: 每 `logging_steps` 步进行日志记录。

+   `logging_first_step` (`bool`，*可选*，默认为 `False`) — 是否记录和评估第一个 `global_step`。

+   `logging_steps` (`int` 或 `float`，*可选*，默认为 500) — 如果 `logging_strategy="steps"`，则在两次日志之间的更新步数。应为整数或范围为 `[0,1)` 的浮点数。如果小于 1，则将被解释为总训练步数的比率。

+   `logging_nan_inf_filter` (`bool`，*可选*，默认为 `True`) — 是否过滤用于记录的 `nan` 和 `inf` 损失。如果设置为 `True`，则会过滤每个步骤的损失值为 `nan` 或 `inf`，并取当前日志窗口的平均损失值。 

    `logging_nan_inf_filter` 仅影响损失值的记录，不会改变梯度的计算或应用于模型的行为。

+   `save_strategy` (`str` 或 [IntervalStrategy](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.IntervalStrategy)，*可选*，默认为 `"steps"`) — 训练过程中采用的检查点保存策略。可能的取值有：

    +   `"no"`: 训练过程中不进行保存。

    +   `"epoch"`: 每个时代结束时保存。

    +   `"steps"`: 每 `save_steps` 步保存一次。

+   `save_steps` (`int` 或 `float`，*可选*，默认为 500) — 如果 `save_strategy="steps"`，则在两次检查点保存之间的更新步数。应为整数或范围为 `[0,1)` 的浮点数。如果小于 1，则将被解释为总训练步数的比率。

+   `save_total_limit` (`int`，*可选*) — 如果传递了一个值，将限制检查点的总量。删除 `output_dir` 中的旧检查点。当启用 `load_best_model_at_end` 时，“最佳”检查点始终会保留，而且还会保留最近的检查点。例如，对于 `save_total_limit=5` 和 `load_best_model_at_end`，最后四个检查点将始终与最佳模型一起保留。当 `save_total_limit=1` 和 `load_best_model_at_end` 时，可能保存两个检查点：最后一个和最佳一个（如果它们不同）。

+   `save_safetensors` (`bool`，*可选*，默认为 `True`) — 使用 [safetensors](https://huggingface.co/docs/safetensors) 保存和加载状态字典，而不是默认的 `torch.load` 和 `torch.save`。

+   `save_on_each_node` (`bool`，*可选*，默认为 `False`) — 在进行多节点分布式训练时，是否在每个节点上保存模型和检查点，还是仅在主节点上保存。

    当不同节点使用相同存储时，不应激活此选项，因为文件将以相同名称保存在每个节点上。

+   `save_only_model` (`bool`，*可选*，默认为 `False`) — 在检查点时，是否仅保存模型，还是同时保存优化器、调度器和 RNG 状态。请注意，当此选项为真时，您将无法从检查点恢复训练。这样可以通过不存储优化器、调度器和 RNG 状态来节省存储空间。您只能使用 `from_pretrained` 加载模型，并将此选项设置为 `True`。

+   `use_cpu` (`bool`，*可选*，默认为 `False`) — 是否使用 CPU。如果设置为 False，将使用 cuda 或 mps 设备（如果可用）。

+   `seed` (`int`，*可选*，默认为 42) — 在训练开始时设置的随机种子。为了确保跨运行的可重现性，请使用 `~Trainer.model_init` 函数来实例化模型，如果模型具有一些随机初始化的参数。

+   `data_seed` (`int`, *optional*) — 用于数据采样的随机种子。如果未设置，数据采样的随机生成器将使用与`seed`相同的种子。这可用于确保数据采样的可重现性，与模型种子无关。

+   `jit_mode_eval` (`bool`, *optional*, defaults to `False`) — 是否使用PyTorch jit跟踪进行推断。

+   `use_ipex` (`bool`, *optional*, defaults to `False`) — 在PyTorch可用时使用Intel扩展。[IPEX安装](https://github.com/intel/intel-extension-for-pytorch)。

+   `bf16` (`bool`, *optional*, defaults to `False`) — 是否使用bf16 16位（混合）精度训练，而不是32位训练。需要安普尔或更高的NVIDIA架构或使用CPU（use_cpu）或Ascend NPU。这是一个实验性的API，可能会发生变化。

+   `fp16` (`bool`, *optional*, defaults to `False`) — 是否使用fp16 16位（混合）精度训练，而不是32位训练。

+   `fp16_opt_level` (`str`, *optional*, defaults to ‘O1’) — 对于`fp16`训练，选择在[‘O0’, ‘O1’, ‘O2’, 和 ‘O3’]中的Apex AMP优化级别。有关详细信息，请参阅[Apex文档](https://nvidia.github.io/apex/amp)。

+   `fp16_backend` (`str`, *optional*, defaults to `"auto"`) — 此参数已弃用。请改用`half_precision_backend`。

+   `half_precision_backend` (`str`, *optional*, defaults to `"auto"`) — 用于混合精度训练的后端。必须是`"auto", "apex", "cpu_amp"`之一。`"auto"`将根据检测到的PyTorch版本使用CPU/CUDA AMP或APEX，而其他选择将强制使用请求的后端。

+   `bf16_full_eval` (`bool`, *optional*, defaults to `False`) — 是否使用完整的bfloat16评估，而不是32位。这将更快，节省内存，但可能会损害指标值。这是一个实验性的API，可能会发生变化。

+   `fp16_full_eval` (`bool`, *optional*, defaults to `False`) — 是否使用完整的float16评估，而不是32位。这将更快，节省内存，但可能会损害指标值。

+   `tf32` (`bool`, *optional*) — 是否启用TF32模式，适用于Ampere和更新的GPU架构。默认值取决于PyTorch的版本默认值`torch.backends.cuda.matmul.allow_tf32`。有关更多详细信息，请参阅[TF32](https://huggingface.co/docs/transformers/performance#tf32)文档。这是一个实验性的API，可能会发生变化。

+   `local_rank` (`int`, *optional*, defaults to -1) — 分布式训练期间进程的排名。

+   `ddp_backend` (`str`, *optional*) — 用于分布式训练的后端。必须是`"nccl"`, `"mpi"`, `"ccl"`, `"gloo"`, `"hccl"`之一。

+   `tpu_num_cores` (`int`, *optional*) — 在TPU上训练时，TPU核心的数量（由启动脚本自动传递）。

+   `dataloader_drop_last` (`bool`, *optional*, defaults to `False`) — 是否丢弃最后一个不完整的批次（如果数据集的长度不能被批次大小整除）。

+   `eval_steps` (`int` or `float`, *optional*) — 如果`evaluation_strategy="steps"`，则两次评估之间的更新步数。如果未设置，将默认为与`logging_steps`相同的值。应为范围在`[0,1)`的整数或浮点数。如果小于1，将被解释为总训练步数的比率。

+   `dataloader_num_workers` (`int`, *optional*, defaults to 0) — 用于数据加载的子进程数（仅适用于PyTorch）。0表示数据将在主进程中加载。

+   `past_index` (`int`, *optional*, defaults to -1) — 一些模型（如[TransformerXL](../model_doc/transformerxl)或[XLNet](../model_doc/xlnet)）可以利用过去的隐藏状态进行预测。如果将此参数设置为正整数，则`Trainer`将使用相应的输出（通常为索引2）作为过去状态，并在下一个训练步骤中将其作为关键字参数`mems`提供给模型。

+   `run_name` (`str`, *optional*) — 运行的描述符。通常用于[wandb](https://www.wandb.com/)和[mlflow](https://www.mlflow.org/)日志记录。

+   `disable_tqdm` (`bool`, *optional*) — 是否禁用Jupyter笔记本中`~notebook.NotebookTrainingTracker`生成的tqdm进度条和指标表。如果日志级别设置为warn或更低（默认），则默认为`True`，否则为`False`。

+   `remove_unused_columns` (`bool`, *optional*, defaults to `True`) — 是否自动删除模型前向方法未使用的列。

+   `label_names` (`List[str]`, *optional*) — 您的输入字典中对应于标签的键列表。

    最终将默认为模型接受的参数名称列表，其中包含单词“label”，除非使用的模型是`XxxForQuestionAnswering`之一，在这种情况下还将包括`["start_positions", "end_positions"]`键。

+   `load_best_model_at_end` (`bool`, *optional*, defaults to `False`) — 是否在训练结束时加载训练过程中找到的最佳模型。启用此选项时，最佳检查点将始终被保存。更多信息请参见[`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)。

    设置为`True`时，参数`save_strategy`需要与`evaluation_strategy`相同，如果是“steps”，则`save_steps`必须是`eval_steps`的整数倍。

+   `metric_for_best_model` (`str`, *optional*) — 与`load_best_model_at_end`一起使用，指定用于比较两个不同模型的度量标准。必须是评估返回的度量的名称，带有或不带有前缀`"eval_"`。如果未指定且`load_best_model_at_end=True`（使用评估损失），将默认为`"loss"`。

    如果设置了此值，`greater_is_better`将默认为`True`。如果您的度量标准较低时更好，请不要忘记将其设置为`False`。

+   `greater_is_better` (`bool`, *optional*) — 与`load_best_model_at_end`和`metric_for_best_model`一起使用，指定更好的模型是否应具有更大的度量标准。默认为：

    +   如果`metric_for_best_model`设置为不是`"loss"`或`"eval_loss"`的值，则为`True`。

    +   如果未设置`metric_for_best_model`，或设置为`"loss"`或`"eval_loss"`，则为`False`。

+   `ignore_data_skip` (`bool`, *optional*, defaults to `False`) — 恢复训练时，是否跳过批次和轮次以使数据加载与先前训练的阶段相同。如果设置为`True`，训练将更快开始（因为跳过步骤可能需要很长时间），但不会产生与中断训练相同的结果。

+   `fsdp` (`bool`, `str` or list of `FSDPOption`, *optional*, defaults to `''`) — 使用PyTorch分布式并行训练（仅在分布式训练中）。

    以下选项列表：

    +   `"full_shard"`: 分片参数、梯度和优化器状态。

    +   `"shard_grad_op"`: 分片优化器状态和梯度。

    +   `"hybrid_shard"`: 在节点内应用`FULL_SHARD`，并在节点之间复制参数。

    +   `"hybrid_shard_zero2"`: 在节点内应用`SHARD_GRAD_OP`，并在节点之间复制参数。

    +   `"offload"`: 将参数和梯度卸载到CPU（仅与`"full_shard"`和`"shard_grad_op"`兼容）。

    +   `"auto_wrap"`: 使用`default_auto_wrap_policy`自动递归包装层与FSDP。

+   `fsdp_config` (`str` or `dict`, *optional*) — 用于fsdp（Pytorch分布式并行训练）的配置。该值可以是fsdp json配置文件的位置（例如，`fsdp_config.json`）或已加载的json文件作为`dict`。

    配置及其选项列表：

    +   min_num_params (`int`, *optional*, defaults to `0`): FSDP的默认自动包装的参数最小数量。（仅在传递`fsdp`字段时有用）。

    +   transformer_layer_cls_to_wrap（`List[str]`，*可选*）：要包装的transformer层类名称列表（区分大小写），例如，`BertLayer`，`GPTJBlock`，`T5Block` …（仅在传递`fsdp`标志时有用）。

    +   backward_prefetch（`str`，*可选*）FSDP的后向预取模式。控制何时预取下一组参数（仅在传递`fsdp`字段时有用）。

        以下是一系列选项：

        +   `"backward_pre"`：在当前参数梯度计算之前预取下一组参数。

        +   `"backward_post"`：在当前参数梯度计算之后预取下一组参数。

    +   forward_prefetch（`bool`，*可选*，默认为`False`）FSDP的前向预取模式（仅在传递`fsdp`字段时有用）。如果为`"True"`，则FSDP会在前向传递中显式预取下一个即将到来的all-gather。

    +   limit_all_gathers（`bool`，*可选*，默认为`False`）FSDP的limit_all_gathers（仅在传递`fsdp`字段时有用）。如果为`"True"`，FSDP会显式同步CPU线程，以防止太多的in-flight all-gathers。

    +   use_orig_params（`bool`，*可选*，默认为`True`）如果为`"True"`，允许在初始化期间使用非均匀的`requires_grad`，这意味着支持交替冻结和可训练参数。在参数高效微调等情况下非常有用。请参考此[博客]（[https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019)

    +   sync_module_states（`bool`，*可选*，默认为`True`）如果为`"True"`，每个单独包装的FSDP单元将从rank 0广播模块参数，以确保在初始化后所有rank中的参数相同

    +   activation_checkpointing（`bool`，*可选*，默认为`False`）：如果为`"True"`，激活检查点是一种通过清除某些层的激活并在反向传递期间重新计算它们来减少内存使用的技术。实际上，这是以额外的计算时间换取减少内存使用。

    +   xla（`bool`，*可选*，默认为`False`）：是否使用PyTorch/XLA完全分片数据并行训练。这是一个实验性功能，其API可能会在未来发生变化。

    +   xla_fsdp_settings（`dict`，*可选*）该值是一个存储XLA FSDP包装参数的字典。

        有关所有选项的完整列表，请参见[此处](https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py)。

    +   xla_fsdp_grad_ckpt（`bool`，*可选*，默认为`False`）：将在每个嵌套的XLA FSDP包装层上使用梯度检查点。只有在将xla标志设置为true，并通过fsdp_min_num_params或fsdp_transformer_layer_cls_to_wrap指定了自动包装策略时才能使用此设置。

+   `deepspeed`（`str`或`dict`，*可选*）—使用[Deepspeed](https://github.com/microsoft/deepspeed)。这是一个实验性功能，其API可能会在未来发生变化。该值可以是DeepSpeed json配置文件的位置（例如，`ds_config.json`）或已加载的json文件作为`dict`”

+   `label_smoothing_factor`（`float`，*可选*，默认为0.0）—要使用的标签平滑因子。零表示不进行标签平滑，否则基础的onehot编码标签将从0和1更改为`label_smoothing_factor/num_labels`和`1 - label_smoothing_factor + label_smoothing_factor/num_labels`。

+   `debug`（`str`或`DebugOption`列表，*可选*，默认为`""`）—启用一个或多个调试功能。这是一个实验性功能。

    可能的选项有：

    +   `"underflow_overflow"`：检测模型输入/输出中的溢出并报告导致事件的最后帧

    +   `"tpu_metrics_debug"`：在TPU上打印调试指标

    选项应该用空格分隔。

+   `optim` (`str` or `training_args.OptimizerNames`, *optional*, defaults to `"adamw_torch"`) — 要使用的优化器：adamw_hf、adamw_torch、adamw_torch_fused、adamw_apex_fused、adamw_anyprecision 或 adafactor。

+   `optim_args` (`str`, *optional*) — 提供给 AnyPrecisionAdamW 的可选参数。

+   `group_by_length` (`bool`, *optional*, defaults to `False`) — 是否在训练数据集中将大致相同长度的样本分组在一起（以最小化填充并提高效率）。仅在应用动态填充时有用。

+   `length_column_name` (`str`, *optional*, defaults to `"length"`) — 预先计算长度的列名。如果该列存在，则按长度分组将使用这些值而不是在训练启动时计算它们。仅在 `group_by_length` 为 `True` 且数据集是 `Dataset` 的实例时才会被忽略。

+   `report_to` (`str` or `List[str]`, *optional*, defaults to `"all"`) — 报告结果和日志的集成列表。支持的平台有 `"azure_ml"`、`"clearml"`、`"codecarbon"`、`"comet_ml"`、`"dagshub"`、`"dvclive"`、`"flyte"`、`"mlflow"`、`"neptune"`、`"tensorboard"` 和 `"wandb"`。使用 `"all"` 报告到所有已安装的集成，使用 `"none"` 不报告到任何集成。

+   `ddp_find_unused_parameters` (`bool`, *optional*) — 在使用分布式训练时，传递给 `DistributedDataParallel` 的标志 `find_unused_parameters` 的值。如果使用了梯度检查点，则默认为 `False`，否则为 `True`。

+   `ddp_bucket_cap_mb` (`int`, *optional*) — 在使用分布式训练时，传递给 `DistributedDataParallel` 的标志 `bucket_cap_mb` 的值。

+   `ddp_broadcast_buffers` (`bool`, *optional*) — 在使用分布式训练时，传递给 `DistributedDataParallel` 的标志 `broadcast_buffers` 的值。如果使用了梯度检查点，则默认为 `False`，否则为 `True`。

+   `dataloader_pin_memory` (`bool`, *optional*, defaults to `True`) — 是否要在数据加载器中固定内存。默认为 `True`。

+   `dataloader_persistent_workers` (`bool`, *optional*, defaults to `False`) — 如果为 True，则数据加载器在数据集被消耗一次后不会关闭工作进程。这允许保持工作人员数据集实例的活动状态。可能会加快训练速度，但会增加 RAM 使用量。默认为 `False`。

+   `skip_memory_metrics` (`bool`, *optional*, defaults to `True`) — 是否跳过将内存分析报告添加到指标中。默认跳过此步骤，因为它会减慢训练和评估速度。

+   `push_to_hub` (`bool`, *optional*, defaults to `False`) — 每次保存模型时是否将模型推送到 Hub。如果激活了此选项，`output_dir` 将开始一个与存储库同步的 git 目录（由 `hub_model_id` 确定），并且每次触发保存时都会推送内容（取决于您的 `save_strategy`）。调用 [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model) 也会触发推送。

    如果 `output_dir` 存在，则需要是将 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 推送到的存储库的本地克隆。

+   `resume_from_checkpoint` (`str`, *optional*) — 您的模型的有效检查点所在文件夹的路径。此参数不会直接被 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 使用，而是打算由您的训练/评估脚本使用。有关更多详细信息，请参阅 [示例脚本](https://github.com/huggingface/transformers/tree/main/examples)。

+   `hub_model_id` (`str`, *optional*) — 与本地 *output_dir* 同步的存储库的名称。它可以是一个简单的模型 ID，此时模型将被推送到您的命名空间。否则，它应该是整个存储库名称，例如 `"user_name/model"`，这样您就可以推送到您所属的组织，例如 `"organization_name/model"`。将默认为 `user_name/output_dir_name`，其中 *output_dir_name* 是 `output_dir` 的名称。

    将默认为 `output_dir` 的名称。

+   `hub_strategy` (`str` 或 `HubStrategy`, *optional*, 默认为 `"every_save"`) — 定义推送到 Hub 的范围和时间。可能的值有：

    +   `"end"`: 当调用 [save_model()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.save_model) 方法时，推送模型、其配置、分词器（如果传递给 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)）以及模型卡片的草稿。

    +   `"every_save"`: 每次保存模型时，推送模型、其配置、分词器（如果传递给 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)）以及模型卡片的草稿。推送是异步的，以避免阻塞训练，如果保存非常频繁，则只有在上一个推送完成后才会尝试新的推送。在训练结束时，使用最终模型进行最后一次推送。

    +   `"checkpoint"`: 类似于 `"every_save"`，但最新的检查点也会被推送到名为 last-checkpoint 的子文件夹中，这样您可以轻松地使用 `trainer.train(resume_from_checkpoint="last-checkpoint")` 恢复训练。

    +   `"all_checkpoints"`: 类似于 `"checkpoint"`，但所有检查点都像它们出现在输出文件夹中一样被推送（因此您将在最终存储库中获得一个检查点文件夹）。

+   `hub_token` (`str`, *optional*) — 用于将模型推送到 Hub 的令牌。将默认为使用 `huggingface-cli login` 获取的缓存文件夹中的令牌。

+   `hub_private_repo` (`bool`, *optional*, 默认为 `False`) — 如果为 True，则 Hub 存储库将设置为私有。

+   `hub_always_push` (`bool`, *optional*, 默认为 `False`) — 除非为 `True`，否则 `Trainer` 在上一个推送未完成时将跳过推送检查点。

+   `gradient_checkpointing` (`bool`, *optional*, 默认为 `False`) — 如果为 True，则使用梯度检查点来节省内存，但会导致反向传播速度变慢。

+   `gradient_checkpointing_kwargs` (`dict`, *optional*, 默认为 `None`) — 要传递给 `gradient_checkpointing_enable` 方法的关键字参数。

+   `include_inputs_for_metrics` (`bool`, *optional*, 默认为 `False`) — 是否将输入传递给 `compute_metrics` 函数。这适用于需要输入、预测和参考值进行评分计算的指标类。

+   `auto_find_batch_size` (`bool`, *optional*, 默认为 `False`) — 是否通过指数衰减自动找到适合内存的批量大小，避免 CUDA 内存不足错误。需要安装 accelerate (`pip install accelerate`)。

+   `full_determinism` (`bool`, *optional*, 默认为 `False`) — 如果为 `True`，将调用 [enable_full_determinism()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.enable_full_determinism) 而不是 [set_seed()](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.set_seed) 来确保在分布式训练中获得可重现的结果。重要提示：这会对性能产生负面影响，因此只能用于调试目的。

+   `torchdynamo` (`str`, *optional*) — 如果设置，TorchDynamo 的后端编译器。可能的选择是 `"eager"`, `"aot_eager"`, `"inductor"`, `"nvfuser"`, `"aot_nvfuser"`, `"aot_cudagraphs"`, `"ofi"`, `"fx2trt"`, `"onnxrt"` 和 `"ipex"`。

+   `ray_scope`（`str`，*可选*，默认为`"last"`）— 在使用Ray进行超参数搜索时要使用的范围。默认情况下，将使用`"last"`。然后，Ray将使用所有试验的最后一个检查点，进行比较并选择最佳的一个。但也有其他选项可用。查看[Ray文档](https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial)以获取更多选项。

+   `ddp_timeout`（`int`，*可选*，默认为1800）— `torch.distributed.init_process_group`调用的超时时间，用于避免在分布式运行中执行缓慢操作时发生GPU套接字超时。请参考[PyTorch文档]([https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group))以获取更多信息。

+   `use_mps_device`（`bool`，*可选*，默认为`False`）— 此参数已弃用。如果可用，将使用`mps`设备，类似于`cuda`设备。

+   `torch_compile`（`bool`，*可选*，默认为`False`）— 是否使用PyTorch 2.0 [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/)编译模型。

    这将使用[`torch.compile` API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile)的最佳默认值。您可以使用参数`torch_compile_backend`和`torch_compile_mode`自定义默认值，但我们不能保证它们中的任何一个会起作用，因为支持逐步在PyTorch中推出。

    此标志和整个编译API是实验性的，可能会在未来的版本中发生变化。

+   `torch_compile_backend`（`str`，*可选*）— 在`torch.compile`中要使用的后端。如果设置为任何值，`torch_compile`将被设置为`True`。

    请参考PyTorch文档以获取可能的值，并注意它们可能会随着PyTorch版本的变化而改变。

    此标志是实验性的，可能会在未来的版本中发生变化。

+   `torch_compile_mode`（`str`，*可选*）— 在`torch.compile`中要使用的模式。如果设置为任何值，`torch_compile`将被设置为`True`。

    请参考PyTorch文档以获取可能的值，并注意它们可能会随着PyTorch版本的变化而改变。

    此标志是实验性的，可能会在未来的版本中发生变化。

+   `split_batches`（`bool`，*可选*）— 在分布式训练期间，加速器是否应该在设备之间分割数据加载器产生的批次。如果

    设置为`True`，实际使用的批量大小将在任何类型的分布式进程上相同，但必须是

    将多个进程的数量（例如GPU）的倍数四舍五入。

+   `include_tokens_per_second`（`bool`，*可选*）— 是否计算每个设备每秒的标记数，用于训练速度指标。

    这将在训练数据加载器之前迭代整个训练数据加载器一次，

    并且会减慢整个过程。

+   `include_num_input_tokens_seen`（`bool`，*可选*）— 是否要跟踪整个训练过程中看到的输入标记数量。

    在分布式训练中可能会较慢，因为必须调用gather操作。

+   `neftune_noise_alpha`（`Optional[float]`）— 如果不是`None`，将激活NEFTune噪声嵌入。这可以极大地提高指导微调的模型性能。查看[原始论文](https://arxiv.org/abs/2310.05914)和[原始代码](https://github.com/neelsjain/NEFTune)。支持transformers的`PreTrainedModel`和peft的`PeftModel`。

+   `sortish_sampler`（`bool`，*可选*，默认为`False`）— 是否使用*sortish sampler*。目前仅在底层数据集为*Seq2SeqDataset*时才可能，但将在不久的将来普遍可用。

    根据长度对输入进行排序，以最小化填充大小，并在训练集中加入一些随机性。

+   `predict_with_generate`（`bool`，*可选*，默认为`False`）— 是否使用生成来计算生成指标（ROUGE，BLEU）。

+   `generation_max_length` (`int`, *optional*) — 在`predict_with_generate=True`时，在每个评估循环中使用的`max_length`。将默认为模型配置的`max_length`值。

+   `generation_num_beams` (`int`, *optional*) — 在`predict_with_generate=True`时，在每个评估循环中使用的`num_beams`。将默认为模型配置的`num_beams`值。

+   `generation_config` (`str`或`Path`或[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig), *optional*) — 允许从`from_pretrained`方法加载一个[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)。这可以是：

    +   一个字符串，预训练模型配置的*模型id*，托管在huggingface.co上的模型存储库内。有效的模型id可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。

    +   一个*目录*的路径，其中包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained)方法保存的配置文件，例如，`./my_model_directory/`。

    +   一个[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)对象。

TrainingArguments是我们在示例脚本中使用的与训练循环本身相关的参数的子集。

使用[HfArgumentParser](/docs/transformers/v4.37.2/en/internal/trainer_utils#transformers.HfArgumentParser)，我们可以将这个类转换为可以在命令行上指定的[argparse](https://docs.python.org/3/library/argparse#module-argparse)参数。

#### `to_dict`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args_seq2seq.py#L87)

```py
( )
```

将此实例序列化，将`Enum`替换为它们的值，将`GenerationConfig`替换为字典（用于JSON序列化支持）。通过删除其值来混淆标记值。
