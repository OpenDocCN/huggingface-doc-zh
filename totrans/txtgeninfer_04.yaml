- en: Installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/text-generation-inference/installation](https://huggingface.co/docs/text-generation-inference/installation)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/text-generation-inference/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/entry/start.96d64f85.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/scheduler.9680c161.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/singletons.5632daf5.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/index.9d57cde4.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/paths.5eca520f.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/entry/app.48a2a24c.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/index.38d74ee1.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/nodes/0.c01ff294.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/nodes/15.30a63270.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/Tip.9bb23095.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/CodeBlock.1371964c.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/Heading.74c51a96.js">
  prefs: []
  type: TYPE_NORMAL
- en: This section explains how to install the CLI tool as well as installing TGI
    from source. **The strongly recommended approach is to use Docker, as it does
    not require much setup. Check [the Quick Tour](./quicktour) to learn how to run
    TGI with Docker.**
  prefs: []
  type: TYPE_NORMAL
- en: Install CLI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use TGI command-line interface (CLI) to download weights, serve and
    quantize models, or get information on serving parameters.
  prefs: []
  type: TYPE_NORMAL
- en: To install the CLI, you need to first clone the TGI repository and then run
    `make`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you would like to serve models with custom kernels, run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Local Installation from Source
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you start, you will need to setup your environment, and install Text
    Generation Inference. Text Generation Inference is tested on **Python 3.9+**.
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation Inference is available on pypi, conda and GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install and launch locally, first [install Rust](https://rustup.rs/) and
    create a Python virtual environment with at least Python 3.9, e.g. using conda:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You may also need to install Protoc.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Linux:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'On MacOS, using Homebrew:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then run to install Text Generation Inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'On some machines, you may also need the OpenSSL libraries and gcc. On Linux
    machines, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once installation is done, simply run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This will serve Falcon 7B Instruct model from the port 8080, which we can query.
  prefs: []
  type: TYPE_NORMAL
