- en: Installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/installation](https://huggingface.co/docs/text-generation-inference/installation)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: This section explains how to install the CLI tool as well as installing TGI
    from source. **The strongly recommended approach is to use Docker, as it does
    not require much setup. Check [the Quick Tour](./quicktour) to learn how to run
    TGI with Docker.**
  prefs: []
  type: TYPE_NORMAL
- en: Install CLI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use TGI command-line interface (CLI) to download weights, serve and
    quantize models, or get information on serving parameters.
  prefs: []
  type: TYPE_NORMAL
- en: To install the CLI, you need to first clone the TGI repository and then run
    `make`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you would like to serve models with custom kernels, run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Local Installation from Source
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you start, you will need to setup your environment, and install Text
    Generation Inference. Text Generation Inference is tested on **Python 3.9+**.
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation Inference is available on pypi, conda and GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install and launch locally, first [install Rust](https://rustup.rs/) and
    create a Python virtual environment with at least Python 3.9, e.g. using conda:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You may also need to install Protoc.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Linux:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'On MacOS, using Homebrew:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then run to install Text Generation Inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'On some machines, you may also need the OpenSSL libraries and gcc. On Linux
    machines, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once installation is done, simply run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This will serve Falcon 7B Instruct model from the port 8080, which we can query.
  prefs: []
  type: TYPE_NORMAL
