# 使用多个 GPU 进行分布式推理

> 原始文本：[`huggingface.co/docs/diffusers/training/distributed_inference`](https://huggingface.co/docs/diffusers/training/distributed_inference)

在分布式设置中，您可以使用🤗 [Accelerate](https://huggingface.co/docs/accelerate/index)或[PyTorch Distributed](https://pytorch.org/tutorials/beginner/dist_overview.html)在多个 GPU 上运行推理，这对于并行生成多个提示很有用。

本指南将向您展示如何使用🤗 Accelerate 和 PyTorch Distributed 进行分布式推理。

## 🤗 Accelerate

🤗 [Accelerate](https://huggingface.co/docs/accelerate/index)是一个旨在简化跨分布式设置训练或运行推理的库。它简化了设置分布式环境的过程，让您可以专注于您的 PyTorch 代码。

首先，创建一个 Python 文件并初始化一个[accelerate.PartialState](https://huggingface.co/docs/accelerate/v0.27.0/en/package_reference/state#accelerate.PartialState)来创建一个分布式环境；您的设置会自动检测，因此不需要显式定义`rank`或`world_size`。将 DiffusionPipeline 移动到`distributed_state.device`以为每个进程分配一个 GPU。

现在使用[split_between_processes](https://huggingface.co/docs/accelerate/v0.27.0/en/package_reference/state#accelerate.PartialState.split_between_processes)实用程序作为上下文管理器，自动在进程数量之间分发提示。

```py
import torch
from accelerate import PartialState
from diffusers import DiffusionPipeline

pipeline = DiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, use_safetensors=True
)
distributed_state = PartialState()
pipeline.to(distributed_state.device)

with distributed_state.split_between_processes(["a dog", "a cat"]) as prompt:
    result = pipeline(prompt).images[0]
    result.save(f"result_{distributed_state.process_index}.png")
```

使用`--num_processes`参数指定要使用的 GPU 数量，并调用`accelerate launch`来运行脚本：

```py
accelerate launch run_distributed.py --num_processes=2
```

要了解更多，请查看[使用🤗 Accelerate 进行分布式推理](https://huggingface.co/docs/accelerate/en/usage_guides/distributed_inference#distributed-inference-with-accelerate)指南。

## PyTorch Distributed

PyTorch 支持[`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)，它实现了数据并行。

首先，创建一个 Python 文件，导入`torch.distributed`和`torch.multiprocessing`来设置分布式进程组，并在每个 GPU 上生成推理进程。您还应该初始化一个 DiffusionPipeline：

```py
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

from diffusers import DiffusionPipeline

sd = DiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, use_safetensors=True
)
```

您需要创建一个运行推理的函数；[`init_process_group`](https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group)用于创建一个分布式环境，指定要使用的后端类型、当前进程的`rank`和参与的进程数量`world_size`。如果您要在 2 个 GPU 上并行运行推理，则`world_size`为 2。

将 DiffusionPipeline 移动到`rank`并使用`get_rank`为每个进程分配一个 GPU，每个进程处理不同的提示：

```py
def run_inference(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

    sd.to(rank)

    if torch.distributed.get_rank() == 0:
        prompt = "a dog"
    elif torch.distributed.get_rank() == 1:
        prompt = "a cat"

    image = sd(prompt).images[0]
    image.save(f"./{'_'.join(prompt)}.png")
```

要运行分布式推理，调用[`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn)在`world_size`中定义的 GPU 数量上运行`run_inference`函数：

```py
def main():
    world_size = 2
    mp.spawn(run_inference, args=(world_size,), nprocs=world_size, join=True)

if __name__ == "__main__":
    main()
```

完成推理脚本后，使用`--nproc_per_node`参数指定要使用的 GPU 数量，并调用`torchrun`来运行脚本：

```py
torchrun run_distributed.py --nproc_per_node=2
```
