# Kandinsky 2.1

> 原文链接: [https://huggingface.co/docs/diffusers/api/pipelines/kandinsky](https://huggingface.co/docs/diffusers/api/pipelines/kandinsky)

Kandinsky 2.1由[Arseniy Shakhmatov](https://github.com/cene555)、[Anton Razzhigaev](https://github.com/razzant)、[Aleksandr Nikolich](https://github.com/AlexWortega)、[Vladimir Arkhipkin](https://github.com/oriBetelgeuse)、[Igor Pavlov](https://github.com/boomb0om)、[Andrey Kuznetsov](https://github.com/kuznetsoffandrey)和[Denis Dimitrov](https://github.com/denndimitrov)创建。

它在GitHub页面上的描述是：

*Kandinsky 2.1继承了Dall-E 2和潜在扩散的最佳实践，同时引入了一些新的想法。作为文本和图像编码器，它使用CLIP模型和潜在空间之间的扩散图像先验（映射）。这种方法提高了模型的视觉性能，并揭示了在图像和文本引导的图像操作中开启新的视野。*

原始代码库可在[ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2)找到。

查看Hub上的[Kandinsky Community](https://huggingface.co/kandinsky-community)组织，获取官方模型检查点，用于文本到图像、图像到图像和修复等任务。

请确保查看调度器的[指南](../../using-diffusers/schedulers)，了解如何探索调度器速度和质量之间的权衡，并查看[跨管道重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分，了解如何有效地将相同的组件加载到多个管道中。

## KandinskyPriorPipeline

### `class diffusers.KandinskyPriorPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py#L128)

```py
( prior: PriorTransformer image_encoder: CLIPVisionModelWithProjection text_encoder: CLIPTextModelWithProjection tokenizer: CLIPTokenizer scheduler: UnCLIPScheduler image_processor: CLIPImageProcessor )
```

参数

+   `prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer)) — 用于从文本嵌入中近似图像嵌入的经典unCLIP先验。

+   `image_encoder` (`CLIPVisionModelWithProjection`) — 冻结的图像编码器。

+   `text_encoder` (`CLIPTextModelWithProjection`) — 冻结的文本编码器。

+   `tokenizer` (`CLIPTokenizer`) — 类[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)的分词器。

+   `scheduler` (`UnCLIPScheduler`) — 与`prior`结合使用的调度器，用于生成图像嵌入。

用于生成Kandinsky图像先验的管道

该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档，了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py#L397)

```py
( prompt: Union negative_prompt: Union = None num_images_per_prompt: int = 1 num_inference_steps: int = 25 generator: Union = None latents: Optional = None guidance_scale: float = 4.0 output_type: Optional = 'pt' return_dict: bool = True ) → export const metadata = 'undefined';KandinskyPriorPipelineOutput or tuple
```

参数

+   `prompt` (`str`或`List[str]`) — 用于指导图像生成的提示。

+   `negative_prompt` (`str`或`List[str]`, *optional*) — 不用于指导图像生成的提示。如果不使用指导（即如果`guidance_scale`小于`1`，则忽略）。

+   `num_images_per_prompt` (`int`, *optional*, 默认为1) — 每个提示生成的图像数量。

+   `num_inference_steps` (`int`, *optional*, 默认为25) — 降噪步骤的数量。更多的降噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `generator` (`torch.Generator`或`List[torch.Generator]`, *optional*) — 一个或多个[torch生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *可选*) — 预先生成的噪声潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，则将使用提供的随机 `generator` 进行采样生成潜变量张量。

+   `guidance_scale` (`float`, *可选*, 默认为 4.0) — 如 [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598) 中定义的引导比例。`guidance_scale` 定义为 [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf) 中方程式 2 的 `w`。通过设置 `guidance_scale > 1` 来启用引导比例。更高的引导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。

+   `output_type` (`str`, *可选*, 默认为 `"pt"`) — 生成图像的输出格式。选择之间: `"np"` (`np.array`) 或 `"pt"` (`torch.Tensor`)。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回 [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) 而不是普通元组。

返回

`KandinskyPriorPipelineOutput` 或 `tuple`

调用管道以进行生成时调用的函数。

示例:

```py
>>> from diffusers import KandinskyPipeline, KandinskyPriorPipeline
>>> import torch

>>> pipe_prior = KandinskyPriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-1-prior")
>>> pipe_prior.to("cuda")

>>> prompt = "red cat, 4k photo"
>>> out = pipe_prior(prompt)
>>> image_emb = out.image_embeds
>>> negative_image_emb = out.negative_image_embeds

>>> pipe = KandinskyPipeline.from_pretrained("kandinsky-community/kandinsky-2-1")
>>> pipe.to("cuda")

>>> image = pipe(
...     prompt,
...     image_embeds=image_emb,
...     negative_image_embeds=negative_image_emb,
...     height=768,
...     width=768,
...     num_inference_steps=100,
... ).images

>>> image[0].save("cat.png")
```

#### `插值`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py#L172)

```py
( images_and_prompts: List weights: List num_images_per_prompt: int = 1 num_inference_steps: int = 25 generator: Union = None latents: Optional = None negative_prior_prompt: Optional = None negative_prompt: str = '' guidance_scale: float = 4.0 device = None ) → export const metadata = 'undefined';KandinskyPriorPipelineOutput or tuple
```

参数

+   `images_and_prompts` (`List[Union[str, PIL.Image.Image, torch.FloatTensor]]`) — 用于引导图像生成的提示和图像列表。权重 — (`List[float]`): `images_and_prompts` 中每个条件的权重列表

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `num_inference_steps` (`int`, *可选*, 默认为 25) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个 [torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html) 以使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *可选*) — 预先生成的噪声潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，则将使用提供的随机 `generator` 进行采样生成潜变量张量。

+   `negative_prior_prompt` (`str`, *可选*) — 不用来引导先前扩散过程的提示。当不使用引导时被忽略（即如果 `guidance_scale` 小于 `1` 则被忽略）。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用来引导图像生成的提示。当不使用引导时被忽略（即如果 `guidance_scale` 小于 `1` 则被忽略）。

+   `guidance_scale` (`float`, *可选*, 默认为 4.0) — 如 [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598) 中定义的引导比例。`guidance_scale` 定义为 [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf) 中方程式 2 的 `w`。通过设置 `guidance_scale > 1` 来启用引导比例。更高的引导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。

返回

`KandinskyPriorPipelineOutput` 或 `tuple`

使用先前管道进行插值时调用的函数。

示例:

```py
>>> from diffusers import KandinskyPriorPipeline, KandinskyPipeline
>>> from diffusers.utils import load_image
>>> import PIL

>>> import torch
>>> from torchvision import transforms

>>> pipe_prior = KandinskyPriorPipeline.from_pretrained(
...     "kandinsky-community/kandinsky-2-1-prior", torch_dtype=torch.float16
... )
>>> pipe_prior.to("cuda")

>>> img1 = load_image(
...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
...     "/kandinsky/cat.png"
... )

>>> img2 = load_image(
...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
...     "/kandinsky/starry_night.jpeg"
... )

>>> images_texts = ["a cat", img1, img2]
>>> weights = [0.3, 0.3, 0.4]
>>> image_emb, zero_image_emb = pipe_prior.interpolate(images_texts, weights)

>>> pipe = KandinskyPipeline.from_pretrained("kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16)
>>> pipe.to("cuda")

>>> image = pipe(
...     "",
...     image_embeds=image_emb,
...     negative_image_embeds=zero_image_emb,
...     height=768,
...     width=768,
...     num_inference_steps=150,
... ).images[0]

>>> image.save("starry_cat.png")
```

## KandinskyPipeline

### `class diffusers.KandinskyPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky.py#L76)

```py
( text_encoder: MultilingualCLIP tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: Union movq: VQModel )
```

参数

+   `text_encoder` (`MultilingualCLIP`) — 冻结的文本编码器。

+   `tokenizer` (`XLMRobertaTokenizer`) — 类的分词器

+   `scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) — 用于与 `unet` 结合使用以生成图像潜变量的调度器。

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) — 用于去噪图像嵌入的条件 U-Net 架构。

+   `movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)) — 用于从潜在空间生成图像的 MoVQ 解码器。

用于使用 Kandinsky 进行文本到图像生成的管道

此模型继承自 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky.py#L231)

```py
( prompt: Union image_embeds: Union negative_image_embeds: Union negative_prompt: Union = None height: int = 512 width: int = 512 num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`) — 用于指导图像生成的提示或提示。

+   `image_embeds` (`torch.FloatTensor` 或 `List[torch.FloatTensor]`) — 用于文本提示的剪辑图像嵌入，将用于条件图像生成。

+   `negative_image_embeds` (`torch.FloatTensor` 或 `List[torch.FloatTensor]`) — 负文本提示的剪辑图像嵌入，将用于条件图像生成。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用于指导图像生成的提示或提示。如果不使用指导（即如果 `guidance_scale` 小于 `1`，则忽略）。

+   `height` (`int`, *可选*, 默认为 512) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为 512) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *可选*, 默认为 100) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *可选*, 默认为 4.0) — 在 [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598) 中定义的指导比例。`guidance_scale` 定义为 [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf) 中方程式 2 的 `w`。通过设置 `guidance_scale > 1` 来启用指导比例。更高的指导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个 [torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *可选*) — 预先生成的噪声潜在空间，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，则将使用提供的随机 `generator` 进行采样生成潜在空间张量。

+   `output_type` (`str`, *可选*, 默认为 `"pil"`) — 生成图像的输出格式。可选择 `"pil"` (`PIL.Image.Image`)、`"np"` (`np.array`) 或 `"pt"` (`torch.Tensor`)。

+   `callback` (`Callable`, *可选*) — 在推理过程中每隔 `callback_steps` 步调用的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *可选*, 默认为 1) — 调用 `callback` 函数的频率。如果未指定，则在每一步都会调用回调函数。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回 [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) 而不是普通元组。

返回

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) 或 `tuple`

调用管道进行生成时调用的函数。

示例：

```py
>>> from diffusers import KandinskyPipeline, KandinskyPriorPipeline
>>> import torch

>>> pipe_prior = KandinskyPriorPipeline.from_pretrained("kandinsky-community/Kandinsky-2-1-prior")
>>> pipe_prior.to("cuda")

>>> prompt = "red cat, 4k photo"
>>> out = pipe_prior(prompt)
>>> image_emb = out.image_embeds
>>> negative_image_emb = out.negative_image_embeds

>>> pipe = KandinskyPipeline.from_pretrained("kandinsky-community/kandinsky-2-1")
>>> pipe.to("cuda")

>>> image = pipe(
...     prompt,
...     image_embeds=image_emb,
...     negative_image_embeds=negative_image_emb,
...     height=768,
...     width=768,
...     num_inference_steps=100,
... ).images

>>> image[0].save("cat.png")
```

## KandinskyCombinedPipeline

### `class diffusers.KandinskyCombinedPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L113)

```py
( text_encoder: MultilingualCLIP tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: Union movq: VQModel prior_prior: PriorTransformer prior_image_encoder: CLIPVisionModelWithProjection prior_text_encoder: CLIPTextModelWithProjection prior_tokenizer: CLIPTokenizer prior_scheduler: UnCLIPScheduler prior_image_processor: CLIPImageProcessor )
```

参数

+   `text_encoder` (`MultilingualCLIP`) — 冻结的文本编码器。

+   `tokenizer` (`XLMRobertaTokenizer`) — 类的分词器

+   `scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) — 用于与 `unet` 结合使用以生成图像潜变量的调度器。

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) — 用于降噪图像嵌入的条件 U-Net 架构。

+   `movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)) — MoVQ 解码器，用于从潜变量生成图像。

+   `prior_prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer)) — 用于近似从文本嵌入到图像嵌入的经典 unCLIP 先验。

+   `prior_image_encoder` (`CLIPVisionModelWithProjection`) — 冻结的图像编码器。

+   `prior_text_encoder` (`CLIPTextModelWithProjection`) — 冻结的文本编码器。

+   `prior_tokenizer` (`CLIPTokenizer`) — 类的分词器 [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)。

+   `prior_scheduler` (`UnCLIPScheduler`) — 用于与 `prior` 结合使用以生成图像嵌入的调度器。

使用 Kandinsky 进行文本到图像生成的组合管道

此模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L214)

```py
( prompt: Union negative_prompt: Union = None num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 height: int = 512 width: int = 512 prior_guidance_scale: float = 4.0 prior_num_inference_steps: int = 25 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`) — 用于引导图像生成的提示或多个提示。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用于引导图像生成的提示或多个提示。如果不使用引导（即如果 `guidance_scale` 小于 `1`，则忽略）。

+   `num_images_per_prompt` (`int`, *可选*，默认为 1) — 每个提示生成的图像数量。

+   `num_inference_steps` (`int`, *可选*，默认为 100) — 降噪步骤的数量。更多的降噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `height` (`int`, *可选*，默认为 512) — 生成图像的像素高度。

+   `width` (`int`, *可选*，默认为 512) — 生成图像的像素宽度。

+   `prior_guidance_scale` (`float`, *可选*，默认为 4.0) — 在[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。 `guidance_scale` 定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程式 2 的 `w`。 通过设置 `guidance_scale > 1` 来启用引导比例。更高的引导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以牺牲图像质量为代价。

+   `prior_num_inference_steps` (`int`, *可选*，默认为 100) — 降噪步骤的数量。更多的降噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `guidance_scale` (`float`, *可选*，默认为 4.0) — 在[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。 `guidance_scale` 定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程式 2 的 `w`。 通过设置 `guidance_scale > 1` 来启用引导比例。更高的引导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以牺牲图像质量为代价。

+   `generator`（`torch.Generator`或`List[torch.Generator]`，*可选*）— 一个或多个[torch生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents`（`torch.FloatTensor`，*可选*）— 预生成的嘈杂潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机`generator`进行采样生成潜变量张量。

+   `output_type`（`str`，*可选*，默认为`"pil"`）— 生成图像的输出格式。可选择："pil"（`PIL.Image.Image`）、"np"（`np.array`）或"pt"（`torch.Tensor`）。

+   `callback`（`Callable`，*可选*）— 在推断过程中每`callback_steps`步调用的函数。该函数使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps`（`int`，*可选*，默认为1）— 调用`callback`函数的频率。如果未指定，则在每个步骤时调用回调。

+   `return_dict`（`bool`，*可选*，默认为`True`）— 是否返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)而不是普通元组。

返回

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)或`tuple`

调用管道进行生成时调用的函数。

示例：

```py
from diffusers import AutoPipelineForText2Image
import torch

pipe = AutoPipelineForText2Image.from_pretrained(
    "kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16
)
pipe.enable_model_cpu_offload()

prompt = "A lion in galaxies, spirals, nebulae, stars, smoke, iridescent, intricate detail, octane render, 8k"

image = pipe(prompt=prompt, num_inference_steps=25).images[0]
```

#### `enable_sequential_cpu_offload`

[<源代码>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L195)

```py
( gpu_id = 0 )
```

使用🤗 Accelerate将所有模型（`unet`、`text_encoder`、`vae`和`safety checker`状态字典）转移到CPU，显著减少内存使用。模型移至`torch.device('meta')`，仅在调用其特定子模块的`forward`方法时才加载到GPU。卸载是基于子模块的。内存节省高于使用`enable_model_cpu_offload`，但性能较低。

## KandinskyImg2ImgPipeline

### `class diffusers.KandinskyImg2ImgPipeline`

[<源代码>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_img2img.py#L98)

```py
( text_encoder: MultilingualCLIP movq: VQModel tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: DDIMScheduler )
```

参数

+   `text_encoder`（`MultilingualCLIP`）— 冻结的文本编码器。

+   `tokenizer`（`XLMRobertaTokenizer`）— 类的分词器

+   `scheduler`（[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)）— 用于与`unet`结合使用以生成图像潜变量的调度器。

+   `unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)）— 用于去噪图像嵌入的条件U-Net架构。

+   `movq`（[VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)）— MoVQ图像编码器和解码器

Kandinsky的图像到图像生成管道

该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

#### `__call__`

[<源代码>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_img2img.py#L293)

```py
( prompt: Union image: Union image_embeds: FloatTensor negative_image_embeds: FloatTensor negative_prompt: Union = None height: int = 512 width: int = 512 num_inference_steps: int = 100 strength: float = 0.3 guidance_scale: float = 7.0 num_images_per_prompt: int = 1 generator: Union = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt`（`str`或`List[str]`）— 用于引导图像生成的提示或提示。

+   `image`（`torch.FloatTensor`，`PIL.Image.Image`）— `Image`，或表示图像批次的张量，将用作过程的起始点。

+   `image_embeds`（`torch.FloatTensor`或`List[torch.FloatTensor]`）— 用于文本提示的剪辑图像嵌入，将用于条件图像生成。

+   `negative_image_embeds` (`torch.FloatTensor` 或 `List[torch.FloatTensor]`) — 用于负文本提示的clip图像嵌入，将用于条件图像生成。

+   `negative_prompt` (`str` 或 `List[str]`, *optional*) — 不用于指导图像生成的提示或提示。如果不使用指导（即如果 `guidance_scale` 小于 `1`），则忽略。

+   `height` (`int`, *optional*, 默认为512) — 生成图像的高度（像素）。

+   `width` (`int`, *optional*, 默认为512) — 生成图像的宽度（像素）。

+   `num_inference_steps` (`int`, *optional*, 默认为100) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `strength` (`float`, *optional*, 默认为0.3) — 在概念上，指示要转换参考`image`的程度。必须在0和1之间。`image`将被用作起点，添加的噪音越多，`strength`越大。去噪步骤的数量取决于最初添加的噪音量。当 `strength` 为1 时，添加的噪音将最大化，去噪过程将运行指定的 `num_inference_steps` 的全部迭代次数。因此，值为1 的情况下，基本上忽略了 `image`。

+   `guidance_scale` (`float`, *optional*, 默认为4.0) — 在[Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)中定义的指导比例。`guidance_scale` 在[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)的方程式2中定义为`w`。通过设置 `guidance_scale > 1` 来启用指导比例。更高的指导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `num_images_per_prompt` (`int`, *optional*, 默认为1) — 每个提示生成的图像数量。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 一个或多个用于使生成过程确定性的[torch生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `output_type` (`str`, *optional*, 默认为`"pil"`) — 生成图像的输出格式。可选择："pil" (`PIL.Image.Image`)、"np" (`np.array`) 或 "pt" (`torch.Tensor`)。

+   `callback` (`Callable`, *optional*) — 在推理过程中每隔 `callback_steps` 步调用的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *optional*, 默认为1) — 调用 `callback` 函数的频率。如果未指定，将在每一步调用回调函数。

+   `return_dict` (`bool`, *optional*, 默认为`True`) — 是否返回一个[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)而不是一个普通的元组。

返回

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) 或 `tuple`

调用管道进行生成时调用的函数。

示例:

```py
>>> from diffusers import KandinskyImg2ImgPipeline, KandinskyPriorPipeline
>>> from diffusers.utils import load_image
>>> import torch

>>> pipe_prior = KandinskyPriorPipeline.from_pretrained(
...     "kandinsky-community/kandinsky-2-1-prior", torch_dtype=torch.float16
... )
>>> pipe_prior.to("cuda")

>>> prompt = "A red cartoon frog, 4k"
>>> image_emb, zero_image_emb = pipe_prior(prompt, return_dict=False)

>>> pipe = KandinskyImg2ImgPipeline.from_pretrained(
...     "kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16
... )
>>> pipe.to("cuda")

>>> init_image = load_image(
...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
...     "/kandinsky/frog.png"
... )

>>> image = pipe(
...     prompt,
...     image=init_image,
...     image_embeds=image_emb,
...     negative_image_embeds=zero_image_emb,
...     height=768,
...     width=768,
...     num_inference_steps=100,
...     strength=0.2,
... ).images

>>> image[0].save("red_frog.png")
```

## KandinskyImg2ImgCombinedPipeline

### `class diffusers.KandinskyImg2ImgCombinedPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L330)

```py
( text_encoder: MultilingualCLIP tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: Union movq: VQModel prior_prior: PriorTransformer prior_image_encoder: CLIPVisionModelWithProjection prior_text_encoder: CLIPTextModelWithProjection prior_tokenizer: CLIPTokenizer prior_scheduler: UnCLIPScheduler prior_image_processor: CLIPImageProcessor )
```

参数

+   `text_encoder` (`MultilingualCLIP`) — 冻结的文本编码器。

+   `tokenizer` (`XLMRobertaTokenizer`) — 类的分词器

+   `scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) — 与 `unet` 结合使用的调度器，用于生成图像潜在空间。

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) — 用于去噪图像嵌入的条件U-Net架构。

+   `movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)) — 用于从潜在空间生成图像的MoVQ解码器。

+   `prior_prior`（[PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer)）—用于近似从文本嵌入到图像嵌入的规范化unCLIP先验。

+   `prior_image_encoder`（`CLIPVisionModelWithProjection`）—冻结的图像编码器。

+   `prior_text_encoder`（`CLIPTextModelWithProjection`）—冻结的文本编码器。

+   `prior_tokenizer`（`CLIPTokenizer`）—类[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)的分词器。

+   `prior_scheduler`（`UnCLIPScheduler`）—用于与`prior`结合使用以生成图像嵌入的调度器。

使用Kandinsky进行图像到图像生成的组合管道

这个模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。检查超类文档以了解库为所有管道实现的通用方法（例如下载或保存，运行在特定设备上等）。

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L432)

```py
( prompt: Union image: Union negative_prompt: Union = None num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 strength: float = 0.3 height: int = 512 width: int = 512 prior_guidance_scale: float = 4.0 prior_num_inference_steps: int = 25 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt`（`str`或`List[str]`）—用于指导图像生成的提示或提示。

+   `image`（`torch.FloatTensor`，`PIL.Image.Image`，`np.ndarray`，`List[torch.FloatTensor]`，`List[PIL.Image.Image]`或`List[np.ndarray]`）—将用作过程起点的`Image`或表示图像批次的张量。如果直接传递潜在值，则不会再次编码。

+   `negative_prompt`（`str`或`List[str]`，*可选*）—不用于指导图像生成的提示或提示。当不使用引导时（即，如果`guidance_scale`小于`1`，则忽略）。

+   `num_images_per_prompt`（`int`，*可选*，默认为1）—每个提示生成的图像数量。

+   `num_inference_steps`（`int`，*可选*，默认为100）—去噪步骤的数量。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `height`（`int`，*可选*，默认为512）—生成图像的像素高度。

+   `width`（`int`，*可选*，默认为512）—生成图像的像素宽度。

+   `strength`（`float`，*可选*，默认为0.3）—在概念上，指示要转换参考`image`的程度。必须介于0和1之间。`image`将被用作起点，添加更多的噪音，`strength`越大。去噪步骤的数量取决于最初添加的噪音量。当`strength`为1时，添加的噪音将是最大的，并且去噪过程将运行指定的`num_inference_steps`的完整迭代次数。因此，值为1基本上忽略了`image`。

+   `prior_guidance_scale`（`float`，*可选*，默认为4.0）—在[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程式2的`w`。通过设置`guidance_scale > 1`来启用引导比例。更高的引导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `prior_num_inference_steps`（`int`，*可选*，默认为100）—去噪步骤的数量。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `guidance_scale`（`float`，*可选*，默认为4.0）—在[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程式2的`w`。通过设置`guidance_scale > 1`来启用引导比例。更高的引导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `generator`（`torch.Generator`或`List[torch.Generator]`，*可选*）— 一个或多个[torch生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)用于使生成具有确定性。

+   `latents`（`torch.FloatTensor`，*可选*）— 预生成的噪声潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示微调相同的生成。如果未提供，将使用提供的随机`generator`进行采样生成潜变量张量。

+   `output_type`（`str`，*可选*，默认为`"pil"`）— 生成图像的输出格式。选择`"pil"`（`PIL.Image.Image`），`"np"`（`np.array`）或`"pt"`（`torch.Tensor`）之间。

+   `callback`（`Callable`，*可选*）— 每`callback_steps`步在推断期间调用的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps`（`int`，*可选*，默认为1）— 调用`callback`函数的频率。如果未指定，将在每一步调用回调。

+   `return_dict`（`bool`，*可选*，默认为`True`）— 是否返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)而不是普通元组。

返回

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)或`tuple`

在调用管道进行生成时调用的函数。

示例：

```py
from diffusers import AutoPipelineForImage2Image
import torch
import requests
from io import BytesIO
from PIL import Image
import os

pipe = AutoPipelineForImage2Image.from_pretrained(
    "kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16
)
pipe.enable_model_cpu_offload()

prompt = "A fantasy landscape, Cinematic lighting"
negative_prompt = "low quality, bad quality"

url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"

response = requests.get(url)
image = Image.open(BytesIO(response.content)).convert("RGB")
image.thumbnail((768, 768))

image = pipe(prompt=prompt, image=original_image, num_inference_steps=25).images[0]
```

#### `enable_sequential_cpu_offload`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L412)

```py
( gpu_id = 0 )
```

使用加速将所有模型转移到CPU，显着减少内存使用。调用时，unet、text_encoder、vae和safety checker的状态字典将保存到CPU，然后移动到`torch.device('meta')，仅在它们的特定子模块调用其`forward`方法时才加载到GPU。请注意，卸载是基于子模块的。内存节省高于`enable_model_cpu_offload`，但性能较低。

## KandinskyInpaintPipeline

### `class diffusers.KandinskyInpaintPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_inpaint.py#L240)

```py
( text_encoder: MultilingualCLIP movq: VQModel tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: DDIMScheduler )
```

参数

+   `text_encoder`（`MultilingualCLIP`）— 冻结的文本编码器。

+   `tokenizer`（`XLMRobertaTokenizer`）— 类的分词器

+   `scheduler`（[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)）— 用于与`unet`结合使用以生成图像潜变量的调度器。

+   `unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)）— 用于去噪图像嵌入的条件U-Net架构。

+   `movq`（[VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)）— MoVQ图像编码器和解码器

使用Kandinsky2.1进行文本引导图像修复的管道

这个模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。检查超类文档以了解库为所有管道实现的通用方法（例如下载或保存，运行在特定设备上等）。

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_inpaint.py#L396)

```py
( prompt: Union image: Union mask_image: Union image_embeds: FloatTensor negative_image_embeds: FloatTensor negative_prompt: Union = None height: int = 512 width: int = 512 num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt`（`str`或`List[str]`）— 用于引导图像生成的提示或提示。

+   `image`（`torch.FloatTensor`，`PIL.Image.Image`或`np.ndarray`）— `Image`，或表示图像批次的张量，将用作过程的起始点。

+   `mask_image` (`PIL.Image.Image`,`torch.FloatTensor` 或 `np.ndarray`) — `Image`，或表示图像批次的张量，用于遮罩 `image`。遮罩中的白色像素将被重新绘制，而黑色像素将被保留。只有当传递的图像是 pytorch 张量时，才能将 pytorch 张量作为遮罩传递，它应该包含一个颜色通道（L）而不是 3，因此预期的形状应为 `(B, 1, H, W,)`、`(B, H, W)`、`(1, H, W)` 或 `(H, W)`。如果图像是 PIL 图像或 numpy 数组，则遮罩也应该是 PIL 图像或 numpy 数组。如果是 PIL 图像，它将在使用之前转换为单通道（亮度）。如果是 numpy 数组，则预期形状为 `(H, W)`。

+   `image_embeds` (`torch.FloatTensor` 或 `List[torch.FloatTensor]`) — 用于文本提示的 clip 图像嵌入，将用于条件图像生成。

+   `negative_image_embeds` (`torch.FloatTensor` 或 `List[torch.FloatTensor]`) — 用于负面文本提示的 clip 图像嵌入，将用于条件图像生成。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用来引导图像生成的提示或提示。当不使用引导时被忽略（即如果 `guidance_scale` 小于 `1` 则被忽略）。

+   `height` (`int`, *可选*, 默认为 512) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为 512) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *可选*, 默认为 100) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *可选*, 默认为 4.0) — 在[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`guidance_scale` 定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程式 2 的 `w`。通过设置 `guidance_scale > 1` 来启用引导比例。更高的引导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个[torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *可选*) — 预先生成的嘈杂潜变量，从高斯分布中采样，用作图像生成的输入。可以用来使用不同提示调整相同生成。如果未提供，则将使用随机 `generator` 采样生成潜变量张量。

+   `output_type` (`str`, *可选*, 默认为 `"pil"`) — 生成图像的输出格式。可选择 `"pil"` (`PIL.Image.Image`)、`"np"` (`np.array`) 或 `"pt"` (`torch.Tensor`)。

+   `callback` (`Callable`, *可选*) — 在推理过程中每 `callback_steps` 步调用的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *可选*, 默认为 1) — 调用 `callback` 函数的频率。如果未指定，将在每一步调用回调函数。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回 [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) 而不是普通元组。

返回

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) 或 `tuple`

调用管道进行生成时调用的函数。

示例：

```py
>>> from diffusers import KandinskyInpaintPipeline, KandinskyPriorPipeline
>>> from diffusers.utils import load_image
>>> import torch
>>> import numpy as np

>>> pipe_prior = KandinskyPriorPipeline.from_pretrained(
...     "kandinsky-community/kandinsky-2-1-prior", torch_dtype=torch.float16
... )
>>> pipe_prior.to("cuda")

>>> prompt = "a hat"
>>> image_emb, zero_image_emb = pipe_prior(prompt, return_dict=False)

>>> pipe = KandinskyInpaintPipeline.from_pretrained(
...     "kandinsky-community/kandinsky-2-1-inpaint", torch_dtype=torch.float16
... )
>>> pipe.to("cuda")

>>> init_image = load_image(
...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
...     "/kandinsky/cat.png"
... )

>>> mask = np.zeros((768, 768), dtype=np.float32)
>>> mask[:250, 250:-250] = 1

>>> out = pipe(
...     prompt,
...     image=init_image,
...     mask_image=mask,
...     image_embeds=image_emb,
...     negative_image_embeds=zero_image_emb,
...     height=768,
...     width=768,
...     num_inference_steps=50,
... )

>>> image = out.images[0]
>>> image.save("cat_with_hat.png")
```

## KandinskyInpaintCombinedPipeline

### `class diffusers.KandinskyInpaintCombinedPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L570)

```py
( text_encoder: MultilingualCLIP tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: Union movq: VQModel prior_prior: PriorTransformer prior_image_encoder: CLIPVisionModelWithProjection prior_text_encoder: CLIPTextModelWithProjection prior_tokenizer: CLIPTokenizer prior_scheduler: UnCLIPScheduler prior_image_processor: CLIPImageProcessor )
```

参数

+   `text_encoder` (`MultilingualCLIP`) — 冻结的文本编码器。

+   `tokenizer` (`XLMRobertaTokenizer`) — 类的分词器

+   `scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) — 用于与`unet`结合使用以生成图像潜在的调度器。

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) — 用于降噪图像嵌入的条件U-Net架构。

+   `movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)) — 用于从潜在图像生成图像的MoVQ解码器。

+   `prior_prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer)) — 用于从文本嵌入中近似生成图像嵌入的标准unCLIP先验。

+   `prior_image_encoder` (`CLIPVisionModelWithProjection`) — 冻结的图像编码器。

+   `prior_text_encoder` (`CLIPTextModelWithProjection`) — 冻结的文本编码器。

+   `prior_tokenizer` (`CLIPTokenizer`) — 类[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)的分词器。

+   `prior_scheduler` (`UnCLIPScheduler`) — 用于与`prior`结合使用以生成图像嵌入的调度器。

使用Kandinsky进行生成的组合管道

该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L672)

```py
( prompt: Union image: Union mask_image: Union negative_prompt: Union = None num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 height: int = 512 width: int = 512 prior_guidance_scale: float = 4.0 prior_num_inference_steps: int = 25 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`) — 用于指导图像生成的提示或多个提示。

+   `image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, 或 `List[np.ndarray]`) — 用作过程起点的`Image`或表示图像批次的张量。如果直接传递潜在图像，则不会再次编码。

+   `mask_image` (`np.array`) — 表示图像批次的张量，用于遮罩`image`。遮罩中的白色像素将被重新绘制，而黑色像素将被保留。如果`mask_image`是PIL图像，则在使用之前将其转换为单通道（亮度）。如果它是张量，则应该包含一个颜色通道（L）而不是3个，因此预期形状将是`(B, H, W, 1)`。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用于指导图像生成的提示或多个提示。如果不使用引导（即如果`guidance_scale`小于`1`，则忽略）。

+   `num_images_per_prompt` (`int`, *可选*, 默认为1) — 每个提示生成的图像数量。

+   `num_inference_steps` (`int`, *可选*, 默认为100) — 降噪步骤的数量。更多的降噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `height` (`int`, *可选*, 默认为512) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为512) — 生成图像的像素宽度。

+   `prior_guidance_scale` (`float`, *可选*, 默认为4.0) — 如[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`guidance_scale` 在[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)的方程式2中定义为`w`。通过设置`guidance_scale > 1`来启用引导比例。更高的引导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `prior_num_inference_steps` (`int`, *可选*, 默认为100) — 降噪步骤的数量。更多的降噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `guidance_scale` (`float`, *可选*, 默认为 4.0) — 在[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`guidance_scale` 定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程式 2 的 `w`。通过设置 `guidance_scale > 1` 启用引导比例。更高的引导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个 [torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html) 以使生成具有确定性。

+   `latents` (`torch.FloatTensor`, *可选*) — 预先生成的噪声潜变量，从高斯分布中采样，用作图像生成的输入。可以用来使用不同提示微调相同的生成。如果未提供，则将使用提供的随机 `generator` 进行采样生成潜变量张量。

+   `output_type` (`str`, *可选*, 默认为 `"pil"`) — 生成图像的输出格式。可选择：`"pil"` (`PIL.Image.Image`)、`"np"` (`np.array`) 或 `"pt"` (`torch.Tensor`)。

+   `callback` (`Callable`, *可选*) — 在推理过程中每隔 `callback_steps` 步调用一次的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *可选*, 默认为 1) — 调用 `callback` 函数的频率。如果未指定，则在每一步调用回调。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回 [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) 而不是普通的元组。

返回

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) 或 `tuple`

调用管道进行生成时调用的函数。

示例：

```py
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image
import torch
import numpy as np

pipe = AutoPipelineForInpainting.from_pretrained(
    "kandinsky-community/kandinsky-2-1-inpaint", torch_dtype=torch.float16
)
pipe.enable_model_cpu_offload()

prompt = "A fantasy landscape, Cinematic lighting"
negative_prompt = "low quality, bad quality"

original_image = load_image(
    "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main" "/kandinsky/cat.png"
)

mask = np.zeros((768, 768), dtype=np.float32)
# Let's mask out an area above the cat's head
mask[:250, 250:-250] = 1

image = pipe(prompt=prompt, image=original_image, mask_image=mask, num_inference_steps=25).images[0]
```

#### `enable_sequential_cpu_offload`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L652)

```py
( gpu_id = 0 )
```

使用加速器将所有模型转移到 CPU，显著减少内存使用。调用时，unet、text_encoder、vae 和安全检查器的状态字典将保存到 CPU，然后移动到 `torch.device('meta')`，仅在它们的特定子模块调用`forward`方法时才加载到 GPU。请注意，卸载是基于子模块的。与`enable_model_cpu_offload`相比，内存节省更高，但性能较低。
