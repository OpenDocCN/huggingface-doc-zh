# Kandinsky 2.1

> åŸæ–‡é“¾æ¥: [https://huggingface.co/docs/diffusers/api/pipelines/kandinsky](https://huggingface.co/docs/diffusers/api/pipelines/kandinsky)

Kandinsky 2.1ç”±[Arseniy Shakhmatov](https://github.com/cene555)ã€[Anton Razzhigaev](https://github.com/razzant)ã€[Aleksandr Nikolich](https://github.com/AlexWortega)ã€[Vladimir Arkhipkin](https://github.com/oriBetelgeuse)ã€[Igor Pavlov](https://github.com/boomb0om)ã€[Andrey Kuznetsov](https://github.com/kuznetsoffandrey)å’Œ[Denis Dimitrov](https://github.com/denndimitrov)åˆ›å»ºã€‚

å®ƒåœ¨GitHubé¡µé¢ä¸Šçš„æè¿°æ˜¯ï¼š

*Kandinsky 2.1ç»§æ‰¿äº†Dall-E 2å’Œæ½œåœ¨æ‰©æ•£çš„æœ€ä½³å®è·µï¼ŒåŒæ—¶å¼•å…¥äº†ä¸€äº›æ–°çš„æƒ³æ³•ã€‚ä½œä¸ºæ–‡æœ¬å’Œå›¾åƒç¼–ç å™¨ï¼Œå®ƒä½¿ç”¨CLIPæ¨¡å‹å’Œæ½œåœ¨ç©ºé—´ä¹‹é—´çš„æ‰©æ•£å›¾åƒå…ˆéªŒï¼ˆæ˜ å°„ï¼‰ã€‚è¿™ç§æ–¹æ³•æé«˜äº†æ¨¡å‹çš„è§†è§‰æ€§èƒ½ï¼Œå¹¶æ­ç¤ºäº†åœ¨å›¾åƒå’Œæ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ“ä½œä¸­å¼€å¯æ–°çš„è§†é‡ã€‚*

åŸå§‹ä»£ç åº“å¯åœ¨[ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2)æ‰¾åˆ°ã€‚

æŸ¥çœ‹Hubä¸Šçš„[Kandinsky Community](https://huggingface.co/kandinsky-community)ç»„ç»‡ï¼Œè·å–å®˜æ–¹æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒå’Œä¿®å¤ç­‰ä»»åŠ¡ã€‚

è¯·ç¡®ä¿æŸ¥çœ‹è°ƒåº¦å™¨çš„[æŒ‡å—](../../using-diffusers/schedulers)ï¼Œäº†è§£å¦‚ä½•æ¢ç´¢è°ƒåº¦å™¨é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æŸ¥çœ‹[è·¨ç®¡é“é‡ç”¨ç»„ä»¶](../../using-diffusers/loading#reuse-components-across-pipelines)éƒ¨åˆ†ï¼Œäº†è§£å¦‚ä½•æœ‰æ•ˆåœ°å°†ç›¸åŒçš„ç»„ä»¶åŠ è½½åˆ°å¤šä¸ªç®¡é“ä¸­ã€‚

## KandinskyPriorPipeline

### `class diffusers.KandinskyPriorPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py#L128)

```py
( prior: PriorTransformer image_encoder: CLIPVisionModelWithProjection text_encoder: CLIPTextModelWithProjection tokenizer: CLIPTokenizer scheduler: UnCLIPScheduler image_processor: CLIPImageProcessor )
```

å‚æ•°

+   `prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer)) â€” ç”¨äºä»æ–‡æœ¬åµŒå…¥ä¸­è¿‘ä¼¼å›¾åƒåµŒå…¥çš„ç»å…¸unCLIPå…ˆéªŒã€‚

+   `image_encoder` (`CLIPVisionModelWithProjection`) â€” å†»ç»“çš„å›¾åƒç¼–ç å™¨ã€‚

+   `text_encoder` (`CLIPTextModelWithProjection`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚

+   `tokenizer` (`CLIPTokenizer`) â€” ç±»[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)çš„åˆ†è¯å™¨ã€‚

+   `scheduler` (`UnCLIPScheduler`) â€” ä¸`prior`ç»“åˆä½¿ç”¨çš„è°ƒåº¦å™¨ï¼Œç”¨äºç”Ÿæˆå›¾åƒåµŒå…¥ã€‚

ç”¨äºç”ŸæˆKandinskyå›¾åƒå…ˆéªŒçš„ç®¡é“

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py#L397)

```py
( prompt: Union negative_prompt: Union = None num_images_per_prompt: int = 1 num_inference_steps: int = 25 generator: Union = None latents: Optional = None guidance_scale: float = 4.0 output_type: Optional = 'pt' return_dict: bool = True ) â†’ export const metadata = 'undefined';KandinskyPriorPipelineOutput or tuple
```

å‚æ•°

+   `prompt` (`str`æˆ–`List[str]`) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºã€‚

+   `negative_prompt` (`str`æˆ–`List[str]`, *optional*) â€” ä¸ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºã€‚å¦‚æœä¸ä½¿ç”¨æŒ‡å¯¼ï¼ˆå³å¦‚æœ`guidance_scale`å°äº`1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚

+   `num_images_per_prompt` (`int`, *optional*, é»˜è®¤ä¸º1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `num_inference_steps` (`int`, *optional*, é»˜è®¤ä¸º25) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚

+   `generator` (`torch.Generator`æˆ–`List[torch.Generator]`, *optional*) â€” ä¸€ä¸ªæˆ–å¤šä¸ª[torchç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ï¼Œç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚

+   `latents` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„å…ˆç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºè°ƒæ•´ç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†ä½¿ç”¨æä¾›çš„éšæœº `generator` è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚

+   `guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 4.0) â€” å¦‚ [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598) ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`guidance_scale` å®šä¹‰ä¸º [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf) ä¸­æ–¹ç¨‹å¼ 2 çš„ `w`ã€‚é€šè¿‡è®¾ç½® `guidance_scale > 1` æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚

+   `output_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"pt"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚é€‰æ‹©ä¹‹é—´: `"np"` (`np.array`) æˆ– `"pt"` (`torch.Tensor`)ã€‚

+   `return_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å› [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

`KandinskyPriorPipelineOutput` æˆ– `tuple`

è°ƒç”¨ç®¡é“ä»¥è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚

ç¤ºä¾‹:

```py
>>> from diffusers import KandinskyPipeline, KandinskyPriorPipeline
>>> import torch

>>> pipe_prior = KandinskyPriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-1-prior")
>>> pipe_prior.to("cuda")

>>> prompt = "red cat, 4k photo"
>>> out = pipe_prior(prompt)
>>> image_emb = out.image_embeds
>>> negative_image_emb = out.negative_image_embeds

>>> pipe = KandinskyPipeline.from_pretrained("kandinsky-community/kandinsky-2-1")
>>> pipe.to("cuda")

>>> image = pipe(
...     prompt,
...     image_embeds=image_emb,
...     negative_image_embeds=negative_image_emb,
...     height=768,
...     width=768,
...     num_inference_steps=100,
... ).images

>>> image[0].save("cat.png")
```

#### `æ’å€¼`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py#L172)

```py
( images_and_prompts: List weights: List num_images_per_prompt: int = 1 num_inference_steps: int = 25 generator: Union = None latents: Optional = None negative_prior_prompt: Optional = None negative_prompt: str = '' guidance_scale: float = 4.0 device = None ) â†’ export const metadata = 'undefined';KandinskyPriorPipelineOutput or tuple
```

å‚æ•°

+   `images_and_prompts` (`List[Union[str, PIL.Image.Image, torch.FloatTensor]]`) â€” ç”¨äºå¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºå’Œå›¾åƒåˆ—è¡¨ã€‚æƒé‡ â€” (`List[float]`): `images_and_prompts` ä¸­æ¯ä¸ªæ¡ä»¶çš„æƒé‡åˆ—è¡¨

+   `num_images_per_prompt` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 25) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚

+   `generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *å¯é€‰*) â€” ä¸€ä¸ªæˆ–å¤šä¸ª [torch ç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html) ä»¥ä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚

+   `latents` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„å…ˆç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºè°ƒæ•´ç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†ä½¿ç”¨æä¾›çš„éšæœº `generator` è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚

+   `negative_prior_prompt` (`str`, *å¯é€‰*) â€” ä¸ç”¨æ¥å¼•å¯¼å…ˆå‰æ‰©æ•£è¿‡ç¨‹çš„æç¤ºã€‚å½“ä¸ä½¿ç”¨å¼•å¯¼æ—¶è¢«å¿½ç•¥ï¼ˆå³å¦‚æœ `guidance_scale` å°äº `1` åˆ™è¢«å¿½ç•¥ï¼‰ã€‚

+   `negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸ç”¨æ¥å¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºã€‚å½“ä¸ä½¿ç”¨å¼•å¯¼æ—¶è¢«å¿½ç•¥ï¼ˆå³å¦‚æœ `guidance_scale` å°äº `1` åˆ™è¢«å¿½ç•¥ï¼‰ã€‚

+   `guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 4.0) â€” å¦‚ [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598) ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`guidance_scale` å®šä¹‰ä¸º [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf) ä¸­æ–¹ç¨‹å¼ 2 çš„ `w`ã€‚é€šè¿‡è®¾ç½® `guidance_scale > 1` æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚

è¿”å›

`KandinskyPriorPipelineOutput` æˆ– `tuple`

ä½¿ç”¨å…ˆå‰ç®¡é“è¿›è¡Œæ’å€¼æ—¶è°ƒç”¨çš„å‡½æ•°ã€‚

ç¤ºä¾‹:

```py
>>> from diffusers import KandinskyPriorPipeline, KandinskyPipeline
>>> from diffusers.utils import load_image
>>> import PIL

>>> import torch
>>> from torchvision import transforms

>>> pipe_prior = KandinskyPriorPipeline.from_pretrained(
...     "kandinsky-community/kandinsky-2-1-prior", torch_dtype=torch.float16
... )
>>> pipe_prior.to("cuda")

>>> img1 = load_image(
...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
...     "/kandinsky/cat.png"
... )

>>> img2 = load_image(
...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
...     "/kandinsky/starry_night.jpeg"
... )

>>> images_texts = ["a cat", img1, img2]
>>> weights = [0.3, 0.3, 0.4]
>>> image_emb, zero_image_emb = pipe_prior.interpolate(images_texts, weights)

>>> pipe = KandinskyPipeline.from_pretrained("kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16)
>>> pipe.to("cuda")

>>> image = pipe(
...     "",
...     image_embeds=image_emb,
...     negative_image_embeds=zero_image_emb,
...     height=768,
...     width=768,
...     num_inference_steps=150,
... ).images[0]

>>> image.save("starry_cat.png")
```

## KandinskyPipeline

### `class diffusers.KandinskyPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky.py#L76)

```py
( text_encoder: MultilingualCLIP tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: Union movq: VQModel )
```

å‚æ•°

+   `text_encoder` (`MultilingualCLIP`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚

+   `tokenizer` (`XLMRobertaTokenizer`) â€” ç±»çš„åˆ†è¯å™¨

+   `scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) â€” ç”¨äºä¸ `unet` ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒæ½œå˜é‡çš„è°ƒåº¦å™¨ã€‚

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) â€” ç”¨äºå»å™ªå›¾åƒåµŒå…¥çš„æ¡ä»¶ U-Net æ¶æ„ã€‚

+   `movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)) â€” ç”¨äºä»æ½œåœ¨ç©ºé—´ç”Ÿæˆå›¾åƒçš„ MoVQ è§£ç å™¨ã€‚

ç”¨äºä½¿ç”¨ Kandinsky è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç®¡é“

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky.py#L231)

```py
( prompt: Union image_embeds: Union negative_image_embeds: Union negative_prompt: Union = None height: int = 512 width: int = 512 num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) â†’ export const metadata = 'undefined';ImagePipelineOutput or tuple
```

å‚æ•°

+   `prompt` (`str` æˆ– `List[str]`) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚

+   `image_embeds` (`torch.FloatTensor` æˆ– `List[torch.FloatTensor]`) â€” ç”¨äºæ–‡æœ¬æç¤ºçš„å‰ªè¾‘å›¾åƒåµŒå…¥ï¼Œå°†ç”¨äºæ¡ä»¶å›¾åƒç”Ÿæˆã€‚

+   `negative_image_embeds` (`torch.FloatTensor` æˆ– `List[torch.FloatTensor]`) â€” è´Ÿæ–‡æœ¬æç¤ºçš„å‰ªè¾‘å›¾åƒåµŒå…¥ï¼Œå°†ç”¨äºæ¡ä»¶å›¾åƒç”Ÿæˆã€‚

+   `negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœä¸ä½¿ç”¨æŒ‡å¯¼ï¼ˆå³å¦‚æœ `guidance_scale` å°äº `1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚

+   `height` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚

+   `width` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚

+   `num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 100) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚

+   `guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 4.0) â€” åœ¨ [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598) ä¸­å®šä¹‰çš„æŒ‡å¯¼æ¯”ä¾‹ã€‚`guidance_scale` å®šä¹‰ä¸º [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf) ä¸­æ–¹ç¨‹å¼ 2 çš„ `w`ã€‚é€šè¿‡è®¾ç½® `guidance_scale > 1` æ¥å¯ç”¨æŒ‡å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„æŒ‡å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚

+   `num_images_per_prompt` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *å¯é€‰*) â€” ä¸€ä¸ªæˆ–å¤šä¸ª [torch ç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ï¼Œç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚

+   `latents` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„å…ˆç”Ÿæˆçš„å™ªå£°æ½œåœ¨ç©ºé—´ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºå¾®è°ƒç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†ä½¿ç”¨æä¾›çš„éšæœº `generator` è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œåœ¨ç©ºé—´å¼ é‡ã€‚

+   `output_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚å¯é€‰æ‹© `"pil"` (`PIL.Image.Image`)ã€`"np"` (`np.array`) æˆ– `"pt"` (`torch.Tensor`)ã€‚

+   `callback` (`Callable`, *å¯é€‰*) â€” åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¯éš” `callback_steps` æ­¥è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback(step: int, timestep: int, latents: torch.FloatTensor)`ã€‚

+   `callback_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” è°ƒç”¨ `callback` å‡½æ•°çš„é¢‘ç‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œåˆ™åœ¨æ¯ä¸€æ­¥éƒ½ä¼šè°ƒç”¨å›è°ƒå‡½æ•°ã€‚

+   `return_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å› [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) æˆ– `tuple`

è°ƒç”¨ç®¡é“è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from diffusers import KandinskyPipeline, KandinskyPriorPipeline
>>> import torch

>>> pipe_prior = KandinskyPriorPipeline.from_pretrained("kandinsky-community/Kandinsky-2-1-prior")
>>> pipe_prior.to("cuda")

>>> prompt = "red cat, 4k photo"
>>> out = pipe_prior(prompt)
>>> image_emb = out.image_embeds
>>> negative_image_emb = out.negative_image_embeds

>>> pipe = KandinskyPipeline.from_pretrained("kandinsky-community/kandinsky-2-1")
>>> pipe.to("cuda")

>>> image = pipe(
...     prompt,
...     image_embeds=image_emb,
...     negative_image_embeds=negative_image_emb,
...     height=768,
...     width=768,
...     num_inference_steps=100,
... ).images

>>> image[0].save("cat.png")
```

## KandinskyCombinedPipeline

### `class diffusers.KandinskyCombinedPipeline`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L113)

```py
( text_encoder: MultilingualCLIP tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: Union movq: VQModel prior_prior: PriorTransformer prior_image_encoder: CLIPVisionModelWithProjection prior_text_encoder: CLIPTextModelWithProjection prior_tokenizer: CLIPTokenizer prior_scheduler: UnCLIPScheduler prior_image_processor: CLIPImageProcessor )
```

å‚æ•°

+   `text_encoder` (`MultilingualCLIP`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚

+   `tokenizer` (`XLMRobertaTokenizer`) â€” ç±»çš„åˆ†è¯å™¨

+   `scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) â€” ç”¨äºä¸ `unet` ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒæ½œå˜é‡çš„è°ƒåº¦å™¨ã€‚

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) â€” ç”¨äºé™å™ªå›¾åƒåµŒå…¥çš„æ¡ä»¶ U-Net æ¶æ„ã€‚

+   `movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)) â€” MoVQ è§£ç å™¨ï¼Œç”¨äºä»æ½œå˜é‡ç”Ÿæˆå›¾åƒã€‚

+   `prior_prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer)) â€” ç”¨äºè¿‘ä¼¼ä»æ–‡æœ¬åµŒå…¥åˆ°å›¾åƒåµŒå…¥çš„ç»å…¸ unCLIP å…ˆéªŒã€‚

+   `prior_image_encoder` (`CLIPVisionModelWithProjection`) â€” å†»ç»“çš„å›¾åƒç¼–ç å™¨ã€‚

+   `prior_text_encoder` (`CLIPTextModelWithProjection`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚

+   `prior_tokenizer` (`CLIPTokenizer`) â€” ç±»çš„åˆ†è¯å™¨ [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)ã€‚

+   `prior_scheduler` (`UnCLIPScheduler`) â€” ç”¨äºä¸ `prior` ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒåµŒå…¥çš„è°ƒåº¦å™¨ã€‚

ä½¿ç”¨ Kandinsky è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç»„åˆç®¡é“

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L214)

```py
( prompt: Union negative_prompt: Union = None num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 height: int = 512 width: int = 512 prior_guidance_scale: float = 4.0 prior_num_inference_steps: int = 25 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) â†’ export const metadata = 'undefined';ImagePipelineOutput or tuple
```

å‚æ•°

+   `prompt` (`str` æˆ– `List[str]`) â€” ç”¨äºå¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–å¤šä¸ªæç¤ºã€‚

+   `negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸ç”¨äºå¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–å¤šä¸ªæç¤ºã€‚å¦‚æœä¸ä½¿ç”¨å¼•å¯¼ï¼ˆå³å¦‚æœ `guidance_scale` å°äº `1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚

+   `num_images_per_prompt` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `num_inference_steps` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 100) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚

+   `height` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚

+   `width` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚

+   `prior_guidance_scale` (`float`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 4.0) â€” åœ¨[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚ `guidance_scale` å®šä¹‰ä¸º[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹å¼ 2 çš„ `w`ã€‚ é€šè¿‡è®¾ç½® `guidance_scale > 1` æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥ç‰ºç‰²å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚

+   `prior_num_inference_steps` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 100) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚

+   `guidance_scale` (`float`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 4.0) â€” åœ¨[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚ `guidance_scale` å®šä¹‰ä¸º[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹å¼ 2 çš„ `w`ã€‚ é€šè¿‡è®¾ç½® `guidance_scale > 1` æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥ç‰ºç‰²å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚

+   `generator`ï¼ˆ`torch.Generator`æˆ–`List[torch.Generator]`ï¼Œ*å¯é€‰*ï¼‰â€” ä¸€ä¸ªæˆ–å¤šä¸ª[torchç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ï¼Œç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚

+   `latents`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” é¢„ç”Ÿæˆçš„å˜ˆæ‚æ½œå˜é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºè°ƒæ•´ç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä½¿ç”¨æä¾›çš„éšæœº`generator`è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚

+   `output_type`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"pil"`ï¼‰â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚å¯é€‰æ‹©ï¼š"pil"ï¼ˆ`PIL.Image.Image`ï¼‰ã€"np"ï¼ˆ`np.array`ï¼‰æˆ–"pt"ï¼ˆ`torch.Tensor`ï¼‰ã€‚

+   `callback`ï¼ˆ`Callable`ï¼Œ*å¯é€‰*ï¼‰â€” åœ¨æ¨æ–­è¿‡ç¨‹ä¸­æ¯`callback_steps`æ­¥è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback(step: int, timestep: int, latents: torch.FloatTensor)`ã€‚

+   `callback_steps`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1ï¼‰â€” è°ƒç”¨`callback`å‡½æ•°çš„é¢‘ç‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œåˆ™åœ¨æ¯ä¸ªæ­¥éª¤æ—¶è°ƒç”¨å›è°ƒã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦è¿”å›[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)æˆ–`tuple`

è°ƒç”¨ç®¡é“è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
from diffusers import AutoPipelineForText2Image
import torch

pipe = AutoPipelineForText2Image.from_pretrained(
    "kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16
)
pipe.enable_model_cpu_offload()

prompt = "A lion in galaxies, spirals, nebulae, stars, smoke, iridescent, intricate detail, octane render, 8k"

image = pipe(prompt=prompt, num_inference_steps=25).images[0]
```

#### `enable_sequential_cpu_offload`

[<æºä»£ç >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L195)

```py
( gpu_id = 0 )
```

ä½¿ç”¨ğŸ¤— Accelerateå°†æ‰€æœ‰æ¨¡å‹ï¼ˆ`unet`ã€`text_encoder`ã€`vae`å’Œ`safety checker`çŠ¶æ€å­—å…¸ï¼‰è½¬ç§»åˆ°CPUï¼Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨ã€‚æ¨¡å‹ç§»è‡³`torch.device('meta')`ï¼Œä»…åœ¨è°ƒç”¨å…¶ç‰¹å®šå­æ¨¡å—çš„`forward`æ–¹æ³•æ—¶æ‰åŠ è½½åˆ°GPUã€‚å¸è½½æ˜¯åŸºäºå­æ¨¡å—çš„ã€‚å†…å­˜èŠ‚çœé«˜äºä½¿ç”¨`enable_model_cpu_offload`ï¼Œä½†æ€§èƒ½è¾ƒä½ã€‚

## KandinskyImg2ImgPipeline

### `class diffusers.KandinskyImg2ImgPipeline`

[<æºä»£ç >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_img2img.py#L98)

```py
( text_encoder: MultilingualCLIP movq: VQModel tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: DDIMScheduler )
```

å‚æ•°

+   `text_encoder`ï¼ˆ`MultilingualCLIP`ï¼‰â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚

+   `tokenizer`ï¼ˆ`XLMRobertaTokenizer`ï¼‰â€” ç±»çš„åˆ†è¯å™¨

+   `scheduler`ï¼ˆ[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ï¼‰â€” ç”¨äºä¸`unet`ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒæ½œå˜é‡çš„è°ƒåº¦å™¨ã€‚

+   `unet`ï¼ˆ[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)ï¼‰â€” ç”¨äºå»å™ªå›¾åƒåµŒå…¥çš„æ¡ä»¶U-Netæ¶æ„ã€‚

+   `movq`ï¼ˆ[VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)ï¼‰â€” MoVQå›¾åƒç¼–ç å™¨å’Œè§£ç å™¨

Kandinskyçš„å›¾åƒåˆ°å›¾åƒç”Ÿæˆç®¡é“

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

#### `__call__`

[<æºä»£ç >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_img2img.py#L293)

```py
( prompt: Union image: Union image_embeds: FloatTensor negative_image_embeds: FloatTensor negative_prompt: Union = None height: int = 512 width: int = 512 num_inference_steps: int = 100 strength: float = 0.3 guidance_scale: float = 7.0 num_images_per_prompt: int = 1 generator: Union = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) â†’ export const metadata = 'undefined';ImagePipelineOutput or tuple
```

å‚æ•°

+   `prompt`ï¼ˆ`str`æˆ–`List[str]`ï¼‰â€” ç”¨äºå¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚

+   `image`ï¼ˆ`torch.FloatTensor`ï¼Œ`PIL.Image.Image`ï¼‰â€” `Image`ï¼Œæˆ–è¡¨ç¤ºå›¾åƒæ‰¹æ¬¡çš„å¼ é‡ï¼Œå°†ç”¨ä½œè¿‡ç¨‹çš„èµ·å§‹ç‚¹ã€‚

+   `image_embeds`ï¼ˆ`torch.FloatTensor`æˆ–`List[torch.FloatTensor]`ï¼‰â€” ç”¨äºæ–‡æœ¬æç¤ºçš„å‰ªè¾‘å›¾åƒåµŒå…¥ï¼Œå°†ç”¨äºæ¡ä»¶å›¾åƒç”Ÿæˆã€‚

+   `negative_image_embeds` (`torch.FloatTensor` æˆ– `List[torch.FloatTensor]`) â€” ç”¨äºè´Ÿæ–‡æœ¬æç¤ºçš„clipå›¾åƒåµŒå…¥ï¼Œå°†ç”¨äºæ¡ä»¶å›¾åƒç”Ÿæˆã€‚

+   `negative_prompt` (`str` æˆ– `List[str]`, *optional*) â€” ä¸ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœä¸ä½¿ç”¨æŒ‡å¯¼ï¼ˆå³å¦‚æœ `guidance_scale` å°äº `1`ï¼‰ï¼Œåˆ™å¿½ç•¥ã€‚

+   `height` (`int`, *optional*, é»˜è®¤ä¸º512) â€” ç”Ÿæˆå›¾åƒçš„é«˜åº¦ï¼ˆåƒç´ ï¼‰ã€‚

+   `width` (`int`, *optional*, é»˜è®¤ä¸º512) â€” ç”Ÿæˆå›¾åƒçš„å®½åº¦ï¼ˆåƒç´ ï¼‰ã€‚

+   `num_inference_steps` (`int`, *optional*, é»˜è®¤ä¸º100) â€” å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚

+   `strength` (`float`, *optional*, é»˜è®¤ä¸º0.3) â€” åœ¨æ¦‚å¿µä¸Šï¼ŒæŒ‡ç¤ºè¦è½¬æ¢å‚è€ƒ`image`çš„ç¨‹åº¦ã€‚å¿…é¡»åœ¨0å’Œ1ä¹‹é—´ã€‚`image`å°†è¢«ç”¨ä½œèµ·ç‚¹ï¼Œæ·»åŠ çš„å™ªéŸ³è¶Šå¤šï¼Œ`strength`è¶Šå¤§ã€‚å»å™ªæ­¥éª¤çš„æ•°é‡å–å†³äºæœ€åˆæ·»åŠ çš„å™ªéŸ³é‡ã€‚å½“ `strength` ä¸º1 æ—¶ï¼Œæ·»åŠ çš„å™ªéŸ³å°†æœ€å¤§åŒ–ï¼Œå»å™ªè¿‡ç¨‹å°†è¿è¡ŒæŒ‡å®šçš„ `num_inference_steps` çš„å…¨éƒ¨è¿­ä»£æ¬¡æ•°ã€‚å› æ­¤ï¼Œå€¼ä¸º1 çš„æƒ…å†µä¸‹ï¼ŒåŸºæœ¬ä¸Šå¿½ç•¥äº† `image`ã€‚

+   `guidance_scale` (`float`, *optional*, é»˜è®¤ä¸º4.0) â€” åœ¨[Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„æŒ‡å¯¼æ¯”ä¾‹ã€‚`guidance_scale` åœ¨[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)çš„æ–¹ç¨‹å¼2ä¸­å®šä¹‰ä¸º`w`ã€‚é€šè¿‡è®¾ç½® `guidance_scale > 1` æ¥å¯ç”¨æŒ‡å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„æŒ‡å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚

+   `num_images_per_prompt` (`int`, *optional*, é»˜è®¤ä¸º1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *optional*) â€” ä¸€ä¸ªæˆ–å¤šä¸ªç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§çš„[torchç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ã€‚

+   `output_type` (`str`, *optional*, é»˜è®¤ä¸º`"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚å¯é€‰æ‹©ï¼š"pil" (`PIL.Image.Image`)ã€"np" (`np.array`) æˆ– "pt" (`torch.Tensor`)ã€‚

+   `callback` (`Callable`, *optional*) â€” åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¯éš” `callback_steps` æ­¥è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback(step: int, timestep: int, latents: torch.FloatTensor)`ã€‚

+   `callback_steps` (`int`, *optional*, é»˜è®¤ä¸º1) â€” è°ƒç”¨ `callback` å‡½æ•°çš„é¢‘ç‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†åœ¨æ¯ä¸€æ­¥è°ƒç”¨å›è°ƒå‡½æ•°ã€‚

+   `return_dict` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

è¿”å›

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) æˆ– `tuple`

è°ƒç”¨ç®¡é“è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚

ç¤ºä¾‹:

```py
>>> from diffusers import KandinskyImg2ImgPipeline, KandinskyPriorPipeline
>>> from diffusers.utils import load_image
>>> import torch

>>> pipe_prior = KandinskyPriorPipeline.from_pretrained(
...     "kandinsky-community/kandinsky-2-1-prior", torch_dtype=torch.float16
... )
>>> pipe_prior.to("cuda")

>>> prompt = "A red cartoon frog, 4k"
>>> image_emb, zero_image_emb = pipe_prior(prompt, return_dict=False)

>>> pipe = KandinskyImg2ImgPipeline.from_pretrained(
...     "kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16
... )
>>> pipe.to("cuda")

>>> init_image = load_image(
...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
...     "/kandinsky/frog.png"
... )

>>> image = pipe(
...     prompt,
...     image=init_image,
...     image_embeds=image_emb,
...     negative_image_embeds=zero_image_emb,
...     height=768,
...     width=768,
...     num_inference_steps=100,
...     strength=0.2,
... ).images

>>> image[0].save("red_frog.png")
```

## KandinskyImg2ImgCombinedPipeline

### `class diffusers.KandinskyImg2ImgCombinedPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L330)

```py
( text_encoder: MultilingualCLIP tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: Union movq: VQModel prior_prior: PriorTransformer prior_image_encoder: CLIPVisionModelWithProjection prior_text_encoder: CLIPTextModelWithProjection prior_tokenizer: CLIPTokenizer prior_scheduler: UnCLIPScheduler prior_image_processor: CLIPImageProcessor )
```

å‚æ•°

+   `text_encoder` (`MultilingualCLIP`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚

+   `tokenizer` (`XLMRobertaTokenizer`) â€” ç±»çš„åˆ†è¯å™¨

+   `scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) â€” ä¸ `unet` ç»“åˆä½¿ç”¨çš„è°ƒåº¦å™¨ï¼Œç”¨äºç”Ÿæˆå›¾åƒæ½œåœ¨ç©ºé—´ã€‚

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) â€” ç”¨äºå»å™ªå›¾åƒåµŒå…¥çš„æ¡ä»¶U-Netæ¶æ„ã€‚

+   `movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)) â€” ç”¨äºä»æ½œåœ¨ç©ºé—´ç”Ÿæˆå›¾åƒçš„MoVQè§£ç å™¨ã€‚

+   `prior_prior`ï¼ˆ[PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer)ï¼‰â€”ç”¨äºè¿‘ä¼¼ä»æ–‡æœ¬åµŒå…¥åˆ°å›¾åƒåµŒå…¥çš„è§„èŒƒåŒ–unCLIPå…ˆéªŒã€‚

+   `prior_image_encoder`ï¼ˆ`CLIPVisionModelWithProjection`ï¼‰â€”å†»ç»“çš„å›¾åƒç¼–ç å™¨ã€‚

+   `prior_text_encoder`ï¼ˆ`CLIPTextModelWithProjection`ï¼‰â€”å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚

+   `prior_tokenizer`ï¼ˆ`CLIPTokenizer`ï¼‰â€”ç±»[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)çš„åˆ†è¯å™¨ã€‚

+   `prior_scheduler`ï¼ˆ`UnCLIPScheduler`ï¼‰â€”ç”¨äºä¸`prior`ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒåµŒå…¥çš„è°ƒåº¦å™¨ã€‚

ä½¿ç”¨Kandinskyè¿›è¡Œå›¾åƒåˆ°å›¾åƒç”Ÿæˆçš„ç»„åˆç®¡é“

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼Œè¿è¡Œåœ¨ç‰¹å®šè®¾å¤‡ä¸Šç­‰ï¼‰ã€‚

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L432)

```py
( prompt: Union image: Union negative_prompt: Union = None num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 strength: float = 0.3 height: int = 512 width: int = 512 prior_guidance_scale: float = 4.0 prior_num_inference_steps: int = 25 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) â†’ export const metadata = 'undefined';ImagePipelineOutput or tuple
```

å‚æ•°

+   `prompt`ï¼ˆ`str`æˆ–`List[str]`ï¼‰â€”ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚

+   `image`ï¼ˆ`torch.FloatTensor`ï¼Œ`PIL.Image.Image`ï¼Œ`np.ndarray`ï¼Œ`List[torch.FloatTensor]`ï¼Œ`List[PIL.Image.Image]`æˆ–`List[np.ndarray]`ï¼‰â€”å°†ç”¨ä½œè¿‡ç¨‹èµ·ç‚¹çš„`Image`æˆ–è¡¨ç¤ºå›¾åƒæ‰¹æ¬¡çš„å¼ é‡ã€‚å¦‚æœç›´æ¥ä¼ é€’æ½œåœ¨å€¼ï¼Œåˆ™ä¸ä¼šå†æ¬¡ç¼–ç ã€‚

+   `negative_prompt`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€”ä¸ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å½“ä¸ä½¿ç”¨å¼•å¯¼æ—¶ï¼ˆå³ï¼Œå¦‚æœ`guidance_scale`å°äº`1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚

+   `num_images_per_prompt`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1ï¼‰â€”æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `num_inference_steps`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º100ï¼‰â€”å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚

+   `height`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º512ï¼‰â€”ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚

+   `width`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º512ï¼‰â€”ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚

+   `strength`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.3ï¼‰â€”åœ¨æ¦‚å¿µä¸Šï¼ŒæŒ‡ç¤ºè¦è½¬æ¢å‚è€ƒ`image`çš„ç¨‹åº¦ã€‚å¿…é¡»ä»‹äº0å’Œ1ä¹‹é—´ã€‚`image`å°†è¢«ç”¨ä½œèµ·ç‚¹ï¼Œæ·»åŠ æ›´å¤šçš„å™ªéŸ³ï¼Œ`strength`è¶Šå¤§ã€‚å»å™ªæ­¥éª¤çš„æ•°é‡å–å†³äºæœ€åˆæ·»åŠ çš„å™ªéŸ³é‡ã€‚å½“`strength`ä¸º1æ—¶ï¼Œæ·»åŠ çš„å™ªéŸ³å°†æ˜¯æœ€å¤§çš„ï¼Œå¹¶ä¸”å»å™ªè¿‡ç¨‹å°†è¿è¡ŒæŒ‡å®šçš„`num_inference_steps`çš„å®Œæ•´è¿­ä»£æ¬¡æ•°ã€‚å› æ­¤ï¼Œå€¼ä¸º1åŸºæœ¬ä¸Šå¿½ç•¥äº†`image`ã€‚

+   `prior_guidance_scale`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º4.0ï¼‰â€”åœ¨[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`guidance_scale`è¢«å®šä¹‰ä¸º[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹å¼2çš„`w`ã€‚é€šè¿‡è®¾ç½®`guidance_scale > 1`æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚

+   `prior_num_inference_steps`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º100ï¼‰â€”å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚

+   `guidance_scale`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º4.0ï¼‰â€”åœ¨[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`guidance_scale`è¢«å®šä¹‰ä¸º[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹å¼2çš„`w`ã€‚é€šè¿‡è®¾ç½®`guidance_scale > 1`æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚

+   `generator`ï¼ˆ`torch.Generator`æˆ–`List[torch.Generator]`ï¼Œ*å¯é€‰*ï¼‰â€” ä¸€ä¸ªæˆ–å¤šä¸ª[torchç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ç”¨äºä½¿ç”Ÿæˆå…·æœ‰ç¡®å®šæ€§ã€‚

+   `latents`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” é¢„ç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºå¾®è°ƒç›¸åŒçš„ç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä½¿ç”¨æä¾›çš„éšæœº`generator`è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚

+   `output_type`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"pil"`ï¼‰â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚é€‰æ‹©`"pil"`ï¼ˆ`PIL.Image.Image`ï¼‰ï¼Œ`"np"`ï¼ˆ`np.array`ï¼‰æˆ–`"pt"`ï¼ˆ`torch.Tensor`ï¼‰ä¹‹é—´ã€‚

+   `callback`ï¼ˆ`Callable`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯`callback_steps`æ­¥åœ¨æ¨æ–­æœŸé—´è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback(step: int, timestep: int, latents: torch.FloatTensor)`ã€‚

+   `callback_steps`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1ï¼‰â€” è°ƒç”¨`callback`å‡½æ•°çš„é¢‘ç‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†åœ¨æ¯ä¸€æ­¥è°ƒç”¨å›è°ƒã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦è¿”å›[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)æˆ–`tuple`

åœ¨è°ƒç”¨ç®¡é“è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
from diffusers import AutoPipelineForImage2Image
import torch
import requests
from io import BytesIO
from PIL import Image
import os

pipe = AutoPipelineForImage2Image.from_pretrained(
    "kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16
)
pipe.enable_model_cpu_offload()

prompt = "A fantasy landscape, Cinematic lighting"
negative_prompt = "low quality, bad quality"

url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"

response = requests.get(url)
image = Image.open(BytesIO(response.content)).convert("RGB")
image.thumbnail((768, 768))

image = pipe(prompt=prompt, image=original_image, num_inference_steps=25).images[0]
```

#### `enable_sequential_cpu_offload`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L412)

```py
( gpu_id = 0 )
```

ä½¿ç”¨åŠ é€Ÿå°†æ‰€æœ‰æ¨¡å‹è½¬ç§»åˆ°CPUï¼Œæ˜¾ç€å‡å°‘å†…å­˜ä½¿ç”¨ã€‚è°ƒç”¨æ—¶ï¼Œunetã€text_encoderã€vaeå’Œsafety checkerçš„çŠ¶æ€å­—å…¸å°†ä¿å­˜åˆ°CPUï¼Œç„¶åç§»åŠ¨åˆ°`torch.device('meta')ï¼Œä»…åœ¨å®ƒä»¬çš„ç‰¹å®šå­æ¨¡å—è°ƒç”¨å…¶`forward`æ–¹æ³•æ—¶æ‰åŠ è½½åˆ°GPUã€‚è¯·æ³¨æ„ï¼Œå¸è½½æ˜¯åŸºäºå­æ¨¡å—çš„ã€‚å†…å­˜èŠ‚çœé«˜äº`enable_model_cpu_offload`ï¼Œä½†æ€§èƒ½è¾ƒä½ã€‚

## KandinskyInpaintPipeline

### `class diffusers.KandinskyInpaintPipeline`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_inpaint.py#L240)

```py
( text_encoder: MultilingualCLIP movq: VQModel tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: DDIMScheduler )
```

å‚æ•°

+   `text_encoder`ï¼ˆ`MultilingualCLIP`ï¼‰â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚

+   `tokenizer`ï¼ˆ`XLMRobertaTokenizer`ï¼‰â€” ç±»çš„åˆ†è¯å™¨

+   `scheduler`ï¼ˆ[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ï¼‰â€” ç”¨äºä¸`unet`ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒæ½œå˜é‡çš„è°ƒåº¦å™¨ã€‚

+   `unet`ï¼ˆ[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)ï¼‰â€” ç”¨äºå»å™ªå›¾åƒåµŒå…¥çš„æ¡ä»¶U-Netæ¶æ„ã€‚

+   `movq`ï¼ˆ[VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)ï¼‰â€” MoVQå›¾åƒç¼–ç å™¨å’Œè§£ç å™¨

ä½¿ç”¨Kandinsky2.1è¿›è¡Œæ–‡æœ¬å¼•å¯¼å›¾åƒä¿®å¤çš„ç®¡é“

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼Œè¿è¡Œåœ¨ç‰¹å®šè®¾å¤‡ä¸Šç­‰ï¼‰ã€‚

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_inpaint.py#L396)

```py
( prompt: Union image: Union mask_image: Union image_embeds: FloatTensor negative_image_embeds: FloatTensor negative_prompt: Union = None height: int = 512 width: int = 512 num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) â†’ export const metadata = 'undefined';ImagePipelineOutput or tuple
```

å‚æ•°

+   `prompt`ï¼ˆ`str`æˆ–`List[str]`ï¼‰â€” ç”¨äºå¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚

+   `image`ï¼ˆ`torch.FloatTensor`ï¼Œ`PIL.Image.Image`æˆ–`np.ndarray`ï¼‰â€” `Image`ï¼Œæˆ–è¡¨ç¤ºå›¾åƒæ‰¹æ¬¡çš„å¼ é‡ï¼Œå°†ç”¨ä½œè¿‡ç¨‹çš„èµ·å§‹ç‚¹ã€‚

+   `mask_image` (`PIL.Image.Image`,`torch.FloatTensor` æˆ– `np.ndarray`) â€” `Image`ï¼Œæˆ–è¡¨ç¤ºå›¾åƒæ‰¹æ¬¡çš„å¼ é‡ï¼Œç”¨äºé®ç½© `image`ã€‚é®ç½©ä¸­çš„ç™½è‰²åƒç´ å°†è¢«é‡æ–°ç»˜åˆ¶ï¼Œè€Œé»‘è‰²åƒç´ å°†è¢«ä¿ç•™ã€‚åªæœ‰å½“ä¼ é€’çš„å›¾åƒæ˜¯ pytorch å¼ é‡æ—¶ï¼Œæ‰èƒ½å°† pytorch å¼ é‡ä½œä¸ºé®ç½©ä¼ é€’ï¼Œå®ƒåº”è¯¥åŒ…å«ä¸€ä¸ªé¢œè‰²é€šé“ï¼ˆLï¼‰è€Œä¸æ˜¯ 3ï¼Œå› æ­¤é¢„æœŸçš„å½¢çŠ¶åº”ä¸º `(B, 1, H, W,)`ã€`(B, H, W)`ã€`(1, H, W)` æˆ– `(H, W)`ã€‚å¦‚æœå›¾åƒæ˜¯ PIL å›¾åƒæˆ– numpy æ•°ç»„ï¼Œåˆ™é®ç½©ä¹Ÿåº”è¯¥æ˜¯ PIL å›¾åƒæˆ– numpy æ•°ç»„ã€‚å¦‚æœæ˜¯ PIL å›¾åƒï¼Œå®ƒå°†åœ¨ä½¿ç”¨ä¹‹å‰è½¬æ¢ä¸ºå•é€šé“ï¼ˆäº®åº¦ï¼‰ã€‚å¦‚æœæ˜¯ numpy æ•°ç»„ï¼Œåˆ™é¢„æœŸå½¢çŠ¶ä¸º `(H, W)`ã€‚

+   `image_embeds` (`torch.FloatTensor` æˆ– `List[torch.FloatTensor]`) â€” ç”¨äºæ–‡æœ¬æç¤ºçš„ clip å›¾åƒåµŒå…¥ï¼Œå°†ç”¨äºæ¡ä»¶å›¾åƒç”Ÿæˆã€‚

+   `negative_image_embeds` (`torch.FloatTensor` æˆ– `List[torch.FloatTensor]`) â€” ç”¨äºè´Ÿé¢æ–‡æœ¬æç¤ºçš„ clip å›¾åƒåµŒå…¥ï¼Œå°†ç”¨äºæ¡ä»¶å›¾åƒç”Ÿæˆã€‚

+   `negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸ç”¨æ¥å¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å½“ä¸ä½¿ç”¨å¼•å¯¼æ—¶è¢«å¿½ç•¥ï¼ˆå³å¦‚æœ `guidance_scale` å°äº `1` åˆ™è¢«å¿½ç•¥ï¼‰ã€‚

+   `height` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚

+   `width` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚

+   `num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 100) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚

+   `guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 4.0) â€” åœ¨[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`guidance_scale` å®šä¹‰ä¸º[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹å¼ 2 çš„ `w`ã€‚é€šè¿‡è®¾ç½® `guidance_scale > 1` æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚

+   `num_images_per_prompt` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *å¯é€‰*) â€” ä¸€ä¸ªæˆ–å¤šä¸ª[torch ç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ï¼Œç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚

+   `latents` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„å…ˆç”Ÿæˆçš„å˜ˆæ‚æ½œå˜é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ä»¥ç”¨æ¥ä½¿ç”¨ä¸åŒæç¤ºè°ƒæ•´ç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†ä½¿ç”¨éšæœº `generator` é‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚

+   `output_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚å¯é€‰æ‹© `"pil"` (`PIL.Image.Image`)ã€`"np"` (`np.array`) æˆ– `"pt"` (`torch.Tensor`)ã€‚

+   `callback` (`Callable`, *å¯é€‰*) â€” åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¯ `callback_steps` æ­¥è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback(step: int, timestep: int, latents: torch.FloatTensor)`ã€‚

+   `callback_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” è°ƒç”¨ `callback` å‡½æ•°çš„é¢‘ç‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†åœ¨æ¯ä¸€æ­¥è°ƒç”¨å›è°ƒå‡½æ•°ã€‚

+   `return_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å› [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) æˆ– `tuple`

è°ƒç”¨ç®¡é“è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from diffusers import KandinskyInpaintPipeline, KandinskyPriorPipeline
>>> from diffusers.utils import load_image
>>> import torch
>>> import numpy as np

>>> pipe_prior = KandinskyPriorPipeline.from_pretrained(
...     "kandinsky-community/kandinsky-2-1-prior", torch_dtype=torch.float16
... )
>>> pipe_prior.to("cuda")

>>> prompt = "a hat"
>>> image_emb, zero_image_emb = pipe_prior(prompt, return_dict=False)

>>> pipe = KandinskyInpaintPipeline.from_pretrained(
...     "kandinsky-community/kandinsky-2-1-inpaint", torch_dtype=torch.float16
... )
>>> pipe.to("cuda")

>>> init_image = load_image(
...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
...     "/kandinsky/cat.png"
... )

>>> mask = np.zeros((768, 768), dtype=np.float32)
>>> mask[:250, 250:-250] = 1

>>> out = pipe(
...     prompt,
...     image=init_image,
...     mask_image=mask,
...     image_embeds=image_emb,
...     negative_image_embeds=zero_image_emb,
...     height=768,
...     width=768,
...     num_inference_steps=50,
... )

>>> image = out.images[0]
>>> image.save("cat_with_hat.png")
```

## KandinskyInpaintCombinedPipeline

### `class diffusers.KandinskyInpaintCombinedPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L570)

```py
( text_encoder: MultilingualCLIP tokenizer: XLMRobertaTokenizer unet: UNet2DConditionModel scheduler: Union movq: VQModel prior_prior: PriorTransformer prior_image_encoder: CLIPVisionModelWithProjection prior_text_encoder: CLIPTextModelWithProjection prior_tokenizer: CLIPTokenizer prior_scheduler: UnCLIPScheduler prior_image_processor: CLIPImageProcessor )
```

å‚æ•°

+   `text_encoder` (`MultilingualCLIP`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚

+   `tokenizer` (`XLMRobertaTokenizer`) â€” ç±»çš„åˆ†è¯å™¨

+   `scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) â€” ç”¨äºä¸`unet`ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒæ½œåœ¨çš„è°ƒåº¦å™¨ã€‚

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) â€” ç”¨äºé™å™ªå›¾åƒåµŒå…¥çš„æ¡ä»¶U-Netæ¶æ„ã€‚

+   `movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)) â€” ç”¨äºä»æ½œåœ¨å›¾åƒç”Ÿæˆå›¾åƒçš„MoVQè§£ç å™¨ã€‚

+   `prior_prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer)) â€” ç”¨äºä»æ–‡æœ¬åµŒå…¥ä¸­è¿‘ä¼¼ç”Ÿæˆå›¾åƒåµŒå…¥çš„æ ‡å‡†unCLIPå…ˆéªŒã€‚

+   `prior_image_encoder` (`CLIPVisionModelWithProjection`) â€” å†»ç»“çš„å›¾åƒç¼–ç å™¨ã€‚

+   `prior_text_encoder` (`CLIPTextModelWithProjection`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚

+   `prior_tokenizer` (`CLIPTokenizer`) â€” ç±»[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)çš„åˆ†è¯å™¨ã€‚

+   `prior_scheduler` (`UnCLIPScheduler`) â€” ç”¨äºä¸`prior`ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒåµŒå…¥çš„è°ƒåº¦å™¨ã€‚

ä½¿ç”¨Kandinskyè¿›è¡Œç”Ÿæˆçš„ç»„åˆç®¡é“

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L672)

```py
( prompt: Union image: Union mask_image: Union negative_prompt: Union = None num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 height: int = 512 width: int = 512 prior_guidance_scale: float = 4.0 prior_num_inference_steps: int = 25 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) â†’ export const metadata = 'undefined';ImagePipelineOutput or tuple
```

å‚æ•°

+   `prompt` (`str` æˆ– `List[str]`) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–å¤šä¸ªæç¤ºã€‚

+   `image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, æˆ– `List[np.ndarray]`) â€” ç”¨ä½œè¿‡ç¨‹èµ·ç‚¹çš„`Image`æˆ–è¡¨ç¤ºå›¾åƒæ‰¹æ¬¡çš„å¼ é‡ã€‚å¦‚æœç›´æ¥ä¼ é€’æ½œåœ¨å›¾åƒï¼Œåˆ™ä¸ä¼šå†æ¬¡ç¼–ç ã€‚

+   `mask_image` (`np.array`) â€” è¡¨ç¤ºå›¾åƒæ‰¹æ¬¡çš„å¼ é‡ï¼Œç”¨äºé®ç½©`image`ã€‚é®ç½©ä¸­çš„ç™½è‰²åƒç´ å°†è¢«é‡æ–°ç»˜åˆ¶ï¼Œè€Œé»‘è‰²åƒç´ å°†è¢«ä¿ç•™ã€‚å¦‚æœ`mask_image`æ˜¯PILå›¾åƒï¼Œåˆ™åœ¨ä½¿ç”¨ä¹‹å‰å°†å…¶è½¬æ¢ä¸ºå•é€šé“ï¼ˆäº®åº¦ï¼‰ã€‚å¦‚æœå®ƒæ˜¯å¼ é‡ï¼Œåˆ™åº”è¯¥åŒ…å«ä¸€ä¸ªé¢œè‰²é€šé“ï¼ˆLï¼‰è€Œä¸æ˜¯3ä¸ªï¼Œå› æ­¤é¢„æœŸå½¢çŠ¶å°†æ˜¯`(B, H, W, 1)`ã€‚

+   `negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–å¤šä¸ªæç¤ºã€‚å¦‚æœä¸ä½¿ç”¨å¼•å¯¼ï¼ˆå³å¦‚æœ`guidance_scale`å°äº`1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚

+   `num_images_per_prompt` (`int`, *å¯é€‰*, é»˜è®¤ä¸º1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º100) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚

+   `height` (`int`, *å¯é€‰*, é»˜è®¤ä¸º512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚

+   `width` (`int`, *å¯é€‰*, é»˜è®¤ä¸º512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚

+   `prior_guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º4.0) â€” å¦‚[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`guidance_scale` åœ¨[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)çš„æ–¹ç¨‹å¼2ä¸­å®šä¹‰ä¸º`w`ã€‚é€šè¿‡è®¾ç½®`guidance_scale > 1`æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚

+   `prior_num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º100) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚

+   `guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 4.0) â€” åœ¨[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`guidance_scale` å®šä¹‰ä¸º[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹å¼ 2 çš„ `w`ã€‚é€šè¿‡è®¾ç½® `guidance_scale > 1` å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚

+   `generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *å¯é€‰*) â€” ä¸€ä¸ªæˆ–å¤šä¸ª [torch ç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html) ä»¥ä½¿ç”Ÿæˆå…·æœ‰ç¡®å®šæ€§ã€‚

+   `latents` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„å…ˆç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ä»¥ç”¨æ¥ä½¿ç”¨ä¸åŒæç¤ºå¾®è°ƒç›¸åŒçš„ç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†ä½¿ç”¨æä¾›çš„éšæœº `generator` è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚

+   `output_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚å¯é€‰æ‹©ï¼š`"pil"` (`PIL.Image.Image`)ã€`"np"` (`np.array`) æˆ– `"pt"` (`torch.Tensor`)ã€‚

+   `callback` (`Callable`, *å¯é€‰*) â€” åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¯éš” `callback_steps` æ­¥è°ƒç”¨ä¸€æ¬¡çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback(step: int, timestep: int, latents: torch.FloatTensor)`ã€‚

+   `callback_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” è°ƒç”¨ `callback` å‡½æ•°çš„é¢‘ç‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œåˆ™åœ¨æ¯ä¸€æ­¥è°ƒç”¨å›è°ƒã€‚

+   `return_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å› [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) è€Œä¸æ˜¯æ™®é€šçš„å…ƒç»„ã€‚

è¿”å›

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) æˆ– `tuple`

è°ƒç”¨ç®¡é“è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image
import torch
import numpy as np

pipe = AutoPipelineForInpainting.from_pretrained(
    "kandinsky-community/kandinsky-2-1-inpaint", torch_dtype=torch.float16
)
pipe.enable_model_cpu_offload()

prompt = "A fantasy landscape, Cinematic lighting"
negative_prompt = "low quality, bad quality"

original_image = load_image(
    "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main" "/kandinsky/cat.png"
)

mask = np.zeros((768, 768), dtype=np.float32)
# Let's mask out an area above the cat's head
mask[:250, 250:-250] = 1

image = pipe(prompt=prompt, image=original_image, mask_image=mask, num_inference_steps=25).images[0]
```

#### `enable_sequential_cpu_offload`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L652)

```py
( gpu_id = 0 )
```

ä½¿ç”¨åŠ é€Ÿå™¨å°†æ‰€æœ‰æ¨¡å‹è½¬ç§»åˆ° CPUï¼Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨ã€‚è°ƒç”¨æ—¶ï¼Œunetã€text_encoderã€vae å’Œå®‰å…¨æ£€æŸ¥å™¨çš„çŠ¶æ€å­—å…¸å°†ä¿å­˜åˆ° CPUï¼Œç„¶åç§»åŠ¨åˆ° `torch.device('meta')`ï¼Œä»…åœ¨å®ƒä»¬çš„ç‰¹å®šå­æ¨¡å—è°ƒç”¨`forward`æ–¹æ³•æ—¶æ‰åŠ è½½åˆ° GPUã€‚è¯·æ³¨æ„ï¼Œå¸è½½æ˜¯åŸºäºå­æ¨¡å—çš„ã€‚ä¸`enable_model_cpu_offload`ç›¸æ¯”ï¼Œå†…å­˜èŠ‚çœæ›´é«˜ï¼Œä½†æ€§èƒ½è¾ƒä½ã€‚
