- en: Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/huggingface_hub/package_reference/inference_client](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/huggingface_hub/v0.20.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/huggingface_hub/v0.20.3/en/_app/immutable/entry/start.81f0ceaa.js">
    <link rel="modulepreload" href="/docs/huggingface_hub/v0.20.3/en/_app/immutable/chunks/scheduler.6062bdaf.js">
    <link rel="modulepreload" href="/docs/huggingface_hub/v0.20.3/en/_app/immutable/chunks/singletons.3c2729d2.js">
    <link rel="modulepreload" href="/docs/huggingface_hub/v0.20.3/en/_app/immutable/chunks/paths.566d2d8a.js">
    <link rel="modulepreload" href="/docs/huggingface_hub/v0.20.3/en/_app/immutable/entry/app.77e31cee.js">
    <link rel="modulepreload" href="/docs/huggingface_hub/v0.20.3/en/_app/immutable/chunks/index.4bca734e.js">
    <link rel="modulepreload" href="/docs/huggingface_hub/v0.20.3/en/_app/immutable/nodes/0.d6dfa8c0.js">
    <link rel="modulepreload" href="/docs/huggingface_hub/v0.20.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/huggingface_hub/v0.20.3/en/_app/immutable/nodes/29.f1159476.js">
    <link rel="modulepreload" href="/docs/huggingface_hub/v0.20.3/en/_app/immutable/chunks/Tip.b9ac1f03.js">
    <link rel="modulepreload" href="/docs/huggingface_hub/v0.20.3/en/_app/immutable/chunks/Docstring.ed07512f.js">
    <link rel="modulepreload" href="/docs/huggingface_hub/v0.20.3/en/_app/immutable/chunks/Heading.723dceba.js">
    <link rel="modulepreload" href="/docs/huggingface_hub/v0.20.3/en/_app/immutable/chunks/CodeBlock.fa01571c.js">
    <link rel="modulepreload" href="/docs/huggingface_hub/v0.20.3/en/_app/immutable/chunks/ExampleCodeBlock.823fb173.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'Inference is the process of using a trained model to make predictions on new
    data. As this process can be compute-intensive, running on a dedicated server
    can be an interesting option. The `huggingface_hub` library provides an easy way
    to call a service that runs inference for hosted models. There are several services
    you can connect to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Inference API](https://huggingface.co/docs/api-inference/index): a service
    that allows you to run accelerated inference on Hugging Face’s infrastructure
    for free. This service is a fast way to get started, test different models, and
    prototype AI products.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Inference Endpoints](https://huggingface.co/inference-endpoints): a product
    to easily deploy models to production. Inference is run by Hugging Face in a dedicated,
    fully managed infrastructure on a cloud provider of your choice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These services can be called with the [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    object. Please refer to [this guide](../guides/inference) for more information
    on how to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Inference Client
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class huggingface_hub.InferenceClient'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L105)'
  prefs: []
  type: TYPE_NORMAL
- en: '( model: Optional = None token: Union = None timeout: Optional = None headers:
    Optional = None cookies: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**model** (`str`, `optional`) — The model to run inference with. Can be a model
    id hosted on the Hugging Face Hub, e.g. `bigcode/starcoder` or a URL to a deployed
    Inference Endpoint. Defaults to None, in which case a recommended model is automatically
    selected for the task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token** (`str`, *optional*) — Hugging Face token. Will default to the locally
    saved token. Pass `token=False` if you don’t want to send your token to the server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**timeout** (`float`, `optional`) — The maximum number of seconds to wait for
    a response from the server. Loading a new model in Inference API can take up to
    several minutes. Defaults to None, meaning it will loop until the server is available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**headers** (`Dict[str, str]`, `optional`) — Additional headers to send to
    the server. By default only the authorization and user-agent headers are sent.
    Values in this dictionary will override the default values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cookies** (`Dict[str, str]`, `optional`) — Additional cookies to send to
    the server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize a new Inference Client.
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    aims to provide a unified experience to perform inference. The client can be used
    seamlessly with either the (free) Inference API or self-hosted Inference Endpoints.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### audio_classification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L264)'
  prefs: []
  type: TYPE_NORMAL
- en: '( audio: Union model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**audio** (Union[str, Path, bytes, BinaryIO]) — The audio content to classify.
    It can be raw audio bytes, a local audio file, or a URL pointing to an audio file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for audio classification.
    Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended model for audio classification
    will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: The classification output containing the predicted label and its confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform audio classification on the provided audio content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#### automatic_speech_recognition'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L302)'
  prefs: []
  type: TYPE_NORMAL
- en: '( audio: Union model: Optional = None ) → str'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**audio** (Union[str, Path, bytes, BinaryIO]) — The content to transcribe.
    It can be raw audio bytes, local audio file, or a URL to an audio file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for ASR. Can be a model ID
    hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. If not
    provided, the default recommended model for ASR will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: str
  prefs: []
  type: TYPE_NORMAL
- en: The transcribed text.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform automatic speech recognition (ASR or audio-to-text) on the given audio
    content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#### conversational'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L338)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str generated_responses: Optional = None past_user_inputs: Optional
    = None parameters: Optional = None model: Optional = None ) → `Dict`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — The last input from the user in the conversation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generated_responses** (`List[str]`, *optional*) — A list of strings corresponding
    to the earlier replies from the model. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_user_inputs** (`List[str]`, *optional*) — A list of strings corresponding
    to the earlier replies from the user. Should be the same length as `generated_responses`.
    Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**parameters** (`Dict[str, Any]`, *optional*) — Additional parameters for the
    conversational task. Defaults to None. For more details about the available parameters,
    please refer to [this page](https://huggingface.co/docs/api-inference/detailed_parameters#conversational-task)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the conversational task.
    Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended conversational model will be
    used. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated conversational output.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate conversational responses based on the given input text (i.e. chat with
    the API).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#### document_question_answering'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L443)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: Union question: str model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`Union[str, Path, bytes, BinaryIO]`) — The input image for the context.
    It can be raw bytes, an image file, or a URL to an online image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**question** (`str`) — Question to be answered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the document question
    answering task. Can be a model ID hosted on the Hugging Face Hub or a URL to a
    deployed Inference Endpoint. If not provided, the default recommended document
    question answering model will be used. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: a list of dictionaries containing the predicted label, associated probability,
    word ids, and page number.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answer questions on document images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#### feature_extraction'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L484)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str model: Optional = None ) → `np.ndarray`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — The text to embed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the conversational task.
    Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended conversational model will be
    used. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`np.ndarray`'
  prefs: []
  type: TYPE_NORMAL
- en: The embedding representing the input text as a float32 numpy array.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate embeddings for a given text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#### fill_mask'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L520)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — a string to be filled from, must contain the [MASK] token
    (check model card for exact name of the mask).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the fill mask task. Can
    be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended fill mask model will be used.
    Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: a list of fill mask output dictionaries containing the predicted label, associated
    probability, token reference, and completed text.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fill in a hole with a missing word (token to be precise).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#### get_model_status'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L1941)'
  prefs: []
  type: TYPE_NORMAL
- en: '( model: Optional = None ) → `ModelStatus`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — Identifier of the model for witch the status
    gonna be checked. If model is not provided, the model associated with this instance
    of [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    will be used. Only InferenceAPI service can be checked so the identifier cannot
    be a URL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`ModelStatus`'
  prefs: []
  type: TYPE_NORMAL
- en: 'An instance of ModelStatus dataclass, containing information, about the state
    of the model: load, state, compute type and framework.'
  prefs: []
  type: TYPE_NORMAL
- en: Get the status of a model hosted on the Inference API.
  prefs: []
  type: TYPE_NORMAL
- en: This endpoint is mostly useful when you already know which model you want to
    use and want to check its availability. If you want to discover already deployed
    models, you should rather use [list_deployed_models()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.list_deployed_models).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#### get_recommended_model'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L1917)'
  prefs: []
  type: TYPE_NORMAL
- en: '( task: str ) → `str`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**task** (`str`) — The Hugging Face task to get which model Hugging Face recommends.
    All available tasks can be found [here](https://huggingface.co/tasks).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`str`'
  prefs: []
  type: TYPE_NORMAL
- en: Name of the model recommended for the input task.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '`ValueError`'
  prefs: []
  type: TYPE_NORMAL
- en: '`ValueError` — If Hugging Face has no recommendation for the input task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the model Hugging Face recommends for the input task.
  prefs: []
  type: TYPE_NORMAL
- en: '#### image_classification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L560)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: Union model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`Union[str, Path, bytes, BinaryIO]`) — The image to classify. It
    can be raw bytes, an image file, or a URL to an online image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for image classification.
    Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended model for image classification
    will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: a list of dictionaries containing the predicted label and associated probability.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform image classification on the given image using the specified model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#### image_segmentation'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L596)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: Union model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`Union[str, Path, bytes, BinaryIO]`) — The image to segment. It
    can be raw bytes, an image file, or a URL to an online image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for image segmentation. Can
    be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended model for image segmentation
    will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: A list of dictionaries containing the segmented masks and associated attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform image segmentation on the given image using the specified model.
  prefs: []
  type: TYPE_NORMAL
- en: You must have `PIL` installed if you want to work with images (`pip install
    Pillow`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#### image_to_image'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L647)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: Union prompt: Optional = None negative_prompt: Optional = None height:
    Optional = None width: Optional = None num_inference_steps: Optional = None guidance_scale:
    Optional = None model: Optional = None **kwargs ) → `Image`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`Union[str, Path, bytes, BinaryIO]`) — The input image for translation.
    It can be raw bytes, an image file, or a URL to an online image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt** (`str`, *optional*) — The text prompt to guide the image generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt** (`str`, *optional*) — A negative prompt to guide the translation
    process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**height** (`int`, *optional*) — The height in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**width** (`int`, *optional*) — The width in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_inference_steps** (`int`, *optional*) — The number of denoising steps.
    More denoising steps usually lead to a higher quality image at the expense of
    slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**guidance_scale** (`float`, *optional*) — Higher guidance scale encourages
    to generate images that are closely linked to the text `prompt`, usually at the
    expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This
    parameter overrides the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Image`'
  prefs: []
  type: TYPE_NORMAL
- en: The translated image.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform image-to-image translation using a specified model.
  prefs: []
  type: TYPE_NORMAL
- en: You must have `PIL` installed if you want to work with images (`pip install
    Pillow`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#### image_to_text'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L731)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: Union model: Optional = None ) → `str`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`Union[str, Path, bytes, BinaryIO]`) — The input image to caption.
    It can be raw bytes, an image file, or a URL to an online image..'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This
    parameter overrides the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`str`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated text.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takes an input image and return text.
  prefs: []
  type: TYPE_NORMAL
- en: Models can have very different outputs depending on your use case (image captioning,
    optical character recognition (OCR), Pix2Struct, etc). Please have a look to the
    model card to learn more about a model’s specificities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#### list_deployed_models'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L767)'
  prefs: []
  type: TYPE_NORMAL
- en: '( frameworks: Union = None ) → `Dict[str, List[str]]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**frameworks** (`Literal["all"]` or `List[str]` or `str`, *optional*) — The
    frameworks to filter on. By default only a subset of the available frameworks
    are tested. If set to “all”, all available frameworks will be tested. It is also
    possible to provide a single framework or a custom set of frameworks to check.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict[str, List[str]]`'
  prefs: []
  type: TYPE_NORMAL
- en: A dictionary mapping task names to a sorted list of model IDs.
  prefs: []
  type: TYPE_NORMAL
- en: List models currently deployed on the Inference API service.
  prefs: []
  type: TYPE_NORMAL
- en: This helper checks deployed models framework by framework. By default, it will
    check the 4 main frameworks that are supported and account for 95% of the hosted
    models. However, if you want a complete list of models you can specify `frameworks="all"`
    as input. Alternatively, if you know before-hand which framework you are interested
    in, you can also restrict to search to this one (e.g. `frameworks="text-generation-inference"`).
    The more frameworks are checked, the more time it will take.
  prefs: []
  type: TYPE_NORMAL
- en: This endpoint is mostly useful for discoverability. If you already know which
    model you want to use and want to check its availability, you can directly use
    [get_model_status()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.get_model_status).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#### object_detection'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L842)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: Union model: Optional = None ) → `List[ObjectDetectionOutput]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`Union[str, Path, bytes, BinaryIO]`) — The image to detect objects
    on. It can be raw bytes, an image file, or a URL to an online image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for object detection. Can
    be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended model for object detection
    (DETR) will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[ObjectDetectionOutput]`'
  prefs: []
  type: TYPE_NORMAL
- en: A list of dictionaries containing the bounding boxes and associated attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError` or `ValueError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ValueError` — If the request output is not a List.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform object detection on the given image using the specified model.
  prefs: []
  type: TYPE_NORMAL
- en: You must have `PIL` installed if you want to work with images (`pip install
    Pillow`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#### post'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L172)'
  prefs: []
  type: TYPE_NORMAL
- en: '( json: Union = None data: Union = None model: Optional = None task: Optional
    = None stream: bool = False ) → bytes'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**json** (`Union[str, Dict, List]`, *optional*) — The JSON data to send in
    the request body, specific to each task. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**data** (`Union[str, Path, bytes, BinaryIO]`, *optional*) — The content to
    send in the request body, specific to each task. It can be raw bytes, a pointer
    to an opened file, a local file path, or a URL to an online resource (image, audio
    file,…). If both `json` and `data` are passed, `data` will take precedence. At
    least `json` or `data` must be provided. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. Will
    override the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**task** (`str`, *optional*) — The task to perform on the inference. All available
    tasks can be found [here](https://huggingface.co/tasks). Used only to default
    to a recommended model if `model` is not provided. At least `model` or `task`
    must be provided. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stream** (`bool`, *optional*) — Whether to iterate over streaming APIs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: bytes
  prefs: []
  type: TYPE_NORMAL
- en: The raw bytes returned by the server.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make a POST request to the inference server.
  prefs: []
  type: TYPE_NORMAL
- en: '#### question_answering'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L890)'
  prefs: []
  type: TYPE_NORMAL
- en: '( question: str context: str model: Optional = None ) → `Dict`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**question** (`str`) — Question to be answered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**context** (`str`) — The context of the question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`) — The model to use for the question answering task. Can be
    a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict`'
  prefs: []
  type: TYPE_NORMAL
- en: a dictionary of question answering output containing the score, start index,
    end index, and answer.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve the answer to a question from a given text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#### sentence_similarity'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L931)'
  prefs: []
  type: TYPE_NORMAL
- en: '( sentence: str other_sentences: List model: Optional = None ) → `List[float]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**sentence** (`str`) — The main sentence to compare to others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**other_sentences** (`List[str]`) — The list of sentences to compare to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the conversational task.
    Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended conversational model will be
    used. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[float]`'
  prefs: []
  type: TYPE_NORMAL
- en: The embedding representing the input text.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the semantic similarity between a sentence and a list of other sentences
    by comparing their embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#### summarization'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L978)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str parameters: Optional = None model: Optional = None ) → `str`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — The input text to summarize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**parameters** (`Dict[str, Any]`, *optional*) — Additional parameters for summarization.
    Check out this [page](https://huggingface.co/docs/api-inference/detailed_parameters#summarization-task)
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This
    parameter overrides the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`str`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated summary text.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate a summary of a given text using a specified model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#### table_question_answering'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L1021)'
  prefs: []
  type: TYPE_NORMAL
- en: '( table: Dict query: str model: Optional = None ) → `Dict`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**table** (`str`) — A table of data represented as a dict of lists where entries
    are headers and the lists are all the values, all lists must have the same size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**query** (`str`) — The query in plain text that you want to ask the table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`) — The model to use for the table-question-answering task.
    Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict`'
  prefs: []
  type: TYPE_NORMAL
- en: a dictionary of table question answering output containing the answer, coordinates,
    cells and the aggregator used.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve the answer to a question from information given in a table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '#### tabular_classification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L1066)'
  prefs: []
  type: TYPE_NORMAL
- en: '( table: Dict model: str ) → `List`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**table** (`Dict[str, Any]`) — Set of attributes to classify.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`) — The model to use for the tabular-classification task. Can
    be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List`'
  prefs: []
  type: TYPE_NORMAL
- en: a list of labels, one per row in the initial table.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying a target category (a group) based on a set of attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#### tabular_regression'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L1110)'
  prefs: []
  type: TYPE_NORMAL
- en: '( table: Dict model: str ) → `List`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**table** (`Dict[str, Any]`) — Set of attributes stored in a table. The attributes
    used to predict the target can be both numerical and categorical.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`) — The model to use for the tabular-regression task. Can be
    a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List`'
  prefs: []
  type: TYPE_NORMAL
- en: a list of predicted numerical target values.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting a numerical target value given a set of attributes/features in a
    table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '#### text_classification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L1149)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — A string to be classified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the text classification
    task. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
    Inference Endpoint. If not provided, the default recommended text classification
    model will be used. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: a list of dictionaries containing the predicted label and associated probability.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform text classification (e.g. sentiment-analysis) on the given text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#### text_generation'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L1277)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt: str details: bool = False stream: bool = False model: Optional =
    None do_sample: bool = False max_new_tokens: int = 20 best_of: Optional = None
    repetition_penalty: Optional = None return_full_text: bool = False seed: Optional
    = None stop_sequences: Optional = None temperature: Optional = None top_k: Optional
    = None top_p: Optional = None truncate: Optional = None typical_p: Optional =
    None watermark: bool = False decoder_input_details: bool = False ) → `Union[str,
    TextGenerationResponse, Iterable[str], Iterable[TextGenerationStreamResponse]]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`str`) — Input text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**details** (`bool`, *optional*) — By default, text_generation returns a string.
    Pass `details=True` if you want a detailed output (tokens, probabilities, seed,
    finish reason, etc.). Only available for models running on with the `text-generation-inference`
    backend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stream** (`bool`, *optional*) — By default, text_generation returns the full
    generated text. Pass `stream=True` if you want a stream of tokens to be returned.
    Only available for models running on with the `text-generation-inference` backend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This
    parameter overrides the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_sample** (`bool`) — Activate logits sampling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_new_tokens** (`int`) — Maximum number of generated tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**best_of** (`int`) — Generate best_of sequences and return the one if the
    highest token logprobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**repetition_penalty** (`float`) — The parameter for repetition penalty. 1.0
    means no penalty. See [this paper](https://arxiv.org/pdf/1909.05858.pdf) for more
    details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_full_text** (`bool`) — Whether to prepend the prompt to the generated
    text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**seed** (`int`) — Random sampling seed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stop_sequences** (`List[str]`) — Stop generating tokens if a member of `stop_sequences`
    is generated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**temperature** (`float`) — The value used to module the logits distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**top_k** (`int`) — The number of highest probability vocabulary tokens to
    keep for top-k-filtering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**top_p** (`float`) — If set to < 1, only the smallest set of most probable
    tokens with probabilities that add up to `top_p` or higher are kept for generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**truncate** (`int`) — Truncate inputs tokens to the given size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**typical_p** (`float`) — Typical Decoding mass See [Typical Decoding for Natural
    Language Generation](https://arxiv.org/abs/2202.00666) for more information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**watermark** (`bool`) — Watermarking with [A Watermark for Large Language
    Models](https://arxiv.org/abs/2301.10226)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decoder_input_details** (`bool`) — Return the decoder input token logprobs
    and ids. You must set `details=True` as well for it to be taken into account.
    Defaults to `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Union[str, TextGenerationResponse, Iterable[str], Iterable[TextGenerationStreamResponse]]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generated text returned from the server:'
  prefs: []
  type: TYPE_NORMAL
- en: if `stream=False` and `details=False`, the generated text is returned as a `str`
    (default)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `stream=True` and `details=False`, the generated text is returned token by
    token as a `Iterable[str]`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `stream=False` and `details=True`, the generated text is returned with more
    details as a [TextGenerationResponse](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.inference._text_generation.TextGenerationResponse)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `details=True` and `stream=True`, the generated text is returned token by
    token as a iterable of [TextGenerationStreamResponse](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.inference._text_generation.TextGenerationStreamResponse)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '`ValidationError` or [InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '`ValidationError` — If input values are not valid. No HTTP call is made to
    the server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a prompt, generate the following text.
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended to have Pydantic installed in order to get inputs validated.
    This is preferable as it allow early failures.
  prefs: []
  type: TYPE_NORMAL
- en: API endpoint is supposed to run with the `text-generation-inference` backend
    (TGI). This backend is the go-to solution to run large language models at scale.
    However, for some smaller models (e.g. “gpt2”) the default `transformers` + `api-inference`
    solution is still in use. Both approaches have very similar APIs, but not exactly
    the same. This method is compatible with both approaches but some parameters are
    only available for `text-generation-inference`. If some parameters are ignored,
    a warning message is triggered but the process continues correctly.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about the TGI project, please refer to [https://github.com/huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '#### text_to_image'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L1544)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt: str negative_prompt: Optional = None height: Optional = None width:
    Optional = None num_inference_steps: Optional = None guidance_scale: Optional
    = None model: Optional = None **kwargs ) → `Image`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`str`) — The prompt to generate an image from.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt** (`str`, *optional*) — An optional negative prompt for the
    image generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**height** (`float`, *optional*) — The height in pixels of the image to generate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**width** (`float`, *optional*) — The width in pixels of the image to generate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_inference_steps** (`int`, *optional*) — The number of denoising steps.
    More denoising steps usually lead to a higher quality image at the expense of
    slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**guidance_scale** (`float`, *optional*) — Higher guidance scale encourages
    to generate images that are closely linked to the text `prompt`, usually at the
    expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This
    parameter overrides the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Image`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated image.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate an image based on a given text using a specified model.
  prefs: []
  type: TYPE_NORMAL
- en: You must have `PIL` installed if you want to work with images (`pip install
    Pillow`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '#### text_to_speech'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L1624)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str model: Optional = None ) → `bytes`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — The text to synthesize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This
    parameter overrides the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`bytes`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated audio.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthesize an audio of a voice pronouncing a given text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '#### token_classification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L1656)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — A string to be classified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the token classification
    task. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
    Inference Endpoint. If not provided, the default recommended token classification
    model will be used. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of token classification outputs containing the entity group, confidence
    score, word, start and end index.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform token classification on the given text. Usually used for sentence parsing,
    either grammatical, or Named Entity Recognition (NER) to understand keywords contained
    within text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '#### translation'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L1703)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str model: Optional = None src_lang: Optional = None tgt_lang: Optional
    = None ) → `str`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — A string to be translated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the translation task.
    Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended translation model will be used.
    Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**src_lang** (`str`, *optional*) — Source language of the translation task,
    i.e. input language. Cannot be passed without `tgt_lang`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tgt_lang** (`str`, *optional*) — Target language of the translation task,
    i.e. output language. Cannot be passed without `src_lang`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`str`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated translated text.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError` or `ValueError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ValueError` — If only one of the `src_lang` and `tgt_lang` arguments are provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert text from one language to another.
  prefs: []
  type: TYPE_NORMAL
- en: Check out [https://huggingface.co/tasks/translation](https://huggingface.co/tasks/translation)
    for more information on how to choose the best model for your specific use case.
    Source and target languages usually depend on the model. However, it is possible
    to specify source and target languages for certain models. If you are working
    with one of these models, you can use `src_lang` and `tgt_lang` arguments to pass
    the relevant information. You can find this information in the model card.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Specifying languages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '#### visual_question_answering'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L399)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: Union question: str model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`Union[str, Path, bytes, BinaryIO]`) — The input image for the context.
    It can be raw bytes, an image file, or a URL to an online image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**question** (`str`) — Question to be answered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the visual question answering
    task. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
    Inference Endpoint. If not provided, the default recommended visual question answering
    model will be used. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: a list of dictionaries containing the predicted label and associated probability.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '`InferenceTimeoutError` or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '`InferenceTimeoutError` — If the model is unavailable or the request times
    out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answering open-ended questions based on an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '#### zero_shot_classification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L1768)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str labels: List multi_label: bool = False model: Optional = None )
    → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — The input text to classify.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`List[str]`) — List of string possible labels. There must be at
    least 2 labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**multi_label** (`bool`) — Boolean that is set to True if classes can overlap.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This
    parameter overrides the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of classification outputs containing the predicted labels and their confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide as input a text and a set of candidate labels to classify the input
    text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '#### zero_shot_image_classification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_client.py#L1840)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: Union labels: List model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`Union[str, Path, bytes, BinaryIO]`) — The input image to caption.
    It can be raw bytes, an image file, or a URL to an online image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`List[str]`) — List of string possible labels. There must be at
    least 2 labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This
    parameter overrides the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of classification outputs containing the predicted labels and their confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `HTTPError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPError` — If the request fails with an HTTP error status code other than
    HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide input image and text labels to predict text labels for the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Async Inference Client
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An async version of the client is also provided, based on `asyncio` and `aiohttp`.
    To use it, you can either install `aiohttp` directly or use the `[inference]`
    extra:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '### class huggingface_hub.AsyncInferenceClient'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L89)'
  prefs: []
  type: TYPE_NORMAL
- en: '( model: Optional = None token: Union = None timeout: Optional = None headers:
    Optional = None cookies: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**model** (`str`, `optional`) — The model to run inference with. Can be a model
    id hosted on the Hugging Face Hub, e.g. `bigcode/starcoder` or a URL to a deployed
    Inference Endpoint. Defaults to None, in which case a recommended model is automatically
    selected for the task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token** (`str`, *optional*) — Hugging Face token. Will default to the locally
    saved token. Pass `token=False` if you don’t want to send your token to the server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**timeout** (`float`, `optional`) — The maximum number of seconds to wait for
    a response from the server. Loading a new model in Inference API can take up to
    several minutes. Defaults to None, meaning it will loop until the server is available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**headers** (`Dict[str, str]`, `optional`) — Additional headers to send to
    the server. By default only the authorization and user-agent headers are sent.
    Values in this dictionary will override the default values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cookies** (`Dict[str, str]`, `optional`) — Additional cookies to send to
    the server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize a new Inference Client.
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    aims to provide a unified experience to perform inference. The client can be used
    seamlessly with either the (free) Inference API or self-hosted Inference Endpoints.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### audio_classification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L259)'
  prefs: []
  type: TYPE_NORMAL
- en: '( audio: Union model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**audio** (Union[str, Path, bytes, BinaryIO]) — The audio content to classify.
    It can be raw audio bytes, a local audio file, or a URL pointing to an audio file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for audio classification.
    Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended model for audio classification
    will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: The classification output containing the predicted label and its confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform audio classification on the provided audio content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '#### automatic_speech_recognition'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L298)'
  prefs: []
  type: TYPE_NORMAL
- en: '( audio: Union model: Optional = None ) → str'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**audio** (Union[str, Path, bytes, BinaryIO]) — The content to transcribe.
    It can be raw audio bytes, local audio file, or a URL to an audio file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for ASR. Can be a model ID
    hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. If not
    provided, the default recommended model for ASR will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: str
  prefs: []
  type: TYPE_NORMAL
- en: The transcribed text.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform automatic speech recognition (ASR or audio-to-text) on the given audio
    content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '#### conversational'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L335)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str generated_responses: Optional = None past_user_inputs: Optional
    = None parameters: Optional = None model: Optional = None ) → `Dict`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — The last input from the user in the conversation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generated_responses** (`List[str]`, *optional*) — A list of strings corresponding
    to the earlier replies from the model. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_user_inputs** (`List[str]`, *optional*) — A list of strings corresponding
    to the earlier replies from the user. Should be the same length as `generated_responses`.
    Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**parameters** (`Dict[str, Any]`, *optional*) — Additional parameters for the
    conversational task. Defaults to None. For more details about the available parameters,
    please refer to [this page](https://huggingface.co/docs/api-inference/detailed_parameters#conversational-task)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the conversational task.
    Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended conversational model will be
    used. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated conversational output.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate conversational responses based on the given input text (i.e. chat with
    the API).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '#### document_question_answering'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L442)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: Union question: str model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`Union[str, Path, bytes, BinaryIO]`) — The input image for the context.
    It can be raw bytes, an image file, or a URL to an online image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**question** (`str`) — Question to be answered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the document question
    answering task. Can be a model ID hosted on the Hugging Face Hub or a URL to a
    deployed Inference Endpoint. If not provided, the default recommended document
    question answering model will be used. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: a list of dictionaries containing the predicted label, associated probability,
    word ids, and page number.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answer questions on document images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '#### feature_extraction'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L484)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str model: Optional = None ) → `np.ndarray`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — The text to embed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the conversational task.
    Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended conversational model will be
    used. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`np.ndarray`'
  prefs: []
  type: TYPE_NORMAL
- en: The embedding representing the input text as a float32 numpy array.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate embeddings for a given text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '#### fill_mask'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L521)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — a string to be filled from, must contain the [MASK] token
    (check model card for exact name of the mask).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the fill mask task. Can
    be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended fill mask model will be used.
    Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: a list of fill mask output dictionaries containing the predicted label, associated
    probability, token reference, and completed text.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fill in a hole with a missing word (token to be precise).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '#### get_model_status'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L1969)'
  prefs: []
  type: TYPE_NORMAL
- en: '( model: Optional = None ) → `ModelStatus`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — Identifier of the model for witch the status
    gonna be checked. If model is not provided, the model associated with this instance
    of [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    will be used. Only InferenceAPI service can be checked so the identifier cannot
    be a URL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`ModelStatus`'
  prefs: []
  type: TYPE_NORMAL
- en: 'An instance of ModelStatus dataclass, containing information, about the state
    of the model: load, state, compute type and framework.'
  prefs: []
  type: TYPE_NORMAL
- en: Get the status of a model hosted on the Inference API.
  prefs: []
  type: TYPE_NORMAL
- en: This endpoint is mostly useful when you already know which model you want to
    use and want to check its availability. If you want to discover already deployed
    models, you should rather use [list_deployed_models()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.list_deployed_models).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '#### get_recommended_model'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L1945)'
  prefs: []
  type: TYPE_NORMAL
- en: '( task: str ) → `str`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**task** (`str`) — The Hugging Face task to get which model Hugging Face recommends.
    All available tasks can be found [here](https://huggingface.co/tasks).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`str`'
  prefs: []
  type: TYPE_NORMAL
- en: Name of the model recommended for the input task.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '`ValueError`'
  prefs: []
  type: TYPE_NORMAL
- en: '`ValueError` — If Hugging Face has no recommendation for the input task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the model Hugging Face recommends for the input task.
  prefs: []
  type: TYPE_NORMAL
- en: '#### image_classification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L562)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: Union model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`Union[str, Path, bytes, BinaryIO]`) — The image to classify. It
    can be raw bytes, an image file, or a URL to an online image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for image classification.
    Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended model for image classification
    will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: a list of dictionaries containing the predicted label and associated probability.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform image classification on the given image using the specified model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '#### image_segmentation'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L599)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: Union model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`Union[str, Path, bytes, BinaryIO]`) — The image to segment. It
    can be raw bytes, an image file, or a URL to an online image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for image segmentation. Can
    be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended model for image segmentation
    will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: A list of dictionaries containing the segmented masks and associated attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform image segmentation on the given image using the specified model.
  prefs: []
  type: TYPE_NORMAL
- en: You must have `PIL` installed if you want to work with images (`pip install
    Pillow`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '#### image_to_image'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L651)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: Union prompt: Optional = None negative_prompt: Optional = None height:
    Optional = None width: Optional = None num_inference_steps: Optional = None guidance_scale:
    Optional = None model: Optional = None **kwargs ) → `Image`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`Union[str, Path, bytes, BinaryIO]`) — The input image for translation.
    It can be raw bytes, an image file, or a URL to an online image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt** (`str`, *optional*) — The text prompt to guide the image generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt** (`str`, *optional*) — A negative prompt to guide the translation
    process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**height** (`int`, *optional*) — The height in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**width** (`int`, *optional*) — The width in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_inference_steps** (`int`, *optional*) — The number of denoising steps.
    More denoising steps usually lead to a higher quality image at the expense of
    slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**guidance_scale** (`float`, *optional*) — Higher guidance scale encourages
    to generate images that are closely linked to the text `prompt`, usually at the
    expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This
    parameter overrides the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Image`'
  prefs: []
  type: TYPE_NORMAL
- en: The translated image.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform image-to-image translation using a specified model.
  prefs: []
  type: TYPE_NORMAL
- en: You must have `PIL` installed if you want to work with images (`pip install
    Pillow`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '#### image_to_text'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L736)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: Union model: Optional = None ) → `str`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`Union[str, Path, bytes, BinaryIO]`) — The input image to caption.
    It can be raw bytes, an image file, or a URL to an online image..'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This
    parameter overrides the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`str`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated text.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takes an input image and return text.
  prefs: []
  type: TYPE_NORMAL
- en: Models can have very different outputs depending on your use case (image captioning,
    optical character recognition (OCR), Pix2Struct, etc). Please have a look to the
    model card to learn more about a model’s specificities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '#### list_deployed_models'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L773)'
  prefs: []
  type: TYPE_NORMAL
- en: '( frameworks: Union = None ) → `Dict[str, List[str]]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**frameworks** (`Literal["all"]` or `List[str]` or `str`, *optional*) — The
    frameworks to filter on. By default only a subset of the available frameworks
    are tested. If set to “all”, all available frameworks will be tested. It is also
    possible to provide a single framework or a custom set of frameworks to check.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict[str, List[str]]`'
  prefs: []
  type: TYPE_NORMAL
- en: A dictionary mapping task names to a sorted list of model IDs.
  prefs: []
  type: TYPE_NORMAL
- en: List models currently deployed on the Inference API service.
  prefs: []
  type: TYPE_NORMAL
- en: This helper checks deployed models framework by framework. By default, it will
    check the 4 main frameworks that are supported and account for 95% of the hosted
    models. However, if you want a complete list of models you can specify `frameworks="all"`
    as input. Alternatively, if you know before-hand which framework you are interested
    in, you can also restrict to search to this one (e.g. `frameworks="text-generation-inference"`).
    The more frameworks are checked, the more time it will take.
  prefs: []
  type: TYPE_NORMAL
- en: This endpoint is mostly useful for discoverability. If you already know which
    model you want to use and want to check its availability, you can directly use
    [get_model_status()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.get_model_status).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '#### object_detection'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L854)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: Union model: Optional = None ) → `List[ObjectDetectionOutput]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`Union[str, Path, bytes, BinaryIO]`) — The image to detect objects
    on. It can be raw bytes, an image file, or a URL to an online image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for object detection. Can
    be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended model for object detection
    (DETR) will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[ObjectDetectionOutput]`'
  prefs: []
  type: TYPE_NORMAL
- en: A list of dictionaries containing the bounding boxes and associated attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError` or `ValueError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ValueError` — If the request output is not a List.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform object detection on the given image using the specified model.
  prefs: []
  type: TYPE_NORMAL
- en: You must have `PIL` installed if you want to work with images (`pip install
    Pillow`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '#### post'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L156)'
  prefs: []
  type: TYPE_NORMAL
- en: '( json: Union = None data: Union = None model: Optional = None task: Optional
    = None stream: bool = False ) → bytes'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**json** (`Union[str, Dict, List]`, *optional*) — The JSON data to send in
    the request body, specific to each task. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**data** (`Union[str, Path, bytes, BinaryIO]`, *optional*) — The content to
    send in the request body, specific to each task. It can be raw bytes, a pointer
    to an opened file, a local file path, or a URL to an online resource (image, audio
    file,…). If both `json` and `data` are passed, `data` will take precedence. At
    least `json` or `data` must be provided. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. Will
    override the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**task** (`str`, *optional*) — The task to perform on the inference. All available
    tasks can be found [here](https://huggingface.co/tasks). Used only to default
    to a recommended model if `model` is not provided. At least `model` or `task`
    must be provided. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stream** (`bool`, *optional*) — Whether to iterate over streaming APIs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: bytes
  prefs: []
  type: TYPE_NORMAL
- en: The raw bytes returned by the server.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make a POST request to the inference server.
  prefs: []
  type: TYPE_NORMAL
- en: '#### question_answering'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L903)'
  prefs: []
  type: TYPE_NORMAL
- en: '( question: str context: str model: Optional = None ) → `Dict`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**question** (`str`) — Question to be answered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**context** (`str`) — The context of the question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`) — The model to use for the question answering task. Can be
    a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict`'
  prefs: []
  type: TYPE_NORMAL
- en: a dictionary of question answering output containing the score, start index,
    end index, and answer.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve the answer to a question from a given text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '#### sentence_similarity'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L945)'
  prefs: []
  type: TYPE_NORMAL
- en: '( sentence: str other_sentences: List model: Optional = None ) → `List[float]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**sentence** (`str`) — The main sentence to compare to others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**other_sentences** (`List[str]`) — The list of sentences to compare to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the conversational task.
    Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended conversational model will be
    used. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[float]`'
  prefs: []
  type: TYPE_NORMAL
- en: The embedding representing the input text.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the semantic similarity between a sentence and a list of other sentences
    by comparing their embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '#### summarization'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L993)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str parameters: Optional = None model: Optional = None ) → `str`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — The input text to summarize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**parameters** (`Dict[str, Any]`, *optional*) — Additional parameters for summarization.
    Check out this [page](https://huggingface.co/docs/api-inference/detailed_parameters#summarization-task)
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This
    parameter overrides the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`str`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated summary text.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate a summary of a given text using a specified model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '#### table_question_answering'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L1037)'
  prefs: []
  type: TYPE_NORMAL
- en: '( table: Dict query: str model: Optional = None ) → `Dict`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**table** (`str`) — A table of data represented as a dict of lists where entries
    are headers and the lists are all the values, all lists must have the same size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**query** (`str`) — The query in plain text that you want to ask the table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`) — The model to use for the table-question-answering task.
    Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict`'
  prefs: []
  type: TYPE_NORMAL
- en: a dictionary of table question answering output containing the answer, coordinates,
    cells and the aggregator used.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve the answer to a question from information given in a table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '#### tabular_classification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L1083)'
  prefs: []
  type: TYPE_NORMAL
- en: '( table: Dict model: str ) → `List`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**table** (`Dict[str, Any]`) — Set of attributes to classify.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`) — The model to use for the tabular-classification task. Can
    be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List`'
  prefs: []
  type: TYPE_NORMAL
- en: a list of labels, one per row in the initial table.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying a target category (a group) based on a set of attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '#### tabular_regression'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L1128)'
  prefs: []
  type: TYPE_NORMAL
- en: '( table: Dict model: str ) → `List`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**table** (`Dict[str, Any]`) — Set of attributes stored in a table. The attributes
    used to predict the target can be both numerical and categorical.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`) — The model to use for the tabular-regression task. Can be
    a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List`'
  prefs: []
  type: TYPE_NORMAL
- en: a list of predicted numerical target values.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting a numerical target value given a set of attributes/features in a
    table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '#### text_classification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L1168)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — A string to be classified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the text classification
    task. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
    Inference Endpoint. If not provided, the default recommended text classification
    model will be used. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: a list of dictionaries containing the predicted label and associated probability.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform text classification (e.g. sentiment-analysis) on the given text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '#### text_generation'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L1297)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt: str details: bool = False stream: bool = False model: Optional =
    None do_sample: bool = False max_new_tokens: int = 20 best_of: Optional = None
    repetition_penalty: Optional = None return_full_text: bool = False seed: Optional
    = None stop_sequences: Optional = None temperature: Optional = None top_k: Optional
    = None top_p: Optional = None truncate: Optional = None typical_p: Optional =
    None watermark: bool = False decoder_input_details: bool = False ) → `Union[str,
    TextGenerationResponse, Iterable[str], Iterable[TextGenerationStreamResponse]]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`str`) — Input text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**details** (`bool`, *optional*) — By default, text_generation returns a string.
    Pass `details=True` if you want a detailed output (tokens, probabilities, seed,
    finish reason, etc.). Only available for models running on with the `text-generation-inference`
    backend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stream** (`bool`, *optional*) — By default, text_generation returns the full
    generated text. Pass `stream=True` if you want a stream of tokens to be returned.
    Only available for models running on with the `text-generation-inference` backend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This
    parameter overrides the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_sample** (`bool`) — Activate logits sampling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_new_tokens** (`int`) — Maximum number of generated tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**best_of** (`int`) — Generate best_of sequences and return the one if the
    highest token logprobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**repetition_penalty** (`float`) — The parameter for repetition penalty. 1.0
    means no penalty. See [this paper](https://arxiv.org/pdf/1909.05858.pdf) for more
    details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_full_text** (`bool`) — Whether to prepend the prompt to the generated
    text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**seed** (`int`) — Random sampling seed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stop_sequences** (`List[str]`) — Stop generating tokens if a member of `stop_sequences`
    is generated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**temperature** (`float`) — The value used to module the logits distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**top_k** (`int`) — The number of highest probability vocabulary tokens to
    keep for top-k-filtering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**top_p** (`float`) — If set to < 1, only the smallest set of most probable
    tokens with probabilities that add up to `top_p` or higher are kept for generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**truncate** (`int`) — Truncate inputs tokens to the given size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**typical_p** (`float`) — Typical Decoding mass See [Typical Decoding for Natural
    Language Generation](https://arxiv.org/abs/2202.00666) for more information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**watermark** (`bool`) — Watermarking with [A Watermark for Large Language
    Models](https://arxiv.org/abs/2301.10226)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decoder_input_details** (`bool`) — Return the decoder input token logprobs
    and ids. You must set `details=True` as well for it to be taken into account.
    Defaults to `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Union[str, TextGenerationResponse, Iterable[str], Iterable[TextGenerationStreamResponse]]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generated text returned from the server:'
  prefs: []
  type: TYPE_NORMAL
- en: if `stream=False` and `details=False`, the generated text is returned as a `str`
    (default)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `stream=True` and `details=False`, the generated text is returned token by
    token as a `Iterable[str]`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `stream=False` and `details=True`, the generated text is returned with more
    details as a [TextGenerationResponse](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.inference._text_generation.TextGenerationResponse)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `details=True` and `stream=True`, the generated text is returned token by
    token as a iterable of [TextGenerationStreamResponse](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.inference._text_generation.TextGenerationStreamResponse)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '`ValidationError` or [InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '`ValidationError` — If input values are not valid. No HTTP call is made to
    the server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a prompt, generate the following text.
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended to have Pydantic installed in order to get inputs validated.
    This is preferable as it allow early failures.
  prefs: []
  type: TYPE_NORMAL
- en: API endpoint is supposed to run with the `text-generation-inference` backend
    (TGI). This backend is the go-to solution to run large language models at scale.
    However, for some smaller models (e.g. “gpt2”) the default `transformers` + `api-inference`
    solution is still in use. Both approaches have very similar APIs, but not exactly
    the same. This method is compatible with both approaches but some parameters are
    only available for `text-generation-inference`. If some parameters are ignored,
    a warning message is triggered but the process continues correctly.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about the TGI project, please refer to [https://github.com/huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '#### text_to_image'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L1566)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt: str negative_prompt: Optional = None height: Optional = None width:
    Optional = None num_inference_steps: Optional = None guidance_scale: Optional
    = None model: Optional = None **kwargs ) → `Image`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`str`) — The prompt to generate an image from.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt** (`str`, *optional*) — An optional negative prompt for the
    image generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**height** (`float`, *optional*) — The height in pixels of the image to generate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**width** (`float`, *optional*) — The width in pixels of the image to generate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_inference_steps** (`int`, *optional*) — The number of denoising steps.
    More denoising steps usually lead to a higher quality image at the expense of
    slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**guidance_scale** (`float`, *optional*) — Higher guidance scale encourages
    to generate images that are closely linked to the text `prompt`, usually at the
    expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This
    parameter overrides the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Image`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated image.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate an image based on a given text using a specified model.
  prefs: []
  type: TYPE_NORMAL
- en: You must have `PIL` installed if you want to work with images (`pip install
    Pillow`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '#### text_to_speech'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L1647)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str model: Optional = None ) → `bytes`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — The text to synthesize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This
    parameter overrides the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`bytes`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated audio.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthesize an audio of a voice pronouncing a given text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '#### token_classification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L1680)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — A string to be classified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the token classification
    task. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
    Inference Endpoint. If not provided, the default recommended token classification
    model will be used. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of token classification outputs containing the entity group, confidence
    score, word, start and end index.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform token classification on the given text. Usually used for sentence parsing,
    either grammatical, or Named Entity Recognition (NER) to understand keywords contained
    within text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '#### translation'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L1728)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str model: Optional = None src_lang: Optional = None tgt_lang: Optional
    = None ) → `str`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — A string to be translated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the translation task.
    Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference
    Endpoint. If not provided, the default recommended translation model will be used.
    Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**src_lang** (`str`, *optional*) — Source language of the translation task,
    i.e. input language. Cannot be passed without `tgt_lang`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tgt_lang** (`str`, *optional*) — Target language of the translation task,
    i.e. output language. Cannot be passed without `src_lang`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`str`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated translated text.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError` or `ValueError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ValueError` — If only one of the `src_lang` and `tgt_lang` arguments are provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert text from one language to another.
  prefs: []
  type: TYPE_NORMAL
- en: Check out [https://huggingface.co/tasks/translation](https://huggingface.co/tasks/translation)
    for more information on how to choose the best model for your specific use case.
    Source and target languages usually depend on the model. However, it is possible
    to specify source and target languages for certain models. If you are working
    with one of these models, you can use `src_lang` and `tgt_lang` arguments to pass
    the relevant information. You can find this information in the model card.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Specifying languages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '#### visual_question_answering'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L397)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: Union question: str model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`Union[str, Path, bytes, BinaryIO]`) — The input image for the context.
    It can be raw bytes, an image file, or a URL to an online image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**question** (`str`) — Question to be answered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for the visual question answering
    task. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed
    Inference Endpoint. If not provided, the default recommended visual question answering
    model will be used. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: a list of dictionaries containing the predicted label and associated probability.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '`InferenceTimeoutError` or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '`InferenceTimeoutError` — If the model is unavailable or the request times
    out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answering open-ended questions based on an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '#### zero_shot_classification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L1794)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: str labels: List multi_label: bool = False model: Optional = None )
    → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`) — The input text to classify.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`List[str]`) — List of string possible labels. There must be at
    least 2 labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**multi_label** (`bool`) — Boolean that is set to True if classes can overlap.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This
    parameter overrides the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of classification outputs containing the predicted labels and their confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide as input a text and a set of candidate labels to classify the input
    text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '#### zero_shot_image_classification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_generated/_async_client.py#L1867)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: Union labels: List model: Optional = None ) → `List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`Union[str, Path, bytes, BinaryIO]`) — The input image to caption.
    It can be raw bytes, an image file, or a URL to an online image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`List[str]`) — List of string possible labels. There must be at
    least 2 labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model** (`str`, *optional*) — The model to use for inference. Can be a model
    ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This
    parameter overrides the model defined at the instance level. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of classification outputs containing the predicted labels and their confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    or `aiohttp.ClientResponseError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    — If the model is unavailable or the request times out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aiohttp.ClientResponseError` — If the request fails with an HTTP error status
    code other than HTTP 503.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide input image and text labels to predict text labels for the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: InferenceTimeoutError
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class huggingface_hub.InferenceTimeoutError'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_common.py#L100)'
  prefs: []
  type: TYPE_NORMAL
- en: ( *args **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Error raised when a model is unavailable or the request times out.
  prefs: []
  type: TYPE_NORMAL
- en: Return types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For most tasks, the return value has a built-in type (string, list, image…).
    Here is a list for the more complex types.
  prefs: []
  type: TYPE_NORMAL
- en: ClassificationOutput
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### class huggingface_hub.inference._types.ClassificationOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_types.py#L22)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**label** (`str`) — The label predicted by the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**score** (`float`) — The score of the label predicted by the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dictionary containing the output of a [audio_classification()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.audio_classification)
    and [image_classification()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.image_classification)
    task.
  prefs: []
  type: TYPE_NORMAL
- en: ConversationalOutputConversation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### class huggingface_hub.inference._types.ConversationalOutputConversation'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_types.py#L36)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**generated_responses** (`List[str]`) — A list of the responses from the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_user_inputs** (`List[str]`) — A list of the inputs from the user. Must
    be the same length as `generated_responses`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dictionary containing the “conversation” part of a [conversational()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.conversational)
    task.
  prefs: []
  type: TYPE_NORMAL
- en: ConversationalOutput
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### class huggingface_hub.inference._types.ConversationalOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_types.py#L50)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**generated_text** (`str`) — The last response from the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**conversation** (`ConversationalOutputConversation`) — The past conversation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**warnings** (`List[str]`) — A list of warnings associated with the process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dictionary containing the output of a [conversational()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.conversational)
    task.
  prefs: []
  type: TYPE_NORMAL
- en: ImageSegmentationOutput
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### class huggingface_hub.inference._types.ImageSegmentationOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_types.py#L87)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**label** (`str`) — The label corresponding to the mask.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mask** (`Image`) — An Image object representing the mask predicted by the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**score** (`float`) — The score associated with the label for this mask.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dictionary containing information about a [image_segmentation()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.image_segmentation)
    task. In practice, image segmentation returns a list of `ImageSegmentationOutput`
    with 1 item per mask.
  prefs: []
  type: TYPE_NORMAL
- en: ModelStatus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### class huggingface_hub.inference._common.ModelStatus'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_common.py#L71)'
  prefs: []
  type: TYPE_NORMAL
- en: '( loaded: bool state: str compute_type: str framework: str )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**loaded** (`bool`) — If the model is currently loaded into Hugging Face’s
    InferenceAPI. Models are loaded on-demand, leading to the user’s first request
    taking longer. If a model is loaded, you can be assured that it is in a healthy
    state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**state** (`str`) — The current state of the model. This can be ‘Loaded’, ‘Loadable’,
    ‘TooBig’. If a model’s state is ‘Loadable’, it’s not too big and has a supported
    backend. Loadable models are automatically loaded when the user first requests
    inference on the endpoint. This means it is transparent for the user to load a
    model, except that the first call takes longer to complete.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**compute_type** (`str`) — The type of compute resource the model is using
    or will use, such as ‘gpu’ or ‘cpu’.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**framework** (`str`) — The name of the framework that the model was built
    with, such as ‘transformers’ or ‘text-generation-inference’.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This Dataclass represents the the model status in the Hugging Face Inference
    API.
  prefs: []
  type: TYPE_NORMAL
- en: TokenClassificationOutput
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### class huggingface_hub.inference._types.TokenClassificationOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_types.py#L163)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**entity_group** (`str`) — The type for the entity being recognized (model
    specific).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**score** (`float`) — The score of the label predicted by the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**word** (`str`) — The string that was captured.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start** (`int`) — The offset stringwise where the answer is located. Useful
    to disambiguate if word occurs multiple times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**end** (`int`) — The offset stringwise where the answer is located. Useful
    to disambiguate if word occurs multiple times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dictionary containing the output of a [token_classification()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.token_classification)
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Text generation types
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[text_generation()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation)
    task has a greater support than other tasks in `InferenceClient`. In particular,
    user inputs and server outputs are validated using [Pydantic](https://docs.pydantic.dev/latest/)
    if this package is installed. Therefore, we recommend installing it (`pip install
    pydantic`) for a better user experience.'
  prefs: []
  type: TYPE_NORMAL
- en: You can find below the dataclasses used to validate data and in particular [TextGenerationParameters](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.inference._text_generation.TextGenerationParameters)
    (input), [TextGenerationResponse](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.inference._text_generation.TextGenerationResponse)
    (output) and [TextGenerationStreamResponse](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.inference._text_generation.TextGenerationStreamResponse)
    (streaming output).
  prefs: []
  type: TYPE_NORMAL
- en: '### class huggingface_hub.inference._text_generation.TextGenerationParameters'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_text_generation.py#L59)'
  prefs: []
  type: TYPE_NORMAL
- en: '( do_sample: bool = False max_new_tokens: int = 20 repetition_penalty: Optional
    = None return_full_text: bool = False stop: List = <factory> seed: Optional =
    None temperature: Optional = None top_k: Optional = None top_p: Optional = None
    truncate: Optional = None typical_p: Optional = None best_of: Optional = None
    watermark: bool = False details: bool = False decoder_input_details: bool = False
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**do_sample** (`bool`, *optional*) — Activate logits sampling. Defaults to
    False.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_new_tokens** (`int`, *optional*) — Maximum number of generated tokens.
    Defaults to 20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**repetition_penalty** (`Optional[float]`, *optional*) — The parameter for
    repetition penalty. A value of 1.0 means no penalty. See [this paper](https://arxiv.org/pdf/1909.05858.pdf)
    for more details. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_full_text** (`bool`, *optional*) — Whether to prepend the prompt to
    the generated text. Defaults to False.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stop** (`List[str]`, *optional*) — Stop generating tokens if a member of
    `stop_sequences` is generated. Defaults to an empty list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**seed** (`Optional[int]`, *optional*) — Random sampling seed. Defaults to
    None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**temperature** (`Optional[float]`, *optional*) — The value used to modulate
    the logits distribution. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**top_k** (`Optional[int]`, *optional*) — The number of highest probability
    vocabulary tokens to keep for top-k-filtering. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**top_p** (`Optional[float]`, *optional*) — If set to a value less than 1,
    only the smallest set of most probable tokens with probabilities that add up to
    `top_p` or higher are kept for generation. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**truncate** (`Optional[int]`, *optional*) — Truncate input tokens to the given
    size. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**typical_p** (`Optional[float]`, *optional*) — Typical Decoding mass. See
    [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666)
    for more information. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**best_of** (`Optional[int]`, *optional*) — Generate `best_of` sequences and
    return the one with the highest token logprobs. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**watermark** (`bool`, *optional*) — Watermarking with [A Watermark for Large
    Language Models](https://arxiv.org/abs/2301.10226). Defaults to False.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**details** (`bool`, *optional*) — Get generation details. Defaults to False.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decoder_input_details** (`bool`, *optional*) — Get decoder input token logprobs
    and ids. Defaults to False.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameters for text generation.
  prefs: []
  type: TYPE_NORMAL
- en: '### class huggingface_hub.inference._text_generation.TextGenerationResponse'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_text_generation.py#L394)'
  prefs: []
  type: TYPE_NORMAL
- en: '( generated_text: str details: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**generated_text** (`str`) — The generated text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**details** (`Optional[Details]`) — Generation details. Returned only if `details=True`
    is sent to the server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Represents a response for text generation.
  prefs: []
  type: TYPE_NORMAL
- en: Only returned when `details=True`, otherwise a string is returned.
  prefs: []
  type: TYPE_NORMAL
- en: '### class huggingface_hub.inference._text_generation.TextGenerationStreamResponse'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_text_generation.py#L444)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token: Token generated_text: Optional = None details: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token** (`Token`) — The generated token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generated_text** (`Optional[str]`, *optional*) — The complete generated text.
    Only available when the generation is finished.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**details** (`Optional[StreamDetails]`, *optional*) — Generation details. Only
    available when the generation is finished.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Represents a response for streaming text generation.
  prefs: []
  type: TYPE_NORMAL
- en: Only returned when `details=True` and `stream=True`.
  prefs: []
  type: TYPE_NORMAL
- en: '### class huggingface_hub.inference._text_generation.InputToken'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_text_generation.py#L238)'
  prefs: []
  type: TYPE_NORMAL
- en: '( id: int text: str logprob: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**id** (`int`) — Token ID from the model tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text** (`str`) — Token text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logprob** (`float` or `None`) — Log probability of the token. Optional since
    the logprob of the first token cannot be computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Represents an input token.
  prefs: []
  type: TYPE_NORMAL
- en: '### class huggingface_hub.inference._text_generation.Token'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_text_generation.py#L262)'
  prefs: []
  type: TYPE_NORMAL
- en: '( id: int text: str logprob: float special: bool )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**id** (`int`) — Token ID from the model tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text** (`str`) — Token text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logprob** (`float`) — Log probability of the token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**special** (`bool`) — Indicates whether the token is a special token. It can
    be used to ignore tokens when concatenating.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Represents a token.
  prefs: []
  type: TYPE_NORMAL
- en: '### class huggingface_hub.inference._text_generation.FinishReason'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_text_generation.py#L291)'
  prefs: []
  type: TYPE_NORMAL
- en: ( value names = None module = None qualname = None type = None start = 1 )
  prefs: []
  type: TYPE_NORMAL
- en: An enumeration.
  prefs: []
  type: TYPE_NORMAL
- en: '### class huggingface_hub.inference._text_generation.BestOfSequence'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_text_generation.py#L301)'
  prefs: []
  type: TYPE_NORMAL
- en: '( generated_text: str finish_reason: FinishReason generated_tokens: int seed:
    Optional = None prefill: List = <factory> tokens: List = <factory> )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**generated_text** (`str`) — The generated text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**finish_reason** (`FinishReason`) — The reason for the generation to finish,
    represented by a `FinishReason` value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generated_tokens** (`int`) — The number of generated tokens in the sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**seed** (`Optional[int]`) — The sampling seed if sampling was activated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prefill** (`List[InputToken]`) — The decoder input tokens. Empty if `decoder_input_details`
    is False. Defaults to an empty list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tokens** (`List[Token]`) — The generated tokens. Defaults to an empty list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Represents a best-of sequence generated during text generation.
  prefs: []
  type: TYPE_NORMAL
- en: '### class huggingface_hub.inference._text_generation.Details'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_text_generation.py#L345)'
  prefs: []
  type: TYPE_NORMAL
- en: '( finish_reason: FinishReason generated_tokens: int seed: Optional = None prefill:
    List = <factory> tokens: List = <factory> best_of_sequences: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**finish_reason** (`FinishReason`) — The reason for the generation to finish,
    represented by a `FinishReason` value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generated_tokens** (`int`) — The number of generated tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**seed** (`Optional[int]`) — The sampling seed if sampling was activated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prefill** (`List[InputToken]`, *optional*) — The decoder input tokens. Empty
    if `decoder_input_details` is False. Defaults to an empty list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tokens** (`List[Token]`) — The generated tokens. Defaults to an empty list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**best_of_sequences** (`Optional[List[BestOfSequence]]`) — Additional sequences
    when using the `best_of` parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Represents details of a text generation.
  prefs: []
  type: TYPE_NORMAL
- en: '### class huggingface_hub.inference._text_generation.StreamDetails'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference/_text_generation.py#L421)'
  prefs: []
  type: TYPE_NORMAL
- en: '( finish_reason: FinishReason generated_tokens: int seed: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**finish_reason** (`FinishReason`) — The reason for the generation to finish,
    represented by a `FinishReason` value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generated_tokens** (`int`) — The number of generated tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**seed** (`Optional[int]`) — The sampling seed if sampling was activated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Represents details of a text generation stream.
  prefs: []
  type: TYPE_NORMAL
- en: InferenceAPI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`InferenceAPI` is the legacy way to call the Inference API. The interface is
    more simplistic and requires knowing the input parameters and output format for
    each task. It also lacks the ability to connect to other services like Inference
    Endpoints or AWS SageMaker. `InferenceAPI` will soon be deprecated so we recommend
    using [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    whenever possible. Check out [this guide](../guides/inference#legacy-inferenceapi-client)
    to learn how to switch from `InferenceAPI` to [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    in your scripts.'
  prefs: []
  type: TYPE_NORMAL
- en: '### class huggingface_hub.InferenceApi'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference_api.py#L46)'
  prefs: []
  type: TYPE_NORMAL
- en: '( repo_id: str task: Optional = None token: Optional = None gpu: bool = False
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Client to configure requests and make calls to the HuggingFace Inference API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '#### __init__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference_api.py#L93)'
  prefs: []
  type: TYPE_NORMAL
- en: '( repo_id: str task: Optional = None token: Optional = None gpu: bool = False
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**repo_id** (`str`) — Id of repository (e.g. *user/bert-base-uncased*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**task** (`str`, *optional*, defaults `None`) — Whether to force a task instead
    of using task specified in the repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token** (*str*, *optional*) — The API token to use as HTTP bearer authorization.
    This is not the authentication token. You can find the token in [https://huggingface.co/settings/token](https://huggingface.co/settings/token).
    Alternatively, you can find both your organizations and personal API tokens using
    *HfApi().whoami(token)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gpu** (*bool*, *optional*, defaults *False*) — Whether to use GPU instead
    of CPU for inference(requires Startup plan at least).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inits headers and API call information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/inference_api.py#L158)'
  prefs: []
  type: TYPE_NORMAL
- en: '( inputs: Union = None params: Optional = None data: Optional = None raw_response:
    bool = False )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**inputs** (`str` or `Dict` or `List[str]` or `List[List[str]]`, *optional*)
    — Inputs for the prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**params** (`Dict`, *optional*) — Additional parameters for the models. Will
    be sent as `parameters` in the payload.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**data** (`bytes`, *optional*) — Bytes content of the request. In this case,
    leave `inputs` and `params` empty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**raw_response** (`bool`, defaults to `False`) — If `True`, the raw `Response`
    object is returned. You can parse its content as preferred. By default, the content
    is parsed into a more practical format (json dictionary or PIL Image for example).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make a call to the Inference API.
  prefs: []
  type: TYPE_NORMAL
