# å¿«é€Ÿå¯¼è§ˆ

> åŸæ–‡ï¼š[https://huggingface.co/docs/optimum/quicktour](https://huggingface.co/docs/optimum/quicktour)

è¿™ä¸ªå¿«é€Ÿå¯¼è§ˆé€‚ç”¨äºå‡†å¤‡æ·±å…¥ä»£ç å¹¶æŸ¥çœ‹å¦‚ä½•å°†ğŸ¤— Optimumé›†æˆåˆ°ä»–ä»¬çš„æ¨¡å‹è®­ç»ƒå’Œæ¨ç†å·¥ä½œæµç¨‹ä¸­çš„å¼€å‘äººå‘˜ã€‚

## åŠ é€Ÿæ¨ç†

#### OpenVINO

è¦åŠ è½½æ¨¡å‹å¹¶ä½¿ç”¨OpenVINO Runtimeè¿›è¡Œæ¨ç†ï¼Œæ‚¨åªéœ€å°†æ‚¨çš„`AutoModelForXxx`ç±»æ›¿æ¢ä¸ºç›¸åº”çš„`OVModelForXxx`ç±»ã€‚å¦‚æœè¦åŠ è½½PyTorchæ£€æŸ¥ç‚¹ï¼Œè¯·è®¾ç½®`export=True`ä»¥å°†æ¨¡å‹è½¬æ¢ä¸ºOpenVINO IRï¼ˆä¸­é—´è¡¨ç¤ºï¼‰ã€‚

```py
- from transformers import AutoModelForSequenceClassification
+ from optimum.intel.openvino import OVModelForSequenceClassification
  from transformers import AutoTokenizer, pipeline

  # Download a tokenizer and model from the Hub and convert to OpenVINO format
  tokenizer = AutoTokenizer.from_pretrained(model_id)
  model_id = "distilbert-base-uncased-finetuned-sst-2-english"
- model = AutoModelForSequenceClassification.from_pretrained(model_id)
+ model = OVModelForSequenceClassification.from_pretrained(model_id, export=True)

  # Run inference!
  classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)
  results = classifier("He's a dreadful magician.")
```

æ‚¨å¯ä»¥åœ¨[æ–‡æ¡£](https://huggingface.co/docs/optimum/intel/inference)å’Œ[ç¤ºä¾‹](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino)ä¸­æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ã€‚

#### ONNX Runtime

ä¸ºäº†åŠ é€Ÿä½¿ç”¨ONNX Runtimeè¿›è¡Œæ¨ç†ï¼ŒğŸ¤— Optimumä½¿ç”¨*é…ç½®å¯¹è±¡*æ¥å®šä¹‰å›¾ä¼˜åŒ–å’Œé‡åŒ–çš„å‚æ•°ã€‚ç„¶åä½¿ç”¨è¿™äº›å¯¹è±¡æ¥å®ä¾‹åŒ–ä¸“ç”¨çš„*ä¼˜åŒ–å™¨*å’Œ*é‡åŒ–å™¨*ã€‚

åœ¨åº”ç”¨é‡åŒ–æˆ–ä¼˜åŒ–ä¹‹å‰ï¼Œé¦–å…ˆæˆ‘ä»¬éœ€è¦åŠ è½½æˆ‘ä»¬çš„æ¨¡å‹ã€‚è¦åŠ è½½æ¨¡å‹å¹¶ä½¿ç”¨ONNX Runtimeè¿›è¡Œæ¨ç†ï¼Œæ‚¨åªéœ€å°†ç»å…¸çš„Transformers [`AutoModelForXxx`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel)ç±»æ›¿æ¢ä¸ºç›¸åº”çš„[`ORTModelForXxx`](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel)ç±»ã€‚å¦‚æœè¦ä»PyTorchæ£€æŸ¥ç‚¹åŠ è½½ï¼Œè¯·è®¾ç½®`export=True`ä»¥å°†æ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼ã€‚

```py
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import AutoTokenizer

>>> model_checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
>>> save_directory = "tmp/onnx/"

>>> # Load a model from transformers and export it to ONNX
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)

>>> # Save the ONNX model and tokenizer
>>> ort_model.save_pretrained(save_directory)
>>> tokenizer.save_pretrained(save_directory)
```

ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ONNX Runtimeåº”ç”¨åŠ¨æ€é‡åŒ–ï¼š

```py
>>> from optimum.onnxruntime.configuration import AutoQuantizationConfig
>>> from optimum.onnxruntime import ORTQuantizer

>>> # Define the quantization methodology
>>> qconfig = AutoQuantizationConfig.arm64(is_static=False, per_channel=False)
>>> quantizer = ORTQuantizer.from_pretrained(ort_model)

>>> # Apply dynamic quantization on the model
>>> quantizer.quantize(save_dir=save_directory, quantization_config=qconfig)
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯¹æ¥è‡ªHugging Face Hubçš„æ¨¡å‹è¿›è¡Œäº†é‡åŒ–ï¼Œä»¥ç›¸åŒçš„æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æä¾›åŒ…å«æ¨¡å‹æƒé‡çš„ç›®å½•è·¯å¾„æ¥å¯¹æœ¬åœ°æ‰˜ç®¡çš„æ¨¡å‹è¿›è¡Œé‡åŒ–ã€‚åº”ç”¨`quantize()`æ–¹æ³•çš„ç»“æœæ˜¯ä¸€ä¸ª`model_quantized.onnx`æ–‡ä»¶ï¼Œå¯ç”¨äºè¿è¡Œæ¨ç†ã€‚è¿™é‡Œæ˜¯å¦‚ä½•åŠ è½½ä¸€ä¸ªONNX Runtimeæ¨¡å‹å¹¶ç”Ÿæˆé¢„æµ‹çš„ç¤ºä¾‹ï¼š

```py
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import pipeline, AutoTokenizer

>>> model = ORTModelForSequenceClassification.from_pretrained(save_directory, file_name="model_quantized.onnx")
>>> tokenizer = AutoTokenizer.from_pretrained(save_directory)
>>> classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)
>>> results = classifier("I love burritos!")
```

æ‚¨å¯ä»¥åœ¨[æ–‡æ¡£](https://huggingface.co/docs/optimum/onnxruntime/quickstart)å’Œ[ç¤ºä¾‹](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime)ä¸­æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ã€‚

## åŠ é€Ÿè®­ç»ƒ

#### Habana

ä¸ºäº†åœ¨Habanaçš„Gaudiå¤„ç†å™¨ä¸Šè®­ç»ƒtransformersï¼ŒğŸ¤— Optimumæä¾›äº†ä¸€ä¸ª`GaudiTrainer`ï¼Œå®ƒä¸ğŸ¤— Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)éå¸¸ç›¸ä¼¼ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š

```py
- from transformers import Trainer, TrainingArguments
+ from optimum.habana import GaudiTrainer, GaudiTrainingArguments

  # Download a pretrained model from the Hub
  model = AutoModelForXxx.from_pretrained("bert-base-uncased")

  # Define the training arguments
- training_args = TrainingArguments(
+ training_args = GaudiTrainingArguments(
      output_dir="path/to/save/folder/",
+     use_habana=True,
+     use_lazy_mode=True,
+     gaudi_config_name="Habana/bert-base-uncased",
      ...
  )

  # Initialize the trainer
- trainer = Trainer(
+ trainer = GaudiTrainer(
      model=model,
      args=training_args,
      train_dataset=train_dataset,
      ...
  )

  # Use Habana Gaudi processor for training!
  trainer.train()
```

æ‚¨å¯ä»¥åœ¨[æ–‡æ¡£](https://huggingface.co/docs/optimum/habana/quickstart)å’Œ[ç¤ºä¾‹](https://github.com/huggingface/optimum-habana/tree/main/examples)ä¸­æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ã€‚

#### ONNX Runtime

ä½¿ç”¨ONNX Runtimeçš„åŠ é€ŸåŠŸèƒ½æ¥è®­ç»ƒtransformersï¼ŒğŸ¤— Optimumæä¾›äº†ä¸€ä¸ª`ORTTrainer`ï¼Œå®ƒä¸ğŸ¤— Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)éå¸¸ç›¸ä¼¼ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š

```py
- from transformers import Trainer, TrainingArguments
+ from optimum.onnxruntime import ORTTrainer, ORTTrainingArguments

  # Download a pretrained model from the Hub
  model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")

  # Define the training arguments
- training_args = TrainingArguments(
+ training_args = ORTTrainingArguments(
      output_dir="path/to/save/folder/",
      optim="adamw_ort_fused",
      ...
  )

  # Create a ONNX Runtime Trainer
- trainer = Trainer(
+ trainer = ORTTrainer(
      model=model,
      args=training_args,
      train_dataset=train_dataset,
+     feature="text-classification", # The model type to export to ONNX
      ...
  )

  # Use ONNX Runtime for training!
  trainer.train()
```

æ‚¨å¯ä»¥åœ¨[æ–‡æ¡£](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer)å’Œ[ç¤ºä¾‹](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training)ä¸­æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ã€‚

## å¼€ç®±å³ç”¨çš„ONNXå¯¼å‡º

Optimumåº“å¯ä»¥ç›´æ¥å¤„ç†Transformerså’ŒDiffusersæ¨¡å‹çš„ONNXå¯¼å‡ºï¼

å°†æ¨¡å‹å¯¼å‡ºåˆ°ONNXå°±åƒè¿™æ ·ç®€å•

```py
optimum-cli export onnx --model gpt2 gpt2_onnx/
```

æŸ¥çœ‹å¸®åŠ©ä»¥è·å–æ›´å¤šé€‰é¡¹ï¼š

```py
optimum-cli export onnx --help
```

æŸ¥çœ‹[æ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model)è·å–æ›´å¤šä¿¡æ¯ã€‚

## PyTorchçš„BetterTransformeræ”¯æŒ

[BetterTransformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) æ˜¯ä¸€ä¸ªå…è´¹çš„ PyTorch æœ¬åœ°ä¼˜åŒ–ï¼Œå¯åœ¨åŸºäº Transformer çš„æ¨¡å‹æ¨ç†ä¸­è·å¾— x1.25 - x4 çš„åŠ é€Ÿã€‚å®ƒå·²åœ¨[PyTorch 1.13](https://pytorch.org/blog/PyTorch-1.13-release/)ä¸­æ ‡è®°ä¸ºç¨³å®šã€‚æˆ‘ä»¬å°† BetterTransformer ä¸ ğŸ¤— Transformers åº“ä¸­æœ€å¸¸ç”¨çš„æ¨¡å‹é›†æˆï¼Œä½¿ç”¨é›†æˆå°±åƒè¿™æ ·ç®€å•ï¼š

```py
>>> from optimum.bettertransformer import BetterTransformer
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
>>> model = BetterTransformer.transform(model)
```

æŸ¥çœ‹æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[æ–‡æ¡£](https://huggingface.co/docs/optimum/bettertransformer/overview)ï¼Œå¹¶æŸ¥çœ‹[PyTorch Medium ä¸Šçš„åšæ–‡](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)ä»¥äº†è§£æ›´å¤šå…³äºé›†æˆçš„ä¿¡æ¯ï¼

## torch.fx é›†æˆ

Optimum ä¸ `torch.fx` é›†æˆï¼Œæä¾›äº†å‡ ä¸ªå›¾å½¢è½¬æ¢çš„ä¸€è¡Œä»£ç ã€‚æˆ‘ä»¬æ—¨åœ¨é€šè¿‡ `torch.fx` æ”¯æŒæ›´å¥½çš„[é‡åŒ–](https://huggingface.co/docs/optimum/concept_guides/quantization)ç®¡ç†ï¼ŒåŒ…æ‹¬é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’Œè®­ç»ƒåé‡åŒ–ï¼ˆPTQï¼‰ã€‚

æŸ¥çœ‹æ›´å¤š[æ–‡æ¡£](https://huggingface.co/docs/optimum/torch_fx/usage_guides/optimization)å’Œ[å‚è€ƒèµ„æ–™](https://huggingface.co/docs/optimum/torch_fx/package_reference/optimization)ï¼
