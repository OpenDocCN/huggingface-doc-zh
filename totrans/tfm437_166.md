# FastSpeech2Conformer

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/fastspeech2_conformer`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/fastspeech2_conformer)

## æ¦‚è¿°

FastSpeech2Conformer æ¨¡å‹æ˜¯ç”± Pengcheng Guoã€Florian Boyerã€Xuankai Changã€Tomoki Hayashiã€Yosuke Higuchiã€Hirofumi Inagumaã€Naoyuki Kamoã€Chenda Liã€Daniel Garcia-Romeroã€Jiatong Shiã€Jing Shiã€Shinji Watanabeã€Kun Weiã€Wangyou Zhang å’Œ Yuekai Zhang åœ¨è®ºæ–‡ [Recent Developments On Espnet Toolkit Boosted By Conformer](https://arxiv.org/abs/2010.13956) ä¸­æå‡ºçš„ã€‚

åŸå§‹ FastSpeech2 è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š

*éè‡ªå›å½’æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œå¦‚ FastSpeechï¼ˆRen ç­‰ï¼Œ2019ï¼‰ï¼Œå¯ä»¥æ¯”ä»¥å‰çš„å…·æœ‰å¯æ¯”è´¨é‡çš„è‡ªå›å½’æ¨¡å‹æ›´å¿«åœ°åˆæˆè¯­éŸ³ã€‚FastSpeech æ¨¡å‹çš„è®­ç»ƒä¾èµ–äºè‡ªå›å½’æ•™å¸ˆæ¨¡å‹è¿›è¡ŒæŒç»­æ—¶é—´é¢„æµ‹ï¼ˆæä¾›æ›´å¤šä¿¡æ¯ä½œä¸ºè¾“å…¥ï¼‰å’ŒçŸ¥è¯†è’¸é¦ï¼ˆç®€åŒ–è¾“å‡ºä¸­çš„æ•°æ®åˆ†å¸ƒï¼‰ï¼Œè¿™å¯ä»¥ç¼“è§£ TTS ä¸­çš„ä¸€å¯¹å¤šæ˜ å°„é—®é¢˜ï¼ˆå³ï¼Œå¤šç§è¯­éŸ³å˜ä½“å¯¹åº”ç›¸åŒçš„æ–‡æœ¬ï¼‰ã€‚ç„¶è€Œï¼ŒFastSpeech æœ‰å‡ ä¸ªç¼ºç‚¹ï¼š1ï¼‰æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦æµç¨‹å¤æ‚ä¸”è€—æ—¶ï¼Œ2ï¼‰ä»æ•™å¸ˆæ¨¡å‹æå–çš„æŒç»­æ—¶é—´ä¸å¤Ÿå‡†ç¡®ï¼Œä»æ•™å¸ˆæ¨¡å‹è’¸é¦çš„ç›®æ ‡ mel-é¢‘è°±å›¾ç”±äºæ•°æ®ç®€åŒ–è€Œé­å—ä¿¡æ¯ä¸¢å¤±ï¼Œè¿™ä¸¤è€…é™åˆ¶äº†è¯­éŸ³è´¨é‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† FastSpeech 2ï¼Œå®ƒè§£å†³äº† FastSpeech ä¸­çš„é—®é¢˜ï¼Œå¹¶é€šè¿‡ä»¥ä¸‹æ–¹å¼æ›´å¥½åœ°è§£å†³äº† TTS ä¸­çš„ä¸€å¯¹å¤šæ˜ å°„é—®é¢˜ï¼š1ï¼‰ç›´æ¥ä½¿ç”¨åœ°é¢çœŸå®ç›®æ ‡è®­ç»ƒæ¨¡å‹ï¼Œè€Œä¸æ˜¯æ¥è‡ªæ•™å¸ˆçš„ç®€åŒ–è¾“å‡ºï¼Œ2ï¼‰å¼•å…¥æ›´å¤šè¯­éŸ³å˜ä½“ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼ŒéŸ³é«˜ã€èƒ½é‡å’Œæ›´å‡†ç¡®çš„æŒç»­æ—¶é—´ï¼‰ä½œä¸ºæ¡ä»¶è¾“å…¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»è¯­éŸ³æ³¢å½¢ä¸­æå–æŒç»­æ—¶é—´ã€éŸ³é«˜å’Œèƒ½é‡ï¼Œå¹¶ç›´æ¥å°†å®ƒä»¬ä½œä¸ºæ¡ä»¶è¾“å…¥è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æ¨æ–­ä¸­ä½¿ç”¨é¢„æµ‹å€¼ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº† FastSpeech 2sï¼Œè¿™æ˜¯ç¬¬ä¸€æ¬¡å°è¯•ä»æ–‡æœ¬ä¸­å¹¶è¡Œç›´æ¥ç”Ÿæˆè¯­éŸ³æ³¢å½¢ï¼Œäº«å—å®Œå…¨ç«¯åˆ°ç«¯æ¨æ–­çš„å¥½å¤„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼š1ï¼‰FastSpeech 2 æ¯” FastSpeech å®ç°äº† 3 å€çš„è®­ç»ƒåŠ é€Ÿï¼ŒFastSpeech 2s ç”šè‡³äº«æœ‰æ›´å¿«çš„æ¨æ–­é€Ÿåº¦ï¼›2ï¼‰FastSpeech 2 å’Œ 2s åœ¨è¯­éŸ³è´¨é‡ä¸Šä¼˜äº FastSpeechï¼ŒFastSpeech 2 ç”šè‡³å¯ä»¥è¶…è¶Šè‡ªå›å½’æ¨¡å‹ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨ [`speechresearch.github.io/fastspeech2/`](https://speechresearch.github.io/fastspeech2/) ä¸Šæ‰¾åˆ°ã€‚*

æ­¤æ¨¡å‹ç”± [Connor Henderson](https://huggingface.co/connor-henderson) è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨ [è¿™é‡Œ](https://github.com/espnet/espnet/blob/master/espnet2/tts/fastspeech2/fastspeech2.py) æ‰¾åˆ°ã€‚

## ğŸ¤— æ¨¡å‹æ¶æ„

FastSpeech2 çš„ä¸€èˆ¬ç»“æ„ä¸ Mel é¢‘è°±å›¾è§£ç å™¨ä¸€èµ·å®ç°ï¼Œå¹¶ä¸”ä¼ ç»Ÿçš„ transformer blocks è¢« ESPnet åº“ä¸­çš„ conformer blocks æ›¿æ¢ã€‚

#### FastSpeech2 æ¨¡å‹æ¶æ„

![FastSpeech2 æ¨¡å‹æ¶æ„](img/19e41c7c11474d823ed6b65f4b973faf.png)

#### Conformer Blocks

![Conformer Blocks](img/e85e7a628d66d560dd93c3858c0061c8.png)

#### å·ç§¯æ¨¡å—

![å·ç§¯æ¨¡å—](img/9f1bb141278007107620b1e6640f061e.png)

## ğŸ¤— Transformers ä½¿ç”¨

æ‚¨å¯ä»¥åœ¨ ğŸ¤— Transformers åº“ä¸­æœ¬åœ°è¿è¡Œ FastSpeech2Conformerã€‚

1.  é¦–å…ˆå®‰è£… ğŸ¤— [Transformers åº“](https://github.com/huggingface/transformers)ï¼Œg2p-enï¼š

```py
pip install --upgrade pip
pip install --upgrade transformers g2p-en
```

1.  é€šè¿‡ Transformers å»ºæ¨¡ä»£ç åˆ†åˆ«è¿è¡Œæ¨æ–­ï¼Œä½¿ç”¨æ¨¡å‹å’Œ hifigan

```py

from transformers import FastSpeech2ConformerTokenizer, FastSpeech2ConformerModel, FastSpeech2ConformerHifiGan
import soundfile as sf

tokenizer = FastSpeech2ConformerTokenizer.from_pretrained("espnet/fastspeech2_conformer")
inputs = tokenizer("Hello, my dog is cute.", return_tensors="pt")
input_ids = inputs["input_ids"]

model = FastSpeech2ConformerModel.from_pretrained("espnet/fastspeech2_conformer")
output_dict = model(input_ids, return_dict=True)
spectrogram = output_dict["spectrogram"]

hifigan = FastSpeech2ConformerHifiGan.from_pretrained("espnet/fastspeech2_conformer_hifigan")
waveform = hifigan(spectrogram)

sf.write("speech.wav", waveform.squeeze().detach().numpy(), samplerate=22050)
```

1.  é€šè¿‡ Transformers å»ºæ¨¡ä»£ç ç»“åˆæ¨¡å‹å’Œ hifigan è¿è¡Œæ¨æ–­

```py
from transformers import FastSpeech2ConformerTokenizer, FastSpeech2ConformerWithHifiGan
import soundfile as sf

tokenizer = FastSpeech2ConformerTokenizer.from_pretrained("espnet/fastspeech2_conformer")
inputs = tokenizer("Hello, my dog is cute.", return_tensors="pt")
input_ids = inputs["input_ids"]

model = FastSpeech2ConformerWithHifiGan.from_pretrained("espnet/fastspeech2_conformer_with_hifigan")
output_dict = model(input_ids, return_dict=True)
waveform = output_dict["waveform"]

sf.write("speech.wav", waveform.squeeze().detach().numpy(), samplerate=22050)
```

1.  ä½¿ç”¨ç®¡é“è¿è¡Œæ¨æ–­ï¼Œå¹¶æŒ‡å®šè¦ä½¿ç”¨çš„å£°ç å™¨

```py
from transformers import pipeline, FastSpeech2ConformerHifiGan
import soundfile as sf

vocoder = FastSpeech2ConformerHifiGan.from_pretrained("espnet/fastspeech2_conformer_hifigan")
synthesiser = pipeline(model="espnet/fastspeech2_conformer", vocoder=vocoder)

speech = synthesiser("Hello, my dog is cooler than you!")

sf.write("speech.wav", speech["audio"].squeeze(), samplerate=speech["sampling_rate"])
```

## FastSpeech2ConformerConfig

### `class transformers.FastSpeech2ConformerConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fastspeech2_conformer/configuration_fastspeech2_conformer.py#L39)

```py
( hidden_size = 384 vocab_size = 78 num_mel_bins = 80 encoder_num_attention_heads = 2 encoder_layers = 4 encoder_linear_units = 1536 decoder_layers = 4 decoder_num_attention_heads = 2 decoder_linear_units = 1536 speech_decoder_postnet_layers = 5 speech_decoder_postnet_units = 256 speech_decoder_postnet_kernel = 5 positionwise_conv_kernel_size = 3 encoder_normalize_before = False decoder_normalize_before = False encoder_concat_after = False decoder_concat_after = False reduction_factor = 1 speaking_speed = 1.0 use_macaron_style_in_conformer = True use_cnn_in_conformer = True encoder_kernel_size = 7 decoder_kernel_size = 31 duration_predictor_layers = 2 duration_predictor_channels = 256 duration_predictor_kernel_size = 3 energy_predictor_layers = 2 energy_predictor_channels = 256 energy_predictor_kernel_size = 3 energy_predictor_dropout = 0.5 energy_embed_kernel_size = 1 energy_embed_dropout = 0.0 stop_gradient_from_energy_predictor = False pitch_predictor_layers = 5 pitch_predictor_channels = 256 pitch_predictor_kernel_size = 5 pitch_predictor_dropout = 0.5 pitch_embed_kernel_size = 1 pitch_embed_dropout = 0.0 stop_gradient_from_pitch_predictor = True encoder_dropout_rate = 0.2 encoder_positional_dropout_rate = 0.2 encoder_attention_dropout_rate = 0.2 decoder_dropout_rate = 0.2 decoder_positional_dropout_rate = 0.2 decoder_attention_dropout_rate = 0.2 duration_predictor_dropout_rate = 0.2 speech_decoder_postnet_dropout = 0.5 max_source_positions = 5000 use_masking = True use_weighted_masking = False num_speakers = None num_languages = None speaker_embed_dim = None is_encoder_decoder = True **kwargs )
```

å‚æ•°

+   `hidden_size` (`int`, *optional*, defaults to 384) â€” éšè—å±‚çš„ç»´åº¦ã€‚

+   `vocab_size` (`int`, *optional*, defaults to 78) â€” è¯æ±‡è¡¨çš„å¤§å°ã€‚

+   `num_mel_bins` (`int`, *optional*, defaults to 80) â€” æ»¤æ³¢å™¨ç»„ä¸­ä½¿ç”¨çš„ mel æ»¤æ³¢å™¨æ•°é‡ã€‚

+   `encoder_num_attention_heads` (`int`, *optional*, defaults to 2) â€” ç¼–ç å™¨ä¸­çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `encoder_layers` (`int`, *optional*, defaults to 4) â€” ç¼–ç å™¨ä¸­çš„å±‚æ•°ã€‚

+   `encoder_linear_units` (`int`, *optional*, defaults to 1536) â€” ç¼–ç å™¨ä¸­çº¿æ€§å±‚çš„å•å…ƒæ•°ã€‚

+   `decoder_layers` (`int`, *optional*, defaults to 4) â€” è§£ç å™¨ä¸­çš„å±‚æ•°ã€‚

+   `decoder_num_attention_heads` (`int`, *optional*, defaults to 2) â€” è§£ç å™¨ä¸­çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `decoder_linear_units` (`int`, *optional*, defaults to 1536) â€” è§£ç å™¨ä¸­çº¿æ€§å±‚çš„å•å…ƒæ•°ã€‚

+   `speech_decoder_postnet_layers` (`int`, *optional*, defaults to 5) â€” è¯­éŸ³è§£ç å™¨åå¤„ç†ç½‘ç»œä¸­çš„å±‚æ•°ã€‚

+   `speech_decoder_postnet_units` (`int`, *optional*, defaults to 256) â€” è¯­éŸ³è§£ç å™¨åå¤„ç†ç½‘ç»œä¸­çš„å•å…ƒæ•°ã€‚

+   `speech_decoder_postnet_kernel` (`int`, *optional*, defaults to 5) â€” è¯­éŸ³è§£ç å™¨åå¤„ç†ç½‘ç»œä¸­çš„å·ç§¯æ ¸å¤§å°ã€‚

+   `positionwise_conv_kernel_size` (`int`, *optional*, defaults to 3) â€” ä½ç½®æ„ŸçŸ¥å±‚ä¸­ä½¿ç”¨çš„å·ç§¯æ ¸å¤§å°ã€‚

+   `encoder_normalize_before` (`bool`, *optional*, defaults to `False`) â€” æŒ‡å®šæ˜¯å¦åœ¨ç¼–ç å™¨å±‚ä¹‹å‰è¿›è¡Œå½’ä¸€åŒ–ã€‚

+   `decoder_normalize_before` (`bool`, *optional*, defaults to `False`) â€” æŒ‡å®šæ˜¯å¦åœ¨è§£ç å™¨å±‚ä¹‹å‰è¿›è¡Œå½’ä¸€åŒ–ã€‚

+   `encoder_concat_after` (`bool`, *optional*, defaults to `False`) â€” æŒ‡å®šæ˜¯å¦åœ¨ç¼–ç å™¨å±‚ä¹‹åè¿›è¡Œè¿æ¥ã€‚

+   `decoder_concat_after` (`bool`, *optional*, defaults to `False`) â€” æŒ‡å®šæ˜¯å¦åœ¨è§£ç å™¨å±‚ä¹‹åè¿›è¡Œè¿æ¥ã€‚

+   `reduction_factor` (`int`, *optional*, defaults to 1) â€” è¯­éŸ³å¸§é€Ÿç‡å‡å°‘çš„å› å­ã€‚

+   `speaking_speed` (`float`, *optional*, defaults to 1.0) â€” ç”Ÿæˆè¯­éŸ³çš„é€Ÿåº¦ã€‚

+   `use_macaron_style_in_conformer` (`bool`, *optional*, defaults to `True`) â€” æŒ‡å®šæ˜¯å¦åœ¨ conformer ä¸­ä½¿ç”¨ macaron é£æ ¼ã€‚

+   `use_cnn_in_conformer` (`bool`, *optional*, defaults to `True`) â€” æŒ‡å®šæ˜¯å¦åœ¨ conformer ä¸­ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œã€‚

+   `encoder_kernel_size` (`int`, *optional*, defaults to 7) â€” ç¼–ç å™¨ä¸­ä½¿ç”¨çš„å·ç§¯æ ¸å¤§å°ã€‚

+   `decoder_kernel_size` (`int`, *optional*, defaults to 31) â€” è§£ç å™¨ä¸­ä½¿ç”¨çš„å·ç§¯æ ¸å¤§å°ã€‚

+   `duration_predictor_layers` (`int`, *optional*, defaults to 2) â€” é¢„æµ‹å™¨ä¸­çš„å±‚æ•°ã€‚

+   `duration_predictor_channels` (`int`, *optional*, defaults to 256) â€” é¢„æµ‹å™¨ä¸­é€šé“çš„æ•°é‡ã€‚

+   `duration_predictor_kernel_size` (`int`, *optional*, defaults to 3) â€” é¢„æµ‹å™¨ä¸­ä½¿ç”¨çš„å·ç§¯æ ¸å¤§å°ã€‚

+   `energy_predictor_layers` (`int`, *optional*, defaults to 2) â€” èƒ½é‡é¢„æµ‹å™¨ä¸­çš„å±‚æ•°ã€‚

+   `energy_predictor_channels` (`int`, *optional*, defaults to 256) â€” èƒ½é‡é¢„æµ‹å™¨ä¸­çš„é€šé“æ•°ã€‚

+   `energy_predictor_kernel_size` (`int`, *optional*, defaults to 3) â€” èƒ½é‡é¢„æµ‹å™¨ä¸­ä½¿ç”¨çš„å·ç§¯æ ¸å¤§å°ã€‚

+   `energy_predictor_dropout` (`float`, *optional*, defaults to 0.5) â€” èƒ½é‡é¢„æµ‹å™¨ä¸­çš„ dropout ç‡ã€‚

+   `energy_embed_kernel_size` (`int`, *optional*, defaults to 1) â€” èƒ½é‡åµŒå…¥å±‚ä¸­ä½¿ç”¨çš„å·ç§¯æ ¸å¤§å°ã€‚

+   `energy_embed_dropout` (`float`, *optional*, defaults to 0.0) â€” èƒ½é‡åµŒå…¥å±‚ä¸­çš„ dropout ç‡ã€‚

+   `stop_gradient_from_energy_predictor` (`bool`, *optional*, defaults to `False`) â€” æŒ‡å®šæ˜¯å¦ä»èƒ½é‡é¢„æµ‹å™¨ä¸­åœæ­¢æ¢¯åº¦ã€‚

+   `pitch_predictor_layers` (`int`, *optional*, defaults to 5) â€” éŸ³é«˜é¢„æµ‹å™¨ä¸­çš„å±‚æ•°ã€‚

+   `pitch_predictor_channels` (`int`, *optional*, defaults to 256) â€” éŸ³é«˜é¢„æµ‹å™¨ä¸­çš„é€šé“æ•°ã€‚

+   `pitch_predictor_kernel_size` (`int`, *optional*, defaults to 5) â€” éŸ³é«˜é¢„æµ‹å™¨ä¸­ä½¿ç”¨çš„å†…æ ¸å¤§å°ã€‚

+   `pitch_predictor_dropout` (`float`, *optional*, defaults to 0.5) â€” éŸ³é«˜é¢„æµ‹å™¨ä¸­çš„ dropout ç‡ã€‚

+   `pitch_embed_kernel_size` (`int`, *optional*, defaults to 1) â€” éŸ³é«˜åµŒå…¥å±‚ä¸­ä½¿ç”¨çš„å†…æ ¸å¤§å°ã€‚

+   `pitch_embed_dropout` (`float`, *optional*, defaults to 0.0) â€” éŸ³é«˜åµŒå…¥å±‚ä¸­çš„ dropout ç‡ã€‚

+   `stop_gradient_from_pitch_predictor` (`bool`, *optional*, defaults to `True`) â€” æŒ‡å®šæ˜¯å¦åœæ­¢æ¥è‡ªéŸ³é«˜é¢„æµ‹å™¨çš„æ¢¯åº¦ã€‚

+   `encoder_dropout_rate` (`float`, *optional*, defaults to 0.2) â€” ç¼–ç å™¨ä¸­çš„ dropout ç‡ã€‚

+   `encoder_positional_dropout_rate` (`float`, *optional*, defaults to 0.2) â€” ç¼–ç å™¨ä¸­çš„ä½ç½® dropout ç‡ã€‚

+   `encoder_attention_dropout_rate` (`float`, *optional*, defaults to 0.2) â€” ç¼–ç å™¨ä¸­çš„æ³¨æ„åŠ› dropout ç‡ã€‚

+   `decoder_dropout_rate` (`float`, *optional*, defaults to 0.2) â€” è§£ç å™¨ä¸­çš„ dropout ç‡ã€‚

+   `decoder_positional_dropout_rate` (`float`, *optional*, defaults to 0.2) â€” è§£ç å™¨ä¸­çš„ä½ç½® dropout ç‡ã€‚

+   `decoder_attention_dropout_rate` (`float`, *optional*, defaults to 0.2) â€” è§£ç å™¨ä¸­çš„æ³¨æ„åŠ› dropout ç‡ã€‚

+   `duration_predictor_dropout_rate` (`float`, *optional*, defaults to 0.2) â€” æŒç»­æ—¶é—´é¢„æµ‹å™¨ä¸­çš„ dropout ç‡ã€‚

+   `speech_decoder_postnet_dropout` (`float`, *optional*, defaults to 0.5) â€” è¯­éŸ³è§£ç å™¨åç½®ç½‘ç»œä¸­çš„ dropout ç‡ã€‚

+   `max_source_positions` (`int`, *optional*, defaults to 5000) â€” å¦‚æœä½¿ç”¨â€œç›¸å¯¹â€ä½ç½®åµŒå…¥ï¼Œåˆ™å®šä¹‰æœ€å¤§æºè¾“å…¥ä½ç½®ã€‚

+   `use_masking` (`bool`, *optional*, defaults to `True`) â€” æŒ‡å®šæ¨¡å‹æ˜¯å¦ä½¿ç”¨æ©ç ã€‚

+   `use_weighted_masking` (`bool`, *optional*, defaults to `False`) â€” æŒ‡å®šæ¨¡å‹æ˜¯å¦ä½¿ç”¨åŠ æƒæ©ç ã€‚

+   `num_speakers` (`int`, *optional*) â€” è¯´è¯è€…æ•°é‡ã€‚å¦‚æœè®¾ç½®ä¸º > 1ï¼Œåˆ™å‡å®šè¯´è¯è€… id å°†ä½œä¸ºè¾“å…¥æä¾›ï¼Œå¹¶ä½¿ç”¨è¯´è¯è€… id åµŒå…¥å±‚ã€‚

+   `num_languages` (`int`, *optional*) â€” è¯­è¨€æ•°é‡ã€‚å¦‚æœè®¾ç½®ä¸º > 1ï¼Œåˆ™å‡å®šè¯­è¨€ id å°†ä½œä¸ºè¾“å…¥æä¾›ï¼Œå¹¶ä½¿ç”¨è¯­è¨€ id åµŒå…¥å±‚ã€‚

+   `speaker_embed_dim` (`int`, *optional*) â€” è¯´è¯è€…åµŒå…¥ç»´åº¦ã€‚å¦‚æœè®¾ç½®ä¸º > 0ï¼Œåˆ™å‡å®šå°†æä¾›è¯´è¯è€…åµŒå…¥ä½œä¸ºè¾“å…¥ã€‚

+   `is_encoder_decoder` (`bool`, *optional*, defaults to `True`) â€” æŒ‡å®šæ¨¡å‹æ˜¯å¦ä¸ºç¼–ç å™¨-è§£ç å™¨ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ FastSpeech2ConformerModel é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª FastSpeech2Conformer æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº FastSpeech2Conformer [espnet/fastspeech2_conformer](https://huggingface.co/espnet/fastspeech2_conformer)æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import FastSpeech2ConformerModel, FastSpeech2ConformerConfig

>>> # Initializing a FastSpeech2Conformer style configuration
>>> configuration = FastSpeech2ConformerConfig()

>>> # Initializing a model from the FastSpeech2Conformer style configuration
>>> model = FastSpeech2ConformerModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## FastSpeech2ConformerHifiGanConfig

### `class transformers.FastSpeech2ConformerHifiGanConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fastspeech2_conformer/configuration_fastspeech2_conformer.py#L340)

```py
( model_in_dim = 80 upsample_initial_channel = 512 upsample_rates = [8, 8, 2, 2] upsample_kernel_sizes = [16, 16, 4, 4] resblock_kernel_sizes = [3, 7, 11] resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]] initializer_range = 0.01 leaky_relu_slope = 0.1 normalize_before = True **kwargs )
```

å‚æ•°

+   `model_in_dim` (`int`, *optional*, defaults to 80) â€” è¾“å…¥å¯¹æ•°æ¢…å°”é¢‘è°±å›¾ä¸­çš„é¢‘ç‡ç®±æ•°ã€‚

+   `upsample_initial_channel` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 512) â€” è¿›å…¥ä¸Šé‡‡æ ·ç½‘ç»œçš„è¾“å…¥é€šé“æ•°ã€‚

+   `upsample_rates` (`Tuple[int]` æˆ– `List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[8, 8, 2, 2]`) â€” ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰äº†ä¸Šé‡‡æ ·ç½‘ç»œä¸­æ¯ä¸ª 1D å·ç§¯å±‚çš„æ­¥å¹…ã€‚*upsample_rates* çš„é•¿åº¦å®šä¹‰äº†å·ç§¯å±‚çš„æ•°é‡ï¼Œå¹¶ä¸”å¿…é¡»ä¸ *upsample_kernel_sizes* çš„é•¿åº¦åŒ¹é…ã€‚

+   `upsample_kernel_sizes` (`Tuple[int]` æˆ– `List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[16, 16, 4, 4]`) â€” ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰äº†ä¸Šé‡‡æ ·ç½‘ç»œä¸­æ¯ä¸ª 1D å·ç§¯å±‚çš„å†…æ ¸å¤§å°ã€‚*upsample_kernel_sizes* çš„é•¿åº¦å®šä¹‰äº†å·ç§¯å±‚çš„æ•°é‡ï¼Œå¹¶ä¸”å¿…é¡»ä¸ *upsample_rates* çš„é•¿åº¦åŒ¹é…ã€‚

+   `resblock_kernel_sizes` (`Tuple[int]` æˆ– `List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[3, 7, 11]`) â€” ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰äº†å¤šæ¥å—åŸŸèåˆï¼ˆMRFï¼‰æ¨¡å—ä¸­ 1D å·ç§¯å±‚çš„å†…æ ¸å¤§å°ã€‚

+   `resblock_dilation_sizes` (`Tuple[Tuple[int]]` æˆ– `List[List[int]]`, *å¯é€‰*, é»˜è®¤ä¸º `[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`) â€” ä¸€ä¸ªåµŒå¥—çš„æ•´æ•°å…ƒç»„ï¼Œå®šä¹‰äº†å¤šæ¥å—åŸŸèåˆï¼ˆMRFï¼‰æ¨¡å—ä¸­æ‰©å¼ çš„ 1D å·ç§¯å±‚çš„æ‰©å¼ ç‡ã€‚

+   `initializer_range` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.01) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `leaky_relu_slope` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.1) â€” leaky ReLU æ¿€æ´»å‡½æ•°ä½¿ç”¨çš„è´Ÿæ–œç‡çš„è§’åº¦ã€‚

+   `normalize_before` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨ä½¿ç”¨è¯­éŸ³åˆæˆå™¨çš„å­¦ä¹ å‡å€¼å’Œæ–¹å·®è¿›è¡Œå£°è°±å½’ä¸€åŒ–ä¹‹å‰å¯¹å£°è°±è¿›è¡Œå½’ä¸€åŒ–ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ `FastSpeech2ConformerHifiGanModel` é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª FastSpeech2Conformer HiFi-GAN è¯­éŸ³åˆæˆæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–ä¸€ä¸ªé…ç½®å°†äº§ç”Ÿç±»ä¼¼äº FastSpeech2Conformer [espnet/fastspeech2_conformer_hifigan](https://huggingface.co/espnet/fastspeech2_conformer_hifigan) æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import FastSpeech2ConformerHifiGan, FastSpeech2ConformerHifiGanConfig

>>> # Initializing a FastSpeech2ConformerHifiGan configuration
>>> configuration = FastSpeech2ConformerHifiGanConfig()

>>> # Initializing a model (with random weights) from the configuration
>>> model = FastSpeech2ConformerHifiGan(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## FastSpeech2ConformerWithHifiGanConfig

### `class transformers.FastSpeech2ConformerWithHifiGanConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fastspeech2_conformer/configuration_fastspeech2_conformer.py#L419)

```py
( model_config: Dict = None vocoder_config: Dict = None **kwargs )
```

å‚æ•°

+   `model_config` (`typing.Dict`, *å¯é€‰*) â€” æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹çš„é…ç½®ã€‚

+   `vocoder_config` (`typing.Dict`, *å¯é€‰*) â€” è¯­éŸ³åˆæˆæ¨¡å‹çš„é…ç½®ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ FastSpeech2ConformerWithHifiGan é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å­æ¨¡å‹é…ç½®å®ä¾‹åŒ–ä¸€ä¸ª `FastSpeech2ConformerWithHifiGanModel` æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚

ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–ä¸€ä¸ªé…ç½®å°†äº§ç”Ÿç±»ä¼¼äº FastSpeech2ConformerModel [espnet/fastspeech2_conformer](https://huggingface.co/espnet/fastspeech2_conformer) å’Œ FastSpeech2ConformerHifiGan [espnet/fastspeech2_conformer_hifigan](https://huggingface.co/espnet/fastspeech2_conformer_hifigan) æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

model_configï¼ˆFastSpeech2ConformerConfigï¼Œ*optional*ï¼‰ï¼šæ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹çš„é…ç½®ã€‚vocoder_configï¼ˆ`FastSpeech2ConformerHiFiGanConfig`ï¼Œ*optional*ï¼‰ï¼šå£°ç å™¨æ¨¡å‹çš„é…ç½®ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import (
...     FastSpeech2ConformerConfig,
...     FastSpeech2ConformerHifiGanConfig,
...     FastSpeech2ConformerWithHifiGanConfig,
...     FastSpeech2ConformerWithHifiGan,
... )

>>> # Initializing FastSpeech2ConformerWithHifiGan sub-modules configurations.
>>> model_config = FastSpeech2ConformerConfig()
>>> vocoder_config = FastSpeech2ConformerHifiGanConfig()

>>> # Initializing a FastSpeech2ConformerWithHifiGan module style configuration
>>> configuration = FastSpeech2ConformerWithHifiGanConfig(model_config.to_dict(), vocoder_config.to_dict())

>>> # Initializing a model (with random weights)
>>> model = FastSpeech2ConformerWithHifiGan(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## FastSpeech2ConformerTokenizer

### `class transformers.FastSpeech2ConformerTokenizer`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fastspeech2_conformer/tokenization_fastspeech2_conformer.py#L43)

```py
( vocab_file bos_token = '<sos/eos>' eos_token = '<sos/eos>' pad_token = '<blank>' unk_token = '<unk>' should_strip_spaces = False **kwargs )
```

å‚æ•°

+   `vocab_file` (`str`) â€” è¯æ±‡è¡¨æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `bos_token` (`str`, *optional*, defaults to `"<sos/eos>"`) â€” åºåˆ—å¼€å§‹æ ‡è®°ã€‚è¯·æ³¨æ„ï¼Œå¯¹äº FastSpeech2ï¼Œå®ƒä¸`eos_token`ç›¸åŒã€‚

+   `eos_token` (`str`, *optional*, defaults to `"<sos/eos>"`) â€” åºåˆ—ç»“æŸæ ‡è®°ã€‚è¯·æ³¨æ„ï¼Œå¯¹äº FastSpeech2ï¼Œå®ƒä¸`bos_token`ç›¸åŒã€‚

+   `pad_token` (`str`, *optional*, defaults to `"<blank>"`) â€” ç”¨äºå¡«å……çš„æ ‡è®°ï¼Œä¾‹å¦‚åœ¨æ‰¹å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—æ—¶ä½¿ç”¨ã€‚

+   `unk_token` (`str`, *optional*, defaults to `"<unk>"`) â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­æ²¡æœ‰çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸º IDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚

+   `should_strip_spaces` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦å»é™¤æ ‡è®°åˆ—è¡¨ä¸­çš„ç©ºæ ¼ã€‚

æ„å»ºä¸€ä¸ª FastSpeech2Conformer åˆ†è¯å™¨ã€‚

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)

```py
( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) â†’ export const metadata = 'undefined';BatchEncoding
```

å‚æ•°

+   `text` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç çš„åºåˆ—æˆ–æ‰¹å¤„ç†çš„åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„å…ˆæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœå°†åºåˆ—æä¾›ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„å…ˆæ ‡è®°åŒ–ï¼‰ï¼Œåˆ™å¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹å¤„ç†åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚

+   `text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç çš„åºåˆ—æˆ–æ‰¹å¤„ç†çš„åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„å…ˆæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœå°†åºåˆ—æä¾›ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„å…ˆæ ‡è®°åŒ–ï¼‰ï¼Œåˆ™å¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹å¤„ç†åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚

+   `text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç ä¸ºç›®æ ‡æ–‡æœ¬çš„åºåˆ—æˆ–æ‰¹å¤„ç†çš„åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„å…ˆæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœå°†åºåˆ—æä¾›ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„å…ˆæ ‡è®°åŒ–ï¼‰ï¼Œåˆ™å¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹å¤„ç†åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚

+   `text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç ä¸ºç›®æ ‡æ–‡æœ¬çš„åºåˆ—æˆ–æ‰¹å¤„ç†çš„åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„å…ˆæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœå°†åºåˆ—æä¾›ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„å…ˆæ ‡è®°åŒ–ï¼‰ï¼Œåˆ™å¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹å¤„ç†åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚

+   `add_special_tokens` (`bool`, *optional*, defaults to `True`) â€” åœ¨ç¼–ç åºåˆ—æ—¶æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€‚è¿™å°†ä½¿ç”¨åº•å±‚çš„`PretrainedTokenizerBase.build_inputs_with_special_tokens`å‡½æ•°ï¼Œè¯¥å‡½æ•°å®šä¹‰äº†è‡ªåŠ¨æ·»åŠ åˆ°è¾“å…¥ ID çš„æ ‡è®°ã€‚å¦‚æœè¦è‡ªåŠ¨æ·»åŠ `bos`æˆ–`eos`æ ‡è®°ï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `padding` (`bool`, `str` or PaddingStrategy, *optional*, defaults to `False`) â€” æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š

    +   `True` æˆ– `'longest'`: å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›äº†å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸å¡«å……ï¼‰ã€‚

    +   `'max_length'`: å¡«å……åˆ°ç”±å‚æ•° `max_length` æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚

    +   `False` æˆ– `'do_not_pad'` (é»˜è®¤): ä¸å¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºå…·æœ‰ä¸åŒé•¿åº¦åºåˆ—çš„æ‰¹æ¬¡ï¼‰ã€‚

+   `truncation` (`bool`, `str` æˆ– TruncationStrategy, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š

    +   `True` æˆ– `'longest_first'`: æˆªæ–­åˆ°ç”±å‚æ•° `max_length` æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™é€æ ‡è®°æˆªæ–­ï¼Œä»ä¸€å¯¹åºåˆ—ä¸­æœ€é•¿çš„åºåˆ—ä¸­åˆ é™¤ä¸€ä¸ªæ ‡è®°ã€‚

    +   `'only_first'`: æˆªæ–­åˆ°ç”±å‚æ•° `max_length` æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™ä»…æˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚

    +   `'only_second'`: æˆªæ–­åˆ°ç”±å‚æ•° `max_length` æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™ä»…æˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚

    +   `False` æˆ– `'do_not_truncate'` (é»˜è®¤): ä¸æˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºåºåˆ—é•¿åº¦å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„æ‰¹æ¬¡ï¼‰ã€‚

+   `max_length` (`int`, *å¯é€‰*) â€” æ§åˆ¶æˆªæ–­/å¡«å……å‚æ•°ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚

    å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º `None`ï¼Œåˆ™å¦‚æœæˆªæ–­/å¡«å……å‚æ•°ä¸­éœ€è¦æœ€å¤§é•¿åº¦ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚ XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚

+   `stride` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0) â€” å¦‚æœä¸ `max_length` ä¸€èµ·è®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œåˆ™å½“ `return_overflowing_tokens=True` æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«æˆªæ–­åºåˆ—æœ«å°¾çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚è¯¥å‚æ•°çš„å€¼å®šä¹‰äº†é‡å æ ‡è®°çš„æ•°é‡ã€‚

+   `is_split_into_words` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” è¾“å…¥æ˜¯å¦å·²ç»é¢„åˆ†è¯ï¼ˆä¾‹å¦‚ï¼Œå·²åˆ†å‰²ä¸ºå•è¯ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸º `True`ï¼Œåˆ†è¯å™¨å°†å‡å®šè¾“å…¥å·²ç»åˆ†å‰²ä¸ºå•è¯ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡åœ¨ç©ºæ ¼ä¸Šåˆ†å‰²ï¼‰ï¼Œç„¶åå¯¹å…¶è¿›è¡Œåˆ†è¯ã€‚è¿™å¯¹äºå‘½åå®ä½“è¯†åˆ«æˆ–æ ‡è®°åˆ†ç±»å¾ˆæœ‰ç”¨ã€‚

+   `pad_to_multiple_of` (`int`, *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†åºåˆ—å¡«å……åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚éœ€è¦æ¿€æ´» `padding`ã€‚è¿™å¯¹äºåœ¨å…·æœ‰è®¡ç®—èƒ½åŠ› `>= 7.5`ï¼ˆVoltaï¼‰çš„ NVIDIA ç¡¬ä»¶ä¸Šå¯ç”¨ Tensor Cores ç‰¹åˆ«æœ‰ç”¨ã€‚

+   `return_tensors` (`str` æˆ– TensorType, *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯ Python æ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š

    +   `'tf'`: è¿”å› TensorFlow `tf.constant` å¯¹è±¡ã€‚

    +   `'pt'`: è¿”å› PyTorch `torch.Tensor` å¯¹è±¡ã€‚

    +   `'np'`: è¿”å› Numpy `np.ndarray` å¯¹è±¡ã€‚

+   `return_token_type_ids` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ ‡è®°ç±»å‹ IDã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤è®¾ç½®è¿”å›æ ‡è®°ç±»å‹ IDï¼Œç”± `return_outputs` å±æ€§å®šä¹‰ã€‚

    ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

+   `return_attention_mask` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ³¨æ„åŠ›é®ç½©ã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤è®¾ç½®è¿”å›æ³¨æ„åŠ›é®ç½©ï¼Œç”± `return_outputs` å±æ€§å®šä¹‰ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `return_overflowing_tokens` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¿”å›æº¢å‡ºçš„ä»¤ç‰Œåºåˆ—ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹è¾“å…¥ ID åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰ï¼Œå¹¶ä¸”`truncation_strategy = longest_first`æˆ–`True`ï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ï¼Œè€Œä¸æ˜¯è¿”å›æº¢å‡ºçš„ä»¤ç‰Œã€‚

+   `return_special_tokens_mask` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¿”å›ç‰¹æ®Šä»¤ç‰Œæ©ç ä¿¡æ¯ã€‚

+   `return_offsets_mapping` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¿”å›æ¯ä¸ªä»¤ç‰Œçš„`(char_startï¼Œchar_end)`ã€‚

    ä»…é€‚ç”¨äºç»§æ‰¿è‡ª PreTrainedTokenizerFast çš„å¿«é€Ÿåˆ†è¯å™¨ï¼Œå¦‚æœä½¿ç”¨ Python çš„åˆ†è¯å™¨ï¼Œæ­¤æ–¹æ³•å°†å¼•å‘`NotImplementedError`ã€‚

+   `return_length` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¿”å›ç¼–ç è¾“å…¥çš„é•¿åº¦ã€‚

+   `verbose` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`) â€” æ˜¯å¦æ‰“å°æ›´å¤šä¿¡æ¯å’Œè­¦å‘Šã€‚**kwargs â€” ä¼ é€’ç»™`self.tokenize()`æ–¹æ³•

è¿”å›

BatchEncoding

å…·æœ‰ä»¥ä¸‹å­—æ®µçš„ BatchEncodingï¼š

+   `input_ids` â€” è¦é¦ˆé€åˆ°æ¨¡å‹çš„ä»¤ç‰Œ ID åˆ—è¡¨ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `token_type_ids` â€” è¦é¦ˆé€åˆ°æ¨¡å‹çš„ä»¤ç‰Œç±»å‹ ID åˆ—è¡¨ï¼ˆå½“`return_token_type_ids=True`æˆ–*`token_type_ids`*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚

    ä»€ä¹ˆæ˜¯ä»¤ç‰Œç±»å‹ IDï¼Ÿ

+   `attention_mask` â€” æŒ‡å®šå“ªäº›ä»¤ç‰Œåº”ç”±æ¨¡å‹å…³æ³¨çš„ç´¢å¼•åˆ—è¡¨ï¼ˆå½“`return_attention_mask=True`æˆ–*`attention_mask`*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `overflowing_tokens` â€” æº¢å‡ºä»¤ç‰Œåºåˆ—çš„åˆ—è¡¨ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚

+   `num_truncated_tokens` â€” æˆªæ–­çš„ä»¤ç‰Œæ•°ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚

+   `special_tokens_mask` â€” ç”± 0 å’Œ 1 ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­ 1 æŒ‡å®šæ·»åŠ çš„ç‰¹æ®Šä»¤ç‰Œï¼Œ0 æŒ‡å®šå¸¸è§„åºåˆ—ä»¤ç‰Œï¼ˆå½“`add_special_tokens=True`å’Œ`return_special_tokens_mask=True`æ—¶ï¼‰ã€‚

+   `length` â€” è¾“å…¥çš„é•¿åº¦ï¼ˆå½“`return_length=True`æ—¶ï¼‰

å¯¹ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—æˆ–ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—å¯¹è¿›è¡Œæ ‡è®°åŒ–å’Œå‡†å¤‡æ¨¡å‹çš„ä¸»è¦æ–¹æ³•ã€‚

#### `save_vocabulary`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fastspeech2_conformer/tokenization_fastspeech2_conformer.py#L159)

```py
( save_directory: str filename_prefix: Optional = None ) â†’ export const metadata = 'undefined';Tuple(str)
```

å‚æ•°

+   `save_directory`ï¼ˆ`str`ï¼‰ â€” è¦ä¿å­˜è¯æ±‡è¡¨çš„ç›®å½•ã€‚

è¿”å›

`Tuple(str)`

ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ã€‚

å°†è¯æ±‡è¡¨å’Œç‰¹æ®Šä»¤ç‰Œæ–‡ä»¶ä¿å­˜åˆ°ç›®å½•ã€‚

#### `decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fastspeech2_conformer/tokenization_fastspeech2_conformer.py#L146)

```py
( token_ids **kwargs )
```

#### `batch_decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)

```py
( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) â†’ export const metadata = 'undefined';List[str]
```

å‚æ•°

+   `sequences`ï¼ˆ`Union[List[int]ï¼ŒList[List[int]]ï¼Œnp.ndarrayï¼Œtorch.Tensorï¼Œtf.Tensor]`ï¼‰ â€” æ ‡è®°åŒ–è¾“å…¥ ID çš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚

+   `skip_special_tokens` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šä»¤ç‰Œã€‚

+   `clean_up_tokenization_spaces` (`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦æ¸…ç†æ ‡è®°åŒ–ç©ºæ ¼ã€‚å¦‚æœä¸º`None`ï¼Œå°†é»˜è®¤ä¸º`self.clean_up_tokenization_spaces`ã€‚

+   `kwargs`ï¼ˆé™„åŠ å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰ â€” å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚

è¿”å›

`List[str]`

è§£ç å¥å­åˆ—è¡¨ã€‚

é€šè¿‡è°ƒç”¨è§£ç å°†ä»¤ç‰Œ ID åˆ—è¡¨çš„åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ã€‚

## FastSpeech2ConformerModel

### `class transformers.FastSpeech2ConformerModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py#L1100)

```py
( config: FastSpeech2ConformerConfig )
```

å‚æ•°

+   `config`ï¼ˆFastSpeech2ConformerConfigï¼‰- å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

FastSpeech2Conformer æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¯¥æ¨¡å‹ä¹Ÿæ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

FastSpeech 2 æ¨¡å—ã€‚

è¿™æ˜¯ FastSpeech 2 ä¸­æè¿°çš„æ¨¡å—ï¼Œè¯¦è§â€˜FastSpeech 2: Fast and High-Quality End-to-End Text to Speechâ€™ [`arxiv.org/abs/2006.04558`](https://arxiv.org/abs/2006.04558)ã€‚æˆ‘ä»¬ä½¿ç”¨ FastPitch ä¸­å¼•å…¥çš„ä»¤ç‰Œå¹³å‡å€¼ï¼Œè€Œä¸æ˜¯é‡åŒ–éŸ³é«˜å’Œèƒ½é‡ã€‚ç¼–ç å™¨å’Œè§£ç å™¨æ˜¯ Conformers è€Œä¸æ˜¯å¸¸è§„ Transformersã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py#L1181)

```py
( input_ids: LongTensor attention_mask: Optional = None spectrogram_labels: Optional = None duration_labels: Optional = None pitch_labels: Optional = None energy_labels: Optional = None speaker_ids: Optional = None lang_ids: Optional = None speaker_embedding: Optional = None return_dict: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.fastspeech2_conformer.modeling_fastspeech2_conformer.FastSpeech2ConformerModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰- æ–‡æœ¬å‘é‡çš„è¾“å…¥åºåˆ—ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰- é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©åœ¨`[0, 1]`ä¸­çš„æ©ç å€¼ï¼š0 è¡¨ç¤º**æ©ç **çš„æ ‡è®°ï¼Œ1 è¡¨ç¤º**æœªæ©ç **çš„æ ‡è®°ã€‚

+   `spectrogram_labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, max_spectrogram_length, num_mel_bins)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰- å¡«å……ç›®æ ‡ç‰¹å¾çš„æ‰¹æ¬¡ã€‚

+   `duration_labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length + 1)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰- å¡«å……çš„æŒç»­æ—¶é—´æ‰¹æ¬¡ã€‚

+   `pitch_labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length + 1, 1)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰- å¡«å……çš„ä»¤ç‰Œå¹³å‡éŸ³é«˜æ‰¹æ¬¡ã€‚

+   `energy_labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length + 1, 1)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰- å¡«å……çš„ä»¤ç‰Œå¹³å‡èƒ½é‡ã€‚

+   `speaker_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, 1)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰- ç”¨äºè°ƒèŠ‚æ¨¡å‹è¾“å‡ºè¯­éŸ³ç‰¹å¾çš„è¯´è¯è€… idã€‚

+   `lang_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, 1)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰- ç”¨äºè°ƒèŠ‚æ¨¡å‹è¾“å‡ºè¯­éŸ³ç‰¹å¾çš„è¯­è¨€ idã€‚

+   `speaker_embedding`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, embedding_dim)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰- åŒ…å«è¯­éŸ³ç‰¹å¾çš„è°ƒèŠ‚ä¿¡å·çš„åµŒå…¥ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰- æ˜¯å¦è¿”å›`FastSpeech2ConformerModelOutput`è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None`ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

è¿”å›

`transformers.models.fastspeech2_conformer.modeling_fastspeech2_conformer.FastSpeech2ConformerModelOutput`æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª`transformers.models.fastspeech2_conformer.modeling_fastspeech2_conformer.FastSpeech2ConformerModelOutput`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆFastSpeech2ConformerConfigï¼‰å’Œè¾“å…¥ã€‚

+   `loss`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” é¢‘è°±å›¾ç”ŸæˆæŸå¤±ã€‚

+   `spectrogram`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, num_bins)`ï¼‰â€” é¢„æµ‹çš„é¢‘è°±å›¾ã€‚

+   `encoder_last_hidden_state`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„ä¸€ä¸ªï¼ŒåŠ ä¸Šæ¯ä¸€å±‚çš„è¾“å‡ºçš„ä¸€ä¸ªï¼‰ã€‚

    ç¼–ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„ä¸€ä¸ªï¼ŒåŠ ä¸Šæ¯ä¸€å±‚çš„è¾“å‡ºçš„ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `duration_outputs`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, max_text_length + 1)`ï¼Œ*å¯é€‰*ï¼‰â€” æ—¶é•¿é¢„æµ‹å™¨çš„è¾“å‡ºã€‚

+   `pitch_outputs`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, max_text_length + 1, 1)`ï¼Œ*å¯é€‰*ï¼‰â€” éŸ³é«˜é¢„æµ‹å™¨çš„è¾“å‡ºã€‚

+   `energy_outputs`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, max_text_length + 1, 1)`ï¼Œ*å¯é€‰*ï¼‰â€” èƒ½é‡é¢„æµ‹å™¨çš„è¾“å‡ºã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import (
...     FastSpeech2ConformerTokenizer,
...     FastSpeech2ConformerModel,
...     FastSpeech2ConformerHifiGan,
... )

>>> tokenizer = FastSpeech2ConformerTokenizer.from_pretrained("espnet/fastspeech2_conformer")
>>> inputs = tokenizer("some text to convert to speech", return_tensors="pt")
>>> input_ids = inputs["input_ids"]

>>> model = FastSpeech2ConformerModel.from_pretrained("espnet/fastspeech2_conformer")
>>> output_dict = model(input_ids, return_dict=True)
>>> spectrogram = output_dict["spectrogram"]

>>> vocoder = FastSpeech2ConformerHifiGan.from_pretrained("espnet/fastspeech2_conformer_hifigan")
>>> waveform = vocoder(spectrogram)
>>> print(waveform.shape)
torch.Size([1, 49664])
```

## FastSpeech2ConformerHifiGan

### `class transformers.FastSpeech2ConformerHifiGan`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py#L1446)

```py
( config: FastSpeech2ConformerHifiGanConfig )
```

å‚æ•°

+   `config`ï¼ˆFastSpeech2ConformerConfigï¼‰â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

HiFi-GAN è¯­éŸ³åˆæˆå™¨ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py#L1516)

```py
( spectrogram: FloatTensor ) â†’ export const metadata = 'undefined';torch.FloatTensor
```

å‚æ•°

+   `spectrogram` (`torch.FloatTensor`) â€” åŒ…å«å¯¹æ•°æ¢…å°”é¢‘è°±å›¾çš„å¼ é‡ã€‚å¯ä»¥æ˜¯æ‰¹é‡å¤„ç†çš„ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, config.model_in_dim)`ï¼Œä¹Ÿå¯ä»¥æ˜¯æœªæ‰¹é‡å¤„ç†çš„ï¼Œå½¢çŠ¶ä¸º `(sequence_length, config.model_in_dim)`ã€‚

è¿”å›

`torch.FloatTensor`

åŒ…å«è¯­éŸ³æ³¢å½¢çš„å¼ é‡ã€‚å¦‚æœè¾“å…¥çš„é¢‘è°±å›¾æ˜¯æ‰¹é‡å¤„ç†çš„ï¼Œåˆ™å½¢çŠ¶ä¸º `(batch_size, num_frames,)`ã€‚å¦‚æœæœªæ‰¹é‡å¤„ç†ï¼Œåˆ™å½¢çŠ¶ä¸º `(num_frames,)`ã€‚

å°†å¯¹æ•°æ¢…å°”é¢‘è°±å›¾è½¬æ¢ä¸ºè¯­éŸ³æ³¢å½¢ã€‚ä¼ é€’ä¸€æ‰¹å¯¹æ•°æ¢…å°”é¢‘è°±å›¾å°†è¿”å›ä¸€æ‰¹è¯­éŸ³æ³¢å½¢ã€‚ä¼ é€’å•ä¸ªæœªæ‰¹é‡å¤„ç†çš„å¯¹æ•°æ¢…å°”é¢‘è°±å›¾å°†è¿”å›å•ä¸ªæœªæ‰¹é‡å¤„ç†çš„è¯­éŸ³æ³¢å½¢ã€‚

## FastSpeech2ConformerWithHifiGan

### `class transformers.FastSpeech2ConformerWithHifiGan`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py#L1564)

```py
( config: FastSpeech2ConformerWithHifiGanConfig )
```

å‚æ•°

+   `config` (FastSpeech2ConformerWithHifiGanConfig) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained() æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

FastSpeech2ConformerModel å…·æœ‰ FastSpeech2ConformerHifiGan è¯­éŸ³åˆæˆå™¨å¤´éƒ¨ï¼Œæ‰§è¡Œæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆæ³¢å½¢ï¼‰ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py#L1579)

```py
( input_ids: LongTensor attention_mask: Optional = None spectrogram_labels: Optional = None duration_labels: Optional = None pitch_labels: Optional = None energy_labels: Optional = None speaker_ids: Optional = None lang_ids: Optional = None speaker_embedding: Optional = None return_dict: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.fastspeech2_conformer.modeling_fastspeech2_conformer.FastSpeech2ConformerWithHifiGanOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€” æ–‡æœ¬å‘é‡çš„è¾“å…¥åºåˆ—ã€‚

+   `attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*, defaults to `None`) â€” é¿å…åœ¨å¡«å……ä»¤ç‰Œç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨ `[0, 1]`ï¼š0 è¡¨ç¤º**æ©ç **çš„ä»¤ç‰Œï¼Œ1 è¡¨ç¤º**æœªæ©ç **çš„ä»¤ç‰Œã€‚

+   `spectrogram_labels` (`torch.FloatTensor` of shape `(batch_size, max_spectrogram_length, num_mel_bins)`, *optional*, defaults to `None`) â€” æ‰¹é‡å¡«å……çš„ç›®æ ‡ç‰¹å¾ã€‚

+   `duration_labels` (`torch.LongTensor` of shape `(batch_size, sequence_length + 1)`, *optional*, defaults to `None`) â€” æ‰¹é‡å¡«å……çš„æŒç»­æ—¶é—´ã€‚

+   `pitch_labels` (`torch.FloatTensor` of shape `(batch_size, sequence_length + 1, 1)`, *optional*, defaults to `None`) â€” æ‰¹é‡å¡«å……çš„ä»¤ç‰Œå¹³å‡éŸ³é«˜ã€‚

+   `energy_labels` (`torch.FloatTensor` of shape `(batch_size, sequence_length + 1, 1)`, *optional*, defaults to `None`) â€” å¡«å……çš„ä»¤ç‰Œå¹³å‡èƒ½é‡çš„æ‰¹é‡ã€‚

+   `speaker_ids` (`torch.LongTensor` of shape `(batch_size, 1)`, *optional*, defaults to `None`) â€” ç”¨äºè°ƒèŠ‚æ¨¡å‹è¾“å‡ºè¯­éŸ³ç‰¹å¾çš„è¯´è¯è€… idã€‚

+   `lang_ids` (`torch.LongTensor` of shape `(batch_size, 1)`, *optional*, defaults to `None`) â€” ç”¨äºè°ƒèŠ‚æ¨¡å‹è¾“å‡ºè¯­éŸ³ç‰¹å¾çš„è¯­è¨€ idã€‚

+   `speaker_embedding` (`torch.FloatTensor` of shape `(batch_size, embedding_dim)`, *optional*, defaults to `None`) â€” åŒ…å«è¯­éŸ³ç‰¹å¾è°ƒèŠ‚ä¿¡å·çš„åµŒå…¥ã€‚

+   `return_dict` (`bool`, *optional*, defaults to `None`) â€” æ˜¯å¦è¿”å›`FastSpeech2ConformerModelOutput`è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `output_attentions` (`bool`, *optional*, defaults to `None`) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*, defaults to `None`) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

è¿”å›

`transformers.models.fastspeech2_conformer.modeling_fastspeech2_conformer.FastSpeech2ConformerWithHifiGanOutput`æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª`transformers.models.fastspeech2_conformer.modeling_fastspeech2_conformer.FastSpeech2ConformerWithHifiGanOutput`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼Œåˆ™è¿”å›ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ`<class 'transformers.models.fastspeech2_conformer.configuration_fastspeech2_conformer.FastSpeech2ConformerWithHifiGanConfig'>`ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `waveform` (`torch.FloatTensor` of shape `(batch_size, audio_length)`) â€” é€šè¿‡å£°ç å™¨å°†é¢„æµ‹çš„æ¢…å°”é¢‘è°±å›¾ä¼ é€’åçš„è¯­éŸ³è¾“å‡ºã€‚

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾›`labels`æ—¶è¿”å›) â€” é¢‘è°±å›¾ç”ŸæˆæŸå¤±ã€‚

+   `spectrogram` (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_bins)`) â€” é¢„æµ‹çš„é¢‘è°±å›¾ã€‚

+   `encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    ç¼–ç å™¨æ¯å±‚çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    æ¯å±‚è§£ç å™¨çš„è¾“å‡ºéšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `duration_outputs` (`torch.LongTensor` of shape `(batch_size, max_text_length + 1)`, *optional*) â€” æŒç»­æ—¶é—´é¢„æµ‹å™¨çš„è¾“å‡ºã€‚

+   `pitch_outputs` (`torch.FloatTensor` of shape `(batch_size, max_text_length + 1, 1)`, *optional*) â€” éŸ³é«˜é¢„æµ‹å™¨çš„è¾“å‡ºã€‚

+   `energy_outputs` (`torch.FloatTensor` of shape `(batch_size, max_text_length + 1, 1)`, *optional*) â€” èƒ½é‡é¢„æµ‹å™¨çš„è¾“å‡ºã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import (
...     FastSpeech2ConformerTokenizer,
...     FastSpeech2ConformerWithHifiGan,
... )

>>> tokenizer = FastSpeech2ConformerTokenizer.from_pretrained("espnet/fastspeech2_conformer")
>>> inputs = tokenizer("some text to convert to speech", return_tensors="pt")
>>> input_ids = inputs["input_ids"]

>>> model = FastSpeech2ConformerWithHifiGan.from_pretrained("espnet/fastspeech2_conformer_with_hifigan")
>>> output_dict = model(input_ids, return_dict=True)
>>> waveform = output_dict["waveform"]
>>> print(waveform.shape)
torch.Size([1, 49664])
```
