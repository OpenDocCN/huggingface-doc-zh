- en: What ğŸ¤— Transformers can do
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¤— Transformers èƒ½åšä»€ä¹ˆ
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/task_summary](https://huggingface.co/docs/transformers/v4.37.2/en/task_summary)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/task_summary](https://huggingface.co/docs/transformers/v4.37.2/en/task_summary)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ¤— Transformers is a library of pretrained state-of-the-art models for natural
    language processing (NLP), computer vision, and audio and speech processing tasks.
    Not only does the library contain Transformer models, but it also has non-Transformer
    models like modern convolutional networks for computer vision tasks. If you look
    at some of the most popular consumer products today, like smartphones, apps, and
    televisions, odds are that some kind of deep learning technology is behind it.
    Want to remove a background object from a picture taken by your smartphone? This
    is an example of a panoptic segmentation task (donâ€™t worry if you donâ€™t know what
    this means yet, weâ€™ll describe it in the following sections!).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Transformers æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„æœ€å…ˆè¿›æ¨¡å‹åº“ï¼Œç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€è®¡ç®—æœºè§†è§‰ä»¥åŠéŸ³é¢‘å’Œè¯­éŸ³å¤„ç†ä»»åŠ¡ã€‚è¿™ä¸ªåº“ä¸ä»…åŒ…å«äº†Transformeræ¨¡å‹ï¼Œè¿˜æœ‰åƒç°ä»£å·ç§¯ç½‘ç»œè¿™æ ·çš„éTransformeræ¨¡å‹ï¼Œç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚å¦‚æœä½ çœ‹ä¸€ä¸‹ä»Šå¤©æœ€æµè¡Œçš„æ¶ˆè´¹äº§å“ï¼Œæ¯”å¦‚æ™ºèƒ½æ‰‹æœºã€åº”ç”¨å’Œç”µè§†ï¼Œå¾ˆå¯èƒ½èƒŒåéƒ½æœ‰æŸç§æ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚æƒ³è¦ä»æ™ºèƒ½æ‰‹æœºæ‹æ‘„çš„ç…§ç‰‡ä¸­ç§»é™¤èƒŒæ™¯ç‰©ä½“ï¼Ÿè¿™å°±æ˜¯ä¸€ä¸ªå…¨æ™¯åˆ†å‰²ä»»åŠ¡çš„ä¾‹å­ï¼ˆå¦‚æœä½ è¿˜ä¸çŸ¥é“è¿™æ˜¯ä»€ä¹ˆï¼Œä¸ç”¨æ‹…å¿ƒï¼Œæˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­æè¿°ï¼ï¼‰ã€‚
- en: This page provides an overview of the different speech and audio, computer vision,
    and NLP tasks that can be solved with the ğŸ¤— Transformers library in just three
    lines of code!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé¡µé¢æä¾›äº†å…³äºğŸ¤— Transformersåº“ä¸­å¯ä»¥ç”¨ä¸‰è¡Œä»£ç è§£å†³çš„ä¸åŒè¯­éŸ³å’ŒéŸ³é¢‘ã€è®¡ç®—æœºè§†è§‰å’ŒNLPä»»åŠ¡çš„æ¦‚è¿°ï¼
- en: Audio
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éŸ³é¢‘
- en: Audio and speech processing tasks are a little different from the other modalities
    mainly because audio as an input is a continuous signal. Unlike text, a raw audio
    waveform canâ€™t be neatly split into discrete chunks the way a sentence can be
    divided into words. To get around this, the raw audio signal is typically sampled
    at regular intervals. If you take more samples within an interval, the sampling
    rate is higher, and the audio more closely resembles the original audio source.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: éŸ³é¢‘å’Œè¯­éŸ³å¤„ç†ä»»åŠ¡ä¸å…¶ä»–æ¨¡æ€æœ‰äº›ä¸åŒï¼Œä¸»è¦æ˜¯å› ä¸ºéŸ³é¢‘ä½œä¸ºè¾“å…¥æ˜¯ä¸€ä¸ªè¿ç»­ä¿¡å·ã€‚ä¸æ–‡æœ¬ä¸åŒï¼ŒåŸå§‹éŸ³é¢‘æ³¢å½¢ä¸èƒ½åƒå¥å­å¯ä»¥è¢«åˆ†æˆå•è¯é‚£æ ·æ•´é½åœ°åˆ†å‰²ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒåŸå§‹éŸ³é¢‘ä¿¡å·é€šå¸¸ä»¥å›ºå®šé—´éš”è¿›è¡Œé‡‡æ ·ã€‚å¦‚æœåœ¨ä¸€ä¸ªé—´éš”å†…å–æ›´å¤šæ ·æœ¬ï¼Œé‡‡æ ·ç‡å°±æ›´é«˜ï¼ŒéŸ³é¢‘æ›´æ¥è¿‘åŸå§‹éŸ³é¢‘æºã€‚
- en: Previous approaches preprocessed the audio to extract useful features from it.
    It is now more common to start audio and speech processing tasks by directly feeding
    the raw audio waveform to a feature encoder to extract an audio representation.
    This simplifies the preprocessing step and allows the model to learn the most
    essential features.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥å‰çš„æ–¹æ³•æ˜¯å¯¹éŸ³é¢‘è¿›è¡Œé¢„å¤„ç†ï¼Œä»ä¸­æå–æœ‰ç”¨çš„ç‰¹å¾ã€‚ç°åœ¨æ›´å¸¸è§çš„åšæ³•æ˜¯ç›´æ¥å°†åŸå§‹éŸ³é¢‘æ³¢å½¢è¾“å…¥ç‰¹å¾ç¼–ç å™¨ï¼Œä»¥æå–éŸ³é¢‘è¡¨ç¤ºï¼Œå¼€å§‹éŸ³é¢‘å’Œè¯­éŸ³å¤„ç†ä»»åŠ¡ã€‚è¿™ç®€åŒ–äº†é¢„å¤„ç†æ­¥éª¤ï¼Œå¹¶å…è®¸æ¨¡å‹å­¦ä¹ æœ€å…³é”®çš„ç‰¹å¾ã€‚
- en: Audio classification
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: éŸ³é¢‘åˆ†ç±»
- en: 'Audio classification is a task that labels audio data from a predefined set
    of classes. It is a broad category with many specific applications, some of which
    include:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: éŸ³é¢‘åˆ†ç±»æ˜¯ä¸€ä¸ªä»é¢„å®šä¹‰ç±»åˆ«é›†ä¸­ä¸ºéŸ³é¢‘æ•°æ®è´´ä¸Šæ ‡ç­¾çš„ä»»åŠ¡ã€‚è¿™æ˜¯ä¸€ä¸ªå¹¿æ³›çš„ç±»åˆ«ï¼Œæœ‰è®¸å¤šå…·ä½“åº”ç”¨ï¼Œå…¶ä¸­ä¸€äº›åŒ…æ‹¬ï¼š
- en: 'acoustic scene classification: label audio with a scene label (â€œofficeâ€, â€œbeachâ€,
    â€œstadiumâ€)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å£°å­¦åœºæ™¯åˆ†ç±»ï¼šä¸ºéŸ³é¢‘è´´ä¸Šåœºæ™¯æ ‡ç­¾ï¼ˆâ€œåŠå…¬å®¤â€ï¼Œâ€œæµ·æ»©â€ï¼Œâ€œä½“è‚²åœºâ€ï¼‰
- en: 'acoustic event detection: label audio with a sound event label (â€œcar hornâ€,
    â€œwhale callingâ€, â€œglass breakingâ€)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å£°å­¦äº‹ä»¶æ£€æµ‹ï¼šä¸ºéŸ³é¢‘è´´ä¸Šå£°éŸ³äº‹ä»¶æ ‡ç­¾ï¼ˆâ€œæ±½è½¦å–‡å­â€ï¼Œâ€œé²¸é±¼å‘¼å«â€ï¼Œâ€œç»ç’ƒç ´ç¢â€ï¼‰
- en: 'tagging: label audio containing multiple sounds (birdsongs, speaker identification
    in a meeting)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ‡è®°ï¼šä¸ºåŒ…å«å¤šä¸ªå£°éŸ³çš„éŸ³é¢‘è´´ä¸Šæ ‡ç­¾ï¼ˆé¸Ÿé¸£å£°ï¼Œä¼šè®®ä¸­çš„å‘è¨€äººè¯†åˆ«ï¼‰
- en: 'music classification: label music with a genre label (â€œmetalâ€, â€œhip-hopâ€, â€œcountryâ€)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éŸ³ä¹åˆ†ç±»ï¼šä¸ºéŸ³ä¹è´´ä¸Šæµæ´¾æ ‡ç­¾ï¼ˆâ€œé‡‘å±â€ï¼Œâ€œå˜»å“ˆâ€ï¼Œâ€œä¹¡æ‘â€ï¼‰
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Automatic speech recognition
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ªåŠ¨è¯­éŸ³è¯†åˆ«
- en: Automatic speech recognition (ASR) transcribes speech into text. It is one of
    the most common audio tasks due partly to speech being such a natural form of
    human communication. Today, ASR systems are embedded in â€œsmartâ€ technology products
    like speakers, phones, and cars. We can ask our virtual assistants to play music,
    set reminders, and tell us the weather.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å°†è¯­éŸ³è½¬å½•ä¸ºæ–‡æœ¬ã€‚ç”±äºè¯­éŸ³æ˜¯ä¸€ç§è‡ªç„¶çš„äººç±»äº¤æµå½¢å¼ï¼Œå®ƒæ˜¯æœ€å¸¸è§çš„éŸ³é¢‘ä»»åŠ¡ä¹‹ä¸€ã€‚ä»Šå¤©ï¼ŒASRç³»ç»ŸåµŒå…¥åœ¨â€œæ™ºèƒ½â€æŠ€æœ¯äº§å“ä¸­ï¼Œå¦‚æ‰¬å£°å™¨ã€æ‰‹æœºå’Œæ±½è½¦ã€‚æˆ‘ä»¬å¯ä»¥è¦æ±‚è™šæ‹ŸåŠ©æ‰‹æ’­æ”¾éŸ³ä¹ï¼Œè®¾ç½®æé†’ï¼Œå¹¶å‘Šè¯‰æˆ‘ä»¬å¤©æ°”ã€‚
- en: But one of the key challenges Transformer architectures have helped with is
    in low-resource languages. By pretraining on large amounts of speech data, finetuning
    the model on only one hour of labeled speech data in a low-resource language can
    still produce high-quality results compared to previous ASR systems trained on
    100x more labeled data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†Transformeræ¶æ„å¸®åŠ©è§£å†³çš„å…³é”®æŒ‘æˆ˜ä¹‹ä¸€æ˜¯ä½èµ„æºè¯­è¨€ã€‚é€šè¿‡åœ¨å¤§é‡è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨ä½èµ„æºè¯­è¨€ä¸­ä»…å¯¹ä¸€ä¸ªå°æ—¶çš„æ ‡è®°è¯­éŸ³æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä»ç„¶å¯ä»¥äº§ç”Ÿä¸ä¹‹å‰åœ¨100å€æ›´å¤šæ ‡è®°æ•°æ®ä¸Šè®­ç»ƒçš„ASRç³»ç»Ÿç›¸æ¯”çš„é«˜è´¨é‡ç»“æœã€‚
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Computer vision
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¡ç®—æœºè§†è§‰
- en: One of the first and earliest successful computer vision tasks was recognizing
    images of zip code numbers using a [convolutional neural network (CNN)](glossary#convolution).
    An image is composed of pixels, and each pixel has a numerical value. This makes
    it easy to represent an image as a matrix of pixel values. Each particular combination
    of pixel values describes the colors of an image.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€æ—©æˆåŠŸçš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¹‹ä¸€æ˜¯ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¯†åˆ«é‚®æ”¿ç¼–ç æ•°å­—çš„å›¾åƒã€‚ä¸€å¹…å›¾åƒç”±åƒç´ ç»„æˆï¼Œæ¯ä¸ªåƒç´ éƒ½æœ‰ä¸€ä¸ªæ•°å€¼ã€‚è¿™ä½¿å¾—å°†å›¾åƒè¡¨ç¤ºä¸ºåƒç´ å€¼çŸ©é˜µå˜å¾—å®¹æ˜“ã€‚æ¯ä¸ªåƒç´ å€¼ç»„åˆæè¿°äº†å›¾åƒçš„é¢œè‰²ã€‚
- en: 'Two general ways computer vision tasks can be solved are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—æœºè§†è§‰ä»»åŠ¡å¯ä»¥é€šè¿‡ä¸¤ç§ä¸€èˆ¬æ–¹å¼è§£å†³ï¼š
- en: Use convolutions to learn the hierarchical features of an image from low-level
    features to high-level abstract things.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å·ç§¯æ¥å­¦ä¹ å›¾åƒçš„å±‚æ¬¡ç‰¹å¾ï¼Œä»ä½çº§ç‰¹å¾åˆ°é«˜çº§æŠ½è±¡äº‹ç‰©ã€‚
- en: Split an image into patches and use a Transformer to gradually learn how each
    image patch is related to each other to form an image. Unlike the bottom-up approach
    favored by a CNN, this is kind of like starting out with a blurry image and then
    gradually bringing it into focus.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†å›¾åƒåˆ†å‰²æˆè¡¥ä¸ï¼Œå¹¶ä½¿ç”¨Transformeré€æ¸å­¦ä¹ æ¯ä¸ªå›¾åƒè¡¥ä¸å¦‚ä½•ç›¸äº’å…³è”ä»¥å½¢æˆå›¾åƒã€‚ä¸CNNæ‰€é’ççš„è‡ªä¸‹è€Œä¸Šæ–¹æ³•ä¸åŒï¼Œè¿™æœ‰ç‚¹åƒä»æ¨¡ç³Šçš„å›¾åƒå¼€å§‹ï¼Œç„¶åé€æ¸ä½¿å…¶èšç„¦ã€‚
- en: Image classification
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ†ç±»
- en: 'Image classification labels an entire image from a predefined set of classes.
    Like most classification tasks, there are many practical use cases for image classification,
    some of which include:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ†ç±»ä»é¢„å®šä¹‰ç±»åˆ«é›†åˆä¸­ä¸ºæ•´ä¸ªå›¾åƒæ ‡è®°ã€‚ä¸å¤§å¤šæ•°åˆ†ç±»ä»»åŠ¡ä¸€æ ·ï¼Œå›¾åƒåˆ†ç±»æœ‰è®¸å¤šå®é™…ç”¨ä¾‹ï¼Œå…¶ä¸­ä¸€äº›åŒ…æ‹¬ï¼š
- en: 'healthcare: label medical images to detect disease or monitor patient health'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒ»ç–—ä¿å¥ï¼šæ ‡è®°åŒ»å­¦å›¾åƒä»¥æ£€æµ‹ç–¾ç—…æˆ–ç›‘æµ‹æ‚£è€…å¥åº·
- en: 'environment: label satellite images to monitor deforestation, inform wildland
    management or detect wildfires'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¯å¢ƒï¼šæ ‡è®°å«æ˜Ÿå›¾åƒä»¥ç›‘æµ‹æ£®æ—ç ä¼ï¼Œé€šçŸ¥é‡åœ°ç®¡ç†æˆ–æ£€æµ‹é‡ç«
- en: 'agriculture: label images of crops to monitor plant health or satellite images
    for land use monitoring'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†œä¸šï¼šæ ‡è®°ä½œç‰©å›¾åƒä»¥ç›‘æµ‹æ¤ç‰©å¥åº·æˆ–å«æ˜Ÿå›¾åƒç”¨äºåœŸåœ°åˆ©ç”¨ç›‘æµ‹
- en: 'ecology: label images of animal or plant species to monitor wildlife populations
    or track endangered species'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”Ÿæ€å­¦ï¼šæ ‡è®°åŠ¨ç‰©æˆ–æ¤ç‰©ç‰©ç§çš„å›¾åƒä»¥ç›‘æµ‹é‡ç”ŸåŠ¨ç‰©ç§ç¾¤æˆ–è·Ÿè¸ªæ¿’å±ç‰©ç§
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Object detection
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ£€æµ‹
- en: 'Unlike image classification, object detection identifies multiple objects within
    an image and the objectsâ€™ positions in an image (defined by the bounding box).
    Some example applications of object detection include:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å›¾åƒåˆ†ç±»ä¸åŒï¼Œç›®æ ‡æ£€æµ‹è¯†åˆ«å›¾åƒä¸­çš„å¤šä¸ªå¯¹è±¡ä»¥åŠå¯¹è±¡åœ¨å›¾åƒä¸­çš„ä½ç½®ï¼ˆç”±è¾¹ç•Œæ¡†å®šä¹‰ï¼‰ã€‚ç›®æ ‡æ£€æµ‹çš„ä¸€äº›ç¤ºä¾‹åº”ç”¨åŒ…æ‹¬ï¼š
- en: 'self-driving vehicles: detect everyday traffic objects such as other vehicles,
    pedestrians, and traffic lights'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼šæ£€æµ‹æ—¥å¸¸äº¤é€šå¯¹è±¡ï¼Œå¦‚å…¶ä»–è½¦è¾†ï¼Œè¡Œäººå’Œäº¤é€šç¯
- en: 'remote sensing: disaster monitoring, urban planning, and weather forecasting'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¥æ„Ÿï¼šç¾å®³ç›‘æµ‹ï¼ŒåŸå¸‚è§„åˆ’å’Œå¤©æ°”é¢„æŠ¥
- en: 'defect detection: detect cracks or structural damage in buildings, and manufacturing
    defects'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¼ºé™·æ£€æµ‹ï¼šæ£€æµ‹å»ºç­‘ç‰©ä¸­çš„è£‚ç¼æˆ–ç»“æ„æŸåï¼Œä»¥åŠåˆ¶é€ ç¼ºé™·
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Image segmentation
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ†å‰²
- en: 'Image segmentation is a pixel-level task that assigns every pixel in an image
    to a class. It differs from object detection, which uses bounding boxes to label
    and predict objects in an image because segmentation is more granular. Segmentation
    can detect objects at a pixel-level. There are several types of image segmentation:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ†å‰²æ˜¯ä¸€ä¸ªåƒç´ çº§ä»»åŠ¡ï¼Œå°†å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ åˆ†é…ç»™ä¸€ä¸ªç±»åˆ«ã€‚å®ƒä¸ç›®æ ‡æ£€æµ‹ä¸åŒï¼Œåè€…ä½¿ç”¨è¾¹ç•Œæ¡†æ¥æ ‡è®°å’Œé¢„æµ‹å›¾åƒä¸­çš„å¯¹è±¡ï¼Œå› ä¸ºåˆ†å‰²æ›´åŠ ç»†ç²’åŒ–ã€‚åˆ†å‰²å¯ä»¥åœ¨åƒç´ çº§åˆ«æ£€æµ‹å¯¹è±¡ã€‚æœ‰å‡ ç§ç±»å‹çš„å›¾åƒåˆ†å‰²ï¼š
- en: 'instance segmentation: in addition to labeling the class of an object, it also
    labels each distinct instance of an object (â€œdog-1â€, â€œdog-2â€)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ä¾‹åˆ†å‰²ï¼šé™¤äº†æ ‡è®°å¯¹è±¡çš„ç±»åˆ«å¤–ï¼Œè¿˜æ ‡è®°å¯¹è±¡çš„æ¯ä¸ªä¸åŒå®ä¾‹ï¼ˆâ€œç‹—-1â€ï¼Œâ€œç‹—-2â€ï¼‰
- en: 'panoptic segmentation: a combination of semantic and instance segmentation;
    it labels each pixel with a semantic class **and** each distinct instance of an
    object'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…¨æ™¯åˆ†å‰²ï¼šè¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²çš„ç»“åˆï¼›å®ƒä½¿ç”¨è¯­ä¹‰ç±»åˆ«æ ‡è®°æ¯ä¸ªåƒç´ **å’Œ**å¯¹è±¡çš„æ¯ä¸ªä¸åŒå®ä¾‹
- en: Segmentation tasks are helpful in self-driving vehicles to create a pixel-level
    map of the world around them so they can navigate safely around pedestrians and
    other vehicles. It is also useful for medical imaging, where the taskâ€™s finer
    granularity can help identify abnormal cells or organ features. Image segmentation
    can also be used in ecommerce to virtually try on clothes or create augmented
    reality experiences by overlaying objects in the real world through your camera.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å‰²ä»»åŠ¡åœ¨è‡ªåŠ¨é©¾é©¶è½¦è¾†ä¸­éå¸¸æœ‰ç”¨ï¼Œå¯ä»¥ä¸ºå®ƒä»¬åˆ›å»ºå‘¨å›´ä¸–ç•Œçš„åƒç´ çº§åœ°å›¾ï¼Œä»¥ä¾¿å®ƒä»¬å¯ä»¥å®‰å…¨åœ°ç»•è¿‡è¡Œäººå’Œå…¶ä»–è½¦è¾†ã€‚åœ¨åŒ»å­¦æˆåƒä¸­ä¹Ÿå¾ˆæœ‰ç”¨ï¼Œä»»åŠ¡çš„æ›´ç»†ç²’åº¦å¯ä»¥å¸®åŠ©è¯†åˆ«å¼‚å¸¸ç»†èƒæˆ–å™¨å®˜ç‰¹å¾ã€‚å›¾åƒåˆ†å‰²è¿˜å¯ä»¥ç”¨äºç”µå­å•†åŠ¡ï¼Œé€šè¿‡åœ¨çœŸå®ä¸–ç•Œä¸­é€šè¿‡æ‚¨çš„ç›¸æœºå åŠ å¯¹è±¡æ¥è™šæ‹Ÿè¯•ç©¿è¡£æœæˆ–åˆ›å»ºå¢å¼ºç°å®ä½“éªŒã€‚
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Depth estimation
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ·±åº¦ä¼°è®¡
- en: Depth estimation predicts the distance of each pixel in an image from the camera.
    This computer vision task is especially important for scene understanding and
    reconstruction. For example, in self-driving cars, vehicles need to understand
    how far objects like pedestrians, traffic signs, and other vehicles are to avoid
    obstacles and collisions. Depth information is also helpful for constructing 3D
    representations from 2D images and can be used to create high-quality 3D representations
    of biological structures or buildings.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦ä¼°è®¡é¢„æµ‹å›¾åƒä¸­æ¯ä¸ªåƒç´ ä¸ç›¸æœºä¹‹é—´çš„è·ç¦»ã€‚è¿™ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡å¯¹äºåœºæ™¯ç†è§£å’Œé‡å»ºå°¤ä¸ºé‡è¦ã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸­ï¼Œè½¦è¾†éœ€è¦äº†è§£è¡Œäººã€äº¤é€šæ ‡å¿—å’Œå…¶ä»–è½¦è¾†ç­‰ç‰©ä½“çš„è·ç¦»ï¼Œä»¥é¿å…éšœç¢å’Œç¢°æ’ã€‚æ·±åº¦ä¿¡æ¯è¿˜æœ‰åŠ©äºä»2Då›¾åƒæ„å»º3Dè¡¨ç¤ºï¼Œå¹¶å¯ç”¨äºåˆ›å»ºç”Ÿç‰©ç»“æ„æˆ–å»ºç­‘ç‰©çš„é«˜è´¨é‡3Dè¡¨ç¤ºã€‚
- en: 'There are two approaches to depth estimation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦ä¼°è®¡æœ‰ä¸¤ç§æ–¹æ³•ï¼š
- en: 'stereo: depths are estimated by comparing two images of the same image from
    slightly different angles'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç«‹ä½“ï¼šé€šè¿‡æ¯”è¾ƒç•¥æœ‰ä¸åŒè§’åº¦æ‹æ‘„çš„ä¸¤å¹…å›¾åƒæ¥ä¼°è®¡æ·±åº¦
- en: 'monocular: depths are estimated from a single image'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å•çœ¼ï¼šä»å•ä¸ªå›¾åƒä¼°è®¡æ·±åº¦
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Natural language processing
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡ªç„¶è¯­è¨€å¤„ç†
- en: NLP tasks are among the most common types of tasks because text is such a natural
    way for us to communicate. To get text into a format recognized by a model, it
    needs to be tokenized. This means dividing a sequence of text into separate words
    or subwords (tokens) and then converting these tokens into numbers. As a result,
    you can represent a sequence of text as a sequence of numbers, and once you have
    a sequence of numbers, it can be input into a model to solve all sorts of NLP
    tasks!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: NLPä»»åŠ¡æ˜¯æœ€å¸¸è§çš„ä»»åŠ¡ç±»å‹ä¹‹ä¸€ï¼Œå› ä¸ºæ–‡æœ¬æ˜¯æˆ‘ä»¬è¿›è¡Œäº¤æµçš„ä¸€ç§è‡ªç„¶æ–¹å¼ã€‚è¦å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¯†åˆ«çš„æ ¼å¼ï¼Œéœ€è¦å¯¹å…¶è¿›è¡Œæ ‡è®°åŒ–ã€‚è¿™æ„å‘³ç€å°†æ–‡æœ¬åºåˆ—åˆ†å‰²ä¸ºå•ç‹¬çš„å•è¯æˆ–å­è¯ï¼ˆæ ‡è®°ï¼‰ï¼Œç„¶åå°†è¿™äº›æ ‡è®°è½¬æ¢ä¸ºæ•°å­—ã€‚å› æ­¤ï¼Œæ‚¨å¯ä»¥å°†æ–‡æœ¬åºåˆ—è¡¨ç¤ºä¸ºæ•°å­—åºåˆ—ï¼Œä¸€æ—¦æ‚¨æœ‰äº†æ•°å­—åºåˆ—ï¼Œå°±å¯ä»¥å°†å…¶è¾“å…¥åˆ°æ¨¡å‹ä¸­ä»¥è§£å†³å„ç§NLPä»»åŠ¡ï¼
- en: Text classification
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ†ç±»
- en: 'Like classification tasks in any modality, text classification labels a sequence
    of text (it can be sentence-level, a paragraph, or a document) from a predefined
    set of classes. There are many practical applications for text classification,
    some of which include:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä»»ä½•æ¨¡æ€ä¸­çš„åˆ†ç±»ä»»åŠ¡ä¸€æ ·ï¼Œæ–‡æœ¬åˆ†ç±»å°†æ–‡æœ¬åºåˆ—ï¼ˆå¯ä»¥æ˜¯å¥å­çº§ã€æ®µè½æˆ–æ–‡æ¡£ï¼‰ä»é¢„å®šä¹‰ç±»åˆ«é›†ä¸­æ ‡è®°ã€‚æ–‡æœ¬åˆ†ç±»æœ‰è®¸å¤šå®é™…åº”ç”¨ï¼Œå…¶ä¸­ä¸€äº›åŒ…æ‹¬ï¼š
- en: 'sentiment analysis: label text according to some polarity like `positive` or
    `negative` which can inform and support decision-making in fields like politics,
    finance, and marketing'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æƒ…æ„Ÿåˆ†æï¼šæ ¹æ®`ç§¯æ`æˆ–`æ¶ˆæ`ç­‰ææ€§ä¸ºæ–‡æœ¬æ ‡è®°ï¼Œå¯ä»¥åœ¨æ”¿æ²»ã€é‡‘èå’Œè¥é”€ç­‰é¢†åŸŸæ”¯æŒå†³ç­–
- en: 'content classification: label text according to some topic to help organize
    and filter information in news and social media feeds (`weather`, `sports`, `finance`,
    etc.)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†…å®¹åˆ†ç±»ï¼šæ ¹æ®æŸä¸ªä¸»é¢˜ä¸ºæ–‡æœ¬æ ‡è®°ï¼Œä»¥å¸®åŠ©ç»„ç»‡å’Œè¿‡æ»¤æ–°é—»å’Œç¤¾äº¤åª’ä½“ä¿¡æ¯æµä¸­çš„ä¿¡æ¯ï¼ˆå¦‚`å¤©æ°”`ã€`ä½“è‚²`ã€`é‡‘è`ç­‰ï¼‰
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Token classification
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ ‡è®°åˆ†ç±»
- en: In any NLP task, text is preprocessed by separating the sequence of text into
    individual words or subwords. These are known as [tokens](glossary#token). Token
    classification assigns each token a label from a predefined set of classes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»»ä½•è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œæ–‡æœ¬éƒ½ä¼šè¢«é¢„å¤„ç†ï¼Œå°†æ–‡æœ¬åºåˆ—åˆ†å‰²æˆå•ä¸ªå•è¯æˆ–å­è¯ã€‚è¿™äº›è¢«ç§°ä¸º[æ ‡è®°](æœ¯è¯­è¡¨#æ ‡è®°)ã€‚æ ‡è®°åˆ†ç±»ä¸ºæ¯ä¸ªæ ‡è®°åˆ†é…ä¸€ä¸ªæ¥è‡ªé¢„å®šä¹‰ç±»åˆ«é›†çš„æ ‡ç­¾ã€‚
- en: 'Two common types of token classification are:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ç§å¸¸è§çš„æ ‡è®°åˆ†ç±»ç±»å‹æ˜¯ï¼š
- en: 'named entity recognition (NER): label a token according to an entity category
    like organization, person, location or date. NER is especially popular in biomedical
    settings, where it can label genes, proteins, and drug names.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ï¼šæ ¹æ®å®ä½“ç±»åˆ«ï¼ˆå¦‚ç»„ç»‡ã€ä¸ªäººã€åœ°ç‚¹æˆ–æ—¥æœŸï¼‰ä¸ºæ ‡è®°åˆ†é…æ ‡ç­¾ã€‚NERåœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸç‰¹åˆ«å—æ¬¢è¿ï¼Œå¯ä»¥æ ‡è®°åŸºå› ã€è›‹ç™½è´¨å’Œè¯ç‰©åç§°ã€‚
- en: 'part-of-speech tagging (POS): label a token according to its part-of-speech
    like noun, verb, or adjective. POS is useful for helping translation systems understand
    how two identical words are grammatically different (bank as a noun versus bank
    as a verb).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰ï¼šæ ¹æ®å…¶è¯æ€§ï¼ˆåè¯ã€åŠ¨è¯æˆ–å½¢å®¹è¯ï¼‰ä¸ºæ ‡è®°åˆ†é…æ ‡ç­¾ã€‚POSå¯¹äºå¸®åŠ©ç¿»è¯‘ç³»ç»Ÿç†è§£ä¸¤ä¸ªç›¸åŒå•è¯åœ¨è¯­æ³•ä¸Šçš„ä¸åŒä¹‹å¤„ï¼ˆåè¯â€œé“¶è¡Œâ€ä¸åŠ¨è¯â€œå­˜æ¬¾â€ï¼‰éå¸¸æœ‰ç”¨ã€‚
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Question answering
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é—®ç­”
- en: Question answering is another token-level task that returns an answer to a question,
    sometimes with context (open-domain) and other times without context (closed-domain).
    This task happens whenever we ask a virtual assistant something like whether a
    restaurant is open. It can also provide customer or technical support and help
    search engines retrieve the relevant information youâ€™re asking for.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: é—®ç­”æ˜¯å¦ä¸€ä¸ªæ ‡è®°çº§ä»»åŠ¡ï¼Œå®ƒè¿”å›ä¸€ä¸ªé—®é¢˜çš„ç­”æ¡ˆï¼Œæœ‰æ—¶åŒ…å«ä¸Šä¸‹æ–‡ï¼ˆå¼€æ”¾åŸŸï¼‰ï¼Œæœ‰æ—¶ä¸åŒ…å«ä¸Šä¸‹æ–‡ï¼ˆå°é—­åŸŸï¼‰ã€‚æ¯å½“æˆ‘ä»¬è¯¢é—®è™šæ‹ŸåŠ©æ‰‹åƒé¤å…æ˜¯å¦è¥ä¸šè¿™æ ·çš„é—®é¢˜æ—¶ï¼Œè¿™ä¸ªä»»åŠ¡å°±ä¼šå‘ç”Ÿã€‚å®ƒè¿˜å¯ä»¥æä¾›å®¢æˆ·æˆ–æŠ€æœ¯æ”¯æŒï¼Œå¹¶å¸®åŠ©æœç´¢å¼•æ“æ£€ç´¢æ‚¨æ‰€è¯¢é—®çš„ç›¸å…³ä¿¡æ¯ã€‚
- en: 'There are two common types of question answering:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: é—®ç­”æœ‰ä¸¤ç§å¸¸è§ç±»å‹ï¼š
- en: 'extractive: given a question and some context, the answer is a span of text
    from the context the model must extract'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŠ½å–å¼ï¼šç»™å®šä¸€ä¸ªé—®é¢˜å’Œä¸€äº›ä¸Šä¸‹æ–‡ï¼Œç­”æ¡ˆæ˜¯æ¨¡å‹å¿…é¡»ä»ä¸Šä¸‹æ–‡ä¸­æå–çš„æ–‡æœ¬ç‰‡æ®µ
- en: 'abstractive: given a question and some context, the answer is generated from
    the context; this approach is handled by the [Text2TextGenerationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Text2TextGenerationPipeline)
    instead of the [QuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline)
    shown below'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŠ½è±¡å¼ï¼šç»™å®šä¸€ä¸ªé—®é¢˜å’Œä¸€äº›ä¸Šä¸‹æ–‡ï¼Œä»ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆï¼›è¿™ç§æ–¹æ³•ç”±[Text2TextGenerationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Text2TextGenerationPipeline)å¤„ç†ï¼Œè€Œä¸æ˜¯ä¸‹é¢æ˜¾ç¤ºçš„[QuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline)
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Summarization
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: Summarization creates a shorter version of a text from a longer one while trying
    to preserve most of the meaning of the original document. Summarization is a sequence-to-sequence
    task; it outputs a shorter text sequence than the input. There are a lot of long-form
    documents that can be summarized to help readers quickly understand the main points.
    Legislative bills, legal and financial documents, patents, and scientific papers
    are a few examples of documents that could be summarized to save readers time
    and serve as a reading aid.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ‘˜è¦åˆ›å»ºä¸€ä¸ªè¾ƒçŸ­çš„æ–‡æœ¬ç‰ˆæœ¬ï¼Œä»è¾ƒé•¿çš„æ–‡æœ¬ä¸­æå–ï¼ŒåŒæ—¶å°½é‡ä¿ç•™åŸå§‹æ–‡æ¡£çš„å¤§éƒ¨åˆ†å«ä¹‰ã€‚æ‘˜è¦æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼›å®ƒè¾“å‡ºæ¯”è¾“å…¥æ›´çŸ­çš„æ–‡æœ¬åºåˆ—ã€‚æœ‰å¾ˆå¤šé•¿ç¯‡æ–‡æ¡£å¯ä»¥è¢«æ‘˜è¦ï¼Œä»¥å¸®åŠ©è¯»è€…å¿«é€Ÿç†è§£ä¸»è¦è§‚ç‚¹ã€‚æ³•æ¡ˆã€æ³•å¾‹å’Œè´¢åŠ¡æ–‡ä»¶ã€ä¸“åˆ©å’Œç§‘å­¦è®ºæ–‡æ˜¯ä¸€äº›å¯ä»¥è¢«æ‘˜è¦ä»¥èŠ‚çœè¯»è€…æ—¶é—´å¹¶ä½œä¸ºé˜…è¯»è¾…åŠ©çš„æ–‡æ¡£ç¤ºä¾‹ã€‚
- en: 'Like question answering, there are two types of summarization:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸é—®ç­”ç±»ä¼¼ï¼Œæ‘˜è¦æœ‰ä¸¤ç§ç±»å‹ï¼š
- en: 'extractive: identify and extract the most important sentences from the original
    text'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŠ½å–å¼ï¼šè¯†åˆ«å¹¶æå–åŸå§‹æ–‡æœ¬ä¸­æœ€é‡è¦çš„å¥å­
- en: 'abstractive: generate the target summary (which may include new words not in
    the input document) from the original text; the [SummarizationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.SummarizationPipeline)
    uses the abstractive approach'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŠ½è±¡å¼ï¼šä»åŸå§‹æ–‡æœ¬ç”Ÿæˆç›®æ ‡æ‘˜è¦ï¼ˆå¯èƒ½åŒ…å«è¾“å…¥æ–‡æ¡£ä¸­æ²¡æœ‰çš„æ–°å•è¯ï¼‰ï¼›[SummarizationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.SummarizationPipeline)ä½¿ç”¨æŠ½è±¡å¼æ–¹æ³•
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Translation
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¿»è¯‘
- en: Translation converts a sequence of text in one language to another. It is important
    in helping people from different backgrounds communicate with each other, help
    translate content to reach wider audiences, and even be a learning tool to help
    people learn a new language. Along with summarization, translation is a sequence-to-sequence
    task, meaning the model receives an input sequence and returns a target output
    sequence.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ç¿»è¯‘å°†ä¸€ç§è¯­è¨€ä¸­çš„æ–‡æœ¬åºåˆ—è½¬æ¢ä¸ºå¦ä¸€ç§è¯­è¨€ã€‚å®ƒå¯¹å¸®åŠ©æ¥è‡ªä¸åŒèƒŒæ™¯çš„äººä»¬ç›¸äº’æ²Ÿé€šã€å¸®åŠ©ç¿»è¯‘å†…å®¹ä»¥æ‰©å¤§å—ä¼—ã€ç”šè‡³ä½œä¸ºå¸®åŠ©äººä»¬å­¦ä¹ æ–°è¯­è¨€çš„å·¥å…·éƒ½éå¸¸é‡è¦ã€‚ç¿»è¯‘ä¸æ‘˜è¦ä¸€æ ·ï¼Œæ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œæ„å‘³ç€æ¨¡å‹æ¥æ”¶ä¸€ä¸ªè¾“å…¥åºåˆ—å¹¶è¿”å›ä¸€ä¸ªç›®æ ‡è¾“å‡ºåºåˆ—ã€‚
- en: In the early days, translation models were mostly monolingual, but recently,
    there has been increasing interest in multilingual models that can translate between
    many pairs of languages.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ—©æœŸï¼Œç¿»è¯‘æ¨¡å‹ä¸»è¦æ˜¯å•è¯­çš„ï¼Œä½†æœ€è¿‘ï¼Œå¯¹äºèƒ½å¤Ÿåœ¨è®¸å¤šè¯­è¨€å¯¹ä¹‹é—´è¿›è¡Œç¿»è¯‘çš„å¤šè¯­è¨€æ¨¡å‹è¶Šæ¥è¶Šæ„Ÿå…´è¶£ã€‚
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Language modeling
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¯­è¨€å»ºæ¨¡
- en: Language modeling is a task that predicts a word in a sequence of text. It has
    become a very popular NLP task because a pretrained language model can be finetuned
    for many other downstream tasks. Lately, there has been a lot of interest in large
    language models (LLMs) which demonstrate zero- or few-shot learning. This means
    the model can solve tasks it wasnâ€™t explicitly trained to do! Language models
    can be used to generate fluent and convincing text, though you need to be careful
    since the text may not always be accurate.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­è¨€å»ºæ¨¡æ˜¯ä¸€ç§é¢„æµ‹æ–‡æœ¬åºåˆ—ä¸­å•è¯çš„ä»»åŠ¡ã€‚å®ƒå·²ç»æˆä¸ºéå¸¸æµè¡Œçš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå› ä¸ºé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¯ä»¥ä¸ºè®¸å¤šå…¶ä»–ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚æœ€è¿‘ï¼Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰äº§ç”Ÿäº†å¾ˆå¤§å…´è¶£ï¼Œè¿™äº›æ¨¡å‹å±•ç¤ºäº†é›¶æˆ–å°‘é‡æ ·æœ¬å­¦ä¹ ã€‚è¿™æ„å‘³ç€æ¨¡å‹å¯ä»¥è§£å†³å®ƒæ²¡æœ‰æ˜ç¡®è®­ç»ƒçš„ä»»åŠ¡ï¼è¯­è¨€æ¨¡å‹å¯ç”¨äºç”Ÿæˆæµç•…ä¸”ä»¤äººä¿¡æœçš„æ–‡æœ¬ï¼Œå°½ç®¡æ‚¨éœ€è¦å°å¿ƒï¼Œå› ä¸ºæ–‡æœ¬å¯èƒ½å¹¶ä¸æ€»æ˜¯å‡†ç¡®çš„ã€‚
- en: 'There are two types of language modeling:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ç§ç±»å‹çš„è¯­è¨€å»ºæ¨¡ï¼š
- en: 'causal: the modelâ€™s objective is to predict the next token in a sequence, and
    future tokens are masked'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å› æœï¼šæ¨¡å‹çš„ç›®æ ‡æ˜¯é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œæœªæ¥çš„æ ‡è®°è¢«æ©ç›–ã€‚
- en: '[PRE11]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'masked: the modelâ€™s objective is to predict a masked token in a sequence with
    full access to the tokens in the sequence'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ©ç ï¼šæ¨¡å‹çš„ç›®æ ‡æ˜¯åœ¨å®Œå…¨è®¿é—®åºåˆ—ä¸­çš„æ ‡è®°çš„æƒ…å†µä¸‹é¢„æµ‹åºåˆ—ä¸­çš„æ©ç æ ‡è®°
- en: '[PRE12]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Multimodal
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤šæ¨¡æ€
- en: Multimodal tasks require a model to process multiple data modalities (text,
    image, audio, video) to solve a particular problem. Image captioning is an example
    of a multimodal task where the model takes an image as input and outputs a sequence
    of text describing the image or some properties of the image.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šæ¨¡æ€ä»»åŠ¡éœ€è¦æ¨¡å‹å¤„ç†å¤šç§æ•°æ®æ¨¡æ€ï¼ˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ï¼‰æ¥è§£å†³ç‰¹å®šé—®é¢˜ã€‚å›¾åƒå­—å¹•æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ä»»åŠ¡çš„ä¾‹å­ï¼Œæ¨¡å‹ä»¥å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºæè¿°å›¾åƒæˆ–å›¾åƒæŸäº›å±æ€§çš„æ–‡æœ¬åºåˆ—ã€‚
- en: Although multimodal models work with different data types or modalities, internally,
    the preprocessing steps help the model convert all the data types into embeddings
    (vectors or list of numbers that holds meaningful information about the data).
    For a task like image captioning, the model learns relationships between image
    embeddings and text embeddings.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¤šæ¨¡æ€æ¨¡å‹å¤„ç†ä¸åŒçš„æ•°æ®ç±»å‹æˆ–æ¨¡æ€ï¼Œä½†åœ¨å†…éƒ¨ï¼Œé¢„å¤„ç†æ­¥éª¤å¸®åŠ©æ¨¡å‹å°†æ‰€æœ‰æ•°æ®ç±»å‹è½¬æ¢ä¸ºåµŒå…¥ï¼ˆåŒ…å«æœ‰å…³æ•°æ®çš„æœ‰æ„ä¹‰ä¿¡æ¯çš„å‘é‡æˆ–æ•°å­—åˆ—è¡¨ï¼‰ã€‚å¯¹äºåƒå›¾åƒå­—å¹•è¿™æ ·çš„ä»»åŠ¡ï¼Œæ¨¡å‹å­¦ä¹ å›¾åƒåµŒå…¥å’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„å…³ç³»ã€‚
- en: Document question answering
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ–‡æ¡£é—®ç­”
- en: Document question answering is a task that answers natural language questions
    from a document. Unlike a token-level question answering task which takes text
    as input, document question answering takes an image of a document as input along
    with a question about the document and returns an answer. Document question answering
    can be used to parse structured documents and extract key information from it.
    In the example below, the total amount and change due can be extracted from a
    receipt.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æ¡£é—®ç­”æ˜¯ä¸€ä¸ªä»æ–‡æ¡£ä¸­å›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜çš„ä»»åŠ¡ã€‚ä¸ä»¥æ–‡æœ¬ä½œä¸ºè¾“å…¥çš„æ ‡è®°çº§åˆ«é—®ç­”ä»»åŠ¡ä¸åŒï¼Œæ–‡æ¡£é—®ç­”ä»¥æ–‡æ¡£çš„å›¾åƒä½œä¸ºè¾“å…¥ï¼ŒåŒæ—¶æå‡ºå…³äºæ–‡æ¡£çš„é—®é¢˜ï¼Œå¹¶è¿”å›ä¸€ä¸ªç­”æ¡ˆã€‚æ–‡æ¡£é—®ç­”å¯ç”¨äºè§£æç»“æ„åŒ–æ–‡æ¡£å¹¶ä»ä¸­æå–å…³é”®ä¿¡æ¯ã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œå¯ä»¥ä»æ”¶æ®ä¸­æå–æ€»é‡‘é¢å’Œæ‰¾é›¶é‡‘é¢ã€‚
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Hopefully, this page has given you some more background information about all
    the types of tasks in each modality and the practical importance of each one.
    In the next [section](tasks_explained), youâ€™ll learn **how** ğŸ¤— Transformers work
    to solve these tasks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›æœ¬é¡µä¸ºæ‚¨æä¾›äº†æœ‰å…³æ¯ç§æ¨¡æ€ä¸­æ‰€æœ‰ä»»åŠ¡ç±»å‹åŠå…¶å®é™…é‡è¦æ€§çš„æ›´å¤šèƒŒæ™¯ä¿¡æ¯ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæ‚¨å°†äº†è§£ğŸ¤—å˜å‹å™¨æ˜¯å¦‚ä½•è§£å†³è¿™äº›ä»»åŠ¡çš„ã€‚
