- en: What ü§ó Transformers can do
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/docs/transformers/v4.37.2/en/task_summary](https://huggingface.co/docs/transformers/v4.37.2/en/task_summary)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/339.64b17b34.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: ü§ó Transformers is a library of pretrained state-of-the-art models for natural
    language processing (NLP), computer vision, and audio and speech processing tasks.
    Not only does the library contain Transformer models, but it also has non-Transformer
    models like modern convolutional networks for computer vision tasks. If you look
    at some of the most popular consumer products today, like smartphones, apps, and
    televisions, odds are that some kind of deep learning technology is behind it.
    Want to remove a background object from a picture taken by your smartphone? This
    is an example of a panoptic segmentation task (don‚Äôt worry if you don‚Äôt know what
    this means yet, we‚Äôll describe it in the following sections!).
  prefs: []
  type: TYPE_NORMAL
- en: This page provides an overview of the different speech and audio, computer vision,
    and NLP tasks that can be solved with the ü§ó Transformers library in just three
    lines of code!
  prefs: []
  type: TYPE_NORMAL
- en: Audio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Audio and speech processing tasks are a little different from the other modalities
    mainly because audio as an input is a continuous signal. Unlike text, a raw audio
    waveform can‚Äôt be neatly split into discrete chunks the way a sentence can be
    divided into words. To get around this, the raw audio signal is typically sampled
    at regular intervals. If you take more samples within an interval, the sampling
    rate is higher, and the audio more closely resembles the original audio source.
  prefs: []
  type: TYPE_NORMAL
- en: Previous approaches preprocessed the audio to extract useful features from it.
    It is now more common to start audio and speech processing tasks by directly feeding
    the raw audio waveform to a feature encoder to extract an audio representation.
    This simplifies the preprocessing step and allows the model to learn the most
    essential features.
  prefs: []
  type: TYPE_NORMAL
- en: Audio classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Audio classification is a task that labels audio data from a predefined set
    of classes. It is a broad category with many specific applications, some of which
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'acoustic scene classification: label audio with a scene label (‚Äúoffice‚Äù, ‚Äúbeach‚Äù,
    ‚Äústadium‚Äù)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'acoustic event detection: label audio with a sound event label (‚Äúcar horn‚Äù,
    ‚Äúwhale calling‚Äù, ‚Äúglass breaking‚Äù)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'tagging: label audio containing multiple sounds (birdsongs, speaker identification
    in a meeting)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'music classification: label music with a genre label (‚Äúmetal‚Äù, ‚Äúhip-hop‚Äù, ‚Äúcountry‚Äù)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Automatic speech recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatic speech recognition (ASR) transcribes speech into text. It is one of
    the most common audio tasks due partly to speech being such a natural form of
    human communication. Today, ASR systems are embedded in ‚Äúsmart‚Äù technology products
    like speakers, phones, and cars. We can ask our virtual assistants to play music,
    set reminders, and tell us the weather.
  prefs: []
  type: TYPE_NORMAL
- en: But one of the key challenges Transformer architectures have helped with is
    in low-resource languages. By pretraining on large amounts of speech data, finetuning
    the model on only one hour of labeled speech data in a low-resource language can
    still produce high-quality results compared to previous ASR systems trained on
    100x more labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Computer vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the first and earliest successful computer vision tasks was recognizing
    images of zip code numbers using a [convolutional neural network (CNN)](glossary#convolution).
    An image is composed of pixels, and each pixel has a numerical value. This makes
    it easy to represent an image as a matrix of pixel values. Each particular combination
    of pixel values describes the colors of an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two general ways computer vision tasks can be solved are:'
  prefs: []
  type: TYPE_NORMAL
- en: Use convolutions to learn the hierarchical features of an image from low-level
    features to high-level abstract things.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split an image into patches and use a Transformer to gradually learn how each
    image patch is related to each other to form an image. Unlike the bottom-up approach
    favored by a CNN, this is kind of like starting out with a blurry image and then
    gradually bringing it into focus.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Image classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Image classification labels an entire image from a predefined set of classes.
    Like most classification tasks, there are many practical use cases for image classification,
    some of which include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'healthcare: label medical images to detect disease or monitor patient health'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'environment: label satellite images to monitor deforestation, inform wildland
    management or detect wildfires'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'agriculture: label images of crops to monitor plant health or satellite images
    for land use monitoring'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ecology: label images of animal or plant species to monitor wildlife populations
    or track endangered species'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Object detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unlike image classification, object detection identifies multiple objects within
    an image and the objects‚Äô positions in an image (defined by the bounding box).
    Some example applications of object detection include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'self-driving vehicles: detect everyday traffic objects such as other vehicles,
    pedestrians, and traffic lights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'remote sensing: disaster monitoring, urban planning, and weather forecasting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'defect detection: detect cracks or structural damage in buildings, and manufacturing
    defects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Image segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Image segmentation is a pixel-level task that assigns every pixel in an image
    to a class. It differs from object detection, which uses bounding boxes to label
    and predict objects in an image because segmentation is more granular. Segmentation
    can detect objects at a pixel-level. There are several types of image segmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'instance segmentation: in addition to labeling the class of an object, it also
    labels each distinct instance of an object (‚Äúdog-1‚Äù, ‚Äúdog-2‚Äù)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'panoptic segmentation: a combination of semantic and instance segmentation;
    it labels each pixel with a semantic class **and** each distinct instance of an
    object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmentation tasks are helpful in self-driving vehicles to create a pixel-level
    map of the world around them so they can navigate safely around pedestrians and
    other vehicles. It is also useful for medical imaging, where the task‚Äôs finer
    granularity can help identify abnormal cells or organ features. Image segmentation
    can also be used in ecommerce to virtually try on clothes or create augmented
    reality experiences by overlaying objects in the real world through your camera.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Depth estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Depth estimation predicts the distance of each pixel in an image from the camera.
    This computer vision task is especially important for scene understanding and
    reconstruction. For example, in self-driving cars, vehicles need to understand
    how far objects like pedestrians, traffic signs, and other vehicles are to avoid
    obstacles and collisions. Depth information is also helpful for constructing 3D
    representations from 2D images and can be used to create high-quality 3D representations
    of biological structures or buildings.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two approaches to depth estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'stereo: depths are estimated by comparing two images of the same image from
    slightly different angles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'monocular: depths are estimated from a single image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Natural language processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NLP tasks are among the most common types of tasks because text is such a natural
    way for us to communicate. To get text into a format recognized by a model, it
    needs to be tokenized. This means dividing a sequence of text into separate words
    or subwords (tokens) and then converting these tokens into numbers. As a result,
    you can represent a sequence of text as a sequence of numbers, and once you have
    a sequence of numbers, it can be input into a model to solve all sorts of NLP
    tasks!
  prefs: []
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Like classification tasks in any modality, text classification labels a sequence
    of text (it can be sentence-level, a paragraph, or a document) from a predefined
    set of classes. There are many practical applications for text classification,
    some of which include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'sentiment analysis: label text according to some polarity like `positive` or
    `negative` which can inform and support decision-making in fields like politics,
    finance, and marketing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'content classification: label text according to some topic to help organize
    and filter information in news and social media feeds (`weather`, `sports`, `finance`,
    etc.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Token classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In any NLP task, text is preprocessed by separating the sequence of text into
    individual words or subwords. These are known as [tokens](glossary#token). Token
    classification assigns each token a label from a predefined set of classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two common types of token classification are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'named entity recognition (NER): label a token according to an entity category
    like organization, person, location or date. NER is especially popular in biomedical
    settings, where it can label genes, proteins, and drug names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'part-of-speech tagging (POS): label a token according to its part-of-speech
    like noun, verb, or adjective. POS is useful for helping translation systems understand
    how two identical words are grammatically different (bank as a noun versus bank
    as a verb).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Question answering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Question answering is another token-level task that returns an answer to a question,
    sometimes with context (open-domain) and other times without context (closed-domain).
    This task happens whenever we ask a virtual assistant something like whether a
    restaurant is open. It can also provide customer or technical support and help
    search engines retrieve the relevant information you‚Äôre asking for.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two common types of question answering:'
  prefs: []
  type: TYPE_NORMAL
- en: 'extractive: given a question and some context, the answer is a span of text
    from the context the model must extract'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'abstractive: given a question and some context, the answer is generated from
    the context; this approach is handled by the [Text2TextGenerationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Text2TextGenerationPipeline)
    instead of the [QuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline)
    shown below'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Summarization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Summarization creates a shorter version of a text from a longer one while trying
    to preserve most of the meaning of the original document. Summarization is a sequence-to-sequence
    task; it outputs a shorter text sequence than the input. There are a lot of long-form
    documents that can be summarized to help readers quickly understand the main points.
    Legislative bills, legal and financial documents, patents, and scientific papers
    are a few examples of documents that could be summarized to save readers time
    and serve as a reading aid.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like question answering, there are two types of summarization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'extractive: identify and extract the most important sentences from the original
    text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'abstractive: generate the target summary (which may include new words not in
    the input document) from the original text; the [SummarizationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.SummarizationPipeline)
    uses the abstractive approach'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Translation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Translation converts a sequence of text in one language to another. It is important
    in helping people from different backgrounds communicate with each other, help
    translate content to reach wider audiences, and even be a learning tool to help
    people learn a new language. Along with summarization, translation is a sequence-to-sequence
    task, meaning the model receives an input sequence and returns a target output
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: In the early days, translation models were mostly monolingual, but recently,
    there has been increasing interest in multilingual models that can translate between
    many pairs of languages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Language modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Language modeling is a task that predicts a word in a sequence of text. It has
    become a very popular NLP task because a pretrained language model can be finetuned
    for many other downstream tasks. Lately, there has been a lot of interest in large
    language models (LLMs) which demonstrate zero- or few-shot learning. This means
    the model can solve tasks it wasn‚Äôt explicitly trained to do! Language models
    can be used to generate fluent and convincing text, though you need to be careful
    since the text may not always be accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of language modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: 'causal: the model‚Äôs objective is to predict the next token in a sequence, and
    future tokens are masked'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'masked: the model‚Äôs objective is to predict a masked token in a sequence with
    full access to the tokens in the sequence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Multimodal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multimodal tasks require a model to process multiple data modalities (text,
    image, audio, video) to solve a particular problem. Image captioning is an example
    of a multimodal task where the model takes an image as input and outputs a sequence
    of text describing the image or some properties of the image.
  prefs: []
  type: TYPE_NORMAL
- en: Although multimodal models work with different data types or modalities, internally,
    the preprocessing steps help the model convert all the data types into embeddings
    (vectors or list of numbers that holds meaningful information about the data).
    For a task like image captioning, the model learns relationships between image
    embeddings and text embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Document question answering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Document question answering is a task that answers natural language questions
    from a document. Unlike a token-level question answering task which takes text
    as input, document question answering takes an image of a document as input along
    with a question about the document and returns an answer. Document question answering
    can be used to parse structured documents and extract key information from it.
    In the example below, the total amount and change due can be extracted from a
    receipt.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Hopefully, this page has given you some more background information about all
    the types of tasks in each modality and the practical importance of each one.
    In the next [section](tasks_explained), you‚Äôll learn **how** ü§ó Transformers work
    to solve these tasks.
  prefs: []
  type: TYPE_NORMAL
