# ä½¿ç”¨ Panda-Gym è¿›è¡Œæœºå™¨äººæ¨¡æ‹Ÿçš„ Advantage Actor Critic (A2C) ğŸ¤–

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/learn/deep-rl-course/unit6/hands-on`](https://huggingface.co/learn/deep-rl-course/unit6/hands-on)

![æé—®](http://hf.co/join/discord) ![åœ¨ Colab ä¸­æ‰“å¼€](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb)

ç°åœ¨æ‚¨å·²ç»å­¦ä¹ äº† Advantage Actor Critic (A2C) çš„ç†è®ºï¼Œ**æ‚¨å·²ç»å‡†å¤‡å¥½è®­ç»ƒæ‚¨çš„ A2C ä»£ç†**ï¼Œä½¿ç”¨ Stable-Baselines3 åœ¨ä¸€ä¸ªæœºå™¨äººç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒã€‚å¹¶è®­ç»ƒä¸€ä¸ªï¼š

+   ä¸€ä¸ªæœºå™¨äººæ‰‹è‡‚ğŸ¦¾ç§»åŠ¨åˆ°æ­£ç¡®çš„ä½ç½®ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨

+   [panda-gym](https://github.com/qgallouedec/panda-gym)

ä¸ºäº†éªŒè¯è¿™ä¸ªå®è·µè¿‡ç¨‹çš„è®¤è¯ï¼Œæ‚¨éœ€è¦å°†æ‚¨çš„ä¸¤ä¸ªè®­ç»ƒæ¨¡å‹æ¨é€åˆ° Hub å¹¶è·å¾—ä»¥ä¸‹ç»“æœï¼š

+   `PandaReachDense-v3` è·å¾—ç»“æœ >= -3.5ã€‚

è¦æ‰¾åˆ°æ‚¨çš„ç»“æœï¼Œè¯·[è½¬åˆ°æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)å¹¶æ‰¾åˆ°æ‚¨çš„æ¨¡å‹ï¼Œ**ç»“æœ = å¹³å‡å¥–åŠ± - å¥–åŠ±çš„æ ‡å‡†å·®**

æœ‰å…³è®¤è¯è¿‡ç¨‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æ­¤éƒ¨åˆ†ğŸ‘‰[`huggingface.co/deep-rl-course/en/unit0/introduction#certification-process`](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)

**ç‚¹å‡»â€œåœ¨ Colab ä¸­æ‰“å¼€â€æŒ‰é’®å¼€å§‹å®è·µ**ğŸ‘‡ï¼š

![åœ¨ Colab ä¸­æ‰“å¼€](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit6/unit6.ipynb)

# ç¬¬ 6 å•å…ƒï¼šä½¿ç”¨ Panda-Gym è¿›è¡Œæœºå™¨äººæ¨¡æ‹Ÿçš„ Advantage Actor Critic (A2C) ğŸ¤–

### ğŸ® ç¯å¢ƒï¼š

+   [Panda-Gym](https://github.com/qgallouedec/panda-gym)

### ğŸ“š å¼ºåŒ–å­¦ä¹ åº“ï¼š

+   [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)

æˆ‘ä»¬ä¸æ–­åŠªåŠ›æ”¹è¿›æˆ‘ä»¬çš„æ•™ç¨‹ï¼Œæ‰€ä»¥**å¦‚æœæ‚¨åœ¨æœ¬ç¬”è®°æœ¬ä¸­å‘ç°é—®é¢˜**ï¼Œè¯·[åœ¨ GitHub ä»“åº“ä¸Šæå‡ºé—®é¢˜](https://github.com/huggingface/deep-rl-class/issues)ã€‚

## æœ¬ç¬”è®°æœ¬çš„ç›®æ ‡ğŸ†

åœ¨ç¬”è®°æœ¬çš„æœ«å°¾ï¼Œæ‚¨å°†ï¼š

+   èƒ½å¤Ÿä½¿ç”¨**Panda-Gym**ï¼Œè¿™ä¸ªç¯å¢ƒåº“ã€‚

+   èƒ½å¤Ÿ**ä½¿ç”¨ A2C è®­ç»ƒæœºå™¨äºº**ã€‚

+   äº†è§£ä¸ºä»€ä¹ˆ**æˆ‘ä»¬éœ€è¦å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–**ã€‚

+   èƒ½å¤Ÿ**å°†æ‚¨è®­ç»ƒå¥½çš„ä»£ç†å’Œä»£ç æ¨é€åˆ° Hub**ï¼Œå¹¶é™„å¸¦ä¸€ä¸ªæ¼‚äº®çš„è§†é¢‘å›æ”¾å’Œè¯„ä¼°åˆ†æ•°ğŸ”¥ã€‚

## å…ˆå†³æ¡ä»¶ğŸ—ï¸

åœ¨æ·±å…¥ç ”ç©¶ç¬”è®°æœ¬ä¹‹å‰ï¼Œæ‚¨éœ€è¦ï¼š

ğŸ”² ğŸ“š å­¦ä¹  [é€šè¿‡é˜…è¯»ç¬¬ 6 å•å…ƒçš„ Actor-Critic æ–¹æ³•](https://huggingface.co/deep-rl-course/unit6/introduction) ğŸ¤—

# è®©æˆ‘ä»¬è®­ç»ƒæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªæœºå™¨äººğŸ¤–

## è®¾ç½® GPU ğŸ’ª

+   ä¸ºäº†**åŠ é€Ÿä»£ç†çš„è®­ç»ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ GPU**ã€‚ä¸ºæ­¤ï¼Œè¯·è½¬åˆ° `è¿è¡Œæ—¶ > æ›´æ”¹è¿è¡Œæ—¶ç±»å‹`

![GPU æ­¥éª¤ 1](img/5378127c314cdd92729aa31b7e11ca44.png)

+   `ç¡¬ä»¶åŠ é€Ÿå™¨ > GPU`

![GPU æ­¥éª¤ 2](img/e0fec252447f98378386ccca8e57a80a.png)

## åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ˜¾ç¤ºğŸ”½

åœ¨ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆä¸€ä¸ªå›æ”¾è§†é¢‘ã€‚ä¸ºæ­¤ï¼Œåœ¨ colab ä¸­ï¼Œ**æˆ‘ä»¬éœ€è¦æœ‰ä¸€ä¸ªè™šæ‹Ÿå±å¹•æ¥æ¸²æŸ“ç¯å¢ƒ**ï¼ˆä»è€Œè®°å½•å¸§ï¼‰ã€‚

ä»¥ä¸‹å•å…ƒæ ¼å°†å®‰è£…åº“å¹¶åˆ›å»ºå¹¶è¿è¡Œä¸€ä¸ªè™šæ‹Ÿå±å¹•ğŸ–¥

```py
%%capture
!apt install python-opengl
!apt install ffmpeg
!apt install xvfb
!pip3 install pyvirtualdisplay
```

```py
# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()
```

### å®‰è£…ä¾èµ–ğŸ”½

æˆ‘ä»¬å°†å®‰è£…å¤šä¸ªï¼š

+   `gymnasium`

+   `panda-gym`ï¼šåŒ…å«æœºå™¨äººæ‰‹è‡‚ç¯å¢ƒã€‚

+   `stable-baselines3`ï¼šSB3 æ·±åº¦å¼ºåŒ–å­¦ä¹ åº“ã€‚

+   `huggingface_sb3`ï¼šç”¨äº Stable-baselines3 çš„é¢å¤–ä»£ç ï¼Œç”¨äºä» Hugging Face ğŸ¤— Hub åŠ è½½å’Œä¸Šä¼ æ¨¡å‹ã€‚

+   `huggingface_hub`ï¼šå…è®¸ä»»ä½•äººä½¿ç”¨ Hub å­˜å‚¨åº“ã€‚

```py
!pip install stable-baselines3[extra]
!pip install gymnasium
!pip install huggingface_sb3
!pip install huggingface_hub
!pip install panda_gym
```

## å¯¼å…¥åŒ…ğŸ“¦

```py
import os

import gymnasium as gym
import panda_gym

from huggingface_sb3 import load_from_hub, package_to_hub

from stable_baselines3 import A2C
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.env_util import make_vec_env

from huggingface_hub import notebook_login
```

## PandaReachDense-v3 ğŸ¦¾

æˆ‘ä»¬å°†è®­ç»ƒçš„ä»£ç†æ˜¯ä¸€ä¸ªéœ€è¦è¿›è¡Œæ§åˆ¶çš„æœºå™¨äººæ‰‹è‡‚ï¼ˆç§»åŠ¨æ‰‹è‡‚å¹¶ä½¿ç”¨æœ«ç«¯æ‰§è¡Œå™¨ï¼‰ã€‚

åœ¨æœºå™¨äººæŠ€æœ¯ä¸­ï¼Œ*æœ«ç«¯æ‰§è¡Œå™¨*æ˜¯è®¾è®¡ç”¨äºä¸ç¯å¢ƒäº¤äº’çš„æœºå™¨äººæ‰‹è‡‚æœ«ç«¯çš„è®¾å¤‡ã€‚

åœ¨ `PandaReach` ä¸­ï¼Œæœºå™¨äººå¿…é¡»å°†å…¶æœ«ç«¯æ‰§è¡Œå™¨æ”¾ç½®åœ¨ç›®æ ‡ä½ç½®ï¼ˆç»¿è‰²çƒï¼‰ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨æ­¤ç¯å¢ƒçš„å¯†é›†ç‰ˆæœ¬ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å°†è·å¾—ä¸€ä¸ª*å¯†é›†å¥–åŠ±å‡½æ•°*ï¼Œ**å°†åœ¨æ¯ä¸ªæ—¶é—´æ­¥æä¾›å¥–åŠ±** (ä»£ç†è¶Šæ¥è¿‘å®Œæˆä»»åŠ¡ï¼Œå¥–åŠ±è¶Šé«˜)ã€‚ä¸*ç¨€ç–å¥–åŠ±å‡½æ•°*ç›¸åï¼Œç¯å¢ƒ**ä»…åœ¨ä»»åŠ¡å®Œæˆæ—¶è¿”å›å¥–åŠ±**ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ *æœ«ç«¯æ‰§è¡Œå™¨ä½ç§»æ§åˆ¶*ï¼Œè¿™æ„å‘³ç€**åŠ¨ä½œå¯¹åº”äºæœ«ç«¯æ‰§è¡Œå™¨çš„ä½ç§»**ã€‚æˆ‘ä»¬ä¸æ§åˆ¶æ¯ä¸ªå…³èŠ‚çš„å•ç‹¬è¿åŠ¨ (å…³èŠ‚æ§åˆ¶)ã€‚

![æœºå™¨äººæŠ€æœ¯](img/d79d62b53f91999defb0b4eae0db003d.png)

è¿™æ ·**è®­ç»ƒå°†æ›´å®¹æ˜“**ã€‚

### åˆ›å»ºç¯å¢ƒ

#### ç¯å¢ƒ ğŸ®

åœ¨ `PandaReachDense-v3` ä¸­ï¼Œæœºæ¢°è‡‚å¿…é¡»å°†å…¶æœ«ç«¯æ‰§è¡Œå™¨æ”¾ç½®åœ¨ç›®æ ‡ä½ç½® (ç»¿è‰²çƒ)ã€‚

```py
env_id = "PandaReachDense-v3"

# Create the env
env = gym.make(env_id)

# Get the state space and action space
s_size = env.observation_space.shape
a_size = env.action_space
```

```py
print("_____OBSERVATION SPACE_____ \n")
print("The State Space is: ", s_size)
print("Sample observation", env.observation_space.sample()) # Get a random observation
```

è§‚å¯Ÿç©ºé—´**æ˜¯ä¸€ä¸ªå…·æœ‰ 3 ä¸ªä¸åŒå…ƒç´ çš„å­—å…¸**ï¼š

+   `achieved_goal`: ç›®æ ‡çš„ä½ç½® (x,y,z)ã€‚

+   `desired_goal`: ç›®æ ‡ä½ç½®ä¸å½“å‰ç‰©ä½“ä½ç½®ä¹‹é—´çš„è·ç¦» (x,y,z)ã€‚

+   `observation`: æœ«ç«¯æ‰§è¡Œå™¨çš„ä½ç½® (x,y,z) å’Œé€Ÿåº¦ (vx, vy, vz)ã€‚

é‰´äºè§‚å¯Ÿæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œ**æˆ‘ä»¬éœ€è¦ä½¿ç”¨ MultiInputPolicy ç­–ç•¥è€Œä¸æ˜¯ MlpPolicy**ã€‚

```py
print("\n _____ACTION SPACE_____ \n")
print("The Action Space is: ", a_size)
print("Action Space Sample", env.action_space.sample()) # Take a random action
```

åŠ¨ä½œç©ºé—´æ˜¯ä¸€ä¸ªå…·æœ‰ 3 ä¸ªå€¼çš„å‘é‡ï¼š

+   æ§åˆ¶ xã€yã€z è¿åŠ¨

### å½’ä¸€åŒ–è§‚å¯Ÿå’Œå¥–åŠ±

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸€ä¸ªå¥½çš„å®è·µæ˜¯[å½’ä¸€åŒ–è¾“å…¥ç‰¹å¾](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)ã€‚

ä¸ºæ­¤ï¼Œæœ‰ä¸€ä¸ªåŒ…è£…å™¨å°†è®¡ç®—è¾“å…¥ç‰¹å¾çš„è¿è¡Œå¹³å‡å€¼å’Œæ ‡å‡†å·®ã€‚

æˆ‘ä»¬è¿˜é€šè¿‡æ·»åŠ  `norm_reward = True` æ¥ä½¿ç”¨ç›¸åŒçš„åŒ…è£…å™¨å¯¹å¥–åŠ±è¿›è¡Œå½’ä¸€åŒ–

[æ‚¨åº”è¯¥æŸ¥çœ‹æ–‡æ¡£ä»¥å¡«å†™æ­¤å•å…ƒæ ¼](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)

```py
env = make_vec_env(env_id, n_envs=4)

# Adding this wrapper to normalize the observation and the reward
env = # TODO: Add the wrapper
```

#### è§£å†³æ–¹æ¡ˆ

```py
env = make_vec_env(env_id, n_envs=4)

env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)
```

### åˆ›å»º A2C æ¨¡å‹ ğŸ¤–

æœ‰å…³ä½¿ç”¨ StableBaselines3 å®ç° A2C çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ï¼š[`stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes`](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes)

ä¸ºäº†æ‰¾åˆ°æœ€ä½³å‚æ•°ï¼Œæˆ‘æ£€æŸ¥äº†[ç”± Stable-Baselines3 å›¢é˜Ÿå®˜æ–¹è®­ç»ƒçš„ä»£ç†](https://huggingface.co/sb3)ã€‚

```py
model = # Create the A2C model and try to find the best parameters
```

#### è§£å†³æ–¹æ¡ˆ

```py
model = A2C(policy = "MultiInputPolicy",
            env = env,
            verbose=1)
```

### è®­ç»ƒ A2C ä»£ç† ğŸƒ

+   è®©æˆ‘ä»¬å¯¹ä»£ç†è¿›è¡Œ 1,000,000 ä¸ªæ—¶é—´æ­¥çš„è®­ç»ƒï¼Œä¸è¦å¿˜è®°åœ¨ Colab ä¸Šä½¿ç”¨ GPUã€‚å¤§çº¦éœ€è¦ 25-40 åˆ†é’Ÿ

```py
model.learn(1_000_000)
```

```py
# Save the model and  VecNormalize statistics when saving the agent
model.save("a2c-PandaReachDense-v3")
env.save("vec_normalize.pkl")
```

### è¯„ä¼°ä»£ç† ğŸ“ˆ

+   ç°åœ¨æˆ‘ä»¬çš„ä»£ç†å·²ç»è®­ç»ƒå¥½äº†ï¼Œæˆ‘ä»¬éœ€è¦**æ£€æŸ¥å…¶æ€§èƒ½**ã€‚

+   Stable-Baselines3 æä¾›äº†ä¸€ä¸ªæ–¹æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼š`evaluate_policy`

```py
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

# Load the saved statistics
eval_env = DummyVecEnv([lambda: gym.make("PandaReachDense-v3")])
eval_env = VecNormalize.load("vec_normalize.pkl", eval_env)

# We need to override the render_mode
eval_env.render_mode = "rgb_array"

#  do not update them at test time
eval_env.training = False
# reward normalization is not needed at test time
eval_env.norm_reward = False

# Load the agent
model = A2C.load("a2c-PandaReachDense-v3")

mean_reward, std_reward = evaluate_policy(model, eval_env)

print(f"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}")
```

### åœ¨ Hub ä¸Šå‘å¸ƒæ‚¨è®­ç»ƒå¥½çš„æ¨¡å‹ ğŸ”¥

ç°åœ¨æˆ‘ä»¬çœ‹åˆ°è®­ç»ƒåå–å¾—äº†è‰¯å¥½çš„ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€è¡Œä»£ç å°†æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹å‘å¸ƒåˆ° Hub ä¸Šã€‚

ğŸ“š å›¾ä¹¦é¦†æ–‡æ¡£ ğŸ‘‰ [`github.com/huggingface/huggingface_sb3/tree/main#hugging-faceâ€”x-stable-baselines3-v20`](https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20)

é€šè¿‡ä½¿ç”¨ `package_to_hub`ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨å‰é¢çš„å•å…ƒä¸­å·²ç»æåˆ°çš„ï¼Œ**æ‚¨å¯ä»¥è¯„ä¼°ã€è®°å½•é‡æ’­ã€ç”Ÿæˆä»£ç†çš„æ¨¡å‹å¡å¹¶å°†å…¶æ¨é€åˆ° Hub**ã€‚

è¿™æ ·ï¼š

+   æ‚¨å¯ä»¥ **å±•ç¤ºæˆ‘ä»¬çš„å·¥ä½œ** ğŸ”¥

+   æ‚¨å¯ä»¥ **æŸ¥çœ‹æ‚¨çš„ä»£ç†è¿›è¡Œæ¸¸æˆ** ğŸ‘€

+   æ‚¨å¯ä»¥ **ä¸ç¤¾åŒºåˆ†äº«å…¶ä»–äººå¯ä»¥ä½¿ç”¨çš„ä»£ç†** ğŸ’¾

+   æ‚¨å¯ä»¥ **è®¿é—®æ’è¡Œæ¦œ ğŸ† æŸ¥çœ‹æ‚¨çš„ä»£ç†ç›¸å¯¹äºåŒå­¦è¡¨ç°å¦‚ä½•** ğŸ‘‰ [`huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard`](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)

è¦èƒ½å¤Ÿä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ï¼Œè¿˜éœ€è¦éµå¾ªä¸‰ä¸ªæ­¥éª¤ï¼š

1ï¸âƒ£ (å¦‚æœå°šæœªå®Œæˆ) åˆ›å»ºä¸€ä¸ª HF å¸æˆ· â¡ [`huggingface.co/join`](https://huggingface.co/join)

2ï¸âƒ£ ç™»å½•ï¼Œç„¶åï¼Œæ‚¨éœ€è¦ä» Hugging Face ç½‘ç«™å­˜å‚¨æ‚¨çš„èº«ä»½éªŒè¯ä»¤ç‰Œã€‚

+   åˆ›å»ºä¸€ä¸ªæ–°çš„ä»¤ç‰Œ ([`huggingface.co/settings/tokens`](https://huggingface.co/settings/tokens)) **å…·æœ‰å†™å…¥æƒé™**

![åˆ›å»º HF ä»¤ç‰Œ](img/d21a97c736edaab9119d2d1c1da9deac.png)

+   å¤åˆ¶ä»¤ç‰Œ

+   è¿è¡Œä¸‹é¢çš„å•å…ƒæ ¼å¹¶ç²˜è´´ä»¤ç‰Œ

```py
notebook_login()
!git config --global credential.helper store
```

å¦‚æœæ‚¨ä¸æƒ³ä½¿ç”¨ Google Colab æˆ– Jupyter Notebookï¼Œæ‚¨éœ€è¦ä½¿ç”¨è¿™ä¸ªå‘½ä»¤ï¼š`huggingface-cli login`

3ï¸âƒ£ ç°åœ¨æˆ‘ä»¬å‡†å¤‡ä½¿ç”¨ `package_to_hub()` å‡½æ•°å°†æˆ‘ä»¬è®­ç»ƒå¥½çš„ agent æ¨é€åˆ° ğŸ¤— Hub ğŸ”¥ã€‚å¯¹äºè¿™ä¸ªç¯å¢ƒï¼Œ**è¿è¡Œè¿™ä¸ªå•å…ƒæ ¼å¯èƒ½éœ€è¦å¤§çº¦ 10 åˆ†é’Ÿ**ã€‚

```py
from huggingface_sb3 import package_to_hub

package_to_hub(
    model=model,
    model_name=f"a2c-{env_id}",
    model_architecture="A2C",
    env_id=env_id,
    eval_env=eval_env,
    repo_id=f"ThomasSimonini/a2c-{env_id}", # Change the username
    commit_message="Initial commit",
)
```

## ä¸€äº›é¢å¤–çš„æŒ‘æˆ˜ ğŸ†

å­¦ä¹ çš„æœ€å¥½æ–¹æ³• **æ˜¯è‡ªå·±å°è¯•**ï¼ä¸ºä»€ä¹ˆä¸å°è¯• `PandaPickAndPlace-v3`ï¼Ÿ

å¦‚æœæ‚¨æƒ³å°è¯•æ›´é«˜çº§çš„ä»»åŠ¡ï¼Œæ‚¨éœ€è¦æ£€æŸ¥ä½¿ç”¨ **TQC æˆ– SAC**ï¼ˆé€‚ç”¨äºæœºå™¨äººä»»åŠ¡çš„æ›´é«˜æ•ˆçš„ç®—æ³•ï¼‰å®Œæˆäº†ä»€ä¹ˆã€‚åœ¨çœŸå®çš„æœºå™¨äººæŠ€æœ¯ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨æ›´é«˜æ•ˆçš„ç®—æ³•ï¼Œä¸€ä¸ªç®€å•çš„åŸå› æ˜¯ï¼šä¸æ¨¡æ‹Ÿç›¸åï¼Œ**å¦‚æœæ‚¨ç§»åŠ¨æœºå™¨äººæ‰‹è‡‚å¤ªå¤šï¼Œæ‚¨æœ‰ç ´åçš„é£é™©**ã€‚

PandaPickAndPlace-v1ï¼ˆæ­¤æ¨¡å‹ä½¿ç”¨ç¯å¢ƒçš„ v1 ç‰ˆæœ¬ï¼‰ï¼š[`huggingface.co/sb3/tqc-PandaPickAndPlace-v1`](https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1)

å¹¶ä¸”ä¸è¦çŠ¹è±«æŸ¥çœ‹ panda-gym æ–‡æ¡£ï¼š[`panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html`](https://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html)

æˆ‘ä»¬ä¸ºæ‚¨æä¾›äº†è®­ç»ƒå¦ä¸€ä¸ª agent çš„æ­¥éª¤ï¼ˆå¯é€‰ï¼‰ï¼š

1.  å®šä¹‰åä¸º â€œPandaPickAndPlace-v3â€ çš„ç¯å¢ƒ

1.  åˆ›å»ºä¸€ä¸ªçŸ¢é‡åŒ–ç¯å¢ƒ

1.  æ·»åŠ ä¸€ä¸ªåŒ…è£…å™¨æ¥è§„èŒƒè§‚å¯Ÿå’Œå¥–åŠ±ã€‚[æŸ¥çœ‹æ–‡æ¡£](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)

1.  åˆ›å»º A2C æ¨¡å‹ï¼ˆä¸è¦å¿˜è®° verbose=1 ä»¥æ‰“å°è®­ç»ƒæ—¥å¿—ï¼‰ã€‚

1.  è®­ç»ƒ 100 ä¸‡ä¸ªæ—¶é—´æ­¥

1.  ä¿å­˜æ¨¡å‹å’Œ VecNormalize ç»Ÿè®¡æ•°æ®æ—¶ä¿å­˜ agent

1.  è¯„ä¼°æ‚¨çš„ agent

1.  ä½¿ç”¨ `package_to_hub` åœ¨ Hub ğŸ”¥ ä¸Šå‘å¸ƒæ‚¨è®­ç»ƒå¥½çš„æ¨¡å‹

### è§£å†³æ–¹æ¡ˆï¼ˆå¯é€‰ï¼‰

```py
# 1 - 2
env_id = "PandaPickAndPlace-v3"
env = make_vec_env(env_id, n_envs=4)

# 3
env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)

# 4
model = A2C(policy = "MultiInputPolicy",
            env = env,
            verbose=1)
# 5
model.learn(1_000_000)
```

```py
# 6
model_name = "a2c-PandaPickAndPlace-v3";
model.save(model_name)
env.save("vec_normalize.pkl")

# 7
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

# Load the saved statistics
eval_env = DummyVecEnv([lambda: gym.make("PandaPickAndPlace-v3")])
eval_env = VecNormalize.load("vec_normalize.pkl", eval_env)

#  do not update them at test time
eval_env.training = False
# reward normalization is not needed at test time
eval_env.norm_reward = False

# Load the agent
model = A2C.load(model_name)

mean_reward, std_reward = evaluate_policy(model, eval_env)

print(f"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}")

# 8
package_to_hub(
    model=model,
    model_name=f"a2c-{env_id}",
    model_architecture="A2C",
    env_id=env_id,
    eval_env=eval_env,
    repo_id=f"ThomasSimonini/a2c-{env_id}", # TODO: Change the username
    commit_message="Initial commit",
)
```

åœ¨ç¬¬ 7 å•å…ƒè§ï¼ğŸ”¥

## ç»§ç»­å­¦ä¹ ï¼Œä¿æŒå‡ºè‰² ğŸ¤—
