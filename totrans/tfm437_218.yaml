- en: Phi
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/phi](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/phi)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/218.5bf53c57.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Markdown.fef84341.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/stores.c16bc1a5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Phi-1 model was proposed in [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644)
    by Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del
    Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli
    Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck,
    Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee and Yuanzhi Li.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Phi-1.5 model was proposed in [Textbooks Are All You Need II: phi-1.5 technical
    report](https://arxiv.org/abs/2309.05463) by Yuanzhi Li, Sébastien Bubeck, Ronen
    Eldan, Allie Del Giorno, Suriya Gunasekar and Yin Tat Lee.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Phi-1 and Phi-1.5 papers, the authors showed how important the quality of
    the data is in training relative to the model size. They selected high quality
    “textbook” data alongside with synthetically generated data for training their
    small sized Transformer based model Phi-1 with 1.3B parameters. Despite this small
    scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. They
    follow the same strategy for Phi-1.5 and created another 1.3B parameter model
    with performance on natural language tasks comparable to models 5x larger, and
    surpassing most non-frontier LLMs. Phi-1.5 exhibits many of the traits of much
    larger LLMs such as the ability to “think step by step” or perform some rudimentary
    in-context learning. With these two experiments the authors successfully showed
    the huge impact of quality of training data when training machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the Phi-1 paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We introduce phi-1, a new large language model for code, with significantly
    smaller size than competing models: phi-1 is a Transformer-based model with 1.3B
    parameters, trained for 4 days on 8 A100s, using a selection of “textbook quality”
    data from the web (6B tokens) and synthetically generated textbooks and exercises
    with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy
    50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties
    compared to phi-1-base, our model before our finetuning stage on a dataset of
    coding exercises, and phi-1-small, a smaller model with 350M parameters trained
    with the same pipeline as phi-1 that still achieves 45% on HumanEval.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the Phi-1.5 paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We continue the investigation into the power of smaller Transformer-based
    language models as initiated by TinyStories – a 10 million parameter model that
    can produce coherent English – and the follow-up work on phi-1, a 1.3 billion
    parameter model with Python coding performance close to the state-of-the-art.
    The latter work proposed to use existing Large Language Models (LLMs) to generate
    “textbook quality” data as a way to enhance the learning process compared to traditional
    web data. We follow the “Textbooks Are All You Need” approach, focusing this time
    on common sense reasoning in natural language, and create a new 1.3 billion parameter
    model named phi-1.5, with performance on natural language tasks comparable to
    models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning
    tasks such as grade-school mathematics and basic coding. More generally, phi-1.5
    exhibits many of the traits of much larger LLMs, both good –such as the ability
    to “think step by step” or perform some rudimentary in-context learning– and bad,
    including hallucinations and the potential for toxic and biased generations –encouragingly
    though, we are seeing improvement on that front thanks to the absence of web data.
    We open-source phi-1.5 to promote further research on these urgent topics.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [Susnato Dhar](https://huggingface.co/susnato).
  prefs: []
  type: TYPE_NORMAL
- en: The original code for Phi-1, Phi-1.5 and Phi-2 can be found [here](https://huggingface.co/microsoft/phi-1),
    [here](https://huggingface.co/microsoft/phi-1_5) and [here](https://huggingface.co/microsoft/phi-2),
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This model is quite similar to `Llama` with the main difference in `PhiDecoderLayer`,
    where they used `PhiAttention` and `PhiMLP` layers in parallel configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tokenizer used for this model is identical to the [CodeGenTokenizer](/docs/transformers/v4.37.2/en/model_doc/codegen#transformers.CodeGenTokenizer).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use Phi-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Phi-2 has been integrated in the development version (4.37.0.dev) of `transformers`.
    Until the official version is released through `pip`, ensure that you are doing
    one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: When loading the model, ensure that `trust_remote_code=True` is passed as an
    argument of the `from_pretrained()` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Update your local `transformers` to the development version: `pip uninstall
    -y transformers && pip install git+https://github.com/huggingface/transformers`.
    The previous command is an alternative to cloning and installing from the source.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Example :'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Combining Phi and Flash Attention 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, make sure to install the latest version of Flash Attention 2 to include
    the sliding window attention feature.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Make also sure that you have a hardware that is compatible with Flash-Attention
    2\. Read more about it in the official documentation of flash-attn repository.
    Make also sure to load your model in half-precision (e.g. `torch.float16“)
  prefs: []
  type: TYPE_NORMAL
- en: 'To load and run a model using Flash Attention 2, refer to the snippet below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Expected speedups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Below is an expected speedup diagram that compares pure inference time between
    the native implementation in transformers using `microsoft/phi-1` checkpoint and
    the Flash Attention 2 version of the model using a sequence length of 2048.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f98ac01294888a88a3a01db2bbd981a.png)'
  prefs: []
  type: TYPE_IMG
- en: PhiConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PhiConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/phi/configuration_phi.py#L32)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 51200) — Vocabulary size of the
    Phi model. Defines the number of different tokens that can be represented by the
    `inputs_ids` passed when calling [PhiModel](/docs/transformers/v4.37.2/en/model_doc/phi#transformers.PhiModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 2048) — Dimension of the hidden
    representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intermediate_size` (`int`, *optional*, defaults to 8192) — Dimension of the
    MLP representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 24) — Number of hidden
    layers in the Transformer decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 32) — Number of attention
    heads for each attention layer in the Transformer decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_key_value_heads` (`int`, *optional*) — This is the number of key_value
    heads that should be used to implement Grouped Query Attention. If `num_key_value_heads=num_attention_heads`,
    the model will use Multi Head Attention (MHA), if `num_key_value_heads=1 the model
    will use Multi Query Attention (MQA) otherwise GQA is used. When converting a
    multi-head checkpoint to a GQA checkpoint, each group key and value head should
    be constructed by meanpooling all the original heads within that group. For more
    details checkout [this paper](https://arxiv.org/pdf/2305.13245.pdf). If it is
    not specified, will default to` num_attention_heads`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resid_pdrop` (`float`, *optional*, defaults to 0.0) — Dropout probability
    for mlp outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embd_pdrop` (`int`, *optional*, defaults to 0.0) — The dropout ratio for the
    embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    after computing the attention scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu_new"`) —
    The non-linear activation function (function or string) in the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 2048) — The maximum
    sequence length that this model might ever be used with. Phi-1 and Phi-1.5 supports
    up to 2048 tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the rms normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models). Only relevant
    if `config.is_decoder=True`. Whether to tie weight embeddings or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tie_word_embeddings` (`bool`, *optional*, defaults to `False`) — Whether to
    tie weight embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rope_theta` (`float`, *optional*, defaults to 10000.0) — The base period of
    the RoPE embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rope_scaling` (`Dict`, *optional*) — Dictionary containing the scaling configuration
    for the RoPE embeddings. Currently supports two scaling strategies: linear and
    dynamic. Their scaling factor must be an float greater than 1\. The expected format
    is `{"type": strategy name, "factor": scaling factor}`. When using this flag,
    don’t update `max_position_embeddings` to the expected new maximum. See the following
    thread for more information on how these scaling strategies behave: [https://www.reddit.com/r/LocalPersimmon/comments/14mrgpr/dynamically_scaled_rope_further_increases/](https://www.reddit.com/r/LocalPersimmon/comments/14mrgpr/dynamically_scaled_rope_further_increases/).
    This is an experimental feature, subject to breaking API changes in future versions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`partial_rotary_factor` (`float`, *optional*, defaults to 0.5) — Percentage
    of the query and keys which will have rotary embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qk_layernorm` (`bool`, *optional*, defaults to `False`) — Whether or not to
    normalize the Queries and Keys after projecting the hidden states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token_id` (`int`, *optional*, defaults to 1) — Denotes beginning of sequences
    token id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`int`, *optional*, defaults to 2) — Denotes end of sequences
    token id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [PhiModel](/docs/transformers/v4.37.2/en/model_doc/phi#transformers.PhiModel).
    It is used to instantiate an Phi model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the Phi [microsoft/phi-1](https://huggingface.co/microsoft/phi-1).
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: PhiModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PhiModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/phi/modeling_phi.py#L801)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PhiConfig](/docs/transformers/v4.37.2/en/model_doc/phi#transformers.PhiConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights. config — PhiConfig'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare Phi Model outputting raw hidden-states without any specific head on
    top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer
    is a `PhiDecoderLayer`
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/phi/modeling_phi.py#L836)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `input_ids` have to be
    input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    — Pre-computed hidden-states (key and values in the self-attention blocks and
    in the cross-attention blocks) that can be used to speed up sequential decoding.
    This typically consists in the `past_key_values` returned by the model at a previous
    stage of decoding, when `use_cache=True` or `config.use_cache=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two formats are allowed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a [Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)
    instance;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple
    having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`).
    This is also known as the legacy cache format.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model will output the same cache format that is fed as input. If no `past_key_values`
    are passed, the legacy cache format will be returned.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [PhiModel](/docs/transformers/v4.37.2/en/model_doc/phi#transformers.PhiModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: PhiForCausalLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PhiForCausalLM`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/phi/modeling_phi.py#L961)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/phi/modeling_phi.py#L998)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `input_ids` have to be
    input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    — Pre-computed hidden-states (key and values in the self-attention blocks and
    in the cross-attention blocks) that can be used to speed up sequential decoding.
    This typically consists in the `past_key_values` returned by the model at a previous
    stage of decoding, when `use_cache=True` or `config.use_cache=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two formats are allowed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a [Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)
    instance;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple
    having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`).
    This is also known as the legacy cache format.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model will output the same cache format that is fed as input. If no `past_key_values`
    are passed, the legacy cache format will be returned.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Args — labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*): Labels for computing the masked language modeling loss. Indices should
    either be in `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring).
    Tokens with indices set to `-100` are ignored (masked), the loss is only computed
    for the tokens with labels in `[0, ..., config.vocab_size]`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([PhiConfig](/docs/transformers/v4.37.2/en/model_doc/phi#transformers.PhiConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [PhiForCausalLM](/docs/transformers/v4.37.2/en/model_doc/phi#transformers.PhiForCausalLM)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L1173)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`inputs` (`torch.Tensor` of varying shape depending on the modality, *optional*)
    — The sequence used as a prompt for the generation or as model inputs to the encoder.
    If `None` the method initializes it with `bos_token_id` and a batch size of 1\.
    For decoder-only models `inputs` should of in the format of `input_ids`. For encoder-decoder
    models *inputs* can represent any of `input_ids`, `input_values`, `input_features`,
    or `pixel_values`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — Custom logits processors
    that complement the default logits processors built from arguments and generation
    config. If a logit processor is passed that is already created with the arguments
    or a generation config an error is thrown. This feature is intended for advanced
    users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — Custom stopping
    criteria that complement the default stopping criteria built from arguments and
    a generation config. If a stopping criteria is passed that is already created
    with the arguments or a generation config an error is thrown. If your stopping
    criteria depends on the `scores` input, make sure you pass `return_dict_in_generate=True,
    output_scores=True` to `generate`. This feature is intended for advanced users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`, *optional*)
    — If provided, this function constraints the beam search to allowed tokens only
    at each step. If not provided no constraint is applied. This function takes 2
    arguments: the batch ID `batch_id` and `input_ids`. It has to return a list with
    the allowed tokens for the next generation step conditioned on the batch ID `batch_id`
    and the previously generated tokens `inputs_ids`. This argument is useful for
    constrained generation conditioned on the prefix, as described in [Autoregressive
    Entity Retrieval](https://arxiv.org/abs/2010.00904).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`synced_gpus` (`bool`, *optional*) — Whether to continue running the while
    loop until max_length. Unless overridden this flag will be set to `True` under
    DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished
    generating before other GPUs. Otherwise it’ll be set to `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assistant_model` (`PreTrainedModel`, *optional*) — An assistant model that
    can be used to accelerate generation. The assistant model must have the exact
    same tokenizer. The acceleration is achieved when forecasting candidate tokens
    with the assistent model is much faster than running generation with the model
    you’re calling generate from. As such, the assistant model should be much smaller.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`streamer` (`BaseStreamer`, *optional*) — Streamer object that will be used
    to stream the generated sequences. Generated tokens are passed through `streamer.put(token_ids)`
    and the streamer is responsible for any further processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — The negative prompt needed for some processors such as CFG. The
    batch size must match the input batch size. This is an experimental feature, subject
    to breaking API changes in future versions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_attention_mask` (`torch.LongTensor` of shape `(batch_size,
    sequence_length)`, *optional*) — Attention_mask for `negative_prompt_ids`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Ad hoc parametrization of `generate_config`
    and/or additional model-specific kwargs that will be forwarded to the `forward`
    function of the model. If the model is an encoder-decoder model, encoder specific
    kwargs should not be prefixed and decoder specific kwargs should be prefixed with
    *decoder_*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    or `torch.LongTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: A [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    (if `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`)
    or a `torch.FloatTensor`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`),
    the possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GenerateBeamDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamDecoderOnlyOutput)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`),
    the possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates sequences of token ids for models with a language modeling head.
  prefs: []
  type: TYPE_NORMAL
- en: Most generation-controlling parameters are set in `generation_config` which,
    if not passed, will be set to the model’s default generation configuration. You
    can override any `generation_config` by passing the corresponding parameters to
    generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.
  prefs: []
  type: TYPE_NORMAL
- en: For an overview of generation strategies and code examples, check out the [following
    guide](../generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: PhiForSequenceClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PhiForSequenceClassification`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/phi/modeling_phi.py#L1155)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PhiConfig](/docs/transformers/v4.37.2/en/model_doc/phi#transformers.PhiConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PhiModel with a sequence classification head on top (linear layer).
  prefs: []
  type: TYPE_NORMAL
- en: '[PhiForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/phi#transformers.PhiForSequenceClassification)
    uses the last token in order to do the classification, as other causal models
    (e.g. GPT-2) do.'
  prefs: []
  type: TYPE_NORMAL
- en: Since it does classification on the last token, it requires to know the position
    of the last token. If a `pad_token_id` is defined in the configuration, it finds
    the last token that is not a padding token in each row. If no `pad_token_id` is
    defined, it simply takes the last value in each row of the batch. Since it cannot
    guess the padding tokens when `inputs_embeds` are passed instead of `input_ids`,
    it does the same (take the last value in each row of the batch).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/phi/modeling_phi.py#L1187)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `input_ids` have to be
    input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    — Pre-computed hidden-states (key and values in the self-attention blocks and
    in the cross-attention blocks) that can be used to speed up sequential decoding.
    This typically consists in the `past_key_values` returned by the model at a previous
    stage of decoding, when `use_cache=True` or `config.use_cache=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two formats are allowed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a [Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)
    instance;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple
    having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`).
    This is also known as the legacy cache format.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model will output the same cache format that is fed as input. If no `past_key_values`
    are passed, the legacy cache format will be returned.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [PhiForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/phi#transformers.PhiForSequenceClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: PhiForTokenClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PhiForTokenClassification`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/phi/modeling_phi.py#L1279)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PhiConfig](/docs/transformers/v4.37.2/en/model_doc/phi#transformers.PhiConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PhiModel with a token classification head on top (a linear layer on top of the
    hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/phi/modeling_phi.py#L1305)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `input_ids` have to be
    input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*)
    — Pre-computed hidden-states (key and values in the self-attention blocks and
    in the cross-attention blocks) that can be used to speed up sequential decoding.
    This typically consists in the `past_key_values` returned by the model at a previous
    stage of decoding, when `use_cache=True` or `config.use_cache=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two formats are allowed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a [Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)
    instance;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple
    having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`).
    This is also known as the legacy cache format.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model will output the same cache format that is fed as input. If no `past_key_values`
    are passed, the legacy cache format will be returned.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([PhiConfig](/docs/transformers/v4.37.2/en/model_doc/phi#transformers.PhiConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [PhiForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/phi#transformers.PhiForTokenClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
