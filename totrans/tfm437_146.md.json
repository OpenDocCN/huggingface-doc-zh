["```py\n>>> from transformers import T5ForConditionalGeneration\n>>> import torch\n\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\")\n\n>>> num_special_tokens = 3\n>>> # Model has 3 special tokens which take up the input ids 0,1,2 of ByT5.\n>>> # => Need to shift utf-8 character encodings by 3 before passing ids to model.\n\n>>> input_ids = torch.tensor([list(\"Life is like a box of chocolates.\".encode(\"utf-8\"))]) + num_special_tokens\n\n>>> labels = torch.tensor([list(\"La vie est comme une bo\u00eete de chocolat.\".encode(\"utf-8\"))]) + num_special_tokens\n\n>>> loss = model(input_ids, labels=labels).loss\n>>> loss.item()\n2.66\n```", "```py\n>>> from transformers import T5ForConditionalGeneration, AutoTokenizer\n\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n\n>>> model_inputs = tokenizer(\n...     [\"Life is like a box of chocolates.\", \"Today is Monday.\"], padding=\"longest\", return_tensors=\"pt\"\n... )\n>>> labels_dict = tokenizer(\n...     [\"La vie est comme une bo\u00eete de chocolat.\", \"Aujourd'hui c'est lundi.\"], padding=\"longest\", return_tensors=\"pt\"\n... )\n>>> labels = labels_dict.input_ids\n\n>>> loss = model(**model_inputs, labels=labels).loss\n>>> loss.item()\n17.9\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-base\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/byt5-base\")\n\n>>> input_ids_prompt = \"The dog chases a ball in the park.\"\n>>> input_ids = tokenizer(input_ids_prompt).input_ids\n\n>>> # Note that we cannot add \"{extra_id_...}\" to the string directly\n>>> # as the Byte tokenizer would incorrectly merge the tokens\n>>> # For ByT5, we need to work directly on the character level\n>>> # Contrary to T5, ByT5 does not use sentinel tokens for masking, but instead\n>>> # uses final utf character ids.\n>>> # UTF-8 is represented by 8 bits and ByT5 has 3 special tokens.\n>>> # => There are 2**8+2 = 259 input ids and mask tokens count down from index 258.\n>>> # => mask to \"The dog [258]a ball [257]park.\"\n\n>>> input_ids = torch.tensor([input_ids[:8] + [258] + input_ids[14:21] + [257] + input_ids[28:]])\n>>> input_ids\ntensor([[ 87, 107, 104,  35, 103, 114, 106,  35, 258,  35, 100,  35, 101, 100, 111, 111, 257,  35, 115, 100, 117, 110,  49,   1]])\n\n>>> # ByT5 produces only one char at a time so we need to produce many more output characters here -> set `max_length=100`.\n>>> output_ids = model.generate(input_ids, max_length=100)[0].tolist()\n>>> output_ids\n[0, 258, 108, 118,  35, 119, 107, 104,  35, 114, 113, 104,  35, 122, 107, 114,  35, 103, 114, 104, 118, 257,  35, 108, 113,  35, 119, 107, 104,  35, 103, 108, 118, 102, 114, 256, 108, 113,  35, 119, 107, 104, 35, 115, 100, 117, 110,  49,  35,  87, 107, 104,  35, 103, 114, 106, 35, 108, 118,  35, 119, 107, 104,  35, 114, 113, 104,  35, 122, 107, 114,  35, 103, 114, 104, 118,  35, 100,  35, 101, 100, 111, 111,  35, 108, 113, 255,  35, 108, 113,  35, 119, 107, 104,  35, 115, 100, 117, 110,  49]\n\n>>> # ^- Note how 258 descends to 257, 256, 255\n\n>>> # Now we need to split on the sentinel tokens, let's write a short loop for this\n>>> output_ids_list = []\n>>> start_token = 0\n>>> sentinel_token = 258\n>>> while sentinel_token in output_ids:\n...     split_idx = output_ids.index(sentinel_token)\n...     output_ids_list.append(output_ids[start_token:split_idx])\n...     start_token = split_idx\n...     sentinel_token -= 1\n\n>>> output_ids_list.append(output_ids[start_token:])\n>>> output_string = tokenizer.batch_decode(output_ids_list)\n>>> output_string\n['<pad>', 'is the one who does', ' in the disco', 'in the park. The dog is the one who does a ball in', ' in the park.']\n```", "```py\n( eos_token = '</s>' unk_token = '<unk>' pad_token = '<pad>' extra_ids = 125 additional_special_tokens = None **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( tokens )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```"]