- en: Quantization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é‡åŒ–
- en: 'Original text: [https://huggingface.co/docs/optimum/concept_guides/quantization](https://huggingface.co/docs/optimum/concept_guides/quantization)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/optimum/concept_guides/quantization](https://huggingface.co/docs/optimum/concept_guides/quantization)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Quantization is a technique to reduce the computational and memory costs of
    running inference by representing the weights and activations with low-precision
    data types like 8-bit integer (`int8`) instead of the usual 32-bit floating point
    (`float32`).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: é‡åŒ–æ˜¯ä¸€ç§é€šè¿‡ä½¿ç”¨ä½ç²¾åº¦æ•°æ®ç±»å‹ï¼ˆå¦‚8ä½æ•´æ•°ï¼ˆ`int8`ï¼‰ï¼‰ä»£æ›¿é€šå¸¸çš„32ä½æµ®ç‚¹æ•°ï¼ˆ`float32`ï¼‰æ¥è¡¨ç¤ºæƒé‡å’Œæ¿€æ´»æ¥å‡å°‘è¿è¡Œæ¨æ–­çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬çš„æŠ€æœ¯ã€‚
- en: Reducing the number of bits means the resulting model requires less memory storage,
    consumes less energy (in theory), and operations like matrix multiplication can
    be performed much faster with integer arithmetic. It also allows to run models
    on embedded devices, which sometimes only support integer data types.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å°‘ä½æ•°æ„å‘³ç€ç»“æœæ¨¡å‹éœ€è¦æ›´å°‘çš„å†…å­˜å­˜å‚¨ï¼Œæ¶ˆè€—æ›´å°‘çš„èƒ½é‡ï¼ˆç†è®ºä¸Šï¼‰ï¼Œå¹¶ä¸”åƒçŸ©é˜µä¹˜æ³•è¿™æ ·çš„æ“ä½œå¯ä»¥é€šè¿‡æ•´æ•°è¿ç®—æ›´å¿«åœ°æ‰§è¡Œã€‚å®ƒè¿˜å…è®¸åœ¨åµŒå…¥å¼è®¾å¤‡ä¸Šè¿è¡Œæ¨¡å‹ï¼Œæœ‰æ—¶è¿™äº›è®¾å¤‡åªæ”¯æŒæ•´æ•°æ•°æ®ç±»å‹ã€‚
- en: Theory
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç†è®º
- en: 'The basic idea behind quantization is quite easy: going from high-precision
    representation (usually the regular 32-bit floating-point) for weights and activations
    to a lower precision data type. The most common lower precision data types are:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: é‡åŒ–çš„åŸºæœ¬æ€æƒ³éå¸¸ç®€å•ï¼šä»é«˜ç²¾åº¦è¡¨ç¤ºï¼ˆé€šå¸¸æ˜¯å¸¸è§„çš„32ä½æµ®ç‚¹æ•°ï¼‰è½¬æ¢ä¸ºè¾ƒä½ç²¾åº¦æ•°æ®ç±»å‹ï¼Œæƒé‡å’Œæ¿€æ´»ã€‚æœ€å¸¸è§çš„è¾ƒä½ç²¾åº¦æ•°æ®ç±»å‹æ˜¯ï¼š
- en: '`float16`, accumulation data type `float16`'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`float16`ï¼Œç´¯ç§¯æ•°æ®ç±»å‹`float16`'
- en: '`bfloat16`, accumulation data type `float32`'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bfloat16`ï¼Œç´¯ç§¯æ•°æ®ç±»å‹`float32`'
- en: '`int16`, accumulation data type `int32`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`int16`ï¼Œç´¯ç§¯æ•°æ®ç±»å‹`int32`'
- en: '`int8`, accumulation data type `int32`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`int8`ï¼Œç´¯ç§¯æ•°æ®ç±»å‹`int32`'
- en: 'The accumulation data type specifies the type of the result of accumulating
    (adding, multiplying, etc) values of the data type in question. For example, letâ€™s
    consider two `int8` values `A = 127`, `B = 127`, and letâ€™s define `C` as the sum
    of `A` and `B`:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç´¯ç§¯æ•°æ®ç±»å‹æŒ‡å®šç´¯ç§¯ï¼ˆæ·»åŠ ã€ä¹˜æ³•ç­‰ï¼‰æ•°æ®ç±»å‹å€¼çš„ç»“æœç±»å‹ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬è€ƒè™‘ä¸¤ä¸ª`int8`å€¼`A = 127`ï¼Œ`B = 127`ï¼Œå¹¶ä¸”è®©`C`ä¸º`A`å’Œ`B`çš„å’Œï¼š
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here the result is much bigger than the biggest representable value in `int8`,
    which is `127`. Hence the need for a larger precision data type to avoid a huge
    precision loss that would make the whole quantization process useless.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„ç»“æœæ¯”`int8`ä¸­æœ€å¤§çš„å¯è¡¨ç¤ºå€¼`127`è¦å¤§å¾—å¤šã€‚å› æ­¤ï¼Œéœ€è¦æ›´å¤§ç²¾åº¦çš„æ•°æ®ç±»å‹ä»¥é¿å…å·¨å¤§çš„ç²¾åº¦æŸå¤±ï¼Œå¦åˆ™æ•´ä¸ªé‡åŒ–è¿‡ç¨‹å°†å˜å¾—æ— ç”¨ã€‚
- en: Quantization
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡åŒ–
- en: The two most common quantization cases are `float32 -> float16` and `float32
    -> int8`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¸¸è§çš„é‡åŒ–æƒ…å†µæ˜¯`float32 -> float16`å’Œ`float32 -> int8`ã€‚
- en: Quantization to float16
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é‡åŒ–ä¸ºfloat16
- en: 'Performing quantization to go from `float32` to `float16` is quite straightforward
    since both data types follow the same representation scheme. The questions to
    ask yourself when quantizing an operation to `float16` are:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å°†`float32`è½¬æ¢ä¸º`float16`çš„é‡åŒ–æ˜¯éå¸¸ç®€å•çš„ï¼Œå› ä¸ºè¿™ä¸¤ç§æ•°æ®ç±»å‹éµå¾ªç›¸åŒçš„è¡¨ç¤ºæ–¹æ¡ˆã€‚åœ¨å°†æ“ä½œé‡åŒ–ä¸º`float16`æ—¶ï¼Œè¦é—®è‡ªå·±çš„é—®é¢˜æ˜¯ï¼š
- en: Does my operation have a `float16` implementation?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘çš„æ“ä½œæ˜¯å¦æœ‰`float16`å®ç°ï¼Ÿ
- en: Does my hardware suport `float16`? For instance, Intel CPUs [have been supporting
    `float16` as a storage type, but computation is done after converting to `float32`](https://scicomp.stackexchange.com/a/35193).
    Full support will come in Cooper Lake and Sapphire Rapids.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘çš„ç¡¬ä»¶æ˜¯å¦æ”¯æŒ`float16`ï¼Ÿä¾‹å¦‚ï¼Œè‹±ç‰¹å°”CPU[ä¸€ç›´æ”¯æŒ`float16`ä½œä¸ºå­˜å‚¨ç±»å‹ï¼Œä½†æ˜¯è®¡ç®—æ˜¯åœ¨è½¬æ¢ä¸º`float32`ä¹‹åè¿›è¡Œçš„](https://scicomp.stackexchange.com/a/35193)ã€‚å®Œå…¨æ”¯æŒå°†åœ¨Cooper
    Lakeå’ŒSapphire Rapidsä¸­å®ç°ã€‚
- en: Is my operation sensitive to lower precision? For instance the value of epsilon
    in `LayerNorm` is usually very small (~ `1e-12`), but the smallest representable
    value in `float16` is ~ `6e-5`, this can cause `NaN` issues. The same applies
    for big values.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘çš„æ“ä½œå¯¹è¾ƒä½ç²¾åº¦æ•æ„Ÿå—ï¼Ÿä¾‹å¦‚ï¼Œåœ¨`LayerNorm`ä¸­epsilonçš„å€¼é€šå¸¸éå¸¸å°ï¼ˆ~ `1e-12`ï¼‰ï¼Œä½†åœ¨`float16`ä¸­æœ€å°çš„å¯è¡¨ç¤ºå€¼ä¸º~
    `6e-5`ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´`NaN`é—®é¢˜ã€‚å¯¹äºå¤§å€¼ä¹Ÿæ˜¯ä¸€æ ·ã€‚
- en: Quantization to int8
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é‡åŒ–ä¸ºint8
- en: Performing quantization to go from `float32` to `int8` is more tricky. Only
    256 values can be represented in `int8`, while `float32` can represent a very
    wide range of values. The idea is to find the best way to project our range `[a,
    b]` of `float32` values to the `int8` space.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å°†`float32`è½¬æ¢ä¸º`int8`çš„é‡åŒ–æ›´åŠ æ£˜æ‰‹ã€‚åœ¨`int8`ä¸­åªèƒ½è¡¨ç¤º256ä¸ªå€¼ï¼Œè€Œ`float32`å¯ä»¥è¡¨ç¤ºéå¸¸å¹¿æ³›çš„å€¼ã€‚å…³é”®æ˜¯æ‰¾åˆ°å°†æˆ‘ä»¬çš„`float32`å€¼èŒƒå›´`[a,
    b]`æŠ•å½±åˆ°`int8`ç©ºé—´çš„æœ€ä½³æ–¹æ³•ã€‚
- en: 'Letâ€™s consider a float `x` in `[a, b]`, then we can write the following quantization
    scheme, also called the *affine quantization scheme*:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªåœ¨`[a, b]`ä¸­çš„æµ®ç‚¹æ•°`x`ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ç¼–å†™ä»¥ä¸‹é‡åŒ–æ–¹æ¡ˆï¼Œä¹Ÿç§°ä¸º*ä»¿å°„é‡åŒ–æ–¹æ¡ˆ*ï¼š
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'where:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼š
- en: '`x_q` is the quantized `int8` value associated to `x`'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x_q`æ˜¯ä¸`x`ç›¸å…³è”çš„é‡åŒ–`int8`å€¼'
- en: '`S` and `Z` are the quantization parameters'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`S`å’Œ`Z`æ˜¯é‡åŒ–å‚æ•°'
- en: '`S` is the scale, and is a positive `float32`'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`S`æ˜¯æ¯”ä¾‹ï¼Œæ˜¯æ­£çš„`float32`'
- en: '`Z` is called the zero-point, it is the `int8` value corresponding to the value
    `0` in the `float32` realm. This is important to be able to represent exactly
    the value `0` because it is used everywhere throughout machine learning models.'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Z`ç§°ä¸ºé›¶ç‚¹ï¼Œå®ƒæ˜¯ä¸`float32`é¢†åŸŸä¸­å€¼`0`å¯¹åº”çš„`int8`å€¼ã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒç”¨äºæœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„å„ä¸ªåœ°æ–¹æ¥å‡†ç¡®è¡¨ç¤ºå€¼`0`ã€‚'
- en: 'The quantized value `x_q` of `x` in `[a, b]` can be computed as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`x`åœ¨`[a, b]`ä¸­çš„é‡åŒ–å€¼`x_q`å¯ä»¥è®¡ç®—å¦‚ä¸‹ï¼š'
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And `float32` values outside of the `[a, b]` range are clipped to the closest
    representable value, so for any floating-point number `x`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”è¶…å‡º`[a, b]`èŒƒå›´çš„`float32`å€¼è¢«å‰ªåˆ‡åˆ°æœ€æ¥è¿‘çš„å¯è¡¨ç¤ºå€¼ï¼Œå› æ­¤å¯¹äºä»»ä½•æµ®ç‚¹æ•°`x`ï¼š
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Usually `round(a/S + Z)` corresponds to the smallest representable value in
    the considered data type, and `round(b/S + Z)` to the biggest one. But this can
    vary, for instance when using a *symmetric quantization scheme* as you will see
    in the next paragraph.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œ`round(a/S + Z)` å¯¹åº”äºè€ƒè™‘çš„æ•°æ®ç±»å‹ä¸­æœ€å°çš„å¯è¡¨ç¤ºå€¼ï¼Œè€Œ`round(b/S + Z)` å¯¹åº”äºæœ€å¤§çš„å¯è¡¨ç¤ºå€¼ã€‚ä½†æ˜¯è¿™å¯èƒ½ä¼šæœ‰æ‰€å˜åŒ–ï¼Œä¾‹å¦‚å½“ä½¿ç”¨*å¯¹ç§°é‡åŒ–æ–¹æ¡ˆ*æ—¶ï¼Œæ‚¨å°†åœ¨ä¸‹ä¸€æ®µä¸­çœ‹åˆ°ã€‚
- en: Symmetric and affine quantization schemes
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¯¹ç§°å’Œä»¿å°„é‡åŒ–æ–¹æ¡ˆ
- en: The equation above is called the *affine quantization sheme* because the mapping
    from `[a, b]` to `int8` is an affine one.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢çš„æ–¹ç¨‹è¢«ç§°ä¸º*ä»¿å°„é‡åŒ–æ–¹æ¡ˆ*ï¼Œå› ä¸ºä»`[a, b]`åˆ°`int8`çš„æ˜ å°„æ˜¯ä»¿å°„çš„ã€‚
- en: A common special case of this scheme is the *symmetric quantization scheme*,
    where we consider a symmetric range of float values `[-a, a]`. In this case the
    integer space is usally `[-127, 127]`, meaning that the `-128` is opted out of
    the regular `[-128, 127]` signed `int8` range. The reason being that having both
    ranges symmetric allows to have `Z = 0`. While one value out of the 256 representable
    values is lost, it can provide a speedup since a lot of addition operations can
    be skipped.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–¹æ¡ˆçš„ä¸€ä¸ªå¸¸è§ç‰¹ä¾‹æ˜¯*å¯¹ç§°é‡åŒ–æ–¹æ¡ˆ*ï¼Œå…¶ä¸­æˆ‘ä»¬è€ƒè™‘æµ®ç‚¹å€¼çš„å¯¹ç§°èŒƒå›´`[-a, a]`ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ•´æ•°ç©ºé—´é€šå¸¸æ˜¯`[-127, 127]`ï¼Œè¿™æ„å‘³ç€`-128`è¢«æ’é™¤åœ¨å¸¸è§„çš„`[-128,
    127]`æœ‰ç¬¦å·`int8`èŒƒå›´ä¹‹å¤–ã€‚åŸå› æ˜¯ä¸¤ä¸ªèŒƒå›´éƒ½å¯¹ç§°å…è®¸æœ‰`Z = 0`ã€‚è™½ç„¶å¤±å»äº†256ä¸ªå¯è¡¨ç¤ºå€¼ä¸­çš„ä¸€ä¸ªï¼Œä½†å®ƒå¯ä»¥æä¾›åŠ é€Ÿï¼Œå› ä¸ºå¾ˆå¤šåŠ æ³•æ“ä½œå¯ä»¥è¢«è·³è¿‡ã€‚
- en: '**Note**: To learn how the quantization parameters `S` and `Z` are computed,
    you can read the [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only
    Inference](https://arxiv.org/abs/1712.05877) paper, or [Lei Maoâ€™s blog post](https://leimao.github.io/article/Neural-Networks-Quantization)
    on the subject.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„**ï¼šè¦äº†è§£é‡åŒ–å‚æ•°`S`å’Œ`Z`æ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Œå¯ä»¥é˜…è¯»[ç”¨äºé«˜æ•ˆæ•´æ•°è¿ç®—æ¨æ–­çš„ç¥ç»ç½‘ç»œé‡åŒ–å’Œè®­ç»ƒ](https://arxiv.org/abs/1712.05877)è®ºæ–‡ï¼Œæˆ–è€…å…³äºè¿™ä¸ªä¸»é¢˜çš„[Lei
    Maoçš„åšå®¢æ–‡ç« ](https://leimao.github.io/article/Neural-Networks-Quantization)ã€‚'
- en: Per-tensor and per-channel quantization
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¯ä¸ªå¼ é‡å’Œæ¯ä¸ªé€šé“çš„é‡åŒ–
- en: 'Depending on the accuracy / latency trade-off you are targetting you can play
    with the granularity of the quantization parameters:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®ç›®æ ‡å‡†ç¡®æ€§/å»¶è¿Ÿæƒè¡¡ï¼Œå¯ä»¥è°ƒæ•´é‡åŒ–å‚æ•°çš„ç²’åº¦ï¼š
- en: Quantization parameters can be computed on a *per-tensor* basis, meaning that
    one pair of `(S, Z)` will be used per tensor.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é‡åŒ–å‚æ•°å¯ä»¥æ ¹æ®*æ¯ä¸ªå¼ é‡*çš„åŸºç¡€è®¡ç®—ï¼Œè¿™æ„å‘³ç€æ¯ä¸ªå¼ é‡å°†ä½¿ç”¨ä¸€ä¸ª`(S, Z)`å¯¹ã€‚
- en: Quantization parameters can be computed on a *per-channel* basis, meaning that
    it is possible to store a pair of `(S, Z)` per element along one of the dimensions
    of a tensor. For example for a tensor of shape `[N, C, H, W]`, having *per-channel*
    quantization parameters for the second dimension would result in having `C` pairs
    of `(S, Z)`. While this can give a better accuracy, it requires more memory.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é‡åŒ–å‚æ•°å¯ä»¥æ ¹æ®*æ¯ä¸ªé€šé“*çš„åŸºç¡€è®¡ç®—ï¼Œè¿™æ„å‘³ç€å¯ä»¥åœ¨å¼ é‡çš„ä¸€ä¸ªç»´åº¦ä¸Šå­˜å‚¨ä¸€ä¸ª`(S, Z)`å¯¹ã€‚ä¾‹å¦‚ï¼Œå¯¹äºå½¢çŠ¶ä¸º`[N, C, H, W]`çš„å¼ é‡ï¼Œå¯¹ç¬¬äºŒä¸ªç»´åº¦è¿›è¡Œ*æ¯ä¸ªé€šé“*çš„é‡åŒ–å‚æ•°è®¡ç®—å°†å¯¼è‡´æœ‰`C`å¯¹`(S,
    Z)`ã€‚è™½ç„¶è¿™å¯èƒ½ä¼šæä¾›æ›´å¥½çš„å‡†ç¡®æ€§ï¼Œä½†éœ€è¦æ›´å¤šçš„å†…å­˜ã€‚
- en: Calibration
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ ¡å‡†
- en: 'The section above described how quantization from `float32` to `int8` works,
    but one question remains: how is the `[a, b]` range of `float32` values determined?
    That is where calibration comes in to play.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢çš„éƒ¨åˆ†æè¿°äº†å¦‚ä½•ä»`float32`åˆ°`int8`çš„é‡åŒ–å·¥ä½œï¼Œä½†ä¸€ä¸ªé—®é¢˜ä»ç„¶å­˜åœ¨ï¼šå¦‚ä½•ç¡®å®š`float32`å€¼çš„`[a, b]`èŒƒå›´ï¼Ÿè¿™å°±æ˜¯æ ¡å‡†å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚
- en: 'Calibration is the step during quantization where the `float32` ranges are
    computed. For weights it is quite easy since the actual range is known at *quantization-time*.
    But it is less clear for activations, and different approaches exist:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¡å‡†æ˜¯é‡åŒ–è¿‡ç¨‹ä¸­è®¡ç®—`float32`èŒƒå›´çš„æ­¥éª¤ã€‚å¯¹äºæƒé‡æ¥è¯´ï¼Œè¿™ç›¸å½“å®¹æ˜“ï¼Œå› ä¸ºåœ¨*é‡åŒ–æ—¶é—´*å·²çŸ¥å®é™…èŒƒå›´ã€‚ä½†å¯¹äºæ¿€æ´»æ¥è¯´ï¼Œæƒ…å†µä¸å¤ªæ¸…æ¥šï¼Œå­˜åœ¨ä¸åŒçš„æ–¹æ³•ï¼š
- en: 'Post training **dynamic quantization**: the range for each activation is computed
    on the fly at *runtime*. While this gives great results without too much work,
    it can be a bit slower than static quantization because of the overhead introduced
    by computing the range each time. It is also not an option on certain hardware.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®­ç»ƒå**åŠ¨æ€é‡åŒ–**ï¼šæ¯ä¸ªæ¿€æ´»çš„èŒƒå›´åœ¨*è¿è¡Œæ—¶*åŠ¨æ€è®¡ç®—ã€‚è™½ç„¶è¿™æ ·åšå¯ä»¥è·å¾—å¾ˆå¥½çš„ç»“æœè€Œä¸éœ€è¦å¤ªå¤šå·¥ä½œï¼Œä½†ç”±äºæ¯æ¬¡è®¡ç®—èŒƒå›´å¼•å…¥çš„å¼€é”€ï¼Œå®ƒå¯èƒ½æ¯”é™æ€é‡åŒ–æ…¢ä¸€ç‚¹ã€‚åœ¨æŸäº›ç¡¬ä»¶ä¸Šä¹Ÿä¸æ˜¯ä¸€ä¸ªé€‰é¡¹ã€‚
- en: 'Post training **static quantization**: the range for each activation is computed
    in advance at *quantization-time*, typically by passing representative data through
    the model and recording the activation values. In practice, the steps are:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®­ç»ƒå**é™æ€é‡åŒ–**ï¼šæ¯ä¸ªæ¿€æ´»çš„èŒƒå›´åœ¨*é‡åŒ–æ—¶é—´*æå‰è®¡ç®—ï¼Œé€šå¸¸é€šè¿‡å°†ä»£è¡¨æ€§æ•°æ®é€šè¿‡æ¨¡å‹å¹¶è®°å½•æ¿€æ´»å€¼æ¥å®ç°ã€‚åœ¨å®è·µä¸­ï¼Œæ­¥éª¤æ˜¯ï¼š
- en: Observers are put on activations to record their values.
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ¿€æ´»ä¸Šæ”¾ç½®è§‚å¯Ÿå™¨ä»¥è®°å½•å®ƒä»¬çš„å€¼ã€‚
- en: A certain number of forward passes on a calibration dataset is done (around
    `200` examples is enough).
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ ¡å‡†æ•°æ®é›†ä¸Šè¿›è¡Œä¸€å®šæ•°é‡çš„å‰å‘ä¼ é€’ï¼ˆå¤§çº¦`200`ä¸ªç¤ºä¾‹è¶³å¤Ÿï¼‰ã€‚
- en: The ranges for each computation are computed according to some *calibration
    technique*.
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ ¹æ®æŸç§*æ ¡å‡†æŠ€æœ¯*è®¡ç®—æ¯ä¸ªè®¡ç®—çš„èŒƒå›´ã€‚
- en: '**Quantization aware training**: the range for each activation is computed
    at *training-time*, following the same idea than post training static quantization.
    But â€œfake quantizeâ€ operators are used instead of observers: they record values
    just as observers do, but they also simulate the error induced by quantization
    to let the model adapt to it.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ**ï¼šæ¯ä¸ªæ¿€æ´»çš„èŒƒå›´åœ¨*è®­ç»ƒæ—¶é—´*è®¡ç®—ï¼Œéµå¾ªä¸è®­ç»ƒåé™æ€é‡åŒ–ç›¸åŒçš„æ€æƒ³ã€‚ä½†æ˜¯ä½¿ç”¨â€œä¼ªé‡åŒ–â€æ“ä½œç¬¦ä»£æ›¿è§‚å¯Ÿå™¨ï¼šå®ƒä»¬è®°å½•å€¼å°±åƒè§‚å¯Ÿå™¨ä¸€æ ·ï¼Œä½†ä¹Ÿæ¨¡æ‹Ÿç”±é‡åŒ–å¼•èµ·çš„è¯¯å·®ï¼Œè®©æ¨¡å‹é€‚åº”å®ƒã€‚'
- en: 'For both post training static quantization and quantization aware training,
    it is necessary to define calibration techniques, the most common are:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè®­ç»ƒåé™æ€é‡åŒ–å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼Œéœ€è¦å®šä¹‰æ ¡å‡†æŠ€æœ¯ï¼Œæœ€å¸¸è§çš„æ˜¯ï¼š
- en: 'Min-max: the computed range is `[min observed value, max observed value]`,
    this works well with weights.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€å°-æœ€å¤§ï¼šè®¡ç®—çš„èŒƒå›´æ˜¯`[è§‚å¯Ÿåˆ°çš„æœ€å°å€¼ï¼Œè§‚å¯Ÿåˆ°çš„æœ€å¤§å€¼]`ï¼Œè¿™å¯¹æƒé‡æ•ˆæœå¾ˆå¥½ã€‚
- en: 'Moving average min-max: the computed range is `[moving average min observed
    value, moving average max observed value]`, this works well with activations.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç§»åŠ¨å¹³å‡æœ€å°-æœ€å¤§ï¼šè®¡ç®—çš„èŒƒå›´æ˜¯`[ç§»åŠ¨å¹³å‡æœ€å°è§‚å¯Ÿå€¼ï¼Œç§»åŠ¨å¹³å‡æœ€å¤§è§‚å¯Ÿå€¼]`ï¼Œè¿™å¯¹æ¿€æ´»æ•ˆæœå¾ˆå¥½ã€‚
- en: 'Histogram: records a histogram of values along with min and max values, then
    chooses according to some criterion:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›´æ–¹å›¾ï¼šè®°å½•å€¼çš„ç›´æ–¹å›¾ä»¥åŠæœ€å°å’Œæœ€å¤§å€¼ï¼Œç„¶åæ ¹æ®æŸäº›æ ‡å‡†è¿›è¡Œé€‰æ‹©ï¼š
- en: 'Entropy: the range is computed as the one minimizing the error between the
    full-precision and the quantized data.'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç†µï¼šèŒƒå›´æ˜¯é€šè¿‡æœ€å°åŒ–å®Œæ•´ç²¾åº¦å’Œé‡åŒ–æ•°æ®ä¹‹é—´çš„è¯¯å·®æ¥è®¡ç®—çš„ã€‚
- en: 'Mean Square Error: the range is computed as the one minimizing the mean square
    error between the full-precision and the quantized data.'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡æ–¹è¯¯å·®ï¼šèŒƒå›´æ˜¯é€šè¿‡æœ€å°åŒ–å…¨ç²¾åº¦å’Œé‡åŒ–æ•°æ®ä¹‹é—´çš„å‡æ–¹è¯¯å·®æ¥è®¡ç®—çš„ã€‚
- en: 'Percentile: the range is computed using a given percentile value `p` on the
    observed values. The idea is to try to have `p%` of the observed values in the
    computed range. While this is possible when doing affine quantization, it is not
    always possible to exactly match that when doing symmetric quantization. You can
    check [how it is done in ONNX Runtime](https://github.com/microsoft/onnxruntime/blob/2cb12caf9317f1ded37f6db125cb03ba99320c40/onnxruntime/python/tools/quantization/calibrate.py#L698)
    for more details.'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç™¾åˆ†ä½æ•°ï¼šä½¿ç”¨ç»™å®šçš„ç™¾åˆ†ä½æ•°å€¼`p`åœ¨è§‚å¯Ÿå€¼ä¸Šè®¡ç®—èŒƒå›´ã€‚å…¶æ€æƒ³æ˜¯å°è¯•ä½¿è®¡ç®—èŒƒå›´ä¸­æœ‰`p%`çš„è§‚å¯Ÿå€¼ã€‚åœ¨è¿›è¡Œä»¿å°„é‡åŒ–æ—¶ï¼Œè¿™æ˜¯å¯èƒ½çš„ï¼Œä½†åœ¨è¿›è¡Œå¯¹ç§°é‡åŒ–æ—¶ï¼Œä¸æ€»æ˜¯å¯èƒ½å®Œå…¨åŒ¹é…ã€‚æ‚¨å¯ä»¥æŸ¥çœ‹[ONNX
    Runtimeä¸­çš„å®ç°æ–¹å¼](https://github.com/microsoft/onnxruntime/blob/2cb12caf9317f1ded37f6db125cb03ba99320c40/onnxruntime/python/tools/quantization/calibrate.py#L698)ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
- en: Pratical steps to follow to quantize a model to int8
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹é‡åŒ–ä¸ºint8çš„å®ç”¨æ­¥éª¤
- en: 'To effectively quantize a model to `int8`, the steps to follow are:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æœ‰æ•ˆåœ°å°†æ¨¡å‹é‡åŒ–ä¸º`int8`ï¼Œéœ€è¦éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š
- en: Choose which operators to quantize. Good operators to quantize are the one dominating
    it terms of computation time, for instance linear projections and matrix multiplications.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©è¦é‡åŒ–çš„è¿ç®—ç¬¦ã€‚å¥½çš„è¿ç®—ç¬¦æ˜¯åœ¨è®¡ç®—æ—¶é—´æ–¹é¢å ä¸»å¯¼åœ°ä½çš„è¿ç®—ç¬¦ï¼Œä¾‹å¦‚çº¿æ€§æŠ•å½±å’ŒçŸ©é˜µä¹˜æ³•ã€‚
- en: Try post-training dynamic quantization, if it is fast enough stop here, otherwise
    continue to step 3.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°è¯•åè®­ç»ƒåŠ¨æ€é‡åŒ–ï¼Œå¦‚æœè¶³å¤Ÿå¿«åˆ™åœ¨æ­¤åœæ­¢ï¼Œå¦åˆ™ç»§ç»­åˆ°ç¬¬3æ­¥ã€‚
- en: Try post-training static quantization which can be faster than dynamic quantization
    but often with a drop in terms of accuracy. Apply observers to your models in
    places where you want to quantize.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°è¯•åè®­ç»ƒé™æ€é‡åŒ–ï¼Œè¿™å¯èƒ½æ¯”åŠ¨æ€é‡åŒ–æ›´å¿«ï¼Œä½†é€šå¸¸ä¼šå¯¼è‡´ç²¾åº¦ä¸‹é™ã€‚åœ¨å¸Œæœ›é‡åŒ–çš„æ¨¡å‹ä¸­åº”ç”¨è§‚å¯Ÿå™¨ã€‚
- en: Choose a calibration technique and perform it.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©æ ¡å‡†æŠ€æœ¯å¹¶æ‰§è¡Œã€‚
- en: 'Convert the model to its quantized form: the observers are removed and the
    `float32` operators are converted to their `int8` coutnerparts.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹è½¬æ¢ä¸ºå…¶é‡åŒ–å½¢å¼ï¼šè§‚å¯Ÿå™¨è¢«ç§»é™¤ï¼Œ`float32`è¿ç®—ç¬¦è¢«è½¬æ¢ä¸ºå®ƒä»¬çš„`int8`å¯¹åº”é¡¹ã€‚
- en: 'Evaluate the quantized model: is the accuracy good enough? If yes, stop here,
    otherwise start again at step 3 but with quantization aware training this time.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¯„ä¼°é‡åŒ–æ¨¡å‹ï¼šç²¾åº¦æ˜¯å¦è¶³å¤Ÿå¥½ï¼Ÿå¦‚æœæ˜¯ï¼Œå°±åœ¨è¿™é‡Œåœæ­¢ï¼Œå¦åˆ™ä»ç¬¬3æ­¥é‡æ–°å¼€å§‹ï¼Œä½†è¿™æ¬¡è¿›è¡Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒã€‚
- en: Supported tools to perform quantization in ğŸ¤— Optimum
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ”¯æŒåœ¨ğŸ¤— Optimumä¸­æ‰§è¡Œé‡åŒ–çš„å·¥å…·
- en: 'ğŸ¤— Optimum provides APIs to perform quantization using different tools for different
    targets:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Optimumæä¾›äº†ä½¿ç”¨ä¸åŒå·¥å…·ä¸ºä¸åŒç›®æ ‡æ‰§è¡Œé‡åŒ–çš„APIã€‚
- en: The `optimum.onnxruntime` package allows to [quantize and run ONNX models](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization)
    using the ONNX Runtime tool.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimum.onnxruntime`åŒ…å…è®¸[é‡åŒ–å’Œè¿è¡ŒONNXæ¨¡å‹](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization)ä½¿ç”¨ONNX
    Runtimeå·¥å…·ã€‚'
- en: The `optimum.intel` package enables to [quantize](https://huggingface.co/docs/optimum/intel/optimization_inc)
    ğŸ¤— Transformers models while respecting accuracy and latency constraints.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimum.intel`åŒ…ä½¿å¾—å¯ä»¥åœ¨å°Šé‡ç²¾åº¦å’Œå»¶è¿Ÿçº¦æŸçš„æƒ…å†µä¸‹[é‡åŒ–](https://huggingface.co/docs/optimum/intel/optimization_inc)ğŸ¤—
    Transformersæ¨¡å‹ã€‚'
- en: The `optimum.fx` package provides wrappers around the [PyTorch quantization
    functions](https://pytorch.org/docs/stable/quantization-support.html#torch-quantization-quantize-fx)
    to allow graph-mode quantization of ğŸ¤— Transformers models in PyTorch. This is
    a lower-level API compared to the two mentioned above, giving more flexibility,
    but requiring more work on your end.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimum.fx`åŒ…æä¾›äº†å¯¹[PyTorché‡åŒ–å‡½æ•°](https://pytorch.org/docs/stable/quantization-support.html#torch-quantization-quantize-fx)çš„åŒ…è£…ï¼Œä»¥å…è®¸åœ¨PyTorchä¸­ä»¥å›¾æ¨¡å¼é‡åŒ–ğŸ¤—
    Transformersæ¨¡å‹ã€‚ä¸ä¸Šè¿°ä¸¤ç§æ–¹æ³•ç›¸æ¯”ï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´ä½çº§åˆ«çš„APIï¼Œæä¾›æ›´å¤šçµæ´»æ€§ï¼Œä½†éœ€è¦æ‚¨åšæ›´å¤šå·¥ä½œã€‚'
- en: The `optimum.gptq` package allows to [quantize and run LLM models](../llm_quantization/usage_guides/quantization)
    with GPTQ.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimum.gptq`åŒ…å…è®¸ä½¿ç”¨GPTQ[é‡åŒ–å’Œè¿è¡ŒLLMæ¨¡å‹](../llm_quantization/usage_guides/quantization)ã€‚'
- en: 'Going further: How do machines represent numbers?'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥äº†è§£ï¼šæœºå™¨å¦‚ä½•è¡¨ç¤ºæ•°å­—ï¼Ÿ
- en: The section is not fundamental to understand the rest. It explains in brief
    how numbers are represented in computers. Since quantization is about going from
    one representation to another, it can be useful to have some basics, but it is
    definitely not mandatory.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚å¯¹äºç†è§£å…¶ä½™éƒ¨åˆ†å¹¶ä¸æ˜¯å¿…ä¸å¯å°‘çš„ã€‚å®ƒç®€è¦è§£é‡Šäº†è®¡ç®—æœºä¸­æ•°å­—çš„è¡¨ç¤ºæ–¹å¼ã€‚ç”±äºé‡åŒ–æ˜¯ä»ä¸€ç§è¡¨ç¤ºå½¢å¼åˆ°å¦ä¸€ç§è¡¨ç¤ºå½¢å¼çš„è½¬æ¢ï¼Œäº†è§£ä¸€äº›åŸºç¡€çŸ¥è¯†å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼Œä½†ç»å¯¹ä¸æ˜¯å¼ºåˆ¶æ€§çš„ã€‚
- en: The most fundamental unit of representation for computers is the bit. Everything
    in computers is represented as a sequence of bits, including numbers. But the
    representation varies whether the numbers in question are integers or real numbers.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—æœºçš„æœ€åŸºæœ¬çš„è¡¨ç¤ºå•ä½æ˜¯ä½ã€‚è®¡ç®—æœºä¸­çš„æ‰€æœ‰å†…å®¹éƒ½è¡¨ç¤ºä¸ºä¸€ç³»åˆ—ä½ï¼ŒåŒ…æ‹¬æ•°å­—ã€‚ä½†æ˜¯ï¼Œè¡¨ç¤ºæ–¹å¼å–å†³äºæ‰€è®¨è®ºçš„æ•°å­—æ˜¯æ•´æ•°è¿˜æ˜¯å®æ•°ã€‚
- en: Integer representation
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ•´æ•°è¡¨ç¤º
- en: 'Integers are usually represented with the following bit lengths: `8`, `16`,
    `32`, `64`. When representing integers, two cases are considered:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ•´æ•°é€šå¸¸ä½¿ç”¨ä»¥ä¸‹ä½é•¿åº¦è¡¨ç¤ºï¼š`8`ã€`16`ã€`32`ã€`64`ã€‚åœ¨è¡¨ç¤ºæ•´æ•°æ—¶ï¼Œè€ƒè™‘ä¸¤ç§æƒ…å†µï¼š
- en: 'Unsigned (positive) integers: they are simply represented as a sequence of
    bits. Each bit corresponds to a power of two (from `0` to `n-1` where `n` is the
    bit-length), and the resulting number is the sum of those powers of two.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ— ç¬¦å·ï¼ˆæ­£ï¼‰æ•´æ•°ï¼šå®ƒä»¬ç®€å•åœ°è¡¨ç¤ºä¸ºä¸€ç³»åˆ—ä½ã€‚æ¯ä¸ªä½å¯¹åº”äºäºŒçš„å¹‚ï¼ˆä»`0`åˆ°`n-1`ï¼Œå…¶ä¸­`n`æ˜¯ä½é•¿åº¦ï¼‰ï¼Œç»“æœæ•°å­—æ˜¯è¿™äº›äºŒçš„å¹‚çš„å’Œã€‚
- en: 'Example: `19` is represented as an unsigned int8 as `00010011` because :'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š`19`è¡¨ç¤ºä¸ºæ— ç¬¦å·int8ä¸º`00010011`ï¼Œå› ä¸ºï¼š
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Signed integers: it is less straightforward to represent signed integers, and
    multiple approachs exist, the most common being the *twoâ€™s complement*. For more
    information, you can check the [Wikipedia page](https://en.wikipedia.org/wiki/Signed_number_representations)
    on the subject.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ‰ç¬¦å·æ•´æ•°ï¼šè¡¨ç¤ºæœ‰ç¬¦å·æ•´æ•°ä¸é‚£ä¹ˆç›´æ¥ï¼Œå­˜åœ¨å¤šç§æ–¹æ³•ï¼Œæœ€å¸¸è§çš„æ˜¯*äºŒè¿›åˆ¶è¡¥ç *ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹æœ‰å…³è¯¥ä¸»é¢˜çš„[Wikipediaé¡µé¢](https://en.wikipedia.org/wiki/Signed_number_representations)ã€‚
- en: Real numbers representation
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å®æ•°è¡¨ç¤º
- en: 'Real numbers are usually represented with the following bit lengths: `16`,
    `32`, `64`. The two main ways of representing real numbers are:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å®æ•°é€šå¸¸ç”¨ä»¥ä¸‹ä½é•¿åº¦è¡¨ç¤ºï¼š`16`ï¼Œ`32`ï¼Œ`64`ã€‚è¡¨ç¤ºå®æ•°çš„ä¸¤ç§ä¸»è¦æ–¹æ³•æ˜¯ï¼š
- en: 'Fixed-point: there are fixed number of digits reserved for representing the
    integer part and the fractional part.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šç‚¹ï¼šç”¨äºè¡¨ç¤ºæ•´æ•°éƒ¨åˆ†å’Œå°æ•°éƒ¨åˆ†çš„å›ºå®šä½æ•°ã€‚
- en: 'Floating-point: the number of digits for representing the integer and the fractional
    parts can vary.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æµ®ç‚¹ï¼šç”¨äºè¡¨ç¤ºæ•´æ•°å’Œå°æ•°éƒ¨åˆ†çš„ä½æ•°å¯ä»¥å˜åŒ–ã€‚
- en: 'The floating-point representation can represent bigger ranges of values, and
    this is the one we will be focusing on since it is the most commonly used. There
    are three components in the floating-point representation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æµ®ç‚¹è¡¨ç¤ºå¯ä»¥è¡¨ç¤ºæ›´å¤§èŒƒå›´çš„å€¼ï¼Œè¿™æ˜¯æˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨çš„ï¼Œå› ä¸ºå®ƒæ˜¯æœ€å¸¸ç”¨çš„ã€‚æµ®ç‚¹è¡¨ç¤ºä¸­æœ‰ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼š
- en: 'The sign bit: this is the bit specifying the sign of the number.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¬¦å·ä½ï¼šè¿™æ˜¯æŒ‡å®šæ•°å­—ç¬¦å·çš„ä½ã€‚
- en: The exponent part
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŒ‡æ•°éƒ¨åˆ†
- en: 5 bits in `float16`
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨`float16`ä¸­æœ‰5ä½
- en: 8 bits in `bfloat16`
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨`bfloat16`ä¸­æœ‰8ä½
- en: 8 bits in `float32`
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨`float32`ä¸­æœ‰8ä½
- en: 11 bits in `float64`
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨`float64`ä¸­æœ‰11ä½
- en: The mantissa
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°¾æ•°
- en: 11 bits in `float16` (10 explictly stored)
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨`float16`ä¸­æœ‰11ä½ï¼ˆ10ä½æ˜ç¡®å­˜å‚¨ï¼‰
- en: 8 bits in `bfloat16` (7 explicitly stored)
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨`bfloat16`ä¸­æœ‰8ä½ï¼ˆ7ä½æ˜ç¡®å­˜å‚¨ï¼‰
- en: 24 bits in `float32` (23 explictly stored)
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨`float32`ä¸­æœ‰24ä½ï¼ˆ23ä½æ˜ç¡®å­˜å‚¨ï¼‰
- en: 53 bits in `float64` (52 explictly stored)
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨`float64`ä¸­æœ‰53ä½ï¼ˆ52ä½æ˜ç¡®å­˜å‚¨ï¼‰
- en: For more information on the bits allocation for each data type, check the nice
    illustration on the Wikipedia page about the [bfloat16 floating-point format](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ¯ç§æ•°æ®ç±»å‹çš„ä½åˆ†é…çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ç»´åŸºç™¾ç§‘é¡µé¢ä¸Šå…³äº[bfloat16æµ®ç‚¹æ ¼å¼](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)çš„æ¼‚äº®æ’å›¾ã€‚
- en: 'For a real number `x` we have:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå®æ•°`x`ï¼Œæˆ‘ä»¬æœ‰ï¼š
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: References
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒèµ„æ–™
- en: The [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only
    Inference](https://arxiv.org/abs/1712.05877) paper
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œçš„é‡åŒ–å’Œè®­ç»ƒä»¥è¿›è¡Œé«˜æ•ˆçš„æ•´æ•°ç®—æœ¯æ¨æ–­
- en: The [Basics of Quantization in Machine Learning (ML) for Beginners](https://iq.opengenus.org/basics-of-quantization-in-ml/)
    blog post
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä¸­é‡åŒ–åŸºç¡€å…¥é—¨çš„åšæ–‡
- en: The [How to accelerate and compress neural networks with quantization](https://tivadardanka.com/blog/neural-networks-quantization)
    blog post
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ é€Ÿå’Œå‹ç¼©ç¥ç»ç½‘ç»œçš„é‡åŒ–æ–¹æ³•çš„åšæ–‡
- en: The Wikipedia pages on integers representation [here](https://en.wikipedia.org/wiki/Integer_(computer_science))
    and [here](https://en.wikipedia.org/wiki/Signed_number_representations)
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•´æ•°è¡¨ç¤ºçš„ç»´åŸºç™¾ç§‘é¡µé¢[è¿™é‡Œ](https://en.wikipedia.org/wiki/Integer_(computer_science))å’Œ[è¿™é‡Œ](https://en.wikipedia.org/wiki/Signed_number_representations)
- en: The Wikipedia pages on
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…³äºç»´åŸºç™¾ç§‘é¡µé¢
- en: '[bfloat16 floating-point format](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[bfloat16æµ®ç‚¹æ ¼å¼](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)'
- en: '[Half-precision floating-point format](https://en.wikipedia.org/wiki/Half-precision_floating-point_format)'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åŠç²¾åº¦æµ®ç‚¹æ ¼å¼](https://en.wikipedia.org/wiki/Half-precision_floating-point_format)'
- en: '[Single-precision floating-point format](https://en.wikipedia.org/wiki/Single-precision_floating-point_format)'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å•ç²¾åº¦æµ®ç‚¹æ ¼å¼](https://en.wikipedia.org/wiki/Single-precision_floating-point_format)'
- en: '[Double-precision floating-point format](https://en.wikipedia.org/wiki/Double-precision_floating-point_format)'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åŒç²¾åº¦æµ®ç‚¹æ ¼å¼](https://en.wikipedia.org/wiki/Double-precision_floating-point_format)'
