# 配置

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/configuration](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/configuration)

基类 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 实现了从本地文件或目录加载/保存配置的常用方法，或者从库提供的预训练模型配置（从 HuggingFace 的 AWS S3 存储库下载）。

每个派生的配置类都实现了特定于模型的属性。所有配置类中共有的属性是：`hidden_size`、`num_attention_heads` 和 `num_hidden_layers`。文本模型进一步实现了：`vocab_size`。

## PretrainedConfig

### `class transformers.PretrainedConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/configuration_utils.py#L49)

```py
( **kwargs )
```

参数

+   `name_or_path` (`str`, *可选*, 默认为 `""`) — 存储传递给 [PreTrainedModel.from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 或 [TFPreTrainedModel.from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained) 作为 `pretrained_model_name_or_path` 的字符串，如果配置是使用这种方法创建的。

+   `output_hidden_states` (`bool`, *可选*, 默认为 `False`) — 模型是否应返回所有隐藏状态。

+   `output_attentions` (`bool`, *可选*, 默认为 `False`) — 模型是否应返回所有注意力。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 模型是否应返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通元组。

+   `is_encoder_decoder` (`bool`, *可选*, 默认为 `False`) — 模型是否用作编码器/解码器。

+   `is_decoder` (`bool`, *可选*, 默认为 `False`) — 模型是否用作解码器或不用（在这种情况下，它用作编码器）。

+   `cross_attention_hidden_size**` (`bool`, *可选*) — 如果模型用作编码器-解码器设置中的解码器，并且交叉注意力隐藏维度与 `self.config.hidden_size` 不同，则为交叉注意力层的隐藏大小。

+   `add_cross_attention` (`bool`, *可选*, 默认为 `False`) — 是否应向模型添加交叉注意力层。请注意，此选项仅适用于可以在 [EncoderDecoderModel](/docs/transformers/v4.37.2/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel) 类中用作解码器模型的模型，其中包括 `AUTO_MODELS_FOR_CAUSAL_LM` 中的所有模型。

+   `tie_encoder_decoder` (`bool`, *可选*, 默认为 `False`) — 是否应将所有编码器权重绑定到它们对应的解码器权重。这要求编码器和解码器模型具有完全相同的参数名称。

+   `prune_heads` (`Dict[int, List[int]]`, *可选*, 默认为 `{}`) — 模型的剪枝头部。键是所选层索引，相关值是要在该层中剪枝的头部列表。

    例如 `{1: [0, 2], 2: [2, 3]}` 将在第 1 层剪枝头部 0 和 2，第 2 层剪枝头部 2 和 3。

+   `chunk_size_feed_forward` (`int`, *可选*, 默认为 `0`) — 残差注意力块中所有前馈层的块大小。块大小为 `0` 表示前馈层未分块。大小为 n 的块表示前馈层一次处理 `n` 个序列长度的嵌入。有关前馈分块的更多信息，请参见 [前馈分块是如何工作的？](../glossary.html#feed-forward-chunking)。

用于序列生成的参数

+   `max_length` (`int`, *可选*, 默认为 20) — 默认情况下在模型的 `generate` 方法中将使用的最大长度。

+   `min_length` (`int`, *optional*, 默认为0) — 在模型的`generate`方法中默认使用的最小长度。

+   `do_sample` (`bool`, *optional*, 默认为`False`) — 在模型的`generate`方法中默认使用的标志。是否使用采样；否则使用贪婪解码。

+   `early_stopping` (`bool`, *optional*, 默认为`False`) — 在模型的`generate`方法中默认使用的标志。是否在每批至少完成`num_beams`个句子时停止束搜索。

+   `num_beams` (`int`, *optional*, 默认为1) — 在模型的`generate`方法中默认使用的束搜索数量。1表示不使用束搜索。

+   `num_beam_groups` (`int`, *optional*, 默认为1) — 为了确保在模型的`generate`方法中默认使用的不同束组之间的多样性，将`num_beams`分成的组数。1表示不使用组束搜索。

+   `diversity_penalty` (`float`, *optional*, 默认为0.0) — 用于控制组束搜索多样性的值，将在模型的`generate`方法中默认使用。0表示没有多样性惩罚。惩罚越高，输出越多样化。

+   `temperature` (`float`, *optional*, 默认为1.0) — 用于调节下一个标记概率的值，将在模型的`generate`方法中默认使用。必须严格为正数。

+   `top_k` (`int`, *optional*, 默认为50) — 在模型的`generate`方法中默认使用的保留最高概率词汇标记的数量，用于top-k过滤。

+   `top_p` (`float`, *optional*, 默认为1) — 用于`top_p`的默认值，将在模型的`generate`方法中使用。如果设置为float < 1，只保留概率加起来达到`top_p`或更高的最有可能的标记用于生成。

+   `typical_p` (`float`, *optional*, 默认为1) — 本地典型性衡量了预测下一个目标标记的条件概率与预期的条件概率有多相似，给定已生成的部分文本。如果设置为float < 1，将保留概率加起来达到`typical_p`或更高的最具本地典型性的标记集。有关更多详细信息，请参阅[此论文](https://arxiv.org/pdf/2202.00666.pdf)。

+   `repetition_penalty` (`float`, *optional*, 默认为1) — 用于重复惩罚的参数，将在模型的`generate`方法中默认使用。1.0表示没有惩罚。

+   `length_penalty` (`float`, *optional*, 默认为1) — 用于基于束的生成的长度的指数惩罚。它作为指数应用于序列长度，然后用于分割序列的分数。由于分数是序列的对数似然（即负数），`length_penalty` > 0.0 促进更长的序列，而`length_penalty` < 0.0 鼓励更短的序列。

+   `no_repeat_ngram_size` (`int`, *optional*, 默认为0) — 用于`no_repeat_ngram_size`的默认值，将在模型的`generate`方法中使用。如果设置为int > 0，该大小的所有ngram只能出现一次。

+   `encoder_no_repeat_ngram_size` (`int`, *optional*, 默认为0) — 用于`encoder_no_repeat_ngram_size`的默认值，将在模型的`generate`方法中使用。如果设置为int > 0，`encoder_input_ids`中出现的该大小的所有ngram不能出现在`decoder_input_ids`中。

+   `bad_words_ids` (`List[int]`, *optional*) — 不允许生成的标记id列表，将在模型的`generate`方法中默认使用。为了获取不应出现在生成文本中的单词的标记，请使用`tokenizer.encode(bad_word, add_prefix_space=True)`。

+   `num_return_sequences` (`int`, *optional*, 默认为1) — 每个批次中每个元素默认在模型的`generate`方法中使用的独立计算返回序列的数量。

+   `output_scores` (`bool`, *optional*, 默认为`False`) — 当用于生成时，模型是否应返回logits。

+   `return_dict_in_generate` (`bool`, *optional*, 默认为`False`) — 模型在生成时是否应返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是`torch.LongTensor`。

+   `forced_bos_token_id` (`int`, *optional*) — 在`decoder_start_token_id`之后强制作为第一个生成的标记的id。对于多语言模型（如[mBART](../model_doc/mbart)）很有用，其中第一个生成的标记需要是目标语言标记。

+   `forced_eos_token_id` (`int`, *optional*) — 当达到`max_length`时，强制作为最后生成的标记的id。

+   `remove_invalid_values` (`bool`, *optional*) — 是否删除模型可能产生的*nan*和*inf*输出，以防止生成方法崩溃。请注意，使用`remove_invalid_values`可能会减慢生成速度。

用于微调任务的参数

+   `architectures` (`List[str]`, *optional*) — 可与模型预训练权重一起使用的模型架构。

+   `finetuning_task` (`str`, *optional*) — 用于微调模型的任务名称。在从原始（TensorFlow或PyTorch）检查点转换时可以使用。

+   `id2label` (`Dict[int, str]`, *optional*) — 从索引（例如预测索引或目标索引）到标签的映射。

+   `label2id` (`Dict[str, int]`, *optional*) — 从标签到模型索引的映射。

+   `num_labels` (`int`, *optional*) — 在模型中添加的最后一层中要使用的标签数，通常用于分类任务。

+   `task_specific_params` (`Dict[str, Any]`, *optional*) — 存储当前任务的额外关键字参数。

+   `problem_type` (`str`, *optional*) — `XxxForSequenceClassification`模型的问题类型。可以是`"regression"`、`"single_label_classification"`或`"multi_label_classification"`中的一个。

与分词器相关的参数

+   `tokenizer_class` (`str`, *optional*) — 要使用的关联分词器类的名称（如果未设置，则默认使用与模型关联的分词器）。

+   `prefix` (`str`, *optional*) — 每个文本前调用模型之前应添加的特定提示。

+   `bos_token_id` (`int`, *optional*) — *流的开始*标记的id。

+   `pad_token_id` (`int`, *optional*) — *填充*标记的id。

+   `eos_token_id` (`int`, *optional*) — *流的结束*标记的id。

+   `decoder_start_token_id` (`int`, *optional*) — 如果编码器-解码器模型开始解码时使用与*bos*不同的标记，则该标记的id。

+   `sep_token_id` (`int`, *optional*) — *分隔*标记的id。

PyTorch特定参数

+   `torchscript` (`bool`, *optional*, 默认为`False`) — 模型是否应与Torchscript一起使用。

+   `tie_word_embeddings` (`bool`, *optional*, 默认为`True`) — 模型的输入和输出词嵌入是否应该绑定。请注意，这仅在模型具有输出词嵌入层时才相关。

+   `torch_dtype` (`str`, *optional*) — 权重的`dtype`。此属性可用于将模型初始化为非默认的`dtype`（通常为`float32`），从而允许进行最佳存储分配。例如，如果保存的模型是`float16`，理想情况下我们希望使用最少的内存来加载`float16`权重。由于配置对象以纯文本形式存储，因此此属性仅包含浮点类型字符串，不包含`torch.`前缀。例如，对于`torch.float16`，`torch_dtype`是`"float16"`字符串。

    此属性当前在模型加载时未被使用，但在将来的版本中可能会更改。但我们可以通过使用 save_pretrained 保存 dtype 来为未来做好准备。

+   `attn_implementation` (`str`, *optional*) — 模型中要使用的注意力实现。可以是 `"eager"`（注意力的手动实现），`"sdpa"`（使用 [`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) 的注意力），或 `"flash_attention_2"`（使用 [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention) 的注意力）。默认情况下，如果可用，torch>=2.1.1 会使用 SDPA。否则，默认为手动的 `"eager"` 实现。

TensorFlow 特定参数

+   `use_bfloat16` (`bool`, *optional*, 默认为 `False`) — 模型是否应该使用 BFloat16 标量（仅由某些 TensorFlow 模型使用）。

+   `tf_legacy_loss` (`bool`, *optional*, 默认为 `False`) — 模型是否应该使用传统的 TensorFlow 损失。传统损失具有可变的输出形状，可能不兼容 XLA。此选项用于向后兼容，将在 Transformers v5 中删除。

所有配置类的基类。处理一些所有模型配置共有的参数，以及用于加载/下载/保存配置的方法。

可以加载和保存配置文件到磁盘。加载配置文件并使用此文件初始化模型 **不会** 加载模型权重。它只影响模型的配置。

类属性（由派生类覆盖）：

+   `model_type` (`str`) — 模型类型的标识符，序列化到 JSON 文件中，并用于在 [AutoConfig](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoConfig) 中重新创建正确的对象。

+   `is_composition` (`bool`) — 配置类是否由多个子配置组成。在这种情况下，配置必须从两个或更多类型为 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 的配置初始化，如：[EncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig) 或 [~RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)。

+   `keys_to_ignore_at_inference` (`List[str]`) — 推理期间查看模型字典输出时默认要忽略的键列表。

+   `attribute_map` (`Dict[str, str]`) — 将模型特定属性名称映射到属性的标准化命名的字典。

所有子类中存在的公共属性：

+   `vocab_size` (`int`) — 词汇表中的标记数，也是嵌入矩阵的第一个维度（对于没有文本模态的模型，此属性可能缺失）。

+   `hidden_size` (`int`) — 模型的隐藏大小。

+   `num_attention_heads` (`int`) — 模型中多头注意力层中使用的注意力头数。

+   `num_hidden_layers` (`int`) — 模型中的块数。

#### `push_to_hub`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)

```py
( repo_id: str use_temp_dir: Optional = None commit_message: Optional = None private: Optional = None token: Union = None max_shard_size: Union = '5GB' create_pr: bool = False safe_serialization: bool = True revision: str = None commit_description: str = None tags: Optional = None **deprecated_kwargs )
```

参数

+   `repo_id` (`str`) — 您要将配置推送到的存储库的名称。在推送到给定组织时，应包含您的组织名称。

+   `use_temp_dir` (`bool`, *optional*) — 是否使用临时目录存储在推送到 Hub 之前保存的文件。如果没有名为 `repo_id` 的目录，则默认为 `True`，否则为 `False`。

+   `commit_message` (`str`, *optional*) — 推送时要提交的消息。默认为 `"Upload config"`。

+   `private` (`bool`, *optional*) — 是否应该创建私有存储库。

+   `token` (`bool` 或 `str`, *可选*) — 用作远程文件的HTTP bearer授权的令牌。如果为`True`，将使用运行`huggingface-cli login`时生成的令牌（存储在`~/.huggingface`）。如果未指定`repo_url`，则默认为`True`。

+   `max_shard_size` (`int` 或 `str`, *可选*, 默认为 `"5GB"`) — 仅适用于模型。在分片之前的检查点的最大大小。然后，检查点将分片为每个大小低于此大小的部分。如果表示为字符串，需要是数字后跟一个单位（如`"5MB"`）。我们将其默认为`"5GB"`，以便用户可以在免费的Google Colab实例上轻松加载模型，而不会出现任何CPU OOM问题。

+   `create_pr` (`bool`, *可选*, 默认为 `False`) — 是否创建一个带有上传文件的PR或直接提交。

+   `safe_serialization` (`bool`, *可选*, 默认为 `True`) — 是否将模型权重转换为safetensors格式以进行更安全的序列化。

+   `revision` (`str`, *可选*) — 要将上传的文件推送到的分支。

+   `commit_description` (`str`, *可选*) — 将要创建的提交描述

+   `tags` (`List[str]`, *可选*) — 要推送到Hub上的标签列表。

将配置文件上传到🤗模型Hub。

示例：

```py
from transformers import AutoConfig

config = AutoConfig.from_pretrained("bert-base-cased")

# Push the config to your namespace with the name "my-finetuned-bert".
config.push_to_hub("my-finetuned-bert")

# Push the config to an organization with the name "my-finetuned-bert".
config.push_to_hub("huggingface/my-finetuned-bert")
```

#### `dict_torch_dtype_to_str`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/configuration_utils.py#L1005)

```py
( d: Dict )
```

检查传递的字典及其嵌套字典是否具有*torch_dtype*键，如果不是None，则将torch.dtype转换为仅类型的字符串。例如，`torch.float32`被转换为*“float32”*字符串，然后可以存储在json格式中。

#### `from_dict`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/configuration_utils.py#L737)

```py
( config_dict: Dict **kwargs ) → export const metadata = 'undefined';PretrainedConfig
```

参数

+   `config_dict` (`Dict[str, Any]`) — 将用于实例化配置对象的字典。可以通过利用[get_config_dict()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.get_config_dict)方法从预训练检查点中检索这样的字典。

+   `kwargs` (`Dict[str, Any]`) — 用于初始化配置对象的其他参数。

返回

[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)

从这些参数实例化的配置对象。

从参数字典实例化一个[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)。

#### `from_json_file`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/configuration_utils.py#L798)

```py
( json_file: Union ) → export const metadata = 'undefined';PretrainedConfig
```

参数

+   `json_file` (`str` 或 `os.PathLike`) — 包含参数的JSON文件的路径。

返回

[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)

从该JSON文件实例化的配置对象。

从参数文件的路径实例化一个[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)。

#### `from_pretrained`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/configuration_utils.py#L511)

```py
( pretrained_model_name_or_path: Union cache_dir: Union = None force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' **kwargs ) → export const metadata = 'undefined';PretrainedConfig
```

参数

+   `pretrained_model_name_or_path` (`str` 或 `os.PathLike`) — 这可以是：

    +   一个字符串，托管在huggingface.co模型存储库中的预训练模型配置的*模型ID*。有效的模型ID可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。

    +   一个*目录*的路径，其中包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained)方法保存的配置文件，例如，`./my_model_directory/`。

    +   一个保存的配置JSON *文件*的路径或URL，例如，`./my_model_directory/configuration.json`。

+   `cache_dir` (`str` 或 `os.PathLike`, *可选*) — 下载的预训练模型配置应缓存在其中的目录路径，如果不使用标准缓存。

+   `force_download` (`bool`, *可选*, 默认为 `False`) — 是否强制（重新）下载配置文件并覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`, *可选*, 默认为 `False`) — 是否删除接收不完整的文件。如果存在这样的文件，尝试恢复下载。

+   `proxies` (`Dict[str, str]`, *可选*) — 要使用的代理服务器字典，按协议或端点，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。代理在每个请求上使用。

+   `token` (`str` 或 `bool`, *可选*) — 用作远程文件的HTTP bearer授权的令牌。如果为 `True`，或未指定，将使用运行`huggingface-cli login`时生成的令牌（存储在`~/.huggingface`）。

+   `revision` (`str`, *可选*, 默认为 `"main"`) — 要使用的特定模型版本。它可以是分支名称、标签名称或提交ID，因为我们在huggingface.co上使用基于git的系统存储模型和其他工件，所以`revision`可以是git允许的任何标识符。

    要测试您在Hub上提交的拉取请求，可以传递`revision=“refs/pr/<pr_number>“。</pr_number>

+   `return_unused_kwargs` (`bool`, *可选*, 默认为 `False`) — 如果为 `False`，则此函数仅返回最终配置对象。

    如果为 `True`，则此函数返回一个`Tuple(config, unused_kwargs)`，其中*unused_kwargs*是一个字典，由那些不是配置属性的键/值对组成：即，未用于更新`config`的`kwargs`部分，否则将被忽略。

+   `subfolder` (`str`, *可选*, 默认为 `""`) — 如果相关文件位于huggingface.co模型存储库的子文件夹中，您可以在此处指定文件夹名称。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 任何键的kwargs值，这些键是配置属性，将用于覆盖加载的值。关于键/值对中键不是配置属性的行为由`return_unused_kwargs`关键字参数控制。

返回

[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)

从此预训练模型实例化的配置对象。

从预训练模型配置实例化一个[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)（或派生类）。

示例：

```py
# We can't instantiate directly the base class *PretrainedConfig* so let's show the examples on a
# derived class: BertConfig
config = BertConfig.from_pretrained(
    "bert-base-uncased"
)  # Download configuration from huggingface.co and cache.
config = BertConfig.from_pretrained(
    "./test/saved_model/"
)  # E.g. config (or model) was saved using *save_pretrained('./test/saved_model/')*
config = BertConfig.from_pretrained("./test/saved_model/my_configuration.json")
config = BertConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
assert config.output_attentions == True
config, unused_kwargs = BertConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
assert config.output_attentions == True
assert unused_kwargs == {"foo": False}
```

#### `get_config_dict`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/configuration_utils.py#L614)

```py
( pretrained_model_name_or_path: Union **kwargs ) → export const metadata = 'undefined';Tuple[Dict, Dict]
```

参数

+   `pretrained_model_name_or_path` (`str` 或 `os.PathLike`) — 我们想要参数字典的预训练检查点的标识符。

返回

`Tuple[Dict, Dict]`

将用于实例化配置对象的字典。

从`pretrained_model_name_or_path`解析为参数字典，用于使用`from_dict`实例化[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)。

#### `register_for_auto_class`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/configuration_utils.py#L1017)

```py
( auto_class = 'AutoConfig' )
```

参数

+   `auto_class` (`str` 或 `type`, *可选*, 默认为 `"AutoConfig"`) — 要将此新配置注册到的自动类。

使用给定的自动类注册此类。这仅应用于自定义配置，因为库中的配置已与`AutoConfig`映射。

此API是实验性的，可能在下一个版本中有一些轻微的破坏性更改。

#### `save_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/configuration_utils.py#L424)

```py
( save_directory: Union push_to_hub: bool = False **kwargs )
```

参数

+   `save_directory` (`str` 或 `os.PathLike`) — 将保存配置JSON文件的目录（如果不存在将被创建）。

+   `push_to_hub` (`bool`, *可选*, 默认为 `False`) — 是否在保存后将模型推送到Hugging Face模型中心。您可以使用 `repo_id` 指定要推送到的存储库（将默认为您的命名空间中的 `save_directory` 的名称）。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 传递给 [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub) 方法的额外关键字参数。

将配置对象保存到目录 `save_directory` 中，以便可以使用 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained) 类方法重新加载。

#### `to_dict`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/configuration_utils.py#L883)

```py
( ) → export const metadata = 'undefined';Dict[str, Any]
```

返回

`Dict[str, Any]`

这个配置实例的所有属性的字典。

将此实例序列化为Python字典。

#### `to_diff_dict`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/configuration_utils.py#L826)

```py
( ) → export const metadata = 'undefined';Dict[str, Any]
```

返回

`Dict[str, Any]`

这个配置实例的所有属性的字典，

从配置中删除所有与默认配置属性对应的属性，以提高可读性并序列化为Python字典。

#### `to_json_file`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/configuration_utils.py#L943)

```py
( json_file_path: Union use_diff: bool = True )
```

参数

+   `json_file_path` (`str` 或 `os.PathLike`) — 此配置实例的参数将被保存在其中的JSON文件的路径。

+   `use_diff` (`bool`, *可选*, 默认为 `True`) — 如果设置为 `True`，则只将配置实例与默认 `PretrainedConfig()` 之间的差异序列化为JSON文件。

将此实例保存到一个JSON文件。

#### `to_json_string`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/configuration_utils.py#L925)

```py
( use_diff: bool = True ) → export const metadata = 'undefined';str
```

参数

+   `use_diff` (`bool`, *可选*, 默认为 `True`) — 如果设置为 `True`，则只将配置实例与默认 `PretrainedConfig()` 之间的差异序列化为JSON字符串。

返回

`str`

包含此配置实例中所有属性的字符串，以JSON格式。

将此实例序列化为JSON字符串。

#### `update`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/configuration_utils.py#L957)

```py
( config_dict: Dict )
```

参数

+   `config_dict` (`Dict[str, Any]`) — 应该更新此类的属性的字典。

使用`config_dict`中的属性更新此类的属性。

#### `update_from_string`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/configuration_utils.py#L967)

```py
( update_str: str )
```

参数

+   `update_str` (`str`) — 应该更新此类的属性的字符串。

使用`update_str`中的属性更新此类的属性。

预期的格式是整数、浮点数和字符串，对于布尔值，请使用 `true` 或 `false`。例如: “n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index”

要更改的键必须已经存在于配置对象中。
