- en: LyCORIS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/peft/package_reference/adapter_utils](https://huggingface.co/docs/peft/package_reference/adapter_utils)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/peft/v0.8.2/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/start.c9bed6ec.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/scheduler.d627b047.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/singletons.95cf6adf.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.a57a1c33.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/paths.5d07c46f.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/app.72c78cae.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.d48c4817.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/0.aa346fde.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/17.d488c887.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Docstring.270658d8.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Heading.47e562a9.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/CodeBlock.5da89496.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/ExampleCodeBlock.a22db1c3.js">
  prefs: []
  type: TYPE_NORMAL
- en: '[LyCORIS](https://hf.co/papers/2309.14859) (Lora beYond Conventional methods,
    Other Rank adaptation Implementations for Stable diffusion) are LoRA-like matrix
    decomposition adapters that modify the cross-attention layer of the UNet. The
    [LoHa](loha) and [LoKr](lokr) methods inherit from the `Lycoris` classes here.'
  prefs: []
  type: TYPE_NORMAL
- en: LycorisConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.tuners.lycoris_utils.LycorisConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lycoris_utils.py#L34)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A base config for LyCORIS like adapters
  prefs: []
  type: TYPE_NORMAL
- en: LycorisLayer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.tuners.lycoris_utils.LycorisLayer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lycoris_utils.py#L60)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: A base layer for LyCORIS like adapters
  prefs: []
  type: TYPE_NORMAL
- en: '#### `merge`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lycoris_utils.py#L110)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`safe_merge` (`bool`, *optional*) — If `True`, the merge operation will be
    performed in a copy of the original weights and check for NaNs before merging
    the weights. This is useful if you want to check if the merge operation will produce
    NaNs. Defaults to `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_names` (`List[str]`, *optional*) — The list of adapter names that
    should be merged. If `None`, all active adapters will be merged. Defaults to `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merge the active adapter weights into the base weights
  prefs: []
  type: TYPE_NORMAL
- en: '#### `unmerge`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lycoris_utils.py#L166)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This method unmerges all merged adapter layers from the base weights.
  prefs: []
  type: TYPE_NORMAL
- en: LycorisTuner
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.tuners.lycoris_utils.LycorisTuner`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lycoris_utils.py#L193)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: A base tuner for LyCORIS like adapters
  prefs: []
  type: TYPE_NORMAL
- en: '#### `delete_adapter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lycoris_utils.py#L403)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`) — Name of the adapter to be deleted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deletes an existing adapter.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_adapter_layers`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lycoris_utils.py#L345)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Disable all adapters.
  prefs: []
  type: TYPE_NORMAL
- en: When disabling all adapters, the model output corresponds to the output of the
    base model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_adapter_layers`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lycoris_utils.py#L338)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Enable all adapters.
  prefs: []
  type: TYPE_NORMAL
- en: Call this if you have previously disabled all adapters and want to re-enable
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `merge_and_unload`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lycoris_utils.py#L352)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`progressbar` (`bool`) — whether to show a progressbar indicating the unload
    and merge process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safe_merge` (`bool`) — whether to activate the safe merging check to check
    if there is any potential Nan in the adapter weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_names` (`List[str]`, *optional*) — The list of adapter names that
    should be merged. If None, all active adapters will be merged. Defaults to `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method merges the adapter layers into the base model. This is needed if
    someone wants to use the base model as a standalone model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_adapter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lycoris_utils.py#L381)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`adapter_name` (`str` or `list[str]`) — Name of the adapter(s) to be activated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the active adapter(s).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, this function will set the specified adapters to trainable (i.e.,
    requires_grad=True). If this is not desired, use the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#### `unload`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lycoris_utils.py#L374)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Gets back the base model by removing all the lora modules without merging. This
    gives back the original base model.
  prefs: []
  type: TYPE_NORMAL
