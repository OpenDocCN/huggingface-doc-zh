- en: Pipelines for inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”¨äºæ¨æ–­çš„ç®¡é“
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/pipeline_tutorial](https://huggingface.co/docs/transformers/v4.37.2/en/pipeline_tutorial)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/pipeline_tutorial](https://huggingface.co/docs/transformers/v4.37.2/en/pipeline_tutorial)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'The [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    makes it simple to use any model from the [Hub](https://huggingface.co/models)
    for inference on any language, computer vision, speech, and multimodal tasks.
    Even if you donâ€™t have experience with a specific modality or arenâ€™t familiar
    with the underlying code behind the models, you can still use them for inference
    with the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)!
    This tutorial will teach you to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ä½¿å¾—åœ¨ä»»ä½•è¯­è¨€ã€è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šä½¿ç”¨Hubä¸­çš„ä»»ä½•æ¨¡å‹è¿›è¡Œæ¨æ–­å˜å¾—ç®€å•ã€‚å³ä½¿æ‚¨æ²¡æœ‰ä½¿ç”¨ç‰¹å®šæ¨¡æ€çš„ç»éªŒæˆ–ä¸ç†Ÿæ‚‰æ¨¡å‹èƒŒåçš„ä»£ç ï¼Œæ‚¨ä»ç„¶å¯ä»¥ä½¿ç”¨[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)è¿›è¡Œæ¨æ–­ï¼æœ¬æ•™ç¨‹å°†æ•™æ‚¨ï¼š'
- en: Use a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    for inference.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨äºæ¨æ–­çš„[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ã€‚
- en: Use a specific tokenizer or model.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç‰¹å®šçš„åˆ†è¯å™¨æˆ–æ¨¡å‹ã€‚
- en: Use a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    for audio, vision, and multimodal tasks.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºéŸ³é¢‘ã€è§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ä½¿ç”¨[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ã€‚
- en: Take a look at the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    documentation for a complete list of supported tasks and available parameters.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)æ–‡æ¡£ï¼Œäº†è§£æ”¯æŒçš„ä»»åŠ¡å’Œå¯ç”¨å‚æ•°çš„å®Œæ•´åˆ—è¡¨ã€‚
- en: Pipeline usage
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç®¡é“ç”¨æ³•
- en: While each task has an associated [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline),
    it is simpler to use the general [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    abstraction which contains all the task-specific pipelines. The [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    automatically loads a default model and a preprocessing class capable of inference
    for your task. Letâ€™s take the example of using the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    for automatic speech recognition (ASR), or speech-to-text.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ¯ä¸ªä»»åŠ¡éƒ½æœ‰ä¸€ä¸ªç›¸å…³çš„[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ï¼Œä½†ä½¿ç”¨åŒ…å«æ‰€æœ‰ç‰¹å®šä»»åŠ¡ç®¡é“çš„é€šç”¨[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)æŠ½è±¡æ›´ç®€å•ã€‚[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ä¼šè‡ªåŠ¨åŠ è½½é»˜è®¤æ¨¡å‹å’Œé€‚ç”¨äºæ‚¨ä»»åŠ¡çš„æ¨æ–­é¢„å¤„ç†ç±»ã€‚è®©æˆ‘ä»¬ä»¥ä½¿ç”¨[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æˆ–è¯­éŸ³è½¬æ–‡æœ¬ä¸ºä¾‹ã€‚
- en: 'Start by creating a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    and specify the inference task:'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é¦–å…ˆåˆ›å»ºä¸€ä¸ª[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ï¼Œå¹¶æŒ‡å®šæ¨æ–­ä»»åŠ¡ï¼š
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Pass your input to the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline).
    In the case of speech recognition, this is an audio input file:'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ã€‚åœ¨è¯­éŸ³è¯†åˆ«çš„æƒ…å†µä¸‹ï¼Œè¿™æ˜¯ä¸€ä¸ªéŸ³é¢‘è¾“å…¥æ–‡ä»¶ï¼š
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Not the result you had in mind? Check out some of the [most downloaded automatic
    speech recognition models](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=trending)
    on the Hub to see if you can get a better transcription.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ˜¯æ‚¨æƒ³è¦çš„ç»“æœï¼ŸæŸ¥çœ‹Hubä¸Šä¸€äº›[æœ€å—æ¬¢è¿çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=trending)ï¼Œçœ‹çœ‹æ˜¯å¦å¯ä»¥è·å¾—æ›´å¥½çš„è½¬å½•ã€‚
- en: Letâ€™s try the [Whisper large-v2](https://huggingface.co/openai/whisper-large)
    model from OpenAI. Whisper was released 2 years later than Wav2Vec2, and was trained
    on close to 10x more data. As such, it beats Wav2Vec2 on most downstream benchmarks.
    It also has the added benefit of predicting punctuation and casing, neither of
    which are possible with
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°è¯•æ¥è‡ªOpenAIçš„[Whisper large-v2](https://huggingface.co/openai/whisper-large)æ¨¡å‹ã€‚Whisperæ¯”Wav2Vec2æ™šå‘å¸ƒäº†2å¹´ï¼Œè®­ç»ƒæ•°æ®æ¥è¿‘10å€ã€‚å› æ­¤ï¼Œå®ƒåœ¨å¤§å¤šæ•°ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­å‡»è´¥äº†Wav2Vec2ã€‚å®ƒè¿˜å…·æœ‰é¢„æµ‹æ ‡ç‚¹å’Œå¤§å°å†™çš„é™„åŠ å¥½å¤„ï¼Œè€Œè¿™ä¸¤è€…åœ¨Wav2Vec2ä¸­éƒ½ä¸å¯èƒ½ã€‚
- en: Wav2Vec2.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Wav2Vec2ã€‚
- en: 'Letâ€™s give it a try here to see how it performs:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨è¿™é‡Œå°è¯•ä¸€ä¸‹ï¼Œçœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ï¼š
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now this result looks more accurate! For a deep-dive comparison on Wav2Vec2
    vs Whisper, refer to the [Audio Transformers Course](https://huggingface.co/learn/audio-course/chapter5/asr_models).
    We really encourage you to check out the Hub for models in different languages,
    models specialized in your field, and more. You can check out and compare model
    results directly from your browser on the Hub to see if it fits or handles corner
    cases better than other ones. And if you donâ€™t find a model for your use case,
    you can always start [training](training) your own!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿™ä¸ªç»“æœçœ‹èµ·æ¥æ›´å‡†ç¡®äº†ï¼è¦æ·±å…¥æ¯”è¾ƒWav2Vec2å’ŒWhisperï¼Œè¯·å‚è€ƒ[éŸ³é¢‘å˜æ¢å™¨è¯¾ç¨‹](https://huggingface.co/learn/audio-course/chapter5/asr_models)ã€‚æˆ‘ä»¬çœŸçš„é¼“åŠ±æ‚¨æŸ¥çœ‹Hubä¸­ä¸åŒè¯­è¨€çš„æ¨¡å‹ã€ä¸“é—¨é’ˆå¯¹æ‚¨é¢†åŸŸçš„æ¨¡å‹ç­‰ã€‚æ‚¨å¯ä»¥ç›´æ¥ä»Hubåœ¨æµè§ˆå™¨ä¸ŠæŸ¥çœ‹å’Œæ¯”è¾ƒæ¨¡å‹ç»“æœï¼Œçœ‹çœ‹å®ƒæ˜¯å¦æ¯”å…¶ä»–æ¨¡å‹æ›´é€‚åˆæˆ–æ›´å¥½åœ°å¤„ç†è¾¹ç¼˜æƒ…å†µã€‚å¦‚æœæ‚¨æ‰¾ä¸åˆ°é€‚ç”¨äºæ‚¨ç”¨ä¾‹çš„æ¨¡å‹ï¼Œæ‚¨å§‹ç»ˆå¯ä»¥å¼€å§‹[è®­ç»ƒ](training)æ‚¨è‡ªå·±çš„æ¨¡å‹ï¼
- en: 'If you have several inputs, you can pass your input as a list:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰å¤šä¸ªè¾“å…¥ï¼Œå¯ä»¥å°†è¾“å…¥ä½œä¸ºåˆ—è¡¨ä¼ é€’ï¼š
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Pipelines are great for experimentation as switching from one model to another
    is trivial; however, there are some ways to optimize them for larger workloads
    than experimentation. See the following guides that dive into iterating over whole
    datasets or using pipelines in a webserver: of the docs:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç®¡é“å¯¹äºå®éªŒå¾ˆæœ‰ç”¨ï¼Œå› ä¸ºä»ä¸€ä¸ªæ¨¡å‹åˆ‡æ¢åˆ°å¦ä¸€ä¸ªæ¨¡å‹å¾ˆç®€å•ï¼›ç„¶è€Œï¼Œæœ‰ä¸€äº›æ–¹æ³•å¯ä»¥ä¼˜åŒ–å®ƒä»¬ä»¥å¤„ç†æ¯”å®éªŒæ›´å¤§çš„å·¥ä½œé‡ã€‚æŸ¥çœ‹ä»¥ä¸‹æŒ‡å—ï¼Œæ·±å…¥æ¢è®¨å¦‚ä½•è¿­ä»£æ•´ä¸ªæ•°æ®é›†æˆ–åœ¨webæœåŠ¡å™¨ä¸­ä½¿ç”¨ç®¡é“ï¼šæ–‡æ¡£ä¸­çš„ï¼š
- en: '[Using pipelines on a dataset](#using-pipelines-on-a-dataset)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åœ¨æ•°æ®é›†ä¸Šä½¿ç”¨ç®¡é“](#using-pipelines-on-a-dataset)'
- en: '[Using pipelines for a webserver](./pipeline_webserver)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åœ¨webæœåŠ¡å™¨ä¸Šä½¿ç”¨ç®¡é“](./pipeline_webserver)'
- en: Parameters
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    supports many parameters; some are task specific, and some are general to all
    pipelines. In general, you can specify parameters anywhere you want:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)æ”¯æŒè®¸å¤šå‚æ•°ï¼›ä¸€äº›æ˜¯ä»»åŠ¡ç‰¹å®šçš„ï¼Œä¸€äº›æ˜¯æ‰€æœ‰ç®¡é“é€šç”¨çš„ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ‚¨å¯ä»¥åœ¨ä»»ä½•åœ°æ–¹æŒ‡å®šå‚æ•°ï¼š'
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Letâ€™s check out 3 important ones:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹3ä¸ªé‡è¦çš„å‚æ•°ï¼š
- en: Device
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¾å¤‡
- en: If you use `device=n`, the pipeline automatically puts the model on the specified
    device. This will work regardless of whether you are using PyTorch or Tensorflow.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä½¿ç”¨`device=n`ï¼Œç®¡é“ä¼šè‡ªåŠ¨å°†æ¨¡å‹æ”¾åœ¨æŒ‡å®šçš„è®¾å¤‡ä¸Šã€‚æ— è®ºæ‚¨ä½¿ç”¨PyTorchè¿˜æ˜¯Tensorflowï¼Œè¿™éƒ½å¯ä»¥å·¥ä½œã€‚
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If the model is too large for a single GPU and you are using PyTorch, you can
    set `device_map="auto"` to automatically determine how to load and store the model
    weights. Using the `device_map` argument requires the ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate)
    package:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹å¯¹å•ä¸ªGPUæ¥è¯´å¤ªå¤§ï¼Œå¹¶ä¸”æ‚¨ä½¿ç”¨çš„æ˜¯PyTorchï¼Œæ‚¨å¯ä»¥è®¾ç½®`device_map="auto"`æ¥è‡ªåŠ¨ç¡®å®šå¦‚ä½•åŠ è½½å’Œå­˜å‚¨æ¨¡å‹æƒé‡ã€‚ä½¿ç”¨`device_map`å‚æ•°éœ€è¦
    ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate) è½¯ä»¶åŒ…ï¼š
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following code automatically loads and stores model weights across devices:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç ä¼šè‡ªåŠ¨åœ¨è®¾å¤‡ä¹‹é—´åŠ è½½å’Œå­˜å‚¨æ¨¡å‹æƒé‡ï¼š
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that if `device_map="auto"` is passed, there is no need to add the argument
    `device=device` when instantiating your `pipeline` as you may encounter some unexpected
    behavior!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¦‚æœä¼ é€’äº†`device_map="auto"`ï¼Œåœ¨å®ä¾‹åŒ–æ‚¨çš„`pipeline`æ—¶æ— éœ€æ·»åŠ å‚æ•°`device=device`ï¼Œå¦åˆ™å¯èƒ½ä¼šé‡åˆ°ä¸€äº›æ„å¤–è¡Œä¸ºï¼
- en: Batch size
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‰¹å¤„ç†å¤§å°
- en: By default, pipelines will not batch inference for reasons explained in detail
    [here](https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching).
    The reason is that batching is not necessarily faster, and can actually be quite
    slower in some cases.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œç®¡é“ä¸ä¼šæ‰¹é‡æ¨ç†ï¼ŒåŸå› åœ¨[è¿™é‡Œ](https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching)æœ‰è¯¦ç»†è§£é‡Šã€‚åŸå› æ˜¯æ‰¹å¤„ç†ä¸ä¸€å®šæ›´å¿«ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹å®é™…ä¸Šå¯èƒ½ä¼šæ›´æ…¢ã€‚
- en: 'But if it works in your use case, you can use:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœåœ¨æ‚¨çš„ä½¿ç”¨æ¡ˆä¾‹ä¸­æœ‰æ•ˆï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ï¼š
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This runs the pipeline on the 4 provided audio files, but it will pass them
    in batches of 2 to the model (which is on a GPU, where batching is more likely
    to help) without requiring any further code from you. The output should always
    match what you would have received without batching. It is only meant as a way
    to help you get more speed out of a pipeline.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¼šåœ¨æä¾›çš„4ä¸ªéŸ³é¢‘æ–‡ä»¶ä¸Šè¿è¡Œç®¡é“ï¼Œä½†ä¼šå°†å®ƒä»¬åˆ†æ‰¹ä¼ é€’ç»™æ¨¡å‹ï¼ˆæ¨¡å‹åœ¨GPUä¸Šï¼Œæ‰¹å¤„ç†æ›´æœ‰å¯èƒ½æœ‰æ‰€å¸®åŠ©ï¼‰ï¼Œè€Œæ— éœ€æ‚¨è¿›ä¸€æ­¥ç¼–å†™ä»»ä½•ä»£ç ã€‚è¾“å‡ºåº”å§‹ç»ˆä¸æ²¡æœ‰æ‰¹å¤„ç†æ—¶æ”¶åˆ°çš„ç»“æœç›¸åŒ¹é…ã€‚è¿™åªæ˜¯ä¸€ç§å¸®åŠ©æ‚¨ä»ç®¡é“ä¸­è·å¾—æ›´å¿«é€Ÿåº¦çš„æ–¹æ³•ã€‚
- en: Pipelines can also alleviate some of the complexities of batching because, for
    some pipelines, a single item (like a long audio file) needs to be chunked into
    multiple parts to be processed by a model. The pipeline performs this [*chunk
    batching*](./main_classes/pipelines#pipeline-chunk-batching) for you.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç®¡é“è¿˜å¯ä»¥å‡è½»ä¸€äº›æ‰¹å¤„ç†çš„å¤æ‚æ€§ï¼Œå› ä¸ºå¯¹äºæŸäº›ç®¡é“ï¼Œå•ä¸ªé¡¹ç›®ï¼ˆå¦‚é•¿éŸ³é¢‘æ–‡ä»¶ï¼‰éœ€è¦è¢«åˆ†æˆå¤šä¸ªéƒ¨åˆ†æ‰èƒ½è¢«æ¨¡å‹å¤„ç†ã€‚ç®¡é“ä¼šä¸ºæ‚¨æ‰§è¡Œè¿™ç§[*å—æ‰¹å¤„ç†*](./main_classes/pipelines#pipeline-chunk-batching)ã€‚
- en: Task specific parameters
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä»»åŠ¡ç‰¹å®šå‚æ•°
- en: 'All tasks provide task specific parameters which allow for additional flexibility
    and options to help you get your job done. For instance, the [transformers.AutomaticSpeechRecognitionPipeline.**call**()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline.__call__)
    method has a `return_timestamps` parameter which sounds promising for subtitling
    videos:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ä»»åŠ¡éƒ½æä¾›ä»»åŠ¡ç‰¹å®šå‚æ•°ï¼Œè¿™äº›å‚æ•°å…è®¸é¢å¤–çš„çµæ´»æ€§å’Œé€‰é¡¹ï¼Œå¸®åŠ©æ‚¨å®Œæˆå·¥ä½œã€‚ä¾‹å¦‚ï¼Œ[transformers.AutomaticSpeechRecognitionPipeline.**call**()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline.__call__)æ–¹æ³•æœ‰ä¸€ä¸ª`return_timestamps`å‚æ•°ï¼Œå¯¹äºä¸ºè§†é¢‘æ·»åŠ å­—å¹•å¬èµ·æ¥å¾ˆæœ‰å¸Œæœ›ï¼š
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see, the model inferred the text and also outputted **when** the
    various sentences were pronounced.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œæ¨¡å‹æ¨æ–­äº†æ–‡æœ¬ï¼Œå¹¶ä¸”è¿˜è¾“å‡ºäº†å„ä¸ªå¥å­çš„å‘éŸ³æ—¶é—´ã€‚
- en: 'There are many parameters available for each task, so check out each taskâ€™s
    API reference to see what you can tinker with! For instance, the [AutomaticSpeechRecognitionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)
    has a `chunk_length_s` parameter which is helpful for working on really long audio
    files (for example, subtitling entire movies or hour-long videos) that a model
    typically cannot handle on its own:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªä»»åŠ¡éƒ½æœ‰è®¸å¤šå¯ç”¨çš„å‚æ•°ï¼Œå› æ­¤è¯·æŸ¥çœ‹æ¯ä¸ªä»»åŠ¡çš„APIå‚è€ƒï¼Œçœ‹çœ‹æ‚¨å¯ä»¥è°ƒæ•´å“ªäº›å‚æ•°ï¼ä¾‹å¦‚ï¼Œ[AutomaticSpeechRecognitionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)æœ‰ä¸€ä¸ª`chunk_length_s`å‚æ•°ï¼Œå¯¹äºå¤„ç†éå¸¸é•¿çš„éŸ³é¢‘æ–‡ä»¶ï¼ˆä¾‹å¦‚ï¼Œä¸ºæ•´éƒ¨ç”µå½±æˆ–é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘æ·»åŠ å­—å¹•ï¼‰éå¸¸æœ‰å¸®åŠ©ï¼Œè¿™æ˜¯æ¨¡å‹é€šå¸¸æ— æ³•ç‹¬ç«‹å¤„ç†çš„ï¼š
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you canâ€™t find a parameter that would really help you out, feel free to [request
    it](https://github.com/huggingface/transformers/issues/new?assignees=&labels=feature&template=feature-request.yml)!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‰¾ä¸åˆ°ä¸€ä¸ªçœŸæ­£æœ‰å¸®åŠ©çš„å‚æ•°ï¼Œè¯·éšæ—¶[è¯·æ±‚](https://github.com/huggingface/transformers/issues/new?assignees=&labels=feature&template=feature-request.yml)ï¼
- en: Using pipelines on a dataset
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨æ•°æ®é›†ä¸Šä½¿ç”¨ç®¡é“
- en: 'The pipeline can also run inference on a large dataset. The easiest way we
    recommend doing this is by using an iterator:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç®¡é“è¿˜å¯ä»¥åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¿è¡Œæ¨ç†ã€‚æˆ‘ä»¬å»ºè®®çš„æœ€ç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨è¿­ä»£å™¨ï¼š
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The iterator `data()` yields each result, and the pipeline automatically recognizes
    the input is iterable and will start fetching the data while it continues to process
    it on the GPU (this uses [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
    under the hood). This is important because you donâ€™t have to allocate memory for
    the whole dataset and you can feed the GPU as fast as possible.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿­ä»£å™¨`data()`ä¼šäº§ç”Ÿæ¯ä¸ªç»“æœï¼Œç®¡é“ä¼šè‡ªåŠ¨è¯†åˆ«è¾“å…¥æ˜¯å¯è¿­ä»£çš„ï¼Œå¹¶åœ¨ç»§ç»­åœ¨GPUä¸Šå¤„ç†æ•°æ®çš„åŒæ—¶å¼€å§‹è·å–æ•°æ®ï¼ˆè¿™åœ¨åº•å±‚ä½¿ç”¨[DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)ï¼‰ã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºæ‚¨ä¸å¿…ä¸ºæ•´ä¸ªæ•°æ®é›†åˆ†é…å†…å­˜ï¼Œå¯ä»¥å°½å¯èƒ½å¿«åœ°å°†æ•°æ®é¦ˆé€åˆ°GPUã€‚
- en: Since batching could speed things up, it may be useful to try tuning the `batch_size`
    parameter here.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ‰¹å¤„ç†å¯èƒ½åŠ å¿«é€Ÿåº¦ï¼Œå°è¯•è°ƒæ•´è¿™é‡Œçš„`batch_size`å‚æ•°å¯èƒ½ä¼šæœ‰ç”¨ã€‚
- en: 'The simplest way to iterate over a dataset is to just load one from ğŸ¤— [Datasets](https://github.com/huggingface/datasets/):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è¿­ä»£æ•°æ®é›†çš„æœ€ç®€å•æ–¹æ³•å°±æ˜¯ä» ğŸ¤— [Datasets](https://github.com/huggingface/datasets/) åŠ è½½ä¸€ä¸ªï¼š
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Using pipelines for a webserver
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨webæœåŠ¡å™¨ä¸Šä½¿ç”¨ç®¡é“
- en: Creating an inference engine is a complex topic which deserves it's own page.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºæ¨ç†å¼•æ“æ˜¯ä¸€ä¸ªå¤æ‚çš„ä¸»é¢˜ï¼Œå€¼å¾—æœ‰è‡ªå·±çš„é¡µé¢ã€‚
- en: '[Link](./pipeline_webserver)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[é“¾æ¥](./pipeline_webserver)'
- en: Vision pipeline
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§†è§‰ç®¡é“
- en: Using a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    for vision tasks is practically identical.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè§†è§‰ä»»åŠ¡ä½¿ç”¨[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)å‡ ä¹æ˜¯ç›¸åŒçš„ã€‚
- en: Specify your task and pass your image to the classifier. The image can be a
    link, a local path or a base64-encoded image. For example, what species of cat
    is shown below?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‡å®šæ‚¨çš„ä»»åŠ¡å¹¶å°†å›¾åƒä¼ é€’ç»™åˆ†ç±»å™¨ã€‚å›¾åƒå¯ä»¥æ˜¯é“¾æ¥ï¼Œæœ¬åœ°è·¯å¾„æˆ–base64ç¼–ç çš„å›¾åƒã€‚ä¾‹å¦‚ï¼Œä¸‹é¢æ˜¾ç¤ºäº†ä»€ä¹ˆå“ç§çš„çŒ«ï¼Ÿ
- en: '![pipeline-cat-chonk](../Images/ed522d0b7df733cc4426cbf9141d11c3.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![pipeline-cat-chonk](../Images/ed522d0b7df733cc4426cbf9141d11c3.png)'
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Text pipeline
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–‡æœ¬ç®¡é“
- en: Using a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    for NLP tasks is practically identical.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºNLPä»»åŠ¡ä½¿ç”¨[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)å‡ ä¹æ˜¯ç›¸åŒçš„ã€‚
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Multimodal pipeline
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤šæ¨¡æ€ç®¡é“
- en: The [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    supports more than one modality. For example, a visual question answering (VQA)
    task combines text and image. Feel free to use any image link you like and a question
    you want to ask about the image. The image can be a URL or a local path to the
    image.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)æ”¯æŒå¤šç§æ¨¡æ€ã€‚ä¾‹å¦‚ï¼Œè§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ç»“åˆäº†æ–‡æœ¬å’Œå›¾åƒã€‚éšæ„ä½¿ç”¨æ‚¨å–œæ¬¢çš„ä»»ä½•å›¾åƒé“¾æ¥å’Œæ‚¨æƒ³è¦è¯¢é—®æœ‰å…³å›¾åƒçš„é—®é¢˜ã€‚å›¾åƒå¯ä»¥æ˜¯URLæˆ–å›¾åƒçš„æœ¬åœ°è·¯å¾„ã€‚'
- en: 'For example, if you use this [invoice image](https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä½¿ç”¨è¿™ä¸ª[å‘ç¥¨å›¾åƒ](https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png)ï¼š
- en: '[PRE15]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To run the example above you need to have [`pytesseract`](https://pypi.org/project/pytesseract/)
    installed in addition to ğŸ¤— Transformers:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¿è¡Œä¸Šé¢çš„ç¤ºä¾‹ï¼Œæ‚¨éœ€è¦å®‰è£…[`pytesseract`](https://pypi.org/project/pytesseract/)ä»¥åŠğŸ¤— Transformersï¼š
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Using pipeline on large models with ğŸ¤— accelerate :'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨å¤§å‹æ¨¡å‹ä¸Šä½¿ç”¨ğŸ¤—åŠ é€Ÿå™¨ï¼š
- en: You can easily run `pipeline` on large models using ğŸ¤— `accelerate`! First make
    sure you have installed `accelerate` with `pip install accelerate`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥è½»æ¾åœ°åœ¨å¤§å‹æ¨¡å‹ä¸Šä½¿ç”¨ğŸ¤—`accelerate`è¿è¡Œ`pipeline`ï¼é¦–å…ˆç¡®ä¿æ‚¨å·²ç»å®‰è£…äº†`accelerate`å’Œ`pip install
    accelerate`ã€‚
- en: First load your model using `device_map="auto"`! We will use `facebook/opt-1.3b`
    for our example.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆä½¿ç”¨`device_map="auto"`åŠ è½½æ‚¨çš„æ¨¡å‹ï¼æˆ‘ä»¬å°†åœ¨ç¤ºä¾‹ä¸­ä½¿ç”¨`facebook/opt-1.3b`ã€‚
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You can also pass 8-bit loaded models if you install `bitsandbytes` and add
    the argument `load_in_8bit=True`
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå®‰è£…äº†`bitsandbytes`å¹¶æ·»åŠ å‚æ•°`load_in_8bit=True`ï¼Œè¿˜å¯ä»¥ä¼ é€’8ä½åŠ è½½çš„æ¨¡å‹
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that you can replace the checkpoint with any of the Hugging Face model
    that supports large model loading such as BLOOM!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæ‚¨å¯ä»¥ç”¨æ”¯æŒå¤§å‹æ¨¡å‹åŠ è½½çš„ä»»ä½•Hugging Faceæ¨¡å‹æ›¿æ¢æ£€æŸ¥ç‚¹ï¼Œä¾‹å¦‚BLOOMï¼
