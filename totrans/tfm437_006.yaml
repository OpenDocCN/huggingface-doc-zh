- en: Pipelines for inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/pipeline_tutorial](https://huggingface.co/docs/transformers/v4.37.2/en/pipeline_tutorial)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: 'The [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    makes it simple to use any model from the [Hub](https://huggingface.co/models)
    for inference on any language, computer vision, speech, and multimodal tasks.
    Even if you donâ€™t have experience with a specific modality or arenâ€™t familiar
    with the underlying code behind the models, you can still use them for inference
    with the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)!
    This tutorial will teach you to:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    for inference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a specific tokenizer or model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    for audio, vision, and multimodal tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take a look at the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    documentation for a complete list of supported tasks and available parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While each task has an associated [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline),
    it is simpler to use the general [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    abstraction which contains all the task-specific pipelines. The [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    automatically loads a default model and a preprocessing class capable of inference
    for your task. Letâ€™s take the example of using the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    for automatic speech recognition (ASR), or speech-to-text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by creating a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    and specify the inference task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass your input to the [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline).
    In the case of speech recognition, this is an audio input file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Not the result you had in mind? Check out some of the [most downloaded automatic
    speech recognition models](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=trending)
    on the Hub to see if you can get a better transcription.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s try the [Whisper large-v2](https://huggingface.co/openai/whisper-large)
    model from OpenAI. Whisper was released 2 years later than Wav2Vec2, and was trained
    on close to 10x more data. As such, it beats Wav2Vec2 on most downstream benchmarks.
    It also has the added benefit of predicting punctuation and casing, neither of
    which are possible with
  prefs: []
  type: TYPE_NORMAL
- en: Wav2Vec2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s give it a try here to see how it performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now this result looks more accurate! For a deep-dive comparison on Wav2Vec2
    vs Whisper, refer to the [Audio Transformers Course](https://huggingface.co/learn/audio-course/chapter5/asr_models).
    We really encourage you to check out the Hub for models in different languages,
    models specialized in your field, and more. You can check out and compare model
    results directly from your browser on the Hub to see if it fits or handles corner
    cases better than other ones. And if you donâ€™t find a model for your use case,
    you can always start [training](training) your own!
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have several inputs, you can pass your input as a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Pipelines are great for experimentation as switching from one model to another
    is trivial; however, there are some ways to optimize them for larger workloads
    than experimentation. See the following guides that dive into iterating over whole
    datasets or using pipelines in a webserver: of the docs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Using pipelines on a dataset](#using-pipelines-on-a-dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using pipelines for a webserver](./pipeline_webserver)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    supports many parameters; some are task specific, and some are general to all
    pipelines. In general, you can specify parameters anywhere you want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Letâ€™s check out 3 important ones:'
  prefs: []
  type: TYPE_NORMAL
- en: Device
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you use `device=n`, the pipeline automatically puts the model on the specified
    device. This will work regardless of whether you are using PyTorch or Tensorflow.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If the model is too large for a single GPU and you are using PyTorch, you can
    set `device_map="auto"` to automatically determine how to load and store the model
    weights. Using the `device_map` argument requires the ðŸ¤— [Accelerate](https://huggingface.co/docs/accelerate)
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code automatically loads and stores model weights across devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that if `device_map="auto"` is passed, there is no need to add the argument
    `device=device` when instantiating your `pipeline` as you may encounter some unexpected
    behavior!
  prefs: []
  type: TYPE_NORMAL
- en: Batch size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By default, pipelines will not batch inference for reasons explained in detail
    [here](https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching).
    The reason is that batching is not necessarily faster, and can actually be quite
    slower in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'But if it works in your use case, you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This runs the pipeline on the 4 provided audio files, but it will pass them
    in batches of 2 to the model (which is on a GPU, where batching is more likely
    to help) without requiring any further code from you. The output should always
    match what you would have received without batching. It is only meant as a way
    to help you get more speed out of a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines can also alleviate some of the complexities of batching because, for
    some pipelines, a single item (like a long audio file) needs to be chunked into
    multiple parts to be processed by a model. The pipeline performs this [*chunk
    batching*](./main_classes/pipelines#pipeline-chunk-batching) for you.
  prefs: []
  type: TYPE_NORMAL
- en: Task specific parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All tasks provide task specific parameters which allow for additional flexibility
    and options to help you get your job done. For instance, the [transformers.AutomaticSpeechRecognitionPipeline.**call**()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline.__call__)
    method has a `return_timestamps` parameter which sounds promising for subtitling
    videos:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the model inferred the text and also outputted **when** the
    various sentences were pronounced.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many parameters available for each task, so check out each taskâ€™s
    API reference to see what you can tinker with! For instance, the [AutomaticSpeechRecognitionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)
    has a `chunk_length_s` parameter which is helpful for working on really long audio
    files (for example, subtitling entire movies or hour-long videos) that a model
    typically cannot handle on its own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If you canâ€™t find a parameter that would really help you out, feel free to [request
    it](https://github.com/huggingface/transformers/issues/new?assignees=&labels=feature&template=feature-request.yml)!
  prefs: []
  type: TYPE_NORMAL
- en: Using pipelines on a dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The pipeline can also run inference on a large dataset. The easiest way we
    recommend doing this is by using an iterator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The iterator `data()` yields each result, and the pipeline automatically recognizes
    the input is iterable and will start fetching the data while it continues to process
    it on the GPU (this uses [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
    under the hood). This is important because you donâ€™t have to allocate memory for
    the whole dataset and you can feed the GPU as fast as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Since batching could speed things up, it may be useful to try tuning the `batch_size`
    parameter here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to iterate over a dataset is to just load one from ðŸ¤— [Datasets](https://github.com/huggingface/datasets/):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Using pipelines for a webserver
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating an inference engine is a complex topic which deserves it's own page.
  prefs: []
  type: TYPE_NORMAL
- en: '[Link](./pipeline_webserver)'
  prefs: []
  type: TYPE_NORMAL
- en: Vision pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    for vision tasks is practically identical.
  prefs: []
  type: TYPE_NORMAL
- en: Specify your task and pass your image to the classifier. The image can be a
    link, a local path or a base64-encoded image. For example, what species of cat
    is shown below?
  prefs: []
  type: TYPE_NORMAL
- en: '![pipeline-cat-chonk](../Images/ed522d0b7df733cc4426cbf9141d11c3.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Text pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    for NLP tasks is practically identical.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Multimodal pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    supports more than one modality. For example, a visual question answering (VQA)
    task combines text and image. Feel free to use any image link you like and a question
    you want to ask about the image. The image can be a URL or a local path to the
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you use this [invoice image](https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the example above you need to have [`pytesseract`](https://pypi.org/project/pytesseract/)
    installed in addition to ðŸ¤— Transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Using pipeline on large models with ðŸ¤— accelerate :'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can easily run `pipeline` on large models using ðŸ¤— `accelerate`! First make
    sure you have installed `accelerate` with `pip install accelerate`.
  prefs: []
  type: TYPE_NORMAL
- en: First load your model using `device_map="auto"`! We will use `facebook/opt-1.3b`
    for our example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You can also pass 8-bit loaded models if you install `bitsandbytes` and add
    the argument `load_in_8bit=True`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note that you can replace the checkpoint with any of the Hugging Face model
    that supports large model loading such as BLOOM!
  prefs: []
  type: TYPE_NORMAL
