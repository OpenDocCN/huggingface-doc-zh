- en: Wav2Vec2-BERT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Wav2Vec2-BERT model was proposed in [Seamless: Multilingual Expressive
    and Streaming Speech Translation](https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/)
    by the Seamless Communication team from Meta AI.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: This model was pre-trained on 4.5M hours of unlabeled audio data covering more
    than 143 languages. It requires finetuning to be used for downstream tasks such
    as Automatic Speech Recognition (ASR), or Audio Classification.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: The official results of the model can be found in Section 3.2.1 of the paper.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '*Recent advancements in automatic speech translation have dramatically expanded
    language coverage, improved multimodal capabilities, and enabled a wide range
    of tasks and functionalities. That said, large-scale automatic speech translation
    systems today lack key features that help machine-mediated communication feel
    seamless when compared to human-to-human dialogue. In this work, we introduce
    a family of models that enable end-to-end expressive and multilingual translations
    in a streaming fashion. First, we contribute an improved version of the massively
    multilingual and multimodal SeamlessM4T model—SeamlessM4T v2\. This newer model,
    incorporating an updated UnitY2 framework, was trained on more low-resource language
    data. The expanded version of SeamlessAlign adds 114,800 hours of automatically
    aligned data for a total of 76 languages. SeamlessM4T v2 provides the foundation
    on which our two newest models, SeamlessExpressive and SeamlessStreaming, are
    initiated. SeamlessExpressive enables translation that preserves vocal styles
    and prosody. Compared to previous efforts in expressive speech research, our work
    addresses certain underexplored aspects of prosody, such as speech rate and pauses,
    while also preserving the style of one’s voice. As for SeamlessStreaming, our
    model leverages the Efficient Monotonic Multihead Attention (EMMA) mechanism to
    generate low-latency target translations without waiting for complete source utterances.
    As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text
    translation for multiple source and target languages. To understand the performance
    of these models, we combined novel and modified versions of existing automatic
    metrics to evaluate prosody, latency, and robustness. For human evaluations, we
    adapted existing protocols tailored for measuring the most relevant attributes
    in the preservation of meaning, naturalness, and expressivity. To ensure that
    our models can be used safely and responsibly, we implemented the first known
    red-teaming effort for multimodal machine translation, a system for the detection
    and mitigation of added toxicity, a systematic evaluation of gender bias, and
    an inaudible localized watermarking mechanism designed to dampen the impact of
    deepfakes. Consequently, we bring major components from SeamlessExpressive and
    SeamlessStreaming together to form Seamless, the first publicly available system
    that unlocks expressive cross-lingual communication in real-time. In sum, Seamless
    gives us a pivotal look at the technical foundation needed to turn the Universal
    Speech Translator from a science fiction concept into a real-world technology.
    Finally, contributions in this work—including models, code, and a watermark detector—are
    publicly released and accessible at the link below.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The
    original code can be found [here](https://github.com/facebookresearch/seamless_communication).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Wav2Vec2-BERT follows the same architecture as Wav2Vec2-Conformer, but employs
    a causal depthwise convolutional layer and uses as input a mel-spectrogram representation
    of the audio instead of the raw waveform.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wav2Vec2-BERT遵循与Wav2Vec2-Conformer相同的架构，但采用因果深度卷积层，并使用音频的梅尔频谱表示作为输入，而不是原始波形。
- en: Wav2Vec2-BERT can use either no relative position embeddings, Shaw-like position
    embeddings, Transformer-XL-like position embeddings, or rotary position embeddings
    by setting the correct `config.position_embeddings_type`.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wav2Vec2-BERT可以通过设置正确的`config.position_embeddings_type`来使用无相对位置嵌入、类似Shaw的位置嵌入、类似Transformer-XL的位置嵌入或旋转位置嵌入。
- en: Wav2Vec2-BERT also introduces a Conformer-based adapter network instead of a
    simple convolutional network.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wav2Vec2-BERT还引入了基于Conformer的适配器网络，而不是简单的卷积网络。
- en: Resources
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: Automatic Speech Recognition
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自动语音识别
- en: '[Wav2Vec2BertForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForCTC)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wav2Vec2BertForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForCTC)可以通过这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition)来支持。'
- en: You can also adapt these notebooks on [how to finetune a speech recognition
    model in English](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb),
    and [how to finetune a speech recognition model in any language](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您还可以在[如何在英语中微调语音识别模型](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)和[如何在任何语言中微调语音识别模型](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)上调整这些笔记本。
- en: Audio Classification
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 音频分类
- en: '[Wav2Vec2BertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForSequenceClassification)
    can be used by adapting this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wav2Vec2BertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForSequenceClassification)可以通过调整这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification)来使用。'
- en: 'See also: [Audio classification task guide](../tasks/audio_classification)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另请参阅：[音频分类任务指南](../tasks/audio_classification)
- en: Wav2Vec2BertConfig
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2BertConfig
- en: '### `class transformers.Wav2Vec2BertConfig`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2BertConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/configuration_wav2vec2_bert.py#L31)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/configuration_wav2vec2_bert.py#L31)'
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*) — Vocabulary size of the Wav2Vec2Bert model.
    Defines the number of different tokens that can be represented by the `inputs_ids`
    passed when calling [Wav2Vec2BertModel](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertModel).
    Vocabulary size of the model. Defines the different tokens that can be represented
    by the *inputs_ids* passed to the forward method of [Wav2Vec2BertModel](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertModel).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`（`int`，*可选*）—Wav2Vec2Bert模型的词汇量。定义了在调用[Wav2Vec2BertModel](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertModel)时可以表示的不同令牌的数量。模型的词汇量。定义了可以由传递给[Wav2Vec2BertModel](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertModel)的*inputs_ids*表示的不同令牌。'
- en: '`hidden_size` (`int`, *optional*, defaults to 1024) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`（`int`，*可选*，默认为1024）—编码器层和池化器层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 24) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers`（`int`，*可选*，默认为24）—Transformer编码器中的隐藏层数。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 16) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads`（`int`，*可选*，默认为16）—Transformer编码器中每个注意力层的注意力头数。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 4096) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size`（`int`，*可选*，默认为4096）—Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`feature_projection_input_dim` (`int`, *optional*, defaults to 160) — Input
    dimension of this model, i.e the dimension after processing input audios with
    [SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)
    or [Wav2Vec2BertProcessor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertProcessor).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_projection_input_dim`（`int`，*可选*，默认为160）—此模型的输入维度，即使用[SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)或[Wav2Vec2BertProcessor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertProcessor)处理输入音频后的维度。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"swish"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"`, `"swish"` and `"gelu_new"` are supported.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act`（`str`或`function`，*可选*，默认为`"swish"`）—编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`、`"swish"`和`"gelu_new"`。'
- en: '`hidden_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout`（`float`，*可选*，默认为0.0）—嵌入层、编码器和池化器中所有全连接层的丢失概率。'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for activations inside the fully connected layer.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout`（`float`，*可选*，默认为0.0）—全连接层内激活的丢失比率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for the attention probabilities.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout`（`float`，*可选*，默认为0.0）—注意力概率的丢失比率。'
- en: '`feat_proj_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probabilitiy
    for the feature projection.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_proj_dropout`（`float`，*可选*，默认为0.0）—特征投影的丢失概率。'
- en: '`final_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the final projection layer of [Wav2Vec2BertForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForCTC).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layerdrop` (`float`, *optional*, defaults to 0.1) — The LayerDrop probability.
    See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`apply_spec_augment` (`bool`, *optional*, defaults to `True`) — Whether to
    apply *SpecAugment* data augmentation to the outputs of the feature encoder. For
    reference see [SpecAugment: A Simple Data Augmentation Method for Automatic Speech
    Recognition](https://arxiv.org/abs/1904.08779).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_time_prob` (`float`, *optional*, defaults to 0.05) — Percentage (between
    0 and 1) of all feature vectors along the time axis which will be masked. The
    masking procecure generates `mask_time_prob*len(time_axis)/mask_time_length ``independent
    masks over the axis. If reasoning from the propability of each feature vector
    to be chosen as the start of the vector span to be masked, *mask_time_prob* should
    be` prob_vector_start*mask_time_length`. Note that overlap may decrease the actual
    percentage of masked vectors. This is only relevant if` apply_spec_augment is
    True`.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_time_length` (`int`, *optional*, defaults to 10) — Length of vector span
    along the time axis.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_time_min_masks` (`int`, *optional*, defaults to 2) — The minimum number
    of masks of length `mask_feature_length` generated along the time axis, each time
    step, irrespectively of `mask_feature_prob`. Only relevant if `mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masks`.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_feature_prob` (`float`, *optional*, defaults to 0.0) — Percentage (between
    0 and 1) of all feature vectors along the feature axis which will be masked. The
    masking procecure generates `mask_feature_prob*len(feature_axis)/mask_time_length`
    independent masks over the axis. If reasoning from the propability of each feature
    vector to be chosen as the start of the vector span to be masked, *mask_feature_prob*
    should be `prob_vector_start*mask_feature_length`. Note that overlap may decrease
    the actual percentage of masked vectors. This is only relevant if `apply_spec_augment
    is True`.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_feature_length` (`int`, *optional*, defaults to 10) — Length of vector
    span along the feature axis.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_feature_min_masks` (`int`, *optional*, defaults to 0) — The minimum number
    of masks of length `mask_feature_length` generated along the feature axis, each
    time step, irrespectively of `mask_feature_prob`. Only relevant if `mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masks`.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ctc_loss_reduction` (`str`, *optional*, defaults to `"sum"`) — Specifies the
    reduction to apply to the output of `torch.nn.CTCLoss`. Only relevant when training
    an instance of [Wav2Vec2BertForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForCTC).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ctc_zero_infinity` (`bool`, *optional*, defaults to `False`) — Whether to
    zero infinite losses and the associated gradients of `torch.nn.CTCLoss`. Infinite
    losses mainly occur when the inputs are too short to be aligned to the targets.
    Only relevant when training an instance of [Wav2Vec2BertForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForCTC).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_weighted_layer_sum` (`bool`, *optional*, defaults to `False`) — Whether
    to use a weighted average of layer outputs with learned weights. Only relevant
    when using an instance of [Wav2Vec2BertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForSequenceClassification).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`classifier_proj_size` (`int`, *optional*, defaults to 768) — Dimensionality
    of the projection before token mean-pooling for classification.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_proj_size` (`int`, *optional*, defaults to 768) — 用于分类的令牌均值池化之前的投影的维度。'
- en: '`tdnn_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 1500)`) — A tuple of integers defining the number of output channels
    of each 1D convolutional layer in the *TDNN* module of the *XVector* model. The
    length of *tdnn_dim* defines the number of *TDNN* layers.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 1500)`) — 一个整数元组，定义了 *XVector* 模型中 *TDNN* 模块中每个 1D 卷积层的输出通道数。*tdnn_dim*
    的长度定义了 *TDNN* 层的数量。'
- en: '`tdnn_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 3,
    3, 1, 1)`) — A tuple of integers defining the kernel size of each 1D convolutional
    layer in the *TDNN* module of the *XVector* model. The length of *tdnn_kernel*
    has to match the length of *tdnn_dim*.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 3,
    3, 1, 1)`) — 一个整数元组，定义了 *XVector* 模型中 *TDNN* 模块中每个 1D 卷积层的内核大小。*tdnn_kernel* 的长度必须与
    *tdnn_dim* 的长度相匹配。'
- en: '`tdnn_dilation` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(1,
    2, 3, 1, 1)`) — A tuple of integers defining the dilation factor of each 1D convolutional
    layer in *TDNN* module of the *XVector* model. The length of *tdnn_dilation* has
    to match the length of *tdnn_dim*.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tdnn_dilation` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(1,
    2, 3, 1, 1)`) — 一个整数元组，定义了 *XVector* 模型中 *TDNN* 模块中每个 1D 卷积层的膨胀因子。*tdnn_dilation*
    的长度必须与 *tdnn_dim* 的长度相匹配。'
- en: '`xvector_output_dim` (`int`, *optional*, defaults to 512) — Dimensionality
    of the *XVector* embedding vectors.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xvector_output_dim` (`int`, *optional*, defaults to 512) — *XVector* 嵌入向量的维度。'
- en: '`pad_token_id` (`int`, *optional*, defaults to 0) — The id of the *beginning-of-stream*
    token.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *optional*, defaults to 0) — *开始流* 标记的 id。'
- en: '`bos_token_id` (`int`, *optional*, defaults to 1) — The id of the *padding*
    token.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token_id` (`int`, *optional*, defaults to 1) — *填充* 标记的 id。'
- en: '`eos_token_id` (`int`, *optional*, defaults to 2) — The id of the *end-of-stream*
    token.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`, *optional*, defaults to 2) — *结束流* 标记的 id。'
- en: '`add_adapter` (`bool`, *optional*, defaults to `False`) — Whether a convolutional
    attention network should be stacked on top of the Wav2Vec2Bert Encoder. Can be
    very useful for warm-starting Wav2Vec2Bert for SpeechEncoderDecoder models.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_adapter` (`bool`, *optional*, defaults to `False`) — 是否在 Wav2Vec2Bert
    编码器顶部堆叠卷积注意力网络。对于为 SpeechEncoderDecoder 模型热启动 Wav2Vec2Bert 非常有用。'
- en: '`adapter_kernel_size` (`int`, *optional*, defaults to 3) — Kernel size of the
    convolutional layers in the adapter network. Only relevant if `add_adapter is
    True`.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_kernel_size` (`int`, *optional*, defaults to 3) — 适配器网络中卷积层的内核大小。只有在
    `add_adapter` 为 True 时才相关。'
- en: '`adapter_stride` (`int`, *optional*, defaults to 2) — Stride of the convolutional
    layers in the adapter network. Only relevant if `add_adapter is True`.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_stride` (`int`, *optional*, defaults to 2) — 适配器网络中卷积层的步幅。只有在 `add_adapter`
    为 True 时才相关。'
- en: '`num_adapter_layers` (`int`, *optional*, defaults to 1) — Number of convolutional
    layers that should be used in the adapter network. Only relevant if `add_adapter
    is True`.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_adapter_layers` (`int`, *optional*, defaults to 1) — 应在适配器网络中使用的卷积层的数量。只有在
    `add_adapter` 为 True 时才相关。'
- en: '`adapter_act` (`str` or `function`, *optional*, defaults to `"relu"`) — The
    non-linear activation function (function or string) in the adapter layers. If
    string, `"gelu"`, `"relu"`, `"selu"`, `"swish"` and `"gelu_new"` are supported.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_act` (`str` or `function`, *optional*, defaults to `"relu"`) — 适配器层中的非线性激活函数（函数或字符串）。如果是字符串，支持
    `"gelu"`、`"relu"`、`"selu"`、`"swish"` 和 `"gelu_new"`。'
- en: '`use_intermediate_ffn_before_adapter` (`bool`, *optional*, defaults to `False`)
    — Whether an intermediate feed-forward block should be stacked on top of the Wav2Vec2Bert
    Encoder and before the adapter network. Only relevant if `add_adapter is True`.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_intermediate_ffn_before_adapter` (`bool`, *optional*, defaults to `False`)
    — 是否在 Wav2Vec2Bert 编码器顶部和适配器网络之前堆叠中间前馈块。只有在 `add_adapter` 为 True 时才相关。'
- en: '`output_hidden_size` (`int`, *optional*) — Dimensionality of the encoder output
    layer. If not defined, this defaults to *hidden-size*. Only relevant if `add_adapter
    is True`.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_size` (`int`, *optional*) — 编码器输出层的维度。如果未定义，则默认为 *hidden-size*。只有在
    `add_adapter` 为 True 时才相关。'
- en: '`position_embeddings_type` (`str`, *optional*, defaults to `"relative_key"`)
    — Can be specified to :'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_embeddings_type` (`str`, *optional*, defaults to `"relative_key"`)
    — 可以指定为：'
- en: '`rotary`, for rotary position embeddings.'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rotary`，用于旋转位置嵌入。'
- en: '`relative`, for relative position embeddings.'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`relative`，用于相对位置嵌入。'
- en: '`relative_key`, for relative position embeddings as defined by Shaw in [Self-Attention
    with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155).
    If left to `None`, no relative position embeddings is applied.'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`relative_key`，用于由 Shaw 在 [Self-Attention with Relative Position Representations
    (Shaw et al.)](https://arxiv.org/abs/1803.02155) 中定义的相对位置嵌入。如果保留为 `None`，则不应用相对位置嵌入。'
- en: '`rotary_embedding_base` (`int`, *optional*, defaults to 10000) — If `"rotary"`
    position embeddings are used, defines the size of the embedding base.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rotary_embedding_base` (`int`, *optional*, defaults to 10000) — 如果使用了 "rotary"
    位置嵌入，定义了嵌入基数的大小。'
- en: '`max_source_positions` (`int`, *optional*, defaults to 5000) — if `"relative"`
    position embeddings are used, defines the maximum source input positions.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_source_positions` (`int`, *optional*, defaults to 5000) — 如果使用了 "relative"
    位置嵌入，定义了最大源输入位置。'
- en: '`left_max_position_embeddings` (`int`, *optional*, defaults to 64) — If `"relative_key"`
    (aka Shaw) position embeddings are used, defines the left clipping value for relative
    positions.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`left_max_position_embeddings` (`int`, *optional*, defaults to 64) — 如果使用了
    "relative_key"（又名 Shaw）位置嵌入，定义了相对位置的左剪切值。'
- en: '`right_max_position_embeddings` (`int`, *optional*, defaults to 8) — If `"relative_key"`
    (aka Shaw) position embeddings are used, defines the right clipping value for
    relative positions.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`right_max_position_embeddings` (`int`, *optional*, defaults to 8) — 如果使用了
    "relative_key"（又名 Shaw）位置嵌入，定义了相对位置的右剪切值。'
- en: '`conv_depthwise_kernel_size` (`int`, *optional*, defaults to 31) — Kernel size
    of convolutional depthwise 1D layer in Conformer blocks.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_depthwise_kernel_size` (`int`, *optional*, defaults to 31) — Conformer
    块中深度可分离 1D 卷积层的内核大小。'
- en: '`conformer_conv_dropout` (`float`, *optional*, defaults to 0.1) — The dropout
    probability for all convolutional layers in Conformer blocks.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conformer_conv_dropout` (`float`, *可选*, 默认为0.1) — Conformer块中所有卷积层的丢弃概率。'
- en: This is the configuration class to store the configuration of a [Wav2Vec2BertModel](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertModel).
    It is used to instantiate an Wav2Vec2Bert model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Wav2Vec2Bert [facebook/wav2vec2-bert-rel-pos-large](https://huggingface.co/facebook/wav2vec2-bert-rel-pos-large)
    architecture.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '这是用于存储[Wav2Vec2BertModel](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertModel)配置的配置类。它用于根据指定的参数实例化Wav2Vec2Bert模型，定义模型架构。使用默认值实例化配置将产生类似于Wav2Vec2Bert
    [facebook/wav2vec2-bert-rel-pos-large](https://huggingface.co/facebook/wav2vec2-bert-rel-pos-large)架构的配置。 '
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Wav2Vec2BertProcessor
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2BertProcessor
- en: '### `class transformers.Wav2Vec2BertProcessor`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2BertProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py#L25)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py#L25)'
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`feature_extractor` (`SeamlessM4TFeatureExtractor`) — An instance of [SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor).
    The feature extractor is a required input.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor` (`SeamlessM4TFeatureExtractor`) — [SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)的实例。特征提取器是必需的输入。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — An instance of [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    The tokenizer is a required input.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)的实例。Tokenizer是必需的输入。'
- en: Constructs a Wav2Vec2-BERT processor which wraps a Wav2Vec2-BERT feature extractor
    and a Wav2Vec2 CTC tokenizer into a single processor.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个Wav2Vec2-BERT处理器，将Wav2Vec2-BERT特征提取器和Wav2Vec2 CTC tokenizer封装到单个处理器中。
- en: '[Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)
    offers all the functionalities of [SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)
    and [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See the docstring of [**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    and [decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.decode)
    for more information.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)提供了[SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)和[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)的所有功能。查看[**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)和[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.decode)的文档字符串以获取更多信息。'
- en: '#### `__call__`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py#L65)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py#L65)'
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`, `List[str]`, `List[List[str]]`) — The sequence or batch of sequences
    to be encoded. Each sequence can be a string or a list of strings (pretokenized
    string). If the sequences are provided as list of strings (pretokenized), you
    must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`) — 要编码的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果将序列提供为字符串列表（预分词），则必须设置`is_split_into_words=True`（以消除与序列批次的歧义）。'
- en: '`audio` (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`)
    — The audio or batch of audios to be prepared. Each audio can be NumPy array or
    PyTorch tensor. In case of a NumPy array/PyTorch tensor, each audio should be
    of shape (C, T), where C is a number of channels, and T the sample length of the
    audio.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`audio` (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`)
    — 要准备的音频或音频批次。每个音频可以是NumPy数组或PyTorch张量。在NumPy数组/PyTorch张量的情况下，每个音频的形状应为（C，T），其中C是通道数，T是音频的采样长度。'
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments that will
    be passed to the feature extractor and/or the tokenizer.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*可选*) — 将传递给特征提取器和/或tokenizer的剩余关键字参数字典。'
- en: Returns
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 具有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：
- en: '`input_features` — Audio input features to be fed to a model. Returned when
    `audio` is not `None`.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features` — 要馈送给模型的音频输入特征。当`audio`不为`None`时返回。'
- en: '`attention_mask` — List of indices specifying which timestamps should be attended
    to by the model when `audio` is not `None`. When only `text` is specified, returns
    the token attention mask.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` — 指定当`audio`不为`None`时模型应关注哪些时间戳的索引列表。当仅指定`text`时，返回标记注意力掩码。'
- en: '`labels` — List of token ids to be fed to a model. Returned when both `text`
    and `audio` are not `None`.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` — 要提供给模型的标记id列表。当`text`和`audio`都不是`None`时返回。'
- en: '`input_ids` — List of token ids to be fed to a model. Returned when `text`
    is not `None` and `audio` is `None`.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` — 要提供给模型的标记id列表。当`text`不是`None`且`audio`是`None`时返回。'
- en: Main method to prepare for the model one or several sequences(s) and audio(s).
    This method forwards the `audio` and `kwargs` arguments to SeamlessM4TFeatureExtractor’s
    [**call**()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)
    if `audio` is not `None` to pre-process the audio. To prepare the target sequences(s),
    this method forwards the `text` and `kwargs` arguments to PreTrainedTokenizer’s
    [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    if `text` is not `None`. Please refer to the doctsring of the above two methods
    for more information.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 准备模型一个或多个序列和音频的主要方法。如果`audio`不是`None`，则此方法将`audio`和`kwargs`参数转发给SeamlessM4TFeatureExtractor的[**call**()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)以预处理音频。为了准备目标序列，如果`text`不是`None`，则此方法将`text`和`kwargs`参数转发给PreTrainedTokenizer的[**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。更多信息请参考上述两个方法的文档字符串。
- en: '#### `pad`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`pad`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py#L111)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py#L111)'
- en: '[PRE4]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If `input_features` is not `None`, this method forwards the `input_features`
    and `kwargs` arguments to SeamlessM4TFeatureExtractor’s [pad()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad)
    to pad the input features. If `labels` is not `None`, this method forwards the
    `labels` and `kwargs` arguments to PreTrainedTokenizer’s [pad()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad)
    to pad the label(s). Please refer to the doctsring of the above two methods for
    more information.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`input_features`不是`None`，则此方法将`input_features`和`kwargs`参数转发给SeamlessM4TFeatureExtractor的[pad()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad)以填充输入特征。如果`labels`不是`None`，则此方法将`labels`和`kwargs`参数转发给PreTrainedTokenizer的[pad()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad)以填充标签。更多信息请参考上述两个方法的文档字符串。
- en: '#### `from_pretrained`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py#L46)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py#L46)'
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#### `save_pretrained`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L167)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L167)'
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`save_directory` (`str` or `os.PathLike`) — Directory where the feature extractor
    JSON file and the tokenizer files will be saved (directory will be created if
    it does not exist).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory` (`str`或`os.PathLike`) — 特征提取器JSON文件和标记器文件将保存在的目录（如果目录不存在，则将创建该目录）。'
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) — Whether or not to
    push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`push_to_hub` (`bool`, *optional*, 默认为`False`) — 是否在保存模型后将其推送到Hugging Face模型中心。您可以使用`repo_id`指定要推送到的存储库（将默认为您的命名空间中的`save_directory`的名称）。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional key word arguments passed
    along to the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *optional*) — 传递给[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)方法的额外关键字参数。'
- en: Saves the attributes of this processor (feature extractor, tokenizer…) in the
    specified directory so that it can be reloaded using the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained)
    method.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 将此处理器的属性（特征提取器、标记器等）保存在指定目录中，以便可以使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained)方法重新加载。
- en: This class method is simply calling [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)
    and [save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained).
    Please refer to the docstrings of the methods above for more information.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此类方法简单地调用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)和[save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained)。更多信息请参考上述方法的文档字符串。
- en: '#### `batch_decode`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py#L133)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py#L133)'
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This method forwards all its arguments to PreTrainedTokenizer’s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将其所有参数转发给PreTrainedTokenizer的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。更多信息请参考此方法的文档字符串。
- en: '#### `decode`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py#L140)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py#L140)'
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This method forwards all its arguments to PreTrainedTokenizer’s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将其所有参数转发给PreTrainedTokenizer的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。有关更多信息，请参阅此方法的文档字符串。
- en: Wav2Vec2BertModel
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2BertModel
- en: '### `class transformers.Wav2Vec2BertModel`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '### `类transformers.Wav2Vec2BertModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py#L1045)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py#L1045)'
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Wav2Vec2BertConfig](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Wav2Vec2BertConfig](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: 'The bare Wav2Vec2Bert Model transformer outputting raw hidden-states without
    any specific head on top. Wav2Vec2Bert was proposed in [wav2vec 2.0: A Framework
    for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 裸Wav2Vec2Bert模型变压器输出原始隐藏状态，没有特定的顶部头。Wav2Vec2Bert是由Alexei Baevski、Henry Zhou、Abdelrahman
    Mohamed、Michael Auli提出的[wav2vec 2.0:自监督学习语音表示的框架](https://arxiv.org/abs/2006.11477)。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以了解库为所有模型实现的通用方法（例如下载或保存等）。
- en: This model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `前向`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py#L1117)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py#L1117)'
- en: '[PRE10]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_features`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2BertProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertProcessor.__call__)
    for details.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) —
    输入原始语音波形的浮点值。值可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得，例如通过soundfile库（`pip
    install soundfile`）。要准备好数组为`input_features`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2BertProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertProcessor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*)
    — 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`未屏蔽`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`屏蔽`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2BertConfig](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig))
    and inputs.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    或者一个 `torch.FloatTensor` 元组（如果传入 `return_dict=False` 或者 `config.return_dict=False`）包含不同的元素，取决于配置（[Wav2Vec2BertConfig](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig)）和输入。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`extract_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    conv_dim[-1])`) — Sequence of extracted feature vectors of the last convolutional
    layer of the model.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    conv_dim[-1])`) — 模型最后一个卷积层提取的特征向量序列。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传入 `output_hidden_states=True`
    或者 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出 + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传入 `output_attentions=True`
    或者 `config.output_attentions=True` 时返回） — 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组（每一层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力 softmax 之后，用于计算自注意力头中的加权平均值。
- en: The [Wav2Vec2BertModel](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertModel)
    forward method, overrides the `__call__` special method.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2BertModel](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertModel)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在这个函数内定义，但应该在之后调用 `Module` 实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Wav2Vec2BertForCTC
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wav2Vec2BertForCTC
- en: '### `class transformers.Wav2Vec2BertForCTC`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Wav2Vec2BertForCTC`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py#L1173)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py#L1173)'
- en: '[PRE12]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Wav2Vec2BertConfig](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([Wav2Vec2BertConfig](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: 'Wav2Vec2Bert Model with a `language modeling` head on top for Connectionist
    Temporal Classification (CTC). Wav2Vec2Bert was proposed in [wav2vec 2.0: A Framework
    for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '带有顶部 `语言建模` 头部的 Wav2Vec2Bert 模型，用于 Connectionist Temporal Classification (CTC)。Wav2Vec2Bert
    在 [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
    中由 Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli 提出。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存等）。
- en: This model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是 PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module)
    的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以了解所有与一般用法和行为相关的事项。
- en: '#### `forward`'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py#L1202)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py#L1202)'
- en: '[PRE13]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_features`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2BertProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertProcessor.__call__)
    for details.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*)
    — Labels for connectionist temporal classification. Note that `target_length`
    has to be smaller or equal to the sequence length of the output logits. Indices
    are selected in `[-100, 0, ..., config.vocab_size - 1]`. All labels set to `-100`
    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size
    - 1]`.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2BertConfig](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig))
    and inputs.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [Wav2Vec2BertForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForCTC)
    forward method, overrides the `__call__` special method.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Wav2Vec2BertForSequenceClassification
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.Wav2Vec2BertForSequenceClassification`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py#L1284)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([Wav2Vec2BertConfig](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wav2Vec2Bert Model with a sequence classification head on top (a linear layer
    over the pooled output) for tasks like SUPERB Keyword Spotting.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Wav2Vec2Bert was proposed in [wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
    Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py#L1318)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_features`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2BertProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertProcessor.__call__)
    for details.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2BertConfig](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig))
    and inputs.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [Wav2Vec2BertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Wav2Vec2BertForAudioFrameClassification
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.Wav2Vec2BertForAudioFrameClassification`'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py#L1388)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([Wav2Vec2BertConfig](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wav2Vec2Bert Model with a frame classification head on top for tasks like Speaker
    Diarization.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'Wav2Vec2Bert was proposed in [wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
    Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py#L1421)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_features`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2BertProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertProcessor.__call__)
    for details.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2BertConfig](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig))
    and inputs.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [Wav2Vec2BertForAudioFrameClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForAudioFrameClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Wav2Vec2BertForXVector
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.Wav2Vec2BertForXVector`'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py#L1534)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([Wav2Vec2BertConfig](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wav2Vec2Bert Model with an XVector feature extraction head on top for tasks
    like Speaker Verification.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'Wav2Vec2Bert was proposed in [wav2vec 2.0: A Framework for Self-Supervised
    Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
    Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py#L1586)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_features`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2BertProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertProcessor.__call__)
    for details.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.XVectorOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.XVectorOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Wav2Vec2BertConfig](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig))
    and inputs.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.xvector_output_dim)`)
    — Classification hidden states before AMSoftmax.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embeddings` (`torch.FloatTensor` of shape `(batch_size, config.xvector_output_dim)`)
    — Utterance embeddings used for vector similarity-based retrieval.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [Wav2Vec2BertForXVector](/docs/transformers/v4.37.2/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForXVector)
    forward method, overrides the `__call__` special method.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
