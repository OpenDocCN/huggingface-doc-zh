["```py\n>>> from transformers import CanineModel\n>>> import torch\n\n>>> model = CanineModel.from_pretrained(\"google/canine-c\")  # model pre-trained with autoregressive character loss\n\n>>> text = \"hello world\"\n>>> # use Python's built-in ord() function to turn each character into its unicode code point id\n>>> input_ids = torch.tensor([[ord(char) for char in text]])\n\n>>> outputs = model(input_ids)  # forward pass\n>>> pooled_output = outputs.pooler_output\n>>> sequence_output = outputs.last_hidden_state\n```", "```py\n>>> from transformers import CanineTokenizer, CanineModel\n\n>>> model = CanineModel.from_pretrained(\"google/canine-c\")\n>>> tokenizer = CanineTokenizer.from_pretrained(\"google/canine-c\")\n\n>>> inputs = [\"Life is like a box of chocolates.\", \"You never know what you gonna get.\"]\n>>> encoding = tokenizer(inputs, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n\n>>> outputs = model(**encoding)  # forward pass\n>>> pooled_output = outputs.pooler_output\n>>> sequence_output = outputs.last_hidden_state\n```", "```py\n>>> from transformers import CanineConfig, CanineModel\n\n>>> # Initializing a CANINE google/canine-s style configuration\n>>> configuration = CanineConfig()\n\n>>> # Initializing a model (with random weights) from the google/canine-s style configuration\n>>> model = CanineModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence |\n```", "```py\n>>> from transformers import AutoTokenizer, CanineModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/canine-s\")\n>>> model = CanineModel.from_pretrained(\"google/canine-s\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, CanineForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/canine-s\")\n>>> model = CanineForSequenceClassification.from_pretrained(\"google/canine-s\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = CanineForSequenceClassification.from_pretrained(\"google/canine-s\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, CanineForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/canine-s\")\n>>> model = CanineForSequenceClassification.from_pretrained(\"google/canine-s\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = CanineForSequenceClassification.from_pretrained(\n...     \"google/canine-s\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> from transformers import AutoTokenizer, CanineForMultipleChoice\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/canine-s\")\n>>> model = CanineForMultipleChoice.from_pretrained(\"google/canine-s\")\n\n>>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n>>> choice0 = \"It is eaten with a fork and a knife.\"\n>>> choice1 = \"It is eaten while held in the hand.\"\n>>> labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n\n>>> encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"pt\", padding=True)\n>>> outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1\n\n>>> # the linear classifier still needs to be trained\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, CanineForTokenClassification\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/canine-s\")\n>>> model = CanineForTokenClassification.from_pretrained(\"google/canine-s\")\n\n>>> inputs = tokenizer(\n...     \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n... )\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_token_class_ids = logits.argmax(-1)\n\n>>> # Note that tokens are classified rather then input words which means that\n>>> # there might be more predicted token classes than words.\n>>> # Multiple token classes might account for the same word\n>>> predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n>>> predicted_tokens_classes\n```", "```py\n>>> labels = predicted_token_class_ids\n>>> loss = model(**inputs, labels=labels).loss\n>>> round(loss.item(), 2)\n```", "```py\n>>> from transformers import AutoTokenizer, CanineForQuestionAnswering\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Splend1dchan/canine-c-squad\")\n>>> model = CanineForQuestionAnswering.from_pretrained(\"Splend1dchan/canine-c-squad\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n>>> tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n'nice puppet'\n\n>>> # target is \"nice puppet\"\n>>> target_start_index = torch.tensor([14])\n>>> target_end_index = torch.tensor([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = outputs.loss\n>>> round(loss.item(), 2)\n8.81\n```"]