- en: Monte Carlo vs Temporal Difference Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/mc-vs-td](https://huggingface.co/learn/deep-rl-course/unit2/mc-vs-td)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The last thing we need to discuss before diving into Q-Learning is the two learning
    strategies.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Remember that an RL agent **learns by interacting with its environment.** The
    idea is that **given the experience and the received reward, the agent will update
    its value function or policy.**
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo and Temporal Difference Learning are two different **strategies
    on how to train our value function or our policy function.** Both of them **use
    experience to solve the RL problem.**
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: On one hand, Monte Carlo uses **an entire episode of experience before learning.** On
    the other hand, Temporal Difference uses **only a step (<math><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub><mo
    separator="true">,</mo><msub><mi>A</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    separator="true">,</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">S_t, A_t, R_{t+1}, S_{t+1}</annotation></semantics></math>St​,At​,Rt+1​,St+1​
    ) to learn.**
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: We’ll explain both of them **using a value-based method example.**
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'Monte Carlo: learning at the end of the episode'
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monte Carlo waits until the end of the episode, calculates <math><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">G_t</annotation></semantics></math>Gt​ (return) and
    uses it as **a target for updating <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_t)</annotation></semantics></math>V(St​).**
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: So it requires a **complete episode of interaction before updating our value
    function.**
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![Monte Carlo](../Images/6be3469c44f4969a313e63ae3d083f7f.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: 'If we take an example:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![Monte Carlo](../Images/208dbe55d499c365b9ed37acb6950d52.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: We always start the episode **at the same starting point.**
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The agent takes actions using the policy**. For instance, using an Epsilon
    Greedy Strategy, a policy that alternates between exploration (random actions)
    and exploitation.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We get **the reward and the next state.**
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We terminate the episode if the cat eats the mouse or if the mouse moves > 10
    steps.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the episode, **we have a list of State, Actions, Rewards, and
    Next States tuples** For instance [[State tile 3 bottom, Go Left, +1, State tile
    2 bottom], [State tile 2 bottom, Go Left, +0, State tile 1 bottom]…]
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The agent will sum the total rewards<math><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">G_t</annotation></semantics></math>Gt​** (to see
    how well it did).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It will then **update<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(s_t)</annotation></semantics></math>V(st​)
    based on the formula**
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Monte Carlo](../Images/c9946b98c2ce4cd1229121de2153817a.png)'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Then **start a new game with this new knowledge**
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By running more and more episodes, **the agent will learn to play better and
    better.**
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![Monte Carlo](../Images/ab81d39bcb1c43b9258e84c954f9d623.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: 'For instance, if we train a state-value function using Monte Carlo:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: We initialize our value function **so that it returns 0 value for each state**
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our learning rate (lr) is 0.1 and our discount rate is 1 (= no discount)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our mouse **explores the environment and takes random actions**
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Monte Carlo](../Images/de3ade61595152a7510c90a06231c043.png)'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The mouse made more than 10 steps, so the episode ends .
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Monte Carlo](../Images/80025a29726af24822d02b1a50c2ae81.png)'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: We have a list of state, action, rewards, next_state, **we need to calculate
    the return<math><semantics><mrow><mi>G</mi><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow></mrow><annotation
    encoding="application/x-tex">G{t=0}</annotation></semantics></math>Gt=0** <math><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>3</mn></mrow></msub><mi
    mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation
    encoding="application/x-tex">G_t = R_{t+1} + R_{t+2} + R_{t+3} ...</annotation></semantics></math>Gt​=Rt+1​+Rt+2​+Rt+3​...
    (for simplicity, we don’t discount the rewards) <math><semantics><mrow><msub><mi>G</mi><mn>0</mn></msub><mo>=</mo><msub><mi>R</mi><mn>1</mn></msub><mo>+</mo><msub><mi>R</mi><mn>2</mn></msub><mo>+</mo><msub><mi>R</mi><mn>3</mn></msub><mo>…</mo></mrow><annotation
    encoding="application/x-tex">G_0 = R_{1} + R_{2} + R_{3}…</annotation></semantics></math>G0​=R1​+R2​+R3​…
    <math><semantics><mrow><msub><mi>G</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn><mo>+</mo><mn>0</mn><mo>+</mo><mn>0</mn><mo>+</mo><mn>0</mn><mo>+</mo><mn>0</mn><mo>+</mo><mn>0</mn><mo>+</mo><mn>1</mn><mo>+</mo><mn>1</mn><mo>+</mo><mn>0</mn><mo>+</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">G_0 = 1 + 0 + 0 + 0 + 0 + 0 + 1 + 1 + 0 + 0</annotation></semantics></math>G0​=1+0+0+0+0+0+1+1+0+0
    <math><semantics><mrow><msub><mi>G</mi><mn>0</mn></msub><mo>=</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">G_0 = 3</annotation></semantics></math>G0​=3
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有一个状态、动作、奖励、下一个状态的列表，**我们需要计算回报<math><semantics><mrow><mi>G</mi><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow></mrow><annotation
    encoding="application/x-tex">G{t=0}</annotation></semantics></math>Gt=0** <math><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>3</mn></mrow></msub><mi
    mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation
    encoding="application/x-tex">G_t = R_{t+1} + R_{t+2} + R_{t+3} ...</annotation></semantics></math>Gt​=Rt+1​+Rt+2​+Rt+3​...（为简单起见，我们不打折奖励）
    <math><semantics><mrow><msub><mi>G</mi><mn>0</mn></msub><mo>=</mo><msub><mi>R</mi><mn>1</mn></msub><mo>+</mo><msub><mi>R</mi><mn>2</mn></msub><mo>+</mo><msub><mi>R</mi><mn>3</mn></msub><mo>…</mo></mrow><annotation
    encoding="application/x-tex">G_0 = R_{1} + R_{2} + R_{3}…</annotation></semantics></math>G0​=R1​+R2​+R3​…
    <math><semantics><mrow><msub><mi>G</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn><mo>+</mo><mn>0</mn><mo>+</mo><mn>0</mn><mo>+</mo><mn>0</mn><mo>+</mo><mn>0</mn><mo>+</mo><mn>0</mn><mo>+</mo><mn>1</mn><mo>+</mo><mn>1</mn><mo>+</mo><mn>0</mn><mo>+</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">G_0 = 1 + 0 + 0 + 0 + 0 + 0 + 1 + 1 + 0 + 0</annotation></semantics></math>G0​=1+0+0+0+0+0+1+1+0+0
    <math><semantics><mrow><msub><mi>G</mi><mn>0</mn></msub><mo>=</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">G_0 = 3</annotation></semantics></math>G0​=3
- en: 'We can now compute the **new**<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_0)</annotation></semantics></math>V(S0​):'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们可以计算**新的**<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_0)</annotation></semantics></math>V(S0​)：
- en: '![Monte Carlo](../Images/4a209eb21ceb820fc6968e98524a9e05.png) <math><semantics><mrow><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo>+</mo><mi>l</mi><mi>r</mi><mo>∗</mo><mo
    stretchy="false">[</mo><msub><mi>G</mi><mn>0</mn></msub><mtext>—</mtext><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo
    stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">V(S_0)
    = V(S_0) + lr * [G_0 — V(S_0)]</annotation></semantics></math>V(S0​)=V(S0​)+lr∗[G0​—V(S0​)]
    <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo><mo>=</mo><mn>0</mn><mo>+</mo><mn>0.1</mn><mo>∗</mo><mo
    stretchy="false">[</mo><mn>3</mn><mtext>–</mtext><mn>0</mn><mo stretchy="false">]</mo></mrow><annotation
    encoding="application/x-tex">V(S_0) = 0 + 0.1 * [3 – 0]</annotation></semantics></math>V(S0​)=0+0.1∗[3–0]
    <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo><mo>=</mo><mn>0.3</mn></mrow><annotation encoding="application/x-tex">V(S_0)
    = 0.3</annotation></semantics></math>V(S0​)=0.3'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![蒙特卡洛](../Images/4a209eb21ceb820fc6968e98524a9e05.png) <math><semantics><mrow><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo>+</mo><mi>l</mi><mi>r</mi><mo>∗</mo><mo
    stretchy="false">[</mo><msub><mi>G</mi><mn>0</mn></msub><mtext>—</mtext><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo
    stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">V(S_0)
    = V(S_0) + lr * [G_0 — V(S_0)]</annotation></semantics></math>V(S0​)=V(S0​)+lr∗[G0​—V(S0​)]
    <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo><mo>=</mo><mn>0</mn><mo>+</mo><mn>0.1</mn><mo>∗</mo><mo
    stretchy="false">[</mo><mn>3</mn><mtext>–</mtext><mn>0</mn><mo stretchy="false">]</mo></mrow><annotation
    encoding="application/x-tex">V(S_0) = 0 + 0.1 * [3 – 0]</annotation></semantics></math>V(S0​)=0+0.1∗[3–0]
    <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo><mo>=</mo><mn>0.3</mn></mrow><annotation encoding="application/x-tex">V(S_0)
    = 0.3</annotation></semantics></math>V(S0​)=0.3'
- en: '![Monte Carlo](../Images/63cd724aa6e4576eee8bd60359878abc.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![蒙特卡洛](../Images/63cd724aa6e4576eee8bd60359878abc.png)'
- en: 'Temporal Difference Learning: learning at each step'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间差分学习：每一步学习
- en: '**Temporal Difference, on the other hand, waits for only one interaction (one
    step)<math><semantics><mrow><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">S_{t+1}</annotation></semantics></math>St+1​** to
    form a TD target and update<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_t)</annotation></semantics></math>V(St​)
    using<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​ and<math><semantics><mrow><mi>γ</mi><mo>∗</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">\gamma
    * V(S_{t+1})</annotation></semantics></math>γ∗V(St+1​).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**时间差异**只等待一个交互（一个步骤）<math><semantics><mrow><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">S_{t+1}</annotation></semantics></math>St+1​**来形成一个TD目标并更新<math><semantics><mrow><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">V(S_t)</annotation></semantics></math>V(St​)使用<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​和<math><semantics><mrow><mi>γ</mi><mo>∗</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">\gamma
    * V(S_{t+1})</annotation></semantics></math>γ∗V(St+1​)。
- en: The idea with **TD is to update the<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_t)</annotation></semantics></math>V(St​)
    at each step.**
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**TD的想法是在每一步更新<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_t)</annotation></semantics></math>V(St​)。**'
- en: But because we didn’t experience an entire episode, we don’t have<math><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">G_t</annotation></semantics></math>Gt​ (expected
    return). Instead, **we estimate<math><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">G_t</annotation></semantics></math>Gt​ by adding<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​ and
    the discounted value of the next state.**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 但是因为我们没有经历整个情节，我们没有<math><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">G_t</annotation></semantics></math>Gt​（预期回报）。相反，**我们通过添加<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​和下一个状态的折扣值来估计<math><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">G_t</annotation></semantics></math>Gt​。**
- en: This is called bootstrapping. It’s called this **because TD bases its update
    in part on an existing estimate<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_{t+1})</annotation></semantics></math>V(St+1​)
    and not a complete sample<math><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">G_t</annotation></semantics></math>Gt​.**
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为自举。它被称为这样**因为TD部分地基于现有估计<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_{t+1})</annotation></semantics></math>V(St+1​)的更新，而不是完整的样本<math><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">G_t</annotation></semantics></math>Gt​。**
- en: '![Temporal Difference](../Images/56f8c1151f274ebdbc9bcfe88f3a6f80.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![时间差异](../Images/56f8c1151f274ebdbc9bcfe88f3a6f80.png)'
- en: This method is called TD(0) or **one-step TD (update the value function after
    any individual step).**
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法称为TD(0)或**一步TD（在任何单个步骤之后更新值函数）。**
- en: '![Temporal Difference](../Images/286efaf5c1b5de2ad9e109131ca201df.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![时间差异](../Images/286efaf5c1b5de2ad9e109131ca201df.png)'
- en: If we take the same example,
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们采用相同的例子，
- en: '![Temporal Difference](../Images/175fb7d45ed3bc3efb402d3f79053314.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![时间差异](../Images/175fb7d45ed3bc3efb402d3f79053314.png)'
- en: We initialize our value function so that it returns 0 value for each state.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们初始化我们的值函数，使其对每个状态返回0值。
- en: Our learning rate (lr) is 0.1, and our discount rate is 1 (no discount).
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的学习率（lr）为0.1，折扣率为1（无折扣）。
- en: Our mouse begins to explore the environment and takes a random action: **going
    to the left**
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的老鼠开始探索环境并采取随机行动：**向左走**
- en: It gets a reward <math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">R_{t+1} = 1</annotation></semantics></math>Rt+1​=1
    since **it eats a piece of cheese**
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它得到了奖励<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">R_{t+1} = 1</annotation></semantics></math>Rt+1​=1，因为**它吃了一块奶酪**
- en: '![Temporal Difference](../Images/4e5303ed2cdddedb79026b8bcf87db50.png)'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![时间差异](../Images/4e5303ed2cdddedb79026b8bcf87db50.png)'
- en: '![Temporal Difference](../Images/4d6ac97d6902760f7c73b3041a426db2.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![时间差异](../Images/4d6ac97d6902760f7c73b3041a426db2.png)'
- en: 'We can now update <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_0)</annotation></semantics></math>V(S0​):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以更新<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_0)</annotation></semantics></math>V(S0​)：
- en: New <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo><mo>=</mo><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo><mo>+</mo><mi>l</mi><mi>r</mi><mo>∗</mo><mo stretchy="false">[</mo><msub><mi>R</mi><mn>1</mn></msub><mo>+</mo><mi>γ</mi><mo>∗</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>−</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo
    stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">V(S_0)
    = V(S_0) + lr * [R_1 + \gamma * V(S_1) - V(S_0)]</annotation></semantics></math>V(S0​)=V(S0​)+lr∗[R1​+γ∗V(S1​)−V(S0​)]
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 新的<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo><mo>=</mo><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo><mo>+</mo><mi>l</mi><mi>r</mi><mo>∗</mo><mo stretchy="false">[</mo><msub><mi>R</mi><mn>1</mn></msub><mo>+</mo><mi>γ</mi><mo>∗</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>−</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo
    stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">V(S_0)
    = V(S_0) + lr * [R_1 + \gamma * V(S_1) - V(S_0)]</annotation></semantics></math>V(S0​)=V(S0​)+lr∗[R1​+γ∗V(S1​)−V(S0​)]
- en: New<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo><mo>=</mo><mn>0</mn><mo>+</mo><mn>0.1</mn><mo>∗</mo><mo
    stretchy="false">[</mo><mn>1</mn><mo>+</mo><mn>1</mn><mo>∗</mo><mn>0</mn><mtext>–</mtext><mn>0</mn><mo
    stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">V(S_0)
    = 0 + 0.1 * [1 + 1 * 0–0]</annotation></semantics></math>V(S0​)=0+0.1∗[1+1∗0–0]
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 新的<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo><mo>=</mo><mn>0</mn><mo>+</mo><mn>0.1</mn><mo>∗</mo><mo
    stretchy="false">[</mo><mn>1</mn><mo>+</mo><mn>1</mn><mo>∗</mo><mn>0</mn><mtext>–</mtext><mn>0</mn><mo
    stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">V(S_0)
    = 0 + 0.1 * [1 + 1 * 0–0]</annotation></semantics></math>V(S0​)=0+0.1∗[1+1∗0–0]
- en: New<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">V(S_0)
    = 0.1</annotation></semantics></math>V(S0​)=0.1
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 新的<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">V(S_0)
    = 0.1</annotation></semantics></math>V(S0​)=0.1
- en: So we just updated our value function for State 0.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们刚刚更新了状态0的值函数。
- en: Now we **continue to interact with this environment with our updated value function.**
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们**继续与这个环境互动，使用我们更新后的值函数。**
- en: '![Temporal Difference](../Images/8395ce7b9aecec14c7501e705f7399d7.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![时间差异](../Images/8395ce7b9aecec14c7501e705f7399d7.png)'
- en: 'To summarize:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下：
- en: With *Monte Carlo*, we update the value function from a complete episode, and
    so we **use the actual accurate discounted return of this episode.**
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*蒙特卡洛*方法，我们从完整的一集更新值函数，因此**使用该一集的实际准确的折扣回报。**
- en: With *TD Learning*, we update the value function from a step, and we replace<math><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">G_t</annotation></semantics></math>Gt​, which we
    don’t know, with **an estimated return called the TD target.**
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*TD学习*，我们从一步更新值函数，并用一个估计的回报替换我们不知道的<math><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">G_t</annotation></semantics></math>Gt​，称为TD目标。
- en: '![Summary](../Images/35ab544da41ee9cb8401401181dde763.png)'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![总结](../Images/35ab544da41ee9cb8401401181dde763.png)'
