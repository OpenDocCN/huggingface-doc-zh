# TensorFlow模型的XLA集成

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/tf_xla](https://huggingface.co/docs/transformers/v4.37.2/en/tf_xla)

加速线性代数，简称XLA，是用于加速TensorFlow模型运行时的编译器。来自[官方文档](https://www.tensorflow.org/xla)：

XLA（加速线性代数）是一个专门用于线性代数的编译器，可以加速TensorFlow模型，可能不需要源代码更改。

在TensorFlow中使用XLA很简单 - 它已经打包在`tensorflow`库中，并且可以通过任何创建图形函数（例如[`tf.function`](https://www.tensorflow.org/guide/intro_to_graphs)）中的`jit_compile`参数触发。当使用Keras方法如`fit()`和`predict()`时，您可以通过将`jit_compile`参数传递给`model.compile()`来简单启用XLA。但是，XLA不仅限于这些方法 - 它还可以用于加速任何任意的`tf.function`。

🤗 Transformers中的几个TensorFlow方法已经重写为与XLA兼容，包括用于模型的文本生成，如[GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)、[T5](https://huggingface.co/docs/transformers/model_doc/t5)和[OPT](https://huggingface.co/docs/transformers/model_doc/opt)，以及用于语音处理的模型，如[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)。

在🤗 Transformers内部的TensorFlow文本生成模型中，加速的确切数量非常依赖于模型，我们注意到速度提升了约100倍。本文将解释如何在这些模型中使用XLA来获得最大的性能。我们还将提供额外资源的链接，如果您有兴趣了解更多关于基准测试和我们在XLA集成背后的设计理念。

## 使用XLA运行TF函数

让我们考虑以下TensorFlow模型：

```py
import tensorflow as tf

model = tf.keras.Sequential(
    [tf.keras.layers.Dense(10, input_shape=(10,), activation="relu"), tf.keras.layers.Dense(5, activation="softmax")]
)
```

上述模型接受维度为`(10, )`的输入。我们可以使用该模型来运行前向传递，如下所示：

```py
# Generate random inputs for the model.
batch_size = 16
input_vector_dim = 10
random_inputs = tf.random.normal((batch_size, input_vector_dim))

# Run a forward pass.
_ = model(random_inputs)
```

为了使用XLA编译函数运行前向传递，我们需要执行以下操作：

```py
xla_fn = tf.function(model, jit_compile=True)
_ = xla_fn(random_inputs)
```

`model`的默认`call()`函数用于编译XLA图。但是，如果有任何其他模型函数您想要编译成XLA，也是可能的，例如：

```py
my_xla_fn = tf.function(model.my_xla_fn, jit_compile=True)
```

## 使用🤗 Transformers中的XLA运行TF文本生成模型

要在🤗 Transformers内启用XLA加速生成，您需要安装最新版本的`transformers`。您可以通过运行以下命令来安装：

```py
pip install transformers --upgrade
```

然后您可以运行以下代码：

```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForCausalLM

# Will error if the minimal version of Transformers is not installed.
from transformers.utils import check_min_version

check_min_version("4.21.0")

tokenizer = AutoTokenizer.from_pretrained("gpt2", padding_side="left", pad_token="</s>")
model = TFAutoModelForCausalLM.from_pretrained("gpt2")
input_string = ["TensorFlow is"]

# One line to create an XLA generation function
xla_generate = tf.function(model.generate, jit_compile=True)

tokenized_input = tokenizer(input_string, return_tensors="tf")
generated_tokens = xla_generate(**tokenized_input, num_beams=2)

decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)
print(f"Generated -- {decoded_text}")
# Generated -- TensorFlow is an open-source, open-source, distributed-source application # framework for the
```

正如您可以注意到的，在`generate()`上启用XLA只是一行代码。其余代码保持不变。但是，上面代码片段中有一些特定于XLA的注意事项。您需要注意这些才能实现XLA带来的加速。我们将在下一节中讨论这些。

## 需要注意的事项

当您首次执行启用XLA的函数（如上面的`xla_generate()`）时，它将内部尝试推断计算图，这是耗时的。这个过程被称为[“跟踪”](https://www.tensorflow.org/guide/intro_to_graphs#when_is_a_function_tracing)。

您可能会注意到生成时间不够快。连续调用`xla_generate()`（或任何其他启用XLA的函数）不需要推断计算图，只要函数的输入遵循最初构建计算图时的相同形状。虽然对于具有固定输入形状的模态（例如图像）这不是问题，但如果您正在处理具有可变输入形状的模态（例如文本），则必须注意。

为了确保`xla_generate()`始终使用相同的输入形状，您可以在调用分词器时指定`padding`参数。

```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gpt2", padding_side="left", pad_token="</s>")
model = TFAutoModelForCausalLM.from_pretrained("gpt2")
input_string = ["TensorFlow is"]

xla_generate = tf.function(model.generate, jit_compile=True)

# Here, we call the tokenizer with padding options.
tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors="tf")

generated_tokens = xla_generate(**tokenized_input, num_beams=2)
decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)
print(f"Generated -- {decoded_text}")
```

这样，您可以确保`xla_generate()`的输入始终接收与其跟踪时相同形状的输入，从而加快生成时间。您可以使用下面的代码进行验证：

```py
import time
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gpt2", padding_side="left", pad_token="</s>")
model = TFAutoModelForCausalLM.from_pretrained("gpt2")

xla_generate = tf.function(model.generate, jit_compile=True)

for input_string in ["TensorFlow is", "TensorFlow is a", "TFLite is a"]:
    tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors="tf")
    start = time.time_ns()
    generated_tokens = xla_generate(**tokenized_input, num_beams=2)
    end = time.time_ns()
    print(f"Execution time -- {(end - start) / 1e6:.1f} ms\n")
```

在Tesla T4 GPU上，您可以期望输出如下：

```py
Execution time -- 30819.6 ms

Execution time -- 79.0 ms

Execution time -- 78.9 ms
```

第一次调用`xla_generate()`由于跟踪而耗时，但后续调用速度快得多。请记住，任何时候对生成选项进行更改都会触发重新跟踪，从而导致生成时间变慢。

我们没有在本文档中涵盖🤗 Transformers提供的所有文本生成选项。我们鼓励您阅读高级用例的文档。

## 额外资源

在这里，如果您想深入了解🤗 Transformers中的XLA和一般情况下的XLA，我们为您提供了一些额外资源。

+   [这个Colab笔记本](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/91_tf_xla_generate.ipynb)提供了一个交互式演示，如果您想要尝试XLA兼容的编码器-解码器（如[T5](https://huggingface.co/docs/transformers/model_doc/t5)）和仅解码器（如[GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)）文本生成模型。

+   [这篇博客文章](https://huggingface.co/blog/tf-xla-generate)提供了XLA兼容模型的比较基准概述，以及对TensorFlow中XLA的友好介绍。

+   [这篇博客文章](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html)讨论了我们在🤗 Transformers中为TensorFlow模型添加XLA支持的设计理念。

+   学习更多关于XLA和TensorFlow图的推荐帖子：

    +   [XLA：用于机器学习的优化编译器](https://www.tensorflow.org/xla)

    +   [图形和tf.function简介](https://www.tensorflow.org/guide/intro_to_graphs)

    +   [使用tf.function获得更好的性能](https://www.tensorflow.org/guide/function)
