# TensorFlowæ¨¡å‹çš„XLAé›†æˆ

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tf_xla](https://huggingface.co/docs/transformers/v4.37.2/en/tf_xla)

åŠ é€Ÿçº¿æ€§ä»£æ•°ï¼Œç®€ç§°XLAï¼Œæ˜¯ç”¨äºåŠ é€ŸTensorFlowæ¨¡å‹è¿è¡Œæ—¶çš„ç¼–è¯‘å™¨ã€‚æ¥è‡ª[å®˜æ–¹æ–‡æ¡£](https://www.tensorflow.org/xla)ï¼š

XLAï¼ˆåŠ é€Ÿçº¿æ€§ä»£æ•°ï¼‰æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºçº¿æ€§ä»£æ•°çš„ç¼–è¯‘å™¨ï¼Œå¯ä»¥åŠ é€ŸTensorFlowæ¨¡å‹ï¼Œå¯èƒ½ä¸éœ€è¦æºä»£ç æ›´æ”¹ã€‚

åœ¨TensorFlowä¸­ä½¿ç”¨XLAå¾ˆç®€å• - å®ƒå·²ç»æ‰“åŒ…åœ¨`tensorflow`åº“ä¸­ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ä»»ä½•åˆ›å»ºå›¾å½¢å‡½æ•°ï¼ˆä¾‹å¦‚[`tf.function`](https://www.tensorflow.org/guide/intro_to_graphs)ï¼‰ä¸­çš„`jit_compile`å‚æ•°è§¦å‘ã€‚å½“ä½¿ç”¨Kerasæ–¹æ³•å¦‚`fit()`å’Œ`predict()`æ—¶ï¼Œæ‚¨å¯ä»¥é€šè¿‡å°†`jit_compile`å‚æ•°ä¼ é€’ç»™`model.compile()`æ¥ç®€å•å¯ç”¨XLAã€‚ä½†æ˜¯ï¼ŒXLAä¸ä»…é™äºè¿™äº›æ–¹æ³• - å®ƒè¿˜å¯ä»¥ç”¨äºåŠ é€Ÿä»»ä½•ä»»æ„çš„`tf.function`ã€‚

ğŸ¤— Transformersä¸­çš„å‡ ä¸ªTensorFlowæ–¹æ³•å·²ç»é‡å†™ä¸ºä¸XLAå…¼å®¹ï¼ŒåŒ…æ‹¬ç”¨äºæ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆï¼Œå¦‚[GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)ã€[T5](https://huggingface.co/docs/transformers/model_doc/t5)å’Œ[OPT](https://huggingface.co/docs/transformers/model_doc/opt)ï¼Œä»¥åŠç”¨äºè¯­éŸ³å¤„ç†çš„æ¨¡å‹ï¼Œå¦‚[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)ã€‚

åœ¨ğŸ¤— Transformerså†…éƒ¨çš„TensorFlowæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ä¸­ï¼ŒåŠ é€Ÿçš„ç¡®åˆ‡æ•°é‡éå¸¸ä¾èµ–äºæ¨¡å‹ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°é€Ÿåº¦æå‡äº†çº¦100å€ã€‚æœ¬æ–‡å°†è§£é‡Šå¦‚ä½•åœ¨è¿™äº›æ¨¡å‹ä¸­ä½¿ç”¨XLAæ¥è·å¾—æœ€å¤§çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å°†æä¾›é¢å¤–èµ„æºçš„é“¾æ¥ï¼Œå¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£æ›´å¤šå…³äºåŸºå‡†æµ‹è¯•å’Œæˆ‘ä»¬åœ¨XLAé›†æˆèƒŒåçš„è®¾è®¡ç†å¿µã€‚

## ä½¿ç”¨XLAè¿è¡ŒTFå‡½æ•°

è®©æˆ‘ä»¬è€ƒè™‘ä»¥ä¸‹TensorFlowæ¨¡å‹ï¼š

```py
import tensorflow as tf

model = tf.keras.Sequential(
    [tf.keras.layers.Dense(10, input_shape=(10,), activation="relu"), tf.keras.layers.Dense(5, activation="softmax")]
)
```

ä¸Šè¿°æ¨¡å‹æ¥å—ç»´åº¦ä¸º`(10, )`çš„è¾“å…¥ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¯¥æ¨¡å‹æ¥è¿è¡Œå‰å‘ä¼ é€’ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
# Generate random inputs for the model.
batch_size = 16
input_vector_dim = 10
random_inputs = tf.random.normal((batch_size, input_vector_dim))

# Run a forward pass.
_ = model(random_inputs)
```

ä¸ºäº†ä½¿ç”¨XLAç¼–è¯‘å‡½æ•°è¿è¡Œå‰å‘ä¼ é€’ï¼Œæˆ‘ä»¬éœ€è¦æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```py
xla_fn = tf.function(model, jit_compile=True)
_ = xla_fn(random_inputs)
```

`model`çš„é»˜è®¤`call()`å‡½æ•°ç”¨äºç¼–è¯‘XLAå›¾ã€‚ä½†æ˜¯ï¼Œå¦‚æœæœ‰ä»»ä½•å…¶ä»–æ¨¡å‹å‡½æ•°æ‚¨æƒ³è¦ç¼–è¯‘æˆXLAï¼Œä¹Ÿæ˜¯å¯èƒ½çš„ï¼Œä¾‹å¦‚ï¼š

```py
my_xla_fn = tf.function(model.my_xla_fn, jit_compile=True)
```

## ä½¿ç”¨ğŸ¤— Transformersä¸­çš„XLAè¿è¡ŒTFæ–‡æœ¬ç”Ÿæˆæ¨¡å‹

è¦åœ¨ğŸ¤— Transformerså†…å¯ç”¨XLAåŠ é€Ÿç”Ÿæˆï¼Œæ‚¨éœ€è¦å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„`transformers`ã€‚æ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…ï¼š

```py
pip install transformers --upgrade
```

ç„¶åæ‚¨å¯ä»¥è¿è¡Œä»¥ä¸‹ä»£ç ï¼š

```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForCausalLM

# Will error if the minimal version of Transformers is not installed.
from transformers.utils import check_min_version

check_min_version("4.21.0")

tokenizer = AutoTokenizer.from_pretrained("gpt2", padding_side="left", pad_token="</s>")
model = TFAutoModelForCausalLM.from_pretrained("gpt2")
input_string = ["TensorFlow is"]

# One line to create an XLA generation function
xla_generate = tf.function(model.generate, jit_compile=True)

tokenized_input = tokenizer(input_string, return_tensors="tf")
generated_tokens = xla_generate(**tokenized_input, num_beams=2)

decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)
print(f"Generated -- {decoded_text}")
# Generated -- TensorFlow is an open-source, open-source, distributed-source application # framework for the
```

æ­£å¦‚æ‚¨å¯ä»¥æ³¨æ„åˆ°çš„ï¼Œåœ¨`generate()`ä¸Šå¯ç”¨XLAåªæ˜¯ä¸€è¡Œä»£ç ã€‚å…¶ä½™ä»£ç ä¿æŒä¸å˜ã€‚ä½†æ˜¯ï¼Œä¸Šé¢ä»£ç ç‰‡æ®µä¸­æœ‰ä¸€äº›ç‰¹å®šäºXLAçš„æ³¨æ„äº‹é¡¹ã€‚æ‚¨éœ€è¦æ³¨æ„è¿™äº›æ‰èƒ½å®ç°XLAå¸¦æ¥çš„åŠ é€Ÿã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­è®¨è®ºè¿™äº›ã€‚

## éœ€è¦æ³¨æ„çš„äº‹é¡¹

å½“æ‚¨é¦–æ¬¡æ‰§è¡Œå¯ç”¨XLAçš„å‡½æ•°ï¼ˆå¦‚ä¸Šé¢çš„`xla_generate()`ï¼‰æ—¶ï¼Œå®ƒå°†å†…éƒ¨å°è¯•æ¨æ–­è®¡ç®—å›¾ï¼Œè¿™æ˜¯è€—æ—¶çš„ã€‚è¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸º[â€œè·Ÿè¸ªâ€](https://www.tensorflow.org/guide/intro_to_graphs#when_is_a_function_tracing)ã€‚

æ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°ç”Ÿæˆæ—¶é—´ä¸å¤Ÿå¿«ã€‚è¿ç»­è°ƒç”¨`xla_generate()`ï¼ˆæˆ–ä»»ä½•å…¶ä»–å¯ç”¨XLAçš„å‡½æ•°ï¼‰ä¸éœ€è¦æ¨æ–­è®¡ç®—å›¾ï¼Œåªè¦å‡½æ•°çš„è¾“å…¥éµå¾ªæœ€åˆæ„å»ºè®¡ç®—å›¾æ—¶çš„ç›¸åŒå½¢çŠ¶ã€‚è™½ç„¶å¯¹äºå…·æœ‰å›ºå®šè¾“å…¥å½¢çŠ¶çš„æ¨¡æ€ï¼ˆä¾‹å¦‚å›¾åƒï¼‰è¿™ä¸æ˜¯é—®é¢˜ï¼Œä½†å¦‚æœæ‚¨æ­£åœ¨å¤„ç†å…·æœ‰å¯å˜è¾“å…¥å½¢çŠ¶çš„æ¨¡æ€ï¼ˆä¾‹å¦‚æ–‡æœ¬ï¼‰ï¼Œåˆ™å¿…é¡»æ³¨æ„ã€‚

ä¸ºäº†ç¡®ä¿`xla_generate()`å§‹ç»ˆä½¿ç”¨ç›¸åŒçš„è¾“å…¥å½¢çŠ¶ï¼Œæ‚¨å¯ä»¥åœ¨è°ƒç”¨åˆ†è¯å™¨æ—¶æŒ‡å®š`padding`å‚æ•°ã€‚

```py
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gpt2", padding_side="left", pad_token="</s>")
model = TFAutoModelForCausalLM.from_pretrained("gpt2")
input_string = ["TensorFlow is"]

xla_generate = tf.function(model.generate, jit_compile=True)

# Here, we call the tokenizer with padding options.
tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors="tf")

generated_tokens = xla_generate(**tokenized_input, num_beams=2)
decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)
print(f"Generated -- {decoded_text}")
```

è¿™æ ·ï¼Œæ‚¨å¯ä»¥ç¡®ä¿`xla_generate()`çš„è¾“å…¥å§‹ç»ˆæ¥æ”¶ä¸å…¶è·Ÿè¸ªæ—¶ç›¸åŒå½¢çŠ¶çš„è¾“å…¥ï¼Œä»è€ŒåŠ å¿«ç”Ÿæˆæ—¶é—´ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç è¿›è¡ŒéªŒè¯ï¼š

```py
import time
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gpt2", padding_side="left", pad_token="</s>")
model = TFAutoModelForCausalLM.from_pretrained("gpt2")

xla_generate = tf.function(model.generate, jit_compile=True)

for input_string in ["TensorFlow is", "TensorFlow is a", "TFLite is a"]:
    tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors="tf")
    start = time.time_ns()
    generated_tokens = xla_generate(**tokenized_input, num_beams=2)
    end = time.time_ns()
    print(f"Execution time -- {(end - start) / 1e6:.1f} ms\n")
```

åœ¨Tesla T4 GPUä¸Šï¼Œæ‚¨å¯ä»¥æœŸæœ›è¾“å‡ºå¦‚ä¸‹ï¼š

```py
Execution time -- 30819.6 ms

Execution time -- 79.0 ms

Execution time -- 78.9 ms
```

ç¬¬ä¸€æ¬¡è°ƒç”¨`xla_generate()`ç”±äºè·Ÿè¸ªè€Œè€—æ—¶ï¼Œä½†åç»­è°ƒç”¨é€Ÿåº¦å¿«å¾—å¤šã€‚è¯·è®°ä½ï¼Œä»»ä½•æ—¶å€™å¯¹ç”Ÿæˆé€‰é¡¹è¿›è¡Œæ›´æ”¹éƒ½ä¼šè§¦å‘é‡æ–°è·Ÿè¸ªï¼Œä»è€Œå¯¼è‡´ç”Ÿæˆæ—¶é—´å˜æ…¢ã€‚

æˆ‘ä»¬æ²¡æœ‰åœ¨æœ¬æ–‡æ¡£ä¸­æ¶µç›–ğŸ¤— Transformersæä¾›çš„æ‰€æœ‰æ–‡æœ¬ç”Ÿæˆé€‰é¡¹ã€‚æˆ‘ä»¬é¼“åŠ±æ‚¨é˜…è¯»é«˜çº§ç”¨ä¾‹çš„æ–‡æ¡£ã€‚

## é¢å¤–èµ„æº

åœ¨è¿™é‡Œï¼Œå¦‚æœæ‚¨æƒ³æ·±å…¥äº†è§£ğŸ¤— Transformersä¸­çš„XLAå’Œä¸€èˆ¬æƒ…å†µä¸‹çš„XLAï¼Œæˆ‘ä»¬ä¸ºæ‚¨æä¾›äº†ä¸€äº›é¢å¤–èµ„æºã€‚

+   [è¿™ä¸ªColabç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/91_tf_xla_generate.ipynb)æä¾›äº†ä¸€ä¸ªäº¤äº’å¼æ¼”ç¤ºï¼Œå¦‚æœæ‚¨æƒ³è¦å°è¯•XLAå…¼å®¹çš„ç¼–ç å™¨-è§£ç å™¨ï¼ˆå¦‚[T5](https://huggingface.co/docs/transformers/model_doc/t5)ï¼‰å’Œä»…è§£ç å™¨ï¼ˆå¦‚[GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)ï¼‰æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€‚

+   [è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/tf-xla-generate)æä¾›äº†XLAå…¼å®¹æ¨¡å‹çš„æ¯”è¾ƒåŸºå‡†æ¦‚è¿°ï¼Œä»¥åŠå¯¹TensorFlowä¸­XLAçš„å‹å¥½ä»‹ç»ã€‚

+   [è¿™ç¯‡åšå®¢æ–‡ç« ](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html)è®¨è®ºäº†æˆ‘ä»¬åœ¨ğŸ¤— Transformersä¸­ä¸ºTensorFlowæ¨¡å‹æ·»åŠ XLAæ”¯æŒçš„è®¾è®¡ç†å¿µã€‚

+   å­¦ä¹ æ›´å¤šå…³äºXLAå’ŒTensorFlowå›¾çš„æ¨èå¸–å­ï¼š

    +   [XLAï¼šç”¨äºæœºå™¨å­¦ä¹ çš„ä¼˜åŒ–ç¼–è¯‘å™¨](https://www.tensorflow.org/xla)

    +   [å›¾å½¢å’Œtf.functionç®€ä»‹](https://www.tensorflow.org/guide/intro_to_graphs)

    +   [ä½¿ç”¨tf.functionè·å¾—æ›´å¥½çš„æ€§èƒ½](https://www.tensorflow.org/guide/function)
