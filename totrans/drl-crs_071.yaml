- en: The SnowballTarget Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/learn/deep-rl-course/unit5/snowball-target](https://huggingface.co/learn/deep-rl-course/unit5/snowball-target)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/63.274717bc.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">
  prefs: []
  type: TYPE_NORMAL
- en: '![SnowballTarget](../Images/fb625fa1ee3e280912baaaa56548960f.png)'
  prefs: []
  type: TYPE_IMG
- en: SnowballTarget is an environment we created at Hugging Face using assets from
    [Kay Lousberg](https://kaylousberg.com/). We have an optional section at the end
    of this Unit **if you want to learn to use Unity and create your environments**.
  prefs: []
  type: TYPE_NORMAL
- en: The agent‚Äôs Goal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first agent you‚Äôre going to train is called Julien the bear üêª. Julien is
    trained **to hit targets with snowballs**.
  prefs: []
  type: TYPE_NORMAL
- en: The Goal in this environment is that Julien **hits as many targets as possible
    in the limited time** (1000 timesteps). It will need **to place itself correctly
    in relation to the target and shoot**to do that.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, to avoid ‚Äúsnowball spamming‚Äù (aka shooting a snowball every timestep),
    **Julien has a ‚Äúcool off‚Äù system** (it needs to wait 0.5 seconds after a shoot
    to be able to shoot again).
  prefs: []
  type: TYPE_NORMAL
- en: '![Cool Off System](../Images/c50f4eb3112c190598a995d006aece23.png)'
  prefs: []
  type: TYPE_IMG
- en: The agent needs to wait 0.5s before being able to shoot a snowball again
  prefs: []
  type: TYPE_NORMAL
- en: The reward function and the reward engineering problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The reward function is simple. **The environment gives a +1 reward every time
    the agent‚Äôs snowball hits a target**. Because the agent‚Äôs Goal is to maximize
    the expected cumulative reward, **it will try to hit as many targets as possible**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Reward system](../Images/97bb6395a1762bfb94d1754c058aa292.png)'
  prefs: []
  type: TYPE_IMG
- en: We could have a more complex reward function (with a penalty to push the agent
    to go faster, for example). But when you design an environment, you need to avoid
    the *reward engineering problem*, which is having a too complex reward function
    to force your agent to behave as you want it to do. Why? Because by doing that,
    **you might miss interesting strategies that the agent will find with a simpler
    reward function**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of code, it looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reward](../Images/7b73d2b4241bd0c68e89efd1fe743505.png)'
  prefs: []
  type: TYPE_IMG
- en: The observation space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regarding observations, we don‚Äôt use normal vision (frame), but **we use raycasts**.
  prefs: []
  type: TYPE_NORMAL
- en: Think of raycasts as lasers that will detect if they pass through an object.
  prefs: []
  type: TYPE_NORMAL
- en: '![Raycasts](../Images/93b379dd693a76d5472608228f47e6f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [ML-Agents documentation](https://github.com/Unity-Technologies/ml-agents)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this environment, our agent has multiple set of raycasts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Raycasts](../Images/04047cdc3e12f4d8b6fc04d527b49c3e.png)'
  prefs: []
  type: TYPE_IMG
- en: In addition to raycasts, the agent gets a ‚Äúcan I shoot‚Äù bool as observation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Obs](../Images/781b3f0cb3bdaeaea583e42caa651ef3.png)'
  prefs: []
  type: TYPE_IMG
- en: The action space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The action space is discrete:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Action Space](../Images/1a9d30bdfb0e6cf6ea4e7c6189d64c1f.png)'
  prefs: []
  type: TYPE_IMG
