# LoRA 方法

> 原文链接：[`huggingface.co/docs/peft/task_guides/lora_based_methods`](https://huggingface.co/docs/peft/task_guides/lora_based_methods)

训练大型模型的一种流行方法是在（通常在注意力块中）插入较小的可训练矩阵，这些矩阵是要在微调期间学习的增量权重矩阵的低秩分解。预训练模型的原始权重矩阵被冻结，只有较小的矩阵在训练期间更新。这减少了可训练参数的数量，减少了内存使用和训练时间，对于大型模型来说这可能非常昂贵。

有几种不同的方法可以将权重矩阵表示为低秩分解，但低秩适应（LoRA）是最常见的方法。PEFT 库支持其他几种 LoRA 变体，如低秩 Hadamard 乘积（LoHa）、低秩 Kronecker 乘积（LoKr）和自适应低秩适应（AdaLoRA）。您可以在适配器指南中了解这些方法的概念工作原理。如果您有兴趣将这些方法应用于其他任务和用例，如语义分割、标记分类，请查看我们的[笔记本集合](https://huggingface.co/collections/PEFT/notebooks-6573b28b33e5a4bf5b157fc1)！

本指南将向您展示如何快速训练一个图像分类模型 - 使用低秩分解方法 - 以识别图像中显示的食物类别。

对训练图像分类模型的一般过程有一定了解将非常有帮助，这样可以让您专注于低秩分解方法。如果您是新手，我们建议先查看来自 Transformers 文档的[图像分类](https://huggingface.co/docs/transformers/tasks/image_classification)指南。当您准备好时，回来看看将 PEFT 应用到您的训练中有多么容易！

在开始之前，请确保您已安装所有必要的库。

```py
pip install -q peft transformers datasets
```

## 数据集

在本指南中，您将使用包含 101 种食物类别图像的[Food-101](https://huggingface.co/datasets/food101)数据集（查看[数据集查看器](https://huggingface.co/datasets/food101/viewer/default/train)以更好地了解数据集的外观）。

使用[load_dataset](https://huggingface.co/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)函数加载数据集。

```py
from datasets import load_dataset

ds = load_dataset("food101")
```

每个食物类别都用一个整数标记，为了更容易理解这些整数代表什么，您将创建一个`label2id`和`id2label`字典，将整数映射到其类别标签。

```py
labels = ds["train"].features["label"].names
label2id, id2label = dict(), dict()
for i, label in enumerate(labels):
    label2id[label] = i
    id2label[i] = label

id2label[2]
"baklava"
```

加载一个图像处理器，正确调整训练和评估图像的像素值大小。

```py
from transformers import AutoImageProcessor

image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")
```

您还可以使用图像处理器准备一些转换函数，用于数据增强和像素缩放。

```py
from torchvision.transforms import (
    CenterCrop,
    Compose,
    Normalize,
    RandomHorizontalFlip,
    RandomResizedCrop,
    Resize,
    ToTensor,
)

normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)
train_transforms = Compose(
    [
        RandomResizedCrop(image_processor.size["height"]),
        RandomHorizontalFlip(),
        ToTensor(),
        normalize,
    ]
)

val_transforms = Compose(
    [
        Resize(image_processor.size["height"]),
        CenterCrop(image_processor.size["height"]),
        ToTensor(),
        normalize,
    ]
)

def preprocess_train(example_batch):
    example_batch["pixel_values"] = [train_transforms(image.convert("RGB")) for image in example_batch["image"]]
    return example_batch

def preprocess_val(example_batch):
    example_batch["pixel_values"] = [val_transforms(image.convert("RGB")) for image in example_batch["image"]]
    return example_batch
```

定义训练和验证数据集，并使用[set_transform](https://huggingface.co/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform)函数在运行时应用转换。

```py
train_ds = ds["train"]
val_ds = ds["validation"]

train_ds.set_transform(preprocess_train)
val_ds.set_transform(preprocess_val)
```

最后，您将需要一个数据整理器来创建一批训练和评估数据，并将标签转换为`torch.tensor`对象。

```py
import torch

def collate_fn(examples):
    pixel_values = torch.stack([example["pixel_values"] for example in examples])
    labels = torch.tensor([example["label"] for example in examples])
    return {"pixel_values": pixel_values, "labels": labels}
```

## 模型

现在让我们加载一个预训练模型作为基础模型。本指南使用[google/vit-base-patch16-224-in21k](https://huggingface.co/google/vit-base-patch16-224-in21k)模型，但您可以使用任何您想要的图像分类模型。将`label2id`和`id2label`字典传递给模型，以便它知道如何将整数标签映射到它们的类标签，并且如果您正在微调已经微调过的检查点，则可以选择传递`ignore_mismatched_sizes=True`参数。

```py
from transformers import AutoModelForImageClassification, TrainingArguments, Trainer

model = AutoModelForImageClassification.from_pretrained(
    "google/vit-base-patch16-224-in21k",
    label2id=label2id,
    id2label=id2label,
    ignore_mismatched_sizes=True,
)
```

### PEFT 配置和模型

每种 PEFT 方法都需要一个配置，其中包含指定 PEFT 方法应如何应用的所有参数。一旦设置了配置，将其传递给 get_peft_model()函数，同时传递基础模型，以创建一个可训练的 PeftModel。

调用 print_trainable_parameters()方法，比较 PeftModel 的参数数量与基础模型的参数数量！

LoRALoHaLoKrAdaLoRA

LoRA 将权重更新矩阵分解为*两个*较小的矩阵。这些低秩矩阵的大小由其*秩*或`r`确定。更高的秩意味着模型有更多参数要训练，但也意味着模型有更多的学习能力。您还需要指定`target_modules`，以确定较小矩阵应插入的位置。对于本指南，您将针对注意力块的*query*和*value*矩阵。设置的其他重要参数包括`lora_alpha`（缩放因子）、`bias`（是否应训练`none`、`all`或仅训练 LoRA 偏置参数）、以及`modules_to_save`（除 LoRA 层之外要训练和保存的模块）。所有这些参数 - 以及更多 - 都可以在 LoraConfig 中找到。

```py
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=["query", "value"],
    lora_dropout=0.1,
    bias="none",
    modules_to_save=["classifier"],
)
model = get_peft_model(model, config)
model.print_trainable_parameters()
"trainable params: 667,493 || all params: 86,543,818 || trainable%: 0.7712775047664294"
```

### 训练

对于训练，让我们使用 Transformers 中的[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类。`Trainer`包含一个 PyTorch 训练循环，当您准备好时，调用[train](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)开始训练。要自定义训练运行，请在[TrainingArguments](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)类中配置训练超参数。使用类似 LoRA 的方法，您可以承受更高的批量大小和学习率。

AdaLoRA 有一个 update_and_allocate()方法，应在每个训练步骤中调用以更新参数预算和掩码，否则将不执行适应步骤。这需要编写一个自定义训练循环或对[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)进行子类化以包含此方法。例如，请查看这个[自定义训练循环](https://github.com/huggingface/peft/blob/912ad41e96e03652cabf47522cd876076f7a0c4f/examples/conditional_generation/peft_adalora_seq2seq.py#L120)。

```py
from transformers import TrainingArguments, Trainer

account = "stevhliu"
peft_model_id = f"{account}/google/vit-base-patch16-224-in21k-lora"
batch_size = 128

args = TrainingArguments(
    peft_model_id,
    remove_unused_columns=False,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=5e-3,
    per_device_train_batch_size=batch_size,
    gradient_accumulation_steps=4,
    per_device_eval_batch_size=batch_size,
    fp16=True,
    num_train_epochs=5,
    logging_steps=10,
    load_best_model_at_end=True,
    label_names=["labels"],
)
```

开始使用[train](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)进行训练。

```py
trainer = Trainer(
    model,
    args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    tokenizer=image_processor,
    data_collator=collate_fn,
)
trainer.train()
```

## 分享您的模型

训练完成后，您可以使用[push_to_hub](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)方法将模型上传到 Hub。您需要先登录到您的 Hugging Face 帐户，并在提示时输入您的令牌。

```py
from huggingface_hub import notebook_login

notebook_login()
```

调用[push_to_hub](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)将您的模型保存到您的存储库中。

```py
model.push_to_hub(peft_model_id)
```

## 推断

让我们从 Hub 加载模型并在食物图像上进行测试。

```py
from peft import PeftConfig, PeftModel
from transfomers import AutoImageProcessor
from PIL import Image
import requests

config = PeftConfig.from_pretrained("stevhliu/vit-base-patch16-224-in21k-lora")
model = AutoModelForImageClassification.from_pretrained(
    config.base_model_name_or_path,
    label2id=label2id,
    id2label=id2label,
    ignore_mismatched_sizes=True,
)
model = PeftModel.from_pretrained(model, "stevhliu/vit-base-patch16-224-in21k-lora")

url = "https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/beignets.jpeg"
image = Image.open(requests.get(url, stream=True).raw)
image
```

![](img/e7384973c0a1457da0f0be34f4e94f38.png)

将图像转换为 RGB 并返回底层的 PyTorch 张量。

```py
encoding = image_processor(image.convert("RGB"), return_tensors="pt")
```

现在运行模型并返回预测的类别！

```py
with torch.no_grad():
    outputs = model(**encoding)
    logits = outputs.logits

predicted_class_idx = logits.argmax(-1).item()
print("Predicted class:", model.config.id2label[predicted_class_idx])
"Predicted class: beignets"
```
