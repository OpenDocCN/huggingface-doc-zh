["```py\npip install transformers datasets evaluate sacrebleu\n```", "```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```", "```py\n>>> from datasets import load_dataset\n\n>>> books = load_dataset(\"opus_books\", \"en-fr\")\n```", "```py\n>>> books = books[\"train\"].train_test_split(test_size=0.2)\n```", "```py\n>>> books[\"train\"][0]\n{'id': '90560',\n 'translation': {'en': 'But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.',\n  'fr': 'Mais ce plateau \u00e9lev\u00e9 ne mesurait que quelques toises, et bient\u00f4t nous f\u00fbmes rentr\u00e9s dans notre \u00e9l\u00e9ment.'}}\n```", "```py\n>>> from transformers import AutoTokenizer\n\n>>> checkpoint = \"t5-small\"\n>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n```", "```py\n>>> source_lang = \"en\"\n>>> target_lang = \"fr\"\n>>> prefix = \"translate English to French: \"\n\n>>> def preprocess_function(examples):\n...     inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n...     targets = [example[target_lang] for example in examples[\"translation\"]]\n...     model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n...     return model_inputs\n```", "```py\n>>> tokenized_books = books.map(preprocess_function, batched=True)\n```", "```py\n>>> from transformers import DataCollatorForSeq2Seq\n\n>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n```", "```py\n>>> from transformers import DataCollatorForSeq2Seq\n\n>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")\n```", "```py\n>>> import evaluate\n\n>>> metric = evaluate.load(\"sacrebleu\")\n```", "```py\n>>> import numpy as np\n\n>>> def postprocess_text(preds, labels):\n...     preds = [pred.strip() for pred in preds]\n...     labels = [[label.strip()] for label in labels]\n\n...     return preds, labels\n\n>>> def compute_metrics(eval_preds):\n...     preds, labels = eval_preds\n...     if isinstance(preds, tuple):\n...         preds = preds[0]\n...     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n...     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n...     result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n...     result = {\"bleu\": result[\"score\"]}\n\n...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n...     result[\"gen_len\"] = np.mean(prediction_lens)\n...     result = {k: round(v, 4) for k, v in result.items()}\n...     return result\n```", "```py\n>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n```", "```py\n>>> training_args = Seq2SeqTrainingArguments(\n...     output_dir=\"my_awesome_opus_books_model\",\n...     evaluation_strategy=\"epoch\",\n...     learning_rate=2e-5,\n...     per_device_train_batch_size=16,\n...     per_device_eval_batch_size=16,\n...     weight_decay=0.01,\n...     save_total_limit=3,\n...     num_train_epochs=2,\n...     predict_with_generate=True,\n...     fp16=True,\n...     push_to_hub=True,\n... )\n\n>>> trainer = Seq2SeqTrainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=tokenized_books[\"train\"],\n...     eval_dataset=tokenized_books[\"test\"],\n...     tokenizer=tokenizer,\n...     data_collator=data_collator,\n...     compute_metrics=compute_metrics,\n... )\n\n>>> trainer.train()\n```", "```py\n>>> trainer.push_to_hub()\n```", "```py\n>>> from transformers import AdamWeightDecay\n\n>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n```", "```py\n>>> from transformers import TFAutoModelForSeq2SeqLM\n\n>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n```", "```py\n>>> tf_train_set = model.prepare_tf_dataset(\n...     tokenized_books[\"train\"],\n...     shuffle=True,\n...     batch_size=16,\n...     collate_fn=data_collator,\n... )\n\n>>> tf_test_set = model.prepare_tf_dataset(\n...     tokenized_books[\"test\"],\n...     shuffle=False,\n...     batch_size=16,\n...     collate_fn=data_collator,\n... )\n```", "```py\n>>> import tensorflow as tf\n\n>>> model.compile(optimizer=optimizer)  # No loss argument!\n```", "```py\n>>> from transformers.keras_callbacks import KerasMetricCallback\n\n>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n```", "```py\n>>> from transformers.keras_callbacks import PushToHubCallback\n\n>>> push_to_hub_callback = PushToHubCallback(\n...     output_dir=\"my_awesome_opus_books_model\",\n...     tokenizer=tokenizer,\n... )\n```", "```py\n>>> callbacks = [metric_callback, push_to_hub_callback]\n```", "```py\n>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)\n```", "```py\n>>> text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\"\n```", "```py\n>>> from transformers import pipeline\n\n>>> translator = pipeline(\"translation\", model=\"my_awesome_opus_books_model\")\n>>> translator(text)\n[{'translation_text': 'Legumes partagent des ressources avec des bact\u00e9ries azotantes.'}]\n```", "```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_opus_books_model\")\n>>> inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n```", "```py\n>>> from transformers import AutoModelForSeq2SeqLM\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"my_awesome_opus_books_model\")\n>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n```", "```py\n>>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n'Les lign\u00e9es partagent des ressources avec des bact\u00e9ries enfixant l'azote.'\n```", "```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_opus_books_model\")\n>>> inputs = tokenizer(text, return_tensors=\"tf\").input_ids\n```", "```py\n>>> from transformers import TFAutoModelForSeq2SeqLM\n\n>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"my_awesome_opus_books_model\")\n>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n```", "```py\n>>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n'Les lugumes partagent les ressources avec des bact\u00e9ries fixatrices d'azote.'\n```"]