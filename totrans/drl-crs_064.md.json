["```py\n%%capture\n!apt install python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip install pyvirtualdisplay\n!pip install pyglet==1.5.1\n```", "```py\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```", "```py\n!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt\n```", "```py\nimport numpy as np\n\nfrom collections import deque\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n# Gym\nimport gym\nimport gym_pygame\n\n# Hugging Face Hub\nfrom huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\nimport imageio\n```", "```py\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n```", "```py\nprint(device)\n```", "```py\nenv_id = \"CartPole-v1\"\n# Create the env\nenv = gym.make(env_id)\n\n# Create the evaluation env\neval_env = gym.make(env_id)\n\n# Get the state space and action space\ns_size = env.observation_space.shape[0]\na_size = env.action_space.n\n```", "```py\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample())  # Get a random observation\n```", "```py\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample())  # Take a random action\n```", "```py\nclass Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        # Create two fully connected layers\n\n    def forward(self, x):\n        # Define the forward pass\n        # state goes to fc1 then we apply ReLU activation function\n\n        # fc1 outputs goes to fc2\n\n        # We output the softmax\n\n    def act(self, state):\n        \"\"\"\n        Given a state, take action\n        \"\"\"\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = np.argmax(m)\n        return action.item(), m.log_prob(action)\n```", "```py\nclass Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)\n\n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = np.argmax(m)\n        return action.item(), m.log_prob(action)\n```", "```py\ndebug_policy = Policy(s_size, a_size, 64).to(device)\ndebug_policy.act(env.reset())\n```", "```py\nclass Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)\n\n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)\n```", "```py\ndef reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n    # Help us to calculate the score during the training\n    scores_deque = deque(maxlen=100)\n    scores = []\n    # Line 3 of pseudocode\n    for i_episode in range(1, n_training_episodes+1):\n        saved_log_probs = []\n        rewards = []\n        state = # TODO: reset the environment\n        # Line 4 of pseudocode\n        for t in range(max_t):\n            action, log_prob = # TODO get the action\n            saved_log_probs.append(log_prob)\n            state, reward, done, _ = # TODO: take an env step\n            rewards.append(reward)\n            if done:\n                break\n        scores_deque.append(sum(rewards))\n        scores.append(sum(rewards))\n\n        # Line 6 of pseudocode: calculate the return\n        returns = deque(maxlen=max_t)\n        n_steps = len(rewards)\n        # Compute the discounted returns at each timestep,\n        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n\n        # In O(N) time, where N is the number of time steps\n        # (this definition of the discounted return G_t follows the definition of this quantity\n        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n        # G_t = r_(t+1) + r_(t+2) + ...\n\n        # Given this formulation, the returns at each timestep t can be computed\n        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n        # G_t = r_(t+1) + gamma*G_(t+1)\n        # G_(t-1) = r_t + gamma* G_t\n        # (this follows a dynamic programming approach, with which we memorize solutions in order\n        # to avoid computing them multiple times)\n\n        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n\n        ## Given the above, we calculate the returns at timestep t as:\n        #               gamma[t] * return[t] + reward[t]\n        #\n        ## We compute this starting from the last timestep to the first, in order\n        ## to employ the formula presented above and avoid redundant computations that would be needed\n        ## if we were to do it from first to last.\n\n        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n        ## a normal python list would instead require O(N) to do this.\n        for t in range(n_steps)[::-1]:\n            disc_return_t = (returns[0] if len(returns)>0 else 0)\n            returns.appendleft(    ) # TODO: complete here\n\n        ## standardization of the returns is employed to make training more stable\n        eps = np.finfo(np.float32).eps.item()\n\n        ## eps is the smallest representable float, which is\n        # added to the standard deviation of the returns to avoid numerical instabilities\n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + eps)\n\n        # Line 7:\n        policy_loss = []\n        for log_prob, disc_return in zip(saved_log_probs, returns):\n            policy_loss.append(-log_prob * disc_return)\n        policy_loss = torch.cat(policy_loss).sum()\n\n        # Line 8: PyTorch prefers gradient descent\n        optimizer.zero_grad()\n        policy_loss.backward()\n        optimizer.step()\n\n        if i_episode % print_every == 0:\n            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n\n    return scores\n```", "```py\ndef reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n    # Help us to calculate the score during the training\n    scores_deque = deque(maxlen=100)\n    scores = []\n    # Line 3 of pseudocode\n    for i_episode in range(1, n_training_episodes + 1):\n        saved_log_probs = []\n        rewards = []\n        state = env.reset()\n        # Line 4 of pseudocode\n        for t in range(max_t):\n            action, log_prob = policy.act(state)\n            saved_log_probs.append(log_prob)\n            state, reward, done, _ = env.step(action)\n            rewards.append(reward)\n            if done:\n                break\n        scores_deque.append(sum(rewards))\n        scores.append(sum(rewards))\n\n        # Line 6 of pseudocode: calculate the return\n        returns = deque(maxlen=max_t)\n        n_steps = len(rewards)\n        # Compute the discounted returns at each timestep,\n        # as\n        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n        #\n        # In O(N) time, where N is the number of time steps\n        # (this definition of the discounted return G_t follows the definition of this quantity\n        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n        # G_t = r_(t+1) + r_(t+2) + ...\n\n        # Given this formulation, the returns at each timestep t can be computed\n        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n        # G_t = r_(t+1) + gamma*G_(t+1)\n        # G_(t-1) = r_t + gamma* G_t\n        # (this follows a dynamic programming approach, with which we memorize solutions in order\n        # to avoid computing them multiple times)\n\n        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n\n        ## Given the above, we calculate the returns at timestep t as:\n        #               gamma[t] * return[t] + reward[t]\n        #\n        ## We compute this starting from the last timestep to the first, in order\n        ## to employ the formula presented above and avoid redundant computations that would be needed\n        ## if we were to do it from first to last.\n\n        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n        ## a normal python list would instead require O(N) to do this.\n        for t in range(n_steps)[::-1]:\n            disc_return_t = returns[0] if len(returns) > 0 else 0\n            returns.appendleft(gamma * disc_return_t + rewards[t])\n\n        ## standardization of the returns is employed to make training more stable\n        eps = np.finfo(np.float32).eps.item()\n        ## eps is the smallest representable float, which is\n        # added to the standard deviation of the returns to avoid numerical instabilities\n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + eps)\n\n        # Line 7:\n        policy_loss = []\n        for log_prob, disc_return in zip(saved_log_probs, returns):\n            policy_loss.append(-log_prob * disc_return)\n        policy_loss = torch.cat(policy_loss).sum()\n\n        # Line 8: PyTorch prefers gradient descent\n        optimizer.zero_grad()\n        policy_loss.backward()\n        optimizer.step()\n\n        if i_episode % print_every == 0:\n            print(\"Episode {}\\tAverage Score: {:.2f}\".format(i_episode, np.mean(scores_deque)))\n\n    return scores\n```", "```py\ncartpole_hyperparameters = {\n    \"h_size\": 16,\n    \"n_training_episodes\": 1000,\n    \"n_evaluation_episodes\": 10,\n    \"max_t\": 1000,\n    \"gamma\": 1.0,\n    \"lr\": 1e-2,\n    \"env_id\": env_id,\n    \"state_space\": s_size,\n    \"action_space\": a_size,\n}\n```", "```py\n# Create policy and place it to the device\ncartpole_policy = Policy(\n    cartpole_hyperparameters[\"state_space\"],\n    cartpole_hyperparameters[\"action_space\"],\n    cartpole_hyperparameters[\"h_size\"],\n).to(device)\ncartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])\n```", "```py\nscores = reinforce(\n    cartpole_policy,\n    cartpole_optimizer,\n    cartpole_hyperparameters[\"n_training_episodes\"],\n    cartpole_hyperparameters[\"max_t\"],\n    cartpole_hyperparameters[\"gamma\"],\n    100,\n)\n```", "```py\ndef evaluate_agent(env, max_steps, n_eval_episodes, policy):\n    \"\"\"\n    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n    :param env: The evaluation environment\n    :param n_eval_episodes: Number of episode to evaluate the agent\n    :param policy: The Reinforce agent\n    \"\"\"\n    episode_rewards = []\n    for episode in range(n_eval_episodes):\n        state = env.reset()\n        step = 0\n        done = False\n        total_rewards_ep = 0\n\n        for step in range(max_steps):\n            action, _ = policy.act(state)\n            new_state, reward, done, info = env.step(action)\n            total_rewards_ep += reward\n\n            if done:\n                break\n            state = new_state\n        episode_rewards.append(total_rewards_ep)\n    mean_reward = np.mean(episode_rewards)\n    std_reward = np.std(episode_rewards)\n\n    return mean_reward, std_reward\n```", "```py\nevaluate_agent(\n    eval_env, cartpole_hyperparameters[\"max_t\"], cartpole_hyperparameters[\"n_evaluation_episodes\"], cartpole_policy\n)\n```", "```py\nfrom huggingface_hub import HfApi, snapshot_download\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport json\nimport imageio\n\nimport tempfile\n\nimport os\n```", "```py\ndef record_video(env, policy, out_directory, fps=30):\n    \"\"\"\n    Generate a replay video of the agent\n    :param env\n    :param Qtable: Qtable of our agent\n    :param out_directory\n    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n    \"\"\"\n    images = []\n    done = False\n    state = env.reset()\n    img = env.render(mode=\"rgb_array\")\n    images.append(img)\n    while not done:\n        # Take the action (index) that have the maximum expected future reward given that state\n        action, _ = policy.act(state)\n        state, reward, done, info = env.step(action)  # We directly put next_state = state for recording logic\n        img = env.render(mode=\"rgb_array\")\n        images.append(img)\n    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n```", "```py\ndef push_to_hub(repo_id,\n                model,\n                hyperparameters,\n                eval_env,\n                video_fps=30 ):\n  \"\"\"\n  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n  This method does the complete pipeline:\n  - It evaluates the model\n  - It generates the model card\n  - It generates a replay video of the agent\n  - It pushes everything to the Hub\n\n  :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n  :param model: the pytorch model we want to save\n  :param hyperparameters: training hyperparameters\n  :param eval_env: evaluation environment\n  :param video_fps: how many frame per seconds to record our video replay\n  \"\"\"\n\n  _, repo_name = repo_id.split(\"/\")\n  api = HfApi()\n\n  # Step 1: Create the repo\n  repo_url = api.create_repo(\n        repo_id=repo_id,\n        exist_ok=True,\n  )\n\n  with tempfile.TemporaryDirectory() as tmpdirname:\n    local_directory = Path(tmpdirname)\n\n    # Step 2: Save the model\n    torch.save(model, local_directory / \"model.pt\")\n\n    # Step 3: Save the hyperparameters to JSON\n    with open(local_directory / \"hyperparameters.json\", \"w\") as outfile:\n      json.dump(hyperparameters, outfile)\n\n    # Step 4: Evaluate the model and build JSON\n    mean_reward, std_reward = evaluate_agent(eval_env,\n                                            hyperparameters[\"max_t\"],\n                                            hyperparameters[\"n_evaluation_episodes\"],\n                                            model)\n    # Get datetime\n    eval_datetime = datetime.datetime.now()\n    eval_form_datetime = eval_datetime.isoformat()\n\n    evaluate_data = {\n          \"env_id\": hyperparameters[\"env_id\"],\n          \"mean_reward\": mean_reward,\n          \"n_evaluation_episodes\": hyperparameters[\"n_evaluation_episodes\"],\n          \"eval_datetime\": eval_form_datetime,\n    }\n\n    # Write a JSON file\n    with open(local_directory / \"results.json\", \"w\") as outfile:\n        json.dump(evaluate_data, outfile)\n\n    # Step 5: Create the model card\n    env_name = hyperparameters[\"env_id\"]\n\n    metadata = {}\n    metadata[\"tags\"] = [\n          env_name,\n          \"reinforce\",\n          \"reinforcement-learning\",\n          \"custom-implementation\",\n          \"deep-rl-class\"\n      ]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=repo_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_name,\n        dataset_id=env_name,\n      )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    model_card = f\"\"\"\n  # **Reinforce** Agent playing **{env_id}**\n  This is a trained model of a **Reinforce** agent playing **{env_id}** .\n  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction\n  \"\"\"\n\n    readme_path = local_directory / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n          readme = f.read()\n    else:\n      readme = model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n      f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\n    # Step 6: Record a video\n    video_path =  local_directory / \"replay.mp4\"\n    record_video(env, model, video_path, video_fps)\n\n    # Step 7\\. Push everything to the Hub\n    api.upload_folder(\n          repo_id=repo_id,\n          folder_path=local_directory,\n          path_in_repo=\".\",\n    )\n\n    print(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n```", "```py\nnotebook_login()\n```", "```py\nrepo_id = \"\"  # TODO Define your repo id {username/Reinforce-{model-id}}\npush_to_hub(\n    repo_id,\n    cartpole_policy,  # The model we want to save\n    cartpole_hyperparameters,  # Hyperparameters\n    eval_env,  # Evaluation environment\n    video_fps=30\n)\n```", "```py\nenv_id = \"Pixelcopter-PLE-v0\"\nenv = gym.make(env_id)\neval_env = gym.make(env_id)\ns_size = env.observation_space.shape[0]\na_size = env.action_space.n\n```", "```py\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample())  # Get a random observation\n```", "```py\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample())  # Take a random action\n```", "```py\nclass Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        # Define the three layers here\n\n    def forward(self, x):\n        # Define the forward process here\n        return F.softmax(x, dim=1)\n\n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)\n```", "```py\nclass Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, h_size * 2)\n        self.fc3 = nn.Linear(h_size * 2, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return F.softmax(x, dim=1)\n\n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)\n```", "```py\npixelcopter_hyperparameters = {\n    \"h_size\": 64,\n    \"n_training_episodes\": 50000,\n    \"n_evaluation_episodes\": 10,\n    \"max_t\": 10000,\n    \"gamma\": 0.99,\n    \"lr\": 1e-4,\n    \"env_id\": env_id,\n    \"state_space\": s_size,\n    \"action_space\": a_size,\n}\n```", "```py\n# Create policy and place it to the device\n# torch.manual_seed(50)\npixelcopter_policy = Policy(\n    pixelcopter_hyperparameters[\"state_space\"],\n    pixelcopter_hyperparameters[\"action_space\"],\n    pixelcopter_hyperparameters[\"h_size\"],\n).to(device)\npixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters[\"lr\"])\n```", "```py\nscores = reinforce(\n    pixelcopter_policy,\n    pixelcopter_optimizer,\n    pixelcopter_hyperparameters[\"n_training_episodes\"],\n    pixelcopter_hyperparameters[\"max_t\"],\n    pixelcopter_hyperparameters[\"gamma\"],\n    1000,\n)\n```", "```py\nrepo_id = \"\"  # TODO Define your repo id {username/Reinforce-{model-id}}\npush_to_hub(\n    repo_id,\n    pixelcopter_policy,  # The model we want to save\n    pixelcopter_hyperparameters,  # Hyperparameters\n    eval_env,  # Evaluation environment\n    video_fps=30\n)\n```"]