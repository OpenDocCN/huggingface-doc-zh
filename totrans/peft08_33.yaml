- en: Tuners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/peft/package_reference/tuners](https://huggingface.co/docs/peft/package_reference/tuners)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: A tuner (or adapter) is a module that can be plugged into a `torch.nn.Module`.
    `BaseTuner` base class for other tuners and provides shared methods and attributes
    for preparing an adapter configuration and replacing a target module with the
    adapter module. `BaseTunerLayer` is a base class for adapter layers. It offers
    methods and attributes for managing adapters such as activating and disabling
    adapters.
  prefs: []
  type: TYPE_NORMAL
- en: BaseTuner
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.tuners.tuners_utils.BaseTuner`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L91)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model to which the adapter tuner layers will
    be attached.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forward` (`Callable`) — The forward method of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` (`Union[`PeftConfig`, dict[str, PeftConfig]]`) — The adapter
    configuration object, it should be a dictionary of `str` to `PeftConfig` objects.
    One can also pass a PeftConfig object and a new adapter will be created with the
    default name `adapter` or create a new dictionary with a key `adapter_name` and
    a value of that peft config.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config` (`dict[str, Any]`) — The model configuration object, it should be
    a dictionary of `str` to `Any` objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`targeted_module_names` (`list[str]`) — The list of module names that were
    actually adapted. Can be useful to inspect if you want to quickly double-check
    that the `config.target_modules` where specified correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A base tuner model that provides the common methods and attributes for all tuners
    that are injectable into a torch.nn.Module
  prefs: []
  type: TYPE_NORMAL
- en: 'For adding a new Tuner class, one needs to overwrite the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`_prepare_adapter_config`: A private method to eventually prepare the adapter
    config, for example in case the field `target_modules` is missing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_create_and_replace`: A private method to create and replace the target module
    with the adapter module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_check_target_module_exists`: A private helper method to check if the passed
    module’s key name matches any of the target modules in the adapter_config.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The easiest is to check what is done in the `peft.tuners.lora.LoraModel` class.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `inject_adapter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L245)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`nn.Module`) — The model to be tuned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`) — The adapter name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates adapter layers and replaces the target modules with the adapter layers.
    This method is called under the hood by `peft.mapping.get_peft_model` if a non-prompt
    tuning adapter class is passed.
  prefs: []
  type: TYPE_NORMAL
- en: The corresponding PEFT config is directly retrieved from the `peft_config` attribute
    of the BaseTuner class.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `merge_adapter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L323)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`safe_merge` (`bool`, *optional*) — If `True`, the merge operation will be
    performed in a copy of the original weights and check for NaNs before merging
    the weights. This is useful if you want to check if the merge operation will produce
    NaNs. Defaults to `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_names` (`list[str]`, *optional*) — The list of adapter names that
    should be merged. If `None`, all active adapters will be merged. Defaults to `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method merges the adapter layers into the base model.
  prefs: []
  type: TYPE_NORMAL
- en: Merging adapters can lead to a speed up of the forward pass. A copy of the adapter
    weights is still kept in memory, which is required to unmerge the adapters. In
    order to merge the adapter weights without keeping them in memory, please call
    `merge_and_unload`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `unmerge_adapter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L345)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This method unmerges all merged adapter layers from the base model.
  prefs: []
  type: TYPE_NORMAL
- en: BaseTunerLayer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.tuners.tuners_utils.BaseTunerLayer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L363)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`is_pluggable` (`bool`, *optional*) — Whether the adapter layer can be plugged
    to any pytorch module'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`active_adapters` (Union[List`str`, `str`], *optional*) — The name of the active
    adapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A tuner layer mixin that provides the common methods and attributes for all
    tuners.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `delete_adapter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L505)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`) — The name of the adapter to delete'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete an adapter from the layer
  prefs: []
  type: TYPE_NORMAL
- en: This should be called on all adapter layers, or else we will get an inconsistent
    state.
  prefs: []
  type: TYPE_NORMAL
- en: This method will also set a new active adapter if the deleted adapter was an
    active adapter. It is important that the new adapter is chosen in a deterministic
    way, so that the same adapter is chosen on all layers.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_adapters`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L445)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`enabled` (bool) — True to enable adapters, False to disable adapters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Toggle the enabling and disabling of adapters
  prefs: []
  type: TYPE_NORMAL
- en: Takes care of setting the requires_grad flag for the adapter weights.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_base_layer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L390)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: (Recursively) get the base_layer.
  prefs: []
  type: TYPE_NORMAL
- en: This is necessary for the case that the tuner layer wraps another tuner layer.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_adapter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L463)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`adapter_name` (`str` or `List[str]`) — Name of the adapter(s) to be activated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the active adapter(s).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, this function will set the specified adapters to trainable (i.e.,
    requires_grad=True). If this is not desired, use the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
