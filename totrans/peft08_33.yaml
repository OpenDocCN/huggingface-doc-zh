- en: Tuners
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/peft/package_reference/tuners](https://huggingface.co/docs/peft/package_reference/tuners)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: A tuner (or adapter) is a module that can be plugged into a `torch.nn.Module`.
    `BaseTuner` base class for other tuners and provides shared methods and attributes
    for preparing an adapter configuration and replacing a target module with the
    adapter module. `BaseTunerLayer` is a base class for adapter layers. It offers
    methods and attributes for managing adapters such as activating and disabling
    adapters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: BaseTuner
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.tuners.tuners_utils.BaseTuner`'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L91)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model to which the adapter tuner layers will
    be attached.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forward` (`Callable`) — The forward method of the model.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` (`Union[`PeftConfig`, dict[str, PeftConfig]]`) — The adapter
    configuration object, it should be a dictionary of `str` to `PeftConfig` objects.
    One can also pass a PeftConfig object and a new adapter will be created with the
    default name `adapter` or create a new dictionary with a key `adapter_name` and
    a value of that peft config.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config` (`dict[str, Any]`) — The model configuration object, it should be
    a dictionary of `str` to `Any` objects.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`targeted_module_names` (`list[str]`) — The list of module names that were
    actually adapted. Can be useful to inspect if you want to quickly double-check
    that the `config.target_modules` where specified correctly.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A base tuner model that provides the common methods and attributes for all tuners
    that are injectable into a torch.nn.Module
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'For adding a new Tuner class, one needs to overwrite the following methods:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '`_prepare_adapter_config`: A private method to eventually prepare the adapter
    config, for example in case the field `target_modules` is missing.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_create_and_replace`: A private method to create and replace the target module
    with the adapter module.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_check_target_module_exists`: A private helper method to check if the passed
    module’s key name matches any of the target modules in the adapter_config.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The easiest is to check what is done in the `peft.tuners.lora.LoraModel` class.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '#### `inject_adapter`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L245)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`nn.Module`) — The model to be tuned.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`) — The adapter name.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates adapter layers and replaces the target modules with the adapter layers.
    This method is called under the hood by `peft.mapping.get_peft_model` if a non-prompt
    tuning adapter class is passed.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The corresponding PEFT config is directly retrieved from the `peft_config` attribute
    of the BaseTuner class.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '#### `merge_adapter`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L323)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '`safe_merge` (`bool`, *optional*) — If `True`, the merge operation will be
    performed in a copy of the original weights and check for NaNs before merging
    the weights. This is useful if you want to check if the merge operation will produce
    NaNs. Defaults to `False`.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_names` (`list[str]`, *optional*) — The list of adapter names that
    should be merged. If `None`, all active adapters will be merged. Defaults to `None`.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method merges the adapter layers into the base model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Merging adapters can lead to a speed up of the forward pass. A copy of the adapter
    weights is still kept in memory, which is required to unmerge the adapters. In
    order to merge the adapter weights without keeping them in memory, please call
    `merge_and_unload`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '#### `unmerge_adapter`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L345)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This method unmerges all merged adapter layers from the base model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: BaseTunerLayer
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.tuners.tuners_utils.BaseTunerLayer`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L363)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '`is_pluggable` (`bool`, *optional*) — Whether the adapter layer can be plugged
    to any pytorch module'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_pluggable`（`bool`，*可选*) — 适配器层是否可以插入到任何pytorch模块中'
- en: '`active_adapters` (Union[List`str`, `str`], *optional*) — The name of the active
    adapter.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`active_adapters`（Union[List`str`，`str`], *可选*) — 活动适配器的名称。'
- en: A tuner layer mixin that provides the common methods and attributes for all
    tuners.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一个调谐器层mixin，为所有调谐器提供共同的方法和属性。
- en: '#### `delete_adapter`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `delete_adapter`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L505)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L505)'
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`adapter_name` (`str`) — The name of the adapter to delete'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name`（`str`） — 要删除的适配器的名称'
- en: Delete an adapter from the layer
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从层中删除一个适配器
- en: This should be called on all adapter layers, or else we will get an inconsistent
    state.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该在所有适配器层上调用，否则我们将得到一个不一致的状态。
- en: This method will also set a new active adapter if the deleted adapter was an
    active adapter. It is important that the new adapter is chosen in a deterministic
    way, so that the same adapter is chosen on all layers.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果删除的适配器是活动适配器，此方法还将设置一个新的活动适配器。重要的是选择新的适配器是以确定性方式进行的，以便在所有层上选择相同的适配器。
- en: '#### `enable_adapters`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_adapters`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L445)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L445)'
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`enabled` (bool) — True to enable adapters, False to disable adapters'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enabled`（bool） — True表示启用适配器，False表示禁用适配器'
- en: Toggle the enabling and disabling of adapters
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 切换适配器的启用和禁用
- en: Takes care of setting the requires_grad flag for the adapter weights.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 负责设置适配器权重的requires_grad标志。
- en: '#### `get_base_layer`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_base_layer`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L390)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L390)'
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: (Recursively) get the base_layer.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: （递归地）获取基础层。
- en: This is necessary for the case that the tuner layer wraps another tuner layer.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于调谐器层包装另一个调谐器层的情况是必要的。
- en: '#### `set_adapter`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_adapter`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L463)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L463)'
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`adapter_name` (`str` or `List[str]`) — Name of the adapter(s) to be activated.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name`（`str`或`List[str]`） — 要激活的适配器的名称。'
- en: Set the active adapter(s).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 设置活动适配器。
- en: Additionally, this function will set the specified adapters to trainable (i.e.,
    requires_grad=True). If this is not desired, use the following code.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，此函数将设置指定的适配器为可训练（即requires_grad=True）。如果不需要此功能，请使用以下代码。
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
