# LoRA

> 原始文本：[https://huggingface.co/docs/peft/package_reference/lora](https://huggingface.co/docs/peft/package_reference/lora)

低秩适应（[LoRA](https://huggingface.co/papers/2309.15223)）是一种PEFT方法，它在注意力层中将一个大矩阵分解为两个较小的低秩矩阵。这极大地减少了需要进行微调的参数数量。

论文摘要如下：

*我们提出了一种基于低秩适应（LoRA）的神经语言建模系统，用于语音识别输出重评分。尽管像BERT这样的预训练语言模型（LMs）在第二遍重评分中表现出优越性能，但扩大预训练阶段的计算成本以及将预训练模型调整到特定领域的适应性限制了它们在重评分中的实际应用。在这里，我们提出了一种基于低秩分解的方法，用于训练一个重评分BERT模型，并且仅使用预训练参数的一小部分（0.08%）。这些插入的矩阵通过具有基于相关性的正则化损失的判别式训练目标进行优化。所提出的低秩适应Rescore-BERT（LoRB）架构在LibriSpeech和内部数据集上进行了评估，训练时间减少了5.4到3.6倍。*

## LoraConfig

### `class peft.LoraConfig`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/config.py#L43)

```py
( peft_type: Union = None auto_mapping: Optional = None base_model_name_or_path: Optional = None revision: Optional = None task_type: Union = None inference_mode: bool = False r: int = 8 target_modules: Optional[Union[list[str], str]] = None lora_alpha: int = 8 lora_dropout: float = 0.0 fan_in_fan_out: bool = False bias: Literal['none', 'all', 'lora_only'] = 'none' use_rslora: bool = False modules_to_save: Optional[list[str]] = None init_lora_weights: bool | Literal['gaussian', 'loftq'] = True layers_to_transform: Optional[Union[list[int], int]] = None layers_pattern: Optional[Union[list[str], str]] = None rank_pattern: Optional[dict] = <factory> alpha_pattern: Optional[dict] = <factory> megatron_config: Optional[dict] = None megatron_core: Optional[str] = 'megatron.core' loftq_config: Union[LoftQConfig, dict] = <factory> )
```

参数

+   `r` (`int`) — Lora注意力维度（“秩”）。

+   `target_modules` (`Optional[Union[List[str], str]]`) — 要应用适配器的模块的名称。如果指定了这个，那么只有指定名称的模块将被替换。当传递一个字符串时，将执行正则表达式匹配。当传递一个字符串列表时，要么执行精确匹配，要么检查模块的名称是否以任何传递的字符串结尾。如果指定为'all-linear'，则选择所有线性/Conv1D模块，不包括输出层。如果未指定，则根据模型架构选择模块。如果不知道架构，则会引发错误 — 在这种情况下，您应该手动指定目标模块。

+   `lora_alpha` (`int`) — Lora缩放的alpha参数。

+   `lora_dropout` (`float`) — Lora层的dropout概率。

+   `fan_in_fan_out` (`bool`) — 如果要替换的层存储类似于(fan_in, fan_out)的权重，请将此设置为True。例如，gpt-2使用存储类似于(fan_in, fan_out)的权重的`Conv1D`，因此应将其设置为`True`。

+   `bias` (`str`) — LoRA的偏置类型。可以是'none'、'all'或'lora_only'。如果是'all'或'lora_only'，则相应的偏置将在训练过程中更新。请注意，即使禁用适配器，模型也不会产生与没有适应的基础模型相同的输出。

+   `use_rslora` (`bool`) — 当设置为True时，使用[Rank-Stabilized LoRA](https://doi.org/10.48550/arXiv.2312.03732)，它将适配器缩放因子设置为`lora_alpha/math.sqrt(r)`，因为已经证明效果更好。否则，将使用`lora_alpha/r`的原始默认值。

+   `modules_to_save` (`List[str]`) — 除了适配器层之外要设置为可训练并保存在最终检查点中的模块列表。

+   `init_lora_weights` (`bool` | `Literal["gaussian", "loftq"]`) — 如何初始化适配器层的权重。传递True（默认值）会导致从Microsoft的参考实现中进行默认初始化。传递'gaussian'会导致按线性和层的LoRA秩缩放的高斯初始化。将初始化设置为False会导致完全随机初始化，不建议使用。传递'loftq'以使用LoftQ初始化。

+   `layers_to_transform` (`Union[List[int], int]`) — 要转换的层索引。如果传递了一个整数列表，它将应用于在此列表中指定的层索引上的适配器。如果传递了一个单个整数，它将在该索引处的层上应用转换。

+   `layers_pattern` (`str`) — 层模式名称，仅在`layers_to_transform`与`None`不同的情况下使用。

+   `rank_pattern` (`dict`) — 从层名称或正则表达式到与`r`指定的默认rank不同的rank的映射。

+   `alpha_pattern` (`dict`) — 从层名称或正则表达式到与`lora_alpha`指定的默认alpha不同的alpha的映射。

+   `megatron_config` (`Optional[dict]`) — 用于Megatron的TransformerConfig参数。用于创建LoRA的并行线性层。您可以像这样获取它，`core_transformer_config_from_args(get_args())`，这两个函数来自Megatron。这些参数将用于初始化Megatron的TransformerConfig。当您想要将LoRA应用于megatron的ColumnParallelLinear和RowParallelLinear层时，需要指定此参数。

+   `megatron_core` (`Optional[str]`) — 要使用的Megatron核心模块，默认为`"megatron.core"`。

+   `loftq_config` (`Optional[LoftQConfig]`) — LoftQ的配置。如果不为None，则LoftQ将用于量化骨干权重并初始化Lora层。还传递`init_lora_weights='loftq'`。请注意，在这种情况下，不应传递量化模型，因为LoftQ将量化模型本身。

这是一个配置类，用于存储[LoraModel](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel)的配置。

## LoraModel

### `class peft.LoraModel`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L47)

```py
( model config adapter_name ) → export const metadata = 'undefined';torch.nn.Module
```

参数

+   `model` (`torch.nn.Module`) — 要适配的模型。

+   `config` ([LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)) — Lora模型的配置。

+   `adapter_name` (`str`) — 适配器的名称，默认为`"default"`。

返回

`torch.nn.Module`

Lora模型。

从预训练的transformers模型创建低秩适配器（LoRA）模型。

该方法在[https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)中有详细描述。

示例：

```py
>>> from transformers import AutoModelForSeq2SeqLM
>>> from peft import LoraModel, LoraConfig

>>> config = LoraConfig(
...     task_type="SEQ_2_SEQ_LM",
...     r=8,
...     lora_alpha=32,
...     target_modules=["q", "v"],
...     lora_dropout=0.01,
... )

>>> model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")
>>> lora_model = LoraModel(model, config, "default")
```

```py
>>> import transformers
>>> from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_int8_training

>>> target_modules = ["q_proj", "k_proj", "v_proj", "out_proj", "fc_in", "fc_out", "wte"]
>>> config = LoraConfig(
...     r=4, lora_alpha=16, target_modules=target_modules, lora_dropout=0.1, bias="none", task_type="CAUSAL_LM"
... )

>>> model = transformers.GPTJForCausalLM.from_pretrained(
...     "kakaobrain/kogpt",
...     revision="KoGPT6B-ryan1.5b-float16",  # or float32 version: revision=KoGPT6B-ryan1.5b
...     pad_token_id=tokenizer.eos_token_id,
...     use_cache=False,
...     device_map={"": rank},
...     torch_dtype=torch.float16,
...     load_in_8bit=True,
... )
>>> model = prepare_model_for_int8_training(model)
>>> lora_model = get_peft_model(model, config)
```

**属性**：

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) — 要适配的模型。

+   `peft_config` ([LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig))：Lora模型的配置。

#### `add_weighted_adapter`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L369)

```py
( adapters weights adapter_name combination_type = 'svd' svd_rank = None svd_clamp = None svd_full_matrices = True svd_driver = None density = None majority_sign_method: Literal['total', 'frequency'] = 'total' )
```

参数

+   `adapters` (`list`) — 要合并的适配器名称列表。

+   `weights` (`list`) — 每个适配器的权重列表。

+   `adapter_name` (`str`) — 新适配器的名称。

+   `combination_type` (`str`) — 合并类型可以是[`svd`, `linear`, `cat`, `ties`, `ties_svd`, `dare_ties`, `dare_linear`, `dare_ties_svd`, `dare_linear_svd`]之一。当使用`cat`组合类型时，结果适配器的秩等于所有适配器秩的总和（混合适配器可能太大，导致OOM错误）。

+   `svd_rank` (`int`, *可选*) — svd的输出适配器的秩。如果未提供，则将使用合并适配器的最大秩。

+   `svd_clamp` (`float`, *可选*) — 用于夹紧SVD分解输出的分位阈值。如果未提供，则不执行夹紧。默认为None。

+   `svd_full_matrices` (`bool`, *可选*) — 控制是计算完整的还是简化的SVD，从而控制返回的张量U和Vh的形状。默认为True。

+   `svd_driver`（`str`，*可选*）- 要使用的cuSOLVER方法的名称。此关键字参数仅在CUDA上合并时有效。可以是[None, `gesvd`, `gesvdj`, `gesvda`]之一。有关更多信息，请参考`torch.linalg.svd`文档。默认为None。

+   `density`（`float`，*可选*）- 介于0和1之间的值。0表示所有值都被修剪，1表示没有值被修剪。应与[`ties`, `ties_svd`, `dare_ties`, `dare_linear`, `dare_ties_svd`, `dare_linear_svd`]一起使用

+   `majority_sign_method`（`str`）- 用于获取符号值大小的方法，应为[“total”, “frequency”]之一。应与[`ties`, `ties_svd`, `dare_ties`, `dare_ties_svd`]一起使用

此方法通过将给定的适配器与给定的权重合并来添加新的适配器。

当使用`cat`组合类型时，您应该意识到结果适配器的等级将等于所有适配器等级的总和。因此，混合适配器可能会变得过大，导致OOM错误。

#### `delete_adapter`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L640)

```py
( adapter_name: str )
```

参数

+   `adapter_name`（str）- 要删除的适配器的名称。

删除现有的适配器。

#### `disable_adapter_layers`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L292)

```py
( )
```

禁用所有适配器。

当禁用所有适配器时，模型输出对应于基础模型的输出。

#### `enable_adapter_layers`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L285)

```py
( )
```

启用所有适配器。

如果您之前已禁用了所有适配器并希望重新启用它们，请调用此方法。

#### `merge_and_unload`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L662)

```py
( progressbar: bool = False safe_merge: bool = False adapter_names: Optional[list[str]] = None )
```

参数

+   `progressbar`（`bool`）- 是否显示指示卸载和合并过程的进度条

+   `safe_merge`（`bool`）- 是否激活安全合并检查以检查适配器权重中是否存在潜在的NaN

+   `adapter_names`（`List[str]`，*可选*）- 应合并的适配器名称列表。如果为None，则将合并所有活动适配器。默认为`None`。

此方法将LoRa层合并到基础模型中。如果有人想要将基础模型用作独立模型，则需要这样做。

示例：

```py
>>> from transformers import AutoModelForCausalLM
>>> from peft import PeftModel

>>> base_model = AutoModelForCausalLM.from_pretrained("tiiuae/falcon-40b")
>>> peft_model_id = "smangrul/falcon-40B-int4-peft-lora-sfttrainer-sample"
>>> model = PeftModel.from_pretrained(base_model, peft_model_id)
>>> merged_model = model.merge_and_unload()
```

#### `set_adapter`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L307)

```py
( adapter_name: str | list[str] )
```

参数

+   `adapter_name`（`str`或`list[str]`）- 要激活的适配器的名称。

设置活动适配器。

此外，此函数将指定的适配器设置为可训练（即，requires_grad=True）。如果不需要此功能，请使用以下代码。

```py
>>> for name, param in model_peft.named_parameters():
...     if ...:  # some check on name (ex. if 'lora' in name)
...         param.requires_grad = False
```

#### `unload`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/lora/model.py#L694)

```py
( )
```

通过删除所有lora模块而获得基础模型。这将还原原始基础模型。
