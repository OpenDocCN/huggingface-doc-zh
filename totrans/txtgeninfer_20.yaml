- en: Flash Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention](https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/text-generation-inference/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/entry/start.96d64f85.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/scheduler.9680c161.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/singletons.5632daf5.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/index.9d57cde4.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/paths.5eca520f.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/entry/app.48a2a24c.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/index.38d74ee1.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/nodes/0.c01ff294.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/nodes/8.df1f4d9c.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/Heading.74c51a96.js">
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the transformer architecture is heavily bottlenecked by the self-attention
    mechanism, which has quadratic time and memory complexity. Recent developments
    in accelerator hardware mainly focus on enhancing compute capacities and not memory
    and transferring data between hardware. This results in attention operation having
    a memory bottleneck. **Flash Attention** is an attention algorithm used to reduce
    this problem and scale transformer-based models more efficiently, enabling faster
    training and inference.
  prefs: []
  type: TYPE_NORMAL
- en: Standard attention mechanism uses High Bandwidth Memory (HBM) to store, read
    and write keys, queries and values. HBM is large in memory, but slow in processing,
    meanwhile SRAM is smaller in memory, but faster in operations. In the standard
    attention implementation, the cost of loading and writing keys, queries, and values
    from HBM is high. It loads keys, queries, and values from HBM to GPU on-chip SRAM,
    performs a single step of the attention mechanism, writes it back to HBM, and
    repeats this for every single attention step. Instead, Flash Attention loads keys,
    queries, and values once, fuses the operations of the attention mechanism, and
    writes them back.
  prefs: []
  type: TYPE_NORMAL
- en: '![Flash Attention](../Images/8ca529e4366f4e68a965efc7a0a9ff8d.png)'
  prefs: []
  type: TYPE_IMG
- en: It is implemented for supported models. You can check out the complete list
    of models that support Flash Attention [here](https://github.com/huggingface/text-generation-inference/tree/main/server/text_generation_server/models),
    for models with flash prefix.
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about Flash Attention by reading the paper in this [link](https://arxiv.org/abs/2205.14135).
  prefs: []
  type: TYPE_NORMAL
