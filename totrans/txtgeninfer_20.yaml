- en: Flash Attention
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 闪存注意力
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention](https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention](https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the transformer architecture is heavily bottlenecked by the self-attention
    mechanism, which has quadratic time and memory complexity. Recent developments
    in accelerator hardware mainly focus on enhancing compute capacities and not memory
    and transferring data between hardware. This results in attention operation having
    a memory bottleneck. **Flash Attention** is an attention algorithm used to reduce
    this problem and scale transformer-based models more efficiently, enabling faster
    training and inference.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器架构的扩展受到自注意力机制的严重瓶颈限制，自注意力机制具有二次时间和内存复杂度。最近加速器硬件的发展主要集中在增强计算能力，而不是内存和硬件之间的数据传输。这导致注意力操作存在内存瓶颈。**Flash
    Attention**是一种注意力算法，用于减少这个问题，更有效地扩展基于变压器的模型，实现更快的训练和推断。
- en: Standard attention mechanism uses High Bandwidth Memory (HBM) to store, read
    and write keys, queries and values. HBM is large in memory, but slow in processing,
    meanwhile SRAM is smaller in memory, but faster in operations. In the standard
    attention implementation, the cost of loading and writing keys, queries, and values
    from HBM is high. It loads keys, queries, and values from HBM to GPU on-chip SRAM,
    performs a single step of the attention mechanism, writes it back to HBM, and
    repeats this for every single attention step. Instead, Flash Attention loads keys,
    queries, and values once, fuses the operations of the attention mechanism, and
    writes them back.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 标准注意力机制使用高带宽内存（HBM）来存储、读取和写入键、查询和数值。HBM在内存中很大，但在处理速度上较慢，与此同时，SRAM在内存中较小，但在操作速度上更快。在标准注意力实现中，从HBM加载和写入键、查询和数值的成本很高。它将键、查询和数值从HBM加载到GPU片上SRAM，执行一步注意力机制，将其写回到HBM，并为每个单独的注意力步骤重复此操作。相反，Flash
    Attention只加载一次键、查询和数值，融合注意力机制的操作，并将它们写回。
- en: '![Flash Attention](../Images/8ca529e4366f4e68a965efc7a0a9ff8d.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![Flash Attention](../Images/8ca529e4366f4e68a965efc7a0a9ff8d.png)'
- en: It is implemented for supported models. You can check out the complete list
    of models that support Flash Attention [here](https://github.com/huggingface/text-generation-inference/tree/main/server/text_generation_server/models),
    for models with flash prefix.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 它已经在支持的模型中实现。您可以在此处查看支持Flash Attention的完整模型列表，这些模型具有flash前缀。
- en: You can learn more about Flash Attention by reading the paper in this [link](https://arxiv.org/abs/2205.14135).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过阅读此链接中的论文了解更多关于Flash Attention的信息：[https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)
