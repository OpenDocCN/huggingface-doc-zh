- en: Supported models and hardware
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持的模型和硬件
- en: 'Original text: [https://huggingface.co/docs/text-embeddings-inference/supported_models](https://huggingface.co/docs/text-embeddings-inference/supported_models)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/text-embeddings-inference/supported_models](https://huggingface.co/docs/text-embeddings-inference/supported_models)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: We are continually expanding our support for other model types and plan to include
    them in future updates.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在不断扩大对其他模型类型的支持，并计划在未来更新中包括它们。
- en: Supported embeddings models
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持的嵌入模型
- en: Text Embeddings Inference currently supports BERT, CamemBERT, XLM-RoBERTa models
    with absolute positions and JinaBERT model with Alibi positions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 文本嵌入推理目前支持具有绝对位置的 BERT、CamemBERT、XLM-RoBERTa 模型以及具有 Alibi 位置的 JinaBERT 模型。
- en: 'Below are some examples of the currently supported models:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是当前支持的模型的一些示例：
- en: '| MTEB Rank | Model Type | Model ID |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| MTEB 排名 | 模型类型 | 模型 ID |'
- en: '| --- | --- | --- |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | Bert | [BAAI/bge-large-en-v1.5](https://hf.co/BAAI/bge-large-en-v1.5)
    |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| 1 | Bert | [BAAI/bge-large-en-v1.5](https://hf.co/BAAI/bge-large-en-v1.5)
    |'
- en: '| 2 |  | [BAAI/bge-base-en-v1.5](https://hf.co/BAAI/bge-base-en-v1.5) |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 2 |  | [BAAI/bge-base-en-v1.5](https://hf.co/BAAI/bge-base-en-v1.5) |'
- en: '| 3 |  | [llmrails/ember-v1](https://hf.co/llmrails/ember-v1) |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 3 |  | [llmrails/ember-v1](https://hf.co/llmrails/ember-v1) |'
- en: '| 4 |  | [thenlper/gte-large](https://hf.co/thenlper/gte-large) |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 4 |  | [thenlper/gte-large](https://hf.co/thenlper/gte-large) |'
- en: '| 5 |  | [thenlper/gte-base](https://hf.co/thenlper/gte-base) |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 5 |  | [thenlper/gte-base](https://hf.co/thenlper/gte-base) |'
- en: '| 6 |  | [intfloat/e5-large-v2](https://hf.co/intfloat/e5-large-v2) |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 6 |  | [intfloat/e5-large-v2](https://hf.co/intfloat/e5-large-v2) |'
- en: '| 7 |  | [BAAI/bge-small-en-v1.5](https://hf.co/BAAI/bge-small-en-v1.5) |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 7 |  | [BAAI/bge-small-en-v1.5](https://hf.co/BAAI/bge-small-en-v1.5) |'
- en: '| 10 |  | [intfloat/e5-base-v2](https://hf.co/intfloat/e5-base-v2) |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 10 |  | [intfloat/e5-base-v2](https://hf.co/intfloat/e5-base-v2) |'
- en: '| 11 | XLM-RoBERTa | [intfloat/multilingual-e5-large](https://hf.co/intfloat/multilingual-e5-large)
    |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 11 | XLM-RoBERTa | [intfloat/multilingual-e5-large](https://hf.co/intfloat/multilingual-e5-large)
    |'
- en: '| N/A | JinaBERT | [jinaai/jina-embeddings-v2-base-en](https://hf.co/jinaai/jina-embeddings-v2-base-en)
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| N/A | JinaBERT | [jinaai/jina-embeddings-v2-base-en](https://hf.co/jinaai/jina-embeddings-v2-base-en)
    |'
- en: '| N/A | JinaBERT | [jinaai/jina-embeddings-v2-small-en](https://hf.co/jinaai/jina-embeddings-v2-small-en)
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| N/A | JinaBERT | [jinaai/jina-embeddings-v2-small-en](https://hf.co/jinaai/jina-embeddings-v2-small-en)
    |'
- en: To explore the list of best performing text embeddings models, visit the [Massive
    Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要探索最佳表现的文本嵌入模型列表，请访问[大规模文本嵌入基准（MTEB）排行榜](https://huggingface.co/spaces/mteb/leaderboard)。
- en: Supported re-rankers and sequence classification models
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持的重新排序器和序列分类模型
- en: Text Embeddings Inference currently supports CamemBERT, and XLM-RoBERTa Sequence
    Classification models with absolute positions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 文本嵌入推理目前支持具有绝对位置的 CamemBERT 和 XLM-RoBERTa 序列分类模型。
- en: 'Below are some examples of the currently supported models:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是当前支持的模型的一些示例：
- en: '| Task | Model Type | Model ID | Revision |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 模型类型 | 模型 ID | 修订版本 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Re-Ranking | XLM-RoBERTa | [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)
    | `refs/pr/4` |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 重新排序 | XLM-RoBERTa | [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)
    | `refs/pr/4` |'
- en: '| Re-Ranking | XLM-RoBERTa | [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)
    | `refs/pr/5` |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 重新排序 | XLM-RoBERTa | [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)
    | `refs/pr/5` |'
- en: '| Sentiment Analysis | RoBERTa | [SamLowe/roberta-base-go_emotions](https://huggingface.co/SamLowe/roberta-base-go_emotions)
    |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | RoBERTa | [SamLowe/roberta-base-go_emotions](https://huggingface.co/SamLowe/roberta-base-go_emotions)
    |  |'
- en: Supported hardware
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持的硬件
- en: Text Embeddings Inference supports can be used on CPU, Turing (T4, RTX 2000
    series, …), Ampere 80 (A100, A30), Ampere 86 (A10, A40, …), Ada Lovelace (RTX
    4000 series, …), and Hopper (H100) architectures.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 文本嵌入推理支持可以在 CPU、图灵（T4、RTX 2000 系列，...）、安培 80（A100、A30）、安培 86（A10、A40，...）、艾达·洛维斯（RTX
    4000 系列，...）和霍珀（H100）架构上使用。
- en: The library does **not** support CUDA compute capabilities < 7.5, which means
    V100, Titan V, GTX 1000 series, etc. are not supported. To leverage your GPUs,
    make sure to install the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html),
    and use NVIDIA drivers with CUDA version 12.2 or higher.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该库**不**支持 CUDA 计算能力 < 7.5，这意味着 V100、Titan V、GTX 1000 系列等不受支持。为了利用您的 GPU，请确保安装[NVIDIA
    容器工具包](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)，并使用
    CUDA 版本为 12.2 或更高的 NVIDIA 驱动程序。
- en: 'Find the appropriate Docker image for your hardware in the following table:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下表格中找到适合您硬件的 Docker 镜像：
- en: '| Architecture | Image |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 镜像 |'
- en: '| --- | --- |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| CPU | ghcr.io/huggingface/text-embeddings-inference:cpu-0.6 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| CPU | ghcr.io/huggingface/text-embeddings-inference:cpu-0.6 |'
- en: '| Volta | NOT SUPPORTED |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| Volta | 不支持 |'
- en: '| Turing (T4, RTX 2000 series, …) | ghcr.io/huggingface/text-embeddings-inference:turing-0.6
    (experimental) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 图灵（T4、RTX 2000 系列，...） | ghcr.io/huggingface/text-embeddings-inference:turing-0.6（实验性）
    |'
- en: '| Ampere 80 (A100, A30) | ghcr.io/huggingface/text-embeddings-inference:0.6
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 安培 80（A100、A30） | ghcr.io/huggingface/text-embeddings-inference:0.6 |'
- en: '| Ampere 86 (A10, A40, …) | ghcr.io/huggingface/text-embeddings-inference:86-0.6
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 安培 86（A10、A40，...） | ghcr.io/huggingface/text-embeddings-inference:86-0.6
    |'
- en: '| Ada Lovelace (RTX 4000 series, …) | ghcr.io/huggingface/text-embeddings-inference:89-0.6
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 艾达·洛维斯（RTX 4000 系列，...） | ghcr.io/huggingface/text-embeddings-inference:89-0.6
    |'
- en: '| Hopper (H100) | ghcr.io/huggingface/text-embeddings-inference:hopper-0.4.0
    (experimental) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 霍珀（H100） | ghcr.io/huggingface/text-embeddings-inference:hopper-0.4.0（实验性）
    |'
- en: '**Warning**: Flash Attention is turned off by default for the Turing image
    as it suffers from precision issues. You can turn Flash Attention v1 ON by using
    the `USE_FLASH_ATTENTION=True` environment variable.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告**：由于存在精度问题，图灵镜像默认关闭了 Flash Attention。您可以通过使用`USE_FLASH_ATTENTION=True`环境变量将
    Flash Attention v1 打开。'
