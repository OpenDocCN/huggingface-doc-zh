# NLLB-MOE

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/nllb-moe](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/nllb-moe)

## 概述

NLLB模型是由Marta R. Costa-jussà、James Cross、Onur Çelebi、Maha Elbayad、Kenneth Heafield、Kevin Heffernan、Elahe Kalbassi、Janice Lam、Daniel Licht、Jean Maillard、Anna Sun、Skyler Wang、Guillaume Wenzek、Al Youngblood、Bapi Akula、Loic Barrault、Gabriel Mejia Gonzalez、Prangthip Hansanti、John Hoffman、Semarley Jarrett、Kaushik Ram Sadagopan、Dirk Rowe、Shannon Spruit、Chau Tran、Pierre Andrews、Necip Fazil Ayan、Shruti Bhosale、Sergey Edunov、Angela Fan、Cynthia Gao、Vedanuj Goswami、Francisco Guzmán、Philipp Koehn、Alexandre Mourachko、Christophe Ropers、Safiyyah Saleem、Holger Schwenk和Jeff Wang在[No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672)中提出的。

该论文的摘要如下：

*受到在全球范围内消除语言障碍的目标驱动，机器翻译已经巩固自己作为当今人工智能研究的重点。然而，这些努力已经围绕着一小部分语言展开，抛弃了大多数主要是低资源语言的语言。要突破200种语言障碍，同时确保安全、高质量的结果，同时考虑伦理因素，需要什么？在《No Language Left Behind》中，我们通过首先通过与母语者的探索性访谈来将对低资源语言翻译支持的需求进行情境化，然后创建了旨在缩小低资源语言与高资源语言之间性能差距的数据集和模型。更具体地说，我们开发了一个基于Sparsely Gated Mixture of Experts的条件计算模型，该模型是通过针对低资源语言量身定制的新颖和有效的数据挖掘技术获得的数据进行训练的。我们提出了多种架构和训练改进措施，以抵消在数千个任务上训练时的过拟合。至关重要的是，我们使用人工翻译的基准Flores-200评估了超过40,000个不同的翻译方向的性能，并结合了一个涵盖Flores-200中所有语言的新型毒性基准来评估翻译的安全性。我们的模型相对于先前的最先进技术实现了44%的BLEU改进，为实现通用翻译系统奠定了重要基础。*

该模型由[Arthur Zucker](https://huggingface.co/ArthurZ)贡献。原始代码可以在[这里](https://github.com/facebookresearch/fairseq)找到。

## 使用提示

+   M2M100ForConditionalGeneration是NLLB和NLLB MoE的基础模型

+   NLLB-MoE与NLLB模型非常相似，但其前馈层基于SwitchTransformers的实现。

+   分词器与NLLB模型相同。

## 与SwitchTransformers的实现差异

最大的区别在于令牌路由的方式。NLLB-MoE使用`top-2-gate`，这意味着对于每个输入，只选择两个最高预测概率的专家，其余专家将被忽略。在`SwitchTransformers`中，只计算了前两个最高概率，这意味着令牌被转发的概率较低。此外，如果一个令牌没有路由到任何专家，`SwitchTransformers`仍然会添加其未修改的隐藏状态（类似于残差连接），而在`NLLB`的top-2路由机制中，它们被屏蔽。

## 使用NLLB-MoE生成

可用的检查点需要约350GB的存储空间。如果您的计算机内存不足，请确保使用`accelerate`。

在生成目标文本集时，将`forced_bos_token_id`设置为目标语言id。以下示例显示如何使用*facebook/nllb-200-distilled-600M*模型将英语翻译成法语。

请注意，我们使用法语的BCP-47代码`fra_Latn`。请参阅[Flores 200数据集中所有BCP-47的列表](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)。

```py
>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/nllb-moe-54b")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("facebook/nllb-moe-54b")

>>> article = "Previously, Ring's CEO, Jamie Siminoff, remarked the company started when his doorbell wasn't audible from his shop in his garage."
>>> inputs = tokenizer(article, return_tensors="pt")

>>> translated_tokens = model.generate(
...     **inputs, forced_bos_token_id=tokenizer.lang_code_to_id["fra_Latn"], max_length=50
... )
>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]
"Auparavant, le PDG de Ring, Jamie Siminoff, a fait remarquer que la société avait commencé lorsque sa sonnette n'était pas audible depuis son magasin dans son garage."
```

### 从除英语以外的任何其他语言生成

英语（`eng_Latn`）被设置为默认语言进行翻译。为了指定您希望从其他语言翻译，您应该在分词器初始化的`src_lang`关键字参数中指定BCP-47代码。

请参见下面的示例，将罗马尼亚语翻译成德语：

```py
>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/nllb-moe-54b", src_lang="ron_Latn")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("facebook/nllb-moe-54b")

>>> article = "Şeful ONU spune că nu există o soluţie militară în Siria"
>>> inputs = tokenizer(article, return_tensors="pt")

>>> translated_tokens = model.generate(
...     **inputs, forced_bos_token_id=tokenizer.lang_code_to_id["deu_Latn"], max_length=30
... )
>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]
```

## 资源

+   [翻译任务指南](../tasks/translation)

+   [摘要任务指南](../tasks/summarization)

## NllbMoeConfig

### `class transformers.NllbMoeConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/configuration_nllb_moe.py#L27)

```py
( vocab_size = 128112 max_position_embeddings = 1024 encoder_layers = 12 encoder_ffn_dim = 4096 encoder_attention_heads = 16 decoder_layers = 12 decoder_ffn_dim = 4096 decoder_attention_heads = 16 encoder_layerdrop = 0.05 decoder_layerdrop = 0.05 use_cache = True is_encoder_decoder = True activation_function = 'relu' d_model = 1024 dropout = 0.1 attention_dropout = 0.1 activation_dropout = 0.0 init_std = 0.02 decoder_start_token_id = 2 scale_embedding = True router_bias = False router_dtype = 'float32' router_ignore_padding_tokens = False num_experts = 128 expert_capacity = 64 encoder_sparse_step = 4 decoder_sparse_step = 4 router_z_loss_coef = 0.001 router_aux_loss_coef = 0.001 second_expert_policy = 'all' normalize_router_prob_before_dropping = False batch_prioritized_routing = False moe_eval_capacity_token_fraction = 1.0 moe_token_dropout = 0.2 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 output_router_logits = False **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 50265) — NllbMoe模型的词汇量。定义了在调用[NllbMoeModel](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeModel)时可以表示的不同标记数量。

+   `d_model` (`int`, *optional*, defaults to 1024) — 层和池化器层的维度。

+   `encoder_layers` (`int`, *optional*, defaults to 12) — 编码器层数。

+   `decoder_layers` (`int`, *optional*, defaults to 12) — 解码器层数。

+   `encoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer编码器中每个注意力层的注意力头数。

+   `decoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer解码器中每个注意力层的注意力头数。

+   `decoder_ffn_dim` (`int`, *optional*, defaults to 4096) — 解码器中“中间”（通常称为前馈）层的维度。

+   `encoder_ffn_dim` (`int`, *optional*, defaults to 4096) — 编码器中“中间”（通常称为前馈）层的维度。

+   `activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。

+   `dropout` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的dropout比率。

+   `activation_dropout` (`float`, *optional*, defaults to 0.0) — 全连接层内激活的dropout比率。

+   `classifier_dropout` (`float`, *optional*, defaults to 0.0) — 分类器的dropout比率。

+   `max_position_embeddings` (`int`, *optional*, defaults to 1024) — 该模型可能使用的最大序列长度。通常设置为较大的值以防万一（例如512、1024或2048）。

+   `init_std` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `encoder_layerdrop` (`float`, *optional*, defaults to 0.0) — 编码器的LayerDrop概率。有关更多详细信息，请参阅[LayerDrop论文](https://arxiv.org/abs/1909.11556)。

+   `decoder_layerdrop` (`float`, *optional*, defaults to 0.0) — 解码器的LayerDrop概率。有关更多详细信息，请参阅[LayerDrop论文](https://arxiv.org/abs/1909.11556)。

+   `second_expert_policy` (`str`, *optional*, default to `"all"`) — 用于对每个标记采样到第二专家的概率进行采样的策略。

+   `normalize_router_prob_before_dropping` (`bool`, *optional*, defaults to `True`) — 是否在应用基于专家容量的掩码之前对路由器概率进行归一化（容量降低）。

+   `batch_prioritized_routing` (`bool`, *optional*, 默认为`True`) — 是否按照路由器概率对令牌进行排序以进行容量丢弃。这意味着具有最高概率的令牌将在其他可能在序列中更远的令牌之前路由。

+   `moe_eval_capacity_token_fraction` (`float`, *optional*, 默认为1.0) — 验证期间作为容量的令牌分数，如果设置为负数，则使用与训练相同的值。应在范围内：(0.0, 1.0]。

+   `num_experts` (`int`, *optional*, 默认为128) — 每个NllbMoeSparseMlp层的专家数量。

+   `expert_capacity` (`int`, *optional*, 默认为64) — 每个专家可以存储的令牌数量。

+   `encoder_sparse_step` (`int`, *optional*, defaults to 4) — 编码器中稀疏层的频率。4表示每4层中会有一层是稀疏的。

+   `decoder_sparse_step` (`int`, *optional*, defaults to 4) — 解码器中稀疏层的频率。4表示每4层中会有一层是稀疏的。

+   `router_dtype` (`str`, *optional*, 默认为`"float32"`) — 用于路由器的`dtype`。最好保持`dtype`为`"float32"`，如[论文](https://arxiv.org/abs/2101.03961)中的*selective precision*讨论中所指定的。

+   `router_ignore_padding_tokens` (`bool`, *optional*, 默认为`False`) — 在路由时是否忽略填充令牌。如果为`False`，则填充令牌不会路由到任何专家。

+   `router_bias` (`bool`, *optional*, 默认为`False`) — 路由器的分类器是否应具有偏差。

+   `moe_token_dropout` (`float`, *optional*, 默认为0.2) — MoE专家输出掩码（EOM）的掩码率，通过对专家输出进行Dropout2d实现。

+   `output_router_logits` (`bool`, *optional*, 默认为`False`) — 是否返回路由器logits。仅在训练时设置为`True`以获得辅助损失。

+   `use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。

这是一个配置类，用于存储[NllbMoeModel](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeModel)的配置。它用于根据指定的参数实例化一个NLLB-MoE模型，定义模型架构。使用默认值实例化配置将产生类似于NLLB-MoE [facebook/nllb-moe-54b](https://huggingface.co/facebook/nllb-moe-54b) 架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import NllbMoeModel, NllbMoeConfig

>>> # Initializing a NllbMoe facebook/nllb-moe-54b style configuration
>>> configuration = NllbMoeConfig()

>>> # Initializing a model from the facebook/nllb-moe-54b style configuration
>>> model = NllbMoeModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## NllbMoeTop2Router

### `class transformers.NllbMoeTop2Router`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L217)

```py
( config: NllbMoeConfig )
```

路由器使用令牌选择前两个专家的分配。

此路由器使用与fairseq存储库中的NLLB-MoE相同的机制。项目按照router_probs排序，然后路由到其选择的专家，直到达到专家的expert_capacity。**不能保证每个令牌都由专家处理**，也不能保证每个专家至少收到一个令牌。

还返回路由器组合权重，以确保未更新的状态将被掩码。

#### `route_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L258)

```py
( router_logits: Tensor input_dtype: dtype = torch.float32 padding_mask: Optional = None )
```

计算每个专家的`dispatch_mask`和`dispatch_weights`。这些掩码会根据专家的容量进行调整。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L343)

```py
( hidden_states: Tensor padding_mask: Optional = None ) → export const metadata = 'undefined';top_1_mask (torch.Tensor of shape (batch_size, sequence_length))
```

参数

+   `hidden_states` (`torch.Tensor`) — (batch_size, sequence_length, hidden_dim) 用于计算路由器概率。

返回

top_1_mask (`torch.Tensor`，形状为(batch_size, sequence_length))

形状为[batch_size, sequence_length]的索引张量，对应于使用路由器的top1概率为每个标记选择的专家。router_probabilities (`torch.Tensor`，形状为(batch_size, sequence_length, nump_experts))：形状为(batch_size, sequence_length, num_experts)的张量，对应于每个标记和专家的概率。用于将标记路由到专家。router_logits (`torch.Tensor`，形状为(batch_size, sequence_length))：形状为(batch_size, sequence_length, num_experts)的原始路由器logits张量。稍后用于计算路由器z-loss。

隐藏状态被重新整形以简化路由器概率的计算（为每个专家组合权重）。

## NllbMoeSparseMLP

### `class transformers.NllbMoeSparseMLP`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L394)

```py
( config: NllbMoeConfig ffn_dim: int expert_class: Module = <class 'transformers.models.nllb_moe.modeling_nllb_moe.NllbMoeDenseActDense'> )
```

NLLB-MoE稀疏MLP模块的实现。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L410)

```py
( hidden_states: Tensor padding_mask: Optional = False ) → export const metadata = 'undefined';hidden_states (torch.Tensor of shape (batch_size, sequence_length, hidden_dim))
```

参数

+   `hidden_states` (`torch.Tensor`，形状为`(batch_size, sequence_length, hidden_dim)`) — 隐藏状态

+   `padding_mask` (`torch.Tensor`，*可选*，默认为`False`) — 注意力掩码。可以是因果形式或非因果形式。

返回

hidden_states (`torch.Tensor`，形状为`(batch_size, sequence_length, hidden_dim)`)

更新后的隐藏状态路由器logits (`torch.Tensor`，形状为`(batch_size, sequence_length, num_experts)`)：用于计算损失

前向传递的目标是具有与等效的`NllbMoeDenseActDense`（mlp）层相同数量的操作。这意味着所有隐藏状态最多应该被处理两次（因为我们使用了一个top_2门控机制）。这意味着我们将复杂度保持在O(batch_size x sequence_length x hidden_dim)而不是O(num_experts x batch_size x sequence_length x hidden_dim)。

1- 从`router`获取`router_probs`。`router_mask`的形状为`(batch_size X sequence_length, num_expert)`，对应于`router_probs`的布尔版本。使用`router_mask`对输入进行掩码处理。

2- 将隐藏状态分派给其关联的专家。路由器概率用于在更新掩码隐藏状态时加权每个专家的贡献。

## NllbMoeModel

### `class transformers.NllbMoeModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L1452)

```py
( config: NllbMoeConfig )
```

参数

+   `config` ([NllbMoeConfig](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸NllbMoe模型输出原始隐藏状态，没有特定的头部。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L1490)

```py
( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None output_router_logits: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqMoEModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。

返回

`transformers.modeling_outputs.Seq2SeqMoEModelOutput`或`tuple(torch.FloatTensor)`

一个`transformers.modeling_outputs.Seq2SeqMoEModelOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或者`config.return_dict=False`）包含根据配置（[NllbMoeConfig](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeConfig)）和输入的各种元素。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型解码器最后一层的隐藏状态序列。

    如果仅使用`past_key_values`，则输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或者`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或者`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个 + 每一层的输出一个）。

    解码器每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `decoder_attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或者`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `decoder_router_logits` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_router_logits=True`或者`config.add_router_probs=True`时返回) — 形状为`(batch_size, sequence_length, num_experts)`的`torch.FloatTensor`元组（每层一个）。

    解码器模型的路由器logits，用于计算混合专家模型的辅助损失。

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或者`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或者`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个 + 每一层的输出一个）。

    编码器每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `encoder_router_logits`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_router_logits=True`或`config.add_router_probs=True`时返回）— 形状为`(batch_size, sequence_length, num_experts)`的`torch.FloatTensor`元组（每层一个）。

    编码器模型的路由器logits，用于计算辅助损失和稀疏模块的z_loss。

[NllbMoeModel](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

[NllbMoeModel](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, NllbMoeModel

>>> tokenizer = AutoTokenizer.from_pretrained("hf-internal-testing/random-nllb-moe-2-experts")
>>> model = SwitchTransformersModel.from_pretrained("hf-internal-testing/random-nllb-moe-2-experts")

>>> input_ids = tokenizer(
...     "Studies have been shown that owning a dog is good for you", return_tensors="pt"
... ).input_ids  # Batch size 1
>>> decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1

>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for NllbMoeModel
>>> decoder_input_ids = model._shift_right(decoder_input_ids)

>>> # forward pass
>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)
>>> last_hidden_states = outputs.last_hidden_state
```

## NllbMoeForConditionalGeneration

### `class transformers.NllbMoeForConditionalGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L1590)

```py
( config: NllbMoeConfig )
```

参数

+   `config`（[NllbMoeConfig](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

NllbMoe模型带有一个语言建模头。可用于摘要。该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为其所有模型实现的通用方法（例如下载或保存，调整输入嵌入大小，修剪头等）。

该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb_moe/modeling_nllb_moe.py#L1619)

```py
( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None output_router_logits: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqMoEOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`）— 输入序列标记在词汇表中的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   对于未被屏蔽的标记为1，

    +   对于被屏蔽的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `decoder_input_ids` (`torch.LongTensor` 的形状为 `(batch_size, target_sequence_length)`，*optional*) — 词汇表中解码器输入序列标记的索引。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是解码器输入标记？](../glossary#decoder-input-ids)

    NllbMoe 使用 `eos_token_id` 作为 `decoder_input_ids` 生成的起始标记。如果使用 `past_key_values`，可选择仅输入最后的 `decoder_input_ids`（参见 `past_key_values`）。

+   `decoder_attention_mask` (`torch.LongTensor` 的形状为 `(batch_size, target_sequence_length)`，*optional*) — 默认行为：生成一个忽略 `decoder_input_ids` 中填充标记的张量。因果掩码也将默认使用。

+   `head_mask` (`torch.Tensor` 的形状为 `(encoder_layers, encoder_attention_heads)`，*optional*) — 编码器中注意力模块中选择性头部置零的掩码。掩码值在 `[0, 1]` 之间：

    +   1 表示头部未被掩码，

    +   0 表示头部被掩码。

+   `decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*) — 解码器中注意力模块中选择性头部置零的掩码。掩码值在 `[0, 1]` 之间：

    +   1 表示头部未被掩码，

    +   0 表示头部被掩码。

+   `cross_attn_head_mask` (`torch.Tensor` 的形状为 `(decoder_layers, decoder_attention_heads)`，*optional*) — 解码器中交叉注意力模块中选择性头部置零的掩码。掩码值在 `[0, 1]` 之间：

    +   1 表示头部未被掩码，

    +   0 表示头部被掩码。

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组包括 (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state` 的形状为 `(batch_size, sequence_length, hidden_size)`，*optional*) 是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递 `use_cache=True` 或 `config.use_cache=True` 时返回) — 长度为 `config.n_layers` 的 `tuple(torch.FloatTensor)` 元组，每个元组有 2 个形状为 `(batch_size, num_heads, sequence_length, embed_size_per_head)` 的张量和额外的 2 个形状为 `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)` 的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见 `past_key_values` 输入）。

    如果使用 `past_key_values`，用户可以选择仅输入最后的 `decoder_input_ids`（即未将其过去键值状态提供给此模型的那些）的形状为 `(batch_size, 1)`，而不是形状为 `(batch_size, sequence_length)` 的所有 `decoder_input_ids`。

+   `inputs_embeds` (`torch.FloatTensor` 的形状为 `(batch_size, sequence_length, hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示，而不是传递 `input_ids`。如果您希望更好地控制如何将 `input_ids` 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `decoder_inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, target_sequence_length, hidden_size)`，*可选的*) — 可选地，可以直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用了`past_key_values`，则只需输入最后的`decoder_inputs_embeds`（参见`past_key_values`）。如果要更好地控制如何将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

    如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。

+   `use_cache` (`bool`, *可选的*) — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`, *可选的*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选的*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `output_router_logits` (`bool`, *可选的*) — 是否返回所有路由器的对数。它们对于计算路由器损失很有用，在推断期间不应返回。

+   `return_dict` (`bool`, *可选的*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选的*) — 用于计算掩码语言建模损失的标签。索引应该在`[0, ..., config.vocab_size]`范围内，或者为-100（参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`范围内的标记。

返回值

`transformers.modeling_outputs.Seq2SeqMoEOutput`或者`tuple(torch.FloatTensor)`

一个`transformers.modeling_outputs.Seq2SeqMoEOutput`或者一个`torch.FloatTensor`元组（如果传递`return_dict=False`或者当`config.return_dict=False`时）包含各种元素，取决于配置（[NllbMoeConfig](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeConfig)）和输入。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`, *可选的*, 当提供`labels`时返回) — 语言建模损失。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *可选的*, 当传递`use_cache=True`或者当`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或者当`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有嵌入层，+ 一个用于每个层的输出）。

    每个层的解码器的隐藏状态加上初始嵌入输出。

+   `decoder_attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或者当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。

    [NllbMoeForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration)的前向方法，覆盖了`__call__`特殊方法。

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`。

    `encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`。

+   `encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 编码器模型最后一层的隐藏状态序列。

    编码器每一层输出的隐藏状态，以及初始嵌入输出。

+   编码器模型的路由器logits，用于计算混合专家模型的辅助损失和z_loss。

+   解码器模型的路由器logits，用于计算混合专家模型的辅助损失。

    解码器的注意力权重，在注意力softmax之后使用，用于计算自注意力头中的加权平均值。

+   编码器的注意力权重，在注意力softmax之后使用，用于计算自注意力头中的加权平均值。

    虽然前向传递的步骤需要在这个函数内定义，但应该在之后调用`Module`实例，而不是这个函数，因为前者会负责运行前后处理步骤，而后者会默默地忽略它们。

+   `encoder_router_logits` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_router_logits=True` is passed or when `config.add_router_probs=True`) — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length, num_experts)`.

    解码器的交叉注意力层的注意力权重，在注意力softmax之后使用，用于计算交叉注意力头中的加权平均值。

`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, + one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`。

在传递`output_router_logits=True`或`config.add_router_probs=True`时返回，`encoder_router_logits`是一个元组，包含每一层的`torch.FloatTensor`，形状为`(batch_size, sequence_length, num_experts)`。

`decoder_router_logits` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_router_logits=True` is passed or when `config.add_router_probs=True`) — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length, num_experts)`。

```py
>>> from transformers import AutoTokenizer, NllbMoeForConditionalGeneration

>>> model = NllbMoeForConditionalGeneration.from_pretrained("facebook/nllb-moe-54b")
>>> tokenizer = AutoTokenizer.from_pretrained("facebook/nllb-moe-54b")

>>> text_to_translate = "Life is like a box of chocolates"
>>> model_inputs = tokenizer(text_to_translate, return_tensors="pt")

>>> # translate to French
>>> gen_tokens = model.generate(**model_inputs, forced_bos_token_id=tokenizer.get_lang_id("eng_Latn"))
>>> print(tokenizer.batch_decode(gen_tokens, skip_special_tokens=True))
```
