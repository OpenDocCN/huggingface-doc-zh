["```py\n( use_timm_backbone = True backbone_config = None num_channels = 3 num_queries = 100 encoder_layers = 6 encoder_ffn_dim = 2048 encoder_attention_heads = 8 decoder_layers = 6 decoder_ffn_dim = 2048 decoder_attention_heads = 8 encoder_layerdrop = 0.0 decoder_layerdrop = 0.0 is_encoder_decoder = True activation_function = 'relu' d_model = 256 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 init_std = 0.02 init_xavier_std = 1.0 auxiliary_loss = False position_embedding_type = 'sine' backbone = 'resnet50' use_pretrained_backbone = True dilation = False class_cost = 1 bbox_cost = 5 giou_cost = 2 mask_loss_coefficient = 1 dice_loss_coefficient = 1 bbox_loss_coefficient = 5 giou_loss_coefficient = 2 eos_coefficient = 0.1 **kwargs )\n```", "```py\n>>> from transformers import TableTransformerModel, TableTransformerConfig\n\n>>> # Initializing a Table Transformer microsoft/table-transformer-detection style configuration\n>>> configuration = TableTransformerConfig()\n\n>>> # Initializing a model from the microsoft/table-transformer-detection style configuration\n>>> model = TableTransformerModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config: TableTransformerConfig )\n```", "```py\n( pixel_values: FloatTensor pixel_mask: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.table_transformer.modeling_table_transformer.TableTransformerModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, TableTransformerModel\n>>> from huggingface_hub import hf_hub_download\n>>> from PIL import Image\n\n>>> file_path = hf_hub_download(repo_id=\"nielsr/example-pdf\", repo_type=\"dataset\", filename=\"example_pdf.png\")\n>>> image = Image.open(file_path).convert(\"RGB\")\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/table-transformer-detection\")\n>>> model = TableTransformerModel.from_pretrained(\"microsoft/table-transformer-detection\")\n\n>>> # prepare image for the model\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(**inputs)\n\n>>> # the last hidden states are the final query embeddings of the Transformer decoder\n>>> # these are of shape (batch_size, num_queries, hidden_size)\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 15, 256]\n```", "```py\n( config: TableTransformerConfig )\n```", "```py\n( pixel_values: FloatTensor pixel_mask: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.table_transformer.modeling_table_transformer.TableTransformerObjectDetectionOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from huggingface_hub import hf_hub_download\n>>> from transformers import AutoImageProcessor, TableTransformerForObjectDetection\n>>> import torch\n>>> from PIL import Image\n\n>>> file_path = hf_hub_download(repo_id=\"nielsr/example-pdf\", repo_type=\"dataset\", filename=\"example_pdf.png\")\n>>> image = Image.open(file_path).convert(\"RGB\")\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/table-transformer-detection\")\n>>> model = TableTransformerForObjectDetection.from_pretrained(\"microsoft/table-transformer-detection\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n>>> target_sizes = torch.tensor([image.size[::-1]])\n>>> results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[\n...     0\n... ]\n\n>>> for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(\n...         f\"Detected {model.config.id2label[label.item()]} with confidence \"\n...         f\"{round(score.item(), 3)} at location {box}\"\n...     )\nDetected table with confidence 1.0 at location [202.1, 210.59, 1119.22, 385.09]\n```"]