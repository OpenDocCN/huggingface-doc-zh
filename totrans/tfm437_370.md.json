["```py\n>>> from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n>>> import requests\n>>> from PIL import Image\n\n>>> processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n\n>>> # load image from the IAM dataset\n>>> url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n\n>>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n>>> generated_ids = model.generate(pixel_values)\n\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```", "```py\n>>> from transformers import TrOCRConfig, TrOCRForCausalLM\n\n>>> # Initializing a TrOCR-base style configuration\n>>> configuration = TrOCRConfig()\n\n>>> # Initializing a model (with random weights) from the TrOCR-base style configuration\n>>> model = TrOCRForCausalLM(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import (\n...     TrOCRConfig,\n...     TrOCRProcessor,\n...     TrOCRForCausalLM,\n...     ViTConfig,\n...     ViTModel,\n...     VisionEncoderDecoderModel,\n... )\n>>> import requests\n>>> from PIL import Image\n\n>>> # TrOCR is a decoder model and should be used within a VisionEncoderDecoderModel\n>>> # init vision2text model with random weights\n>>> encoder = ViTModel(ViTConfig())\n>>> decoder = TrOCRForCausalLM(TrOCRConfig())\n>>> model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder)\n\n>>> # If you want to start from the pretrained model, load the checkpoint with `VisionEncoderDecoderModel`\n>>> processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n\n>>> # load image from the IAM dataset\n>>> url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n>>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n>>> text = \"industry, ' Mr. Brown commented icily. ' Let us have a\"\n\n>>> # training\n>>> model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n>>> model.config.pad_token_id = processor.tokenizer.pad_token_id\n>>> model.config.vocab_size = model.config.decoder.vocab_size\n\n>>> labels = processor.tokenizer(text, return_tensors=\"pt\").input_ids\n>>> outputs = model(pixel_values, labels=labels)\n>>> loss = outputs.loss\n>>> round(loss.item(), 2)\n5.30\n\n>>> # inference\n>>> generated_ids = model.generate(pixel_values)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> generated_text\n'industry, \" Mr. Brown commented icily. \" Let us have a'\n```"]