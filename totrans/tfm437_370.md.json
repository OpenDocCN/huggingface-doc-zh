["```py\n>>> from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n>>> import requests\n>>> from PIL import Image\n\n>>> processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n\n>>> # load image from the IAM dataset\n>>> url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n\n>>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n>>> generated_ids = model.generate(pixel_values)\n\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```", "```py\n( vocab_size = 50265 d_model = 1024 decoder_layers = 12 decoder_attention_heads = 16 decoder_ffn_dim = 4096 activation_function = 'gelu' max_position_embeddings = 512 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 decoder_start_token_id = 2 init_std = 0.02 decoder_layerdrop = 0.0 use_cache = True scale_embedding = False use_learned_position_embeddings = True layernorm_embedding = True pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 **kwargs )\n```", "```py\n>>> from transformers import TrOCRConfig, TrOCRForCausalLM\n\n>>> # Initializing a TrOCR-base style configuration\n>>> configuration = TrOCRConfig()\n\n>>> # Initializing a model (with random weights) from the TrOCR-base style configuration\n>>> model = TrOCRForCausalLM(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( image_processor = None tokenizer = None **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( pretrained_model_name_or_path: Union cache_dir: Union = None force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' **kwargs )\n```", "```py\n( save_directory push_to_hub: bool = False **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import (\n...     TrOCRConfig,\n...     TrOCRProcessor,\n...     TrOCRForCausalLM,\n...     ViTConfig,\n...     ViTModel,\n...     VisionEncoderDecoderModel,\n... )\n>>> import requests\n>>> from PIL import Image\n\n>>> # TrOCR is a decoder model and should be used within a VisionEncoderDecoderModel\n>>> # init vision2text model with random weights\n>>> encoder = ViTModel(ViTConfig())\n>>> decoder = TrOCRForCausalLM(TrOCRConfig())\n>>> model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder)\n\n>>> # If you want to start from the pretrained model, load the checkpoint with `VisionEncoderDecoderModel`\n>>> processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n\n>>> # load image from the IAM dataset\n>>> url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n>>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n>>> text = \"industry, ' Mr. Brown commented icily. ' Let us have a\"\n\n>>> # training\n>>> model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n>>> model.config.pad_token_id = processor.tokenizer.pad_token_id\n>>> model.config.vocab_size = model.config.decoder.vocab_size\n\n>>> labels = processor.tokenizer(text, return_tensors=\"pt\").input_ids\n>>> outputs = model(pixel_values, labels=labels)\n>>> loss = outputs.loss\n>>> round(loss.item(), 2)\n5.30\n\n>>> # inference\n>>> generated_ids = model.generate(pixel_values)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> generated_text\n'industry, \" Mr. Brown commented icily. \" Let us have a'\n```"]