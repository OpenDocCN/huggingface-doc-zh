# æŒ‡æ ‡

> åŽŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/datasets/how_to_metrics`](https://huggingface.co/docs/datasets/how_to_metrics)

ðŸ¤— Datasets ä¸­çš„ Metrics å·²è¢«å¼ƒç”¨ã€‚è¦äº†è§£å¦‚ä½•ä½¿ç”¨æŒ‡æ ‡ï¼Œè¯·æŸ¥çœ‹åº“ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)ï¼é™¤äº†æŒ‡æ ‡ï¼Œæ‚¨è¿˜å¯ä»¥æ‰¾åˆ°æ›´å¤šç”¨äºŽè¯„ä¼°æ¨¡åž‹å’Œæ•°æ®é›†çš„å·¥å…·ã€‚

æŒ‡æ ‡å¯¹äºŽè¯„ä¼°æ¨¡åž‹çš„é¢„æµ‹éžå¸¸é‡è¦ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å­¦ä¹ äº†å¦‚ä½•åœ¨æ•´ä¸ªè¯„ä¼°é›†ä¸Šè®¡ç®—æŒ‡æ ‡ã€‚æ‚¨è¿˜çœ‹åˆ°äº†å¦‚ä½•åŠ è½½æŒ‡æ ‡ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

+   æ·»åŠ é¢„æµ‹å’Œå‚è€ƒã€‚

+   ä½¿ç”¨ä¸åŒçš„æ–¹æ³•è®¡ç®—æŒ‡æ ‡ã€‚

+   ç¼–å†™æ‚¨è‡ªå·±çš„æŒ‡æ ‡åŠ è½½è„šæœ¬ã€‚

## æ·»åŠ é¢„æµ‹å’Œå‚è€ƒ

å½“æ‚¨æƒ³è¦å°†æ¨¡åž‹é¢„æµ‹å’Œå‚è€ƒæ·»åŠ åˆ° Metric å®žä¾‹æ—¶ï¼Œæ‚¨æœ‰ä¸¤ä¸ªé€‰é¡¹ï¼š

+   Metric.add()æ·»åŠ å•ä¸ª`prediction`å’Œ`reference`ã€‚

+   Metric.add_batch()æ·»åŠ ä¸€æ‰¹`predictions`å’Œ`references`ã€‚

é€šè¿‡ä½¿ç”¨ Metric.add_batch()ï¼Œä¼ é€’æ‚¨çš„æ¨¡åž‹é¢„æµ‹å’Œæ¨¡åž‹é¢„æµ‹åº”è¯¥è¢«è¯„ä¼°çš„å‚è€ƒï¼š

```py
>>> import datasets
>>> metric = datasets.load_metric('my_metric')
>>> for model_input, gold_references in evaluation_dataset:
...     model_predictions = model(model_inputs)
...     metric.add_batch(predictions=model_predictions, references=gold_references)
>>> final_score = metric.compute()
```

æŒ‡æ ‡æŽ¥å—å„ç§è¾“å…¥æ ¼å¼ï¼ˆPython åˆ—è¡¨ã€NumPy æ•°ç»„ã€PyTorch å¼ é‡ç­‰ï¼‰ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºé€‚åˆå­˜å‚¨å’Œè®¡ç®—çš„æ ¼å¼ã€‚

## è®¡ç®—åˆ†æ•°

è®¡ç®—æŒ‡æ ‡çš„æœ€ç›´æŽ¥æ–¹æ³•æ˜¯è°ƒç”¨ Metric.compute()ã€‚ä½†æ˜¯æœ‰äº›æŒ‡æ ‡å…·æœ‰é¢å¤–çš„å‚æ•°ï¼Œå…è®¸æ‚¨ä¿®æ”¹æŒ‡æ ‡çš„è¡Œä¸ºã€‚

è®©æˆ‘ä»¬åŠ è½½[SacreBLEU](https://huggingface.co/metrics/sacrebleu)æŒ‡æ ‡ï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„å¹³æ»‘æ–¹æ³•è®¡ç®—å®ƒã€‚

1.  åŠ è½½ SacreBLEU æŒ‡æ ‡ï¼š

```py
>>> import datasets
>>> metric = datasets.load_metric('sacrebleu')
```

1.  æ£€æŸ¥ç”¨äºŽè®¡ç®—æŒ‡æ ‡çš„ä¸åŒå‚æ•°æ–¹æ³•ï¼š

```py
>>> print(metric.inputs_description)
Produces BLEU scores along with its sufficient statistics
from a source against one or more references.

Args:
    predictions: The system stream (a sequence of segments).
    references: A list of one or more reference streams (each a sequence of segments).
    smooth_method: The smoothing method to use. (Default: 'exp').
    smooth_value: The smoothing value. Only valid for 'floor' and 'add-k'. (Defaults: floor: 0.1, add-k: 1).
    tokenize: Tokenization method to use for BLEU. If not provided, defaults to 'zh' for Chinese, 'ja-mecab' for Japanese and '13a' (mteval) otherwise.
    lowercase: Lowercase the data. If True, enables case-insensitivity. (Default: False).
    force: Insist that your tokenized input is actually detokenized.
...
```

1.  ä½¿ç”¨`floor`æ–¹æ³•è®¡ç®—æŒ‡æ ‡ï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„`smooth_value`ï¼š

```py
>>> score = metric.compute(smooth_method="floor", smooth_value=0.2)
```

## è‡ªå®šä¹‰æŒ‡æ ‡åŠ è½½è„šæœ¬

ç¼–å†™ä¸€ä¸ªæŒ‡æ ‡åŠ è½½è„šæœ¬æ¥ä½¿ç”¨æ‚¨è‡ªå·±çš„è‡ªå®šä¹‰æŒ‡æ ‡ï¼ˆæˆ–è€…ä¸åœ¨ Hub ä¸Šçš„æŒ‡æ ‡ï¼‰ã€‚ç„¶åŽï¼Œæ‚¨å¯ä»¥åƒå¾€å¸¸ä¸€æ ·ä½¿ç”¨ load_metric()åŠ è½½å®ƒã€‚

ä¸ºäº†å¸®åŠ©æ‚¨å…¥é—¨ï¼Œè¯·æ‰“å¼€[SQuAD æŒ‡æ ‡åŠ è½½è„šæœ¬](https://github.com/huggingface/datasets/blob/main/metrics/squad/squad.py)ï¼Œå¹¶è·Ÿéšæ“ä½œã€‚

ä½¿ç”¨æˆ‘ä»¬çš„æŒ‡æ ‡åŠ è½½è„šæœ¬[æ¨¡æ¿](https://github.com/huggingface/datasets/blob/f9713d2e23813142a02f1b0e965095f528785cff/templates/new_metric_script.py)å¿«é€Ÿå…¥é—¨ï¼

### æ·»åŠ æŒ‡æ ‡å±žæ€§

é¦–å…ˆåœ¨`Metric._info()`ä¸­æ·»åŠ æœ‰å…³æ‚¨çš„æŒ‡æ ‡çš„ä¸€äº›ä¿¡æ¯ã€‚æ‚¨åº”è¯¥æŒ‡å®šçš„æœ€é‡è¦å±žæ€§æ˜¯ï¼š

1.  `MetricInfo.description`æä¾›æœ‰å…³æ‚¨çš„æŒ‡æ ‡çš„ç®€è¦æè¿°ã€‚

1.  `MetricInfo.citation`åŒ…å«æŒ‡æ ‡çš„ BibTex å¼•ç”¨ã€‚

1.  `MetricInfo.inputs_description`æè¿°äº†é¢„æœŸçš„è¾“å…¥å’Œè¾“å‡ºã€‚å®ƒè¿˜å¯èƒ½æä¾›æŒ‡æ ‡çš„ç¤ºä¾‹ç”¨æ³•ã€‚

1.  `MetricInfo.features`å®šä¹‰äº†é¢„æµ‹å’Œå‚è€ƒçš„åç§°å’Œç±»åž‹ã€‚

åœ¨æ¨¡æ¿ä¸­å¡«å†™æ‰€æœ‰è¿™äº›å­—æ®µåŽï¼Œå®ƒåº”è¯¥çœ‹èµ·æ¥åƒæ¥è‡ª SQuAD æŒ‡æ ‡è„šæœ¬çš„ä»¥ä¸‹ç¤ºä¾‹ï¼š

```py
class Squad(datasets.Metric):
    def _info(self):
        return datasets.MetricInfo(
            description=_DESCRIPTION,
            citation=_CITATION,
            inputs_description=_KWARGS_DESCRIPTION,
            features=datasets.Features(
                {
                    "predictions": {"id": datasets.Value("string"), "prediction_text": datasets.Value("string")},
                    "references": {
                        "id": datasets.Value("string"),
                        "answers": datasets.features.Sequence(
                            {
                                "text": datasets.Value("string"),
                                "answer_start": datasets.Value("int32"),
                            }
                        ),
                    },
                }
            ),
            codebase_urls=["https://rajpurkar.github.io/SQuAD-explorer/"],
            reference_urls=["https://rajpurkar.github.io/SQuAD-explorer/"],
        )
```

### ä¸‹è½½æŒ‡æ ‡æ–‡ä»¶

å¦‚æžœæ‚¨çš„æŒ‡æ ‡éœ€è¦ä¸‹è½½æˆ–æ£€ç´¢æœ¬åœ°æ–‡ä»¶ï¼Œåˆ™éœ€è¦ä½¿ç”¨`Metric._download_and_prepare()`æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬æŸ¥çœ‹[BLEURT æŒ‡æ ‡åŠ è½½è„šæœ¬](https://github.com/huggingface/datasets/blob/main/metrics/bleurt/bleurt.py)ã€‚

1.  æä¾›æŒ‡å‘æŒ‡æ ‡æ–‡ä»¶çš„ URL å­—å…¸ï¼š

```py
CHECKPOINT_URLS = {
    "bleurt-tiny-128": "https://storage.googleapis.com/bleurt-oss/bleurt-tiny-128.zip",
    "bleurt-tiny-512": "https://storage.googleapis.com/bleurt-oss/bleurt-tiny-512.zip",
    "bleurt-base-128": "https://storage.googleapis.com/bleurt-oss/bleurt-base-128.zip",
    "bleurt-base-512": "https://storage.googleapis.com/bleurt-oss/bleurt-base-512.zip",
    "bleurt-large-128": "https://storage.googleapis.com/bleurt-oss/bleurt-large-128.zip",
    "bleurt-large-512": "https://storage.googleapis.com/bleurt-oss/bleurt-large-512.zip",
}
```

å¦‚æžœæ–‡ä»¶å­˜å‚¨åœ¨æœ¬åœ°ï¼Œè¯·æä¾›ä¸€ä¸ªè·¯å¾„å­—å…¸ï¼Œè€Œä¸æ˜¯ URLã€‚

1.  `Metric._download_and_prepare()`å°†èŽ·å– URL å¹¶ä¸‹è½½æŒ‡å®šçš„æŒ‡æ ‡æ–‡ä»¶ï¼š

```py
def _download_and_prepare(self, dl_manager):

    # check that config name specifies a valid BLEURT model
    if self.config_name == "default":
        logger.warning(
            "Using default BLEURT-Base checkpoint for sequence maximum length 128\. "
            "You can use a bigger model for better results with e.g.: datasets.load_metric('bleurt', 'bleurt-large-512')."
        )
        self.config_name = "bleurt-base-128"
    if self.config_name not in CHECKPOINT_URLS.keys():
        raise KeyError(
            f"{self.config_name} model not found. You should supply the name of a model checkpoint for bleurt in {CHECKPOINT_URLS.keys()}"
        )

    # download the model checkpoint specified by self.config_name and set up the scorer
    model_path = dl_manager.download_and_extract(CHECKPOINT_URLS[self.config_name])
    self.scorer = score.BleurtScorer(os.path.join(model_path, self.config_name))
```

### è®¡ç®—åˆ†æ•°

`DatasetBuilder._compute` æä¾›äº†å¦‚ä½•æ ¹æ®é¢„æµ‹å’Œå‚è€ƒè®¡ç®—æŒ‡æ ‡çš„å®žé™…æŒ‡ä»¤ã€‚çŽ°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ [GLUE æŒ‡æ ‡åŠ è½½è„šæœ¬](https://github.com/huggingface/datasets/blob/main/metrics/glue/glue.py)ã€‚

1.  æä¾›ç”¨äºŽ `DatasetBuilder._compute` è®¡ç®—æ‚¨çš„æŒ‡æ ‡çš„å‡½æ•°ï¼š

```py
def simple_accuracy(preds, labels):
    return (preds == labels).mean().item()

def acc_and_f1(preds, labels):
    acc = simple_accuracy(preds, labels)
    f1 = f1_score(y_true=labels, y_pred=preds).item()
    return {
        "accuracy": acc,
        "f1": f1,
    }

def pearson_and_spearman(preds, labels):
    pearson_corr = pearsonr(preds, labels)[0].item()
    spearman_corr = spearmanr(preds, labels)[0].item()
    return {
        "pearson": pearson_corr,
        "spearmanr": spearman_corr,
    }
```

1.  ä¸ºæ¯ä¸ªé…ç½®åˆ›å»ºå¸¦æœ‰è®¡ç®—æŒ‡æ ‡æŒ‡ä»¤çš„ `DatasetBuilder._compute`ï¼š

```py
def _compute(self, predictions, references):
    if self.config_name == "cola":
        return {"matthews_correlation": matthews_corrcoef(references, predictions)}
    elif self.config_name == "stsb":
        return pearson_and_spearman(predictions, references)
    elif self.config_name in ["mrpc", "qqp"]:
        return acc_and_f1(predictions, references)
    elif self.config_name in ["sst2", "mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]:
        return {"accuracy": simple_accuracy(predictions, references)}
    else:
        raise KeyError(
            "You should supply a configuration name selected in "
            '["sst2", "mnli", "mnli_mismatched", "mnli_matched", '
            '"cola", "stsb", "mrpc", "qqp", "qnli", "rte", "wnli", "hans"]'
        )
```

### æµ‹è¯•

ä¸€æ—¦æ‚¨å®Œæˆç¼–å†™æŒ‡æ ‡åŠ è½½è„šæœ¬ï¼Œè¯·å°è¯•åœ¨æœ¬åœ°åŠ è½½ï¼š

```py
>>> from datasets import load_metric
>>> metric = load_metric('PATH/TO/MY/SCRIPT.py')
```
