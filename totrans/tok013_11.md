# 分词器

> 原始文本：[https://huggingface.co/docs/tokenizers/api/tokenizer](https://huggingface.co/docs/tokenizers/api/tokenizer)

PythonRustNode

## 分词器

### `class tokenizers.Tokenizer`

```py
( model )
```

参数

+   `model`（[Model](/docs/tokenizers/v0.13.4.rc2/en/api/models#tokenizers.models.Model)） - 此`Tokenizer`应使用的核心算法。

一个`Tokenizer`作为一个管道工作。它将一些原始文本作为输入进行处理，并输出一个[Encoding](/docs/tokenizers/v0.13.4.rc2/en/api/encoding#tokenizers.Encoding)。

属性解码器

分词器使用的*可选*`Decoder`

属性模型

分词器使用的[Model](/docs/tokenizers/v0.13.4.rc2/en/api/models#tokenizers.models.Model)

属性规范化器

分词器使用的*可选*[Normalizer](/docs/tokenizers/v0.13.4.rc2/en/api/normalizers#tokenizers.normalizers.Normalizer)

属性填充

返回

（`dict`，*可选*）

如果启用了填充，则具有当前填充参数的字典

获取当前填充参数

*无法设置，使用* `enable_padding()` *代替*

属性后处理器

分词器使用的*可选*`PostProcessor`

属性预分词器

分词器使用的*可选*[PreTokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/pre-tokenizers#tokenizers.pre_tokenizers.PreTokenizer)

属性截断

返回

（`dict`，*可选*）

如果启用了截断，则具有当前截断参数的字典

获取当前设置的截断参数

*无法设置，使用* `enable_truncation()` *代替*

#### `add_special_tokens`

```py
( tokens ) → int
```

参数

+   `tokens`（一个[AddedToken](/docs/tokenizers/v0.13.4.rc2/en/api/added-tokens#tokenizers.AddedToken)或`str`的`List`） - 我们要添加到词汇表中的特殊令牌列表。每个令牌可以是一个字符串或[AddedToken](/docs/tokenizers/v0.13.4.rc2/en/api/added-tokens#tokenizers.AddedToken)的实例，以进行更多自定义。

返回

`int`

在词汇表中创建的令牌数

将给定的特殊标记添加到分词器中。

如果这些令牌已经是词汇表的一部分，它只是让分词器知道它们。如果它们不存在，分词器会创建它们，并为它们分配一个新的id。

这些特殊标记永远不会被模型处理（即不会被拆分为多个令牌），并且在解码时可以从输出中删除。

#### `add_tokens`

```py
( tokens ) → int
```

参数

+   `tokens`（一个[AddedToken](/docs/tokenizers/v0.13.4.rc2/en/api/added-tokens#tokenizers.AddedToken)或`str`的`List`） - 我们要添加到词汇表中的令牌列表。每个令牌可以是一个字符串或[AddedToken](/docs/tokenizers/v0.13.4.rc2/en/api/added-tokens#tokenizers.AddedToken)的实例，以进行更多自定义。

返回

`int`

在词汇表中创建的令牌数

将给定的令牌添加到词汇表

仅当词汇表中不存在这些令牌时才添加给定的令牌。然后，每个令牌都会获得一个新的属性id。

#### `decode`

```py
( ids skip_special_tokens = True ) → str
```

参数

+   `ids`（一个`List/Tuple`的`int`） - 我们要解码的id列表

+   `skip_special_tokens`（`bool`，默认为`True`） - 是否应从解码字符串中删除特殊标记

返回

`str`

解码后的字符串

将给定的id列表解码回字符串

用于解码从语言模型返回的任何内容

#### `decode_batch`

```py
( sequences skip_special_tokens = True ) → List[str]
```

参数

+   `sequences`（`List`的`List[int]`） - 我们要解码的序列批次

+   `skip_special_tokens`（`bool`，默认为`True`） - 是否应从解码字符串中删除特殊标记

返回

`List[str]`

解码后的字符串列表

将一批id解码回其对应的字符串

#### `enable_padding`

```py
( direction = 'right' pad_id = 0 pad_type_id = 0 pad_token = '[PAD]' length = None pad_to_multiple_of = None )
```

参数

+   `direction`（`str`，*可选*，默认为`right`） - 填充的方向。可以是`right`或`left`

+   `pad_to_multiple_of` (`int`，*可选*) — 如果指定，填充长度应始终对齐到给定值的下一个倍数。例如，如果我们要填充长度为250，但`pad_to_multiple_of=8`，那么我们将填充到256。

+   `pad_id` (`int`，默认为0) — 在填充时要使用的id

+   `pad_type_id` (`int`，默认为0) — 在填充时要使用的类型id

+   `pad_token` (`str`，默认为`[PAD]`) — 在填充时要使用的填充标记

+   `length` (`int`，*可选*) — 如果指定，填充的长度。如果未指定，我们将使用批次中最长序列的大小进行填充。

启用填充

#### `enable_truncation`

```py
( max_length stride = 0 strategy = 'longest_first' direction = 'right' )
```

参数

+   `max_length` (`int`) — 截断的最大长度

+   `stride` (`int`，*可选*) — 要包含在溢出序列中的先前第一个序列的长度

+   `strategy` (`str`，*可选*，默认为`longest_first`) — 用于截断的策略。可以是`longest_first`、`only_first`或`only_second`之一。

+   `direction` (`str`，默认为`right`) — 截断方向

启用截断

#### `encode`

```py
( sequence pair = None is_pretokenized = False add_special_tokens = True ) → Encoding
```

参数

+   `sequence` (`~tokenizers.InputSequence`) — 我们要编码的主要输入序列。根据`is_pretokenized`参数，此序列可以是原始文本或预标记的：

    +   如果`is_pretokenized=False`：`TextInputSequence`

    +   如果`is_pretokenized=True`：`PreTokenizedInputSequence()`

+   `pair`（`~tokenizers.InputSequence`，*可选*） — 一个可选的输入序列。期望的格式与`sequence`相同。

+   `is_pretokenized` (`bool`，默认为`False`) — 输入是否已经预标记

+   `add_special_tokens` (`bool`，默认为`True`) — 是否添加特殊标记

返回

[Encoding](/docs/tokenizers/v0.13.4.rc2/en/api/encoding#tokenizers.Encoding)

编码结果

对给定的序列和对进行编码。此方法可以处理原始文本序列以及已经预标记的序列。

示例：

以下是一些被接受的输入示例：

```py
encode("A single sequence")*
encode("A sequence", "And its pair")*
encode([ "A", "pre", "tokenized", "sequence" ], is_pretokenized=True)`
encode(
[ "A", "pre", "tokenized", "sequence" ], [ "And", "its", "pair" ],
is_pretokenized=True
)
```

#### `encode_batch`

```py
( input is_pretokenized = False add_special_tokens = True ) → A List of [`~tokenizers.Encoding“]
```

参数

+   `input`（`List`/`Tuple` of `~tokenizers.EncodeInput`） — 要编码的单个序列或对序列的列表。每个序列可以是原始文本或根据`is_pretokenized`参数进行预标记：

    +   如果`is_pretokenized=False`：`TextEncodeInput()`

    +   如果`is_pretokenized=True`：`PreTokenizedEncodeInput()`

+   `is_pretokenized` (`bool`，默认为`False`) — 输入是否已经预标记

+   `add_special_tokens` (`bool`，默认为`True`) — 是否添加特殊标记

返回

一个[`~tokenizers.Encoding`]的`List`

编码的批次

对给定的输入批次进行编码。此方法接受原始文本序列以及已经预标记的序列。

示例：

以下是一些被接受的输入示例：

```py
encode_batch([
"A single sequence",
("A tuple with a sequence", "And its pair"),
[ "A", "pre", "tokenized", "sequence" ],
([ "A", "pre", "tokenized", "sequence" ], "And its pair")
])
```

#### `from_buffer`

```py
( buffer ) → Tokenizer
```

参数

+   `buffer` (`bytes`) — 包含先前序列化的[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)的缓冲区

返回

[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)

新的分词器

从给定缓冲区实例化一个新的[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)。

#### `from_file`

```py
( path ) → Tokenizer
```

参数

+   `path` (`str`) — 表示先前序列化的[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)的本地JSON文件的路径

返回

[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)

新的分词器

从给定路径的文件实例化一个新的[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)。

#### `from_pretrained`

```py
( identifier revision = 'main' auth_token = None ) → Tokenizer
```

参数

+   `identifier` (`str`) — Hugging Face Hub上模型的标识符，其中包含一个tokenizer.json文件

+   `revision` (`str`，默认为*main*) — 分支或提交ID

+   `auth_token` (`str`，*可选*，默认为*None*) — 用于访问Hugging Face Hub上私有存储库的可选身份验证令牌

返回

[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)

新的分词器

从Hugging Face Hub上的现有文件实例化一个新的[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)。

#### `from_str`

```py
( json ) → Tokenizer
```

参数

+   `json` (`str`) — 代表先前序列化的[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)的有效JSON字符串

返回

[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)

新的分词器

从给定的JSON字符串实例化一个新的[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)。

#### `get_vocab`

```py
( with_added_tokens = True ) → Dict[str, int]
```

参数

+   `with_added_tokens` (`bool`，默认为`True`) — 是否包括添加的标记

返回

`Dict[str, int]`

词汇表

获取底层词汇表

#### `get_vocab_size`

```py
( with_added_tokens = True ) → int
```

参数

+   `with_added_tokens` (`bool`，默认为`True`) — 是否包括添加的标记

返回

`int`

词汇表的大小

获取底层词汇表的大小

#### `id_to_token`

```py
( id ) → Optional[str]
```

参数

+   `id` (`int`) — 要转换的id

返回

`Optional[str]`

一个可选的标记，如果超出词汇表则为`None`

如果存在，将给定的id转换为相应的标记

#### `no_padding`

```py
( )
```

禁用填充

#### `no_truncation`

```py
( )
```

禁用截断

#### `num_special_tokens_to_add`

```py
( is_pair )
```

返回单/配对句子将添加的特殊标记数量。:param is_pair: 指示输入是单个句子还是一对的布尔值 :return:

#### `post_process`

```py
( encoding pair = None add_special_tokens = True ) → Encoding
```

参数

+   `encoding` ([Encoding](/docs/tokenizers/v0.13.4.rc2/en/api/encoding#tokenizers.Encoding)) — 对应于主序列的[Encoding](/docs/tokenizers/v0.13.4.rc2/en/api/encoding#tokenizers.Encoding)。

+   `pair` ([Encoding](/docs/tokenizers/v0.13.4.rc2/en/api/encoding#tokenizers.Encoding)，*可选*) — 与配对序列对应的可选[Encoding](/docs/tokenizers/v0.13.4.rc2/en/api/encoding#tokenizers.Encoding)。

+   `add_special_tokens` (`bool`) — 是否添加特殊标记

返回

[Encoding](/docs/tokenizers/v0.13.4.rc2/en/api/encoding#tokenizers.Encoding)

最终的后处理编码

对给定的编码应用所有后处理步骤。

各个步骤是：

1.  根据设置的截断参数截断（使用`enable_truncation()`提供）

1.  应用`PostProcessor`

1.  根据设置的填充参数填充（使用`enable_padding()`提供）

#### `save`

```py
( path pretty = True )
```

参数

+   `path` (`str`) — 要保存序列化分词器的文件路径。

+   `pretty` (`bool`，默认为`True`) — JSON文件是否应该格式化。

将[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)保存到给定路径的文件中。

#### `to_str`

```py
( pretty = False ) → str
```

参数

+   `pretty` (`bool`，默认为`False`) — JSON字符串是否应该格式化。

返回

`str`

表示序列化Tokenizer的字符串

获取表示此[Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)的序列化字符串。

#### `token_to_id`

```py
( token ) → Optional[int]
```

参数

+   `token` (`str`) — 要转换的标记

返回

`Optional[int]`

一个可选的id，如果超出词汇表则为`None`

如果存在，将给定的标记转换为相应的id

#### `train`

```py
( files trainer = None )
```

参数

+   `files` (`List[str]`) — 应该用于训练的文件路径列表

+   `trainer` (`~tokenizers.trainers.Trainer`，*可选*) — 应该用于训练我们的模型的可选训练器

使用给定的文件训练Tokenizer。

逐行读取文件，保留所有空格，甚至包括换行符。如果要从内存中存储的数据进行训练，可以查看`train_from_iterator()`

#### `train_from_iterator`

```py
( iterator trainer = None length = None )
```

参数

+   `iterator` (`Iterator`) — 任何字符串或字符串列表的迭代器

+   `trainer` (`~tokenizers.trainers.Trainer`，*可选*) — 应该用于训练我们的模型的可选训练器

+   `length` (`int`, *可选*) — 迭代器中序列的总数。这用于提供有意义的进度跟踪

使用提供的迭代器训练分词器。

您可以提供任何Python迭代器

+   一个序列的列表`List[str]`

+   一个生成器，产生`str`或`List[str]`

+   一个字符串的Numpy数组

+   …
