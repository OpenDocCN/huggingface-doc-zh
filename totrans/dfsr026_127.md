# 流水线

> 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/overview](https://huggingface.co/docs/diffusers/api/pipelines/overview)

流水线提供了一种简单的方式来运行最先进的扩散模型，通过将所有必要的组件（多个独立训练的模型、调度器和处理器）捆绑到一个端到端类中。流水线灵活，可以适应使用不同调度器甚至模型组件。

所有流水线都是基于[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)类构建的，该类提供了加载、下载和保存所有组件的基本功能。加载具体流水线类型（例如[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)）时，使用[from_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained)会自动检测到流水线组件，并将其加载并传递给流水线的`__init__`函数。

不应该使用[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)类进行训练。扩散流水线的各个组件（例如[UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel)和[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)）通常是单独训练的，因此建议直接与它们一起工作。

流水线不提供任何训练功能。您会注意到通过在`__call__()`方法上装饰[`torch.no_grad`](https://pytorch.org/docs/stable/generated/torch.no_grad.html)装饰器来禁用PyTorch的autograd，因为流水线不应该用于训练。如果您对训练感兴趣，请查看[培训](../../training/overview)指南！

下表列出了🤗 Diffusers中当前可用的所有流水线及其支持的任务。单击流水线以查看其摘要和已发表的论文。

| 流水线 | 任务 |
| --- | --- |
| [AltDiffusion](alt_diffusion) | 图像到图像 |
| [AnimateDiff](animatediff) | 文本到视频 |
| [Attend-and-Excite](attend_and_excite) | 文本到图像 |
| [Audio Diffusion](audio_diffusion) | 图像到音频 |
| [AudioLDM](audioldm) | 文本到音频 |
| [AudioLDM2](audioldm2) | 文本到音频 |
| [BLIP Diffusion](blip_diffusion) | 文本到图像 |
| [Consistency Models](consistency_models) | 无条件图像生成 |
| [ControlNet](controlnet) | 文本到图像，图像到图像，修补 |
| [ControlNet with Stable Diffusion XL](controlnet_sdxl) | 文本到图像 |
| [ControlNet-XS](controlnetxs) | 文本到图像 |
| [ControlNet-XS with Stable Diffusion XL](controlnetxs_sdxl) | 文本到图像 |
| [Cycle Diffusion](cycle_diffusion) | 图像到图像 |
| [Dance Diffusion](dance_diffusion) | 无条件音频生成 |
| [DDIM](ddim) | 无条件图像生成 |
| [DDPM](ddpm) | 无条件图像生成 |
| [DeepFloyd IF](deepfloyd_if) | 文本到图像，图像到图像，修补，超分辨率 |
| [DiffEdit](diffedit) | 修补 |
| [DiT](dit) | 文本到图像 |
| [GLIGEN](stable_diffusion/gligen) | 文本到图像 |
| [InstructPix2Pix](pix2pix) | 图像编辑 |
| [Kandinsky 2.1](kandinsky) | 文本到图像，图像到图像，修补，插值 |
| [Kandinsky 2.2](kandinsky_v22) | 文本到图像，图像到图像，修补 |
| [Kandinsky 3](kandinsky3) | 文本到图像，图像到图像 |
| [Latent Consistency Models](latent_consistency_models) | 文本到图像 |
| [Latent Diffusion](latent_diffusion) | 文本到图像，超分辨率 |
| [LDM3D](stable_diffusion/ldm3d_diffusion) | 文本到图像，文本到3D，文本到全景，放大 |
| [MultiDiffusion](panorama) | 文本到图像 |
| [MusicLDM](musicldm) | 文本到音频 |
| [Paint by Example](paint_by_example) | 修补 |
| [ParaDiGMS](paradigms) | 文本到图像 |
| [Pix2Pix Zero](pix2pix_zero) | 图像编辑 |
| [PixArt-α](pixart) | 文本到图像 |
| [PNDM](pndm) | 无条件图像生成 |
| [重新绘制](repaint) | 补漆 |
| [得分SDE VE](score_sde_ve) | 无条件图像生成 |
| [自注意引导](self_attention_guidance) | 文本到图像 |
| [语义引导](semantic_stable_diffusion) | 文本到图像 |
| [Shap-E](shap_e) | 文本到3D，图像到3D |
| [声谱图扩散](spectrogram_diffusion) |  |
| [稳定扩散](stable_diffusion/overview) | 文本到图像，图像到图像，深度到图像，补漆，图像变化，潜在放大器，超分辨率 |
| [稳定扩散模型编辑](model_editing) | 模型编辑 |
| [稳定扩散XL](stable_diffusion/stable_diffusion_xl) | 文本到图像，图像到图像，补漆 |
| [稳定扩散XL Turbo](stable_diffusion/sdxl_turbo) | 文本到图像，图像到图像，补漆 |
| [稳定unCLIP](stable_unclip) | 文本到图像，图像变化 |
| [随机Karras VE](stochastic_karras_ve) | 无条件图像生成 |
| [T2I-Adapter](stable_diffusion/adapter) | 文本到图像 |
| [文本到视频](text_to_video) | 文本到视频，视频到视频 |
| [文本到视频-零](text_to_video_zero) | 文本到视频 |
| [unCLIP](unclip) | 文本到图像，图像变化 |
| [无条件潜在扩散](latent_diffusion_uncond) | 无条件图像生成 |
| [UniDiffuser](unidiffuser) | 文本到图像，图像到文本，图像变化，文本变化，无条件图像生成，无条件音频生成 |
| [价值导向规划](value_guided_sampling) | 价值导向采样 |
| [多功能扩散](versatile_diffusion) | 文本到图像，图像变化 |
| [VQ扩散](vq_diffusion) | 文本到图像 |
| [维斯特琴](wuerstchen) | 文本到图像 |

## 扩散管道

### `class diffusers.DiffusionPipeline`

[<源代码>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L569)

```py
( )
```

所有管道的基类。

[扩散管道](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline) 存储扩散管道的所有组件（模型、调度器和处理器），并提供加载、下载和保存模型的方法。还包括以下方法：

+   将所有PyTorch模块移动到所选设备上

+   启用/禁用去噪迭代的进度条

类属性：

+   `config_name` (`str`) — 存储扩散管道所有组件的类和模块名称的配置文件名。

+   `_optional_components` (`List[str]`) — 不必传递给管道以运行的所有可选组件的列表（应该被子类覆盖）。

#### `__call__`

```py
( *args **kwargs )
```

将自身作为函数调用。

#### `device`

[<源代码>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L901)

```py
( ) → export const metadata = 'undefined';torch.device
```

返回

`torch.device`

管道所在的torch设备。

#### `to`

[<源代码>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L742)

```py
( *args **kwargs ) → export const metadata = 'undefined';DiffusionPipeline
```

参数

+   `dtype` (`torch.dtype`, *可选*) — 返回具有指定[`dtype`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)的管道

+   `device` (`torch.Device`, *可选*) — 返回具有指定[`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device)的管道

+   `silence_dtype_warnings` (`str`, *可选*, 默认为`False`) — 是否忽略警告，如果目标`dtype`与目标`device`不兼容。

返回

[扩散管道](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)

将管道转换为指定的`dtype`和/或`dtype`。

执行管道的数据类型和/或设备转换。从`self.to(*args, **kwargs)`的参数中推断出torch.dtype和torch.device。

如果管道已经具有正确的torch.dtype和torch.device，则返回原样。否则，返回的管道是具有所需torch.dtype和torch.device的self的副本。

以下是调用`to`的方法：

+   `to(dtype, silence_dtype_warnings=False) → DiffusionPipeline` 返回具有指定[`dtype`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)的管道。

+   `to(device, silence_dtype_warnings=False) → DiffusionPipeline` 返回具有指定[`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device)的管道。

+   `to(device=None, dtype=None, silence_dtype_warnings=False) → DiffusionPipeline` 返回具有指定[`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device)和[`dtype`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)的管道。

#### `components`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L1941)

```py
( )
```

`self.components`属性可用于使用相同的权重和配置运行不同的管道，而无需重新分配额外的内存。

返回 (`dict`): 包含初始化管道所需的所有模块的字典。

示例：

```py
>>> from diffusers import (
...     StableDiffusionPipeline,
...     StableDiffusionImg2ImgPipeline,
...     StableDiffusionInpaintPipeline,
... )

>>> text2img = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
>>> img2img = StableDiffusionImg2ImgPipeline(**text2img.components)
>>> inpaint = StableDiffusionInpaintPipeline(**text2img.components)
```

#### `disable_attention_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

禁用切片注意力计算。如果之前调用了`enable_attention_slicing`，则注意力在一步中计算。

#### `disable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

禁用[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。

#### `download`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L1552)

```py
( pretrained_model_name **kwargs ) → export const metadata = 'undefined';os.PathLike
```

参数

+   `pretrained_model_name` (`str` 或 `os.PathLike`, *optional*) — 一个字符串，预训练管道的*存储库id*（例如 `CompVis/ldm-text2im-large-256`），托管在Hub上。

+   `custom_pipeline` (`str`, *optional*) — 可以是：

    +   一个字符串，预训练管道的*存储库id*（例如 `CompVis/ldm-text2im-large-256`），托管在Hub上。存储库必须包含一个名为`pipeline.py`的文件，定义了自定义管道。

    +   一个字符串，托管在GitHub的[Community](https://github.com/huggingface/diffusers/tree/main/examples/community)下的社区管道的*文件名*。有效的文件名必须匹配文件名而不是管道脚本（`clip_guided_stable_diffusion`而不是`clip_guided_stable_diffusion.py`）。社区管道始终从GitHub的当前`main`分支加载。

    +   一个指向包含自定义管道的*目录*（`./my_pipeline_directory/`）的路径。该目录必须包含一个名为`pipeline.py`的文件，定义了自定义管道。

    🧪 这是一个实验性功能，可能会在未来更改。

    有关如何加载和创建自定义管道的更多信息，请查看[如何贡献社区管道](https://huggingface.co/docs/diffusers/main/en/using-diffusers/contribute_pipeline)。

+   `force_download` (`bool`, *optional*, 默认为 `False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`, *optional*, 默认为 `False`) — 是否恢复下载模型权重和配置文件。如果设置为`False`，则删除任何未完全下载的文件。

+   `proxies` (`Dict[str, str]`, *optional*) — 一个代理服务器字典，按协议或端点使用，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。代理服务器在每个请求上使用。

+   `output_loading_info(bool,` *optional*, defaults to `False`) — 是否还返回包含缺失键、意外键和错误消息的字典。

+   `local_files_only` (`bool`, *optional*, defaults to `False`) — 是否仅加载本地模型权重和配置文件。如果设置为 `True`，则不会从 Hub 下载模型。

+   `token` (`str` or *bool*, *optional*) — 用作远程文件 HTTP bearer 授权的令牌。如果为 `True`，则使用从 `diffusers-cli login` 生成的令牌（存储在 `~/.huggingface` 中）。

+   `revision` (`str`, *optional*, defaults to `"main"`) — 使用的特定模型版本。它可以是分支名称、标签名称、提交 ID 或 Git 允许的任何标识符。

+   `custom_revision` (`str`, *optional*, defaults to `"main"`) — 使用的特定模型版本。它可以是分支名称、标签名称或与从 Hub 加载自定义流水线时的 `revision` 类似的提交 ID。从 GitHub 加载自定义流水线时，它可以是一个 🤗 Diffusers 版本，否则在从 Hub 加载时默认为 `"main"`。

+   `mirror` (`str`, *optional*) — 如果您在中国下载模型时遇到可访问性问题，请使用镜像源解决。我们不保证源的及时性或安全性，您应参考镜像站点获取更多信息。

+   `variant` (`str`, *optional*) — 从指定的变体文件名加载权重，例如 `"fp16"` 或 `"ema"`。在加载 `from_flax` 时会忽略此选项。

+   `use_safetensors` (`bool`, *optional*, defaults to `None`) — 如果设置为 `None`，则在可用时下载 safetensors 权重 **并且** 如果已安装 safetensors 库。如果设置为 `True`，则强制从 safetensors 权重加载模型。如果设置为 `False`，则不加载 safetensors 权重。

+   `use_onnx` (`bool`, *optional*, defaults to `False`) — 如果设置为 `True`，则始终会下载 ONNX 权重（如果存在）。如果设置为 `False`，则永远不会下载 ONNX 权重。默认情况下，`use_onnx` 默认为 `_is_onnx` 类属性，对于非 ONNX 流水线为 `False`，对于 ONNX 流水线为 `True`。ONNX 权重包括以 `.onnx` 和 `.pb` 结尾的文件。

+   `trust_remote_code` (`bool`, *optional*, defaults to `False`) — 是否允许在 Hub 上定义的自定义流水线和组件在其自己的文件中执行。此选项应仅对您信任的存储库设置为 `True`，并且您已经阅读了代码，因为它将在本地机器上执行 Hub 上存在的代码。

返回

`os.PathLike`

下载的流水线路径。

从预训练的流水线权重下载并缓存 PyTorch 扩散流水线。

要使用私有或 [门控模型](https://huggingface.co/docs/hub/models-gated#gated-models)，请使用 `huggingface-cli login` 登录。

#### `enable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

参数

+   `slice_size` (`str` or `int`, *optional*, defaults to `"auto"`) — 当为 `"auto"` 时，将输入减半到注意力头，因此注意力将分两步计算。如果为 `"max"`，将通过一次只运行一个切片来节省最大内存量。如果提供一个数字，则使用 `attention_head_dim // slice_size` 个切片。在这种情况下，`attention_head_dim` 必须是 `slice_size` 的倍数。

启用切片注意力计算。启用此选项时，注意力模块将输入张量分割成多个切片，以便在几个步骤中计算注意力。对于多个注意力头，计算将按顺序在每个头上执行。这对于节省一些内存以换取一点速度降低很有用。

⚠️ 如果您已经在使用 PyTorch 2.0 或 xFormers 中的 `scaled_dot_product_attention`（SDPA），请不要启用注意力切片。这些注意力计算已经非常内存高效，因此您不需要启用此功能。如果您在 SDPA 或 xFormers 中启用了注意力切片，可能会导致严重的减速！

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

#### `enable_model_cpu_offload`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L1410)

```py
( gpu_id: Optional = None device: Union = 'cuda' )
```

参数

+   `gpu_id` (`int`，*可选*) — 用于推理的加速器的ID。如果未指定，它将默认为0。

+   `device` (`torch.Device`或`str`，*可选*，默认为“cuda”) — 用于推理的加速器的PyTorch设备类型。如果未指定，它将默认为“cuda”。

使用加速器将所有模型转移到CPU，减少内存使用量对性能影响较小。与`enable_sequential_cpu_offload`相比，此方法在调用其`forward`方法时一次将整个模型移动到GPU，并且模型保持在GPU中，直到下一个模型运行。与`enable_sequential_cpu_offload`相比，内存节省较低，但由于`unet`的迭代执行，性能要好得多。

#### `enable_sequential_cpu_offload`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L1499)

```py
( gpu_id: Optional = None device: Union = 'cuda' )
```

参数

+   `gpu_id` (`int`，*可选*) — 用于推理的加速器的ID。如果未指定，它将默认为0。

+   `device` (`torch.Device`或`str`，*可选*，默认为“cuda”) — 用于推理的加速器的PyTorch设备类型。如果未指定，它将默认为“cuda”。

使用🤗 Accelerate将所有模型转移到CPU，显著减少内存使用。当调用时，所有`torch.nn.Module`组件的状态字典（除了`self._exclude_from_cpu_offload`中的组件）将保存到CPU，然后移动到`torch.device('meta')`，仅当其特定子模块调用其`forward`方法时才加载到GPU。卸载是基于子模块的。与`enable_model_cpu_offload`相比，内存节省更高，但性能较低。

#### `enable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

参数

+   `attention_op` (`Callable`，*可选*) — 覆盖用作xFormers的[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)函数的`op`参数的默认`None`运算符。

启用[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。启用此选项时，您应该观察到GPU内存使用量降低，并在推理过程中潜在加速。训练过程中的加速不被保证。

⚠️ 当内存高效注意力和切片注意力同时启用时，内存高效注意力优先。

示例：

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `from_pretrained`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L931)

```py
( pretrained_model_name_or_path: Union **kwargs )
```

参数

+   `pretrained_model_name_or_path` (`str`或`os.PathLike`，*可选*) — 可以是：

    +   一个字符串，托管在Hub上的预训练流水线的*repo id*（例如`CompVis/ldm-text2im-large-256`）。

    +   一个指向使用[save_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline.save_pretrained)保存的流水线权重的*目录*的路径（例如`./my_pipeline_directory/`）。

+   `torch_dtype` (`str`或`torch.dtype`，*可选*) — 覆盖默认的`torch.dtype`并使用另一种dtype加载模型。如果传递“auto”，则dtype将自动从模型的权重中派生。

+   `custom_pipeline` (`str`, *可选*) —

    🧪 这是一个实验性功能，可能会在未来发生变化。

    可以是：

    +   一个字符串，托管在Hub上的自定义流水线的*repo id*（例如`hf-internal-testing/diffusers-dummy-pipeline`）。存储库必须包含一个名为pipeline.py的文件，定义了自定义流水线。

    +   一个字符串，托管在GitHub上的社区管道的*文件名*，位于[Community](https://github.com/huggingface/diffusers/tree/main/examples/community)下。有效的文件名必须与文件名匹配，而不是管道脚本（`clip_guided_stable_diffusion`而不是`clip_guided_stable_diffusion.py`）。社区管道始终从GitHub的当前主分支加载。

    +   一个包含自定义管道的目录的路径（`./my_pipeline_directory/`）。该目录必须包含一个名为`pipeline.py`的文件，定义了自定义管道。

    有关如何加载和创建自定义管道的更多信息，请查看[加载和添加自定义管道](https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview)

+   `force_download` (`bool`, *optional*, 默认为`False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `cache_dir` (`Union[str, os.PathLike]`, *optional*) — 下载预训练模型配置的缓存路径，如果未使用标准缓存。

+   `resume_download` (`bool`, *optional*, 默认为`False`) — 是否恢复下载模型权重和配置文件。如果设置为`False`，则会删除任何未完全下载的文件。

+   `proxies` (`Dict[str, str]`, *optional*) — 一个按协议或端点使用的代理服务器字典，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。这些代理在每个请求中使用。

+   `output_loading_info(bool,` *optional*, 默认为`False`) — 是否还返回一个包含缺失键、意外键和错误消息的字典。

+   `local_files_only` (`bool`, *optional*, 默认为`False`) — 是否仅加载本地模型权重和配置文件。如果设置为`True`，则模型不会从Hub下载。

+   `token` (`str`或*bool*, *optional*) — 用作远程文件的HTTP bearer授权的令牌。如果为`True`，则使用从`diffusers-cli login`生成的令牌（存储在`~/.huggingface`中）。

+   `revision` (`str`, *optional*, 默认为`"main"`) — 要使用的特定模型版本。它可以是分支名称、标签名称、提交ID或Git允许的任何标识符。

+   `custom_revision` (`str`, *optional*, 默认为`"main"`) — 要使用的特定模型版本。它可以是分支名称、标签名称或与从Hub加载自定义管道时的`revision`类似的提交ID。从GitHub加载自定义管道时可以是🤗 Diffusers版本，否则在从Hub加载时默认为`"main"`。

+   `mirror` (`str`, *optional*) — 镜像源，用于解决在中国下载模型时的可访问性问题。我们不保证源的及时性或安全性，您应参考镜像站点获取更多信息。

+   `device_map` (`str`或`Dict[str, Union[int, str, torch.device]]`, *optional*) — 指定每个子模块应该去的地方的映射。不需要为每个参数/缓冲区名称定义它；一旦给定模块名称在内部，它的每个子模块都将被发送到相同的设备。

    设置`device_map="auto"`以使🤗 Accelerate自动计算最优化的`device_map`。有关每个选项的更多信息，请参阅[设计设备映射](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map)。

+   `max_memory` (`Dict`, *optional*) — 一个字典设备标识符，用于最大内存。如果未设置，将默认为每个GPU和可用CPU RAM的最大内存。

+   `offload_folder` (`str`或`os.PathLike`, *optional*) — 如果`device_map`包含值`"disk"`，则是卸载权重的路径。

+   `offload_state_dict` (`bool`, *optional*) — 如果为 `True`，暂时将CPU状态字典卸载到硬盘，以避免CPU RAM不足，如果CPU状态字典的重量 + 检查点的最大分片重量不适合。当存在一些磁盘卸载时，默认为 `True`。

+   `low_cpu_mem_usage` (`bool`, *optional*, defaults to `True` if torch version >= 1.9.0 else `False`) — 仅加载预训练权重而不初始化权重，加快模型加载速度。同时尝试在加载模型时不使用超过1倍模型大小的CPU内存（包括峰值内存）。仅支持PyTorch >= 1.9.0。如果您使用较旧版本的PyTorch，将此参数设置为 `True` 将引发错误。

+   `use_safetensors` (`bool`, *optional*, defaults to `None`) — 如果设置为 `None`，则在可用时下载safetensors权重 **并且** 如果已安装safetensors库。如果设置为 `True`，则强制从safetensors权重加载模型。如果设置为 `False`，则不加载safetensors权重。

+   `use_onnx` (`bool`, *optional*, defaults to `None`) — 如果设置为 `True`，则始终下载ONNX权重（如果存在）。如果设置为 `False`，则永远不会下载ONNX权重。默认情况下，`use_onnx` 默认为 `_is_onnx` 类属性，对于非ONNX管道为 `False`，对于ONNX管道为 `True`。ONNX权重包括以 `.onnx` 和 `.pb` 结尾的文件。

+   `kwargs`（剩余的关键字参数字典，*optional*） — 可用于覆盖加载和可保存变量（特定管道类的管道组件）。被覆盖的组件直接传递给管道的 `__init__` 方法。有关更多信息，请参见下面的示例。

+   `variant` (`str`, *optional*) — 从指定的变体文件名（如 `"fp16"` 或 `"ema"`）加载权重。在加载 `from_flax` 时会被忽略。

从预训练的管道权重实例化一个PyTorch扩散管道。

默认情况下，管道设置为评估模式（`model.eval()`）。

如果收到下面的错误消息，则需要为下游任务微调权重：

```py
Some weights of UNet2DConditionModel were not initialized from the model checkpoint at runwayml/stable-diffusion-v1-5 and are newly initialized because the shapes did not match:
- conv_in.weight: found shape torch.Size([320, 4, 3, 3]) in the checkpoint and torch.Size([320, 9, 3, 3]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

要使用私有或[门控](https://huggingface.co/docs/hub/models-gated#gated-models)模型，请使用 `huggingface-cli login` 登录。

示例：

```py
>>> from diffusers import DiffusionPipeline

>>> # Download pipeline from huggingface.co and cache.
>>> pipeline = DiffusionPipeline.from_pretrained("CompVis/ldm-text2im-large-256")

>>> # Download pipeline that requires an authorization token
>>> # For more information on access tokens, please refer to this section
>>> # of the documentation](https://huggingface.co/docs/hub/security-tokens)
>>> pipeline = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")

>>> # Use a different scheduler
>>> from diffusers import LMSDiscreteScheduler

>>> scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)
>>> pipeline.scheduler = scheduler
```

#### `maybe_free_model_hooks`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L1480)

```py
( )
```

函数将卸载所有组件，删除在使用 `enable_model_cpu_offload` 时添加的所有模型钩子，然后再次应用它们。如果模型尚未被卸载，则此函数不执行任何操作。确保将此函数添加到管道的 `__call__` 函数的末尾，以便在应用 `enable_model_cpu_offload` 时正确运行。

#### `numpy_to_pil`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L1977)

```py
( images )
```

将NumPy图像或一批图像转换为PIL图像。

#### `save_pretrained`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L624)

```py
( save_directory: Union safe_serialization: bool = True variant: Optional = None push_to_hub: bool = False **kwargs )
```

参数

+   `save_directory` (`str` 或 `os.PathLike`) — 保存管道的目录。如果目录不存在，将会创建。

+   `safe_serialization` (`bool`, *optional*, defaults to `True`) — 是否使用 `safetensors` 或传统的PyTorch方式（使用 `pickle`）保存模型。

+   `variant` (`str`, *optional*) — 如果指定，权重将以 `pytorch_model.<variant>.bin` 格式保存。

+   `push_to_hub` (`bool`, *optional*, defaults to `False`) — 是否在保存模型后将其推送到Hugging Face模型中心。您可以使用 `repo_id` 指定要推送到的存储库（将默认为您的命名空间中的 `save_directory` 名称）。

+   `kwargs` (`Dict[str, Any]`, *optional*) — 传递给 [push_to_hub()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.utils.PushToHubMixin.push_to_hub) 方法的额外关键字参数。

将管道的所有可保存变量保存到一个目录中。如果一个管道变量的类实现了保存和加载方法，则可以保存和加载该变量。可以使用[from_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained)类方法轻松重新加载管道。

## FlaxDiffusionPipeline

### `class diffusers.FlaxDiffusionPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_flax_utils.py#L101)

```py
( )
```

基于Flax的管道的基类。

[FlaxDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.FlaxDiffusionPipeline)存储了扩散管道的所有组件（模型、调度器和处理器），并提供了加载、下载和保存模型的方法。它还包括用于：

+   启用/禁用去噪迭代的进度条

类属性：

+   `config_name` (`str`) — 存储所有扩散管道组件的类和模块名称的配置文件名。

#### `from_pretrained`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_flax_utils.py#L229)

```py
( pretrained_model_name_or_path: Union **kwargs )
```

参数

+   `pretrained_model_name_or_path` (`str`或`os.PathLike`, *optional*) — 可以是：

    +   一个字符串，预训练管道在Hub上托管的*repo id*（例如`runwayml/stable-diffusion-v1-5`）。

    +   一个*目录*的路径（例如`./my_model_directory`），其中包含使用[save_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.FlaxDiffusionPipeline.save_pretrained)保存的模型权重。

+   `dtype` (`str`或`jnp.dtype`, *optional*) — 覆盖默认的`jnp.dtype`并在此dtype下加载模型。如果是`"auto"`，则dtype会根据模型的权重自动推导。

+   `force_download` (`bool`, *optional*, defaults to `False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`, *optional*, defaults to `False`) — 是否恢复下载模型权重和配置文件。如果设置为`False`，则会删除任何未完全下载的文件。

+   `proxies` (`Dict[str, str]`, *optional*) — 一个代理服务器字典，按协议或端点使用，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。这些代理会在每个请求中使用。

+   `output_loading_info(bool,` *optional*, defaults to `False`) — 是否返回包含缺失键、意外键和错误消息的字典。

+   `local_files_only` (`bool`, *optional*, defaults to `False`) — 是否仅加载本地模型权重和配置文件。如果设置为`True`，则不会从Hub下载模型。

+   `token` (`str`或*bool*, *optional*) — 用作远程文件的HTTP bearer授权的令牌。如果为`True`，则使用从`diffusers-cli login`生成的令牌（存储在`~/.huggingface`）。

+   `revision` (`str`, *optional*, defaults to `"main"`) — 要使用的特定模型版本。可以是分支名称、标签名称、提交ID或Git允许的任何标识符。

+   `mirror` (`str`, *optional*) — 镜像源以解决在中国下载模型时的可访问性问题。我们不保证源的及时性或安全性，您应参考镜像站点获取更多信息。

+   `kwargs`（剩余的关键字参数字典，*optional*） — 可以用来覆盖特定管道类的加载和可保存变量（管道组件）。被覆盖的组件直接传递给管道的`__init__`方法。

从预训练的管道权重实例化一个基于Flax的扩散管道。

默认情况下，管道设置为评估模式（`model.eval()`），并且关闭了dropout模块。

如果收到下面的错误消息，则需要为下游任务微调权重：

```py
Some weights of FlaxUNet2DConditionModel were not initialized from the model checkpoint at runwayml/stable-diffusion-v1-5 and are newly initialized because the shapes did not match:
```

要使用私有或[门控模型](https://huggingface.co/docs/hub/models-gated#gated-models)，请使用 `huggingface-cli login` 登录。

示例:

```py
>>> from diffusers import FlaxDiffusionPipeline

>>> # Download pipeline from huggingface.co and cache.
>>> # Requires to be logged in to Hugging Face hub,
>>> # see more in [the documentation](https://huggingface.co/docs/hub/security-tokens)
>>> pipeline, params = FlaxDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     revision="bf16",
...     dtype=jnp.bfloat16,
... )

>>> # Download pipeline, but use a different scheduler
>>> from diffusers import FlaxDPMSolverMultistepScheduler

>>> model_id = "runwayml/stable-diffusion-v1-5"
>>> dpmpp, dpmpp_state = FlaxDPMSolverMultistepScheduler.from_pretrained(
...     model_id,
...     subfolder="scheduler",
... )

>>> dpm_pipe, dpm_params = FlaxStableDiffusionPipeline.from_pretrained(
...     model_id, revision="bf16", dtype=jnp.bfloat16, scheduler=dpmpp
... )
>>> dpm_params["scheduler"] = dpmpp_state
```

#### `numpy_to_pil`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_flax_utils.py#L588)

```py
( images )
```

将 NumPy 图像或一批图像转换为 PIL 图像。

#### `save_pretrained`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_flax_utils.py#L151)

```py
( save_directory: Union params: Union push_to_hub: bool = False **kwargs )
```

参数

+   `save_directory` (`str` 或 `os.PathLike`) — 要保存到的目录。如果不存在，将会创建。

+   `push_to_hub` (`bool`, *optional*, 默认为 `False`) — 在保存模型后是否将其推送到 Hugging Face 模型中心。您可以使用 `repo_id` 指定要推送到的存储库（将默认为您的命名空间中的 `save_directory` 名称）。

+   `kwargs` (`Dict[str, Any]`, *optional*) — 传递给 [push_to_hub()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.utils.PushToHubMixin.push_to_hub) 方法的额外关键字参数。

将管道的所有可保存变量保存到一个目录中。如果管道的类实现了保存和加载方法，则可以保存和加载管道变量。可以使用 [from_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.FlaxDiffusionPipeline.from_pretrained) 类方法轻松重新加载管道。

## PushToHubMixin

### `class diffusers.utils.PushToHubMixin`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/utils/hub_utils.py#L351)

```py
( )
```

一个用于将模型、调度器或管道推送到 Hugging Face Hub 的 Mixin。

#### `push_to_hub`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/utils/hub_utils.py#L380)

```py
( repo_id: str commit_message: Optional = None private: Optional = None token: Optional = None create_pr: bool = False safe_serialization: bool = True variant: Optional = None )
```

参数

+   `repo_id` (`str`) — 您要将模型、调度器或管道文件推送到的存储库的名称。当推送到组织时，它应包含您的组织名称。`repo_id` 也可以是本地目录的路径。

+   `commit_message` (`str`, *optional*) — 推送时要提交的消息。默认为 `"Upload {object}"`。

+   `private` (`bool`, *optional*) — 是否创建的存储库应为私有。

+   `token` (`str`, *optional*) — 用作远程文件的 HTTP bearer 授权的令牌。在运行 `huggingface-cli login` 时生成的令牌（存储在 `~/.huggingface` 中）。

+   `create_pr` (`bool`, *optional*, 默认为 `False`) — 是否创建一个包含上传文件的 PR 或直接提交。

+   `safe_serialization` (`bool`, *optional*, 默认为 `True`) — 是否将模型权重转换为 `safetensors` 格式。

+   `variant` (`str`, *optional*) — 如果指定，权重将以 `pytorch_model.<variant>.bin` 格式保存。

将模型、调度器或管道文件上传到 🤗 Hugging Face Hub。

示例:

```py
from diffusers import UNet2DConditionModel

unet = UNet2DConditionModel.from_pretrained("stabilityai/stable-diffusion-2", subfolder="unet")

# Push the `unet` to your namespace with the name "my-finetuned-unet".
unet.push_to_hub("my-finetuned-unet")

# Push the `unet` to an organization with the name "my-finetuned-unet".
unet.push_to_hub("your-org/my-finetuned-unet")
```
