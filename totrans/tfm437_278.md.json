["```py\n( num_channels = 3 image_size = 256 patch_size = 2 expand_ratio = 2.0 hidden_act = 'swish' conv_kernel_size = 3 output_stride = 32 classifier_dropout_prob = 0.1 initializer_range = 0.02 layer_norm_eps = 1e-05 aspp_out_channels = 512 atrous_rates = [6, 12, 18] aspp_dropout_prob = 0.1 semantic_loss_ignore_index = 255 n_attn_blocks = [2, 4, 3] base_attn_unit_dims = [128, 192, 256] width_multiplier = 1.0 ffn_multiplier = 2 attn_dropout = 0.0 ffn_dropout = 0.0 **kwargs )\n```", "```py\n>>> from transformers import MobileViTV2Config, MobileViTV2Model\n\n>>> # Initializing a mobilevitv2-small style configuration\n>>> configuration = MobileViTV2Config()\n\n>>> # Initializing a model from the mobilevitv2-small style configuration\n>>> model = MobileViTV2Model(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config: MobileViTV2Config expand_output: bool = True )\n```", "```py\n( pixel_values: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, MobileViTV2Model\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"apple/mobilevitv2-1.0-imagenet1k-256\")\n>>> model = MobileViTV2Model.from_pretrained(\"apple/mobilevitv2-1.0-imagenet1k-256\")\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 512, 8, 8]\n```", "```py\n( config: MobileViTV2Config )\n```", "```py\n( pixel_values: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutputWithNoAttention or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, MobileViTV2ForImageClassification\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"apple/mobilevitv2-1.0-imagenet1k-256\")\n>>> model = MobileViTV2ForImageClassification.from_pretrained(\"apple/mobilevitv2-1.0-imagenet1k-256\")\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_label = logits.argmax(-1).item()\n>>> print(model.config.id2label[predicted_label])\ntabby, tabby cat\n```", "```py\n( config: MobileViTV2Config )\n```", "```py\n( pixel_values: Optional = None labels: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SemanticSegmenterOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import requests\n>>> import torch\n>>> from PIL import Image\n>>> from transformers import AutoImageProcessor, MobileViTV2ForSemanticSegmentation\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"apple/mobilevitv2-1.0-imagenet1k-256\")\n>>> model = MobileViTV2ForSemanticSegmentation.from_pretrained(\"apple/mobilevitv2-1.0-imagenet1k-256\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> # logits are of shape (batch_size, num_labels, height, width)\n>>> logits = outputs.logits\n```"]