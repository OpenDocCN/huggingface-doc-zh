["```py\npip install accelerate\n```", "```py\naccelerate config\n```", "```py\n# directory containing checkpoints\naccelerator.load_state(\"ckpt\")\n```", "```py\nif trainer.is_fsdp_enabled:\n    trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n\ntrainer.save_model(script_args.output_dir)\n```", "```py\nxla: True # must be set to True to enable PyTorch/XLA\nxla_fsdp_settings: # XLA-specific FSDP parameters\nxla_fsdp_grad_ckpt: True # use gradient checkpointing\n```", "```py\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndistributed_type: FSDP\ndowncast_bf16: 'no'\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch_policy: BACKWARD_PRE\n  fsdp_cpu_ram_efficient_loading: true\n  fsdp_forward_prefetch: false\n  fsdp_offload_params: true\n  fsdp_sharding_strategy: 1\n  fsdp_state_dict_type: SHARDED_STATE_DICT\n  fsdp_sync_module_states: true\n  fsdp_transformer_layer_cls_to_wrap: BertLayer\n  fsdp_use_orig_params: true\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```", "```py\naccelerate launch my-trainer-script.py\n```", "```py\naccelerate launch --fsdp=\"full shard\" --fsdp_config=\"path/to/fsdp_config/ my-trainer-script.py\n```"]