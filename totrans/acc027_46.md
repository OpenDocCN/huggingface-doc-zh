# 处理大型模型

> 原始文本：[https://huggingface.co/docs/accelerate/package_reference/big_modeling](https://huggingface.co/docs/accelerate/package_reference/big_modeling)

## 调度和卸载模型

#### `accelerate.init_empty_weights`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/big_modeling.py#L53)

```py
( include_buffers: bool = None )
```

参数

+   `include_buffers` (`bool`, *可选*) — 是否在初始化时也将所有缓冲区放在元设备上。

一个上下文管理器，在其中模型使用元设备上的所有参数进行初始化，因此创建一个空模型。当仅初始化模型会消耗掉可用的RAM时很有用。

示例：

```py
import torch.nn as nn
from accelerate import init_empty_weights

# Initialize a model with 100 billions parameters in no time and without using any RAM.
with init_empty_weights():
    tst = nn.Sequential(*[nn.Linear(10000, 10000) for _ in range(1000)])
```

在此上下文管理器下创建的任何模型都没有权重。因此，您不能像`model.to(some_device)`这样使用它。要在空模型中加载权重，请参阅[load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch)。确保覆盖[load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch)的默认device_map参数，否则不会调用分发。

#### `accelerate.cpu_offload`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/big_modeling.py#L167)

```py
( model: Module execution_device: Optional = None offload_buffers: bool = False state_dict: Optional = None preload_module_classes: Optional = None )
```

参数

+   `model` (`torch.nn.Module`) — 要卸载的模型。

+   `execution_device` (`torch.device`, *可选*) — 模型前向传播将在其上执行的设备（应为GPU）。将默认为模型的第一个参数设备。

+   `offload_buffers` (`bool`, *可选*, 默认为`False`) — 是否将缓冲区与模型参数一起卸载。

+   `state_dict` (`Dict[str, torch.Tensor]`, *可选*) — 将保留在CPU上的模型的状态字典。

+   `preload_module_classes` (`List[str]`, *可选*) — 一个类的列表，其实例应在前向传播的开始时加载所有权重（即使在子模块中）。这仅应用于具有已注册但在前向传播期间未直接调用的子模块的类，例如，如果注册了一个`dense`线性层，但在前向传播期间，一些操作中使用`dense.weight`和`dense.bias`而不是直接调用`dense`。

激活模型的完全CPU卸载。因此，模型的所有参数将被卸载，并且模型的状态字典的副本将仅保留一份。在前向传播过程中，参数将从该状态字典中提取，并在需要时放在传递的执行设备上，然后再次卸载。

#### `accelerate.cpu_offload_with_hook`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/big_modeling.py#L213)

```py
( model: Module execution_device: Union = None prev_module_hook: Optional = None )
```

参数

+   `model` (`torch.nn.Module`) — 要卸载的模型。

+   `execution_device(str,` `int`或`torch.device`, *可选*) — 模型应在其上执行的设备。如果可用，将默认为MPS设备，然后是GPU 0，最后是CPU。

+   `prev_module_hook` (`UserCpuOffloadHook`, *可选*) — 由此函数返回的用于您正在运行的管道中的先前模型的挂钩。如果传递，其卸载方法将在附加此挂钩的模型的前向传播之前调用。

将模型卸载到CPU上，并在执行时将其放回执行设备。与[cpu_offload()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.cpu_offload)的区别在于，模型在前向传播后仍保留在执行设备上，并且仅在调用返回的`hook`的`offload`方法时再次卸载。适用于在循环中运行模型的管道。

示例：

```py
model_1, hook_1 = cpu_offload_with_hook(model_1, cuda_device)
model_2, hook_2 = cpu_offload_with_hook(model_2, cuda_device, prev_module_hook=hook_1)
model_3, hook_3 = cpu_offload_with_hook(model_3, cuda_device, prev_module_hook=hook_2)

hid_1 = model_1(input)
for i in range(50):
    # model1 is offloaded on the CPU at the first iteration, model 2 stays on the GPU for this whole loop.
    hid_2 = model_2(hid_1)
# model2 is offloaded to the CPU just before this forward.
hid_3 = model_3(hid_3)

# For model3, you need to manually call the hook offload method.
hook_3.offload()
```

#### `accelerate.disk_offload`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/big_modeling.py#L257)

```py
( model: Module offload_dir: Union execution_device: Optional = None offload_buffers: bool = False preload_module_classes: Optional = None )
```

参数

+   `model` (`torch.nn.Module`) — 要卸载的模型。

+   `offload_dir` (`str` 或 `os.PathLike`) — 要卸载模型权重的文件夹（或者模型权重已经被卸载的位置）。

+   `execution_device` (`torch.device`, *可选*) — 将执行模型前向传播的设备（应为GPU）。将默认为模型的第一个参数设备。

+   `offload_buffers` (`bool`, *可选*, 默认为`False`) — 是否卸载带有模型参数的缓冲区。

+   `preload_module_classes` (`List[str]`, *可选*) — 应该在前向传播开始时加载所有权重的类的列表（即使在子模块中）。这仅应用于具有已注册但在前向传播期间未直接调用的子模块的类，例如，如果注册了`dense`线性层，但在前向传播期间，某些操作中使用`dense.weight`和`dense.bias`而不是直接调用`dense`。

激活模型的完整磁盘卸载。结果，模型的所有参数将作为内存映射数组在给定文件夹中卸载。在前向传播过程中，参数将从该文件夹访问，并根据需要放在传递的执行设备上，然后再次卸载。

#### `accelerate.dispatch_model`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/big_modeling.py#L303)

```py
( model: Module device_map: Dict main_device: Optional = None state_dict: Optional = None offload_dir: Union = None offload_index: Optional = None offload_buffers: bool = False skip_keys: Union = None preload_module_classes: Optional = None force_hooks: bool = False )
```

参数

+   `model` (`torch.nn.Module`) — 要分发的模型。

+   `device_map` (`Dict[str, Union[str, int, torch.device]]`) — 将模型`state_dict`中的模块名称映射到它们应该去的设备的字典。请注意，即使`"disk"`不是`torch.device`的正确值，也会被接受。

+   `main_device` (`str`, `int` 或 `torch.device`, *可选*) — 主要执行设备。将默认为`device_map`中第一个不是`"cpu"`或`"disk"`的设备。

+   `state_dict` (`Dict[str, torch.Tensor]`, *可选*) — 将保留在CPU上的模型部分的状态字典。

+   `offload_dir` (`str` 或 `os.PathLike`) — 要卸载模型权重的文件夹（或者模型权重已经被卸载的位置）。

+   `offload_index` (`Dict`, *可选*) — 从权重名称到它们的信息（`dtype`/ `shape`或safetensors文件名）的字典。将默认为保存在`save_folder`中的索引。

+   `offload_buffers` (`bool`, *可选*, 默认为`False`) — 是否卸载带有模型参数的缓冲区。

+   `skip_keys` (`str` 或 `List[str]`, *可选*) — 在移动输入或输出之间时要忽略的键的列表。

+   `preload_module_classes` (`List[str]`, *可选*) — 应该在前向传播开始时加载所有权重的类的列表（即使在子模块中）。这仅应用于具有已注册但在前向传播期间未直接调用的子模块的类，例如，如果注册了`dense`线性层，但在前向传播期间，某些操作中使用`dense.weight`和`dense.bias`而不是直接调用`dense`。

+   `force_hooks` (`bool`, *可选*, 默认为`False`) — 是否强制将设备钩子附加到模型，即使所有层都被分发到单个设备。

根据给定的设备映射分发模型。模型的层可能分布在多个GPU上，在CPU上卸载，甚至在磁盘上。

#### `accelerate.load_checkpoint_and_dispatch`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/big_modeling.py#L478)

```py
( model: Module checkpoint: Union device_map: Union = None max_memory: Optional = None no_split_module_classes: Optional = None offload_folder: Union = None offload_buffers: bool = False dtype: Union = None offload_state_dict: Optional = None skip_keys: Union = None preload_module_classes: Optional = None force_hooks: bool = False )
```

参数

+   `model` (`torch.nn.Module`) — 我们想要加载检查点的模型。

+   `checkpoint` (`str` 或 `os.PathLike`) — 要加载的文件夹检查点。可以是：

    +   包含整个模型状态字典的文件路径

    +   包含分片检查点索引的`.json`文件的路径

    +   包含唯一的`.index.json`文件和检查点分片的文件夹路径。

+   `device_map` (`Dict[str, Union[int, str, torch.device]]`，*可选*) — 一个指定每个子模块应该去哪里的映射。它不需要被细化到每个参数/缓冲器名称，一旦给定模块名称在内，它的每个子模块都将被发送到相同的设备。

    要让 Accelerate 自动计算最优化的 `device_map`，请设置 `device_map="auto"`。有关每个选项的更多信息，请参见[这里](../concept_guides/big_model_inference#designing-a-device-map)。默认为 None，这意味着不会调用 [dispatch_model()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.dispatch_model)。

+   `max_memory` (`Dict`，*可选*) — 一个字典设备标识符到最大内存的映射。如果未设置，将默认为每个 GPU 和可用 CPU RAM 的最大内存。

+   `no_split_module_classes` (`List[str]`，*可选*) — 一个不应该跨设备分割的层类名称列表（例如，任何具有残差连接的层）。

+   `offload_folder` (`str` 或 `os.PathLike`，*可选*) — 如果 `device_map` 包含任何值 `"disk"`，则是我们将卸载权重的文件夹。

+   `offload_buffers` (`bool`，*可选*，默认为 `False`) — 在被卸载到 CPU 或硬盘上的层中，是否同时卸载缓冲区和参数。

+   `dtype` (`str` 或 `torch.dtype`，*可选*) — 如果提供，加载时权重将转换为该类型。

+   `offload_state_dict` (`bool`，*可选*) — 如果为 `True`，将临时将 CPU 状态字典卸载到硬盘上，以避免如果 CPU 状态字典的权重 + 最大分片的权重不适合 CPU RAM 时会超出 CPU RAM。如果选定的设备映射包含 `"disk"` 值，则默认为 `True`。

+   `skip_keys` (`str` 或 `List[str]`，*可选*) — 在移动输入或输出之间时要忽略的键列表。

+   `preload_module_classes` (`List[str]`，*可选*) — 一个类的列表，其实例应该在前向传播的开始时加载所有它们的权重（甚至在子模块中）。这应该仅用于具有子模块的类，这些子模块已注册但在前向传播期间没有直接调用，例如，如果一个 `dense` 线性层已注册，但在前向传播时，一些操作中使用 `dense.weight` 和 `dense.bias` 而不是直接调用 `dense`。

+   `force_hooks` (`bool`，*可选*，默认为 `False`) — 是否强制将设备钩子附加到模型，即使所有层都分派到单个设备。

在模型中加载（可能是分片的）检查点，可能在加载时将权重发送到给定设备，并添加各种钩子，使该模型能够正确运行（即使跨设备分割）。

示例：

```py
>>> from accelerate import init_empty_weights, load_checkpoint_and_dispatch
>>> from huggingface_hub import hf_hub_download
>>> from transformers import AutoConfig, AutoModelForCausalLM

>>> # Download the Weights
>>> checkpoint = "EleutherAI/gpt-j-6B"
>>> weights_location = hf_hub_download(checkpoint, "pytorch_model.bin")

>>> # Create a model and initialize it with empty weights
>>> config = AutoConfig.from_pretrained(checkpoint)
>>> with init_empty_weights():
...     model = AutoModelForCausalLM.from_config(config)

>>> # Load the checkpoint and dispatch it to the right devices
>>> model = load_checkpoint_and_dispatch(
...     model, weights_location, device_map="auto", no_split_module_classes=["GPTJBlock"]
... )
```

#### `accelerate.load_checkpoint_in_model`

[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1442)

```py
( model: Module checkpoint: Union device_map: Optional = None offload_folder: Union = None dtype: Union = None offload_state_dict: bool = False offload_buffers: bool = False keep_in_fp32_modules: List = None offload_8bit_bnb: bool = False )
```

参数

+   `model` (`torch.nn.Module`) — 我们想要加载检查点的模型。

+   `checkpoint` (`str` 或 `os.PathLike`) — 要加载的文件夹检查点。它可以是：

    +   包含整个模型状态字典的文件的路径

    +   包含分片检查点索引的 `.json` 文件的路径

    +   包含唯一的 `.index.json` 文件和检查点分片的文件夹路径。

    +   包含唯一的 pytorch_model.bin 或 model.safetensors 文件的文件夹路径。

+   `device_map` (`Dict[str, Union[int, str, torch.device]]`，*可选*) — 一个指定每个子模块应该去哪里的映射。它不需要被细化到每个参数/缓冲器名称，一旦给定模块名称在内，它的每个子模块都将被发送到相同的设备。

+   `offload_folder` (`str` 或 `os.PathLike`，*可选*) — 如果 `device_map` 包含任何值 `"disk"`，则是我们将卸载权重的文件夹。

+   `dtype` (`str` 或 `torch.dtype`，*可选*) — 如果提供，加载时权重将转换为该类型。

+   `offload_state_dict`（`bool`，*可选*，默认为`False`）— 如果为`True`，将临时将CPU状态字典卸载到硬盘上，以避免CPU RAM不足，如果CPU状态字典的重量+最大碎片的重量不适合。

+   `offload_buffers`（`bool`，*可选*，默认为`False`）— 是否在卸载到磁盘的权重中包含缓冲区。

+   `keep_in_fp32_modules(List[str],` *可选*) — 我们保留在`torch.float32`数据类型中的模块列表。

+   `offload_8bit_bnb`（`bool`，*可选*）— 是否启用在cpu/disk上卸载8位模块。

在模型内加载（可能是分片的）检查点，可能在加载时将权重发送到给定设备。

一旦跨设备加载，您仍然需要调用[dispatch_model()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.dispatch_model)来使模型能够运行。要在一个单一调用中组合检查点加载和分发，请使用[load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch)。

#### `accelerate.infer_auto_device_map`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1022)

```py
( model: Module max_memory: Optional = None no_split_module_classes: Optional = None dtype: Union = None special_dtypes: Optional = None verbose: bool = False clean_result: bool = True )
```

参数

+   `model`（`torch.nn.Module`）— 要分析的模型。

+   `max_memory`（`Dict`，*可选*）— 设备标识符到最大内存的字典。如果未设置，将默认为可用的最大内存。示例：`max_memory={0: "1GB"}`。

+   `no_split_module_classes`（`List[str]`，*可选*）— 不应跨设备分割的层类名称列表（例如，具有残差连接的任何层）。

+   `dtype`（`str`或`torch.dtype`，*可选*）— 如果提供，加载时权重将转换为该类型。

+   `special_dtypes`（`Dict[str, Union[str, torch.device]]`，*可选*）— 如果提供，特定权重的特殊数据类型（将覆盖用作所有权重默认值的数据类型）。

+   `verbose`（`bool`，*可选*，默认为`False`）— 是否在函数构建设备映射时提供调试语句。

+   `clean_result`（`bool`，*可选*，默认为`True`）— 通过将所有进入相同设备的子模块分组来清理结果设备映射。

为给定模型计算设备映射，优先考虑GPU，然后在CPU上卸载，最后卸载到磁盘，使得：

+   我们不会超出任何GPU的可用内存。

+   如果需要卸载到CPU，GPU 0上始终有空间可以放回卸载到CPU上的最大尺寸的层。

+   如果需要卸载到CPU，我们不会超出CPU上可用的RAM。

+   如果需要卸载到磁盘，CPU上始终有空间可以放回卸载到磁盘上的最大尺寸的层。

所有计算都是通过分析模型参数的大小和数据类型来完成的。因此，模型可以在元设备上（就像在`init_empty_weights`上下文管理器中初始化时一样）。

## 模型钩子

### 钩子类

### `class accelerate.hooks.ModelHook`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/hooks.py#L33)

```py
( )
```

一个包含回调的钩子，在模型的前向方法之前和之后执行。与PyTorch现有的钩子的区别在于它们会传递kwargs。

类属性：

+   `no_grad`（`bool`，*可选*，默认为`False`）— 是否在`torch.no_grad()`上下文管理器下执行实际的前向传递。

#### `detach_hook`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/hooks.py#L81)

```py
( module )
```

参数

+   `模块`（`torch.nn.Module`）— 从此钩子中分离的模块。

当钩子从模块中分离时执行。

#### `init_hook`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/hooks.py#L45)

```py
( module )
```

参数

+   `模块`（`torch.nn.Module`）— 附加到此钩子的模块。

当钩子附加到模块时执行。

#### `post_forward`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/hooks.py#L68)

```py
( module output ) → export const metadata = 'undefined';Any
```

参数

+   `module`（`torch.nn.Module`）— 在此事件之前执行前向传递的模块。

+   `output`（`Any`）— 模块的输出。

返回

`Any`

处理过的`output`。

在模型的前向方法之后执行。

#### `pre_forward`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/hooks.py#L54)

```py
( module *args **kwargs ) → export const metadata = 'undefined';Tuple[Tuple[Any], Dict[Str, Any]]
```

参数

+   `module`（`torch.nn.Module`）— 将在此事件之后执行其前向传递的模块。

+   `args`（`Tuple[Any]`）— 传递给模块的位置参数。

+   `kwargs`（`Dict[Str, Any]`）— 传递给模块的关键字参数。

返回

`Tuple[Tuple[Any], Dict[Str, Any]]`

一个包含处理过的`args`和`kwargs`的元组。

在模型的前向方法之前执行。

### `class accelerate.hooks.AlignDevicesHook`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/hooks.py#L212)

```py
( execution_device: Union = None offload: bool = False io_same_device: bool = False weights_map: Optional = None offload_buffers: bool = False place_submodules: bool = False skip_keys: Union = None tied_params_map: Optional = None )
```

参数

+   `execution_device`（`torch.device`，*可选*）— 在前向传递之前应该将输入和模型权重放置在的设备。

+   `offload`（`bool`，*可选*，默认为`False`）— 是否在前向传递后卸载权重。

+   `io_same_device`（`bool`，*可选*，默认为`False`）— 输出是否应该放置在与输入相同的设备上。

+   `weights_map`（`Mapping[str, torch.Tensor]`，*可选*）— 当模型权重被卸载时，从参数名称到张量值的（可能是惰性的）映射。

+   `offload_buffers`（`bool`，*可选*，默认为`False`）— 是否在卸载时包含关联模块的缓冲区。

+   `place_submodules`（`bool`，*可选*，默认为`False`）— 是否在`init_hook`事件期间将子模块放置在`execution_device`上。

一个通用的`ModelHook`，确保输入和模型权重在关联模块的前向传递中位于相同设备上，可能在前向传递后卸载权重。

### `class accelerate.hooks.SequentialHook`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/hooks.py#L91)

```py
( *hooks )
```

一个可以包含多个钩子并在每个事件中迭代它们的钩子。

### 添加钩子

#### `accelerate.hooks.add_hook_to_module`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/hooks.py#L120)

```py
( module: Module hook: ModelHook append: bool = False ) → export const metadata = 'undefined';torch.nn.Module
```

参数

+   `module`（`torch.nn.Module`）— 要附加钩子的模块。

+   `hook`（`ModelHook`）— 要附加的钩子。

+   `append`（`bool`，*可选*，默认为`False`）— 钩子是否应该与现有的钩子链接（如果模块已经包含一个钩子）或不链接。

返回

`torch.nn.Module`

相同的模块，附加了钩子（模块会被就地修改，所以结果可以被丢弃）。

向给定模块添加一个钩子。这将重写模块的`forward`方法以包含钩子，要删除此行为并恢复原始的`forward`方法，请使用`remove_hook_from_module`。

如果模块已经包含一个钩子，这将用默认传递的新钩子替换它。要将两个钩子链接在一起，传递`append=True`，这样它会将当前钩子和新钩子链接成`SequentialHook`类的一个实例。

#### `accelerate.hooks.attach_execution_device_hook`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/hooks.py#L392)

```py
( module: Module execution_device: Union skip_keys: Union = None preload_module_classes: Optional = None tied_params_map: Optional = None )
```

参数

+   `module`（`torch.nn.Module`）— 我们想要附加钩子的模块。

+   `execution_device`（`int`，`str`或`torch.device`）— 在前向传递之前应该将输入和模型权重放置在的设备。

+   `skip_keys`（`str`或`List[str]`，*可选*）— 在设备之间移动输入或输出时要忽略的键列表。

+   `preload_module_classes` (`List[str]`, *可选*) — 应在前向传递开始时加载所有权重的类实例列表（即使在子模块中）。这仅应用于具有已注册但在前向传递期间未直接调用的子模块的类，例如，如果注册了 `dense` 线性层，但在前向传递中，某些操作中使用 `dense.weight` 和 `dense.bias` 而不是直接调用 `dense`。

+   `tied_params_map` (Optional[Dict[int, Dict[torch.device, torch.Tensor]]], *可选*, 默认为 `None`) — 数据指针到已分派的绑定权重的设备字典的映射。对于给定的执行设备，此参数对于重用共享权重的第一个可用指针以供所有其他指针使用而不是复制内存是有用的。

递归地将 `AlignDevicesHook` 附加到给定模型的所有子模块，以确保它们具有正确的执行设备

#### `accelerate.hooks.attach_align_device_hook`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/hooks.py#L434)

```py
( module: Module execution_device: Optional = None offload: bool = False weights_map: Optional = None offload_buffers: bool = False module_name: str = '' skip_keys: Union = None preload_module_classes: Optional = None tied_params_map: Optional = None )
```

参数

+   `module` (`torch.nn.Module`) — 我们要附加钩子的模块。

+   `execution_device` (`torch.device`, *可选*) — 在前向传递之前应将输入和模型权重放置在的设备上。

+   `offload` (`bool`, *可选*, 默认为 `False`) — 是否在前向传递后卸载权重。

+   `weights_map` (`Mapping[str, torch.Tensor]`, *可选*) — 当模型权重被卸载时，从参数名称到张量值的（潜在惰性）映射。

+   `offload_buffers` (`bool`, *可选*, 默认为 `False`) — 是否在卸载时包括相关模块的缓冲区。

+   `module_name` (`str`, *可选*, 默认为 `""`) — 模块的名称。

+   `skip_keys` (`str` 或 `List[str]`, *可选*) — 在设备之间移动输入或输出时要忽略的键列表。

+   `preload_module_classes` (`List[str]`, *可选*) — 应在前向传递开始时加载所有权重的类实例列表（即使在子模块中）。这仅应用于具有已注册但在前向传递期间未直接调用的子模块的类，例如，如果注册了 `dense` 线性层，但在前向传递中，某些操作中使用 `dense.weight` 和 `dense.bias` 而不是直接调用 `dense`。

+   `tied_params_map` (Optional[Dict[int, Dict[torch.device, torch.Tensor]]], *可选*, 默认为 `None`) — 数据指针到已分派的绑定权重的设备字典的映射。对于给定的执行设备，此参数对于重用共享权重的第一个可用指针以供所有其他指针使用而不是复制内存是有用的。

递归地将 `AlignDevicesHook` 附加到给定模型的所有具有直接参数和/或缓冲区的子模块。

#### `accelerate.hooks.attach_align_device_hook_on_blocks`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/hooks.py#L529)

```py
( module: Module execution_device: Union = None offload: Union = False weights_map: Mapping = None offload_buffers: bool = False module_name: str = '' skip_keys: Union = None preload_module_classes: Optional = None tied_params_map: Optional = None )
```

参数

+   `module` (`torch.nn.Module`) — 我们要附加钩子的模块。

+   `execution_device` (`torch.device` 或 `Dict[str, torch.device]`, *可选*) — 在前向传递之前应将输入和模型权重放置在的设备上。可以是整个模块的一个设备，也可以是将模块名称映射到设备的字典。

+   `offload` (`bool`, *可选*, 默认为 `False`) — 是否在前向传递后卸载权重。可以是整个模块的一个布尔值，也可以是将模块名称映射到布尔值的字典。

+   `weights_map` (`Mapping[str, torch.Tensor]`, *可选*) — 当模型权重被卸载时，从参数名称到张量值的（潜在惰性）映射。

+   `offload_buffers` (`bool`, *可选*, 默认为 `False`) — 是否在卸载时包括相关模块的缓冲区。

+   `module_name` (`str`, *可选*, 默认为 `""`) — 模块的名称。

+   `skip_keys` (`str` 或 `List[str]`, *可选*) — 在设备之间移动输入或输出时要忽略的键列表。

+   `preload_module_classes` (`List[str]`, *可选*) — 一个类的列表，其实例应在前向传播的开始时加载所有权重（甚至在子模块中）。这仅应用于具有已注册但在前向传播期间未直接调用的子模块的类，例如，如果注册了`dense`线性层，但在前向传播时，一些操作中使用`dense.weight`和`dense.bias`而不是直接调用`dense`。

+   `tied_params_map` (Optional[Dict[int, Dict[torch.device, torch.Tensor]]], *可选*, 默认为`None`) — 数据指针到已分派的绑定权重设备字典的映射。对于给定的执行设备，此参数对于重用共享权重的第一个可用指针以供所有其他设备使用而不是复制内存非常有用。

根据需要将`AlignDevicesHook`附加到给定模型的所有块。

### 移除钩子

#### `accelerate.hooks.remove_hook_from_module`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/hooks.py#L179)

```py
( module: Module recurse = False ) → export const metadata = 'undefined';torch.nn.Module
```

参数

+   `module` (`torch.nn.Module`) — 要附加钩子的模块。

+   `recurse` (`bool`, **可选**) — 是否递归地删除钩子

返回值

`torch.nn.Module`

相同的模块，已分离的钩子（模块会就地修改，因此结果可以丢弃）。

通过`add_hook_to_module`删除附加到模块的任何钩子。

#### `accelerate.hooks.remove_hook_from_submodules`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/hooks.py#L517)

```py
( module: Module )
```

参数

+   `module` (`torch.nn.Module`) — 要删除所有钩子的模块。

递归地删除给定模型的子模块上附加的所有钩子。
