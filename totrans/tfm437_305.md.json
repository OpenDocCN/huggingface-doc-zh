["```py\n( text_config = None audio_config = None logit_scale_init_value = 14.285714285714285 projection_dim = 512 projection_hidden_act = 'relu' initializer_factor = 1.0 **kwargs )\n```", "```py\n>>> from transformers import ClapConfig, ClapModel\n\n>>> # Initializing a ClapConfig with laion-ai/base style configuration\n>>> configuration = ClapConfig()\n\n>>> # Initializing a ClapModel (with random weights) from the laion-ai/base style configuration\n>>> model = ClapModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a ClapConfig from a ClapTextConfig and a ClapAudioConfig\n>>> from transformers import ClapTextConfig, ClapAudioConfig\n\n>>> # Initializing a ClapText and ClapAudioConfig configuration\n>>> config_text = ClapTextConfig()\n>>> config_audio = ClapAudioConfig()\n\n>>> config = ClapConfig.from_text_audio_configs(config_text, config_audio)\n```", "```py\n( text_config: ClapTextConfig audio_config: ClapAudioConfig **kwargs ) \u2192 export const metadata = 'undefined';ClapConfig\n```", "```py\n( vocab_size = 50265 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 514 type_vocab_size = 1 initializer_factor = 1.0 layer_norm_eps = 1e-12 projection_dim = 512 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 position_embedding_type = 'absolute' use_cache = True projection_hidden_act = 'relu' **kwargs )\n```", "```py\n>>> from transformers import ClapTextConfig, ClapTextModel\n\n>>> # Initializing a CLAP text configuration\n>>> configuration = ClapTextConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = ClapTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( window_size = 8 num_mel_bins = 64 spec_size = 256 hidden_act = 'gelu' patch_size = 4 patch_stride = [4, 4] num_classes = 527 hidden_size = 768 projection_dim = 512 depths = [2, 2, 6, 2] num_attention_heads = [4, 8, 16, 32] enable_fusion = False hidden_dropout_prob = 0.1 fusion_type = None patch_embed_input_channels = 1 flatten_patch_embeds = True patch_embeds_hidden_size = 96 enable_patch_layer_norm = True drop_path_rate = 0.0 attention_probs_dropout_prob = 0.0 qkv_bias = True mlp_ratio = 4.0 aff_block_r = 4 num_hidden_layers = 4 projection_hidden_act = 'relu' layer_norm_eps = 1e-05 initializer_factor = 1.0 **kwargs )\n```", "```py\n>>> from transformers import ClapAudioConfig, ClapAudioModel\n\n>>> # Initializing a ClapAudioConfig with laion/clap-htsat-fused style configuration\n>>> configuration = ClapAudioConfig()\n\n>>> # Initializing a ClapAudioModel (with random weights) from the laion/clap-htsat-fused style configuration\n>>> model = ClapAudioModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( feature_size = 64 sampling_rate = 48000 hop_length = 480 max_length_s = 10 fft_window_size = 1024 padding_value = 0.0 return_attention_mask = False frequency_min: float = 0 frequency_max: float = 14000 top_db: int = None truncation: str = 'fusion' padding: str = 'repeatpad' **kwargs )\n```", "```py\n( ) \u2192 export const metadata = 'undefined';Dict[str, Any]\n```", "```py\n( feature_extractor tokenizer )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config: ClapConfig )\n```", "```py\n( input_ids: Optional = None input_features: Optional = None is_longer: Optional = None attention_mask: Optional = None position_ids: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.clap.modeling_clap.ClapOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import AutoProcessor, ClapModel\n\n>>> dataset = load_dataset(\"ashraq/esc50\")\n>>> audio_sample = dataset[\"train\"][\"audio\"][0][\"array\"]\n\n>>> model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n>>> processor = AutoProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n\n>>> input_text = [\"Sound of a dog\", \"Sound of vaccum cleaner\"]\n\n>>> inputs = processor(text=input_text, audios=audio_sample, return_tensors=\"pt\", padding=True)\n\n>>> outputs = model(**inputs)\n>>> logits_per_audio = outputs.logits_per_audio  # this is the audio-text similarity score\n>>> probs = logits_per_audio.softmax(dim=-1)  # we can take the softmax to get the label probabilities\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from transformers import AutoTokenizer, ClapModel\n\n>>> model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"laion/clap-htsat-unfused\")\n\n>>> inputs = tokenizer([\"the sound of a cat\", \"the sound of a dog\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n( input_features: Optional = None is_longer: Optional = None attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';audio_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from transformers import AutoFeatureExtractor, ClapModel\n>>> import torch\n\n>>> model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"laion/clap-htsat-unfused\")\n>>> random_audio = torch.rand((16_000))\n>>> inputs = feature_extractor(random_audio, return_tensors=\"pt\")\n>>> audio_features = model.get_audio_features(**inputs)\n```", "```py\n( config add_pooling_layer = True )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( config: ClapTextConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.clap.modeling_clap.ClapTextModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, ClapTextModelWithProjection\n\n>>> model = ClapTextModelWithProjection.from_pretrained(\"laion/clap-htsat-unfused\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"laion/clap-htsat-unfused\")\n\n>>> inputs = tokenizer([\"a sound of a cat\", \"a sound of a dog\"], padding=True, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> text_embeds = outputs.text_embeds\n```", "```py\n( config: ClapAudioConfig )\n```", "```py\n( input_features: Optional = None is_longer: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import AutoProcessor, ClapAudioModel\n\n>>> dataset = load_dataset(\"ashraq/esc50\")\n>>> audio_sample = dataset[\"train\"][\"audio\"][0][\"array\"]\n\n>>> model = ClapAudioModel.from_pretrained(\"laion/clap-htsat-fused\")\n>>> processor = AutoProcessor.from_pretrained(\"laion/clap-htsat-fused\")\n\n>>> inputs = processor(audios=audio_sample, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n```", "```py\n( config: ClapAudioConfig )\n```", "```py\n( input_features: Optional = None is_longer: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.clap.modeling_clap.ClapAudioModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import ClapAudioModelWithProjection, ClapProcessor\n\n>>> model = ClapAudioModelWithProjection.from_pretrained(\"laion/clap-htsat-fused\")\n>>> processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-fused\")\n\n>>> dataset = load_dataset(\"ashraq/esc50\")\n>>> audio_sample = dataset[\"train\"][\"audio\"][0][\"array\"]\n\n>>> inputs = processor(audios=audio_sample, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> audio_embeds = outputs.audio_embeds\n```"]