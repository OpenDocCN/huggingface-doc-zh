["```py\n>>> from transformers import ClapConfig, ClapModel\n\n>>> # Initializing a ClapConfig with laion-ai/base style configuration\n>>> configuration = ClapConfig()\n\n>>> # Initializing a ClapModel (with random weights) from the laion-ai/base style configuration\n>>> model = ClapModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a ClapConfig from a ClapTextConfig and a ClapAudioConfig\n>>> from transformers import ClapTextConfig, ClapAudioConfig\n\n>>> # Initializing a ClapText and ClapAudioConfig configuration\n>>> config_text = ClapTextConfig()\n>>> config_audio = ClapAudioConfig()\n\n>>> config = ClapConfig.from_text_audio_configs(config_text, config_audio)\n```", "```py\n>>> from transformers import ClapTextConfig, ClapTextModel\n\n>>> # Initializing a CLAP text configuration\n>>> configuration = ClapTextConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = ClapTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import ClapAudioConfig, ClapAudioModel\n\n>>> # Initializing a ClapAudioConfig with laion/clap-htsat-fused style configuration\n>>> configuration = ClapAudioConfig()\n\n>>> # Initializing a ClapAudioModel (with random weights) from the laion/clap-htsat-fused style configuration\n>>> model = ClapAudioModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import AutoProcessor, ClapModel\n\n>>> dataset = load_dataset(\"ashraq/esc50\")\n>>> audio_sample = dataset[\"train\"][\"audio\"][0][\"array\"]\n\n>>> model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n>>> processor = AutoProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n\n>>> input_text = [\"Sound of a dog\", \"Sound of vaccum cleaner\"]\n\n>>> inputs = processor(text=input_text, audios=audio_sample, return_tensors=\"pt\", padding=True)\n\n>>> outputs = model(**inputs)\n>>> logits_per_audio = outputs.logits_per_audio  # this is the audio-text similarity score\n>>> probs = logits_per_audio.softmax(dim=-1)  # we can take the softmax to get the label probabilities\n```", "```py\n>>> from transformers import AutoTokenizer, ClapModel\n\n>>> model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"laion/clap-htsat-unfused\")\n\n>>> inputs = tokenizer([\"the sound of a cat\", \"the sound of a dog\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n>>> from transformers import AutoFeatureExtractor, ClapModel\n>>> import torch\n\n>>> model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"laion/clap-htsat-unfused\")\n>>> random_audio = torch.rand((16_000))\n>>> inputs = feature_extractor(random_audio, return_tensors=\"pt\")\n>>> audio_features = model.get_audio_features(**inputs)\n```", "```py\n>>> from transformers import AutoTokenizer, ClapTextModelWithProjection\n\n>>> model = ClapTextModelWithProjection.from_pretrained(\"laion/clap-htsat-unfused\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"laion/clap-htsat-unfused\")\n\n>>> inputs = tokenizer([\"a sound of a cat\", \"a sound of a dog\"], padding=True, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> text_embeds = outputs.text_embeds\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import AutoProcessor, ClapAudioModel\n\n>>> dataset = load_dataset(\"ashraq/esc50\")\n>>> audio_sample = dataset[\"train\"][\"audio\"][0][\"array\"]\n\n>>> model = ClapAudioModel.from_pretrained(\"laion/clap-htsat-fused\")\n>>> processor = AutoProcessor.from_pretrained(\"laion/clap-htsat-fused\")\n\n>>> inputs = processor(audios=audio_sample, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import ClapAudioModelWithProjection, ClapProcessor\n\n>>> model = ClapAudioModelWithProjection.from_pretrained(\"laion/clap-htsat-fused\")\n>>> processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-fused\")\n\n>>> dataset = load_dataset(\"ashraq/esc50\")\n>>> audio_sample = dataset[\"train\"][\"audio\"][0][\"array\"]\n\n>>> inputs = processor(audios=audio_sample, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> audio_embeds = outputs.audio_embeds\n```"]