- en: Checkpointing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ£€æŸ¥ç‚¹
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/checkpoint](https://huggingface.co/docs/accelerate/usage_guides/checkpoint)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/accelerate/usage_guides/checkpoint](https://huggingface.co/docs/accelerate/usage_guides/checkpoint)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'When training a PyTorch model with ğŸ¤— Accelerate, you may often want to save
    and continue a state of training. Doing so requires saving and loading the model,
    optimizer, RNG generators, and the GradScaler. Inside ğŸ¤— Accelerate are two convenience
    functions to achieve this quickly:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨ğŸ¤— Accelerateè®­ç»ƒPyTorchæ¨¡å‹æ—¶ï¼Œæ‚¨å¯èƒ½ç»å¸¸å¸Œæœ›ä¿å­˜å’Œç»§ç»­è®­ç»ƒçŠ¶æ€ã€‚ä¸ºæ­¤ï¼Œéœ€è¦ä¿å­˜å’ŒåŠ è½½æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€RNGç”Ÿæˆå™¨å’ŒGradScalerã€‚åœ¨ğŸ¤—
    Accelerateå†…éƒ¨æœ‰ä¸¤ä¸ªä¾¿åˆ©å‡½æ•°å¯ä»¥å¿«é€Ÿå®ç°è¿™ä¸€ç‚¹ï¼š
- en: Use [save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)
    for saving everything mentioned above to a folder location
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)å°†ä¸Šè¿°æ‰€æœ‰å†…å®¹ä¿å­˜åˆ°æ–‡ä»¶å¤¹ä½ç½®
- en: Use [load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)
    for loading everything stored from an earlier `save_state`
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)åŠ è½½å…ˆå‰`save_state`å­˜å‚¨çš„æ‰€æœ‰å†…å®¹
- en: To further customize where and how states are saved through [save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)
    the [ProjectConfiguration](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.ProjectConfiguration)
    class can be used. For example if `automatic_checkpoint_naming` is enabled each
    saved checkpoint will be located then at `Accelerator.project_dir/checkpoints/checkpoint_{checkpoint_number}`.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡[save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)è¿›ä¸€æ­¥è‡ªå®šä¹‰çŠ¶æ€ä¿å­˜çš„ä½ç½®å’Œæ–¹å¼ï¼Œå¯ä»¥ä½¿ç”¨[ProjectConfiguration](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.ProjectConfiguration)ç±»ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå¯ç”¨äº†`automatic_checkpoint_naming`ï¼Œæ¯ä¸ªä¿å­˜çš„æ£€æŸ¥ç‚¹å°†ä½äº`Accelerator.project_dir/checkpoints/checkpoint_{checkpoint_number}`ã€‚
- en: It should be noted that the expectation is that those states come from the same
    training script, they should not be from two separate scripts.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åº”è¯¥æ³¨æ„åˆ°ï¼ŒæœŸæœ›æ˜¯è¿™äº›çŠ¶æ€æ¥è‡ªç›¸åŒçš„è®­ç»ƒè„šæœ¬ï¼Œè€Œä¸åº”è¯¥æ¥è‡ªä¸¤ä¸ªå•ç‹¬çš„è„šæœ¬ã€‚
- en: By using [register_for_checkpointing()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_for_checkpointing),
    you can register custom objects to be automatically stored or loaded from the
    two prior functions, so long as the object has a `state_dict` **and** a `load_state_dict`
    functionality. This could include objects such as a learning rate scheduler.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨[register_for_checkpointing()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_for_checkpointing)ï¼Œæ‚¨å¯ä»¥æ³¨å†Œè‡ªå®šä¹‰å¯¹è±¡ï¼Œä»¥ä¾¿ä»å‰ä¸¤ä¸ªå‡½æ•°ä¸­è‡ªåŠ¨å­˜å‚¨æˆ–åŠ è½½ï¼Œåªè¦è¯¥å¯¹è±¡å…·æœ‰`state_dict`
    **å’Œ** `load_state_dict`åŠŸèƒ½ã€‚è¿™å¯èƒ½åŒ…æ‹¬å­¦ä¹ ç‡è°ƒåº¦å™¨ç­‰å¯¹è±¡ã€‚
- en: 'Below is a brief example using checkpointing to save and reload a state during
    training:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€çŸ­çš„ç¤ºä¾‹ï¼Œä½¿ç”¨æ£€æŸ¥ç‚¹æ¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜å’Œé‡æ–°åŠ è½½çŠ¶æ€ï¼š
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Restoring the state of the DataLoader
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢å¤DataLoaderçš„çŠ¶æ€
- en: After resuming from a checkpoint, it may also be desirable to resume from a
    particular point in the active `DataLoader` if the state was saved during the
    middle of an epoch. You can use [skip_first_batches()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.skip_first_batches)
    to do so.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ£€æŸ¥ç‚¹æ¢å¤åï¼Œå¦‚æœåœ¨ä¸€ä¸ªepochçš„ä¸­é—´ä¿å­˜äº†çŠ¶æ€ï¼Œå¯èƒ½è¿˜å¸Œæœ›ä»æ´»åŠ¨çš„`DataLoader`ä¸­çš„ç‰¹å®šç‚¹æ¢å¤ã€‚æ‚¨å¯ä»¥ä½¿ç”¨[skip_first_batches()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.skip_first_batches)æ¥å®ç°ã€‚
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
