- en: Checkpointing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/accelerate/usage_guides/checkpoint](https://huggingface.co/docs/accelerate/usage_guides/checkpoint)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/accelerate/v0.27.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/start.6e0fb178.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/scheduler.69131cc3.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/singletons.ac467c20.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/paths.b2f3aeca.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/app.67e11fc0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/index.e1f30d73.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/0.bfeed9f0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/32.50e68766.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/CodeBlock.30cef355.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Heading.0aab6758.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'When training a PyTorch model with ðŸ¤— Accelerate, you may often want to save
    and continue a state of training. Doing so requires saving and loading the model,
    optimizer, RNG generators, and the GradScaler. Inside ðŸ¤— Accelerate are two convenience
    functions to achieve this quickly:'
  prefs: []
  type: TYPE_NORMAL
- en: Use [save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)
    for saving everything mentioned above to a folder location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use [load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)
    for loading everything stored from an earlier `save_state`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To further customize where and how states are saved through [save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)
    the [ProjectConfiguration](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.ProjectConfiguration)
    class can be used. For example if `automatic_checkpoint_naming` is enabled each
    saved checkpoint will be located then at `Accelerator.project_dir/checkpoints/checkpoint_{checkpoint_number}`.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that the expectation is that those states come from the same
    training script, they should not be from two separate scripts.
  prefs: []
  type: TYPE_NORMAL
- en: By using [register_for_checkpointing()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_for_checkpointing),
    you can register custom objects to be automatically stored or loaded from the
    two prior functions, so long as the object has a `state_dict` **and** a `load_state_dict`
    functionality. This could include objects such as a learning rate scheduler.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Below is a brief example using checkpointing to save and reload a state during
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Restoring the state of the DataLoader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After resuming from a checkpoint, it may also be desirable to resume from a
    particular point in the active `DataLoader` if the state was saved during the
    middle of an epoch. You can use [skip_first_batches()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.skip_first_batches)
    to do so.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
