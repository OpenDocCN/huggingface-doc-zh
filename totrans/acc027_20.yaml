- en: Checkpointing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/checkpoint](https://huggingface.co/docs/accelerate/usage_guides/checkpoint)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: 'When training a PyTorch model with ðŸ¤— Accelerate, you may often want to save
    and continue a state of training. Doing so requires saving and loading the model,
    optimizer, RNG generators, and the GradScaler. Inside ðŸ¤— Accelerate are two convenience
    functions to achieve this quickly:'
  prefs: []
  type: TYPE_NORMAL
- en: Use [save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)
    for saving everything mentioned above to a folder location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use [load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)
    for loading everything stored from an earlier `save_state`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To further customize where and how states are saved through [save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)
    the [ProjectConfiguration](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.ProjectConfiguration)
    class can be used. For example if `automatic_checkpoint_naming` is enabled each
    saved checkpoint will be located then at `Accelerator.project_dir/checkpoints/checkpoint_{checkpoint_number}`.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that the expectation is that those states come from the same
    training script, they should not be from two separate scripts.
  prefs: []
  type: TYPE_NORMAL
- en: By using [register_for_checkpointing()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_for_checkpointing),
    you can register custom objects to be automatically stored or loaded from the
    two prior functions, so long as the object has a `state_dict` **and** a `load_state_dict`
    functionality. This could include objects such as a learning rate scheduler.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Below is a brief example using checkpointing to save and reload a state during
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Restoring the state of the DataLoader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After resuming from a checkpoint, it may also be desirable to resume from a
    particular point in the active `DataLoader` if the state was saved during the
    middle of an epoch. You can use [skip_first_batches()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.skip_first_batches)
    to do so.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
