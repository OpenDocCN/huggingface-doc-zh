- en: Run Inference on servers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/huggingface_hub/guides/inference](https://huggingface.co/docs/huggingface_hub/guides/inference)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: 'Inference is the process of using a trained model to make predictions on new
    data. As this process can be compute-intensive, running on a dedicated server
    can be an interesting option. The `huggingface_hub` library provides an easy way
    to call a service that runs inference for hosted models. There are several services
    you can connect to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Inference API](https://huggingface.co/docs/api-inference/index): a service
    that allows you to run accelerated inference on Hugging Face’s infrastructure
    for free. This service is a fast way to get started, test different models, and
    prototype AI products.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index):
    a product to easily deploy models to production. Inference is run by Hugging Face
    in a dedicated, fully managed infrastructure on a cloud provider of your choice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These services can be called with the [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    object. It acts as a replacement for the legacy [InferenceApi](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceApi)
    client, adding specific support for tasks and handling inference on both [Inference
    API](https://huggingface.co/docs/api-inference/index) and [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index).
    Learn how to migrate to the new client in the [Legacy InferenceAPI client](#legacy-inferenceapi-client)
    section.
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    is a Python client making HTTP calls to our APIs. If you want to make the HTTP
    calls directly using your preferred tool (curl, postman,…), please refer to the
    [Inference API](https://huggingface.co/docs/api-inference/index) or to the [Inference
    Endpoints](https://huggingface.co/docs/inference-endpoints/index) documentation
    pages.'
  prefs: []
  type: TYPE_NORMAL
- en: For web development, a [JS client](https://huggingface.co/docs/huggingface.js/inference/README)
    has been released. If you are interested in game development, you might have a
    look at our [C# project](https://github.com/huggingface/unity-api).
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s get started with a text-to-image task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We initialized an [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    with the default parameters. The only thing you need to know is the [task](#supported-tasks)
    you want to perform. By default, the client will connect to the Inference API
    and select a model to complete the task. In our example, we generated an image
    from a text prompt. The returned value is a `PIL.Image` object that can be saved
    to a file.
  prefs: []
  type: TYPE_NORMAL
- en: The API is designed to be simple. Not all parameters and options are available
    or described for the end user. Check out [this page](https://huggingface.co/docs/api-inference/detailed_parameters)
    if you are interested in learning more about all the parameters available for
    each task.
  prefs: []
  type: TYPE_NORMAL
- en: Using a specific model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What if you want to use a specific model? You can specify it either as a parameter
    or directly at an instance level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: There are more than 200k models on the Hugging Face Hub! Each task in the [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    comes with a recommended model. Be aware that the HF recommendation can change
    over time without prior notice. Therefore it is best to explicitly set a model
    once you are decided. Also, in most cases you’ll be interested in finding a model
    specific to *your* needs. Visit the [Models](https://huggingface.co/models) page
    on the Hub to explore your possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: Using a specific URL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The examples we saw above use the free-hosted Inference API. This proves to
    be very useful for prototyping and testing things quickly. Once you’re ready to
    deploy your model to production, you’ll need to use a dedicated infrastructure.
    That’s where [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index)
    comes into play. It allows you to deploy any model and expose it as a private
    API. Once deployed, you’ll get a URL that you can connect to using exactly the
    same code as before, changing only the `model` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Authentication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Calls made with the [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    can be authenticated using a [User Access Token](https://huggingface.co/docs/hub/security-tokens).
    By default, it will use the token saved on your machine if you are logged in (check
    out [how to authenticate](https://huggingface.co/docs/huggingface_hub/quick-start#authentication)).
    If you are not logged in, you can pass your token as an instance parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Authentication is NOT mandatory when using the Inference API. However, authenticated
    users get a higher free-tier to play with the service. Token is also mandatory
    if you want to run inference on your private models or on private endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Supported tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)’s
    goal is to provide the easiest interface to run inference on Hugging Face models.
    It has a simple API that supports the most common tasks. Here is a list of the
    currently supported tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Domain | Task | Supported | Documentation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Audio | [Audio Classification](https://huggingface.co/tasks/audio-classification)
    | ✅ | [audio_classification()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.audio_classification)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Automatic Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)
    | ✅ | [automatic_speech_recognition()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.automatic_speech_recognition)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Text-to-Speech](https://huggingface.co/tasks/text-to-speech) | ✅ | [text_to_speech()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_to_speech)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Computer Vision | [Image Classification](https://huggingface.co/tasks/image-classification)
    | ✅ | [image_classification()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.image_classification)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Image Segmentation](https://huggingface.co/tasks/image-segmentation)
    | ✅ | [image_segmentation()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.image_segmentation)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Image-to-Image](https://huggingface.co/tasks/image-to-image) | ✅ | [image_to_image()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.image_to_image)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Image-to-Text](https://huggingface.co/tasks/image-to-text) | ✅ | [image_to_text()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.image_to_text)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Object Detection](https://huggingface.co/tasks/object-detection) | ✅
    | [object_detection()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.object_detection)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Text-to-Image](https://huggingface.co/tasks/text-to-image) | ✅ | [text_to_image()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_to_image)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Zero-Shot-Image-Classification](https://huggingface.co/tasks/zero-shot-image-classification)
    | ✅ | [zero_shot_image_classification()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.zero_shot_image_classification)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multimodal | [Documentation Question Answering](https://huggingface.co/tasks/document-question-answering)
    | ✅ | [document_question_answering()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.document_question_answering)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Visual Question Answering](https://huggingface.co/tasks/visual-question-answering)
    | ✅ | [visual_question_answering()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.visual_question_answering)
    |'
  prefs: []
  type: TYPE_TB
- en: '| NLP | [Conversational](https://huggingface.co/tasks/conversational) | ✅ |
    [conversational()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.conversational)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Feature Extraction](https://huggingface.co/tasks/feature-extraction)
    | ✅ | [feature_extraction()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.feature_extraction)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Fill Mask](https://huggingface.co/tasks/fill-mask) | ✅ | [fill_mask()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.fill_mask)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Question Answering](https://huggingface.co/tasks/question-answering)
    | ✅ | [question_answering()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.question_answering)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Sentence Similarity](https://huggingface.co/tasks/sentence-similarity)
    | ✅ | [sentence_similarity()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.sentence_similarity)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Summarization](https://huggingface.co/tasks/summarization) | ✅ | [summarization()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.summarization)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Table Question Answering](https://huggingface.co/tasks/table-question-answering)
    | ✅ | [table_question_answering()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.table_question_answering)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Text Classification](https://huggingface.co/tasks/text-classification)
    | ✅ | [text_classification()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_classification)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Text Generation](https://huggingface.co/tasks/text-generation) | ✅ |
    [text_generation()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Token Classification](https://huggingface.co/tasks/token-classification)
    | ✅ | [token_classification()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.token_classification)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Translation](https://huggingface.co/tasks/translation) | ✅ | [translation()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.translation)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Zero Shot Classification](https://huggingface.co/tasks/zero-shot-classification)
    | ✅ | [zero_shot_classification()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.zero_shot_classification)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Tabular | [Tabular Classification](https://huggingface.co/tasks/tabular-classification)
    | ✅ | [tabular_classification()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.tabular_classification)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Tabular Regression](https://huggingface.co/tasks/tabular-regression)
    | ✅ | [tabular_regression()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.tabular_regression)
    |'
  prefs: []
  type: TYPE_TB
- en: Check out the [Tasks](https://huggingface.co/tasks) page to learn more about
    each task, how to use them, and the most popular models for each task.
  prefs: []
  type: TYPE_NORMAL
- en: Custom requests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: However, it is not always possible to cover all use cases. For custom requests,
    the [InferenceClient.post()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.post)
    method gives you the flexibility to send any request to the Inference API. For
    example, you can specify how to parse the inputs and outputs. In the example below,
    the generated image is returned as raw bytes instead of parsing it as a `PIL Image`.
    This can be helpful if you don’t have `Pillow` installed in your setup and just
    care about the binary content of the image. [InferenceClient.post()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient.post)
    is also useful to handle tasks that are not yet officially supported.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Async client
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An async version of the client is also provided, based on `asyncio` and `aiohttp`.
    You can either install `aiohttp` directly or use the `[inference]` extra:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: After installation all async API endpoints are available via [AsyncInferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient).
    Its initialization and APIs are strictly the same as the sync-only version.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For more information about the `asyncio` module, please refer to the [official
    documentation](https://docs.python.org/3/library/asyncio.html).
  prefs: []
  type: TYPE_NORMAL
- en: Advanced tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the above section, we saw the main aspects of [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient).
    Let’s dive into some more advanced tips.
  prefs: []
  type: TYPE_NORMAL
- en: Timeout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When doing inference, there are two main causes for a timeout:'
  prefs: []
  type: TYPE_NORMAL
- en: The inference process takes a long time to complete.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is not available, for example when Inference API is loading it for
    the first time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    has a global `timeout` parameter to handle those two aspects. By default, it is
    set to `None`, meaning that the client will wait indefinitely for the inference
    to complete. If you want more control in your workflow, you can set it to a specific
    value in seconds. If the timeout delay expires, an [InferenceTimeoutError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceTimeoutError)
    is raised. You can catch it and handle it in your code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Binary inputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some tasks require binary inputs, for example, when dealing with images or
    audio files. In this case, [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    tries to be as permissive as possible and accept different types:'
  prefs: []
  type: TYPE_NORMAL
- en: raw `bytes`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a file-like object, opened as binary (`with open("audio.flac", "rb") as f:
    ...`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a path (`str` or `Path`) pointing to a local file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a URL (`str`) pointing to a remote file (e.g. `https://...`). In this case,
    the file will be downloaded locally before sending it to the Inference API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Legacy InferenceAPI client
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    acts as a replacement for the legacy [InferenceApi](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceApi)
    client. It adds specific support for tasks and handles inference on both [Inference
    API](https://huggingface.co/docs/api-inference/index) and [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index).'
  prefs: []
  type: TYPE_NORMAL
- en: Here is a short guide to help you migrate from [InferenceApi](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceApi)
    to [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient).
  prefs: []
  type: TYPE_NORMAL
- en: Initialization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Change from
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Run on a specific task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Change from
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This is the recommended way to adapt your code to [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient).
    It lets you benefit from the task-specific methods like `feature_extraction`.
  prefs: []
  type: TYPE_NORMAL
- en: Run custom request
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Change from
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Run with parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Change from
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
