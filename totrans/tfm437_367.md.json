["```py\n>>> from transformers import pipeline\n>>> from PIL import Image\n>>> import requests\n\n>>> # load pipe\n>>> image_classifier = pipeline(task=\"zero-shot-image-classification\", model=\"google/siglip-base-patch16-224\")\n\n>>> # load image\n>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> # inference\n>>> outputs = image_classifier(image, candidate_labels=[\"2 cats\", \"a plane\", \"a remote\"])\n>>> outputs = [{\"score\": round(output[\"score\"], 4), \"label\": output[\"label\"] } for output in outputs]\n>>> print(outputs)\n[{'score': 0.1979, 'label': '2 cats'}, {'score': 0.0, 'label': 'a remote'}, {'score': 0.0, 'label': 'a plane'}]\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AutoModel\n>>> import torch\n\n>>> model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\")\n>>> processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> texts = [\"a photo of 2 cats\", \"a photo of 2 dogs\"]\n>>> # important: we pass `padding=max_length` since the model was trained with this\n>>> inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> logits_per_image = outputs.logits_per_image\n>>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n>>> print(f\"{probs[0][0]:.1%} that image 0 is '{texts[0]}'\")\n31.9% that image 0 is 'a photo of 2 cats'\n```", "```py\n( text_config = None vision_config = None **kwargs )\n```", "```py\n>>> from transformers import SiglipConfig, SiglipModel\n\n>>> # Initializing a SiglipConfig with google/siglip-base-patch16-224 style configuration\n>>> configuration = SiglipConfig()\n\n>>> # Initializing a SiglipModel (with random weights) from the google/siglip-base-patch16-224 style configuration\n>>> model = SiglipModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a SiglipConfig from a SiglipTextConfig and a SiglipVisionConfig\n>>> from transformers import SiglipTextConfig, SiglipVisionConfig\n\n>>> # Initializing a SiglipText and SiglipVision configuration\n>>> config_text = SiglipTextConfig()\n>>> config_vision = SiglipVisionConfig()\n\n>>> config = SiglipConfig.from_text_vision_configs(config_text, config_vision)\n```", "```py\n( text_config: SiglipTextConfig vision_config: SiglipVisionConfig **kwargs ) \u2192 export const metadata = 'undefined';SiglipConfig\n```", "```py\n( vocab_size = 32000 hidden_size = 768 intermediate_size = 3072 num_hidden_layers = 12 num_attention_heads = 12 max_position_embeddings = 64 hidden_act = 'gelu_pytorch_tanh' layer_norm_eps = 1e-06 attention_dropout = 0.0 pad_token_id = 1 bos_token_id = 49406 eos_token_id = 49407 **kwargs )\n```", "```py\n>>> from transformers import SiglipTextConfig, SiglipTextModel\n\n>>> # Initializing a SiglipTextConfig with google/siglip-base-patch16-224 style configuration\n>>> configuration = SiglipTextConfig()\n\n>>> # Initializing a SiglipTextModel (with random weights) from the google/siglip-base-patch16-224 style configuration\n>>> model = SiglipTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( hidden_size = 768 intermediate_size = 3072 num_hidden_layers = 12 num_attention_heads = 12 num_channels = 3 image_size = 224 patch_size = 16 hidden_act = 'gelu_pytorch_tanh' layer_norm_eps = 1e-06 attention_dropout = 0.0 **kwargs )\n```", "```py\n>>> from transformers import SiglipVisionConfig, SiglipVisionModel\n\n>>> # Initializing a SiglipVisionConfig with google/siglip-base-patch16-224 style configuration\n>>> configuration = SiglipVisionConfig()\n\n>>> # Initializing a SiglipVisionModel (with random weights) from the google/siglip-base-patch16-224 style configuration\n>>> model = SiglipVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file eos_token = '</s>' unk_token = '<unk>' pad_token = '</s>' additional_special_tokens = None sp_model_kwargs: Optional = None model_max_length = 64 do_lower_case = True **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BICUBIC: 3> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None **kwargs )\n```", "```py\n( images: Union do_resize: bool = None size: Dict = None resample: Resampling = None do_rescale: bool = None rescale_factor: float = None do_normalize: bool = None image_mean: Union = None image_std: Union = None return_tensors: Union = None data_format: Optional = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( image_processor tokenizer )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config: SiglipConfig )\n```", "```py\n( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None position_ids: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.siglip.modeling_siglip.SiglipOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AutoModel\n>>> import torch\n\n>>> model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\")\n>>> processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> texts = [\"a photo of 2 cats\", \"a photo of 2 dogs\"]\n>>> # important: we pass `padding=max_length` since the model was trained with this\n>>> inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> logits_per_image = outputs.logits_per_image\n>>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n>>> print(f\"{probs[0][0]:.1%} that image 0 is '{texts[0]}'\")\n31.9% that image 0 is 'a photo of 2 cats'\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModel\n>>> import torch\n\n>>> model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/siglip-base-patch16-224\")\n\n>>> # important: make sure to set padding=\"max_length\" as that's how the model was trained\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=\"max_length\", return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     text_features = model.get_text_features(**inputs)\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AutoModel\n>>> import torch\n\n>>> model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\")\n>>> processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     image_features = model.get_image_features(**inputs)\n```", "```py\n( config: SiglipTextConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, SiglipTextModel\n\n>>> model = SiglipTextModel.from_pretrained(\"google/siglip-base-patch16-224\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/siglip-base-patch16-224\")\n\n>>> # important: make sure to set padding=\"max_length\" as that's how the model was trained\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=\"max_length\", return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n( config: SiglipVisionConfig )\n```", "```py\n( pixel_values output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, SiglipVisionModel\n\n>>> model = SiglipVisionModel.from_pretrained(\"google/siglip-base-patch16-224\")\n>>> processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled features\n```"]