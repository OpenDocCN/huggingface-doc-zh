["```py\n>>> from transformers import pipeline\n>>> from PIL import Image\n>>> import requests\n\n>>> # load pipe\n>>> image_classifier = pipeline(task=\"zero-shot-image-classification\", model=\"google/siglip-base-patch16-224\")\n\n>>> # load image\n>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> # inference\n>>> outputs = image_classifier(image, candidate_labels=[\"2 cats\", \"a plane\", \"a remote\"])\n>>> outputs = [{\"score\": round(output[\"score\"], 4), \"label\": output[\"label\"] } for output in outputs]\n>>> print(outputs)\n[{'score': 0.1979, 'label': '2 cats'}, {'score': 0.0, 'label': 'a remote'}, {'score': 0.0, 'label': 'a plane'}]\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AutoModel\n>>> import torch\n\n>>> model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\")\n>>> processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> texts = [\"a photo of 2 cats\", \"a photo of 2 dogs\"]\n>>> # important: we pass `padding=max_length` since the model was trained with this\n>>> inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> logits_per_image = outputs.logits_per_image\n>>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n>>> print(f\"{probs[0][0]:.1%} that image 0 is '{texts[0]}'\")\n31.9% that image 0 is 'a photo of 2 cats'\n```", "```py\n>>> from transformers import SiglipConfig, SiglipModel\n\n>>> # Initializing a SiglipConfig with google/siglip-base-patch16-224 style configuration\n>>> configuration = SiglipConfig()\n\n>>> # Initializing a SiglipModel (with random weights) from the google/siglip-base-patch16-224 style configuration\n>>> model = SiglipModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a SiglipConfig from a SiglipTextConfig and a SiglipVisionConfig\n>>> from transformers import SiglipTextConfig, SiglipVisionConfig\n\n>>> # Initializing a SiglipText and SiglipVision configuration\n>>> config_text = SiglipTextConfig()\n>>> config_vision = SiglipVisionConfig()\n\n>>> config = SiglipConfig.from_text_vision_configs(config_text, config_vision)\n```", "```py\n>>> from transformers import SiglipTextConfig, SiglipTextModel\n\n>>> # Initializing a SiglipTextConfig with google/siglip-base-patch16-224 style configuration\n>>> configuration = SiglipTextConfig()\n\n>>> # Initializing a SiglipTextModel (with random weights) from the google/siglip-base-patch16-224 style configuration\n>>> model = SiglipTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import SiglipVisionConfig, SiglipVisionModel\n\n>>> # Initializing a SiglipVisionConfig with google/siglip-base-patch16-224 style configuration\n>>> configuration = SiglipVisionConfig()\n\n>>> # Initializing a SiglipVisionModel (with random weights) from the google/siglip-base-patch16-224 style configuration\n>>> model = SiglipVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AutoModel\n>>> import torch\n\n>>> model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\")\n>>> processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> texts = [\"a photo of 2 cats\", \"a photo of 2 dogs\"]\n>>> # important: we pass `padding=max_length` since the model was trained with this\n>>> inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> logits_per_image = outputs.logits_per_image\n>>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n>>> print(f\"{probs[0][0]:.1%} that image 0 is '{texts[0]}'\")\n31.9% that image 0 is 'a photo of 2 cats'\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModel\n>>> import torch\n\n>>> model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/siglip-base-patch16-224\")\n\n>>> # important: make sure to set padding=\"max_length\" as that's how the model was trained\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=\"max_length\", return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     text_features = model.get_text_features(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AutoModel\n>>> import torch\n\n>>> model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\")\n>>> processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     image_features = model.get_image_features(**inputs)\n```", "```py\n>>> from transformers import AutoTokenizer, SiglipTextModel\n\n>>> model = SiglipTextModel.from_pretrained(\"google/siglip-base-patch16-224\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/siglip-base-patch16-224\")\n\n>>> # important: make sure to set padding=\"max_length\" as that's how the model was trained\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=\"max_length\", return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, SiglipVisionModel\n\n>>> model = SiglipVisionModel.from_pretrained(\"google/siglip-base-patch16-224\")\n>>> processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled features\n```"]