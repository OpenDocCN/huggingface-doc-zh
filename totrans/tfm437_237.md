# T5

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/t5`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/t5)

![æ¨¡å‹](https://huggingface.co/models?filter=t5) ![ç©ºé—´](https://huggingface.co/spaces/docs-demos/t5-base) ![è®ºæ–‡é¡µé¢](https://huggingface.co/papers/1910.10683)

## æ¦‚è¿°

T5 æ¨¡å‹åœ¨[æ¢ç´¢ç»Ÿä¸€æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨çš„è¿ç§»å­¦ä¹ æé™](https://arxiv.org/pdf/1910.10683.pdf)ä¸­ç”±[Colin Raffel](https://huggingface.co/craffel)ã€Noam Shazeerã€[Adam Roberts](https://huggingface.co/adarob)ã€Katherine Leeã€Sharan Narangã€Michael Matenaã€Yanqi Zhouã€Wei Liã€[Peter J. Liu](https://huggingface.co/peterjliu)æå‡ºã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*è¿ç§»å­¦ä¹ ï¼Œå³æ¨¡å‹é¦–å…ˆåœ¨æ•°æ®ä¸°å¯Œçš„ä»»åŠ¡ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå·²æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„ä¸€ç§å¼ºå¤§æŠ€æœ¯ã€‚è¿ç§»å­¦ä¹ çš„æœ‰æ•ˆæ€§å‚¬ç”Ÿäº†å¤šç§æ–¹æ³•ã€æ–¹æ³•è®ºå’Œå®è·µã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ä¸ªå°†æ¯ä¸ªè¯­è¨€é—®é¢˜è½¬æ¢ä¸ºæ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ¢ç´¢äº† NLP çš„è¿ç§»å­¦ä¹ æŠ€æœ¯é¢†åŸŸã€‚æˆ‘ä»¬çš„ç³»ç»Ÿç ”ç©¶æ¯”è¾ƒäº†æ•°åä¸ªè¯­è¨€ç†è§£ä»»åŠ¡ä¸Šçš„é¢„è®­ç»ƒç›®æ ‡ã€æ¶æ„ã€æ— æ ‡ç­¾æ•°æ®é›†ã€è¿ç§»æ–¹æ³•å’Œå…¶ä»–å› ç´ ã€‚é€šè¿‡å°†æˆ‘ä»¬çš„æ¢ç´¢è§è§£ä¸è§„æ¨¡å’Œæˆ‘ä»¬çš„æ–°â€œå·¨å¤§å¹²å‡€çˆ¬å–è¯­æ–™åº“â€ç›¸ç»“åˆï¼Œæˆ‘ä»¬åœ¨è®¸å¤šæ¶µç›–æ‘˜è¦ã€é—®ç­”ã€æ–‡æœ¬åˆ†ç±»ç­‰æ–¹é¢çš„åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚ä¸ºäº†ä¿ƒè¿›æœªæ¥å…³äº NLP çš„è¿ç§»å­¦ä¹ çš„å·¥ä½œï¼Œæˆ‘ä»¬å‘å¸ƒäº†æˆ‘ä»¬çš„æ•°æ®é›†ã€é¢„è®­ç»ƒæ¨¡å‹å’Œä»£ç ã€‚*

æ‰€æœ‰æ£€æŸ¥ç‚¹éƒ½å¯ä»¥åœ¨[hub](https://huggingface.co/models?search=t5)ä¸Šæ‰¾åˆ°ã€‚

è¯¥æ¨¡å‹ç”±[thomwolf](https://huggingface.co/thomwolf)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/google-research/text-to-text-transfer-transformer)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   T5 æ˜¯ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œé¢„è®­ç»ƒäºæ— ç›‘ç£å’Œç›‘ç£ä»»åŠ¡çš„å¤šä»»åŠ¡æ··åˆä¸­ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½è½¬æ¢ä¸ºæ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼ã€‚é€šè¿‡ä¸ºæ¯ä¸ªä»»åŠ¡çš„è¾“å…¥æ·»åŠ ä¸åŒçš„å‰ç¼€ï¼Œä¾‹å¦‚ï¼Œå¯¹äºç¿»è¯‘ï¼š*å°†è‹±è¯­ç¿»è¯‘æˆå¾·è¯­ï¼šâ€¦*ï¼Œå¯¹äºæ‘˜è¦ï¼š*æ€»ç»“ï¼šâ€¦*ï¼ŒT5 å¯ä»¥åœ¨å„ç§ä»»åŠ¡ä¸Šç›´æ¥ä½¿ç”¨ã€‚

+   é¢„è®­ç»ƒåŒ…æ‹¬ç›‘ç£å’Œè‡ªç›‘ç£è®­ç»ƒã€‚ç›‘ç£è®­ç»ƒæ˜¯åœ¨ GLUE å’Œ SuperGLUE åŸºå‡†æä¾›çš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œçš„ï¼ˆå°†å®ƒä»¬è½¬æ¢ä¸ºä¸Šé¢è§£é‡Šçš„æ–‡æœ¬åˆ°æ–‡æœ¬ä»»åŠ¡ï¼‰ã€‚

+   è‡ªç›‘ç£è®­ç»ƒä½¿ç”¨æŸåçš„æ ‡è®°ï¼Œéšæœºåˆ é™¤ 15%çš„æ ‡è®°å¹¶ç”¨å•ç‹¬çš„æ ‡è®°æ›¿æ¢å®ƒä»¬ï¼ˆå¦‚æœæœ‰å‡ ä¸ªè¿ç»­çš„æ ‡è®°è¢«æ ‡è®°ä¸ºåˆ é™¤ï¼Œåˆ™æ•´ä¸ªç»„å°†è¢«æ›¿æ¢ä¸ºå•ä¸ªæ ‡è®°ï¼‰ã€‚ç¼–ç å™¨çš„è¾“å…¥æ˜¯æŸåçš„å¥å­ï¼Œè§£ç å™¨çš„è¾“å…¥æ˜¯åŸå§‹å¥å­ï¼Œç›®æ ‡æ˜¯è¢«åˆ é™¤çš„æ ‡è®°ï¼Œç”±å®ƒä»¬çš„æ ‡è®°æ ‡è®°ã€‚

+   T5 ä½¿ç”¨ç›¸å¯¹æ ‡é‡åµŒå…¥ã€‚ç¼–ç å™¨è¾“å…¥çš„å¡«å……å¯ä»¥åœ¨å·¦ä¾§å’Œå³ä¾§è¿›è¡Œã€‚

+   æŸ¥çœ‹ä¸‹é¢çš„è®­ç»ƒã€æ¨ç†å’Œè„šæœ¬éƒ¨åˆ†ï¼Œäº†è§£æœ‰å…³ä½¿ç”¨çš„æ‰€æœ‰è¯¦ç»†ä¿¡æ¯ã€‚

T5 æœ‰ä¸åŒçš„å°ºå¯¸ï¼š

+   [t5-small](https://huggingface.co/t5-small)

+   [t5-base](https://huggingface.co/t5-base)

+   [t5-large](https://huggingface.co/t5-large)

+   [t5-3b](https://huggingface.co/t5-3b)

+   [t5-11b](https://huggingface.co/t5-11b)ã€‚

åŸºäºåŸå§‹ T5 æ¨¡å‹ï¼ŒGoogle å‘å¸ƒäº†ä¸€äº›åç»­ä½œå“ï¼š

+   **T5v1.1**ï¼šT5v1.1 æ˜¯ T5 çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œè¿›è¡Œäº†ä¸€äº›æ¶æ„è°ƒæ•´ï¼Œä»…åœ¨ C4 ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œæ²¡æœ‰æ··åˆç›‘ç£ä»»åŠ¡ã€‚è¯·å‚é˜… T5v1.1 çš„æ–‡æ¡£ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ã€‚

+   MT5ï¼šmT5 æ˜¯ä¸€ä¸ªå¤šè¯­è¨€ T5 æ¨¡å‹ã€‚å®ƒåœ¨åŒ…æ‹¬ 101 ç§è¯­è¨€çš„ mC4 è¯­æ–™åº“ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚è¯·å‚è€ƒå¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°çš„ mT5 çš„æ–‡æ¡£ã€‚

+   byT5ï¼šbyT5 æ˜¯ä¸€ä¸ªåœ¨å­—èŠ‚åºåˆ—è€Œä¸æ˜¯ SentencePiece å­è¯æ ‡è®°åºåˆ—ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ T5 æ¨¡å‹ã€‚è¯·å‚è€ƒå¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°çš„ byT5 çš„æ–‡æ¡£ã€‚

+   UL2ï¼šUL2 æ˜¯ä¸€ä¸ªç±»ä¼¼ T5 çš„æ¨¡å‹ï¼Œé¢„è®­ç»ƒäºå„ç§å»å™ªç›®æ ‡ã€‚

+   Flan-T5ï¼šFlan æ˜¯ä¸€ç§åŸºäºæç¤ºçš„é¢„è®­ç»ƒæ–¹æ³•ã€‚Flan-T5 æ˜¯åœ¨ Flan æ•°æ®é›†ä¸Šè®­ç»ƒçš„ T5 æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š`taskmaster2`ã€`djaym7/wiki_dialog`ã€`deepmind/code_contests`ã€`lambada`ã€`gsm8k`ã€`aqua_rat`ã€`esnli`ã€`quasc`å’Œ`qed`ã€‚

+   FLan-UL2ï¼šä½¿ç”¨â€œFlanâ€æç¤ºè°ƒæ•´å’Œæ•°æ®é›†æ”¶é›†å¯¹ UL2 æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

+   UMT5ï¼šUmT5 æ˜¯ä¸€ä¸ªå¤šè¯­è¨€ T5 æ¨¡å‹ï¼Œè®­ç»ƒäºä¸€ä¸ªæ”¹è¿›å’Œæ›´æ–°çš„ mC4 å¤šè¯­è¨€è¯­æ–™åº“ï¼Œè·¨ 107 ç§è¯­è¨€ï¼Œä½¿ç”¨ä¸€ç§æ–°çš„é‡‡æ ·æ–¹æ³• UniMaxã€‚è¯·å‚è€ƒå¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°çš„ mT5 çš„æ–‡æ¡£ã€‚

## è®­ç»ƒ

T5 æ˜¯ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œå°†æ‰€æœ‰ NLP é—®é¢˜è½¬æ¢ä¸ºæ–‡æœ¬åˆ°æ–‡æœ¬çš„æ ¼å¼ã€‚å®ƒä½¿ç”¨æ•™å¸ˆå¼ºåˆ¶è¿›è¡Œè®­ç»ƒã€‚è¿™æ„å‘³ç€åœ¨è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬æ€»æ˜¯éœ€è¦ä¸€ä¸ªè¾“å…¥åºåˆ—å’Œä¸€ä¸ªç›¸åº”çš„ç›®æ ‡åºåˆ—ã€‚è¾“å…¥åºåˆ—é€šè¿‡`input_ids`é¦ˆé€åˆ°æ¨¡å‹ã€‚ç›®æ ‡åºåˆ—å‘å³ç§»åŠ¨ï¼Œå³åœ¨å‰é¢åŠ ä¸Šä¸€ä¸ªèµ·å§‹åºåˆ—æ ‡è®°ï¼Œå¹¶é€šè¿‡`decoder_input_ids`é¦ˆé€åˆ°è§£ç å™¨ã€‚åœ¨æ•™å¸ˆå¼ºåˆ¶é£æ ¼ä¸­ï¼Œç›®æ ‡åºåˆ—ç„¶åé™„åŠ  EOS æ ‡è®°ï¼Œå¹¶å¯¹åº”äº`labels`ã€‚åœ¨è¿™é‡Œï¼ŒPAD æ ‡è®°è¢«ç”¨ä½œèµ·å§‹åºåˆ—æ ‡è®°ã€‚T5 å¯ä»¥ä»¥ç›‘ç£å’Œæ— ç›‘ç£çš„æ–¹å¼è¿›è¡Œè®­ç»ƒ/å¾®è°ƒã€‚

å¯ä»¥ä½¿ç”¨ T5ForConditionalGenerationï¼ˆæˆ– Tensorflow/Flax å˜ä½“ï¼‰ï¼Œå®ƒåœ¨è§£ç å™¨é¡¶éƒ¨åŒ…å«äº†è¯­è¨€å»ºæ¨¡å¤´ã€‚

+   æ— ç›‘ç£å»å™ªè®­ç»ƒ

åœ¨è¿™ç§è®¾ç½®ä¸­ï¼Œè¾“å…¥åºåˆ—çš„ç‰‡æ®µè¢«æ‰€è°“çš„å“¨å…µæ ‡è®°ï¼ˆ*åˆå*å”¯ä¸€çš„æ©ç æ ‡è®°ï¼‰å±è”½ï¼Œè¾“å‡ºåºåˆ—ç”±ç›¸åŒçš„å“¨å…µæ ‡è®°å’Œ*çœŸå®*è¢«å±è”½çš„æ ‡è®°çš„ä¸²è”å½¢æˆã€‚æ¯ä¸ªå“¨å…µæ ‡è®°ä»£è¡¨è¿™ä¸ªå¥å­çš„ä¸€ä¸ªå”¯ä¸€çš„æ©ç æ ‡è®°ï¼Œåº”è¯¥ä»¥`<extra_id_0>`ã€`<extra_id_1>`ç­‰å½¢å¼å¼€å§‹ï¼Œä¸€ç›´åˆ°`<extra_id_99>`ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒT5Tokenizer ä¸­æœ‰ 100 ä¸ªå“¨å…µæ ‡è®°å¯ç”¨ã€‚

ä¾‹å¦‚ï¼Œå¥å­â€œThe cute dog walks in the parkâ€ä¸­çš„â€œcute dogâ€å’Œâ€œtheâ€è¢«å±è”½ååº”è¯¥è¢«å¤„ç†å¦‚ä¸‹ï¼š

```py
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> input_ids = tokenizer("The <extra_id_0> walks in <extra_id_1> park", return_tensors="pt").input_ids
>>> labels = tokenizer("<extra_id_0> cute dog <extra_id_1> the <extra_id_2>", return_tensors="pt").input_ids

>>> # the forward function automatically creates the correct decoder_input_ids
>>> loss = model(input_ids=input_ids, labels=labels).loss
>>> loss.item()
3.7837
```

å¦‚æœæ‚¨æœ‰å…´è¶£åœ¨æ–°è¯­æ–™åº“ä¸Šé¢„è®­ç»ƒ T5ï¼Œè¯·æŸ¥çœ‹ Examples ç›®å½•ä¸­çš„[run_t5_mlm_flax.py](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling)è„šæœ¬ã€‚

+   ç›‘ç£è®­ç»ƒ

åœ¨è¿™ç§è®¾ç½®ä¸­ï¼Œè¾“å…¥åºåˆ—å’Œè¾“å‡ºåºåˆ—æ˜¯æ ‡å‡†çš„åºåˆ—åˆ°åºåˆ—çš„è¾“å…¥è¾“å‡ºæ˜ å°„ã€‚å‡è®¾æˆ‘ä»¬æƒ³è¦ä¸ºç¿»è¯‘è°ƒæ•´æ¨¡å‹ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªè®­ç»ƒç¤ºä¾‹ï¼šè¾“å…¥åºåˆ—â€œThe house is wonderful.â€å’Œè¾“å‡ºåºåˆ—â€œDas Haus ist wunderbar.â€ï¼Œé‚£ä¹ˆå®ƒä»¬åº”è¯¥è¢«å‡†å¤‡ä¸ºæ¨¡å‹å¦‚ä¸‹ï¼š

```py
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> input_ids = tokenizer("translate English to German: The house is wonderful.", return_tensors="pt").input_ids
>>> labels = tokenizer("Das Haus ist wunderbar.", return_tensors="pt").input_ids

>>> # the forward function automatically creates the correct decoder_input_ids
>>> loss = model(input_ids=input_ids, labels=labels).loss
>>> loss.item()
0.2542
```

å¦‚æ‚¨æ‰€è§ï¼Œä¸ºäº†è®¡ç®—æŸå¤±ï¼Œæ¨¡å‹åªéœ€è¦ 2 ä¸ªè¾“å…¥ï¼š`input_ids`ï¼ˆç¼–ç è¾“å…¥åºåˆ—çš„`input_ids`ï¼‰å’Œ`labels`ï¼ˆç¼–ç ç›®æ ‡åºåˆ—çš„`input_ids`ï¼‰ã€‚æ¨¡å‹å°†æ ¹æ®`labels`è‡ªåŠ¨åˆ›å»º`decoder_input_ids`ï¼Œå°†å®ƒä»¬å‘å³ç§»åŠ¨ä¸€ä¸ªä½ç½®å¹¶åœ¨å‰é¢æ·»åŠ `config.decoder_start_token_id`ï¼Œå¯¹äº T5 æ¥è¯´ç­‰äº 0ï¼ˆå³å¡«å……æ ‡è®°çš„ idï¼‰ã€‚è¿˜è¦æ³¨æ„ä»»åŠ¡å‰ç¼€ï¼šæˆ‘ä»¬åœ¨ç¼–ç ä¹‹å‰åœ¨è¾“å…¥åºåˆ—å‰é¢æ·»åŠ äº†â€˜translate English to German: â€™ã€‚è¿™å°†æœ‰åŠ©äºæé«˜æ€§èƒ½ï¼Œå› ä¸ºè¿™ä¸ªä»»åŠ¡å‰ç¼€åœ¨ T5 çš„é¢„è®­ç»ƒä¸­ä½¿ç”¨è¿‡ã€‚

ç„¶è€Œï¼Œä¸Šé¢çš„ç¤ºä¾‹åªæ˜¾ç¤ºäº†ä¸€ä¸ªè®­ç»ƒç¤ºä¾‹ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬é€šå¸¸æ‰¹é‡è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¿…é¡»å°†ç¤ºä¾‹å¡«å……/æˆªæ–­åˆ°ç›¸åŒçš„é•¿åº¦ã€‚å¯¹äºç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œé€šå¸¸å®šä¹‰`max_source_length`å’Œ`max_target_length`ï¼Œåˆ†åˆ«ç¡®å®šè¾“å…¥å’Œè¾“å‡ºåºåˆ—çš„æœ€å¤§é•¿åº¦ï¼ˆå¦åˆ™å°†è¢«æˆªæ–­ï¼‰ã€‚è¿™äº›åº”æ ¹æ®ä»»åŠ¡ä»”ç»†è®¾ç½®ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬å¿…é¡»ç¡®ä¿`labels`çš„å¡«å……æ ‡è®° ID ä¸è¢«æŸå¤±å‡½æ•°è€ƒè™‘ã€‚åœ¨ PyTorch å’Œ Tensorflow ä¸­ï¼Œå¯ä»¥é€šè¿‡ç”¨-100 æ›¿æ¢å®ƒä»¬æ¥å®ç°ï¼Œ-100 æ˜¯`CrossEntropyLoss`çš„`ignore_index`ã€‚åœ¨ Flax ä¸­ï¼Œå¯ä»¥ä½¿ç”¨`decoder_attention_mask`æ¥å¿½ç•¥æŸå¤±ä¸­çš„å¡«å……æ ‡è®°ï¼ˆæœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[Flax æ‘˜è¦è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/summarization)ï¼‰ã€‚æˆ‘ä»¬è¿˜å°†`attention_mask`ä½œä¸ºæ¨¡å‹çš„é™„åŠ è¾“å…¥ä¼ é€’ï¼Œä»¥ç¡®ä¿å¿½ç•¥è¾“å…¥çš„å¡«å……æ ‡è®°ã€‚ä¸‹é¢çš„ä»£ç ç¤ºä¾‹è¯´æ˜äº†æ‰€æœ‰è¿™äº›ã€‚

```py
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>> import torch

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> # the following 2 hyperparameters are task-specific
>>> max_source_length = 512
>>> max_target_length = 128

>>> # Suppose we have the following 2 training examples:
>>> input_sequence_1 = "Welcome to NYC"
>>> output_sequence_1 = "Bienvenue Ã  NYC"

>>> input_sequence_2 = "HuggingFace is a company"
>>> output_sequence_2 = "HuggingFace est une entreprise"

>>> # encode the inputs
>>> task_prefix = "translate English to French: "
>>> input_sequences = [input_sequence_1, input_sequence_2]

>>> encoding = tokenizer(
...     [task_prefix + sequence for sequence in input_sequences],
...     padding="longest",
...     max_length=max_source_length,
...     truncation=True,
...     return_tensors="pt",
... )

>>> input_ids, attention_mask = encoding.input_ids, encoding.attention_mask

>>> # encode the targets
>>> target_encoding = tokenizer(
...     [output_sequence_1, output_sequence_2],
...     padding="longest",
...     max_length=max_target_length,
...     truncation=True,
...     return_tensors="pt",
... )
>>> labels = target_encoding.input_ids

>>> # replace padding token id's of the labels by -100 so it's ignored by the loss
>>> labels[labels == tokenizer.pad_token_id] = -100

>>> # forward pass
>>> loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss
>>> loss.item()
0.188
```

é¢å¤–çš„è®­ç»ƒæç¤ºï¼š

+   å½“ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨æ—¶ï¼ŒT5 æ¨¡å‹éœ€è¦æ¯”`Trainer`ä¸­è®¾ç½®çš„é»˜è®¤å­¦ä¹ ç‡ç•¥é«˜ã€‚é€šå¸¸ï¼Œå¯¹äºå¤§å¤šæ•°é—®é¢˜ï¼ˆåˆ†ç±»ã€æ‘˜è¦ã€ç¿»è¯‘ã€é—®ç­”ã€é—®é¢˜ç”Ÿæˆï¼‰ï¼Œ1e-4 å’Œ 3e-4 æ•ˆæœå¾ˆå¥½ã€‚è¯·æ³¨æ„ï¼ŒT5 æ˜¯ä½¿ç”¨ AdaFactor ä¼˜åŒ–å™¨è¿›è¡Œé¢„è®­ç»ƒçš„ã€‚

æ ¹æ®[è¿™ç¯‡è®ºå›å¸–å­](https://discuss.huggingface.co/t/t5-finetuning-tips/684)ï¼Œä»»åŠ¡å‰ç¼€åœ¨ï¼ˆ1ï¼‰è¿›è¡Œå¤šä»»åŠ¡è®­ç»ƒæ—¶ï¼ˆ2ï¼‰æ‚¨çš„ä»»åŠ¡ä¸ T5 é¢„è®­ç»ƒæ··åˆä¸­ä½¿ç”¨çš„ç›‘ç£ä»»åŠ¡ä¹‹ä¸€ç±»ä¼¼æˆ–ç›¸å…³æ—¶å¾ˆé‡è¦ï¼ˆè¯·å‚é˜…[è®ºæ–‡](https://arxiv.org/pdf/1910.10683.pdf)çš„é™„å½• Dï¼Œäº†è§£ä½¿ç”¨çš„ä»»åŠ¡å‰ç¼€ï¼‰ã€‚

å¦‚æœåœ¨ TPU ä¸Šè®­ç»ƒï¼Œå»ºè®®å°†æ•°æ®é›†çš„æ‰€æœ‰ç¤ºä¾‹å¡«å……åˆ°ç›¸åŒçš„é•¿åº¦ï¼Œæˆ–è€…ä½¿ç”¨*pad_to_multiple_of*æ¥ä½¿ç”¨å°‘é‡é¢„å®šä¹‰çš„æ¡¶å¤§å°ä»¥é€‚åº”æ‰€æœ‰ç¤ºä¾‹ã€‚åœ¨ TPU ä¸ŠåŠ¨æ€å¡«å……æ‰¹æ¬¡åˆ°æœ€é•¿ç¤ºä¾‹ä¸å»ºè®®ï¼Œå› ä¸ºå®ƒä¼šåœ¨è®­ç»ƒæœŸé—´é‡åˆ°çš„æ¯ä¸ªæ‰¹æ¬¡å½¢çŠ¶è§¦å‘é‡æ–°ç¼–è¯‘ï¼Œä»è€Œæ˜¾è‘—å‡æ…¢è®­ç»ƒé€Ÿåº¦ã€‚åªå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„ç¤ºä¾‹ä¼šå¯¼è‡´åœ¨ TPU ä¸Šè®­ç»ƒéå¸¸ç¼“æ…¢ã€‚

## æ¨ç†

åœ¨æ¨ç†æ—¶ï¼Œå»ºè®®ä½¿ç”¨ generate()ã€‚è¿™ç§æ–¹æ³•è´Ÿè´£å¯¹è¾“å…¥è¿›è¡Œç¼–ç ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›å±‚å°†ç¼–ç éšè—çŠ¶æ€é¦ˆé€åˆ°è§£ç å™¨ï¼Œè‡ªå›å½’åœ°ç”Ÿæˆè§£ç å™¨è¾“å‡ºã€‚æŸ¥çœ‹[è¿™ç¯‡åšæ–‡](https://huggingface.co/blog/how-to-generate)äº†è§£ä½¿ç”¨ Transformers ç”Ÿæˆæ–‡æœ¬çš„æ‰€æœ‰ç»†èŠ‚ã€‚è¿˜æœ‰[è¿™ç¯‡åšæ–‡](https://huggingface.co/blog/encoder-decoder#encoder-decoder)è§£é‡Šäº†ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ä¸­ç”Ÿæˆçš„å·¥ä½œåŸç†ã€‚

```py
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> input_ids = tokenizer("translate English to German: The house is wonderful.", return_tensors="pt").input_ids
>>> outputs = model.generate(input_ids)
>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))
Das Haus ist wunderbar.
```

è¯·æ³¨æ„ï¼ŒT5 ä½¿ç”¨`pad_token_id`ä½œä¸º`decoder_start_token_id`ï¼Œå› æ­¤åœ¨ç”Ÿæˆæ—¶å¦‚æœä¸ä½¿ç”¨ generate()ï¼Œè¯·ç¡®ä¿ä»¥`pad_token_id`å¼€å¤´ã€‚

ä¸Šé¢çš„ç¤ºä¾‹åªæ˜¾ç¤ºäº†ä¸€ä¸ªå•ç‹¬çš„ç¤ºä¾‹ã€‚æ‚¨ä¹Ÿå¯ä»¥è¿›è¡Œæ‰¹é‡æ¨ç†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> task_prefix = "translate English to German: "
>>> # use different length sentences to test batching
>>> sentences = ["The house is wonderful.", "I like to work in NYC."]

>>> inputs = tokenizer([task_prefix + sentence for sentence in sentences], return_tensors="pt", padding=True)

>>> output_sequences = model.generate(
...     input_ids=inputs["input_ids"],
...     attention_mask=inputs["attention_mask"],
...     do_sample=False,  # disable sampling to test if batching affects output
... )

>>> print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))
['Das Haus ist wunderbar.', 'Ich arbeite gerne in NYC.']
```

ç”±äº T5 å·²ç»ä½¿ç”¨äº†è·¨åº¦æ©ç å»å™ªç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œå› æ­¤å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”¨äºé¢„æµ‹æ ‡è®°ï¼ˆè¢«æ©ç çš„ï¼‰æ ‡è®°ã€‚ç„¶åï¼Œé¢„æµ‹çš„æ ‡è®°å°†è¢«æ”¾ç½®åœ¨æ ‡è®°ä¹‹é—´ã€‚

```py
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> input_ids = tokenizer("The <extra_id_0> walks in <extra_id_1> park", return_tensors="pt").input_ids

>>> sequence_ids = model.generate(input_ids)
>>> sequences = tokenizer.batch_decode(sequence_ids)
>>> sequences
['<pad><extra_id_0> park offers<extra_id_1> the<extra_id_2> park.</s>']
```

## æ€§èƒ½

å¦‚æœå¸Œæœ›è·å¾—æ›´å¿«çš„è®­ç»ƒå’Œæ¨ç†æ€§èƒ½ï¼Œè¯·ä¸º NVIDIA GPU å®‰è£…[NVIDIA APEX](https://github.com/NVIDIA/apex#quick-start)ï¼Œæˆ–ä¸º AMD GPU å®‰è£…[ROCm APEX](https://github.com/ROCmSoftwarePlatform/apex)ï¼Œç„¶åæ¨¡å‹å°†è‡ªåŠ¨ä½¿ç”¨`apex.normalization.FusedRMSNorm`è€Œä¸æ˜¯`T5LayerNorm`ã€‚å‰è€…ä½¿ç”¨ä¼˜åŒ–çš„èåˆå†…æ ¸ï¼Œæ¯”åè€…å¿«å‡ å€ã€‚

## èµ„æº

ä¸€ä¸ªå®˜æ–¹ Hugging Face å’Œç¤¾åŒºèµ„æºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ T5ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ª Pull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

æ–‡æœ¬åˆ†ç±»

+   ä¸€ä¸ªå…³äºå¦‚ä½•[å¾®è°ƒ T5 è¿›è¡Œåˆ†ç±»å’Œå¤šé¡¹é€‰æ‹©](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb)çš„ç¬”è®°æœ¬ã€‚

+   ä¸€ä¸ªå…³äºå¦‚ä½•[å¾®è°ƒ T5 è¿›è¡Œæƒ…æ„Ÿè·¨åº¦æå–](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb)çš„ç¬”è®°æœ¬ã€‚ğŸŒ

æ ‡è®°åˆ†ç±»

+   ä¸€ä¸ªå…³äºå¦‚ä½•[å¾®è°ƒ T5 è¿›è¡Œå‘½åå®ä½“è¯†åˆ«](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing)çš„ç¬”è®°æœ¬ã€‚ğŸŒ

æ–‡æœ¬ç”Ÿæˆ

+   ä¸€ä¸ªå…³äº[å¯¹ Ruby ä»£ç ç”Ÿæˆ docstrings è¿›è¡Œ Fine-tuning CodeT5](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tune_CodeT5_for_generating_docstrings_from_Ruby_code.ipynb)çš„ç¬”è®°æœ¬ã€‚

æ‘˜è¦

+   ä¸€ä¸ªå…³äº[åœ¨ TPU ä¸Šå¯¹ T5-base-dutch è¿›è¡Œè·å…°è¯­æŠ½è±¡æ‘˜è¦çš„ Fine-tuning](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tuning_Dutch_T5_base_on_CNN_Daily_Mail_for_summarization_(on_TPU_using_HuggingFace_Accelerate).ipynb)çš„ç¬”è®°æœ¬ã€‚

+   ä¸€ä¸ªå…³äºå¦‚ä½•[åœ¨ PyTorch ä¸­å¾®è°ƒ T5 è¿›è¡Œæ‘˜è¦å¹¶ä½¿ç”¨ WandB è·Ÿè¸ªå®éªŒ](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb#scrollTo=OKRpFvYhBauC)çš„ç¬”è®°æœ¬ã€‚ğŸŒ

+   ä¸€ç¯‡å…³äº[åˆ†å¸ƒå¼è®­ç»ƒï¼šä½¿ç”¨ğŸ¤— Transformers å’Œ Amazon SageMaker è®­ç»ƒ BART/T5 è¿›è¡Œæ‘˜è¦](https://huggingface.co/blog/sagemaker-distributed-training-seq2seq)çš„åšå®¢æ–‡ç« ã€‚

+   T5ForConditionalGeneration ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)æ”¯æŒã€‚

+   TFT5ForConditionalGeneration ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)æ”¯æŒã€‚

+   FlaxT5ForConditionalGeneration ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/summarization)æ”¯æŒã€‚

+   ğŸ¤— Hugging Face è¯¾ç¨‹çš„[æ‘˜è¦](https://huggingface.co/course/chapter7/5?fw=pt#summarization)ç« èŠ‚ã€‚

+   æ‘˜è¦ä»»åŠ¡æŒ‡å—

å¡«å……æ©ç 

+   FlaxT5ForConditionalGeneration ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#t5-like-span-masked-language-modeling)æ”¯æŒï¼Œç”¨äºè®­ç»ƒå…·æœ‰è·¨åº¦æ©ç è¯­è¨€æ¨¡å‹ç›®æ ‡çš„ T5ã€‚è¯¥è„šæœ¬è¿˜å±•ç¤ºäº†å¦‚ä½•è®­ç»ƒ T5 åˆ†è¯å™¨ã€‚FlaxT5ForConditionalGeneration ä¹Ÿç”±è¿™ä¸ª[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb)æ”¯æŒã€‚

ç¿»è¯‘

+   T5ForConditionalGeneration ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/translation)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)æ”¯æŒã€‚

+   TFT5ForConditionalGeneration ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/translation)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)æ”¯æŒã€‚

+   ç¿»è¯‘ä»»åŠ¡æŒ‡å—

é—®ç­”

+   ä¸€ä¸ªå…³äºå¦‚ä½•ä½¿ç”¨ TensorFlow 2 å¯¹[T5 è¿›è¡Œé—®é¢˜å›ç­”å¾®è°ƒçš„ç¬”è®°æœ¬](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb)ã€‚ğŸŒ

+   ä¸€ä¸ªå…³äºå¦‚ä½•åœ¨ TPU ä¸Šå¯¹[T5 è¿›è¡Œé—®é¢˜å›ç­”å¾®è°ƒçš„ç¬”è®°æœ¬](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil)ã€‚

ğŸš€ **éƒ¨ç½²**

+   å…³äºå¦‚ä½•ä»¥ä½äº 500 ç¾å…ƒçš„ä»·æ ¼éƒ¨ç½²[T5 11B è¿›è¡Œæ¨ç†çš„åšå®¢æ–‡ç« ](https://www.philschmid.de/deploy-t5-11b)ã€‚

## T5Config

### `class transformers.T5Config`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/configuration_t5.py#L34)

```py
( vocab_size = 32128 d_model = 512 d_kv = 64 d_ff = 2048 num_layers = 6 num_decoder_layers = None num_heads = 8 relative_attention_num_buckets = 32 relative_attention_max_distance = 128 dropout_rate = 0.1 layer_norm_epsilon = 1e-06 initializer_factor = 1.0 feed_forward_proj = 'relu' is_encoder_decoder = True use_cache = True pad_token_id = 0 eos_token_id = 1 classifier_dropout = 0.0 **kwargs )
```

å‚æ•°

+   `vocab_size` (`int`, *optional*, defaults to 32128) â€” T5 æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨ T5Model æˆ– TFT5Model æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚

+   `d_model` (`int`, *optional*, defaults to 512) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„å¤§å°ã€‚

+   `d_kv` (`int`, *optional*, defaults to 64) â€” æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„é”®ã€æŸ¥è¯¢ã€å€¼æŠ•å½±çš„å¤§å°ã€‚æŠ•å½±å±‚çš„`inner_dim`å°†è¢«å®šä¹‰ä¸º`num_heads * d_kv`ã€‚

+   `d_ff` (`int`, *optional*, defaults to 2048) â€” æ¯ä¸ª`T5Block`ä¸­é—´çš„å‰é¦ˆå±‚çš„å¤§å°ã€‚

+   `num_layers` (`int`, *optional*, defaults to 6) â€” Transformer ç¼–ç å™¨ä¸­éšè—å±‚çš„æ•°é‡ã€‚

+   `num_decoder_layers` (`int`, *optional*) â€” Transformer è§£ç å™¨ä¸­éšè—å±‚çš„æ•°é‡ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†ä½¿ç”¨ä¸`num_layers`ç›¸åŒçš„å€¼ã€‚

+   `num_heads` (`int`, *optional*, defaults to 8) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `relative_attention_num_buckets` (`int`, *optional*, defaults to 32) â€” æ¯ä¸ªæ³¨æ„åŠ›å±‚ä½¿ç”¨çš„æ¡¶æ•°é‡ã€‚

+   `relative_attention_max_distance` (`int`, *optional*, defaults to 128) â€” è¾ƒé•¿åºåˆ—çš„æœ€å¤§è·ç¦»ï¼Œç”¨äºæ¡¶åˆ†ç¦»ã€‚

+   `dropout_rate` (`float`, *optional*, defaults to 0.1) â€” æ‰€æœ‰ dropout å±‚çš„æ¯”ç‡ã€‚

+   `classifier_dropout` (`float`, *optional*, defaults to 0.0) â€” åˆ†ç±»å™¨çš„ dropout æ¯”ç‡ã€‚

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-6) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `initializer_factor` (`float`, *optional*, defaults to 1) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„å› å­ï¼ˆåº”ä¿æŒä¸º 1ï¼Œç”¨äºå†…éƒ¨åˆå§‹åŒ–æµ‹è¯•ï¼‰ã€‚

+   `feed_forward_proj` (`string`, *optional*, defaults to `"relu"`) â€” è¦ä½¿ç”¨çš„å‰é¦ˆå±‚ç±»å‹ã€‚åº”ä¸º`"relu"`æˆ–`"gated-gelu"`ä¹‹ä¸€ã€‚T5v1.1 ä½¿ç”¨`"gated-gelu"`å‰é¦ˆæŠ•å½±ã€‚åŸå§‹ T5 ä½¿ç”¨`"relu"`ã€‚

+   `use_cache` (`bool`, *optional*, defaults to `True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ T5Model æˆ– TFT5Model é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ– T5 æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº T5 [t5-small](https://huggingface.co/t5-small)æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

## T5Tokenizer

### `class transformers.T5Tokenizer`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/tokenization_t5.py#L63)

```py
( vocab_file eos_token = '</s>' unk_token = '<unk>' pad_token = '<pad>' extra_ids = 100 additional_special_tokens = None sp_model_kwargs: Optional = None legacy = None **kwargs )
```

å‚æ•°

+   `vocab_file`ï¼ˆ`str`ï¼‰â€”åŒ…å«å®ä¾‹åŒ–åˆ†è¯å™¨æ‰€éœ€è¯æ±‡è¡¨çš„[SentencePiece](https://github.com/google/sentencepiece)æ–‡ä»¶ï¼ˆé€šå¸¸å…·æœ‰*.spm*æ‰©å±•åï¼‰ã€‚

+   `eos_token`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"</s>"`ï¼‰â€”åºåˆ—ç»“æŸæ ‡è®°ã€‚

    æ„å»ºåºåˆ—æ—¶ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ï¼Œè¿™ä¸æ˜¯ç”¨äºåºåˆ—ç»“æŸçš„æ ‡è®°ã€‚ä½¿ç”¨çš„æ ‡è®°æ˜¯`sep_token`ã€‚

+   `unk_token`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"<unk>"`ï¼‰â€”æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸º IDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚

+   `pad_token`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"<pad>"`ï¼‰â€”ç”¨äºå¡«å……çš„æ ‡è®°ï¼Œä¾‹å¦‚åœ¨æ‰¹å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—æ—¶ä½¿ç”¨ã€‚

+   `extra_ids`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 100ï¼‰â€”æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ç”¨ä½œå“¨å…µçš„é¢å¤– ID æ•°é‡ã€‚è¿™äº›æ ‡è®°å¯é€šè¿‡è°ƒç”¨ get_sentinel_tokens æ–¹æ³•è®¿é—®ä¸ºâ€œ<extra>id{%d}>â€ï¼Œå…¶ä¸­â€{%d}â€æ˜¯ 0 åˆ° extra_ids-1 ä¹‹é—´çš„æ•°å­—ã€‚è¿™äº›æ ‡è®°å¯ä»¥é€šè¿‡è°ƒç”¨ get_sentinel_token_ids æ–¹æ³•æ£€ç´¢ï¼Œé¢å¤–çš„ç‰¹æ®Šæ ‡è®°ï¼ˆ`List[str]`ï¼Œ*å¯é€‰*ï¼‰ï¼šåˆ†è¯å™¨ä½¿ç”¨çš„å…¶ä»–ç‰¹æ®Šæ ‡è®°ã€‚</extra>

+   `sp_model_kwargs`ï¼ˆ`dict`ï¼Œ*å¯é€‰*ï¼‰â€”å°†ä¼ é€’ç»™`SentencePieceProcessor.__init__()`æ–¹æ³•ã€‚[SentencePiece çš„ Python åŒ…è£…å™¨](https://github.com/google/sentencepiece/tree/master/python)å¯ç”¨äºè®¾ç½®ï¼š

    +   `enable_sampling`ï¼šå¯ç”¨å­è¯æ­£åˆ™åŒ–ã€‚

    +   `nbest_size`ï¼šunigram çš„é‡‡æ ·å‚æ•°ã€‚å¯¹äº BPE-Dropout æ— æ•ˆã€‚

        +   `nbest_size = {0,1}`ï¼šä¸æ‰§è¡Œé‡‡æ ·ã€‚

        +   `nbest_size > 1`ï¼šä» nbest_size ç»“æœä¸­é‡‡æ ·ã€‚

        +   `nbest_size < 0`ï¼šå‡è®¾ nbest_size æ˜¯æ— é™çš„ï¼Œå¹¶ä½¿ç”¨å‰å‘è¿‡æ»¤å’Œåå‘é‡‡æ ·ç®—æ³•ä»æ‰€æœ‰å‡è®¾ï¼ˆæ ¼ï¼‰ä¸­é‡‡æ ·ã€‚

    +   `alpha`ï¼šunigram é‡‡æ ·çš„å¹³æ»‘å‚æ•°ï¼Œä»¥åŠ BPE-dropout çš„åˆå¹¶æ“ä½œçš„ä¸¢å¼ƒæ¦‚ç‡ã€‚

+   `legacy`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€”æ˜¯å¦åº”ä½¿ç”¨åˆ†è¯å™¨çš„`legacy`è¡Œä¸ºã€‚Legacy æ˜¯åœ¨#24622 å’Œ#25224 åˆå¹¶ä¹‹å‰ï¼ŒåŒ…æ‹¬ä¿®å¤æ­£ç¡®å¤„ç†å‡ºç°åœ¨ç‰¹æ®Šæ ‡è®°ä¹‹åçš„æ ‡è®°çš„å†…å®¹ã€‚ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š

    +   `legacy=True`ï¼š

æ„å»ºä¸€ä¸ª T5 åˆ†è¯å™¨ã€‚åŸºäº[SentencePiece](https://github.com/google/sentencepiece)ã€‚

æ­¤åˆ†è¯å™¨ç»§æ‰¿è‡ª PreTrainedTokenizerï¼Œå…¶ä¸­åŒ…å«å¤§å¤šæ•°ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

#### `build_inputs_with_special_tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/tokenization_t5.py#L333)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0`ï¼ˆ`List[int]`ï¼‰â€”è¦æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„ ID åˆ—è¡¨ã€‚

+   `token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€”ç”¨äºåºåˆ—å¯¹çš„å¯é€‰ç¬¬äºŒä¸ª ID åˆ—è¡¨ã€‚

è¿”å›

`List[int]`

å…·æœ‰é€‚å½“ç‰¹æ®Šæ ‡è®°çš„ input IDs åˆ—è¡¨ã€‚

é€šè¿‡è¿æ¥å’Œæ·»åŠ ç‰¹æ®Šæ ‡è®°ä»åºåˆ—æˆ–åºåˆ—å¯¹æ„å»ºç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹è¾“å…¥ã€‚åºåˆ—çš„æ ¼å¼å¦‚ä¸‹ï¼š

+   å•ä¸ªåºåˆ—ï¼š`X </s>`

+   åºåˆ—å¯¹ï¼š`A </s> B </s>`

#### `get_special_tokens_mask`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/tokenization_t5.py#L264)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” ID åˆ—è¡¨ã€‚

+   `token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰çš„ç¬¬äºŒä¸ª ID åˆ—è¡¨ï¼Œç”¨äºåºåˆ—å¯¹ã€‚

+   `already_has_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ ‡è®°åˆ—è¡¨æ˜¯å¦å·²ç»æ ¼å¼åŒ–ä¸ºæ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°ã€‚

è¿”å›

`List[int]`

ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼ŒèŒƒå›´ä¸º[0, 1]ï¼š1 è¡¨ç¤ºç‰¹æ®Šæ ‡è®°ï¼Œ0 è¡¨ç¤ºåºåˆ—æ ‡è®°ã€‚

ä»æ²¡æœ‰æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„æ ‡è®°åˆ—è¡¨ä¸­æ£€ç´¢åºåˆ— IDã€‚å½“ä½¿ç”¨åˆ†è¯å™¨çš„`prepare_for_model`æ–¹æ³•æ·»åŠ ç‰¹æ®Šæ ‡è®°æ—¶ï¼Œå°†è°ƒç”¨æ­¤æ–¹æ³•ã€‚

#### `create_token_type_ids_from_sequences`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/tokenization_t5.py#L311)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” ID åˆ—è¡¨ã€‚

+   `token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰çš„ç¬¬äºŒä¸ª ID åˆ—è¡¨ï¼Œç”¨äºåºåˆ—å¯¹ã€‚

è¿”å›

`List[int]`

é›¶åˆ—è¡¨ã€‚

ä»ä¼ é€’çš„ä¸¤ä¸ªåºåˆ—åˆ›å»ºä¸€ä¸ªç”¨äºåºåˆ—å¯¹åˆ†ç±»ä»»åŠ¡çš„æ©ç ã€‚T5 ä¸ä½¿ç”¨æ ‡è®°ç±»å‹ IDï¼Œå› æ­¤è¿”å›ä¸€ä¸ªé›¶åˆ—è¡¨ã€‚

#### `save_vocabulary`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/tokenization_t5.py#L442)

```py
( save_directory: str filename_prefix: Optional = None )
```

## T5TokenizerFast

### `class transformers.T5TokenizerFast`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/tokenization_t5_fast.py#L66)

```py
( vocab_file = None tokenizer_file = None eos_token = '</s>' unk_token = '<unk>' pad_token = '<pad>' extra_ids = 100 additional_special_tokens = None **kwargs )
```

å‚æ•°

+   `vocab_file`ï¼ˆ`str`ï¼‰â€” åŒ…å«å®ä¾‹åŒ–åˆ†è¯å™¨æ‰€éœ€è¯æ±‡çš„[SentencePiece](https://github.com/google/sentencepiece)æ–‡ä»¶ï¼ˆé€šå¸¸å…·æœ‰*.spm*æ‰©å±•åï¼‰ã€‚

+   `eos_token`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"</s>"`ï¼‰â€” åºåˆ—ç»“æŸæ ‡è®°ã€‚

    åœ¨æ„å»ºä½¿ç”¨ç‰¹æ®Šæ ‡è®°çš„åºåˆ—æ—¶ï¼Œè¿™ä¸æ˜¯ç”¨äºåºåˆ—ç»“æŸçš„æ ‡è®°ã€‚ä½¿ç”¨çš„æ ‡è®°æ˜¯`sep_token`ã€‚

+   `unk_token`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"<unk>"`ï¼‰â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­æ²¡æœ‰çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸º IDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚

+   `pad_token`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"<pad>"`ï¼‰â€” ç”¨äºå¡«å……çš„æ ‡è®°ï¼Œä¾‹å¦‚åœ¨æ‰¹å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—æ—¶ä½¿ç”¨ã€‚

+   `extra_ids`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 100ï¼‰â€” æ·»åŠ ä¸€äº›é¢å¤–çš„ ID åˆ°è¯æ±‡è¡¨ä¸­ï¼Œç”¨ä½œå“¨å…µã€‚è¿™äº›æ ‡è®°å¯ä»¥ä½œä¸ºâ€œ<extra>id{%d}>â€è®¿é—®ï¼Œå…¶ä¸­â€{%d}â€æ˜¯ 0 åˆ° extra_ids-1 ä¹‹é—´çš„æ•°å­—ã€‚å¯ä»¥é€šè¿‡è°ƒç”¨ get_sentinel_tokens æ–¹æ³•æ£€ç´¢è¿™äº›æ ‡è®°ï¼Œé€šè¿‡è°ƒç”¨ get_sentinel_token_ids æ–¹æ³•è·å–æ ‡è®° ID</extra>

+   `additional_special_tokens`ï¼ˆ`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€” åˆ†è¯å™¨ä½¿ç”¨çš„é¢å¤–ç‰¹æ®Šæ ‡è®°ã€‚

æ„å»ºä¸€ä¸ªâ€œå¿«é€Ÿâ€T5 åˆ†è¯å™¨ï¼ˆç”± HuggingFace çš„*tokenizers*åº“æ”¯æŒï¼‰ã€‚åŸºäº[Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models)ã€‚

è¿™ä¸ªåˆ†è¯å™¨ç»§æ‰¿è‡ª PreTrainedTokenizerFastï¼Œå…¶ä¸­åŒ…å«å¤§éƒ¨åˆ†ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒè¿™ä¸ªè¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

#### `build_inputs_with_special_tokens`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/tokenization_t5_fast.py#L195)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” å°†æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„ ID åˆ—è¡¨ã€‚

+   `token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰çš„ç¬¬äºŒä¸ª ID åˆ—è¡¨ï¼Œç”¨äºåºåˆ—å¯¹ã€‚

è¿”å›

`List[int]`

å…·æœ‰é€‚å½“ç‰¹æ®Šæ ‡è®°çš„ input IDs åˆ—è¡¨ã€‚

é€šè¿‡è¿æ¥å’Œæ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œä»åºåˆ—æˆ–åºåˆ—å¯¹æ„å»ºç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹è¾“å…¥ã€‚åºåˆ—çš„æ ¼å¼å¦‚ä¸‹ï¼š

+   å•ä¸ªåºåˆ—ï¼š`X </s>`

+   åºåˆ—å¯¹ï¼š`A </s> B </s>`

#### `create_token_type_ids_from_sequences`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/tokenization_t5_fast.py#L221)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0`ï¼ˆ`List[int]`ï¼‰- ID åˆ—è¡¨ã€‚

+   `token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰- åºåˆ—å¯¹çš„ç¬¬äºŒä¸ª ID åˆ—è¡¨ã€‚

è¿”å›

`List[int]`

é›¶çš„åˆ—è¡¨ã€‚

ä»ä¼ é€’çš„ä¸¤ä¸ªåºåˆ—åˆ›å»ºä¸€ä¸ªç”¨äºåºåˆ—å¯¹åˆ†ç±»ä»»åŠ¡çš„æ©ç ã€‚T5 ä¸ä½¿ç”¨ token type idsï¼Œå› æ­¤è¿”å›ä¸€ä¸ªé›¶çš„åˆ—è¡¨ã€‚

PytorchHide Pytorch å†…å®¹

## T5Model

### `class transformers.T5Model`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_t5.py#L1340)

```py
( config: T5Config )
```

å‚æ•°

+   `config`ï¼ˆT5Configï¼‰- å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„ T5 æ¨¡å‹è½¬æ¢å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚

T5 æ¨¡å‹æ˜¯ç”± Colin Raffelã€Noam Shazeerã€Adam Robertsã€Katherine Leeã€Sharan Narangã€Michael Matenaã€Yanqi Zhouã€Wei Liã€Peter J. Liu åœ¨[æ¢ç´¢ç»Ÿä¸€æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨çš„è¿ç§»å­¦ä¹ æé™](https://arxiv.org/abs/1910.10683)ä¸­æå‡ºçš„ã€‚å®ƒæ˜¯ä¸€ä¸ªåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬å»å™ªç”Ÿæˆè®¾ç½®ä¸­é¢„è®­ç»ƒçš„ç¼–ç å™¨è§£ç å™¨è½¬æ¢å™¨ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_t5.py#L1433)

```py
( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚T5 æ˜¯ä¸€ä¸ªå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤æ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨å³ä¾§å’Œå·¦ä¾§éƒ½å¡«å……è¾“å…¥ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æŸ¥çœ‹ PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()è·å–è¯¦ç»†ä¿¡æ¯ã€‚

    ä»€ä¹ˆæ˜¯ input IDsï¼Ÿ

    äº†è§£å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`input_ids`ï¼Œè¯·æŸ¥çœ‹ T5 Trainingã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š

    +   å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º 1ã€‚

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º 0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥ IDï¼Ÿ

    T5 ä½¿ç”¨`pad_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œå¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ª`decoder_input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

    è¦äº†è§£æ›´å¤šå…³äºå¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`decoder_input_ids`çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ T5 Trainingã€‚

+   `decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*) â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¼ é‡ï¼Œå¿½ç•¥`decoder_input_ids`ä¸­çš„å¡«å……æ ‡è®°ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” ç¼–ç å™¨ä¸­è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©æ€§å±è”½å¤´éƒ¨çš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚

+   `decoder_head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” è§£ç å™¨ä¸­è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©æ€§å±è”½å¤´éƒ¨çš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚

+   `cross_attn_head_mask` (`torch.Tensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” è§£ç å™¨ä¸­äº¤å‰æ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©æ€§å±è”½å¤´éƒ¨çš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” å…ƒç»„åŒ…æ‹¬ï¼ˆ`last_hidden_state`ï¼Œå¯é€‰ï¼š*hidden_states*ï¼Œå¯é€‰ï¼š*attentions*ï¼‰`last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œæ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`) â€” åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ª`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™è¯¥æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†å¾ˆæœ‰ç”¨ã€‚

+   `decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œå¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ª`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†å¾ˆæœ‰ç”¨ã€‚

    å¦‚æœ`decoder_input_ids`å’Œ`decoder_inputs_embeds`éƒ½æœªè®¾ç½®ï¼Œåˆ™`decoder_inputs_embeds`å–`inputs_embeds`çš„å€¼ã€‚

+   `use_cache` (`bool`, *optional*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_outputs.Seq2SeqModelOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.Seq2SeqModelOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆT5Configï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼‰- æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

    å¦‚æœä»…ä½¿ç”¨`past_key_values`ï¼Œåˆ™è¾“å‡ºå½¢çŠ¶ä¸º`(batch_size, 1, hidden_size)`çš„åºåˆ—çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ã€‚

+   `past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰- é•¿åº¦ä¸º`config.n_layers`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚

+   `decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„æ¯å±‚çš„`torch.FloatTensor`å…ƒç»„ã€‚

    è§£ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„æ¯å±‚çš„`torch.FloatTensor`å…ƒç»„ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„æ¯å±‚çš„`torch.FloatTensor`å…ƒç»„ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„æ¯å±‚çš„`torch.FloatTensor`å…ƒç»„ã€‚

    æ¯å±‚ç¼–ç å™¨çš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åä½¿ç”¨ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

T5Model çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, T5Model

>>> tokenizer = AutoTokenizer.from_pretrained("t5-small")
>>> model = T5Model.from_pretrained("t5-small")

>>> input_ids = tokenizer(
...     "Studies have been shown that owning a dog is good for you", return_tensors="pt"
... ).input_ids  # Batch size 1
>>> decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1

>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.
>>> # This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.
>>> decoder_input_ids = model._shift_right(decoder_input_ids)

>>> # forward pass
>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)
>>> last_hidden_states = outputs.last_hidden_state
```

## T5ForConditionalGeneration

### `class transformers.T5ForConditionalGeneration`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_t5.py#L1548)

```py
( config: T5Config )
```

å‚æ•°

+   `config`ï¼ˆT5Configï¼‰â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

å¸¦æœ‰`è¯­è¨€å»ºæ¨¡`å¤´éƒ¨çš„ T5 æ¨¡å‹ã€‚

T5 æ¨¡å‹æ˜¯ç”± Colin Raffelï¼ŒNoam Shazeerï¼ŒAdam Robertsï¼ŒKatherine Leeï¼ŒSharan Narangï¼ŒMichael Matenaï¼ŒYanqi Zhouï¼ŒWei Liï¼ŒPeter J. Liu åœ¨[æ¢ç´¢ç»Ÿä¸€æ–‡æœ¬åˆ°æ–‡æœ¬å˜æ¢å™¨çš„è¿ç§»å­¦ä¹ æé™](https://arxiv.org/abs/1910.10683)ä¸­æå‡ºçš„ã€‚å®ƒæ˜¯ä¸€ä¸ªåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬å»å™ªç”Ÿæˆè®¾ç½®ä¸­é¢„è®­ç»ƒçš„ç¼–ç å™¨è§£ç å™¨å˜æ¢å™¨ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼Œè°ƒæ•´è¾“å…¥åµŒå…¥å¤§å°ï¼Œä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_t5.py#L1642)

```py
( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚T5 æ˜¯ä¸€ä¸ªå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤æ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨å³ä¾§å’Œå·¦ä¾§éƒ½å¡«å……è¾“å…¥ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æŸ¥çœ‹ PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ä»¥è·å–è¯¦ç»†ä¿¡æ¯ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

    äº†è§£å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`input_ids`ï¼Œè¯·æŸ¥çœ‹ T5 è®­ç»ƒã€‚

+   `attention_mask`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º 1ï¼Œ

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º 0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `decoder_input_ids`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`ï¼Œ*å¯é€‰*ï¼‰â€” è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥ IDï¼Ÿ

    T5 ä½¿ç”¨`pad_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™å¯èƒ½åªéœ€è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

    æœ‰å…³å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`decoder_input_ids`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ T5 Trainingã€‚

+   `decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.BoolTensor`ï¼Œ*å¯é€‰*ï¼‰- é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¼ é‡ï¼Œå¿½ç•¥`decoder_input_ids`ä¸­çš„å¡«å……æ ‡è®°ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºå°†ç¼–ç å™¨ä¸­è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `decoder_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºå°†è§£ç å™¨ä¸­è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `cross_attn_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºå°†è§£ç å™¨ä¸­äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `encoder_outputs`ï¼ˆå…ƒç»„ï¼ˆå…ƒç»„ï¼ˆ`torch.FloatTensor`ï¼‰ï¼Œ*å¯é€‰*ï¼‰- å…ƒç»„ç”±ï¼ˆ`last_hidden_state`ï¼Œ*å¯é€‰*ï¼š*hidden_states*ï¼Œ*å¯é€‰*ï¼š*attentions*ï¼‰ç»„æˆï¼Œ`last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œæ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `past_key_values`ï¼ˆé•¿åº¦ä¸º`config.n_layers`çš„å…ƒç»„ï¼ˆå…ƒç»„ï¼ˆ`torch.FloatTensor`ï¼‰ï¼‰ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å« 4 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`çš„å¼ é‡ï¼‰- åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¿™äº›æœªå°†å…¶è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ IDï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`çš„å¼ é‡ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`decoder_input_ids`ã€‚

+   `inputs_embeds`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰- å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨å¸Œæœ›æ›´å¤šåœ°æ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `decoder_inputs_embeds`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, target_sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰- å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™å¯èƒ½åªéœ€è¾“å…¥æœ€åçš„`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨å¸Œæœ›æ›´å¤šåœ°æ§åˆ¶å¦‚ä½•å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

    å¦‚æœ`decoder_input_ids`å’Œ`decoder_inputs_embeds`éƒ½æœªè®¾ç½®ï¼Œåˆ™`decoder_inputs_embeds`å–`inputs_embeds`çš„å€¼ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor` of shape `(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[-100, 0, ..., config.vocab_size - 1]`ä¸­ã€‚æ‰€æœ‰æ ‡ç­¾è®¾ç½®ä¸º`-100`éƒ½å°†è¢«å¿½ç•¥ï¼ˆæ©ç›–ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—æ ‡ç­¾åœ¨`[0, ..., config.vocab_size]`ä¸­çš„æƒ…å†µã€‚

è¿”å›

transformers.modeling_outputs.Seq2SeqLMOutput æˆ–`tuple(torch.FloatTensor)`

transformers.modeling_outputs.Seq2SeqLMOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…æ‹¬å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆT5Configï¼‰å’Œè¾“å…¥ã€‚

+   `loss` (`torch.FloatTensor` of shape `(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚

+   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰ â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆè§`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¯å±‚è§£ç å™¨çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› SoftMax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› SoftMax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰ä¸€ä¸ªåµŒå…¥å±‚ï¼Œ+ ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    ç¼–ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

T5ForConditionalGeneration çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, T5ForConditionalGeneration

>>> tokenizer = AutoTokenizer.from_pretrained("t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("t5-small")

>>> # training
>>> input_ids = tokenizer("The <extra_id_0> walks in <extra_id_1> park", return_tensors="pt").input_ids
>>> labels = tokenizer("<extra_id_0> cute dog <extra_id_1> the <extra_id_2>", return_tensors="pt").input_ids
>>> outputs = model(input_ids=input_ids, labels=labels)
>>> loss = outputs.loss
>>> logits = outputs.logits

>>> # inference
>>> input_ids = tokenizer(
...     "summarize: studies have shown that owning a dog is good for you", return_tensors="pt"
... ).input_ids  # Batch size 1
>>> outputs = model.generate(input_ids)
>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))
>>> # studies have shown that owning a dog is good for you.
```

## T5EncoderModel

### `class transformers.T5EncoderModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_t5.py#L1869)

```py
( config: T5Config )
```

å‚æ•°

+   `config`ï¼ˆT5Configï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„ T5 æ¨¡å‹å˜æ¢å™¨è¾“å‡ºç¼–ç å™¨çš„åŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨åœ¨é¡¶éƒ¨ã€‚

T5 æ¨¡å‹ç”± Colin Raffelã€Noam Shazeerã€Adam Robertsã€Katherine Leeã€Sharan Narangã€Michael Matenaã€Yanqi Zhouã€Wei Liã€Peter J. Liu åœ¨[æ¢ç´¢ç»Ÿä¸€æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢çš„è¿ç§»å­¦ä¹ æé™](https://arxiv.org/abs/1910.10683)ä¸­æå‡ºã€‚å®ƒæ˜¯ä¸€ä¸ªåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬å»å™ªç”Ÿæˆè®¾ç½®ä¸­é¢„è®­ç»ƒçš„ç¼–ç å™¨è§£ç å™¨å˜æ¢å™¨ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥äº†è§£æ‰€æœ‰ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„äº‹é¡¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_t5.py#L1945)

```py
( input_ids: Optional = None attention_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚T5 æ˜¯ä¸€ä¸ªå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤æ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨å³ä¾§å’Œå·¦ä¾§éƒ½å¡«å……è¾“å…¥ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è§ PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    è¦äº†è§£æœ‰å…³å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`input_ids`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ T5 Trainingã€‚

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º 1ï¼Œ

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º 0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   å¯¹äºæœªè¢«`masked`çš„å¤´éƒ¨ä¸º 1ï¼Œ

    +   å¯¹äºè¢«`masked`çš„å¤´éƒ¨ä¸º 0ã€‚

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_outputs.BaseModelOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.BaseModelOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆT5Configï¼‰å’Œè¾“å…¥ã€‚

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    åœ¨è‡ªæ³¨æ„åŠ›å¤´ä¸­ä½¿ç”¨æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼ã€‚

T5EncoderModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤ä¹‹åè°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, T5EncoderModel

>>> tokenizer = AutoTokenizer.from_pretrained("t5-small")
>>> model = T5EncoderModel.from_pretrained("t5-small")
>>> input_ids = tokenizer(
...     "Studies have been shown that owning a dog is good for you", return_tensors="pt"
... ).input_ids  # Batch size 1
>>> outputs = model(input_ids=input_ids)
>>> last_hidden_states = outputs.last_hidden_state
```

## T5ForSequenceClassification

### `class transformers.T5ForSequenceClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_t5.py#L1988)

```py
( config: T5Config )
```

å‚æ•°

+   `config`ï¼ˆT5Configï¼‰- å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

T5 æ¨¡å‹åœ¨é¡¶éƒ¨å¸¦æœ‰åºåˆ—åˆ†ç±»/å¤´ï¼ˆæ±‡èšè¾“å‡ºçš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äº GLUE ä»»åŠ¡ã€‚

T5 æ¨¡å‹ç”± Colin Raffelã€Noam Shazeerã€Adam Robertsã€Katherine Leeã€Sharan Narangã€Michael Matenaã€Yanqi Zhouã€Wei Liã€Peter J. Liu åœ¨[æ¢ç´¢ç»Ÿä¸€æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢çš„è¿ç§»å­¦ä¹ æé™](https://arxiv.org/abs/1910.10683)ä¸­æå‡ºã€‚å®ƒæ˜¯ä¸€ä¸ªåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬å»å™ªç”Ÿæˆè®¾ç½®ä¸­é¢„è®­ç»ƒçš„ç¼–ç å™¨è§£ç å™¨å˜å‹å™¨ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥äº†è§£ä¸ä¸€èˆ¬ä½¿ç”¨å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_t5.py#L2009)

```py
( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚T5 æ˜¯ä¸€ä¸ªå¸¦æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤æ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨å³ä¾§å’Œå·¦ä¾§éƒ½å¡«å……è¾“å…¥ã€‚

    ç´¢å¼•å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

    è¦äº†è§£æœ‰å…³å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`input_ids`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ T5 è®­ç»ƒã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   1 ç”¨äºæœªè¢«â€œæ©ç â€æ‰çš„æ ‡è®°ï¼Œ

    +   0 ç”¨äºè¢«â€œæ©ç â€æ‰çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    ç´¢å¼•å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥ IDï¼Ÿ

    T5 ä½¿ç”¨`pad_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚

    è¦äº†è§£æœ‰å…³å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`decoder_input_ids`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ T5 è®­ç»ƒã€‚

+   `decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.BoolTensor`ï¼Œ*å¯é€‰*ï¼‰- é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥`decoder_input_ids`ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºåœ¨ç¼–ç å™¨ä¸­ä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `decoder_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºåœ¨è§£ç å™¨ä¸­ä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `cross_attn_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºåœ¨è§£ç å™¨ä¸­ä½¿äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `encoder_outputs`ï¼ˆ`tuple(tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼‰- å…ƒç»„åŒ…æ‹¬ï¼ˆ`last_hidden_state`ï¼Œ*å¯é€‰*ï¼š*hidden_states*ï¼Œ*å¯é€‰*ï¼š*attentions*ï¼‰`last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œæ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `past_key_values`ï¼ˆé•¿åº¦ä¸º`config.n_layers`çš„`tuple(tuple(torch.FloatTensor))`ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å« 4 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`çš„å¼ é‡ï¼‰- åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¿™äº›æœªå°†å…¶è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„è¾“å…¥ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨å¸Œæœ›æ›´å¤šåœ°æ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `decoder_inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œå¯é€‰åœ°åªéœ€è¾“å…¥æœ€åçš„`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨å¸Œæœ›æ›´å¤šåœ°æ§åˆ¶å¦‚ä½•å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

    å¦‚æœ`decoder_input_ids`å’Œ`decoder_inputs_embeds`éƒ½æœªè®¾ç½®ï¼Œåˆ™`decoder_inputs_embeds`å–`inputs_embeds`çš„å€¼ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`ä¸­ã€‚å¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆT5Configï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`label`æ—¶è¿”å›) â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`ï¼‰ â€” SoftMax ä¹‹å‰çš„åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›) â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥å±‚çš„è¾“å‡ºåŠ ä¸Šæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    è§£ç å™¨åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºéšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥å±‚çš„è¾“å‡ºåŠ ä¸Šæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    ç¼–ç å™¨åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºéšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

T5ForSequenceClassification çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

## T5ForQuestionAnswering

### `class transformers.T5ForQuestionAnswering`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_t5.py#L2121)

```py
( config: T5Config )
```

å‚æ•°

+   `config` (T5Config) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained() æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

T5 æ¨¡å‹åœ¨é¡¶éƒ¨å…·æœ‰ç”¨äºæå–å¼é—®ç­”ä»»åŠ¡çš„è·¨åº¦åˆ†ç±»å¤´ï¼Œå¦‚ SQuADï¼ˆåœ¨éšè—çŠ¶æ€è¾“å‡ºçš„é¡¶éƒ¨è¿›è¡Œçº¿æ€§å±‚è®¡ç®— `span start logits` å’Œ `span end logits`ï¼‰ã€‚

T5 æ¨¡å‹åœ¨ [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) ä¸€æ–‡ä¸­ç”± Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu æå‡ºã€‚å®ƒæ˜¯ä¸€ä¸ªåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬å»å™ªç”Ÿæˆè®¾ç½®ä¸­é¢„è®­ç»ƒçš„ç¼–ç å™¨è§£ç å™¨å˜æ¢å™¨ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹ä¹Ÿæ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥äº†è§£æ‰€æœ‰ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„äº‹é¡¹ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_t5.py#L2177)

```py
( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None start_positions: Optional = None end_positions: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚T5 æ˜¯ä¸€ä¸ªå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤æ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨å³ä¾§å’Œå·¦ä¾§éƒ½å¡«å……è¾“å…¥ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode() å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

    è¦äº†è§£å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡ `input_ids`ï¼Œè¯·æŸ¥çœ‹ T5 Trainingã€‚

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨ `[0, 1]` ä¹‹é—´ï¼š

    +   å¯¹äºæœªè¢«æ©ç çš„æ ‡è®°ä¸º `1`ï¼Œ

    +   å¯¹äºè¢«æ©ç çš„æ ‡è®°ä¸º `0`ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*) â€” è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode() å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥ IDï¼Ÿ

    T5 ä½¿ç”¨`pad_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œå¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ª`decoder_input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

    è¦äº†è§£æœ‰å…³å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`decoder_input_ids`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ T5 Trainingã€‚

+   `decoder_attention_mask` (`torch.BoolTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`ï¼Œ*optional*) â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¼ é‡ï¼Œå¿½ç•¥`decoder_input_ids`ä¸­çš„å¡«å……æ ‡è®°ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

+   `head_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*optional*) â€” ç”¨äºä½¿ç¼–ç å™¨ä¸­çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ã€‚

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `decoder_head_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*optional*) â€” ç”¨äºä½¿è§£ç å™¨ä¸­çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ã€‚

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `cross_attn_head_mask` (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*optional*) â€” ç”¨äºä½¿è§£ç å™¨ä¸­çš„äº¤å‰æ³¨æ„åŠ›æ¨¡å—ä¸­çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ã€‚

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`ï¼Œ*optional*) â€” å…ƒç»„åŒ…æ‹¬ï¼ˆ`last_hidden_state`ï¼Œå¯é€‰ï¼š*hidden_states*ï¼Œå¯é€‰ï¼š*attentions*ï¼‰`last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œæ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œé•¿åº¦ä¸º`config.n_layers`ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`çš„ 4 ä¸ªå¼ é‡ï¼‰ â€” åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ª`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `decoder_inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, target_sequence_length, hidden_size)`ï¼Œ*optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œå¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ª`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

    å¦‚æœ`decoder_input_ids`å’Œ`decoder_inputs_embeds`éƒ½æœªè®¾ç½®ï¼Œåˆ™`decoder_inputs_embeds`å–`inputs_embeds`çš„å€¼ã€‚

+   `use_cache` (`bool`, *optional*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¯ä»¥ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

+   `start_positions`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè®¡ç®—æ ‡è®°è·¨åº¦çš„èµ·å§‹ä½ç½®ï¼ˆç´¢å¼•ï¼‰çš„æ ‡ç­¾ã€‚ä½ç½®è¢«å¤¹åœ¨åºåˆ—çš„é•¿åº¦ï¼ˆ*sequence_length*ï¼‰å†…ã€‚åºåˆ—å¤–çš„ä½ç½®ä¸ä¼šè¢«è€ƒè™‘åœ¨å†…ä»¥è®¡ç®—æŸå¤±ã€‚

+   `end_positions`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè®¡ç®—æ ‡è®°è·¨åº¦çš„ç»“æŸä½ç½®ï¼ˆç´¢å¼•ï¼‰çš„æ ‡ç­¾ã€‚ä½ç½®è¢«å¤¹åœ¨åºåˆ—çš„é•¿åº¦ï¼ˆ*sequence_length*ï¼‰å†…ã€‚åºåˆ—å¤–çš„ä½ç½®ä¸ä¼šè¢«è€ƒè™‘åœ¨å†…ä»¥è®¡ç®—æŸå¤±ã€‚

è¿”å›

transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆT5Configï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰- æ€»è·¨åº¦æå–æŸå¤±æ˜¯èµ·å§‹å’Œç»“æŸä½ç½®çš„äº¤å‰ç†µä¹‹å’Œã€‚

+   `start_logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰- è·¨åº¦èµ·å§‹å¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `end_logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰- è·¨åº¦ç»“æŸå¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰- é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    è§£ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚çš„è¾“å‡ºï¼Œ+ æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¯å±‚ç¼–ç å™¨çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

T5ForQuestionAnswering çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

TensorFlow éšè— TensorFlow å†…å®¹

## TFT5Model

### `class transformers.TFT5Model`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_tf_t5.py#L1177)

```py
( config *inputs **kwargs )
```

å‚æ•°

+   `config`ï¼ˆT5Configï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸ T5 æ¨¡å‹è½¬æ¢å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚

T5 æ¨¡å‹æ˜¯ç”± Colin Raffelï¼ŒNoam Shazeerï¼ŒAdam Robertsï¼ŒKatherine Leeï¼ŒSharan Narangï¼ŒMichael Matenaï¼ŒYanqi Zhouï¼ŒWei Liï¼ŒPeter J. Liu åœ¨[æ¢ç´¢ç»Ÿä¸€æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨çš„è¿ç§»å­¦ä¹ æé™](https://arxiv.org/abs/1910.10683)ä¸­æå‡ºçš„ã€‚å®ƒæ˜¯ä¸€ä¸ªåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬å»å™ªç”Ÿæˆè®¾ç½®ä¸­é¢„è®­ç»ƒçš„ç¼–ç å™¨è§£ç å™¨è½¬æ¢å™¨ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª TFPreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ï¼Œäº†è§£ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

`transformers`ä¸­çš„ TensorFlow æ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äº PyTorch æ¨¡å‹ï¼‰ï¼Œæˆ–è€…

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸çš„ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ Keras æ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºè¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯ä»¥â€œæ­£å¸¸å·¥ä½œâ€ - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’è¾“å…¥å’Œæ ‡ç­¾å³å¯ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Keras æ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œä¾‹å¦‚åœ¨ä½¿ç”¨ Keras`Functional` API åˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­çš„æ‰€æœ‰è¾“å…¥å¼ é‡ï¼š

+   åªæœ‰`input_ids`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`

+   ä¸€ä¸ªé•¿åº¦å¯å˜çš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«æŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºçš„ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼š`model([input_ids, attention_mask])`æˆ–`model([input_ids, attention_mask, token_type_ids])`

+   ä¸€ä¸ªåŒ…å«ä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡çš„å­—å…¸ï¼š`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒä»»ä½•è¿™äº›å†…å®¹ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…å…¶ä»– Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_tf_t5.py#L1209)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None decoder_head_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: np.ndarray | tf.Tensor | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None inputs_embeds: np.ndarray | tf.Tensor | None = None decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqModelOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚T5 æ˜¯ä¸€ä¸ªå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤æ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨å³ä¾§æˆ–å·¦ä¾§å¡«å……è¾“å…¥ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.`call`()å’Œ PreTrainedTokenizer.encode()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

    äº†è§£å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`inputs`ï¼Œè¯·æŸ¥çœ‹ T5 Trainingã€‚

+   `decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºåºåˆ—åˆ°åºåˆ—è®­ç»ƒã€‚T5 ä½¿ç”¨`pad_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯èƒ½åªéœ€è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚

    äº†è§£å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`decoder_input_ids`ï¼Œè¯·æŸ¥çœ‹ T5 Trainingã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤ºæœªè¢«å±è”½çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢«å±è”½çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥`decoder_input_ids`ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºåœ¨ç¼–ç å™¨ä¸­ä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚

+   `decoder_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºåœ¨è§£ç å™¨ä¸­ä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚

+   `encoder_outputs`ï¼ˆ`tuple(tuple(tf.FloatTensor)`ï¼Œ*å¯é€‰*ï¼‰- å…ƒç»„åŒ…æ‹¬ï¼ˆ`last_hidden_state`ï¼Œ*å¯é€‰*ï¼š*hidden_states*ï¼Œ*å¯é€‰*ï¼š*attentions*ï¼‰`last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œæ˜¯ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `past_key_values`ï¼ˆé•¿åº¦ä¸º`config.n_layers`çš„`tuple(tuple(tf.Tensor))`ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å« 4 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`çš„å¼ é‡ï¼‰- åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥å½¢çŠ¶ä¸º`(batch_size, 1)`çš„æœ€åä¸€ä¸ª`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™è¯¥æ¨¡å‹çš„ï¼‰è€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰ â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `decoder_inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length, hidden_size)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰ â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™åªéœ€è¾“å…¥æœ€åä¸€ä¸ª`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

    å¦‚æœ`decoder_input_ids`å’Œ`decoder_inputs_embeds`éƒ½æœªè®¾ç½®ï¼Œåˆ™`decoder_inputs_embeds`å–`inputs_embeds`çš„å€¼ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰ â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆè¯·å‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚æ­¤å‚æ•°ä»…å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸º Trueã€‚

+   `training`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰ â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—å¦‚ dropout æ¨¡å—åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚

è¿”å›

transformers.modeling_tf_outputs.TFSeq2SeqModelOutput æˆ–`tuple(tf.Tensor)`

ä¸€ä¸ª transformers.modeling_tf_outputs.TFSeq2SeqModelOutput æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆT5Configï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`ï¼‰ â€” æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

    å¦‚æœä»…ä½¿ç”¨`past_key_values`ï¼Œåˆ™è¾“å‡ºå½¢çŠ¶ä¸º`(batch_size, 1, hidden_size)`çš„åºåˆ—çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ã€‚

+   `past_key_values`ï¼ˆ`List[tf.Tensor]`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰ â€” é•¿åº¦ä¸º`config.n_layers`çš„`tf.Tensor`åˆ—è¡¨ï¼Œæ¯ä¸ªå¼ é‡çš„å½¢çŠ¶ä¸º`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`ã€‚

    åŒ…å«è§£ç å™¨çš„é¢„è®¡ç®—éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆè¯·å‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `decoder_hidden_states` (`tuple(tf.Tensor)`, *å¯é€‰*, å½“ä¼ é€’ `output_hidden_states=True` æˆ– `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `tf.Tensor` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    è§£ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions` (`tuple(tf.Tensor)`, *å¯é€‰*, å½“ä¼ é€’ `output_attentions=True` æˆ– `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `tf.Tensor` å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åä½¿ç”¨ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(tf.Tensor)`, *å¯é€‰*, å½“ä¼ é€’ `output_attentions=True` æˆ– `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `tf.Tensor` å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åä½¿ç”¨ï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states` (`tuple(tf.Tensor)`, *å¯é€‰*, å½“ä¼ é€’ `output_hidden_states=True` æˆ– `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `tf.Tensor` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    ç¼–ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions` (`tuple(tf.Tensor)`, *å¯é€‰*, å½“ä¼ é€’ `output_attentions=True` æˆ– `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `tf.Tensor` å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åä½¿ç”¨ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

TFT5Model çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, TFT5Model

>>> tokenizer = AutoTokenizer.from_pretrained("t5-small")
>>> model = TFT5Model.from_pretrained("t5-small")

>>> input_ids = tokenizer(
...     "Studies have been shown that owning a dog is good for you", return_tensors="tf"
... ).input_ids  # Batch size 1
>>> decoder_input_ids = tokenizer("Studies show that", return_tensors="tf").input_ids  # Batch size 1

>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.
>>> # This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.
>>> decoder_input_ids = model._shift_right(decoder_input_ids)

>>> # forward pass
>>> outputs = model(input_ids, decoder_input_ids=decoder_input_ids)
>>> last_hidden_states = outputs.last_hidden_state
```

## TFT5ForConditionalGeneration

`transformers.TFT5ForConditionalGeneration` ç±»

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_tf_t5.py#L1329)

```py
( config *inputs **kwargs )
```

å‚æ•°

+   `config` (T5Config) â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained() æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

å¸¦æœ‰é¡¶éƒ¨ `è¯­è¨€å»ºæ¨¡` å¤´çš„ T5 æ¨¡å‹ã€‚

T5 æ¨¡å‹ç”± Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu åœ¨ [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) ä¸­æå‡ºã€‚å®ƒæ˜¯ä¸€ä¸ªåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬å»å™ªç”Ÿæˆè®¾ç½®ä¸­é¢„è®­ç»ƒçš„ç¼–ç å™¨è§£ç å™¨ transformerã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª TFPreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

`transformers`ä¸­çš„ TensorFlow æ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆå¦‚ PyTorch æ¨¡å‹ï¼‰ï¼Œ

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯ï¼ŒKeras æ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºæœ‰äº†è¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯ä»¥â€œæ­£å¸¸å·¥ä½œâ€ - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’è¾“å…¥å’Œæ ‡ç­¾å³å¯ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Keras æ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œæ¯”å¦‚åœ¨ä½¿ç”¨ Keras çš„`Functional` API åˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†æ‰€æœ‰è¾“å…¥å¼ é‡æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ï¼š

+   ä¸€ä¸ªä»…åŒ…å«`input_ids`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`

+   ä¸€ä¸ªé•¿åº¦ä¸å®šçš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šé¡ºåºçš„è¾“å…¥å¼ é‡ï¼š`model([input_ids, attention_mask])`æˆ–`model([input_ids, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šè¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œåœ¨ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒä»»ä½•é—®é¢˜ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå¯¹å¾…ä»»ä½•å…¶ä»– Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_tf_t5.py#L1386)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None decoder_head_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: np.ndarray | tf.Tensor | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None inputs_embeds: np.ndarray | tf.Tensor | None = None decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None labels: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqLMOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`ï¼‰ - è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚T5 æ˜¯ä¸€ä¸ªå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤æ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨å³ä¾§æˆ–å·¦ä¾§å¡«å……è¾“å…¥ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.`call`()å’Œ PreTrainedTokenizer.encode()ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

    è¦äº†è§£æœ‰å…³å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`inputs`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ T5 Trainingã€‚

+   `decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰ - ç”¨äºåºåˆ—åˆ°åºåˆ—è®­ç»ƒã€‚T5 ä½¿ç”¨`pad_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™åªéœ€é€‰æ‹©æœ€åçš„`decoder_input_ids`è¾“å…¥ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

    è¦äº†è§£æœ‰å…³å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`decoder_input_ids`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ T5 Trainingã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰ - ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   å¯¹äºæœªè¢«â€œæ©ç â€çš„æ ‡è®°ï¼Œå°†å…¶è®¾ä¸º 1ï¼Œ

    +   å¯¹äºè¢«â€œæ©ç â€çš„æ ‡è®°ï¼Œå°†å…¶è®¾ä¸º 0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰ - é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥`decoder_input_ids`ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰ - ç”¨äºåœ¨ç¼–ç å™¨çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­ä½¿é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«â€œæ©ç â€ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«â€œæ©ç â€ã€‚

+   `decoder_head_mask` (`tf.Tensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” ç”¨äºå°†è§£ç å™¨ä¸­è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨ `[0, 1]` èŒƒå›´å†…ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚

+   `encoder_outputs` (`tuple(tuple(tf.FloatTensor)`, *optional*) â€” å…ƒç»„åŒ…å« (`last_hidden_state`, å¯é€‰: *hidden_states*, å¯é€‰: *attentions*) `last_hidden_state` çš„å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`ï¼Œæ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `past_key_values` (`tuple(tuple(tf.Tensor))`ï¼Œé•¿åº¦ä¸º `config.n_layers`ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 4 ä¸ªå½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)` çš„å¼ é‡ï¼‰ â€” åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚

    å¦‚æœä½¿ç”¨äº† `past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ªå½¢çŠ¶ä¸º `(batch_size, 1)` çš„ `decoder_input_ids`ï¼ˆè¿™äº›è¾“å…¥æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹ï¼‰è€Œä¸æ˜¯å½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„æ‰€æœ‰ `decoder_input_ids`ã€‚

+   `inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’ `input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒï¼Œä»¥ä¾¿å°† `input_ids` ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `decoder_inputs_embeds` (`tf.Tensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’ `decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨äº† `past_key_values`ï¼Œåˆ™åªéœ€è¾“å…¥æœ€åä¸€ä¸ª `decoder_inputs_embeds`ï¼ˆå‚è§ `past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒï¼Œä»¥ä¾¿å°† `decoder_input_ids` ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

    å¦‚æœ `decoder_input_ids` å’Œ `decoder_inputs_embeds` éƒ½æœªè®¾ç½®ï¼Œåˆ™ `decoder_inputs_embeds` å– `inputs_embeds` çš„å€¼ã€‚

+   `use_cache` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” å¦‚æœè®¾ç½®ä¸º `True`ï¼Œå°†è¿”å› `past_key_values` é”®å€¼çŠ¶æ€ï¼Œå¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§ `past_key_values`ï¼‰ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„ `attentions`ã€‚æ­¤å‚æ•°ä»…å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹ï¼Œå°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚æ­¤å‚æ•°ä»…å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹ï¼Œå°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹ï¼Œè¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸º Trueã€‚

+   `training` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆæŸäº›æ¨¡å—ï¼Œå¦‚ä¸¢å¼ƒæ¨¡å—ï¼Œåœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´å…·æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚

+   `labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) â€” ç”¨äºè®¡ç®—äº¤å‰ç†µåˆ†ç±»æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨ `[0, ..., config.vocab_size - 1]` èŒƒå›´å†…ã€‚

è¿”å›

transformers.modeling_tf_outputs.TFSeq2SeqLMOutput æˆ– `tuple(tf.Tensor)`

ä¸€ä¸ª transformers.modeling_tf_outputs.TFSeq2SeqLMOutput æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆT5Configï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `loss` (`tf.Tensor` of shape `(n,)`, *optional*, å½“æä¾›`labels`æ—¶è¿”å›ï¼Œå…¶ä¸­ n æ˜¯æœªå±è”½æ ‡ç­¾çš„æ•°é‡) â€” è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚

+   `logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `past_key_values` (`List[tf.Tensor]`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›) â€” é•¿åº¦ä¸º`config.n_layers`çš„`tf.Tensor`åˆ—è¡¨ï¼Œæ¯ä¸ªå¼ é‡çš„å½¢çŠ¶ä¸º`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`ã€‚

    åŒ…å«è§£ç å™¨çš„é¢„è®¡ç®—éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `decoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    è§£ç å™¨æ¯ä¸€å±‚çš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions` (`tuple(tf.Tensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› SoftMax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(tf.Tensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› SoftMax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    ç¼–ç å™¨æ¯ä¸€å±‚çš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions` (`tuple(tf.Tensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› SoftMax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

TFT5ForConditionalGeneration çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, TFT5ForConditionalGeneration

>>> tokenizer = AutoTokenizer.from_pretrained("t5-small")
>>> model = TFT5ForConditionalGeneration.from_pretrained("t5-small")

>>> # training
>>> inputs = tokenizer("The <extra_id_0> walks in <extra_id_1> park", return_tensors="tf").input_ids
>>> labels = tokenizer("<extra_id_0> cute dog <extra_id_1> the <extra_id_2>", return_tensors="tf").input_ids
>>> outputs = model(inputs, labels=labels)
>>> loss = outputs.loss
>>> logits = outputs.logits

>>> # inference
>>> inputs = tokenizer(
...     "summarize: studies have shown that owning a dog is good for you", return_tensors="tf"
... ).input_ids  # Batch size 1
>>> outputs = model.generate(inputs)
>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))
>>> # studies have shown that owning a dog is good for you
```

## TFT5EncoderModel

### `class transformers.TFT5EncoderModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_tf_t5.py#L1599)

```py
( config *inputs **kwargs )
```

å‚æ•°

+   `config`ï¼ˆT5Configï¼‰ - å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸ T5 æ¨¡å‹å˜æ¢å™¨è¾“å‡ºç¼–ç å™¨çš„åŸå§‹éšè—çŠ¶æ€ï¼Œè€Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚

T5 æ¨¡å‹æ˜¯ç”± Colin Raffelï¼ŒNoam Shazeerï¼ŒAdam Robertsï¼ŒKatherine Leeï¼ŒSharan Narangï¼ŒMichael Matenaï¼ŒYanqi Zhouï¼ŒWei Liï¼ŒPeter J. Liu åœ¨[æ¢ç´¢å…·æœ‰ç»Ÿä¸€æ–‡æœ¬åˆ°æ–‡æœ¬å˜æ¢å™¨çš„è¿ç§»å­¦ä¹ æé™](https://arxiv.org/abs/1910.10683)ä¸­æå‡ºçš„ã€‚å®ƒæ˜¯ä¸€ä¸ªåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬å»å™ªç”Ÿæˆè®¾ç½®ä¸­è¿›è¡Œé¢„è®­ç»ƒçš„ç¼–ç å™¨è§£ç å™¨å˜æ¢å™¨ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª TFPreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

`transformers`ä¸­çš„ TensorFlow æ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äº PyTorch æ¨¡å‹ï¼‰ï¼Œæˆ–

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

ç¬¬äºŒç§æ ¼å¼å¾—åˆ°æ”¯æŒçš„åŸå› æ˜¯ï¼Œå½“å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶ï¼ŒKeras æ–¹æ³•æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºè¿™ç§æ”¯æŒï¼Œåœ¨ä½¿ç”¨è¯¸å¦‚`model.fit()`ä¹‹ç±»çš„æ–¹æ³•æ—¶ï¼Œå¯¹æ‚¨æ¥è¯´åº”è¯¥â€œåªéœ€å·¥ä½œâ€ - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’æ‚¨çš„è¾“å…¥å’Œæ ‡ç­¾ï¼ç„¶è€Œï¼Œå¦‚æœæ‚¨æƒ³åœ¨ Keras æ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œæ¯”å¦‚åœ¨ä½¿ç”¨ Keras`Functional` API åˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†æ‰€æœ‰è¾“å…¥å¼ é‡åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ï¼š

+   ä¸€ä¸ªåªæœ‰`input_ids`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_ids)`

+   ä¸€ä¸ªé•¿åº¦ä¸åŒçš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼ŒæŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºï¼š`model([input_ids, attention_mask])`æˆ–`model([input_ids, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨ä¸éœ€è¦æ‹…å¿ƒè¿™äº›é—®é¢˜ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå°†è¾“å…¥ä¼ é€’ç»™ä»»ä½•å…¶ä»– Python å‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_tf_t5.py#L1622)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `inputs`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`ï¼‰ - è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚T5 æ˜¯ä¸€ä¸ªå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤æ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨å³ä¾§æˆ–å·¦ä¾§å¡«å……è¾“å…¥ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.`call`()å’Œ PreTrainedTokenizer.encode()ã€‚

    è¦äº†è§£æœ‰å…³å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`inputs`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ T5 Trainingã€‚

+   `attention_mask` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢«`masked`çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `inputs_embeds` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `head_mask` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `output_attentions` (`bool`ï¼Œ*optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

+   `training` (`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—ï¼Œå¦‚ dropout æ¨¡å—ï¼Œåœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´å…·æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚

è¿”å›

transformers.modeling_tf_outputs.TFBaseModelOutput æˆ–`tuple(tf.Tensor)`

ä¸€ä¸ª transformers.modeling_tf_outputs.TFBaseModelOutput æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼Œåˆ™åŒ…æ‹¬å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆT5Configï¼‰å’Œè¾“å…¥ã€‚

+   `last_hidden_state` (`tf.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `hidden_states` (`tuple(tf.FloatTensor)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(tf.Tensor)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨è‡ªæ³¨æ„åŠ›å¤´ä¸­ç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼çš„æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ã€‚

TFT5EncoderModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, TFT5EncoderModel

>>> tokenizer = AutoTokenizer.from_pretrained("t5-small")
>>> model = TFT5EncoderModel.from_pretrained("t5-small")

>>> input_ids = tokenizer(
...     "Studies have been shown that owning a dog is good for you", return_tensors="tf"
... ).input_ids  # Batch size 1
>>> outputs = model(input_ids)
```

JAXHide JAX å†…å®¹

## FlaxT5Model

### `class transformers.FlaxT5Model`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_flax_t5.py#L1367)

```py
( config: T5Config input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )
```

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_flax_t5.py#L986)

```py
( input_ids: Array attention_mask: Optional = None decoder_input_ids: Array = None decoder_attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) â€” è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚T5 æ˜¯ä¸€ä¸ªå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤æ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨å³ä¾§å’Œå·¦ä¾§å¡«å……è¾“å…¥ã€‚

    ç´¢å¼•å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å¾—ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    è¾“å…¥ ID æ˜¯ä»€ä¹ˆï¼Ÿ

    è¦äº†è§£å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`input_ids`ï¼Œè¯·æŸ¥çœ‹ T5 è®­ç»ƒã€‚

+   `attention_mask` (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0, 1]`ï¼š

    +   å¯¹äºæœªè¢«æ©ç çš„æ ‡è®°ï¼Œä¸º 1ï¼Œ

    +   å¯¹äºè¢«æ©ç çš„æ ‡è®°ä¸º 0ã€‚

    æ³¨æ„åŠ›æ©ç æ˜¯ä»€ä¹ˆï¼Ÿ

+   `decoder_input_ids` (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*) â€” è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚

    ç´¢å¼•å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å¾—ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    è§£ç å™¨è¾“å…¥ ID æ˜¯ä»€ä¹ˆï¼Ÿ

    T5 ä½¿ç”¨`pad_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œå¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ª`decoder_input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

    è¦äº†è§£å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`decoder_input_ids`ï¼Œè¯·æŸ¥çœ‹ T5 è®­ç»ƒã€‚

+   `decoder_attention_mask` (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*) â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¼ é‡ï¼Œå¿½ç•¥`decoder_input_ids`ä¸­çš„å¡«å……æ ‡è®°ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

+   `encoder_outputs` (`tuple(tuple(jnp.ndarray)`, *optional*) â€” å…ƒç»„åŒ…æ‹¬ï¼ˆ`last_hidden_state`ï¼Œå¯é€‰ï¼š*hidden_states*ï¼Œå¯é€‰ï¼š*attentions*ï¼‰`last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œæ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `past_key_values` (`tuple(tuple(jnp.ndarray))`ï¼Œé•¿åº¦ä¸º`config.n_layers`ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 4 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`çš„å¼ é‡ï¼‰ â€” åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, 1)`çš„`decoder_input_ids`ï¼ˆè¿™äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹ï¼‰è€Œä¸æ˜¯æ‰€æœ‰å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`decoder_input_ids`ã€‚

è¿”å›

transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆT5Configï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚

+   `logits` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `past_key_values` (`tuple(tuple(jnp.ndarray))`, *å¯é€‰*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›) â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(jnp.ndarray)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ä»¥ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `decoder_hidden_states` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    è§£ç å™¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    ç¼–ç å™¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

`FlaxT5PreTrainedModel`çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªå‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†å‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, FlaxT5Model

>>> tokenizer = AutoTokenizer.from_pretrained("t5-small")
>>> model = FlaxT5Model.from_pretrained("t5-small")

>>> input_ids = tokenizer(
...     "Studies have been shown that owning a dog is good for you", return_tensors="np"
... ).input_ids
>>> decoder_input_ids = tokenizer("Studies show that", return_tensors="np").input_ids

>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.
>>> # This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.
>>> decoder_input_ids = model._shift_right(decoder_input_ids)

>>> # forward pass
>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)
>>> last_hidden_states = outputs.last_hidden_state
```

#### `encode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_flax_t5.py#L1072)

```py
( input_ids: Array attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚T5 æ˜¯ä¸€ä¸ªå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤æ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨å³ä¾§å’Œå·¦ä¾§éƒ½å¡«å……è¾“å…¥ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    å…³äºå¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`input_ids`ï¼Œè¯·æŸ¥çœ‹ T5 è®­ç»ƒã€‚

+   `attention_mask` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º 1ã€‚

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º 0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_flax_outputs.FlaxBaseModelOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_flax_outputs.FlaxBaseModelOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®(`<class 'transformers.models.t5.configuration_t5.T5Config'>`)å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `last_hidden_state` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration

>>> tokenizer = AutoTokenizer.from_pretrained("t5-small")
>>> model = FlaxT5ForConditionalGeneration.from_pretrained("t5-small")

>>> text = "My friends are cool but they eat too many carbs."
>>> inputs = tokenizer(text, return_tensors="np")
>>> encoder_outputs = model.encode(**inputs)
```

#### `decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_flax_t5.py#L1130)

```py
( decoder_input_ids encoder_outputs encoder_attention_mask: Optional = None decoder_attention_mask: Optional = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions or tuple(torch.FloatTensor)
```

å‚æ•°

+   `decoder_input_ids` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥ IDï¼Ÿ

    åœ¨è®­ç»ƒæ—¶ï¼Œåº”æä¾›`decoder_input_ids`ã€‚

+   `encoder_outputs` (`tuple(tuple(jnp.ndarray)`) â€” å…ƒç»„åŒ…å« (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state` çš„å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*) æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `encoder_attention_mask` (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨ `[0, 1]` ä¹‹é—´ï¼š

    +   å¯¹äºæœªè¢« `masked` çš„æ ‡è®°ï¼Œå€¼ä¸º 1ã€‚

    +   å¯¹äºè¢« `masked` çš„æ ‡è®°ï¼Œå€¼ä¸º 0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `decoder_attention_mask` (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*) â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¼ é‡ï¼Œå¿½ç•¥ `decoder_input_ids` ä¸­çš„å¡«å……æ ‡è®°ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

    å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§ [è®ºæ–‡](https://arxiv.org/abs/1910.13461) ä¸­çš„å›¾è¡¨ 1ã€‚

+   `past_key_values` (`Dict[str, np.ndarray]`, *optional*, ç”± `init_cache` è¿”å›æˆ–ä¼ é€’å…ˆå‰çš„ `past_key_values` æ—¶è¿”å›) â€” å¯ç”¨äºå¿«é€Ÿè‡ªå›å½’è§£ç çš„é¢„è®¡ç®—éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰çš„å­—å…¸ã€‚é¢„è®¡ç®—çš„é”®å’Œå€¼éšè—çŠ¶æ€çš„å½¢çŠ¶ä¸º *[batch_size, max_length]*ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„ `attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„ `hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions æˆ– `tuple(torch.FloatTensor)`

transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’ `return_dict=False` æˆ– `config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½® (`<class 'transformers.models.t5.configuration_t5.T5Config'>`) å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚

+   `last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚

    å¦‚æœä»…ä½¿ç”¨ `past_key_values`ï¼Œåˆ™è¾“å‡ºå½¢çŠ¶ä¸º `(batch_size, 1, hidden_size)` çš„åºåˆ—çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ã€‚

+   `past_key_values` (`tuple(tuple(jnp.ndarray))`, *optional*, å½“ä¼ é€’ `use_cache=True` æˆ– `config.use_cache=True` æ—¶è¿”å›) â€” é•¿åº¦ä¸º `config.n_layers` çš„ `tuple(jnp.ndarray)` å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, embed_size_per_head)` çš„å¼ é‡ï¼Œå¦‚æœ `config.is_encoder_decoder=True`ï¼Œè¿˜æœ‰ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)` çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ä»¥åŠåœ¨äº¤å‰æ³¨æ„åŠ›å—ä¸­ï¼Œå¦‚æœ `config.is_encoder_decoder=True`ï¼Œè¿˜åŒ…æ‹¬ï¼‰å¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç çš„ï¼ˆè¯·å‚è§ `past_key_values` è¾“å…¥ï¼‰ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`, *optional*, å½“ä¼ é€’ `output_hidden_states=True` æˆ– `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `jnp.ndarray` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åä½¿ç”¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`å’Œ`config.add_cross_attention=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åä½¿ç”¨ï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration
>>> import jax.numpy as jnp

>>> tokenizer = AutoTokenizer.from_pretrained("t5-small")
>>> model = FlaxT5ForConditionalGeneration.from_pretrained("t5-small")

>>> text = "My friends are cool but they eat too many carbs."
>>> inputs = tokenizer(text, return_tensors="np")
>>> encoder_outputs = model.encode(**inputs)

>>> decoder_start_token_id = model.config.decoder_start_token_id
>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

>>> outputs = model.decode(decoder_input_ids, encoder_outputs)
>>> logits = outputs.logits
```

## FlaxT5ForConditionalGeneration

### `class transformers.FlaxT5ForConditionalGeneration`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_flax_t5.py#L1605)

```py
( config: T5Config input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )
```

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_flax_t5.py#L986)

```py
( input_ids: Array attention_mask: Optional = None decoder_input_ids: Array = None decoder_attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`) â€” è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚T5 æ˜¯ä¸€ä¸ªå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤æ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨å³ä¾§å’Œå·¦ä¾§éƒ½å¯¹è¾“å…¥è¿›è¡Œå¡«å……ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æŸ¥çœ‹ PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()è·å–è¯¦ç»†ä¿¡æ¯ã€‚

    ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

    è¦äº†è§£æœ‰å…³å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`input_ids`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ T5 è®­ç»ƒã€‚

+   `attention_mask` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºæœªè¢«`masked`çš„æ ‡è®°ã€‚

    +   0 è¡¨ç¤ºè¢«`masked`çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `decoder_input_ids` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`ï¼Œ*å¯é€‰*) â€” è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æŸ¥çœ‹ PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()è·å–è¯¦ç»†ä¿¡æ¯ã€‚

    ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥ IDï¼Ÿ

    T5 ä½¿ç”¨`pad_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

    è¦äº†è§£æœ‰å…³å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`decoder_input_ids`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ T5 è®­ç»ƒã€‚

+   `decoder_attention_mask` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`ï¼Œ*å¯é€‰*) â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¼ é‡ï¼Œå¿½ç•¥`decoder_input_ids`ä¸­çš„å¡«å……æ ‡è®°ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

+   `encoder_outputs` (`tuple(tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*) â€” å…ƒç»„åŒ…æ‹¬(`last_hidden_state`, *å¯é€‰*: *hidden_states*, *å¯é€‰*: *attentions*) `last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œæ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `past_key_values` (`tuple(tuple(jnp.ndarray))`ï¼Œé•¿åº¦ä¸º`config.n_layers`ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 4 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`çš„å¼ é‡ï¼‰ â€” åŒ…å«æ³¨æ„åŠ›å—çš„é¢„å…ˆè®¡ç®—çš„é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©åªè¾“å…¥å½¢çŠ¶ä¸º`(batch_size, 1)`çš„æœ€åä¸€ä¸ª`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰è€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

è¿”å›

transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆT5Configï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `logits` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `past_key_values` (`tuple(tuple(jnp.ndarray))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›) â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(jnp.ndarray)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚

+   `decoder_hidden_states` (`tuple(jnp.ndarray)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    è§£ç å™¨åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions` (`tuple(jnp.ndarray)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(jnp.ndarray)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states` (`tuple(jnp.ndarray)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    ç¼–ç å™¨åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

`FlaxT5PreTrainedModel`çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration

>>> tokenizer = AutoTokenizer.from_pretrained("t5-small")
>>> model = FlaxT5ForConditionalGeneration.from_pretrained("t5-small")

>>> ARTICLE_TO_SUMMARIZE = "summarize: My friends are cool but they eat too many carbs."
>>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], return_tensors="np")

>>> # Generate Summary
>>> summary_ids = model.generate(inputs["input_ids"]).sequences
>>> print(tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False))
```

#### `encode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_flax_t5.py#L1072)

```py
( input_ids: Array attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`jnp.ndarray`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚T5 æ˜¯ä¸€ä¸ªå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤æ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨å³ä¾§å’Œå·¦ä¾§éƒ½å¡«å……è¾“å…¥ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    äº†è§£å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`input_ids`ï¼Œè¯·æŸ¥çœ‹ T5 è®­ç»ƒã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`jnp.ndarray`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š

    +   å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º 1ï¼Œ

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º 0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_flax_outputs.FlaxBaseModelOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_flax_outputs.FlaxBaseModelOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®(`<class 'transformers.models.t5.configuration_t5.T5Config'>`)å’Œè¾“å…¥ã€‚

+   `last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`ï¼‰â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—çš„è¾“å‡ºã€‚

+   `hidden_states`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration

>>> tokenizer = AutoTokenizer.from_pretrained("t5-small")
>>> model = FlaxT5ForConditionalGeneration.from_pretrained("t5-small")

>>> text = "My friends are cool but they eat too many carbs."
>>> inputs = tokenizer(text, return_tensors="np")
>>> encoder_outputs = model.encode(**inputs)
```

#### `decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_flax_t5.py#L1608)

```py
( decoder_input_ids encoder_outputs encoder_attention_mask: Optional = None decoder_attention_mask: Optional = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)
```

å‚æ•°

+   `decoder_input_ids` (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥ IDï¼Ÿ

    å¯¹äºè®­ç»ƒï¼Œåº”æä¾›`decoder_input_ids`ã€‚

+   `encoder_outputs` (`tuple(tuple(jnp.ndarray)`) â€” å…ƒç»„åŒ…æ‹¬(`last_hidden_state`, *å¯é€‰*: `hidden_states`, *å¯é€‰*: `attentions`) `last_hidden_state` çš„å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `encoder_attention_mask` (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *å¯é€‰*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨ `[0, 1]` ä¹‹é—´ï¼š

    +   å¯¹äºæœªè¢«æ©ç çš„æ ‡è®°ä¸º 1ï¼Œ

    +   å¯¹äºè¢«æ©ç çš„æ ‡è®°ä¸º 0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `decoder_attention_mask` (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *å¯é€‰*) â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¼ é‡ï¼Œå¿½ç•¥`decoder_input_ids`ä¸­çš„å¡«å……æ ‡è®°ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

    å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨ 1ã€‚

+   `past_key_values` (`Dict[str, np.ndarray]`, *å¯é€‰*, ç”±`init_cache`è¿”å›æˆ–ä¼ é€’å…ˆå‰çš„`past_key_values`æ—¶è¿”å›) â€” é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€å­—å…¸ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºå¿«é€Ÿè‡ªå›å½’è§£ç ã€‚é¢„å…ˆè®¡ç®—çš„é”®å’Œå€¼éšè—çŠ¶æ€çš„å½¢çŠ¶ä¸º *[batch_size, max_length]*ã€‚

+   `output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›å€¼

transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ`<class 'transformers.models.t5.configuration_t5.T5Config'>`ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `logits` (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ› softmax åçš„äº¤å‰æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `past_key_values`ï¼ˆ`tuple(tuple(jnp.ndarray))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰ â€” é•¿åº¦ä¸º`config.n_layers`çš„`jnp.ndarray`å…ƒç»„çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›å±‚çš„ç¼“å­˜é”®ã€å€¼çŠ¶æ€ï¼Œå¦‚æœæ¨¡å‹ç”¨äºç¼–ç å™¨-è§£ç å™¨è®¾ç½®ï¼Œåˆ™ç›¸å…³ã€‚ä»…åœ¨`config.is_decoder = True`æ—¶ç›¸å…³ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ä»¥ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆæŸ¥çœ‹`past_key_values`è¾“å…¥ï¼‰ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration
>>> import jax.numpy as jnp

>>> tokenizer = AutoTokenizer.from_pretrained("t5-small")
>>> model = FlaxT5ForConditionalGeneration.from_pretrained("t5-small")

>>> text = "summarize: My friends are cool but they eat too many carbs."
>>> inputs = tokenizer(text, return_tensors="np")
>>> encoder_outputs = model.encode(**inputs)

>>> decoder_start_token_id = model.config.decoder_start_token_id
>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

>>> outputs = model.decode(decoder_input_ids, encoder_outputs)
>>> logits = outputs.logits
```

## FlaxT5EncoderModel

### `class transformers.FlaxT5EncoderModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_flax_t5.py#L1454)

```py
( config: T5Config input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )
```

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/t5/modeling_flax_t5.py#L1457)

```py
( input_ids: Array attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None )
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`jnp.ndarray`ï¼‰ â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚T5 æ˜¯ä¸€ä¸ªå…·æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤æ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨å³ä¾§å’Œå·¦ä¾§å¡«å……è¾“å…¥ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    äº†è§£å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`input_ids`ï¼Œè¯·æŸ¥çœ‹ T5 è®­ç»ƒã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`jnp.ndarray`ï¼Œ*å¯é€‰*ï¼‰ â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0, 1]`ï¼š

    +   1 è¡¨ç¤º`æœªè¢«æ©ç›–`çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤º`è¢«æ©ç›–`çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›é®ç½©ï¼Ÿ

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

FlaxT5EncoderModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ æ’­çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è°ƒç”¨æ­¤å‡½æ•°ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
