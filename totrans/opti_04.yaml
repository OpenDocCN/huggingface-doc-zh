- en: Quick tour
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/optimum/quicktour](https://huggingface.co/docs/optimum/quicktour)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This quick tour is intended for developers who are ready to dive into the code
    and see examples of how to integrate 🤗 Optimum into their model training and inference
    workflows.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Accelerated inference
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenVINO
  id: totrans-5
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To load a model and run inference with OpenVINO Runtime, you can just replace
    your `AutoModelForXxx` class with the corresponding `OVModelForXxx` class. If
    you want to load a PyTorch checkpoint, set `export=True` to convert your model
    to the OpenVINO IR (Intermediate Representation).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can find more examples in the [documentation](https://huggingface.co/docs/optimum/intel/inference)
    and in the [examples](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: ONNX Runtime
  id: totrans-9
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To accelerate inference with ONNX Runtime, 🤗 Optimum uses *configuration objects*
    to define parameters for graph optimization and quantization. These objects are
    then used to instantiate dedicated *optimizers* and *quantizers*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Before applying quantization or optimization, first we need to load our model.
    To load a model and run inference with ONNX Runtime, you can just replace the
    canonical Transformers [`AutoModelForXxx`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel)
    class with the corresponding [`ORTModelForXxx`](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel)
    class. If you want to load from a PyTorch checkpoint, set `export=True` to export
    your model to the ONNX format.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s see now how we can apply dynamic quantization with ONNX Runtime:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this example, we’ve quantized a model from the Hugging Face Hub, in the
    same manner we can quantize a model hosted locally by providing the path to the
    directory containing the model weights. The result from applying the `quantize()`
    method is a `model_quantized.onnx` file that can be used to run inference. Here’s
    an example of how to load an ONNX Runtime model and generate predictions with
    it:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can find more examples in the [documentation](https://huggingface.co/docs/optimum/onnxruntime/quickstart)
    and in the [examples](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Accelerated training
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Habana
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To train transformers on Habana’s Gaudi processors, 🤗 Optimum provides a `GaudiTrainer`
    that is very similar to the 🤗 Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer).
    Here is a simple example:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can find more examples in the [documentation](https://huggingface.co/docs/optimum/habana/quickstart)
    and in the [examples](https://github.com/huggingface/optimum-habana/tree/main/examples).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: ONNX Runtime
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To train transformers with ONNX Runtime’s acceleration features, 🤗 Optimum
    provides a `ORTTrainer` that is very similar to the 🤗 Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer).
    Here is a simple example:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can find more examples in the [documentation](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer)
    and in the [examples](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Out of the box ONNX export
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Optimum library handles out of the box the ONNX export of Transformers and
    Diffusers models!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Exporting a model to ONNX is as simple as
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Check out the help for more options:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Check out the [documentation](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model)
    for more.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch’s BetterTransformer support
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[BetterTransformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)
    is a free-lunch PyTorch-native optimization to gain x1.25 - x4 speedup on the
    inference of Transformer-based models. It has been marked as stable in [PyTorch
    1.13](https://pytorch.org/blog/PyTorch-1.13-release/). We integrated BetterTransformer
    with the most-used models from the 🤗 Transformers libary, and using the integration
    is as simple as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[BetterTransformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)
    是一个免费的 PyTorch 本地优化，可在基于 Transformer 的模型推理中获得 x1.25 - x4 的加速。它已在[PyTorch 1.13](https://pytorch.org/blog/PyTorch-1.13-release/)中标记为稳定。我们将
    BetterTransformer 与 🤗 Transformers 库中最常用的模型集成，使用集成就像这样简单：'
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Check out the [documentation](https://huggingface.co/docs/optimum/bettertransformer/overview)
    for more details, and the [blog post on PyTorch’s Medium](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)
    to find out more about the integration!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 查看更多详细信息，请查看[文档](https://huggingface.co/docs/optimum/bettertransformer/overview)，并查看[PyTorch
    Medium 上的博文](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)以了解更多关于集成的信息！
- en: torch.fx integration
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: torch.fx 集成
- en: Optimum integrates with `torch.fx`, providing as a one-liner several graph transformations.
    We aim at supporting a better management of [quantization](https://huggingface.co/docs/optimum/concept_guides/quantization)
    through `torch.fx`, both for quantization-aware training (QAT) and post-training
    quantization (PTQ).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Optimum 与 `torch.fx` 集成，提供了几个图形转换的一行代码。我们旨在通过 `torch.fx` 支持更好的[量化](https://huggingface.co/docs/optimum/concept_guides/quantization)管理，包括量化感知训练（QAT）和训练后量化（PTQ）。
- en: Check out the [documentation](https://huggingface.co/docs/optimum/torch_fx/usage_guides/optimization)
    and [reference](https://huggingface.co/docs/optimum/torch_fx/package_reference/optimization)
    for more!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 查看更多[文档](https://huggingface.co/docs/optimum/torch_fx/usage_guides/optimization)和[参考资料](https://huggingface.co/docs/optimum/torch_fx/package_reference/optimization)！
