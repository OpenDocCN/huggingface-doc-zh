- en: Quick tour
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/optimum/quicktour](https://huggingface.co/docs/optimum/quicktour)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: This quick tour is intended for developers who are ready to dive into the code
    and see examples of how to integrate ðŸ¤— Optimum into their model training and inference
    workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Accelerated inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenVINO
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To load a model and run inference with OpenVINO Runtime, you can just replace
    your `AutoModelForXxx` class with the corresponding `OVModelForXxx` class. If
    you want to load a PyTorch checkpoint, set `export=True` to convert your model
    to the OpenVINO IR (Intermediate Representation).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can find more examples in the [documentation](https://huggingface.co/docs/optimum/intel/inference)
    and in the [examples](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino).
  prefs: []
  type: TYPE_NORMAL
- en: ONNX Runtime
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To accelerate inference with ONNX Runtime, ðŸ¤— Optimum uses *configuration objects*
    to define parameters for graph optimization and quantization. These objects are
    then used to instantiate dedicated *optimizers* and *quantizers*.
  prefs: []
  type: TYPE_NORMAL
- en: Before applying quantization or optimization, first we need to load our model.
    To load a model and run inference with ONNX Runtime, you can just replace the
    canonical Transformers [`AutoModelForXxx`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel)
    class with the corresponding [`ORTModelForXxx`](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel)
    class. If you want to load from a PyTorch checkpoint, set `export=True` to export
    your model to the ONNX format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Letâ€™s see now how we can apply dynamic quantization with ONNX Runtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, weâ€™ve quantized a model from the Hugging Face Hub, in the
    same manner we can quantize a model hosted locally by providing the path to the
    directory containing the model weights. The result from applying the `quantize()`
    method is a `model_quantized.onnx` file that can be used to run inference. Hereâ€™s
    an example of how to load an ONNX Runtime model and generate predictions with
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can find more examples in the [documentation](https://huggingface.co/docs/optimum/onnxruntime/quickstart)
    and in the [examples](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime).
  prefs: []
  type: TYPE_NORMAL
- en: Accelerated training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Habana
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To train transformers on Habanaâ€™s Gaudi processors, ðŸ¤— Optimum provides a `GaudiTrainer`
    that is very similar to the ðŸ¤— Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer).
    Here is a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can find more examples in the [documentation](https://huggingface.co/docs/optimum/habana/quickstart)
    and in the [examples](https://github.com/huggingface/optimum-habana/tree/main/examples).
  prefs: []
  type: TYPE_NORMAL
- en: ONNX Runtime
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To train transformers with ONNX Runtimeâ€™s acceleration features, ðŸ¤— Optimum
    provides a `ORTTrainer` that is very similar to the ðŸ¤— Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer).
    Here is a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You can find more examples in the [documentation](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer)
    and in the [examples](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training).
  prefs: []
  type: TYPE_NORMAL
- en: Out of the box ONNX export
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Optimum library handles out of the box the ONNX export of Transformers and
    Diffusers models!
  prefs: []
  type: TYPE_NORMAL
- en: Exporting a model to ONNX is as simple as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Check out the help for more options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Check out the [documentation](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model)
    for more.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorchâ€™s BetterTransformer support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[BetterTransformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)
    is a free-lunch PyTorch-native optimization to gain x1.25 - x4 speedup on the
    inference of Transformer-based models. It has been marked as stable in [PyTorch
    1.13](https://pytorch.org/blog/PyTorch-1.13-release/). We integrated BetterTransformer
    with the most-used models from the ðŸ¤— Transformers libary, and using the integration
    is as simple as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Check out the [documentation](https://huggingface.co/docs/optimum/bettertransformer/overview)
    for more details, and the [blog post on PyTorchâ€™s Medium](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)
    to find out more about the integration!
  prefs: []
  type: TYPE_NORMAL
- en: torch.fx integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Optimum integrates with `torch.fx`, providing as a one-liner several graph transformations.
    We aim at supporting a better management of [quantization](https://huggingface.co/docs/optimum/concept_guides/quantization)
    through `torch.fx`, both for quantization-aware training (QAT) and post-training
    quantization (PTQ).
  prefs: []
  type: TYPE_NORMAL
- en: Check out the [documentation](https://huggingface.co/docs/optimum/torch_fx/usage_guides/optimization)
    and [reference](https://huggingface.co/docs/optimum/torch_fx/package_reference/optimization)
    for more!
  prefs: []
  type: TYPE_NORMAL
