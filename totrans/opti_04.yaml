- en: Quick tour
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速导览
- en: 'Original text: [https://huggingface.co/docs/optimum/quicktour](https://huggingface.co/docs/optimum/quicktour)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/optimum/quicktour](https://huggingface.co/docs/optimum/quicktour)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This quick tour is intended for developers who are ready to dive into the code
    and see examples of how to integrate 🤗 Optimum into their model training and inference
    workflows.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这个快速导览适用于准备深入代码并查看如何将🤗 Optimum集成到他们的模型训练和推理工作流程中的开发人员。
- en: Accelerated inference
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速推理
- en: OpenVINO
  id: totrans-5
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: OpenVINO
- en: To load a model and run inference with OpenVINO Runtime, you can just replace
    your `AutoModelForXxx` class with the corresponding `OVModelForXxx` class. If
    you want to load a PyTorch checkpoint, set `export=True` to convert your model
    to the OpenVINO IR (Intermediate Representation).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载模型并使用OpenVINO Runtime进行推理，您只需将您的`AutoModelForXxx`类替换为相应的`OVModelForXxx`类。如果要加载PyTorch检查点，请设置`export=True`以将模型转换为OpenVINO
    IR（中间表示）。
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can find more examples in the [documentation](https://huggingface.co/docs/optimum/intel/inference)
    and in the [examples](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[文档](https://huggingface.co/docs/optimum/intel/inference)和[示例](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino)中找到更多示例。
- en: ONNX Runtime
  id: totrans-9
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ONNX Runtime
- en: To accelerate inference with ONNX Runtime, 🤗 Optimum uses *configuration objects*
    to define parameters for graph optimization and quantization. These objects are
    then used to instantiate dedicated *optimizers* and *quantizers*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加速使用ONNX Runtime进行推理，🤗 Optimum使用*配置对象*来定义图优化和量化的参数。然后使用这些对象来实例化专用的*优化器*和*量化器*。
- en: Before applying quantization or optimization, first we need to load our model.
    To load a model and run inference with ONNX Runtime, you can just replace the
    canonical Transformers [`AutoModelForXxx`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel)
    class with the corresponding [`ORTModelForXxx`](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel)
    class. If you want to load from a PyTorch checkpoint, set `export=True` to export
    your model to the ONNX format.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用量化或优化之前，首先我们需要加载我们的模型。要加载模型并使用ONNX Runtime进行推理，您只需将经典的Transformers [`AutoModelForXxx`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel)类替换为相应的[`ORTModelForXxx`](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel)类。如果要从PyTorch检查点加载，请设置`export=True`以将模型导出为ONNX格式。
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s see now how we can apply dynamic quantization with ONNX Runtime:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用ONNX Runtime应用动态量化：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this example, we’ve quantized a model from the Hugging Face Hub, in the
    same manner we can quantize a model hosted locally by providing the path to the
    directory containing the model weights. The result from applying the `quantize()`
    method is a `model_quantized.onnx` file that can be used to run inference. Here’s
    an example of how to load an ONNX Runtime model and generate predictions with
    it:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们对来自Hugging Face Hub的模型进行了量化，以相同的方式，我们可以通过提供包含模型权重的目录路径来对本地托管的模型进行量化。应用`quantize()`方法的结果是一个`model_quantized.onnx`文件，可用于运行推理。这里是如何加载一个ONNX
    Runtime模型并生成预测的示例：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can find more examples in the [documentation](https://huggingface.co/docs/optimum/onnxruntime/quickstart)
    and in the [examples](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[文档](https://huggingface.co/docs/optimum/onnxruntime/quickstart)和[示例](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime)中找到更多示例。
- en: Accelerated training
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速训练
- en: Habana
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Habana
- en: 'To train transformers on Habana’s Gaudi processors, 🤗 Optimum provides a `GaudiTrainer`
    that is very similar to the 🤗 Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer).
    Here is a simple example:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在Habana的Gaudi处理器上训练transformers，🤗 Optimum提供了一个`GaudiTrainer`，它与🤗 Transformers
    [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)非常相似。这里是一个简单的例子：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can find more examples in the [documentation](https://huggingface.co/docs/optimum/habana/quickstart)
    and in the [examples](https://github.com/huggingface/optimum-habana/tree/main/examples).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[文档](https://huggingface.co/docs/optimum/habana/quickstart)和[示例](https://github.com/huggingface/optimum-habana/tree/main/examples)中找到更多示例。
- en: ONNX Runtime
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ONNX Runtime
- en: 'To train transformers with ONNX Runtime’s acceleration features, 🤗 Optimum
    provides a `ORTTrainer` that is very similar to the 🤗 Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer).
    Here is a simple example:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ONNX Runtime的加速功能来训练transformers，🤗 Optimum提供了一个`ORTTrainer`，它与🤗 Transformers
    [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)非常相似。这里是一个简单的例子：
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can find more examples in the [documentation](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer)
    and in the [examples](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[文档](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer)和[示例](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training)中找到更多示例。
- en: Out of the box ONNX export
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开箱即用的ONNX导出
- en: The Optimum library handles out of the box the ONNX export of Transformers and
    Diffusers models!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Optimum库可以直接处理Transformers和Diffusers模型的ONNX导出！
- en: Exporting a model to ONNX is as simple as
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型导出到ONNX就像这样简单
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Check out the help for more options:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 查看帮助以获取更多选项：
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Check out the [documentation](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model)
    for more.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[文档](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model)获取更多信息。
- en: PyTorch’s BetterTransformer support
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch的BetterTransformer支持
- en: '[BetterTransformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)
    is a free-lunch PyTorch-native optimization to gain x1.25 - x4 speedup on the
    inference of Transformer-based models. It has been marked as stable in [PyTorch
    1.13](https://pytorch.org/blog/PyTorch-1.13-release/). We integrated BetterTransformer
    with the most-used models from the 🤗 Transformers libary, and using the integration
    is as simple as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[BetterTransformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)
    是一个免费的 PyTorch 本地优化，可在基于 Transformer 的模型推理中获得 x1.25 - x4 的加速。它已在[PyTorch 1.13](https://pytorch.org/blog/PyTorch-1.13-release/)中标记为稳定。我们将
    BetterTransformer 与 🤗 Transformers 库中最常用的模型集成，使用集成就像这样简单：'
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Check out the [documentation](https://huggingface.co/docs/optimum/bettertransformer/overview)
    for more details, and the [blog post on PyTorch’s Medium](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)
    to find out more about the integration!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 查看更多详细信息，请查看[文档](https://huggingface.co/docs/optimum/bettertransformer/overview)，并查看[PyTorch
    Medium 上的博文](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)以了解更多关于集成的信息！
- en: torch.fx integration
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: torch.fx 集成
- en: Optimum integrates with `torch.fx`, providing as a one-liner several graph transformations.
    We aim at supporting a better management of [quantization](https://huggingface.co/docs/optimum/concept_guides/quantization)
    through `torch.fx`, both for quantization-aware training (QAT) and post-training
    quantization (PTQ).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Optimum 与 `torch.fx` 集成，提供了几个图形转换的一行代码。我们旨在通过 `torch.fx` 支持更好的[量化](https://huggingface.co/docs/optimum/concept_guides/quantization)管理，包括量化感知训练（QAT）和训练后量化（PTQ）。
- en: Check out the [documentation](https://huggingface.co/docs/optimum/torch_fx/usage_guides/optimization)
    and [reference](https://huggingface.co/docs/optimum/torch_fx/package_reference/optimization)
    for more!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 查看更多[文档](https://huggingface.co/docs/optimum/torch_fx/usage_guides/optimization)和[参考资料](https://huggingface.co/docs/optimum/torch_fx/package_reference/optimization)！
