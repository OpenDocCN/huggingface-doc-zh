# LoRA

> 原文：[https://huggingface.co/docs/diffusers/api/loaders/lora](https://huggingface.co/docs/diffusers/api/loaders/lora)

LoRA是一种快速轻量级的训练方法，它插入并训练了数量明显较少的参数，而不是所有模型参数。这会产生一个较小的文件（约100 MB），使得可以更快地训练模型来学习一个新概念。LoRA权重通常加载到UNet、文本编码器或两者中。有两个用于加载LoRA权重的类：

+   `LoraLoaderMixin`提供了用于加载和卸载、融合和解除融合、启用和禁用以及管理LoRA权重的更多函数的函数。此类可与任何模型一起使用。

+   `StableDiffusionXLLoraLoaderMixin`是`LoraLoaderMixin`类的[稳定扩散（SDXL）](../../api/pipelines/stable_diffusion/stable_diffusion_xl)版本，用于加载和保存LoRA权重。它只能与SDXL模型一起使用。

要了解如何加载LoRA权重的更多信息，请参阅[LoRA](../../using-diffusers/loading_adapters#lora)加载指南。

## LoraLoaderMixin

### `class diffusers.loaders.LoraLoaderMixin`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L72)

```py
( )
```

将LoRA层加载到[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)和[`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)中。

#### `delete_adapters`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1284)

```py
( adapter_names: Union )
```

参数

+   `删除`适配器名称的LoRA层，用于UNet和文本编码器。 — adapter_names (`Union[List[str], str]`): 要删除的适配器的名称。可以是单个字符串或字符串列表

#### `disable_lora_for_text_encoder`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1208)

```py
( text_encoder: Optional = None )
```

参数

+   `text_encoder` (`torch.nn.Module`, *可选*) — 用于禁用LoRA层的文本编码器模块。如果为`None`，则会尝试获取`text_encoder`属性。

禁用文本编码器的LoRA层。

#### `enable_lora_for_text_encoder`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1225)

```py
( text_encoder: Optional = None )
```

参数

+   `text_encoder` (`torch.nn.Module`, *可选*) — 用于启用LoRA层的文本编码器模块。如果为`None`，则会尝试获取`text_encoder`属性。

启用文本编码器的LoRA层。

#### `fuse_lora`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1000)

```py
( fuse_unet: bool = True fuse_text_encoder: bool = True lora_scale: float = 1.0 safe_fusing: bool = False adapter_names: Optional = None )
```

参数

+   `fuse_unet` (`bool`, 默认为 `True`) — 是否融合UNet的LoRA参数。

+   `fuse_text_encoder` (`bool`, 默认为 `True`) — 是否融合文本编码器LoRA参数。如果文本编码器未使用LoRA参数进行monkey-patch，则不会产生任何效果。

+   `lora_scale` (`float`, 默认为 1.0) — 控制LoRA参数对输出的影响程度。

+   `safe_fusing` (`bool`, 默认为 `False`) — 在融合之前检查融合权重是否为NaN值，如果值为NaN，则不进行融合。

+   `adapter_names` (`List[str]`, *可选*) — 用于融合的适配器名称。如果未传递任何内容，则将融合所有活动适配器。

将LoRA参数融合到相应块的原始参数中。

这是一个实验性API。

示例：

```py
from diffusers import DiffusionPipeline
import torch

pipeline = DiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16
).to("cuda")
pipeline.load_lora_weights("nerijs/pixel-art-xl", weight_name="pixel-art-xl.safetensors", adapter_name="pixel")
pipeline.fuse_lora(lora_scale=0.7)
```

#### `get_active_adapters`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1308)

```py
( )
```

获取当前活动适配器的列表。

示例：

```py
from diffusers import DiffusionPipeline

pipeline = DiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
).to("cuda")
pipeline.load_lora_weights("CiroN2022/toy-face", weight_name="toy_face_sdxl.safetensors", adapter_name="toy")
pipeline.get_active_adapters()
```

#### `get_list_adapters`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1340)

```py
( )
```

获取管道中所有可用适配器的当前列表。

#### `load_lora_into_text_encoder`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L484)

```py
( state_dict network_alphas text_encoder prefix = None lora_scale = 1.0 low_cpu_mem_usage = None adapter_name = None _pipeline = None )
```

参数

+   `state_dict` (`dict`) — 包含lora层参数的标准状态字典。键应该以额外的`text_encoder`为前缀，以区分unet lora层。

+   `network_alphas` (`Dict[str, float]`) — 有关更多详细信息，请参阅`LoRALinearLayer`。

+   `text_encoder` (`CLIPTextModel`) — 要加载LoRA层的文本编码器模型。

+   `prefix` (`str`) — `state_dict`中`text_encoder`的预期前缀。

+   `lora_scale` (`float`) — 在将lora线性层的输出与常规lora层的输出相加之前，需要对其进行缩放的比例。

+   `low_cpu_mem_usage` (`bool`, *可选*, 如果torch版本 >= 1.9.0则默认为`True`，否则为`False`) — 加快模型加载，仅加载预训练权重而不初始化权重。在加载模型时，还尝试不使用超过CPU内存中的1倍模型大小（包括峰值内存）。仅支持PyTorch >= 1.9.0。如果您使用较旧版本的PyTorch，将此参数设置为`True`将引发错误。

+   `adapter_name` (`str`, *可选*) — 用于引用加载的适配器模型的适配器名称。如果未指定，将使用`default_{i}`，其中i是正在加载的适配器的总数。

这将把`state_dict`中指定的LoRA层加载到`text_encoder`中

#### `load_lora_into_transformer`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L667)

```py
( state_dict network_alphas transformer low_cpu_mem_usage = None adapter_name = None _pipeline = None )
```

参数

+   `state_dict` (`dict`) — 包含lora层参数的标准状态字典。键可以直接索引到unet，也可以以额外的`unet`为前缀，用于区分文本编码器lora层。

+   `network_alphas` (`Dict[str, float]`) — 有关更多详细信息，请参阅`LoRALinearLayer`。

+   `unet` (`UNet2DConditionModel`) — 要加载LoRA层的UNet模型。

+   `low_cpu_mem_usage` (`bool`, *可选*, 如果torch版本 >= 1.9.0则默认为`True`，否则为`False`) — 加快模型加载，仅加载预训练权重而不初始化权重。在加载模型时，还尝试不使用超过CPU内存中的1倍模型大小（包括峰值内存）。仅支持PyTorch >= 1.9.0。如果您使用较旧版本的PyTorch，将此参数设置为`True`将引发错误。

+   `adapter_name` (`str`, *可选*) — 用于引用加载的适配器模型的适配器名称。如果未指定，将使用`default_{i}`，其中i是正在加载的适配器的总数。

这将把`state_dict`中指定的LoRA层加载到`transformer`中。

#### `load_lora_into_unet`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L375)

```py
( state_dict network_alphas unet low_cpu_mem_usage = None adapter_name = None _pipeline = None )
```

参数

+   `state_dict` (`dict`) — 包含lora层参数的标准状态字典。键可以直接索引到unet，也可以以额外的`unet`为前缀，用于区分文本编码器lora层。

+   `network_alphas` (`Dict[str, float]`) — 有关更多详细信息，请参阅`LoRALinearLayer`。

+   `unet` (`UNet2DConditionModel`) — 要加载LoRA层的UNet模型。

+   `low_cpu_mem_usage` (`bool`, *可选*, 如果torch版本 >= 1.9.0则默认为`True`，否则为`False`) — 加快模型加载，仅加载预训练权重而不初始化权重。在加载模型时，还尝试不使用超过CPU内存中的1倍模型大小（包括峰值内存）。仅支持PyTorch >= 1.9.0。如果您使用较旧版本的PyTorch，将此参数设置为`True`将引发错误。

+   `adapter_name` (`str`, *可选*) — 用于引用加载的适配器模型的适配器名称。如果未指定，将使用`default_{i}`，其中i是正在加载的适配器的总数。

这将把`state_dict`中指定的LoRA层加载到`unet`中。

#### `load_lora_weights`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L83)

```py
( pretrained_model_name_or_path_or_dict: Union adapter_name = None **kwargs )
```

参数

+   `pretrained_model_name_or_path_or_dict` (`str`或`os.PathLike`或`dict`) — 查看[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)。

+   `kwargs` (`dict`，*可选*) — 查看[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)。

+   `adapter_name` (`str`，*可选*) — 用于引用加载的适配器模型的适配器名称。如果未指定，将使用`default_{i}`，其中i是正在加载的适配器总数。

将`pretrained_model_name_or_path_or_dict`中指定的LoRA权重加载到`self.unet`和`self.text_encoder`中。

所有kwargs都将转发到`self.lora_state_dict`。

查看[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)以获取有关如何加载状态字典的更多详细信息。

查看[load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)以获取有关如何将状态字典加载到`self.unet`中的更多详细信息。

查看[load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)以获取有关如何将状态字典加载到`self.text_encoder`中的更多详细信息。

#### `lora_state_dict`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L138)

```py
( pretrained_model_name_or_path_or_dict: Union **kwargs )
```

参数

+   `pretrained_model_name_or_path_or_dict` (`str`或`os.PathLike`或`dict`) — 可以是：

    +   一个字符串，预训练模型在Hub上托管的*模型ID*（例如`google/ddpm-celebahq-256`）。

    +   一个*目录*路径（例如`./my_model_directory`），其中包含使用[ModelMixin.save_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.save_pretrained)保存的模型权重。

    +   一个[torch状态字典](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)。

+   `cache_dir` (`Union[str, os.PathLike]`，*可选*) — 下载预训练模型配置的目录路径，如果未使用标准缓存。

+   `force_download` (`bool`，*可选*，默认为`False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`，*可选*，默认为`False`) — 是否恢复下载模型权重和配置文件。如果设置为`False`，则删除任何未完全下载的文件。

+   `proxies` (`Dict[str, str]`，*可选*) — 要使用的代理服务器字典，按协议或端点，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。代理在每个请求上使用。

+   `local_files_only` (`bool`，*可选*，默认为`False`) — 是否仅加载本地模型权重和配置文件。如果设置为`True`，则不会从Hub下载模型。

+   `token` (`str`或*bool*，*可选*) — 用作远程文件的HTTP令牌的授权。如果为`True`，则使用从`diffusers-cli login`生成的令牌（存储在`~/.huggingface`）。

+   `revision` (`str`，*可选*，默认为`"main"`) — 要使用的特定模型版本。可以是分支名称、标签名称、提交ID或Git允许的任何标识符。

+   `subfolder` (`str`，*可选*，默认为`""`) — 模型文件在Hub或本地较大模型存储库中的子文件夹位置。

+   `low_cpu_mem_usage` (`bool`，*可选*，如果torch版本>= 1.9.0，默认为`True`，否则为`False`) — 加速模型加载，仅加载预训练权重而不初始化权重。这还尝试在加载模型时不使用超过1倍模型大小的CPU内存（包括峰值内存）。仅支持PyTorch >= 1.9.0。如果您使用较旧版本的PyTorch，将此参数设置为`True`将引发错误。

+   `mirror`（`str`，*可选*）— 如果您在中国下载模型时遇到可访问性问题，可以将源镜像到解决问题。我们不保证源的及时性或安全性，您应参考镜像站点获取更多信息。

返回LoRA权重和网络α的状态字典。

我们支持以有限的能力加载A1111格式的LoRA检查点。

此函数是实验性的，可能会在将来更改。

#### `save_lora_weights`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L869)

```py
( save_directory: Union unet_lora_layers: Dict = None text_encoder_lora_layers: Dict = None transformer_lora_layers: Dict = None is_main_process: bool = True weight_name: str = None save_function: Callable = None safe_serialization: bool = True )
```

参数

+   `save_directory`（`str`或`os.PathLike`）— 要保存LoRA参数的目录。如果目录不存在，将会创建。

+   `unet_lora_layers`（`Dict[str, torch.nn.Module]`或`Dict[str, torch.Tensor]`）— 与`unet`对应的LoRA层的状态字典。

+   `text_encoder_lora_layers`（`Dict[str, torch.nn.Module]`或`Dict[str, torch.Tensor]`）— 与`text_encoder`对应的LoRA层的状态字典。必须显式传递文本编码器LoRA状态字典，因为它来自🤗 Transformers。

+   `is_main_process`（`bool`，*可选*，默认为`True`）— 调用此函数的进程是否为主进程。在分布式训练期间非常有用，当您需要在所有进程上调用此函数时。在这种情况下，仅在主进程上设置`is_main_process=True`，以避免竞争条件。

+   `save_function`（`Callable`）— 用于保存状态字典的函数。在分布式训练期间需要用另一种方法替换`torch.save`时很有用。可以使用环境变量`DIFFUSERS_SAVE_MODE`进行配置。

+   `safe_serialization`（`bool`，*可选*，默认为`True`）— 是否使用`safetensors`保存模型，还是使用传统的PyTorch方式与`pickle`保存。

保存与UNet和文本编码器对应的LoRA参数。

#### `set_adapters_for_text_encoder`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1166)

```py
( adapter_names: Union text_encoder: Optional = None text_encoder_weights: List = None )
```

参数

+   `adapter_names`（`List[str]`或`str`）— 要使用的适配器的名称。

+   `text_encoder`（`torch.nn.Module`，*可选*）— 要设置适配器层的文本编码器模块。如果为`None`，将尝试获取`text_encoder`属性。

+   `text_encoder_weights`（`List[float]`，*可选*）— 用于文本编码器的权重。如果为`None`，则所有适配器的权重都设置为`1.0`。

为文本编码器设置适配器层。

#### `set_lora_device`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1363)

```py
( adapter_names: List device: Union )
```

参数

+   `adapter_names`（`List[str]`）— 要发送到设备的适配器列表。

+   `device`（`Union[torch.device, str, int]`）— 要发送适配器的设备。可以是torch设备、字符串或整数。

将`adapter_names`中列出的LoRA移动到目标设备。如果要加载多个适配器并释放一些GPU内存，可以将LoRA卸载到CPU上。

#### `unfuse_lora`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1106)

```py
( unfuse_unet: bool = True unfuse_text_encoder: bool = True )
```

参数

+   `unfuse_unet`（`bool`，默认为`True`）— 是否解除UNet的LoRA参数。

+   `unfuse_text_encoder`（`bool`，默认为`True`）— 是否解除文本编码器LoRA参数的融合。如果文本编码器未使用LoRA参数进行monkey-patch，则不会产生任何效果。

撤销[`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraLoaderMixin.fuse_lora)的效果。

这是一个实验性API。

#### `unload_lora_weights`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L968)

```py
( )
```

卸载LoRA参数。

示例：

```py
>>> # Assuming `pipeline` is already loaded with the LoRA parameters.
>>> pipeline.unload_lora_weights()
>>> ...
```

## StableDiffusionXLLoraLoaderMixin

### `class diffusers.loaders.StableDiffusionXLLoraLoaderMixin`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1404)

```py
( )
```

此类使用 LoRA 加载/保存代码覆盖了 `LoraLoaderMixin`，该代码特定于 SDXL。

#### `load_lora_weights`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1408)

```py
( pretrained_model_name_or_path_or_dict: Union adapter_name: Optional = None **kwargs )
```

参数

+   `pretrained_model_name_or_path_or_dict` (`str` 或 `os.PathLike` 或 `dict`) — 查看 [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)。

+   `adapter_name` (`str`, *可选*) — 用于引用加载的适配器模型的适配器名称。如果未指定，将使用 `default_{i}`，其中 i 是要加载的适配器总数。

+   `kwargs` (`dict`, *可选*) — 查看 [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)。

将在 `pretrained_model_name_or_path_or_dict` 中指定的 LoRA 权重加载到 `self.unet` 和 `self.text_encoder` 中。

所有 kwargs 都会传递给 `self.lora_state_dict`。

查看 [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict) 以了解如何加载状态字典的更多细节。

查看 [load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet) 以了解如何将状态字典加载到 `self.unet` 中的更多细节。

查看 [load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder) 以了解如何将状态字典加载到 `self.text_encoder` 中的更多细节。
