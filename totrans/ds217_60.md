# 主要类

> 原始文本：[https://huggingface.co/docs/datasets/package_reference/main_classes](https://huggingface.co/docs/datasets/package_reference/main_classes)

## DatasetInfo

### `class datasets.DatasetInfo`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L92)

```py
( description: str = <factory> citation: str = <factory> homepage: str = <factory> license: str = <factory> features: Optional = None post_processed: Optional = None supervised_keys: Optional = None task_templates: Optional = None builder_name: Optional = None dataset_name: Optional = None config_name: Optional = None version: Union = None splits: Optional = None download_checksums: Optional = None download_size: Optional = None post_processing_size: Optional = None dataset_size: Optional = None size_in_bytes: Optional = None )
```

参数

+   `description` (`str`) — 数据集的描述。

+   `citation` (`str`) — 数据集的BibTeX引用。

+   `homepage` (`str`) — 数据集官方主页的URL。

+   `license` (`str`) — 数据集的许可证。可以是许可证的名称或包含许可证条款的段落。

+   `features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features), *optional*) — 用于指定数据集列类型的特征。

+   `post_processed` (`PostProcessedInfo`, *optional*) — 有关数据集可能的后处理资源的信息。例如，它可以包含索引的信息。

+   `supervised_keys` (`SupervisedKeysData`, *optional*) — 如果适用于数据集，则指定监督学习的输入特征和标签（来自TFDS的遗留）。

+   `builder_name` (`str`, *optional*) — 用于创建数据集的`GeneratorBasedBuilder`子类的名称。通常与相应的脚本名称匹配。它也是数据集构建器类名称的蛇形命名版本。

+   `config_name` (`str`, *optional*) — 派生自[BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)的配置名称。

+   `version` (`str` or [Version](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.Version), *optional*) — 数据集的版本。

+   `splits` (`dict`, *optional*) — 拆分名称和元数据之间的映射。

+   `download_checksums` (`dict`, *optional*) — 下载数据集校验和及相应元数据之间的映射。

+   `download_size` (`int`, *optional*) — 下载文件以生成数据集的大小，以字节为单位。

+   `post_processing_size` (`int`, *optional*) — 经过后处理后数据集的大小，以字节为单位（如果有的话）。

+   `dataset_size` (`int`, *optional*) — 所有拆分的Arrow表的组合大小（以字节为单位）。

+   `size_in_bytes` (`int`, *optional*) — 与数据集相关的所有文件的组合大小（下载的文件+Arrow文件）。

+   `task_templates` (`List[TaskTemplate]`, *optional*) — 在训练和评估期间为数据集准备任务模板。每个模板将数据集的[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)转换为`datasets.tasks`中详细说明的标准化列名和类型。

+   *`*config_kwargs`（额外的关键字参数） — 要传递给[BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)并在[DatasetBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder)中使用的关键字参数。

有关数据集的信息。

`DatasetInfo` 文档数据集，包括其名称、版本和特征。查看构造函数参数和属性以获取完整列表。

并非所有字段在构建时都已知，可能稍后更新。

#### `from_directory`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L304)

```py
( dataset_info_dir: str fs = 'deprecated' storage_options: Optional = None )
```

参数

+   `dataset_info_dir` (`str`) — 包含元数据文件的目录。这应该是特定数据集版本的根目录。

+   `fs` (`fsspec.spec.AbstractFileSystem`, *optional*) — 用于从中下载文件的远程文件系统的实例。

    2.9.0中弃用

    `fs` 在版本2.9.0中已弃用，并将在3.0.0中删除。请改用`storage_options`，例如`storage_options=fs.storage_options`。

+   `storage_options` (`dict`, *optional*) — 要传递给文件系统后端的键/值对（如果有的话）。

    2.9.0中添加

从`dataset_info_dir`中的JSON文件创建[DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)。

此函数更新[DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)的所有动态生成字段（num_examples、hash、创建时间等）。

这将覆盖所有先前的元数据。

示例：

```py
>>> from datasets import DatasetInfo
>>> ds_info = DatasetInfo.from_directory("/path/to/directory/")
```

#### `write_to_directory`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L212)

```py
( dataset_info_dir pretty_print = False fs = 'deprecated' storage_options: Optional = None )
```

参数

+   `dataset_info_dir` (`str`) — 目标目录。

+   `pretty_print` (`bool`, 默认为`False`) — 如果为`True`，JSON将以缩进级别4进行漂亮打印。

+   `fs` (`fsspec.spec.AbstractFileSystem`, *可选*) — 用于从远程文件系统下载文件的实例。

    在2.9.0中弃用

    `fs`在版本2.9.0中已弃用，并将在3.0.0中移除。请改用`storage_options`，例如`storage_options=fs.storage_options`。

+   `storage_options` (`dict`, *可选*) — 要传递给文件系统后端的键/值对。

    添加于2.9.0

将`DatasetInfo`和许可证（如果存在）写入`dataset_info_dir`作为JSON文件。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.info.write_to_directory("/path/to/directory/")
```

## 数据集

基类[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)实现了由Apache Arrow表支持的数据集。

### `class datasets.Dataset`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L659)

```py
( arrow_table: Table info: Optional = None split: Optional = None indices_table: Optional = None fingerprint: Optional = None )
```

由Arrow表支持的数据集。

#### `add_column`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5602)

```py
( name: str column: Union new_fingerprint: str )
```

参数

+   `name` (`str`) — 列名。

+   `column` (`list` 或 `np.array`) — 要添加的列数据。

向数据集添加列。

添加于1.7

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> more_text = ds["text"]
>>> ds.add_column(name="text_2", column=more_text)
Dataset({
    features: ['text', 'label', 'text_2'],
    num_rows: 1066
})
```

#### `add_item`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5849)

```py
( item: dict new_fingerprint: str )
```

参数

+   `item` (`dict`) — 要添加的项目数据。

向数据集添加项目。

添加于1.7

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> new_review = {'label': 0, 'text': 'this movie is the absolute worst thing I have ever seen'}
>>> ds = ds.add_item(new_review)
>>> ds[-1]
{'label': 0, 'text': 'this movie is the absolute worst thing I have ever seen'}
```

#### `from_file`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L737)

```py
( filename: str info: Optional = None split: Optional = None indices_filename: Optional = None in_memory: bool = False )
```

参数

+   `filename` (`str`) — 数据集的文件名。

+   `info` (`DatasetInfo`, *可选*) — 数据集信息，如描述、引用等。

+   `split` (`NamedSplit`, *可选*) — 数据集拆分的名称。

+   `indices_filename` (`str`, *可选*) — 索引文件名。

+   `in_memory` (`bool`, 默认为`False`) — 是否将数据复制到内存中。

在文件名处实例化由Arrow表支持的数据集。

#### `from_buffer`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L777)

```py
( buffer: Buffer info: Optional = None split: Optional = None indices_buffer: Optional = None )
```

参数

+   `buffer` (`pyarrow.Buffer`) — Arrow缓冲区。

+   `info` (`DatasetInfo`, *可选*) — 数据集信息，如描述、引用等。

+   `split` (`NamedSplit`, *可选*) — 数据集拆分的名称。

+   `indices_buffer` (`pyarrow.Buffer`, *可选*) — 索引Arrow缓冲区。

实例化由Arrow缓冲区支持的数据集。

#### `from_pandas`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L809)

```py
( df: DataFrame features: Optional = None info: Optional = None split: Optional = None preserve_index: Optional = None )
```

参数

+   `df` (`pandas.DataFrame`) — 包含数据集的数据框。

+   `features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features), *可选*) — 数据集特征。

+   `info` (`DatasetInfo`, *可选*) — 数据集信息，如描述、引用等。

+   `split` (`NamedSplit`, *可选*) — 数据集拆分的名称。

+   `preserve_index` (`bool`, *可选*) — 是否将索引存储为结果数据集中的附加列。`None`的默认值将索引存储为列，除了`RangeIndex`，它仅存储为元数据。使用`preserve_index=True`强制将其存储为列。

将`pandas.DataFrame`转换为`pyarrow.Table`以创建[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)。

生成的 Arrow 表中的列类型是从 DataFrame 中的 `pandas.Series` 的 dtype 推断出来的。对于非对象 Series，NumPy dtype 被转换为其对应的 Arrow 类型。对于 `object`，我们需要通过查看该 Series 中的 Python 对象来猜测数据类型。

请注意，`object` 类型的 Series 不包含足够的信息，无法始终导致有意义的 Arrow 类型。在我们无法推断类型的情况下，例如因为 DataFrame 的长度为 0 或 Series 只包含 `None/nan` 对象，类型将被设置为 `null`。可以通过构建明确的特征并将其传递给此函数来避免这种行为。

示例：

```py
>>> ds = Dataset.from_pandas(df)
```

#### `from_dict`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L871)

```py
( mapping: dict features: Optional = None info: Optional = None split: Optional = None )
```

参数

+   `mapping` (`Mapping`) — 字符串到数组或 Python 列表的映射。

+   `features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features), *optional*) — 数据集特征。

+   `info` (`DatasetInfo`, *optional*) — 数据集信息，如描述、引用等。

+   `split` (`NamedSplit`, *optional*) — 数据集拆分的名称。

将 `dict` 转换为 `pyarrow.Table` 以创建 [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)。

#### `from_generator`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1007)

```py
( generator: Callable features: Optional = None cache_dir: str = None keep_in_memory: bool = False gen_kwargs: Optional = None num_proc: Optional = None **kwargs )
```

参数

+   `generator` ( —`Callable`): 生成器函数，`yield` 例子。

+   `features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features), *optional*) — 数据集特征。

+   `cache_dir` (`str`, *optional*, 默认为 `"~/.cache/huggingface/datasets"`) — 缓存数据的目录。

+   `keep_in_memory` (`bool`, 默认为 `False`) — 是否将数据复制到内存中。

+   `gen_kwargs(dict,` *optional*) — 要传递给 `generator` 可调用对象的关键字参数。您可以通过在 `gen_kwargs` 中传递分片列表并将 `num_proc` 设置为大于 1 来定义分片数据集。

+   `num_proc` (`int`, *optional*, 默认为 `None`) — 下载和本地生成数据集时的进程数。如果数据集由多个文件组成，则这很有帮助。默认情况下禁用多进程。如果 `num_proc` 大于一，则 `gen_kwargs` 中的所有列表值必须具有相同的长度。这些值将在对生成器的调用之间进行分割。分片数将是 `gen_kwargs` 中最短列表和 `num_proc` 中最小值。

    2.7.0 中添加

+   *`*kwargs`（额外的关键字参数） — 要传递给 :`GeneratorConfig` 的关键字参数。

从生成器创建数据集。

示例：

```py
>>> def gen():
...     yield {"text": "Good", "label": 0}
...     yield {"text": "Bad", "label": 1}
...
>>> ds = Dataset.from_generator(gen)
```

```py
>>> def gen(shards):
...     for shard in shards:
...         with open(shard) as f:
...             for line in f:
...                 yield {"line": line}
...
>>> shards = [f"data{i}.txt" for i in range(32)]
>>> ds = Dataset.from_generator(gen, gen_kwargs={"shards": shards})
```

#### `data`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1739)

```py
( )
```

支持数据集的 Apache Arrow 表。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.data
MemoryMappedTable
text: string
label: int64
----
text: [["compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .","the soundtrack alone is worth the price of admission .","rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .","beneath the film's obvious determination to shock at any cost lies considerable skill and determination , backed by sheer nerve .","bielinsky is a filmmaker of impressive talent .","so beautifully acted and directed , it's clear that washington most certainly has a new career ahead of him if he so chooses .","a visual spectacle full of stunning images and effects .","a gentle and engrossing character study .","it's enough to watch huppert scheming , with her small , intelligent eyes as steady as any noir villain , and to enjoy the perfectly pitched web of tension that chabrol spins .","an engrossing portrait of uncompromising artists trying to create something original against the backdrop of a corporate music industry that only seems to care about the bottom line .",...,"ultimately , jane learns her place as a girl , softens up and loses some of the intensity that made her an interesting character to begin with .","ah-nuld's action hero days might be over .","it's clear why deuces wild , which was shot two years ago , has been gathering dust on mgm's shelf .","feels like nothing quite so much as a middle-aged moviemaker's attempt to surround himself with beautiful , half-naked women .","when the precise nature of matthew's predicament finally comes into sharp focus , the revelation fails to justify the build-up .","this picture is murder by numbers , and as easy to be bored by as your abc's , despite a few whopping shootouts .","hilarious musical comedy though stymied by accents thick as mud .","if you are into splatter movies , then you will probably have a reasonably good time with the salton sea .","a dull , simple-minded and stereotypical tale of drugs , death and mind-numbing indifference on the inner-city streets .","the feature-length stretch . . . strains the show's concept ."]]
label: [[1,1,1,1,1,1,1,1,1,1,...,0,0,0,0,0,0,0,0,0,0]]
```

#### `cache_files`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1759)

```py
( )
```

包含支持数据集的 Apache Arrow 表的缓存文件。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.cache_files
[{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-validation.arrow'}]
```

#### `num_columns`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1777)

```py
( )
```

数据集中的列数。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.num_columns
2
```

#### `num_rows`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1792)

```py
( )
```

数据集中的行数（与 [Dataset.**len**()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.__len__) 相同）。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.num_rows
1066
```

#### `column_names`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1809)

```py
( )
```

数据集中的列名。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.column_names
['text', 'label']
```

#### `shape`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1824)

```py
( )
```

数据集的形状（列数、行数）。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.shape
(1066, 2)
```

#### `unique`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1841)

```py
( column: str ) → export const metadata = 'undefined';list
```

参数

+   `column` (`str`) — 列名（使用 [column_names](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.column_names) 列出所有列名）。

返回

`list`

给定列中唯一元素的列表。

返回列中唯一元素的列表。

这在底层后端中实现，因此非常快。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.unique('label')
[1, 0]
```

#### `flatten`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1947)

```py
( new_fingerprint: Optional = None max_depth = 16 ) → export const metadata = 'undefined';Dataset
```

参数

+   `new_fingerprint` (`str`, *可选*) — 转换后数据集的新指纹。如果为 `None`，则使用先前指纹和转换参数的哈希值计算新指纹。

返回

[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)

具有展平列的数据集的副本。

展平表格。具有结构类型的每列都展平为每个结构字段的一列。其他列保持不变。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("squad", split="train")
>>> ds.features
{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),
 'context': Value(dtype='string', id=None),
 'id': Value(dtype='string', id=None),
 'question': Value(dtype='string', id=None),
 'title': Value(dtype='string', id=None)}
>>> ds.flatten()
Dataset({
    features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],
    num_rows: 87599
})
```

#### `cast`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1992)

```py
( features: Features batch_size: Optional = 1000 keep_in_memory: bool = False load_from_cache_file: Optional = None cache_file_name: Optional = None writer_batch_size: Optional = 1000 num_proc: Optional = None ) → export const metadata = 'undefined';Dataset
```

参数

+   `features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)) — 将数据集转换为的新特征。特征中字段的名称必须与当前列名匹配。数据的类型也必须可以从一种类型转换为另一种类型。对于非平凡的转换，例如 `str` <-> `ClassLabel`，您应该使用 [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map) 来更新数据集。

+   `batch_size` (`int`, 默认为 `1000`) — 提供给转换的每个批次的示例数。如果 `batch_size <= 0` 或 `batch_size == None`，则将整个数据集作为一个批次提供给转换。

+   `keep_in_memory` (`bool`, 默认为 `False`) — 是否将数据复制到内存中。

+   `load_from_cache_file` (`bool`, 默认为 `True` 如果启用缓存) — 如果可以识别到存储当前计算结果的缓存文件，就使用它而不是重新计算。

+   `cache_file_name` (`str`, *可选*, 默认为 `None`) — 提供缓存文件的路径名称。用于存储计算结果而不是自动生成的缓存文件名称。

+   `writer_batch_size` (`int`, 默认为 `1000`) — 每个缓存文件写操作的行数。这个值在处理过程中内存使用和处理速度之间取得了很好的平衡。较高的值使处理过程中查找次数减少，较低的值在运行时消耗较少的临时内存 [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)。

+   `num_proc` (`int`, *可选*, 默认为 `None`) — 用于多进程的进程数。默认情况下不使用多进程。

返回

[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)

具有转换特征的数据集的副本。

将数据集转换为一组新特征。

示例：

```py
>>> from datasets import load_dataset, ClassLabel, Value
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.features
{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),
 'text': Value(dtype='string', id=None)}
>>> new_features = ds.features.copy()
>>> new_features['label'] = ClassLabel(names=['bad', 'good'])
>>> new_features['text'] = Value('large_string')
>>> ds = ds.cast(new_features)
>>> ds.features
{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),
 'text': Value(dtype='large_string', id=None)}
```

#### `cast_column`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2075)

```py
( column: str feature: Union new_fingerprint: Optional = None )
```

参数

+   `column` (`str`) — 列名。

+   `feature` (`FeatureType`) — 目标特征。

+   `new_fingerprint` (`str`, *可选*) — 转换后数据集的新指纹。如果为 `None`，则使用先前指纹和转换参数的哈希值计算新指纹。

将列转换为特征以进行解码。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.features
{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),
 'text': Value(dtype='string', id=None)}
>>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))
>>> ds.features
{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),
 'text': Value(dtype='string', id=None)}
```

#### `remove_columns`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2117)

```py
( column_names: Union new_fingerprint: Optional = None ) → export const metadata = 'undefined';Dataset
```

参数

+   `column_names` (`Union[str, List[str]]`) — 要移除的列的名称。

+   `new_fingerprint` (`str`, *可选*) — 转换后数据集的新指纹。如果为 `None`，则使用先前指纹和转换参数的哈希值计算新指纹。

返回

[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)

具有要移除的列的数据集对象的副本。

删除数据集中的一个或多个列以及与它们关联的特征。

您也可以使用[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)和`remove_columns`来删除列，但是当前方法是原地操作（不会将数据复制到新数据集），因此更快。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.remove_columns('label')
Dataset({
    features: ['text'],
    num_rows: 1066
})
>>> ds.remove_columns(column_names=ds.column_names) # Removing all the columns returns an empty dataset with the `num_rows` property set to 0
Dataset({
    features: [],
    num_rows: 0
})
```

#### `rename_column`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2173)

```py
( original_column_name: str new_column_name: str new_fingerprint: Optional = None ) → export const metadata = 'undefined';Dataset
```

参数

+   `original_column_name` (`str`) — 要重命名的列的名称。

+   `new_column_name` (`str`) — 列的新名称。

+   `new_fingerprint` (`str`, *可选*) — 转换后数据集的新指纹。如果为`None`，则使用先前指纹和转换参数的哈希计算新指纹。

返回

[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)

具有重命名列的数据集的副本。

重命名数据集中的列，并将与原始列关联的特征移动到新列名下。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.rename_column('label', 'label_new')
Dataset({
    features: ['text', 'label_new'],
    num_rows: 1066
})
```

#### `rename_columns`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2240)

```py
( column_mapping: Dict new_fingerprint: Optional = None ) → export const metadata = 'undefined';Dataset
```

参数

+   `column_mapping` (`Dict[str, str]`) — 要重命名为其新名称的列的映射

+   `new_fingerprint` (`str`, *可选*) — 转换后数据集的新指纹。如果为`None`，则使用先前指纹和转换参数的哈希计算新指纹。

返回

[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)

具有重命名列的数据集的副本

重命名数据集中的多个列，并将与原始列关联的特征移动到新列名下。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.rename_columns({'text': 'text_new', 'label': 'label_new'})
Dataset({
    features: ['text_new', 'label_new'],
    num_rows: 1066
})
```

#### `select_columns`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2308)

```py
( column_names: Union new_fingerprint: Optional = None ) → export const metadata = 'undefined';Dataset
```

参数

+   `column_names` (`Union[str, List[str]]`) — 要保留的列的名称。

+   `new_fingerprint` (`str`, *可选*) — 转换后数据集的新指纹。如果为`None`，则使用先前指纹和转换参数的哈希计算新指纹。

返回

[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)

数据集对象的副本，仅包含选定的列。

选择数据集中的一个或多个列以及与它们关联的特征。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.select_columns(['text'])
Dataset({
    features: ['text'],
    num_rows: 1066
})
```

#### `class_encode_column`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1872)

```py
( column: str include_nulls: bool = False )
```

参数

+   `column` (`str`) — 要转换的列的名称（使用[column_names](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.column_names)列出所有列名）

+   `include_nulls` (`bool`, 默认为 `False`) — 是否在类标签中包含空值。如果为`True`，则空值将被编码为`"None"`类标签。

    添加于1.14.2

将给定的列转换为[ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)并更新表格。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("boolq", split="validation")
>>> ds.features
{'answer': Value(dtype='bool', id=None),
 'passage': Value(dtype='string', id=None),
 'question': Value(dtype='string', id=None)}
>>> ds = ds.class_encode_column('answer')
>>> ds.features
{'answer': ClassLabel(num_classes=2, names=['False', 'True'], id=None),
 'passage': Value(dtype='string', id=None),
 'question': Value(dtype='string', id=None)}
```

#### `__len__`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2357)

```py
( )
```

数据集中的行数。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.__len__
<bound method Dataset.__len__ of Dataset({
    features: ['text', 'label'],
    num_rows: 1066
})>
```

#### `__iter__`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2374)

```py
( )
```

遍历示例。

如果使用[Dataset.set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format)设置了格式，则将以所选格式返回行。

#### `iter`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2403)

```py
( batch_size: int drop_last_batch: bool = False )
```

参数

+   `batch_size` (`int`) — 每个批次的大小。

+   `drop_last_batch` (`bool`, 默认 *False*) — 是否应删除小于batch_size的最后一个批次

遍历大小为*batch_size*的批次。

如果使用[*~datasets.Dataset.set_format*]设置了格式，则将以所选格式返回行。

#### `formatted_as`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2447)

```py
( type: Optional = None columns: Optional = None output_all_columns: bool = False **format_kwargs )
```

参数

+   `type`（`str`，*可选*）— 在`[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`中选择的输出类型。`None`表示`**getitem**`返回Python对象（默认）。

+   `columns`（`List[str]`，*可选*）— 要在输出中格式化的列。`None`表示`__getitem__`返回所有列（默认）。

+   `output_all_columns`（`bool`，默认为`False`）— 在输出中也保留未格式化的列（作为Python对象）。

+   *`*format_kwargs`（额外的关键字参数）— 传递给转换函数（如`np.array`、`torch.tensor`或`tensorflow.ragged.constant`）的关键字参数。

用于`with`语句。设置`__getitem__`返回格式（类型和列）。

#### `set_format`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2479)

```py
( type: Optional = None columns: Optional = None output_all_columns: bool = False **format_kwargs )
```

参数

+   `type`（`str`，*可选*）— 在`[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`中选择的输出类型。`None`表示`__getitem__`返回Python对象（默认）。

+   `columns`（`List[str]`，*可选*）— 要在输出中格式化的列。`None`表示`__getitem__`返回所有列（默认）。

+   `output_all_columns`（`bool`，默认为`False`）— 在输出中也保留未格式化的列（作为Python对象）。

+   *`*format_kwargs`（额外的关键字参数）— 传递给转换函数（如`np.array`、`torch.tensor`或`tensorflow.ragged.constant`）的关键字参数。

设置`__getitem__`返回格式（类型和列）。数据格式化是即时应用的。当使用`__getitem__`时，格式`type`（例如“numpy”）用于格式化批次。还可以使用自定义转换来进行格式化，使用[set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform)。

在调用`set_format`后，可以调用[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)。由于`map`可能会添加新列，因此格式化列的列表

更新。在这种情况下，如果您在数据集上应用`map`以添加新列，则此列将被格式化为：

```py
new formatted columns = (all columns - previously unformatted columns)
```

示例：

```py
>>> from datasets import load_dataset
>>> from transformers import AutoTokenizer
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
>>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)
>>> ds.set_format(type='numpy', columns=['text', 'label'])
>>> ds.format
{'type': 'numpy',
'format_kwargs': {},
'columns': ['text', 'label'],
'output_all_columns': False}
```

#### `set_transform`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2587)

```py
( transform: Optional columns: Optional = None output_all_columns: bool = False )
```

参数

+   `transform`（`Callable`，*可选*）— 用户定义的格式转换，替换由[set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format)定义的格式。格式化函数是一个可调用对象，它以批处理（作为`dict`）作为输入并返回批处理。此函数在返回`__getitem__`中的对象之前应用。

+   `columns`（`List[str]`，*可选*）— 要在输出中格式化的列。如果指定，则转换的输入批次仅包含这些列。

+   `output_all_columns`（`bool`，默认为`False`）— 在输出中也保留未格式化的列（作为Python对象）。如果设置为True，则其他未格式化的列将与转换的输出一起保留。

使用此转换设置`__getitem__`返回格式。当调用`__getitem__`时，转换将即时应用于批次。与[set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format)一样，可以使用[reset_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.reset_format)重置。

示例：

```py
>>> from datasets import load_dataset
>>> from transformers import AutoTokenizer
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
>>> def encode(batch):
...     return tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt')
>>> ds.set_transform(encode)
>>> ds[0]
{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
 1, 1]),
 'input_ids': tensor([  101, 29353,  2135, 15102,  1996,  9428, 20868,  2890,  8663,  6895,
         20470,  2571,  3663,  2090,  4603,  3017,  3008,  1998,  2037, 24211,
         5637,  1998, 11690,  2336,  1012,   102]),
 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0])}
```

#### `reset_format`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2558)

```py
( )
```

将`__getitem__`返回格式重置为Python对象和所有列。

与`self.set_format()`相同

示例：

```py
>>> from datasets import load_dataset
>>> from transformers import AutoTokenizer
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
>>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)
>>> ds.set_format(type='numpy', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])
>>> ds.format
{'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],
 'format_kwargs': {},
 'output_all_columns': False,
 'type': 'numpy'}
>>> ds.reset_format()
>>> ds.format
{'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],
 'format_kwargs': {},
 'output_all_columns': False,
 'type': None}
```

#### `with_format`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2630)

```py
( type: Optional = None columns: Optional = None output_all_columns: bool = False **format_kwargs )
```

参数

+   `type` (`str`, *可选*) — 选择在 `[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']` 中的输出类型。`None` 表示 `__getitem__` 返回python对象（默认）。

+   `columns` (`List[str]`, *可选*) — 要在输出中格式化的列。`None` 表示 `__getitem__` 返回所有列（默认）。

+   `output_all_columns` (`bool`, 默认为 `False`) — 在输出中保留未格式化的列（作为python对象）。

+   *`*format_kwargs`（额外的关键字参数） — 传递给转换函数的关键字参数，如 `np.array`、`torch.tensor` 或 `tensorflow.ragged.constant`。

设置 `__getitem__` 返回格式（类型和列）。数据格式化是实时应用的。当使用 `__getitem__` 时，格式 `type`（例如“numpy”）用于格式化批处理。

还可以使用自定义转换来进行格式化，使用 [with_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.with_transform)。

与 [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format) 不同，`with_format` 返回一个新的 [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset) 对象。

示例：

```py
>>> from datasets import load_dataset
>>> from transformers import AutoTokenizer
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
>>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)
>>> ds.format
{'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],
 'format_kwargs': {},
 'output_all_columns': False,
 'type': None}
>>> ds = ds.with_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])
>>> ds.format
{'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],
 'format_kwargs': {},
 'output_all_columns': False,
 'type': 'tensorflow'}
```

#### `with_transform`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2681)

```py
( transform: Optional columns: Optional = None output_all_columns: bool = False )
```

参数

+   `transform` (`Callable`, `可选`) — 用户定义的格式化转换，替换 [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format) 定义的格式。格式化函数是一个可调用对象，接受批处理（作为 `dict`）作为输入并返回一个批处理。此函数在返回 `__getitem__` 中的对象之前应用。

+   `columns` (`List[str]`, `可选`) — 要在输出中格式化的列。如果指定，则转换的输入批处理仅包含这些列。

+   `output_all_columns` (`bool`, 默认为 `False`) — 在输出中保留未格式化的列（作为python对象）。如果设置为 `True`，则其他未格式化的列将与转换的输出一起保留。

使用此转换设置 `__getitem__` 返回格式。当调用 `__getitem__` 时，转换会在批处理时实时应用。

与 [set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format) 不同，可以使用 [reset_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.reset_format) 进行重置。

与 [set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform) 不同，`with_transform` 返回一个新的 [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset) 对象。

示例：

```py
>>> from datasets import load_dataset
>>> from transformers import AutoTokenizer
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
>>> def encode(example):
...     return tokenizer(example["text"], padding=True, truncation=True, return_tensors='pt')
>>> ds = ds.with_transform(encode)
>>> ds[0]
{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
 1, 1, 1, 1, 1]),
 'input_ids': tensor([  101, 18027, 16310, 16001,  1103,  9321,   178, 11604,  7235,  6617,
         1742,  2165,  2820,  1206,  6588, 22572, 12937,  1811,  2153,  1105,
         1147, 12890, 19587,  6463,  1105, 15026,  1482,   119,   102]),
 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0])}
```

#### `__getitem__`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2808)

```py
( key )
```

可用于按字符串名称索引列或按整数索引或索引的可迭代对象或布尔值索引行。

#### `cleanup_cache_files`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2818)

```py
( ) → export const metadata = 'undefined';int
```

返回

`int`

删除的文件数量。

清理数据集缓存目录中的所有缓存文件，除非当前正在使用的缓存文件。

运行此命令时请注意，当前没有其他进程正在使用其他缓存文件。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.cleanup_cache_files()
10
```

#### `map`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2865)

```py
( function: Optional = None with_indices: bool = False with_rank: bool = False input_columns: Union = None batched: bool = False batch_size: Optional = 1000 drop_last_batch: bool = False remove_columns: Union = None keep_in_memory: bool = False load_from_cache_file: Optional = None cache_file_name: Optional = None writer_batch_size: Optional = 1000 features: Optional = None disable_nullable: bool = False fn_kwargs: Optional = None num_proc: Optional = None suffix_template: str = '_{rank:05d}_of_{num_proc:05d}' new_fingerprint: Optional = None desc: Optional = None )
```

参数

+   `function` (`Callable`) — 具有以下签名之一的函数：

    +   `function(example: Dict[str, Any]) -> Dict[str, Any]` 如果 `batched=False` 且 `with_indices=False` 且 `with_rank=False`

    +   `function(example: Dict[str, Any], *extra_args) -> Dict[str, Any]` 如果 `batched=False` 且 `with_indices=True` 和/或 `with_rank=True`（每个额外参数一个）

    +   `function(batch: Dict[str, List]) -> Dict[str, List]` 如果 `batched=True` 且 `with_indices=False` 且 `with_rank=False`

    +   `function(batch: Dict[str, List], *extra_args) -> Dict[str, List]` 如果 `batched=True` 并且 `with_indices=True` 和/或 `with_rank=True`（每个额外参数一个）

    对于高级用法，函数还可以返回 `pyarrow.Table`。此外，如果您的函数返回空（`None`），则 `map` 将运行您的函数并返回数据集。如果未提供函数，则默认为恒等函数：`lambda x: x`。

+   `with_indices` (`bool`, 默认为 `False`) — 向 `function` 提供示例索引。请注意，在这种情况下，`function` 的签名应为 `def function(example, idx[, rank]): ...`。

+   `with_rank` (`bool`, 默认为 `False`) — 向 `function` 提供进程排名。请注意，在这种情况下，`function` 的签名应为 `def function(example[, idx], rank): ...`。

+   `input_columns` (`Optional[Union[str, List[str]]]`, 默认为 `None`) — 要作为位置参数传递给 `function` 的列。如果为 `None`，则传递一个映射到所有格式化列的 `dict` 作为一个参数。

+   `batched` (`bool`, 默认为 `False`) — 向 `function` 提供示例的批次。

+   `batch_size` (`int`, *optional*, 默认为 `1000`) — 如果 `batched=True`，则提供给 `function` 的每个批次的示例数。如果 `batch_size <= 0` 或 `batch_size == None`，则将整个数据集作为单个批次提供给 `function`。

+   `drop_last_batch` (`bool`, 默认为 `False`) — 是否应删除小于 `batch_size` 的最后一个批次，而不是由函数处理。

+   `remove_columns` (`Optional[Union[str, List[str]]]`, 默认为 `None`) — 在执行映射时删除一些列。在使用 `function` 的输出更新示例之前将删除列，即如果 `function` 添加具有 `remove_columns` 中名称的列，则这些列将被保留。

+   `keep_in_memory` (`bool`, 默认为 `False`) — 将数据集保留在内存中，而不是将其写入缓存文件。

+   `load_from_cache_file` (`Optional[bool]`, 默认为 `True` 如果启用了缓存) — 如果可以识别存储来自 `function` 的当前计算的缓存文件，则使用它而不是重新计算。

+   `cache_file_name` (`str`, *optional*, 默认为 `None`) — 提供缓存文件的路径名称。用于存储计算结果，而不是自动生成的缓存文件名称。

+   `writer_batch_size` (`int`, 默认为 `1000`) — 缓存文件写入器的每次写入操作的行数。此值在处理期间的内存使用和处理速度之间取得良好的折衷。较高的值使处理执行更少的查找，较低的值在运行 `map` 时消耗更少的临时内存。

+   `features` (`Optional[datasets.Features]`, 默认为 `None`) — 使用特定的 Features 来存储缓存文件，而不是自动生成的文件。

+   `disable_nullable` (`bool`, 默认为 `False`) — 在表中禁止空值。

+   `fn_kwargs` (`Dict`, *optional*, 默认为 `None`) — 要传递给 `function` 的关键字参数。

+   `num_proc` (`int`, *optional*, 默认为 `None`) — 生成缓存时的最大进程数。已缓存的分片将按顺序加载。

+   `suffix_template` (`str`) — 如果指定了 `cache_file_name`，则此后缀将添加到每个基本名称的末尾。默认为 `"_{rank:05d}_of_{num_proc:05d}"`。例如，如果 `cache_file_name` 是“processed.arrow”，那么对于 `rank=1` 和 `num_proc=4`，生成的文件将是 `"processed_00001_of_00004.arrow"`。

+   `new_fingerprint` (`str`, *optional*, 默认为 `None`) — 转换后数据集的新指纹。如果为 `None`，则使用先前指纹的哈希和转换参数计算新指纹。

+   `desc` (`str`, *optional*, 默认为 `None`) — 映射示例时显示在进度条旁边的有意义的描述。

对表中的所有示例（单独或批量）应用函数并更新表。如果您的函数返回一个已经存在的列，则会覆盖它。

您可以使用 `batched` 参数指定函数是否应该批量处理：

+   如果 `batched` 为 `False`，则函数接收一个示例并应返回一个示例。示例是一个字典，例如 `{"text": "Hello there !"}`。

+   如果 `batched=True`，且 `batch_size` 为 1，那么函数接收一个包含 1 个示例的批次作为输入，并可以返回一个包含 1 个或多个示例的批次。一个批次是一个字典，例如，1 个示例的批次是 `{"text": ["Hello there !"]}`。

+   如果 `batched=True`，且 `batch_size` 为 `n > 1`，则函数接收一个包含 `n` 个示例的批次作为输入，并可以返回一个包含 `n` 个示例或任意数量示例的批次。请注意，最后一个批次可能少于 `n` 个示例。一个批次是一个字典，例如，`n` 个示例的批次是 `{"text": ["Hello there !"] * n}`。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> def add_prefix(example):
...     example["text"] = "Review: " + example["text"]
...     return example
>>> ds = ds.map(add_prefix)
>>> ds[0:3]["text"]
['Review: compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .',
 'Review: the soundtrack alone is worth the price of admission .',
 'Review: rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .']

# process a batch of examples
>>> ds = ds.map(lambda example: tokenizer(example["text"]), batched=True)
# set number of processors
>>> ds = ds.map(add_prefix, num_proc=4)
```

#### `filter`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L3540)

```py
( function: Optional = None with_indices: bool = False with_rank: bool = False input_columns: Union = None batched: bool = False batch_size: Optional = 1000 keep_in_memory: bool = False load_from_cache_file: Optional = None cache_file_name: Optional = None writer_batch_size: Optional = 1000 fn_kwargs: Optional = None num_proc: Optional = None suffix_template: str = '_{rank:05d}_of_{num_proc:05d}' new_fingerprint: Optional = None desc: Optional = None )
```

参数

+   `function` (`Callable`) — 具有以下签名之一的可调用函数：

    +   如果 `batched=False`，`with_indices=False`，`with_rank=False`，则 `function(example: Dict[str, Any]) -> bool`

    +   如果 `batched=False`，`with_indices=True`，和/或 `with_rank=True`，则 `function(example: Dict[str, Any], *extra_args) -> bool`（每个额外参数一个）

    +   如果 `batched=True`，`with_indices=False`，`with_rank=False`，则 `function(batch: Dict[str, List]) -> List[bool]`

    +   如果 `batched=True`，`with_indices=True`，和/或 `with_rank=True`，则 `function(batch: Dict[str, List], *extra_args) -> List[bool]`（每个额外参数一个）

    如果未提供函数，则默认为始终为 `True` 的函数：`lambda x: True`。

+   `with_indices` (`bool`, 默认为 `False`) — 向 `function` 提供示例索引。请注意，在这种情况下，`function` 的签名应为 `def function(example, idx[, rank]): ...`。

+   `with_rank` (`bool`, 默认为 `False`) — 向 `function` 提供进程排名。请注意，在这种情况下，`function` 的签名应为 `def function(example[, idx], rank): ...`。

+   `input_columns` (`str` 或 `List[str]`, *可选*) — 要作为位置参数传递给 `function` 的列。如果为 `None`，则传递一个映射到所有格式化列的 `dict` 作为一个参数。

+   `batched` (`bool`, 默认为 `False`) — 向 `function` 提供示例的批次。

+   `batch_size` (`int`, *可选*, 默认为 `1000`) — 如果 `batched = True`，则每批次提供给 `function` 的示例数量。如果 `batched = False`，则每批次传递给 `function` 一个示例。如果 `batch_size <= 0` 或 `batch_size == None`，则将整个数据集作为单个批次提供给 `function`。

+   `keep_in_memory` (`bool`, 默认为 `False`) — 将数据集保留在内存中，而不是将其写入缓存文件。

+   `load_from_cache_file` (`Optional[bool]`, 默认为 `True`，如果启用缓存) — 如果可以识别存储从 `function` 计算得到的当前结果的缓存文件，则使用它而不是重新计算。

+   `cache_file_name` (`str`, *可选*) — 提供缓存文件的路径名称。它用于存储计算结果，而不是自动生成的缓存文件名称。

+   `writer_batch_size` (`int`, 默认为 `1000`) — 缓存文件写入操作的每行数量。这个值在处理过程中内存使用和处理速度之间取得了很好的平衡。较高的值使处理过程中查找次数减少，较低的值在运行 `map` 时消耗更少的临时内存。

+   `fn_kwargs` (`dict`, *可选*) — 要传递给 `function` 的关键字参数。

+   `num_proc` (`int`, *可选*) — 多进程的进程数。默认情况下不使用多进程。

+   `suffix_template` (`str`) — 如果指定了 `cache_file_name`，则此后缀将添加到每个基本名称的末尾。例如，如果 `cache_file_name` 为 `"processed.arrow"`，那么对于 `rank = 1` 和 `num_proc = 4`，生成的文件将是 `"processed_00001_of_00004.arrow"`，对于默认后缀（默认为 `_{rank:05d}_of_{num_proc:05d}`）。

+   `new_fingerprint` (`str`, *optional*) — 转换后数据集的新指纹。如果为 `None`，则使用先前指纹的哈希和转换参数计算新指纹。

+   `desc` (`str`, *optional*, 默认为 `None`) — 在过滤示例时显示在进度条旁边的有意义的描述。

对表中的所有元素应用过滤函数，并更新表，使数据集仅包含根据过滤函数选择的示例。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.filter(lambda x: x["label"] == 1)
Dataset({
    features: ['text', 'label'],
    num_rows: 533
})
```

#### `select`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L3751)

```py
( indices: Iterable keep_in_memory: bool = False indices_cache_file_name: Optional = None writer_batch_size: Optional = 1000 new_fingerprint: Optional = None )
```

参数

+   `indices` (`range`, `list`, `iterable`, `ndarray` 或 `Series`) — 用于索引的整数索引的范围、列表或 1D 数组。如果索引对应于连续范围，则 Arrow 表将简单地被切片。然而，传递一个不连续的索引列表会创建索引映射，这样做效率要低得多，但仍然比重新创建由请求的行组成的 Arrow 表要快。

+   `keep_in_memory` (`bool`, 默认为 `False`) — 将索引映射保留在内存中，而不是写入缓存文件。

+   `indices_cache_file_name` (`str`, *optional*, 默认为 `None`) — 提供缓存文件的路径名称。它用于存储索引映射，而不是自动生成的缓存文件名称。

+   `writer_batch_size` (`int`, 默认为 `1000`) — 每次写入操作的行数，用于缓存文件写入器。这个值在处理过程中内存使用和处理速度之间取得了很好的平衡。较高的值使处理过程中查找次数减少，较低的值在运行 `map` 时消耗更少的临时内存。

+   `new_fingerprint` (`str`, *optional*, 默认为 `None`) — 转换后数据集的新指纹。如果为 `None`，则使用先前指纹的哈希和转换参数计算新指纹。

使用选择的列表/数组中的索引创建新数据集。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds.select(range(4))
Dataset({
    features: ['text', 'label'],
    num_rows: 4
})
```

#### `sort`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4001)

```py
( column_names: Union reverse: Union = False kind = 'deprecated' null_placement: str = 'at_end' keep_in_memory: bool = False load_from_cache_file: Optional = None indices_cache_file_name: Optional = None writer_batch_size: Optional = 1000 new_fingerprint: Optional = None )
```

参数

+   `column_names` (`Union[str, Sequence[str]]`) — 要排序的列名。

+   `reverse` (`Union[bool, Sequence[bool]]`, 默认为 `False`) — 如果为 `True`，则按降序排序而不是升序。如果提供单个布尔值，则该值将应用于所有列名的排序。否则，必须提供与 `column_names` 长度和顺序相同的布尔值列表。

+   `kind` (`str`, *optional*) — 选择排序的 Pandas 算法，可选值为 `{quicksort, mergesort, heapsort, stable}`，默认为 `quicksort`。请注意，`stable` 和 `mergesort` 都在内部使用 `timsort`，通常实际实现会随数据类型而变化。`mergesort` 选项保留了向后兼容性。

    在 2.8.0 版中弃用

    `kind` 在 2.10.0 版中已弃用，并将在 3.0.0 版中移除。

+   `null_placement` (`str`, 默认为 `at_end`) — 如果为 `at_start`、`first`，则将 `None` 值放在开头；如果为 `at_end`、`last`，则将 `None` 值放在末尾。

    在 1.14.2 版中添加

+   `keep_in_memory` (`bool`, 默认为 `False`) — 将排序后的索引保留在内存中，而不是写入缓存文件。

+   `load_from_cache_file` (`Optional[bool]`, 默认为 `True`，如果启用缓存，则使用存储排序后的索引的缓存文件，而不是重新计算。

+   `indices_cache_file_name` (`str`, *optional*, 默认为 `None`) — 提供缓存文件的路径名称。它用于存储排序后的索引，而不是自动生成的缓存文件名称。

+   `writer_batch_size` (`int`, 默认为`1000`) — 缓存文件写入操作的每行行数。较高的值会产生较小的缓存文件，较低的值会消耗更少的临时内存。

+   `new_fingerprint` (`str`, *可选*, 默认为`None`) — 变换后数据集的新指纹。如果为`None`，则使用先前指纹和变换参数的哈希值计算新指纹。

创建一个根据单个或多个列排序的新数据集。

示例:

```py
>>> from datasets import load_dataset
>>> ds = load_dataset('rotten_tomatoes', split='validation')
>>> ds['label'][:10]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
>>> sorted_ds = ds.sort('label')
>>> sorted_ds['label'][:10]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
>>> another_sorted_ds = ds.sort(['label', 'text'], reverse=[True, False])
>>> another_sorted_ds['label'][:10]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

#### `shuffle`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4146)

```py
( seed: Optional = None generator: Optional = None keep_in_memory: bool = False load_from_cache_file: Optional = None indices_cache_file_name: Optional = None writer_batch_size: Optional = 1000 new_fingerprint: Optional = None )
```

参数

+   `seed` (`int`, *可选*) — 如果`generator=None`，则用于初始化默认BitGenerator的种子。如果为`None`，则将从操作系统中提取新鲜、不可预测的熵。如果传递了一个`int`或`array_like[ints]`，则将传递给SeedSequence以推导初始BitGenerator状态。

+   `generator` (`numpy.random.Generator`, *可选*) — 用于计算数据集行的排列的Numpy随机生成器。如果`generator=None`（默认），则使用`np.random.default_rng`（NumPy的默认BitGenerator（PCG64））。

+   `keep_in_memory` (`bool`, 默认为`False`) — 将洗牌后的索引保留在内存中，而不是写入缓存文件。

+   `load_from_cache_file` (`Optional[bool]`, 默认为`True`，如果启用了缓存，则为`True`) — 如果可以识别出存储洗牌后的索引的缓存文件，则使用它而不是重新计算。

+   `indices_cache_file_name` (`str`, *可选*) — 提供缓存文件路径的名称。用于存储洗牌后的索引，而不是自动生成的缓存文件名称。

+   `writer_batch_size` (`int`, 默认为`1000`) — 缓存文件写入操作的每行行数。这个值在处理过程中的内存使用和处理速度之间取得了很好的平衡。较高的值使处理过程减少查找次数，较低的值在运行`map`时消耗更少的临时内存。

+   `new_fingerprint` (`str`, *可选*, 默认为`None`) — 变换后数据集的新指纹。如果为`None`，则使用先前指纹和变换参数的哈希值计算新指纹。

创建一个新的数据集，其中行被洗牌。

当前洗牌使用了numpy随机生成器。您可以提供一个NumPy BitGenerator来使用，或者提供一个种子来初始化NumPy的默认随机生成器（PCG64）。

洗牌将索引列表`[0:len(my_dataset)]`进行洗牌，以创建一个索引映射。但是一旦您的[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)有了索引映射，速度可能会变慢10倍。这是因为有一个额外的步骤来使用索引映射获取要读取的行索引，最重要的是，您不再读取连续的数据块。要恢复速度，您需要再次使用[Dataset.flatten_indices()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.flatten_indices)在您的磁盘上重新写入整个数据集，这将删除索引映射。

这可能会根据数据集的大小而花费很长时间:

```py
my_dataset[0]  # fast
my_dataset = my_dataset.shuffle(seed=42)
my_dataset[0]  # up to 10x slower
my_dataset = my_dataset.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data
my_dataset[0]  # fast again
```

在这种情况下，我们建议切换到[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)，并利用其快速近似洗牌方法[IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle)。

它只对碎片顺序进行洗牌，并向数据集添加了一个洗牌缓冲区，保持数据集的速度最优:

```py
my_iterable_dataset = my_dataset.to_iterable_dataset(num_shards=128)
for example in enumerate(my_iterable_dataset):  # fast
    pass

shuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)

for example in enumerate(shuffled_iterable_dataset):  # as fast as before
    pass
```

示例:

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds['label'][:10]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]

# set a seed
>>> shuffled_ds = ds.shuffle(seed=42)
>>> shuffled_ds['label'][:10]
[1, 0, 1, 1, 0, 0, 0, 0, 0, 0]
```

#### `train_test_split`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4278)

```py
( test_size: Union = None train_size: Union = None shuffle: bool = True stratify_by_column: Optional = None seed: Optional = None generator: Optional = None keep_in_memory: bool = False load_from_cache_file: Optional = None train_indices_cache_file_name: Optional = None test_indices_cache_file_name: Optional = None writer_batch_size: Optional = 1000 train_new_fingerprint: Optional = None test_new_fingerprint: Optional = None )
```

参数

+   `test_size` (`numpy.random.Generator`, *可选*) — 测试集大小。如果是 `float`，应该在 `0.0` 和 `1.0` 之间，表示要包含在测试集中的数据集比例。如果是 `int`，表示测试样本的绝对数量。如果是 `None`，则该值设置为训练集大小的补集。如果 `train_size` 也是 `None`，则设置为 `0.25`。

+   `train_size` (`numpy.random.Generator`, *可选*) — 训练集大小。如果是 `float`，应该在 `0.0` 和 `1.0` 之间，表示要包含在训练集中的数据集比例。如果是 `int`，表示训练样本的绝对数量。如果是 `None`，则该值会自动设置为测试集大小的补集。

+   `shuffle` (`bool`, *可选*, 默认为 `True`) — 在拆分之前是否对数据进行洗牌。

+   `stratify_by_column` (`str`, *可选*, 默认为 `None`) — 用于执行数据分层拆分的标签列的列名。

+   `seed` (`int`, *可选*) — 用于初始化默认 BitGenerator 的种子，如果 `generator=None`。如果是 `None`，则会从操作系统中获取新鲜、不可预测的熵。如果传递了一个 `int` 或 `array_like[ints]`，则会传递给 SeedSequence 以派生初始 BitGenerator 状态。

+   `generator` (`numpy.random.Generator`, *可选*) — 用于计算数据集行的排列的 Numpy 随机生成器。如果 `generator=None`（默认），则使用 `np.random.default_rng`（NumPy 的默认 BitGenerator（PCG64））。

+   `keep_in_memory` (`bool`, 默认为 `False`) — 将拆分的索引保留在内存中，而不是写入缓存文件。

+   `load_from_cache_file` (`Optional[bool]`, 如果启用缓存，默认为 `True`) — 如果可以识别到存储拆分索引的缓存文件，则使用它而不是重新计算。

+   `train_cache_file_name` (`str`, *可选*) — 提供缓存文件的路径名称。用于存储训练集索引，而不是自动生成的缓存文件名称。

+   `test_cache_file_name` (`str`, *可选*) — 提供缓存文件的路径名称。用于存储测试集索引，而不是自动生成的缓存文件名称。

+   `writer_batch_size` (`int`, 默认为 `1000`) — 缓存文件写入操作的每行数量。这个值在处理过程中内存使用和处理速度之间取得了很好的平衡。较高的值使处理过程减少查找次数，较低的值在运行 `map` 时消耗更少的临时内存。

+   `train_new_fingerprint` (`str`, *可选*, 默认为 `None`) — 变换后训练集的新指纹。如果是 `None`，则使用先前指纹的哈希和变换参数计算新指纹。

+   `test_new_fingerprint` (`str`, *可选*, 默认为 `None`) — 变换后测试集的新指纹。如果是 `None`，则使用先前指纹的哈希和变换参数计算新指纹。

返回一个字典（[datasets.DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)），其中包含两个随机的训练和测试子集（`train` 和 `test` `Dataset` 拆分）。根据 `test_size`、`train_size` 和 `shuffle` 从数据集创建拆分。

此方法类似于 scikit-learn 的 `train_test_split`。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds = ds.train_test_split(test_size=0.2, shuffle=True)
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 852
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 214
    })
})

# set a seed
>>> ds = ds.train_test_split(test_size=0.2, seed=42)

# stratified split
>>> ds = load_dataset("imdb",split="train")
Dataset({
    features: ['text', 'label'],
    num_rows: 25000
})
>>> ds = ds.train_test_split(test_size=0.2, stratify_by_column="label")
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 20000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 5000
    })
})
```

#### `shard`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4561)

```py
( num_shards: int index: int contiguous: bool = False keep_in_memory: bool = False indices_cache_file_name: Optional = None writer_batch_size: Optional = 1000 )
```

参数

+   `num_shards` (`int`) — 将数据集拆分为多少个分片。

+   `index` (`int`) — 选择并返回哪个分片。contiguous — (`bool`, 默认为 `False`): 是否选择分片的连续索引块。

+   `keep_in_memory` (`bool`, 默认为 `False`) — 将数据集保留在内存中，而不是写入缓存文件。

+   `indices_cache_file_name` (`str`, *可选*) — 提供缓存文件的路径名称。用于存储每个分片的索引，而不是自动生成的缓存文件名称。

+   `writer_batch_size` (`int`，默认为 `1000`) — 缓存文件写入操作的每行行数。这个值在处理期间内存使用和处理速度之间取得了良好的平衡。较高的值使处理执行更少的查找操作，较低的值在运行 `map` 时消耗更少的临时内存。

返回数据集分割为 `num_shards` 个部分后的第 `index`-个分片。

这是确定性的分片。`dset.shard(n, i)` 将包含数据集中索引模 `n = i` 的所有元素。

`dset.shard(n, i, contiguous=True)` 将会将数据集分割成连续的块，因此在处理后可以轻松地将其拼接在一起。如果 `n % i == l`，那么前 `l` 个块的长度将为 `(n // i) + 1`，剩余的块的长度将为 `(n // i)`。`datasets.concatenate([dset.shard(n, i, contiguous=True) for i in range(n)])` 将返回一个与原始数据集相同顺序的数据集。

在使用任何随机化操作（如 `shuffle`）之前，请务必进行分片。最好在数据集管道的早期使用分片操作符。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="validation")
>>> ds
Dataset({
    features: ['text', 'label'],
    num_rows: 1066
})
>>> ds.shard(num_shards=2, index=0)
Dataset({
    features: ['text', 'label'],
    num_rows: 533
})
```

#### `to_tf_dataset`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L327)

```py
( batch_size: Optional = None columns: Union = None shuffle: bool = False collate_fn: Optional = None drop_remainder: bool = False collate_fn_args: Optional = None label_cols: Union = None prefetch: bool = True num_workers: int = 0 num_test_batches: int = 20 )
```

参数

+   `batch_size` (`int`，*可选*) — 从数据集中加载的批次大小。默认为 `None`，这意味着数据集不会被分批，但返回的数据集可以稍后使用 `tf_dataset.batch(batch_size)` 进行分批。

+   `columns` (`List[str]` 或 `str`，*可选*) — 要在 `tf.data.Dataset` 中加载的数据集列。由 `collate_fn` 创建的列名，且原始数据集中不存在的列名可以使用。

+   `shuffle(bool,` 默认为 `False`) — 在加载时对数据集顺序进行洗牌。建议对训练使用 `True`，对验证/评估使用 `False`。

+   `drop_remainder(bool,` 默认为 `False`) — 在加载时丢弃最后不完整的批次。确保数据集产生的所有批次在批次维度上具有相同的长度。

+   `collate_fn(Callable,` *可选*) — 一个函数或可调用对象（如 `DataCollator`），用于将样本列表整理成一个批次。

+   `collate_fn_args` (`Dict`，*可选*) — 要传递给 `collate_fn` 的关键字参数的可选 `dict`。

+   `label_cols` (`List[str]` 或 `str`，默认为 `None`) — 要作为标签加载的数据集列。请注意，许多模型在内部计算损失而不是让 Keras 进行计算，因此在这里传递标签是可选的，只要它们在输入 `columns` 中即可。

+   `prefetch` (`bool`，默认为 `True`) — 是否在单独的线程中运行数据加载器，并保持一小批次的缓冲区用于训练。通过允许在模型训练时在后台加载数据来提高性能。

+   `num_workers` (`int`，默认为 `0`) — 用于加载数据集的工作线程数。仅支持 Python 版本 >= 3.8。

+   `num_test_batches` (`int`，默认为 `20`) — 用于推断数据集输出签名的批次数。这个数字越高，签名就越准确，但创建数据集所需的时间就越长。

从底层数据集创建一个 `tf.data.Dataset`。这个 `tf.data.Dataset` 将从数据集中加载和整理批次，并适合传递给 `model.fit()` 或 `model.predict()` 等方法。数据集将为输入和标签都产生 `dicts`，除非 `dict` 只包含一个键，此时将产生一个原始的 `tf.Tensor`。

示例：

```py
>>> ds_train = ds["train"].to_tf_dataset(
...    columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'],
...    shuffle=True,
...    batch_size=16,
...    collate_fn=data_collator,
... )
```

#### `push_to_hub`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5248)

```py
( repo_id: str config_name: str = 'default' set_default: Optional = None split: Optional = None data_dir: Optional = None commit_message: Optional = None commit_description: Optional = None private: Optional = False token: Optional = None revision: Optional = None branch = 'deprecated' create_pr: Optional = False max_shard_size: Union = None num_shards: Optional = None embed_external_files: bool = True )
```

参数

+   `repo_id` (`str`) — 要推送到的存储库的 ID，格式如下：`<user>/<dataset_name>` 或 `<org>/<dataset_name>`。也接受 `<dataset_name>`，将默认为已登录用户的命名空间。

+   `config_name` (`str`，默认为 “default”) — 数据集的配置名称（或子集）。默认为 “default”。

+   `set_default` (`bool`, *optional*) — 是否将此配置设置为默认配置。否则，默认配置为名称为“default”的配置。

+   `split` (`str`, *optional*) — 将分配给该数据集的拆分名称。默认为 `self.split`。

+   `data_dir` (`str`, *optional*) — 包含上传数据文件的目录名称。如果与“default”不同，默认为`config_name`，否则为“data”。

    在 2.17.0 版本中添加

+   `commit_message` (`str`, *optional*) — 推送时要提交的消息。默认为 `"Upload dataset"`。

+   `commit_description` (`str`, *optional*) — 将创建的提交的描述。此外，如果创建了 PR（`create_pr` 为 True），还会是 PR 的描述。

    在 2.16.0 版本中添加

+   `private` (`bool`, *optional*, 默认为 `False`) — 数据集仓库是否应设置为私有。仅影响仓库创建：已存在的仓库不会受到该参数的影响。

+   `token` (`str`, *optional*) — Hugging Face Hub 的可选身份验证令牌。如果未传递令牌，将默认使用在 `huggingface-cli login` 登录时本地保存的令牌。如果未传递令牌且用户未登录，将引发错误。

+   `revision` (`str`, *optional*) — 要将上传的文件推送到的分支。默认为 `"main"` 分支。

    在 2.15.0 版本中添加

+   `branch` (`str`, *optional*) — 推送数据集的 git 分支。默认为存储库中指定的默认分支，即 `"main"`。

    在 2.15.0 版本中已弃用

    `branch` 在 2.15.0 版本中已弃用，推荐使用 `revision`，并将在 3.0.0 版本中移除。

+   `create_pr` (`bool`, *optional*, 默认为 `False`) — 是否创建带有上传文件的 PR 或直接提交。

    在 2.15.0 版本中添加

+   `max_shard_size` (`int` 或 `str`, *optional*, 默认为 `"500MB"`) — 要上传到 hub 的数据集分片的最大大小。如果表示为字符串，需要是数字后跟一个单位（如 `"5MB"`）。

+   `num_shards` (`int`, *optional*) — 要写入的分片数。默认情况下，分片数取决于 `max_shard_size`。

    在 2.8.0 版本中添加

+   `embed_external_files` (`bool`, 默认为 `True`) — 是否在分片中嵌入文件字节。特别是，在推送之前，将为类型字段执行以下操作：

    +   [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio) 和 [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)：删除本地路径信息，并在 Parquet 文件中嵌入文件内容。

将数据集作为 Parquet 数据集推送到 hub。数据集使用 HTTP 请求推送，不需要安装 git 或 git-lfs。

默认情况下，生成的 Parquet 文件是自包含的。如果数据集包含 [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image) 或 [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio) 数据，则 Parquet 文件将存储图像或音频文件的字节。您可以通过将 `embed_external_files` 设置为 `False` 来禁用此功能。

示例：

```py
>>> dataset.push_to_hub("<organization>/<dataset_id>")
>>> dataset_dict.push_to_hub("<organization>/<dataset_id>", private=True)
>>> dataset.push_to_hub("<organization>/<dataset_id>", max_shard_size="1GB")
>>> dataset.push_to_hub("<organization>/<dataset_id>", num_shards=1024)
```

如果数据集有多个拆分（例如训练/验证/测试）：

```py
>>> train_dataset.push_to_hub("<organization>/<dataset_id>", split="train")
>>> val_dataset.push_to_hub("<organization>/<dataset_id>", split="validation")
>>> # later
>>> dataset = load_dataset("<organization>/<dataset_id>")
>>> train_dataset = dataset["train"]
>>> val_dataset = dataset["validation"]
```

如果要向数据集添加新的配置（或子集）（例如，如果数据集有多个任务/版本/语言）：

```py
>>> english_dataset.push_to_hub("<organization>/<dataset_id>", "en")
>>> french_dataset.push_to_hub("<organization>/<dataset_id>", "fr")
>>> # later
>>> english_dataset = load_dataset("<organization>/<dataset_id>", "en")
>>> french_dataset = load_dataset("<organization>/<dataset_id>", "fr")
```

#### `save_to_disk`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1383)

```py
( dataset_path: Union fs = 'deprecated' max_shard_size: Union = None num_shards: Optional = None num_proc: Optional = None storage_options: Optional = None )
```

参数

+   `dataset_path` (`str`) — 数据集目录的路径（例如 `dataset/train`）或远程 URI（例如 `s3://my-bucket/dataset/train`），数据集将保存到该目录。

+   `fs` (`fsspec.spec.AbstractFileSystem`, *optional*) — 数据集将保存到的远程文件系统的实例。

    在 2.8.0 版本中已弃用

    `fs` 在 2.8.0 版本中已弃用，将在 3.0.0 版本中移除。请改用 `storage_options`，例如 `storage_options=fs.storage_options`

+   `max_shard_size` (`int` 或 `str`, *可选*, 默认为`"500MB"`) — 要上传到hub的数据集分片的最大大小。如果表示为字符串，需要是数字后跟一个单位（如`"50MB"`）。

+   `num_shards` (`int`, *可选*) — 要写入的分片数。默认情况下，分片数取决于`max_shard_size`和`num_proc`。

    2.8.0版本中添加

+   `num_proc` (`int`, *可选*) — 在本地下载和生成数据集时的进程数。默认情况下禁用多进程。

    2.8.0版本中添加

+   `storage_options` (`dict`, *可选*) — 传递给文件系统后端的键/值对，如果有的话。

    2.8.0版本中添加

将数据集保存到数据集目录，或者使用`fsspec.spec.AbstractFileSystem`的任何实现保存到文件系统。

对于[Image](/docs/datasets/v2.17.0/zh/package_reference/main_classes#datasets.Image)和[Audio](/docs/datasets/v2.17.0/zh/package_reference/main_classes#datasets.Audio)数据：

所有的Image()和Audio()数据都存储在arrow文件中。如果要存储路径或URL，请使用Value("string")类型。

示例：

```py
>>> ds.save_to_disk("path/to/dataset/directory")
>>> ds.save_to_disk("path/to/dataset/directory", max_shard_size="1GB")
>>> ds.save_to_disk("path/to/dataset/directory", num_shards=1024)
```

#### `load_from_disk`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1599)

```py
( dataset_path: str fs = 'deprecated' keep_in_memory: Optional = None storage_options: Optional = None ) → export const metadata = 'undefined';Dataset or DatasetDict
```

参数

+   `dataset_path` (`str`) — 数据集目录的路径（例如`"dataset/train"`）或远程URI（例如`"s3//my-bucket/dataset/train"`），数据集将从这里加载。

+   `fs` (`fsspec.spec.AbstractFileSystem`, *可选*) — 数据集将保存到的远程文件系统的实例。

    2.8.0版本中弃用

    `fs`在2.8.0版本中已弃用，并将在3.0.0中删除。请改用`storage_options`，例如`storage_options=fs.storage_options`

+   `keep_in_memory` (`bool`, 默认为`None`) — 是否将数据集复制到内存中。如果为`None`，除非通过将`datasets.config.IN_MEMORY_MAX_SIZE`设置为非零来显式启用，否则数据集不会被复制到内存中。在[提高性能](../cache#improve-performance)部分中查看更多细节。

+   `storage_options` (`dict`, *可选*) — 传递给文件系统后端的键/值对，如果有的话。

    2.8.0版本中添加

返回

[数据集](/docs/datasets/v2.17.0/zh/package_reference/main_classes#datasets.Dataset) 或 [DatasetDict](/docs/datasets/v2.17.0/zh/package_reference/main_classes#datasets.DatasetDict)

+   如果`dataset_path`是数据集目录的路径，则请求的数据集。

+   如果`dataset_path`是数据集字典目录的路径，则返回一个带有每个拆分的`datasets.DatasetDict`。

加载之前使用`save_to_disk`保存的数据集，可以从数据集目录加载，也可以使用`fsspec.spec.AbstractFileSystem`的任何实现从文件系统加载。

示例：

```py
>>> ds = load_from_disk("path/to/dataset/directory")
```

#### `flatten_indices`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L3672)

```py
( keep_in_memory: bool = False cache_file_name: Optional = None writer_batch_size: Optional = 1000 features: Optional = None disable_nullable: bool = False num_proc: Optional = None new_fingerprint: Optional = None )
```

参数

+   `keep_in_memory` (`bool`, 默认为`False`) — 将数据集保留在内存中，而不是写入缓存文件。

+   `cache_file_name` (`str`, *可选*, 默认为`None`) — 提供缓存文件的路径名称。它用于存储计算结果，而不是自动生成的缓存文件名。

+   `writer_batch_size` (`int`, 默认为`1000`) — 每个缓存文件写入操作的行数。这个值在处理过程中内存使用和处理速度之间取得了很好的平衡。较高的值使处理过程减少查找次数，较低的值在运行`map`时消耗较少的临时内存。

+   `features` (`Optional[datasets.Features]`, 默认为`None`) — 使用特定的[Features](/docs/datasets/v2.17.0/zh/package_reference/main_classes#datasets.Features)来存储缓存文件，而不是自动生成的缓存文件。

+   `disable_nullable` (`bool`, 默认为`False`) — 允许表中存在空值。

+   `num_proc` (`int`, 可选, 默认为`None`) — 生成缓存时的最大进程数。已缓存的分片按顺序加载

+   `new_fingerprint` (`str`, *可选*, 默认为`None`) — 转换后数据集的新指纹。如果为`None`，则使用先前指纹的哈希计算新指纹，以及转换参数

通过展平索引映射创建并缓存一个新的数据集。

#### `to_csv`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4727)

```py
( path_or_buf: Union batch_size: Optional = None num_proc: Optional = None **to_csv_kwargs ) → export const metadata = 'undefined';int
```

参数

+   `path_or_buf` (`PathLike`或`FileOrBuffer`) — 要么是文件的路径，要么是BinaryIO。

+   `batch_size` (`int`, *可选*) — 要加载到内存并一次写入的批次大小。默认为`datasets.config.DEFAULT_MAX_BATCH_SIZE`。

+   `num_proc` (`int`, *可选*) — 多进程的进程数。默认情况下不使用多进程。在这种情况下，`batch_size`默认为`datasets.config.DEFAULT_MAX_BATCH_SIZE`，但如果计算能力足够，可以随意将其设置为默认值的5倍或10倍。

+   *`*to_csv_kwargs`（额外的关键字参数） — 传递给pandas的参数[`pandas.DataFrame.to_csv`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html)。

    2.10.0中的更改

    现在，如果未指定，`index`默认为`False`。

    如果要写入索引，请传递`index=True`，并通过传递`index_label`为索引列设置名称。

返回

`int`

写入的字符或字节数。

将数据集导出为csv

示例：

```py
>>> ds.to_csv("path/to/dataset/directory")
```

#### `to_pandas`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4887)

```py
( batch_size: Optional = None batched: bool = False )
```

参数

+   `batched` (`bool`) — 设置为`True`以返回一个生成器，该生成器将数据集作为`batch_size`行的批次生成。默认为`False`（一次返回整个数据集）。

+   `batch_size` (`int`, *可选*) — 如果`batched`为`True`，则为批次的大小（行数）。默认为`datasets.config.DEFAULT_MAX_BATCH_SIZE`。

将数据集返回为`pandas.DataFrame`。也可以为大型数据集返回一个生成器。

示例：

```py
>>> ds.to_pandas()
```

#### `to_dict`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4773)

```py
( batch_size: Optional = None batched = 'deprecated' )
```

参数

+   `batched` (`bool`) — 设置为`True`以返回一个生成器，该生成器将数据集作为`batch_size`行的批次生成。默认为`False`（一次返回整个数据集）。

    2.11.0中弃用

    在单独的批次上使用`.iter(batch_size=batch_size)`，然后使用`.to_dict()`。

+   `batch_size` (`int`, *可选*) — 如果`batched`为`True`，则为批次的大小（行数）。默认为`datasets.config.DEFAULT_MAX_BATCH_SIZE`。

将数据集返回为Python字典。也可以为大型数据集返回一个生成器。

示例：

```py
>>> ds.to_dict()
```

#### `to_json`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4842)

```py
( path_or_buf: Union batch_size: Optional = None num_proc: Optional = None **to_json_kwargs ) → export const metadata = 'undefined';int
```

参数

+   `path_or_buf` (`PathLike`或`FileOrBuffer`) — 要么是文件的路径，要么是BinaryIO。

+   `batch_size` (`int`, *可选*) — 要加载到内存并一次写入的批次大小。默认为`datasets.config.DEFAULT_MAX_BATCH_SIZE`。

+   `num_proc` (`int`, *可选*) — 多进程的进程数。默认情况下不使用多进程。在这种情况下，`batch_size`默认为`datasets.config.DEFAULT_MAX_BATCH_SIZE`，但如果计算能力足够，可以随意将其设置为默认值的5倍或10倍。

+   *`*to_json_kwargs`（额外的关键字参数） — 传递给pandas的参数[`pandas.DataFrame.to_json`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html)。

    2.11.0中的更改

    现在，如果`orient`为`"split"`或`"table"`，`index`默认为`False`。

    如果要写入索引，请传递`index=True`。

返回

`int`

写入的字符或字节数。

将数据集导出为JSON Lines或JSON。

示例：

```py
>>> ds.to_json("path/to/dataset/directory")
```

#### `to_parquet`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4926)

```py
( path_or_buf: Union batch_size: Optional = None **parquet_writer_kwargs ) → export const metadata = 'undefined';int
```

参数

+   `path_or_buf` (`PathLike`或`FileOrBuffer`) — 要么是文件的路径，要么是BinaryIO。

+   `batch_size` (`int`, *可选*) — 一次加载到内存并写入的批次大小。默认为 `datasets.config.DEFAULT_MAX_BATCH_SIZE`。

+   *`*parquet_writer_kwargs` (额外的关键字参数) — 传递给PyArrow的 `pyarrow.parquet.ParquetWriter` 的参数。

返回

`int`

写入的字符或字节数。

将数据集导出到parquet

示例：

```py
>>> ds.to_parquet("path/to/dataset/directory")
```

#### `to_sql`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L4957)

```py
( name: str con: Union batch_size: Optional = None **sql_writer_kwargs ) → export const metadata = 'undefined';int
```

参数

+   `name` (`str`) — SQL表的名称。

+   `con` (`str` 或 `sqlite3.Connection` 或 `sqlalchemy.engine.Connection` 或 `sqlalchemy.engine.Connection`) — 用于写入数据库的 [URI字符串](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls) 或 SQLite3/SQLAlchemy连接对象。

+   `batch_size` (`int`, *可选*) — 一次加载到内存并写入的批次大小。默认为 `datasets.config.DEFAULT_MAX_BATCH_SIZE`。

+   *`*sql_writer_kwargs` (额外的关键字参数) — 传递给pandas的 [`pandas.DataFrame.to_sql`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html) 的参数。

    在2.11.0中更改

    现在，如果未指定，`index` 默认为 `False`。

    如果要写入索引，请传递 `index=True` 并通过传递 `index_label` 设置索引列的名称。

返回

`int`

写入的记录数。

将数据集导出到SQL数据库。

示例：

```py
>>> # con provided as a connection URI string
>>> ds.to_sql("data", "sqlite:///my_own_db.sql")
>>> # con provided as a sqlite3 connection object
>>> import sqlite3
>>> con = sqlite3.connect("my_own_db.sql")
>>> with con:
...     ds.to_sql("data", con)
```

#### `to_iterable_dataset`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5049)

```py
( num_shards: Optional = 1 )
```

参数

+   `num_shards` (`int`, 默认为 `1`) — 实例化可迭代数据集时定义的分片数。这对于大型数据集特别有用，可以正确地进行洗牌，并且还可以使用PyTorch DataLoader或在分布式设置中进行快速并行加载。分片使用 [datasets.Dataset.shard()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.shard) 定义：它只是切片数据而不将任何内容写入磁盘。

从映射样式的 [datasets.Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset) 获取一个 [datasets.IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)。这相当于使用 [datasets.load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset) 以流式模式加载数据集，但速度更快，因为数据是从本地文件流式传输的。

与映射样式数据集相反，可迭代数据集是惰性的，只能进行迭代（例如使用for循环）。由于它们在训练循环中按顺序读取，可迭代数据集比映射样式数据集快得多。对可迭代数据集应用的所有转换（如过滤或处理）都是在开始迭代数据集时即时进行的。

但是，可以使用 [datasets.IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle) 对可迭代数据集进行洗牌。这是一种快速的近似洗牌方法，如果有多个分片并且指定的缓冲区大小足够大，则效果最佳。

为了获得最佳速度性能，请确保数据集没有索引映射。如果是这种情况，数据不会连续读取，有时可能会很慢。您可以使用 `ds = ds.flatten_indices()` 将数据集写入连续的数据块，并在切换到可迭代数据集之前获得最佳速度。

示例：

基本用法：

```py
>>> ids = ds.to_iterable_dataset()
>>> for example in ids:
...     pass
```

使用延迟过滤和处理：

```py
>>> ids = ds.to_iterable_dataset()
>>> ids = ids.filter(filter_fn).map(process_fn)  # will filter and process on-the-fly when you start iterating over the iterable dataset
>>> for example in ids:
...     pass
```

通过分片实现有效洗牌：

```py
>>> ids = ds.to_iterable_dataset(num_shards=64)  # the dataset is split into 64 shards to be iterated over
>>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer for fast approximate shuffling when you start iterating
>>> for example in ids:
...     pass
```

使用PyTorch DataLoader：

```py
>>> import torch
>>> ids = ds.to_iterable_dataset(num_shards=64)
>>> ids = ids.filter(filter_fn).map(process_fn)
>>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards to each worker to load, filter and process when you start iterating
>>> for example in ids:
...     pass
```

使用PyTorch DataLoader和洗牌：

```py
>>> import torch
>>> ids = ds.to_iterable_dataset(num_shards=64)
>>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating
>>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from the shuffled list of shards to each worker when you start iterating
>>> for example in ids:
...     pass
```

在像PyTorch DDP这样的分布式设置中，使用PyTorch DataLoader和洗牌

```py
>>> from datasets.distributed import split_dataset_by_node
>>> ids = ds.to_iterable_dataset(num_shards=512)
>>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating
>>> ids = split_dataset_by_node(ds, world_size=8, rank=0)  # will keep only 512 / 8 = 64 shards from the shuffled lists of shards when you start iterating
>>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from this node's list of shards to each worker when you start iterating
>>> for example in ids:
...     pass
```

带有洗牌和多个epochs：

```py
>>> ids = ds.to_iterable_dataset(num_shards=64)
>>> ids = ids.shuffle(buffer_size=10_000, seed=42)  # will shuffle the shards order and use a shuffle buffer when you start iterating
>>> for epoch in range(n_epochs):
...     ids.set_epoch(epoch)  # will use effective_seed = seed + epoch to shuffle the shards and for the shuffle buffer when you start iterating
...     for example in ids:
...         pass
```

在使用PyTorch DataLoader或分布式设置时，也可以使用 `IterableDataset.set_epoch()`。#### `add_faiss_index`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5642)

```py
( column: str index_name: Optional = None device: Optional = None string_factory: Optional = None metric_type: Optional = None custom_index: Optional = None batch_size: int = 1000 train_size: Optional = None faiss_verbose: bool = False dtype = <class 'numpy.float32'> )
```

参数

+   `column` (`str`) — 要添加到索引的向量的列。

+   `index_name` (`str`, *optional*) — 索引的 `index_name`/标识符。这是用于调用 [get_nearest_examples()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.get_nearest_examples) 或 [search()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.search) 的 `index_name`。默认情况下对应于 `column`。

+   `device` (`Union[int, List[int]]`, *optional*) — 如果是正整数，这是要使用的 GPU 的索引。如果是负整数，使用所有 GPU。如果传入一组正整数的列表，则仅在这些 GPU 上运行。默认情况下使用 CPU。

+   `string_factory` (`str`, *optional*) — 这将传递给 Faiss 的索引工厂，用于创建索引。默认索引类是 `IndexFlat`。

+   `metric_type` (`int`, *optional*) — 度量类型。例如：`faiss.METRIC_INNER_PRODUCT` 或 `faiss.METRIC_L2`。

+   `custom_index` (`faiss.Index`, *optional*) — 您已经实例化并为您的需求配置的自定义 Faiss 索引。

+   `batch_size` (`int`) — 在向 `FaissIndex` 添加向量时使用的批量大小。默认值为 `1000`。

    在 2.4.0 版本中添加

+   `train_size` (`int`, *optional*) — 如果索引需要训练步骤，指定将用于训练索引的向量数量。

+   `faiss_verbose` (`bool`, 默认为 `False`) — 启用 Faiss 索引的详细信息。

+   `dtype` (`data-type`) — 被索引的 numpy 数组的数据类型。默认为 `np.float32`。

添加使用 Faiss 进行快速检索的稠密索引。默认情况下，索引是在指定列的向量上完成的。如果要在 GPU 上运行，可以指定 `device`（`device` 必须是 GPU 索引）。您可以在这里找到有关 Faiss 的更多信息：

+   有关 [string factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory) 的信息

示例：

```py
>>> ds = datasets.load_dataset('crime_and_punish', split='train')
>>> ds_with_embeddings = ds.map(lambda example: {'embeddings': embed(example['line']}))
>>> ds_with_embeddings.add_faiss_index(column='embeddings')
>>> # query
>>> scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embeddings', embed('my new query'), k=10)
>>> # save index
>>> ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')

>>> ds = datasets.load_dataset('crime_and_punish', split='train')
>>> # load index
>>> ds.load_faiss_index('embeddings', 'my_index.faiss')
>>> # query
>>> scores, retrieved_examples = ds.get_nearest_examples('embeddings', embed('my new query'), k=10)
```

#### `add_faiss_index_from_external_arrays`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5722)

```py
( external_arrays: array index_name: str device: Optional = None string_factory: Optional = None metric_type: Optional = None custom_index: Optional = None batch_size: int = 1000 train_size: Optional = None faiss_verbose: bool = False dtype = <class 'numpy.float32'> )
```

参数

+   `external_arrays` (`np.array`) — 如果要使用来自库外的数组进行索引，可以设置 `external_arrays`。它将使用 `external_arrays` 来创建 Faiss 索引，而不是给定 `column` 中的数组。

+   `index_name` (`str`) — 索引的 `index_name`/标识符。这是用于调用 [get_nearest_examples()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.get_nearest_examples) 或 [search()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.search) 的 `index_name`。 

+   `device`（可选的 `Union[int, List[int]]`, *optional*) — 如果是正整数，这是要使用的 GPU 的索引。如果是负整数，使用所有 GPU。如果传入一组正整数的列表，则仅在这些 GPU 上运行。默认情况下使用 CPU。

+   `string_factory` (`str`, *optional*) — 这将传递给 Faiss 的索引工厂，用于创建索引。默认索引类是 `IndexFlat`。

+   `metric_type` (`int`, *optional*) — 度量类型。例如：`faiss.faiss.METRIC_INNER_PRODUCT` 或 `faiss.METRIC_L2`。

+   `custom_index` (`faiss.Index`, *optional*) — 您已经实例化并为您的需求配置的自定义 Faiss 索引。

+   `batch_size` (`int`, *optional*) — 在向 FaissIndex 添加向量时使用的批量大小。默认值为 1000。

    在 2.4.0 版本中添加

+   `train_size` (`int`, *optional*) — 如果索引需要训练步骤，指定将用于训练索引的向量数量。

+   `faiss_verbose` (`bool`, 默认为 False) — 启用 Faiss 索引的详细信息。

+   `dtype` (`numpy.dtype`) — 被索引的 numpy 数组的数据类型。默认为 np.float32。

使用Faiss添加密集索引以进行快速检索。索引是使用`external_arrays`的向量创建的。如果要在GPU上运行，可以指定`device`（`device`必须是GPU索引）。您可以在这里找到有关Faiss的更多信息：

+   对于[string factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory)

#### `save_faiss_index`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L529)

```py
( index_name: str file: Union storage_options: Optional = None )
```

参数

+   `index_name` (`str`) — 索引的`index_name`/标识符。这是用于调用`.get_nearest`或`.search`的索引名称。

+   `file` (`str`) — 磁盘上序列化faiss索引的路径或远程URI（例如`"s3://my-bucket/index.faiss"`）。

+   `storage_options` (`dict`, *可选*) — 要传递给文件系统后端的键/值对。

    2.11.0版本中添加

将FaissIndex保存到磁盘。

#### `load_faiss_index`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L547)

```py
( index_name: str file: Union device: Union = None storage_options: Optional = None )
```

参数

+   `index_name` (`str`) — 索引的`index_name`/标识符。这是用于调用`.get_nearest`或`.search`的索引名称。

+   `file` (`str`) — 磁盘上序列化faiss索引的路径或远程URI（例如`"s3://my-bucket/index.faiss"`）。

+   `device` (可选 `Union[int, List[int]]`) — 如果是正整数，则为要使用的GPU的索引。如果是负整数，则使用所有GPU。如果传入一组正整数，则仅在这些GPU上运行。默认情况下使用CPU。

+   `storage_options` (`dict`, *可选*) — 要传递给文件系统后端的键/值对。

    2.11.0版本中添加

从磁盘加载FaissIndex。

如果您想进行额外的配置，可以通过`.get_index(index_name).faiss_index`访问faiss索引对象，以使其符合您的需求。

#### `add_elasticsearch_index`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5781)

```py
( column: str index_name: Optional = None host: Optional = None port: Optional = None es_client: Optional = None es_index_name: Optional = None es_index_config: Optional = None )
```

参数

+   `column` (`str`) — 要添加到索引的文档的列。

+   `index_name` (`str`, *可选*) — 索引的`index_name`/标识符。这是用于调用[get_nearest_examples()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.get_nearest_examples)或[Dataset.search()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.search)的索引名称。默认情况下对应于`column`。

+   `host` (`str`, *可选*, 默认为`localhost`) — 运行ElasticSearch的主机。

+   `port` (`str`, *可选*, 默认为`9200`) — 运行ElasticSearch的端口。

+   `es_client` (`elasticsearch.Elasticsearch`, *可选*) — 用于创建索引的elasticsearch客户端，如果主机和端口为`None`。

+   `es_index_name` (`str`, *可选*) — 用于创建索引的elasticsearch索引名称。

+   `es_index_config` (`dict`, *可选*) — elasticsearch索引的配置。默认配置为：

使用ElasticSearch添加文本索引以进行快速检索。这是原地完成的。

示例：

```py
>>> es_client = elasticsearch.Elasticsearch()
>>> ds = datasets.load_dataset('crime_and_punish', split='train')
>>> ds.add_elasticsearch_index(column='line', es_client=es_client, es_index_name="my_es_index")
>>> scores, retrieved_examples = ds.get_nearest_examples('line', 'my new query', k=10)
```

#### `load_elasticsearch_index`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L631)

```py
( index_name: str es_index_name: str host: Optional = None port: Optional = None es_client: Optional = None es_index_config: Optional = None )
```

参数

+   `index_name` (`str`) — 索引的`index_name`/标识符。这是用于调用`get_nearest`或`search`的索引名称。

+   `es_index_name` (`str`) — 要加载的elasticsearch索引的名称。

+   `host` (`str`, *可选*, 默认为`localhost`) — 运行ElasticSearch的主机。

+   `port` (`str`, *可选*, 默认为`9200`) — 运行ElasticSearch的端口。

+   `es_client` (`elasticsearch.Elasticsearch`, *可选*) — 用于创建索引的elasticsearch客户端，如果主机和端口为`None`。

+   `es_index_config` (`dict`, *可选*) — elasticsearch索引的配置。默认配置为：

使用ElasticSearch加载现有文本索引以进行快速检索。

#### `list_indexes`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L432)

```py
( )
```

列出所有附加索引的`colindex_nameumns`/标识符。

#### `get_index`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L436)

```py
( index_name: str )
```

参数

+   `index_name` (`str`) — 索引名称。

列出所有附加索引的`index_name`/标识符。

#### `drop_index`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L678)

```py
( index_name: str )
```

参数

+   `index_name` (`str`) — 索引的`index_name`/标识符。

删除具有指定列的索引。

#### `search`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L687)

```py
( index_name: str query: Union k: int = 10 **kwargs ) → export const metadata = 'undefined';(scores, indices)
```

参数

+   `index_name` (`str`) — 索引的名称/标识符。

+   `query` (`Union[str, np.ndarray]`) — 如果`index_name`是文本索引，则查询为字符串，如果`index_name`是向量索引，则为numpy数组。

+   `k` (`int`) — 要检索的示例数量。

返回

`(scores, indices)`

一个元组`(scores, indices)`，其中：

+   `scores` (`List[List[float]`): 检索到的示例的来自FAISS（默认为`IndexFlatL2`）或ElasticSearch的检索分数

+   `indices` (`List[List[int]]`): 检索示例的索引

找到数据集中与查询最近的示例索引。

#### `search_batch`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L707)

```py
( index_name: str queries: Union k: int = 10 **kwargs ) → export const metadata = 'undefined';(total_scores, total_indices)
```

参数

+   `index_name` (`str`) — 索引的`index_name`/标识符。

+   `queries` (`Union[List[str], np.ndarray]`) — 如果`index_name`是文本索引，则查询为字符串列表，如果`index_name`是向量索引，则为numpy数组。

+   `k` (`int`) — 每个查询要检索的示例数量。

返回

`(total_scores, total_indices)`

一个元组`(total_scores, total_indices)`，其中：

+   `total_scores` (`List[List[float]`): 每个查询的检索分数，来自FAISS（默认为`IndexFlatL2`）或ElasticSearch的检索示例

+   `total_indices` (`List[List[int]]`): 每个查询的检索示例的索引

找到数据集中与查询最近的示例索引。

#### `get_nearest_examples`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L729)

```py
( index_name: str query: Union k: int = 10 **kwargs ) → export const metadata = 'undefined';(scores, examples)
```

参数

+   `index_name` (`str`) — 索引的`index_name`/标识符。

+   `query` (`Union[str, np.ndarray]`) — 如果`index_name`是文本索引，则查询为字符串，如果`index_name`是向量索引，则为numpy数组。

+   `k` (`int`) — 要检索的示例数量。

返回

`(scores, examples)`

一个元组`(scores, examples)`，其中：

+   `scores` (`List[float]`): 检索到的示例的来自FAISS（默认为`IndexFlatL2`）或ElasticSearch的检索分数

+   `examples` (`dict`): 检索到的示例

找到数据集中与查询最近的示例。

#### `get_nearest_examples_batch`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/search.py#L753)

```py
( index_name: str queries: Union k: int = 10 **kwargs ) → export const metadata = 'undefined';(total_scores, total_examples)
```

参数

+   `index_name` (`str`) — 索引的`index_name`/标识符。

+   `queries` (`Union[List[str], np.ndarray]`) — 如果`index_name`是文本索引，则查询为字符串列表，如果`index_name`是向量索引，则为numpy数组。

+   `k` (`int`) — 每个查询要检索的示例数量。

返回

`(total_scores, total_examples)`

一个元组`(total_scores, total_examples)`，其中：

+   `total_scores` (`List[List[float]`): 每个查询的检索分数，来自FAISS（默认为`IndexFlatL2`）或ElasticSearch的检索示例

+   `total_examples` (`List[dict]`): 每个查询的检索示例

找到数据集中与查询最近的示例。

#### `info`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L159)

```py
( )
```

包含数据集中所有元数据的[DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)对象。

#### `split`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L164)

```py
( )
```

[NamedSplit](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.NamedSplit)对象，对应于命名数据集拆分。

#### `构建器名称`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L169)

```py
( )
```

#### `引用`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L173)

```py
( )
```

#### `配置名称`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L177)

```py
( )
```

#### `数据集大小`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L181)

```py
( )
```

#### `描述`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L185)

```py
( )
```

#### `下载校验和`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L189)

```py
( )
```

#### `下载大小`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L193)

```py
( )
```

#### `特征`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L730)

```py
( )
```

#### `主页`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L201)

```py
( )
```

#### `许可证`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L205)

```py
( )
```

#### `字节大小`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L209)

```py
( )
```

#### `监督键`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L213)

```py
( )
```

#### `版本`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L221)

```py
( )
```

#### `from_csv`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L954)

```py
( path_or_paths: Union split: Optional = None features: Optional = None cache_dir: str = None keep_in_memory: bool = False num_proc: Optional = None **kwargs )
```

参数

+   `path_or_paths`（`path-like`或`path-like`列表）— CSV文件的路径。

+   `split`（[NamedSplit](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.NamedSplit)，*可选*）— 要分配给数据集的拆分名称。

+   `features`（[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)，*可选*）— 数据集特征。

+   `cache_dir`（`str`，*可选*，默认为`"~/.cache/huggingface/datasets"`）— 缓存数据的目录。

+   `keep_in_memory`（`bool`，默认为`False`）— 是否将数据复制到内存中。

+   `num_proc`（`int`，*可选*，默认为`None`）— 下载和本地生成数据集时的进程数。如果数据集由多个文件组成，则这很有帮助。默认情况下禁用多进程。

    添加于2.8.0

+   *`*kwargs`（额外的关键字参数）— 要传递给`pandas.read_csv`的关键字参数。

从CSV文件创建数据集。

示例：

```py
>>> ds = Dataset.from_csv('path/to/dataset.csv')
```

#### `from_json`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1076)

```py
( path_or_paths: Union split: Optional = None features: Optional = None cache_dir: str = None keep_in_memory: bool = False field: Optional = None num_proc: Optional = None **kwargs )
```

参数

+   `path_or_paths`（`path-like`或`path-like`列表）— JSON或JSON Lines文件的路径。

+   `split`（[NamedSplit](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.NamedSplit)，*可选*）— 要分配给数据集的拆分名称。

+   `features`（[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)，*可选*）— 数据集特征。

+   `cache_dir`（`str`，*可选*，默认为`"~/.cache/huggingface/datasets"`）— 缓存数据的目录。

+   `keep_in_memory`（`bool`，默认为`False`）— 是否将数据复制到内存中。

+   `field`（`str`，*可选*）— 数据集所在的JSON文件的字段名称。

+   `num_proc`（`int`，*可选*，默认为`None`）— 下载和本地生成数据集时的进程数。如果数据集由多个文件组成，则这很有帮助。默认情况下禁用多进程。

    添加于2.8.0

+   *`*kwargs`（额外的关键字参数）— 要传递给`JsonConfig`的关键字参数。

从JSON或JSON Lines文件创建数据集。

示例：

```py
>>> ds = Dataset.from_json('path/to/dataset.json')
```

#### `from_parquet`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1133)

```py
( path_or_paths: Union split: Optional = None features: Optional = None cache_dir: str = None keep_in_memory: bool = False columns: Optional = None num_proc: Optional = None **kwargs )
```

参数

+   `path_or_paths`（`path-like`或`path-like`列表）—Parquet文件的路径。

+   `split`（`NamedSplit`，*可选*）—要分配给数据集的拆分名称。

+   `features`（`Features`，*可选*）—数据集特征。

+   `cache_dir`（`str`，*可选*，默认为`"~/.cache/huggingface/datasets"`）—缓存数据的目录。

+   `keep_in_memory`（`bool`，默认为`False`）—是否将数据复制到内存中。

+   `columns`（`List[str]`，*可选*）—如果不是`None`，则只会从文件中读取这些列。列名可能是嵌套字段的前缀，例如‘a’将选择‘a.b’，‘a.c’和‘a.d.e’。

+   `num_proc`（`int`，*可选*，默认为`None`）—在本地下载和生成数据集时的进程数。如果数据集由多个文件组成，则这很有帮助。默认情况下禁用多进程。

    添加于2.8.0

+   *`*kwargs`（额外的关键字参数）—要传递给`ParquetConfig`的关键字参数。

从Parquet文件创建数据集。

示例：

```py
>>> ds = Dataset.from_parquet('path/to/dataset.parquet')
```

#### `from_text`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1192)

```py
( path_or_paths: Union split: Optional = None features: Optional = None cache_dir: str = None keep_in_memory: bool = False num_proc: Optional = None **kwargs )
```

参数

+   `path_or_paths`（`path-like`或`path-like`列表）—文本文件的路径。

+   `split`（`NamedSplit`，*可选*）—要分配给数据集的拆分名称。

+   `features`（`Features`，*可选*）—数据集特征。

+   `cache_dir`（`str`，*可选*，默认为`"~/.cache/huggingface/datasets"`）—缓存数据的目录。

+   `keep_in_memory`（`bool`，默认为`False`）—是否将数据复制到内存中。

+   `num_proc`（`int`，*可选*，默认为`None`）—在本地下载和生成数据集时的进程数。如果数据集由多个文件组成，则这很有帮助。默认情况下禁用多进程。

    添加于2.8.0

+   *`*kwargs`（额外的关键字参数）—要传递给`TextConfig`的关键字参数。

从文本文件创建数据集。

示例：

```py
>>> ds = Dataset.from_text('path/to/dataset.txt')
```

#### `from_sql`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L1307)

```py
( sql: Union con: Union features: Optional = None cache_dir: str = None keep_in_memory: bool = False **kwargs )
```

参数

+   `sql`（`str`或`sqlalchemy.sql.Selectable`）—要执行的SQL查询或表名。

+   `con`（`str`或`sqlite3.Connection`或`sqlalchemy.engine.Connection`或`sqlalchemy.engine.Connection`）—用于实例化数据库连接或SQLite3/SQLAlchemy连接对象的[URI字符串](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls)。

+   `features`（[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)，*可选*）—数据集特征。

+   `cache_dir`（`str`，*可选*，默认为`"~/.cache/huggingface/datasets"`）—缓存数据的目录。

+   `keep_in_memory`（`bool`，默认为`False`）—是否将数据复制到内存中。

+   *`*kwargs`（额外的关键字参数）—要传递给`SqlConfig`的关键字参数。

从SQL查询或数据库表创建数据集。

示例：

```py
>>> # Fetch a database table
>>> ds = Dataset.from_sql("test_data", "postgres:///db_name")
>>> # Execute a SQL query on the table
>>> ds = Dataset.from_sql("SELECT sentence FROM test_data", "postgres:///db_name")
>>> # Use a Selectable object to specify the query
>>> from sqlalchemy import select, text
>>> stmt = select([text("sentence")]).select_from(text("test_data"))
>>> ds = Dataset.from_sql(stmt, "postgres:///db_name")
```

只有在指定`con`为URI字符串时，返回的数据集才能被缓存。

#### `prepare_for_task`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L2729)

```py
( task: Union id: int = 0 )
```

参数

+   `task`（`Union[str, TaskTemplate]`）—在训练和评估期间为数据集准备的任务。如果是`str`，支持的任务包括：

    +   `"text-classification"`

    +   `"question-answering"`

    如果是`TaskTemplate`，必须是[`datasets.tasks`](./task_templates)中的任务模板之一。

+   `id`（`int`，默认为`0`）—在支持同一类型的多个任务模板时，用于明确标识任务模板的ID。

通过将数据集的[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)转换为标准化的列名和类型，为给定任务准备数据集，详细信息在[`datasets.tasks`](./task_templates)中。

根据任务特定模式将`datasets.DatasetInfo.features`转换。仅用于一次性使用，因此在转换后从`datasets.DatasetInfo.task_templates`中删除所有任务模板。

#### `align_labels_with_mapping`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L5903)

```py
( label2id: Dict label_column: str )
```

参数

+   `label2id` (`dict`) — 用于将数据集与之对齐的标签名称到ID的映射。

+   `label_column` (`str`) — 标签列的列名对齐。

将数据集的标签ID和标签名称映射对齐到匹配输入`label2id`映射。当您希望确保模型预测的标签与数据集对齐时，这将非常有用。对齐是使用小写标签名称完成的。

示例：

```py
>>> # dataset with mapping {'entailment': 0, 'neutral': 1, 'contradiction': 2}
>>> ds = load_dataset("glue", "mnli", split="train")
>>> # mapping to align with
>>> label2id = {'CONTRADICTION': 0, 'NEUTRAL': 1, 'ENTAILMENT': 2}
>>> ds_aligned = ds.align_labels_with_mapping(label2id, "label")
```

#### `datasets.concatenate_datasets`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/combine.py#L158)

```py
( dsets: List info: Optional = None split: Optional = None axis: int = 0 )
```

参数

+   `dsets` (`List[datasets.Dataset]`) — 要连接的数据集列表。

+   `info` (`DatasetInfo`，*可选*) — 数据集信息，如描述、引用等。

+   `split` (`NamedSplit`，*可选*) — 数据集拆分的名称。

+   `axis` (`{0, 1}`，默认为`0`) — 要连接的轴，其中`0`表示按行（垂直）连接，`1`表示按列（水平）连接。

    1.6.0中添加

将具有相同模式的[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)列表转换为单个[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)。

示例：

```py
>>> ds3 = concatenate_datasets([ds1, ds2])
```

#### `datasets.interleave_datasets`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/combine.py#L18)

```py
( datasets: List probabilities: Optional = None seed: Optional = None info: Optional = None split: Optional = None stopping_strategy: Literal = 'first_exhausted' ) → export const metadata = 'undefined';Dataset or IterableDataset
```

参数

+   `datasets` (`List[Dataset]`或`List[IterableDataset]`) — 要交错的数据集列表。

+   `probabilities` (`List[float]`，*可选*，默认为`None`) — 如果指定，则根据这些概率从一个源逐个抽取示例构建新数据集。

+   `seed` (`int`，*可选*，默认为`None`) — 用于为每个示例选择源的随机种子。

+   `info` ([DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)，*可选*) — 数据集信息，如描述、引用等。

    2.4.0中添加

+   `split`（[NamedSplit](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.NamedSplit)，*可选*） — 数据集拆分的名称。

    2.4.0中添加

+   `stopping_strategy` (`str`，默认为`first_exhausted`) — 目前提出了两种策略，`first_exhausted`和`all_exhausted`。默认情况下，`first_exhausted`是一种欠采样策略，即数据集构建在一个数据集耗尽样本时停止。如果策略是`all_exhausted`，则使用一种过采样策略，即数据集构建在每个数据集的每个样本至少添加一次后停止。请注意，如果策略是`all_exhausted`，交错数据集的大小可能会变得非常庞大：

    +   如果没有概率，生成的数据集将有`max_length_datasets*nb_dataset`个样本。

    +   如果给定概率，则生成的数据集将具有更多样本，如果某些数据集访问的概率非常低。

返回

[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)或[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)

返回类型取决于输入`datasets`参数。如果输入是`Dataset`列表，则返回`Dataset`，如果输入是`IterableDataset`列表，则返回`IterableDataset`。

将多个数据集（源）交错到单个数据集中。新数据集通过在源之间交替获取示例来构建。

您可以在[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)对象列表或[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)对象列表上使用此函数。

+   如果`probabilities`是`None`（默认值），则通过在每个源之间循环获取示例来构建新数据集。

+   如果`probabilities`不是`None`，则根据提供的概率一次从随机源获取一个示例来构建新数据集。

当其中一个源数据集的示例用尽时，生成的数据集结束，除非`oversampling`为`True`，在这种情况下，生成的数据集在所有数据集至少用尽一次时结束。

可迭代数据集的注意事项：

在分布式设置或PyTorch DataLoader工作器中，停止策略是针对每个进程应用的。因此，在分片的可迭代数据集上，“first_exhausted”策略可能会生成总体少量样本（每个子数据集每个工作器最多缺少1个样本）。

示例：

对于常规数据集（映射样式）：

```py
>>> from datasets import Dataset, interleave_datasets
>>> d1 = Dataset.from_dict({"a": [0, 1, 2]})
>>> d2 = Dataset.from_dict({"a": [10, 11, 12]})
>>> d3 = Dataset.from_dict({"a": [20, 21, 22]})
>>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42, stopping_strategy="all_exhausted")
>>> dataset["a"]
[10, 0, 11, 1, 2, 20, 12, 10, 0, 1, 2, 21, 0, 11, 1, 2, 0, 1, 12, 2, 10, 0, 22]
>>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42)
>>> dataset["a"]
[10, 0, 11, 1, 2]
>>> dataset = interleave_datasets([d1, d2, d3])
>>> dataset["a"]
[0, 10, 20, 1, 11, 21, 2, 12, 22]
>>> dataset = interleave_datasets([d1, d2, d3], stopping_strategy="all_exhausted")
>>> dataset["a"]
[0, 10, 20, 1, 11, 21, 2, 12, 22]
>>> d1 = Dataset.from_dict({"a": [0, 1, 2]})
>>> d2 = Dataset.from_dict({"a": [10, 11, 12, 13]})
>>> d3 = Dataset.from_dict({"a": [20, 21, 22, 23, 24]})
>>> dataset = interleave_datasets([d1, d2, d3])
>>> dataset["a"]
[0, 10, 20, 1, 11, 21, 2, 12, 22]
>>> dataset = interleave_datasets([d1, d2, d3], stopping_strategy="all_exhausted")
>>> dataset["a"]
[0, 10, 20, 1, 11, 21, 2, 12, 22, 0, 13, 23, 1, 10, 24]
>>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42)
>>> dataset["a"]
[10, 0, 11, 1, 2]
>>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42, stopping_strategy="all_exhausted")
>>> dataset["a"]
[10, 0, 11, 1, 2, 20, 12, 13, ..., 0, 1, 2, 0, 24]
For datasets in streaming mode (iterable):

>>> from datasets import load_dataset, interleave_datasets
>>> d1 = load_dataset("oscar", "unshuffled_deduplicated_en", split="train", streaming=True)
>>> d2 = load_dataset("oscar", "unshuffled_deduplicated_fr", split="train", streaming=True)
>>> dataset = interleave_datasets([d1, d2])
>>> iterator = iter(dataset)
>>> next(iterator)
{'text': 'Mtendere Village was inspired by the vision...}
>>> next(iterator)
{'text': "Média de débat d'idées, de culture...}
```

#### `datasets.distributed.split_dataset_by_node`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/distributed.py#L10)

```py
( dataset: DatasetType rank: int world_size: int ) → export const metadata = 'undefined';Dataset or IterableDataset
```

参数

+   `dataset`（[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)或[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)） — 要按节点拆分的数据集。

+   `rank` (`int`) — 当前节点的等级。

+   `world_size` (`int`) — 节点的总数。

返回

数据集或IterableDataset

在等级`rank`的节点上要使用的数据集。

在节点池中将数据集拆分为等级`rank`的节点，节点池的大小为`world_size`。

对于映射样式数据集：

每个节点被分配一块数据，例如，rank 0被给予数据集的第一块。为了最大化数据加载吞吐量，如果可能的话，块是由磁盘上的连续数据组成的。

对于可迭代数据集：

如果数据集的分片数是`world_size`的因子（即如果`dataset.n_shards % world_size == 0`），那么分片将均匀分配给节点，这是最优化的。否则，每个节点保留`world_size`中的一个示例，跳过其他示例。

#### `datasets.enable_caching`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/fingerprint.py#L95)

```py
( )
```

在数据集上应用转换时，数据存储在缓存文件中。缓存机制允许在已经计算过的情况下重新加载现有的缓存文件。

重新加载数据集是可能的，因为缓存文件是使用数据集指纹命名的，每次转换后都会更新。

如果禁用，当对数据集应用转换时，库将不再重新加载缓存的数据集文件。更确切地说，如果禁用缓存：

+   缓存文件总是重新创建的

+   缓存文件被写入一个在会话关闭时被删除的临时目录

+   缓存文件是使用随机哈希而不是数据集指纹命名的

+   使用[save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk)来保存转换后的数据集，否则在会话关闭时将被删除。

+   缓存不会影响[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)。如果要从头开始重新生成数据集，应该使用[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)中的`download_mode`参数。

#### `datasets.disable_caching`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/fingerprint.py#L116)

```py
( )
```

在数据集上应用转换时，数据存储在缓存文件中。缓存机制允许在已经计算过的情况下重新加载现有的缓存文件。

重新加载数据集是可能的，因为缓存文件是使用数据集指纹命名的，每次转换后都会更新。

如果禁用，库将不再在对数据集应用转换时重新加载缓存的数据集文件。更准确地说，如果禁用缓存：

+   缓存文件总是重新创建

+   缓存文件写入临时目录，会话关闭时会被删除

+   缓存文件的命名使用随机哈希而不是数据集指纹

+   使用[save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk)来保存转换后的数据集，否则在会话关闭时将被删除

+   缓存不会影响[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)。如果要从头开始重新生成数据集，应该在[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)中使用`download_mode`参数。

#### `datasets.is_caching_enabled`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/fingerprint.py#L161)

```py
( )
```

在数据集上应用转换时，数据存储在缓存文件中。缓存机制允许在已经计算过的情况下重新加载现有的缓存文件。

重新加载数据集是可能的，因为缓存文件的命名使用数据集指纹，每次转换后都会更新。

如果禁用，库将不再在对数据集应用转换时重新加载缓存的数据集文件。更准确地说，如果禁用缓存：

+   缓存文件总是重新创建

+   缓存文件写入临时目录，会话关闭时会被删除

+   缓存文件的命名使用随机哈希而不是数据集指纹

+   使用[save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk)来保存转换后的数据集，否则在会话关闭时将被删除

+   缓存不会影响[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)。如果要从头开始重新生成数据集，应该在[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)中使用`download_mode`参数。

## DatasetDict

以拆分名称为键（例如‘train’、‘test’）和`Dataset`对象为值的字典。它还具有数据集转换方法，如map或filter，以一次处理所有拆分。

### `class datasets.DatasetDict`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L45)

```py
( )
```

一个字典（str: datasets.Dataset的字典），具有数据集转换方法（map、filter等）

#### `data`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L86)

```py
( )
```

支持每个拆分的Apache Arrow表。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> ds.data
```

#### `cache_files`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L101)

```py
( )
```

包含每个拆分的Apache Arrow表的缓存文件。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> ds.cache_files
{'test': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-test.arrow'}],
 'train': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-train.arrow'}],
 'validation': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-validation.arrow'}]}
```

#### `num_columns`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L119)

```py
( )
```

数据集每个拆分中的列数。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> ds.num_columns
{'test': 2, 'train': 2, 'validation': 2}
```

#### `num_rows`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L135)

```py
( )
```

数据集每个拆分的行数（与[datasets.Dataset.**len**()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.__len__)相同）。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> ds.num_rows
{'test': 1066, 'train': 8530, 'validation': 1066}
```

#### `column_names`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L151)

```py
( )
```

数据集每个拆分中的列名。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> ds.column_names
{'test': ['text', 'label'],
 'train': ['text', 'label'],
 'validation': ['text', 'label']}
```

#### `shape`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L169)

```py
( )
```

数据集每个拆分的形状（列数、行数）。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> ds.shape
{'test': (1066, 2), 'train': (8530, 2), 'validation': (1066, 2)}
```

#### `unique`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L217)

```py
( column: str ) → export const metadata = 'undefined';Dict[str, list]
```

参数

+   `column` (`str`) — 列名（使用 [column_names](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.column_names) 列出所有列名）

返回

Dict[`str`, `list`]

给定列中唯一元素的字典。

返回每个拆分中列的唯一元素列表。

这是在低级后端中实现的，因此非常快。

示例:

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> ds.unique("label")
{'test': [1, 0], 'train': [1, 0], 'validation': [1, 0]}
```

#### `cleanup_cache_files`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L241)

```py
( )
```

清理数据集缓存目录中的所有缓存文件，除了当前正在使用的缓存文件。在运行此命令时，请注意当前没有其他进程正在使用其他缓存文件。

示例:

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> ds.cleanup_cache_files()
{'test': 0, 'train': 0, 'validation': 0}
```

#### `map`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L764)

```py
( function: Optional = None with_indices: bool = False with_rank: bool = False input_columns: Union = None batched: bool = False batch_size: Optional = 1000 drop_last_batch: bool = False remove_columns: Union = None keep_in_memory: bool = False load_from_cache_file: Optional = None cache_file_names: Optional = None writer_batch_size: Optional = 1000 features: Optional = None disable_nullable: bool = False fn_kwargs: Optional = None num_proc: Optional = None desc: Optional = None )
```

参数

+   `function` (`callable`) — 其中之一的签名:

    +   `function(example: Dict[str, Any]) -> Dict[str, Any]` 如果 `batched=False` 和 `with_indices=False`

    +   `function(example: Dict[str, Any], indices: int) -> Dict[str, Any]` 如果 `batched=False` 和 `with_indices=True`

    +   `function(batch: Dict[str, List]) -> Dict[str, List]` 如果 `batched=True` 和 `with_indices=False`

    +   `function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List]` 如果 `batched=True` 和 `with_indices=True`

    对于高级用法，函数还可以返回一个 `pyarrow.Table`。此外，如果您的函数返回 `None`，那么 `map` 将运行您的函数并返回数据集不变。

+   `with_indices` (`bool`, 默认为 `False`) — 向 `function` 提供示例索引。请注意，在这种情况下，`function` 的签名应为 `def function(example, idx): ...`。

+   `with_rank` (`bool`, 默认为 `False`) — 向 `function` 提供进程等级。请注意，在这种情况下，`function` 的签名应为 `def function(example[, idx], rank): ...`。

+   `input_columns` (`[Union[str, List[str]]]`, *可选*, 默认为 `None`) — 要作为位置参数传递给 `function` 的列。如果为 `None`，则传递到所有格式化列的映射中。

+   `batched` (`bool`, 默认为 `False`) — 提供给 `function` 一批示例。

+   `batch_size` (`int`, *可选*, 默认为 `1000`) — 如果 `batched=True`，则提供给 `function` 的每批示例数，如果 `batch_size <= 0` 或 `batch_size == None`，则将整个数据集作为单个批次提供给 `function`。

+   `drop_last_batch` (`bool`, 默认为 `False`) — 是否应该丢弃小于 `batch_size` 的最后一批，而不是由函数处理。

+   `remove_columns` (`[Union[str, List[str]]]`, *可选*, 默认为 `None`) — 在映射时删除一些列。在使用 `function` 的输出更新示例之前，将删除列，即如果 `function` 添加了列名在 `remove_columns` 中的列，这些列将被保留。

+   `keep_in_memory` (`bool`, 默认为 `False`) — 将数据集保留在内存中，而不是写入缓存文件。

+   `load_from_cache_file` (`Optional[bool]`, 默认为 `True` 如果启用了缓存) — 如果可以识别到存储当前计算结果的缓存文件，就使用它而不是重新计算。

+   `cache_file_names` (`[Dict[str, str]]`, *可选*, 默认为 `None`) — 提供缓存文件的路径名称。它用于存储计算结果，而不是自动生成的缓存文件名。您必须为数据集字典中的每个数据集提供一个 `cache_file_name`。

+   `writer_batch_size` (`int`, 默认为 `1000`) — 缓存文件写入操作的每行行数。这个值在处理过程中的内存使用和处理速度之间取得了很好的平衡。较高的值使处理过程减少查找次数，较低的值在运行 `map` 时消耗更少的临时内存。

+   `features` (`[datasets.Features]`, *optional*, 默认为 `None`) — 使用特定的 [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features) 来存储缓存文件，而不是自动生成的特征。

+   `disable_nullable` (`bool`, 默认为 `False`) — 在表中禁止空值。

+   `fn_kwargs` (`Dict`, *optional*, 默认为 `None`) — 要传递给 `function` 的关键字参数

+   `num_proc` (`int`, *optional*, 默认为 `None`) — 用于多进程的进程数。默认情况下，不使用多进程。

+   `desc` (`str`, *optional*, 默认为 `None`) — 在映射示例时显示在进度条旁边的有意义的描述。

对表中的所有元素（单独或批量）应用函数，并更新表（如果函数更新了示例）。该转换应用于数据集字典的所有数据集。 

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> def add_prefix(example):
...     example["text"] = "Review: " + example["text"]
...     return example
>>> ds = ds.map(add_prefix)
>>> ds["train"][0:3]["text"]
['Review: the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',
 'Review: the gorgeously elaborate continuation of " the lord of the rings " trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .',
 'Review: effective but too-tepid biopic']

# process a batch of examples
>>> ds = ds.map(lambda example: tokenizer(example["text"]), batched=True)
# set number of processors
>>> ds = ds.map(add_prefix, num_proc=4)
```

#### `filter`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L892)

```py
( function: Optional = None with_indices: bool = False with_rank: bool = False input_columns: Union = None batched: bool = False batch_size: Optional = 1000 keep_in_memory: bool = False load_from_cache_file: Optional = None cache_file_names: Optional = None writer_batch_size: Optional = 1000 fn_kwargs: Optional = None num_proc: Optional = None desc: Optional = None )
```

参数

+   `function` (`Callable`) — 具有以下签名之一的可调用函数：

    +   `function(example: Dict[str, Any]) -> bool` 如果 `batched=False` 且 `with_indices=False` 且 `with_rank=False`

    +   `function(example: Dict[str, Any], *extra_args) -> bool` 如果 `batched=False` 且 `with_indices=True` 和/或 `with_rank=True`（每个额外参数一个）

    +   `function(batch: Dict[str, List]) -> List[bool]` 如果 `batched=True` 且 `with_indices=False` 且 `with_rank=False`

    +   `function(batch: Dict[str, List], *extra_args) -> List[bool]` 如果 `batched=True` 且 `with_indices=True` 和/或 `with_rank=True`（每个额外参数一个）

    如果没有提供函数，则默认为始终返回 `True` 的函数：`lambda x: True`。

+   `with_indices` (`bool`, 默认为 `False`) — 为 `function` 提供示例索引。请注意，在这种情况下，`function` 的签名应为 `def function(example, idx[, rank]): ...`。

+   `with_rank` (`bool`, 默认为 `False`) — 为 `function` 提供进程排名。请注意，在这种情况下，`function` 的签名应为 `def function(example[, idx], rank): ...`。

+   `input_columns` (`[Union[str, List[str]]]`, *optional*, 默认为 `None`) — 要作为位置参数传递给 `function` 的列。如果为 `None`，则作为一个参数传递所有格式化的列的映射。

+   `batched` (`bool`, 默认为 `False`) — 为 `function` 提供示例批次。

+   `batch_size` (`int`, *optional*, 默认为 `1000`) — 如果 `batched=True`，则提供给 `function` 的每个批次的示例数。如果 `batch_size <= 0` 或 `batch_size == None`，则将整个数据集作为单个批次提供给 `function`。

+   `keep_in_memory` (`bool`, 默认为 `False`) — 将数据集保留在内存中，而不是将其写入缓存文件。

+   `load_from_cache_file` (`Optional[bool]`, 默认为 `True` 如果启用了缓存) — 如果可以识别存储当前计算结果的缓存文件，则使用它，而不是重新计算。

+   `cache_file_names` (`[Dict[str, str]]`, *optional*, 默认为 `None`) — 提供缓存文件路径的名称。它用于存储计算结果，而不是自动生成的缓存文件名。您必须为数据集字典中的每个数据集提供一个 `cache_file_name`。

+   `writer_batch_size` (`int`, 默认为 `1000`) — 缓存文件写入操作的每行行数。这个值在处理过程中的内存使用和处理速度之间取得了很好的平衡。较高的值使处理过程中的查找次数减少，较低的值在运行 `map` 时消耗较少的临时内存。

+   `fn_kwargs` (`Dict`, *optional*, 默认为 `None`) — 要传递给 `function` 的关键字参数

+   `num_proc` (`int`, *optional*, 默认为 `None`) — 用于多进程的进程数。默认情况下，不使用多进程。

+   `desc` (`str`, *optional*, 默认为 `None`) — 在过滤示例时显示在进度条旁边的有意义的描述。

对表中的所有元素应用批处理过滤函数，并更新表，以便数据集仅包含符合过滤函数的示例。该转换应用于数据集字典的所有数据集。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> ds.filter(lambda x: x["label"] == 1)
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 4265
    })
    validation: Dataset({
        features: ['text', 'label'],
        num_rows: 533
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 533
    })
})
```

#### `sort`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1054)

```py
( column_names: Union reverse: Union = False kind = 'deprecated' null_placement: str = 'at_end' keep_in_memory: bool = False load_from_cache_file: Optional = None indices_cache_file_names: Optional = None writer_batch_size: Optional = 1000 )
```

参数

+   `column_names` (`Union[str, Sequence[str]]`) — 要按其排序的列名。

+   `reverse` (`Union[bool, Sequence[bool]]`, defaults to `False`) — 如果为`True`，则按降序而不是升序排序。如果提供单个布尔值，则该值将应用于所有列名的排序。否则，必须提供与`column_names`长度和顺序相同的布尔值列表。

+   `kind` (`str`, *optional*) — Pandas用于排序的算法，可选择`{quicksort, mergesort, heapsort, stable}`，默认为`quicksort`。请注意，`stable`和`mergesort`都在底层使用timsort，通常实际实现会随数据类型而变化。`mergesort`选项保留了向后兼容性。

    在2.8.0中已弃用

    `kind`在版本2.10.0中已弃用，并将在3.0.0中删除。

+   `null_placement` (`str`, defaults to `at_end`) — 如果为`at_start`或`first`，则将`None`值放在开头，如果为`at_end`或`last`，则将其放在末尾

+   `keep_in_memory` (`bool`, defaults to `False`) — 将排序后的索引保留在内存中，而不是写入缓存文件。

+   `load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is enabled) — 如果可以识别出存储排序后索引的缓存文件，则使用它而不是重新计算。

+   `indices_cache_file_names` (`[Dict[str, str]]`, *optional*, defaults to `None`) — 提供缓存文件的路径名称。用于存储索引映射，而不是自动生成的缓存文件名。您必须为数据集字典中的每个数据集提供一个`cache_file_name`。

+   `writer_batch_size` (`int`, defaults to `1000`) — 每次写操作的行数，用于缓存文件写入器。较高的值会生成较小的缓存文件，较低的值会消耗较少的临时内存。

创建一个根据单个或多个列排序的新数据集。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset('rotten_tomatoes')
>>> ds['train']['label'][:10]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
>>> sorted_ds = ds.sort('label')
>>> sorted_ds['train']['label'][:10]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
>>> another_sorted_ds = ds.sort(['label', 'text'], reverse=[True, False])
>>> another_sorted_ds['train']['label'][:10]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

#### `shuffle`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1132)

```py
( seeds: Union = None seed: Optional = None generators: Optional = None keep_in_memory: bool = False load_from_cache_file: Optional = None indices_cache_file_names: Optional = None writer_batch_size: Optional = 1000 )
```

参数

+   `seeds` (`Dict[str, int]` or `int`, *optional*) — 用于初始化默认BitGenerator的种子，如果`generator=None`。如果为`None`，则将从操作系统中获取新鲜、不可预测的熵。如果传递了一个`int`或`array_like[ints]`，则将传递给SeedSequence以推导初始BitGenerator状态。您可以为数据集字典中的每个数据集提供一个`seed`。

+   `seed` (`int`, *optional*) — 用于初始化默认BitGenerator的种子，如果`generator=None`。别名为seeds（如果两者都提供，则会引发`ValueError`）。

+   `generators` (`Dict[str, *optional*, np.random.Generator]`) — 用于计算数据集行的排列的Numpy随机生成器。如果`generator=None`（默认），则使用`np.random.default_rng`（NumPy的默认BitGenerator（PCG64））。您必须为数据集字典中的每个数据集提供一个`generator`。

+   `keep_in_memory` (`bool`, defaults to `False`) — 将数据集保留在内存中，而不是写入缓存文件。

+   `load_from_cache_file` (`Optional[bool]`, defaults to `True` if caching is enabled) — 如果可以识别出存储当前计算结果的缓存文件，就使用它而不是重新计算。

+   `indices_cache_file_names` (`Dict[str, str]`, *optional*) — 提供缓存文件的路径名称。用于存储索引映射，而不是自动生成的缓存文件名。您必须为数据集字典中的每个数据集提供一个`cache_file_name`。

+   `writer_batch_size`（`int`，默认为`1000`）—缓存文件写入操作的每行行数。此值在处理过程中的内存使用和处理速度之间取得了良好的折衷。较高的值使处理执行更少的查找，较低的值在运行`map`时消耗更少的临时内存。

创建一个新的数据集，其中的行被洗牌。

该转换应用于数据集字典中的所有数据集。

当前的洗牌使用numpy随机生成器。您可以提供一个NumPy BitGenerator来使用，或者提供一个种子来启动NumPy的默认随机生成器（PCG64）。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> ds["train"]["label"][:10]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]

# set a seed
>>> shuffled_ds = ds.shuffle(seed=42)
>>> shuffled_ds["train"]["label"][:10]
[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]
```

#### `set_format`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L556)

```py
( type: Optional = None columns: Optional = None output_all_columns: bool = False **format_kwargs )
```

参数

+   `type`（`str`，*可选*）—在`[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`中选择的输出类型。`None`表示`__getitem__`返回python对象（默认）。

+   `columns`（`List[str]`，*可选*）—要在输出中格式化的列。`None`表示`__getitem__`返回所有列（默认）。

+   `output_all_columns`（`bool`，默认为`False`）—在输出中也保留未格式化的列（作为python对象），

+   *`*format_kwargs`（额外的关键字参数）—传递给转换函数的关键字参数，如`np.array`，`torch.tensor`或`tensorflow.ragged.constant`。

设置`__getitem__`返回格式（类型和列）。为数据集字典中的每个数据集设置格式。

在调用`set_format`之后可以调用`map`。由于`map`可能会添加新列，因此格式化列的列表会得到更新。在这种情况下，如果您在数据集上应用`map`来添加新列，则此列将被格式化：

`新格式化列 =（所有列 - 以前未格式化的列）`

示例：

```py
>>> from datasets import load_dataset
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
>>> ds = ds.map(lambda x: tokenizer(x["text"], truncation=True, padding=True), batched=True)
>>> ds.set_format(type="numpy", columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])
>>> ds["train"].format
{'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],
 'format_kwargs': {},
 'output_all_columns': False,
 'type': 'numpy'}
```

#### `reset_format`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L602)

```py
( )
```

将`__getitem__`返回格式重置为python对象和所有列。该转换应用于数据集字典中的所有数据集。

与`self.set_format()`相同

示例：

```py
>>> from datasets import load_dataset
>>> from transformers import AutoTokenizer
>>> ds = load_dataset("rotten_tomatoes")
>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
>>> ds = ds.map(lambda x: tokenizer(x["text"], truncation=True, padding=True), batched=True)
>>> ds.set_format(type="numpy", columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])
>>> ds["train"].format
{'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],
 'format_kwargs': {},
 'output_all_columns': False,
 'type': 'numpy'}
>>> ds.reset_format()
>>> ds["train"].format
{'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],
 'format_kwargs': {},
 'output_all_columns': False,
 'type': None}
```

#### `formatted_as`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L519)

```py
( type: Optional = None columns: Optional = None output_all_columns: bool = False **format_kwargs )
```

参数

+   `type`（`str`，*可选*）—在`[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`中选择的输出类型。`None`表示`__getitem__`返回python对象（默认）。

+   `columns`（`List[str]`，*可选*）—要在输出中格式化的列。`None`表示`__getitem__`返回所有列（默认）。

+   `output_all_columns`（`bool`，默认为`False`）—在输出中也保留未格式化的列（作为python对象）。

+   *`*format_kwargs`（额外的关键字参数）—传递给转换函数的关键字参数，如`np.array`，`torch.tensor`或`tensorflow.ragged.constant`。

在`with`语句中使用。设置`__getitem__`返回格式（类型和列）。该转换应用于数据集字典中的所有数据集。

#### `with_format`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L658)

```py
( type: Optional = None columns: Optional = None output_all_columns: bool = False **format_kwargs )
```

参数

+   `type`（`str`，*可选*）—在`[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`中选择的输出类型。`None`表示`__getitem__`返回python对象（默认）。

+   `columns`（`List[str]`，*可选*）—要在输出中格式化的列。`None`表示`__getitem__`返回所有列（默认）。

+   `output_all_columns`（`bool`，默认为`False`）—在输出中也保留未格式化的列（作为python对象）。

+   *`*format_kwargs`（额外的关键字参数）—传递给转换函数的关键字参数，如`np.array`，`torch.tensor`或`tensorflow.ragged.constant`。

设置`__getitem__`返回格式（类型和列）。数据格式化是实时应用的。当使用`__getitem__`时，格式`type`（例如“numpy”）用于格式化批次。为数据集字典中的每个数据集设置格式。

也可以使用自定义转换来进行格式化，使用[with_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.with_transform)。

与[set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict.set_format)相反，`with_format`返回一个带有新的[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)对象的新的[DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)对象。

示例：

```py
>>> from datasets import load_dataset
>>> from transformers import AutoTokenizer
>>> ds = load_dataset("rotten_tomatoes")
>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
>>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)
>>> ds["train"].format
{'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],
 'format_kwargs': {},
 'output_all_columns': False,
 'type': None}
>>> ds = ds.with_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])
>>> ds["train"].format
{'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],
 'format_kwargs': {},
 'output_all_columns': False,
 'type': 'tensorflow'}
```

#### `with_transform`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L710)

```py
( transform: Optional columns: Optional = None output_all_columns: bool = False )
```

参数

+   `transform`（`Callable`，*可选*） - 用户定义的格式化转换，替换由[set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format)定义的格式。格式化函数是一个可调用的函数，它以批处理（作为字典）作为输入并返回批处理。此函数在在`__getitem__`中返回对象之前应用。

+   `columns`（`List[str]`，*可选*） - 要在输出中格式化的列。如果指定，则转换的输入批次仅包含这些列。

+   `output_all_columns`（`bool`，默认为False） - 在输出中保留未格式化的列（作为Python对象）。如果设置为`True`，则其他未格式化的列将与转换的输出一起保留。

使用此转换设置`__getitem__`返回格式。当调用`__getitem__`时，转换将在批次上动态应用。转换为数据集字典中的每个数据集设置。

与[set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format)一样，可以使用[reset_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.reset_format)来重置。

与`set_transform()`相反，`with_transform`返回一个带有新的[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)对象的新的[DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)对象。

示例：

```py
>>> from datasets import load_dataset
>>> from transformers import AutoTokenizer
>>> ds = load_dataset("rotten_tomatoes")
>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
>>> def encode(example):
...     return tokenizer(example['text'], truncation=True, padding=True, return_tensors="pt")
>>> ds = ds.with_transform(encode)
>>> ds["train"][0]
{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
 1, 1, 1, 1, 1, 1, 1, 1, 1]),
 'input_ids': tensor([  101,  1103,  2067,  1110, 17348,  1106,  1129,  1103,  6880,  1432,
        112,   188,  1207,   107, 14255,  1389,   107,  1105,  1115,  1119,
        112,   188,  1280,  1106,  1294,   170, 24194,  1256,  3407,  1190,
        170, 11791,  5253,   188,  1732,  7200, 10947, 12606,  2895,   117,
        179,  7766,   118,   172, 15554,  1181,  3498,  6961,  3263,  1137,
        188,  1566,  7912, 14516,  6997,   119,   102]),
 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0])}
```

#### `flatten`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L185)

```py
( max_depth = 16 )
```

展平每个拆分的Apache Arrow表（嵌套特征被展平）。具有结构类型的列被展平为每个结构字段的一列。其他列保持不变。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("squad")
>>> ds["train"].features
{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),
 'context': Value(dtype='string', id=None),
 'id': Value(dtype='string', id=None),
 'question': Value(dtype='string', id=None),
 'title': Value(dtype='string', id=None)}
>>> ds.flatten()
DatasetDict({
    train: Dataset({
        features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],
        num_rows: 87599
    })
    validation: Dataset({
        features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],
        num_rows: 10570
    })
})
```

#### `cast`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L265)

```py
( features: Features )
```

参数

+   `features`（[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)） - 要将数据集转换为的新特征。特征中字段的名称和顺序必须与当前列名匹配。数据的类型也必须可以从一种类型转换为另一种类型。对于非平凡的转换，例如`string` <-> `ClassLabel`，您应该使用[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)来更新数据集。

将数据集转换为一组新特征。转换应用于数据集字典中的所有数据集。

您还可以使用[Dataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)和`feature`来删除列，但`cast`是原地操作（不会将数据复制到新数据集），因此更快。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> ds["train"].features
{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),
 'text': Value(dtype='string', id=None)}
>>> new_features = ds["train"].features.copy()
>>> new_features['label'] = ClassLabel(names=['bad', 'good'])
>>> new_features['text'] = Value('large_string')
>>> ds = ds.cast(new_features)
>>> ds["train"].features
{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),
 'text': Value(dtype='large_string', id=None)}
```

#### `cast_column`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L300)

```py
( column: str feature )
```

参数

+   `column`（`str`） - 列名。

+   `feature`（`Feature`） - 目标特征。

将列转换为特征以进行解码。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> ds["train"].features
{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),
 'text': Value(dtype='string', id=None)}
>>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))
>>> ds["train"].features
{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),
 'text': Value(dtype='string', id=None)}
```

#### `remove_columns`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L329)

```py
( column_names: Union )
```

参数

+   `column_names`（`Union[str, List[str]]`） - 要删除的列的名称。

从数据集中的每个拆分中删除一个或多个列以及与列相关联的特征。

该转换应用于数据集字典的所有拆分。

您还可以使用[Dataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)和`remove_columns`删除列，但当前的方法是原地操作（不会将数据复制到新数据集中），因此更快。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> ds.remove_columns("label")
DatasetDict({
    train: Dataset({
        features: ['text'],
        num_rows: 8530
    })
    validation: Dataset({
        features: ['text'],
        num_rows: 1066
    })
    test: Dataset({
        features: ['text'],
        num_rows: 1066
    })
})
```

#### `rename_column`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L368)

```py
( original_column_name: str new_column_name: str )
```

参数

+   `original_column_name`（`str`）—要重命名的列的名称。

+   `new_column_name`（`str`）—列的新名称。

重命名数据集中的列，并将与原始列相关联的特征移至新列名下。该转换应用于数据集字典的所有数据集。

您还可以使用[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)和`remove_columns`重命名列，但当前的方法如下：

+   负责将原始特征移至新列名下。

+   不会将数据复制到新数据集中，因此速度更快。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> ds.rename_column("label", "label_new")
DatasetDict({
    train: Dataset({
        features: ['text', 'label_new'],
        num_rows: 8530
    })
    validation: Dataset({
        features: ['text', 'label_new'],
        num_rows: 1066
    })
    test: Dataset({
        features: ['text', 'label_new'],
        num_rows: 1066
    })
})
```

#### `rename_columns`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L413)

```py
( column_mapping: Dict ) → export const metadata = 'undefined';DatasetDict
```

参数

+   `column_mapping`（`Dict[str, str]`）—要重命名为其新名称的列的映射。

返回

[DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)

具有重命名列的数据集副本。

重命名数据集中的多个列，并将与原始列相关联的特征移至新列名下。该转换应用于数据集字典的所有数据集。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> ds.rename_columns({'text': 'text_new', 'label': 'label_new'})
DatasetDict({
    train: Dataset({
        features: ['text_new', 'label_new'],
        num_rows: 8530
    })
    validation: Dataset({
        features: ['text_new', 'label_new'],
        num_rows: 1066
    })
    test: Dataset({
        features: ['text_new', 'label_new'],
        num_rows: 1066
    })
})
```

#### `select_columns`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L451)

```py
( column_names: Union )
```

参数

+   `column_names`（`Union[str, List[str]]`）—要保留的列的名称。

从数据集中的每个拆分中选择一个或多个列，并选择与这些列相关联的特征。

该转换应用于数据集字典的所有拆分。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes")
>>> ds.select_columns("text")
DatasetDict({
    train: Dataset({
        features: ['text'],
        num_rows: 8530
    })
    validation: Dataset({
        features: ['text'],
        num_rows: 1066
    })
    test: Dataset({
        features: ['text'],
        num_rows: 1066
    })
})
```

#### `class_encode_column`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L487)

```py
( column: str include_nulls: bool = False )
```

参数

+   `column`（`str`）—要转换的列的名称。

+   `include_nulls`（`bool`，默认为`False`）—是否在类标签中包含空值。如果为`True`，则空值将被编码为`"None"`类标签。

    添加于1.14.2

将给定列转换为[ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)并更新表格。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("boolq")
>>> ds["train"].features
{'answer': Value(dtype='bool', id=None),
 'passage': Value(dtype='string', id=None),
 'question': Value(dtype='string', id=None)}
>>> ds = ds.class_encode_column("answer")
>>> ds["train"].features
{'answer': ClassLabel(num_classes=2, names=['False', 'True'], id=None),
 'passage': Value(dtype='string', id=None),
 'question': Value(dtype='string', id=None)}
```

#### `push_to_hub`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1564)

```py
( repo_id config_name: str = 'default' set_default: Optional = None data_dir: Optional = None commit_message: Optional = None commit_description: Optional = None private: Optional = False token: Optional = None revision: Optional = None branch = 'deprecated' create_pr: Optional = False max_shard_size: Union = None num_shards: Optional = None embed_external_files: bool = True )
```

参数

+   `repo_id`（`str`）—要推送到的存储库的ID格式如下：`<user>/<dataset_name>`或`<org>/<dataset_name>`。还接受`<dataset_name>`，它将默认为已登录用户的命名空间。

+   `config_name`（`str`）—数据集的配置名称。默认为“default”。

+   `set_default`（`bool`，*可选*）—是否将此配置设置为默认配置。否则，默认配置为名为“default”的配置。

+   `data_dir`（`str`，*可选*）—将包含上传数据文件的目录名称。如果与“default”不同，则默认为`config_name`，否则为“data”。

    添加于2.17.0

+   `commit_message`（`str`，*可选*）—在推送时要提交的消息。默认为`"Upload dataset"`。

+   `commit_description`（`str`，*可选*）—将创建的提交的描述。此外，如果创建了PR（`create_pr`为True），还会是PR的描述。

    添加于2.16.0

+   `private`（`bool`，*可选*）—数据集存储库是否应设置为私有。仅影响存储库创建：已存在的存储库不会受到该参数的影响。

+   `token` (`str`, *可选*) — 用于Hugging Face Hub的可选身份验证令牌。如果未传递令牌，将默认使用在`huggingface-cli login`登录时本地保存的令牌。如果未传递令牌且用户未登录，则会引发错误。

+   `revision` (`str`, *可选*) — 要推送上传文件的分支。默认为`"main"`分支。

    在2.15.0中添加

+   `branch` (`str`, *可选*) — 要推送数据集的git分支。默认为存储库中指定的默认分支，即`"main"`。

    在2.15.0中弃用

    `branch`在版本2.15.0中已弃用，改用`revision`，并将在3.0.0中删除。

+   `create_pr` (`bool`, *可选*, 默认为`False`) — 是否创建一个带有上传文件的PR或直接提交。

    在2.15.0中添加

+   `max_shard_size` (`int`或`str`, *可选*, 默认为`"500MB"`) — 要上传到hub的数据集分片的最大大小。如果表示为字符串，需要是数字后跟一个单位（如`"500MB"`或`"1GB"`）。

+   `num_shards` (`Dict[str, int]`, *可选*) — 要写入的分片数。默认情况下，分片数取决于`max_shard_size`。使用字典为每个拆分定义不同的num_shards。

    在2.8.0中添加

+   `embed_external_files` (`bool`, 默认为`True`) — 是否在分片中嵌入文件字节。特别是，在推送之前，将对类型为的字段执行以下操作：

    +   [音频](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)和[图像](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)会删除本地路径信息，并将文件内容嵌入Parquet文件中。

将[DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)作为Parquet数据集推送到hub。[DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)使用HTTP请求推送，不需要安装git或git-lfs。

每个数据集拆分将独立推送。推送的数据集将保留原始拆分名称。

默认情况下，生成的Parquet文件是自包含的：如果数据集包含[图像](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)或[音频](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)数据，Parquet文件将存储图像或音频文件的字节。您可以通过将`embed_external_files`设置为False来禁用此功能。

示例：

```py
>>> dataset_dict.push_to_hub("<organization>/<dataset_id>")
>>> dataset_dict.push_to_hub("<organization>/<dataset_id>", private=True)
>>> dataset_dict.push_to_hub("<organization>/<dataset_id>", max_shard_size="1GB")
>>> dataset_dict.push_to_hub("<organization>/<dataset_id>", num_shards={"train": 1024, "test": 8})
```

如果要向数据集添加新的配置（或子集）（例如，如果数据集有多个任务/版本/语言）：

```py
>>> english_dataset.push_to_hub("<organization>/<dataset_id>", "en")
>>> french_dataset.push_to_hub("<organization>/<dataset_id>", "fr")
>>> # later
>>> english_dataset = load_dataset("<organization>/<dataset_id>", "en")
>>> french_dataset = load_dataset("<organization>/<dataset_id>", "fr")
```

#### `save_to_disk`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1215)

```py
( dataset_dict_path: Union fs = 'deprecated' max_shard_size: Union = None num_shards: Optional = None num_proc: Optional = None storage_options: Optional = None )
```

参数

+   `dataset_dict_path` (`str`) — 数据集字典将保存到的路径（例如`dataset/train`）或远程URI（例如`s3://my-bucket/dataset/train`）。

+   `fs` (`fsspec.spec.AbstractFileSystem`, *可选*) — 要保存数据集的远程文件系统的实例。

    在2.8.0中弃用

    `fs`在版本2.8.0中已弃用，并将在3.0.0中删除。请改用`storage_options`，例如`storage_options=fs.storage_options`

+   `max_shard_size` (`int`或`str`, *可选*, 默认为`"500MB"`) — 要上传到hub的数据集分片的最大大小。如果表示为字符串，需要是数字后跟一个单位（如`"50MB"`）。

+   `num_shards` (`Dict[str, int]`, *可选*) — 要写入的分片数。默认情况下，分片数取决于`max_shard_size`和`num_proc`。您需要为数据集字典中的每个数据集提供分片数。使用字典为每个拆分定义不同的num_shards。

    在2.8.0中添加

+   `num_proc` (`int`, *可选*, 默认为`None`) — 在本地下载和生成数据集时的进程数。默认情况下禁用多进程。

    在2.8.0中添加

+   `storage_options`（`dict`，*可选*）— 要传递给文件系统后端的键/值对。

    在2.8.0中添加

使用`fsspec.spec.AbstractFileSystem`将数据集字典保存到文件系统。

对于[Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)和[Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)数据：

所有的Image()和Audio()数据都存储在arrow文件中。如果要存储路径或URL，请使用Value(“string”)类型。

示例：

```py
>>> dataset_dict.save_to_disk("path/to/dataset/directory")
>>> dataset_dict.save_to_disk("path/to/dataset/directory", max_shard_size="1GB")
>>> dataset_dict.save_to_disk("path/to/dataset/directory", num_shards={"train": 1024, "test": 8})
```

#### `load_from_disk`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1305)

```py
( dataset_dict_path: Union fs = 'deprecated' keep_in_memory: Optional = None storage_options: Optional = None )
```

参数

+   `dataset_dict_path`（`str`）— 数据集字典目录的路径（例如`"dataset/train"`）或远程URI（例如`"s3//my-bucket/dataset/train"`），数据集字典将从中加载。

+   `fs`（`fsspec.spec.AbstractFileSystem`，*可选*）— 数据集将保存到的远程文件系统的实例。

    在2.8.0中弃用

    `fs`在版本2.8.0中已弃用，并将在3.0.0中删除。请改用`storage_options`，例如`storage_options=fs.storage_options`

+   `keep_in_memory`（`bool`，默认为`None`）— 是否在内存中复制数据集。如果为`None`，除非通过将`datasets.config.IN_MEMORY_MAX_SIZE`设置为非零来显式启用，否则数据集将不会在内存中复制。在[提高性能](../cache#improve-performance)部分中查看更多细节。

+   `storage_options`（`dict`，*可选*）— 要传递给文件系统后端的键/值对。

    在2.8.0中添加

从文件系统使用`fsspec.spec.AbstractFileSystem`加载先前使用`save_to_disk`保存的数据集。

示例：

```py
>>> ds = load_from_disk('path/to/dataset/directory')
```

#### `from_csv`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1382)

```py
( path_or_paths: Dict features: Optional = None cache_dir: str = None keep_in_memory: bool = False **kwargs )
```

参数

+   `path_or_paths`（`dict`的路径类）— CSV文件的路径。

+   `features`（[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)，*可选*）— 数据集特征。

+   `cache_dir`（str，*可选*，默认为`"~/.cache/huggingface/datasets"`）— 缓存数据的目录。

+   `keep_in_memory`（`bool`，默认为`False`）— 是否在内存中复制数据。

+   *`*kwargs`（额外的关键字参数）— 要传递给`pandas.read_csv`的关键字参数。

从CSV文件(s)创建[DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)。

示例：

```py
>>> from datasets import DatasetDict
>>> ds = DatasetDict.from_csv({'train': 'path/to/dataset.csv'})
```

#### `from_json`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1421)

```py
( path_or_paths: Dict features: Optional = None cache_dir: str = None keep_in_memory: bool = False **kwargs )
```

参数

+   `path_or_paths`（`path-like`或`path-like`列表）— JSON Lines文件的路径。

+   `features`（[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)，*可选*）— 数据集特征。

+   `cache_dir`（str，*可选*，默认为`"~/.cache/huggingface/datasets"`）— 缓存数据的目录。

+   `keep_in_memory`（`bool`，默认为`False`）— 是否在内存中复制数据。

+   *`*kwargs`（额外的关键字参数）— 要传递给`JsonConfig`的关键字参数。

从JSON Lines文件(s)创建[DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)。

示例：

```py
>>> from datasets import DatasetDict
>>> ds = DatasetDict.from_json({'train': 'path/to/dataset.json'})
```

#### `from_parquet`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1460)

```py
( path_or_paths: Dict features: Optional = None cache_dir: str = None keep_in_memory: bool = False columns: Optional = None **kwargs )
```

参数

+   `path_or_paths`（`dict`的路径类）— CSV文件的路径。

+   `features`（[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)，*可选*）— 数据集特征。

+   `cache_dir`（`str`，*可选*，默认为`"~/.cache/huggingface/datasets"`）— 缓存数据的目录。

+   `keep_in_memory`（`bool`，默认为`False`）— 是否在内存中复制数据。

+   `columns`（`List[str]`，*可选*）— 如果不为`None`，则只会从文件中读取这些列。列名可能是嵌套字段的前缀，例如‘a’将选择‘a.b’，‘a.c’和‘a.d.e’。

+   *`*kwargs`（额外的关键字参数） — 要传递给`ParquetConfig`的关键字参数。

从Parquet文件创建[DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)。

示例：

```py
>>> from datasets import DatasetDict
>>> ds = DatasetDict.from_parquet({'train': 'path/to/dataset/parquet'})
```

#### `from_text`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1509)

```py
( path_or_paths: Dict features: Optional = None cache_dir: str = None keep_in_memory: bool = False **kwargs )
```

参数

+   `path_or_paths`（`dict` of path-like） — 文本文件的路径。

+   `features`（[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)，*可选*） — 数据集特征。

+   `cache_dir`（`str`，*可选*，默认为`"~/.cache/huggingface/datasets"`） — 缓存数据的目录。

+   `keep_in_memory`（`bool`，默认为`False`） — 是否将数据复制到内存中。

+   *`*kwargs`（额外的关键字参数） — 要传递给`TextConfig`的关键字参数。

从文本文件创建[DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)。

示例：

```py
>>> from datasets import DatasetDict
>>> ds = DatasetDict.from_text({'train': 'path/to/dataset.txt'})
```

#### `prepare_for_task`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1548)

```py
( task: Union id: int = 0 )
```

参数

+   `task`（`Union[str, TaskTemplate]`） — 在训练和评估期间为数据集准备的任务。如果是`str`，支持的任务包括：

    +   `"text-classification"`

    +   `"question-answering"`

    如果是`TaskTemplate`，则必须是[`datasets.tasks`](./task_templates)中的任务模板之一。

+   `id`（`int`，默认为`0`） — 在支持多个相同类型的任务模板时，用于明确标识任务模板所需的ID。

通过将数据集的[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)转换为`datasets.tasks`中详细说明的标准化列名和类型，为给定任务准备数据集。

根据特定任务模式对`datasets.DatasetInfo.features`进行转换。仅用于一次性使用，因此在转换后，所有任务模板都会从`datasets.DatasetInfo.task_templates`中删除。

## IterableDataset

基类[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)实现了由Python生成器支持的可迭代数据集。

### `class datasets.IterableDataset`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1191)

```py
( ex_iterable: _BaseExamplesIterable info: Optional = None split: Optional = None formatting: Optional = None shuffling: Optional = None distributed: Optional = None token_per_repo_id: Optional = None format_type = 'deprecated' )
```

由可迭代对象支持的数据集。

#### `from_generator`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1439)

```py
( generator: Callable features: Optional = None gen_kwargs: Optional = None ) → export const metadata = 'undefined';IterableDataset
```

参数

+   `generator`（`Callable`） — 生成示例的生成器函数。

+   `features`（`Features`，*可选*） — 数据集特征。

+   `gen_kwargs(dict,` *可选*) — 要传递给`generator`可调用的关键字参数。您可以通过在`gen_kwargs`中传递分片列表来定义一个分片可迭代数据集。这可以用于改善洗牌和在使用多个工作进程迭代数据集时。

返回

`IterableDataset`

从生成器创建可迭代数据集。

示例：

```py
>>> def gen():
...     yield {"text": "Good", "label": 0}
...     yield {"text": "Bad", "label": 1}
...
>>> ds = IterableDataset.from_generator(gen)
```

```py
>>> def gen(shards):
...     for shard in shards:
...         with open(shard) as f:
...             for line in f:
...                 yield {"line": line}
...
>>> shards = [f"data{i}.txt" for i in range(32)]
>>> ds = IterableDataset.from_generator(gen, gen_kwargs={"shards": shards})
>>> ds = ds.shuffle(seed=42, buffer_size=10_000)  # shuffles the shards order + uses a shuffle buffer
>>> from torch.utils.data import DataLoader
>>> dataloader = DataLoader(ds.with_format("torch"), num_workers=4)  # give each worker a subset of 32/4=8 shards
```

#### `remove_columns`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1999)

```py
( column_names: Union ) → export const metadata = 'undefined';IterableDataset
```

参数

+   `column_names`（`Union[str, List[str]]`） — 要删除的列的名称。

返回

`IterableDataset`

没有要删除的列的数据集对象的副本。

从数据集中删除一个或多个列及其相关的特征。在迭代数据集时，对示例进行即时删除。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="train", streaming=True)
>>> next(iter(ds))
{'text': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}
>>> ds = ds.remove_columns("label")
>>> next(iter(ds))
{'text': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}
```

#### `select_columns`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L2039)

```py
( column_names: Union ) → export const metadata = 'undefined';IterableDataset
```

参数

+   `column_names`（`Union[str, List[str]]`） — 要选择的列的名称。

返回

`IterableDataset`

具有选定列的数据集对象的副本。

在数据集中选择一个或多个列及其相关的特征。在迭代数据集时，对示例进行即时选择。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="train", streaming=True)
>>> next(iter(ds))
{'text': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}
>>> ds = ds.select_columns("text")
>>> next(iter(ds))
{'text': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}
```

#### `cast_column`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L2095)

```py
( column: str feature: Union ) → export const metadata = 'undefined';IterableDataset
```

参数

+   `column`（`str`）- 列名。

+   `feature`（`Feature`）- 目标特征。

返回

`IterableDataset`

将列转换为特征以进行解码。

示例：

```py
>>> from datasets import load_dataset, Audio
>>> ds = load_dataset("PolyAI/minds14", name="en-US", split="train", streaming=True)
>>> ds.features
{'audio': Audio(sampling_rate=8000, mono=True, decode=True, id=None),
 'english_transcription': Value(dtype='string', id=None),
 'intent_class': ClassLabel(num_classes=14, names=['abroad', 'address', 'app_error', 'atm_limit', 'balance', 'business_loan',  'card_issues', 'cash_deposit', 'direct_debit', 'freeze', 'high_value_payment', 'joint_account', 'latest_transactions', 'pay_bill'], id=None),
 'lang_id': ClassLabel(num_classes=14, names=['cs-CZ', 'de-DE', 'en-AU', 'en-GB', 'en-US', 'es-ES', 'fr-FR', 'it-IT', 'ko-KR',  'nl-NL', 'pl-PL', 'pt-PT', 'ru-RU', 'zh-CN'], id=None),
 'path': Value(dtype='string', id=None),
 'transcription': Value(dtype='string', id=None)}
>>> ds = ds.cast_column("audio", Audio(sampling_rate=16000))
>>> ds.features
{'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None),
 'english_transcription': Value(dtype='string', id=None),
 'intent_class': ClassLabel(num_classes=14, names=['abroad', 'address', 'app_error', 'atm_limit', 'balance', 'business_loan',  'card_issues', 'cash_deposit', 'direct_debit', 'freeze', 'high_value_payment', 'joint_account', 'latest_transactions', 'pay_bill'], id=None),
 'lang_id': ClassLabel(num_classes=14, names=['cs-CZ', 'de-DE', 'en-AU', 'en-GB', 'en-US', 'es-ES', 'fr-FR', 'it-IT', 'ko-KR',  'nl-NL', 'pl-PL', 'pt-PT', 'ru-RU', 'zh-CN'], id=None),
 'path': Value(dtype='string', id=None),
 'transcription': Value(dtype='string', id=None)}
```

#### `cast`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L2146)

```py
( features: Features ) → export const metadata = 'undefined';IterableDataset
```

参数

+   `features`（[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)）- 要将数据集转换为的新特征。特征中字段的名称必须与当前列名匹配。数据的类型也必须可以从一种类型转换为另一种类型。对于非平凡的转换，例如`string` <-> `ClassLabel`，您应该使用[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)来更新数据集。

返回

`IterableDataset`

数据集的副本，带有转换后的特征。

将数据集转换为一组新的特征。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="train", streaming=True)
>>> ds.features
{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),
 'text': Value(dtype='string', id=None)}
>>> new_features = ds.features.copy()
>>> new_features["label"] = ClassLabel(names=["bad", "good"])
>>> new_features["text"] = Value("large_string")
>>> ds = ds.cast(new_features)
>>> ds.features
{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),
 'text': Value(dtype='large_string', id=None)}
```

#### `__iter__`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1360)

```py
( )
```

#### `iter`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1397)

```py
( batch_size: int drop_last_batch: bool = False )
```

参数

+   `batch_size`（`int`）- 每个批次的大小。

+   `drop_last_batch`（`bool`，默认为*False*）- 是否应删除小于`batch_size`的最后一个批次。

遍历大小为*batch_size*的批次。

#### `map`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1580)

```py
( function: Optional = None with_indices: bool = False input_columns: Union = None batched: bool = False batch_size: Optional = 1000 drop_last_batch: bool = False remove_columns: Union = None features: Optional = None fn_kwargs: Optional = None )
```

参数

+   `function`（`Callable`，*可选*，默认为`None`）- 当您迭代数据集时，应用于示例的即时函数。它必须具有以下签名之一：

    +   `function(example: Dict[str, Any]) -> Dict[str, Any]` 如果`batched=False`且`with_indices=False`

    +   `function(example: Dict[str, Any], idx: int) -> Dict[str, Any]` 如果`batched=False`且`with_indices=True`

    +   `function(batch: Dict[str, List]) -> Dict[str, List]` 如果`batched=True`且`with_indices=False`

    +   `function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List]` 如果`batched=True`且`with_indices=True`

    对于高级用法，函数还可以返回`pyarrow.Table`。此外，如果您的函数返回`None`，那么`map`将运行您的函数并返回未更改的数据集。如果未提供函数，则默认为恒等函数：`lambda x: x`。

+   `with_indices`（`bool`，默认为`False`）- 将示例索引提供给`function`。请注意，在这种情况下，`function`的签名应为`def function(example, idx[, rank]): ...`。

+   `input_columns`（`Optional[Union[str, List[str]]]`，默认为`None`）- 要作为位置参数传递给`function`的列。如果为`None`，则将映射到所有格式化列的字典作为一个参数传递。

+   `batched`（`bool`，默认为`False`）- 提供给`function`的示例批次。

+   `batch_size`（`int`，*可选*，默认为`1000`）- 如果`batched=True`，则提供给`function`的每个批次的示例数。如果`batch_size <= 0`或`batch_size == None`，则将整个数据集作为单个批次提供给`function`。

+   `drop_last_batch`（`bool`，默认为`False`）- 是否应删除小于`batch_size`的最后一个批次，而不是由函数处理。

+   `remove_columns`（`[List[str]`，*可选*，默认为`None`）- 在映射时删除一些列。在使用`function`更新示例输出之前将删除列，即如果`function`添加具有`remove_columns`中名称的列，则这些列将被保留。

+   `features`（`[Features]`，*可选*，默认为`None`）- 结果数据集的特征类型。

+   `fn_kwargs`（`Dict`，*可选*，默认为`None`）- 要传递给`function`的关键字参数。

对可迭代数据集中的所有示例（单独或批处理）应用函数并更新它们。如果您的函数返回一个已经存在的列，则会覆盖它。在迭代数据集时，函数会即时应用于示例。

您可以使用`batched`参数指定函数是否应该批处理：

+   如果`batched`为`False`，则函数接收一个示例并应返回一个示例。示例是一个字典，例如`{"text": "Hello there !"}`。

+   如果`batched`为`True`且`batch_size`为1，则函数接收一个包含1个示例的批次作为输入，并可以返回包含1个或多个示例的批次。一个批次是一个字典，例如，一个示例的批次是{“text”: [“Hello there !”]}。

+   如果`batched`为`True`且`batch_size`为`n` > 1，则函数接收一个包含`n`个示例的批次作为输入，并可以返回包含`n`个示例或任意数量示例的批次。请注意，最后一个批次可能少于`n`个示例。一个批次是一个字典，例如，`n`个示例的批次是`{"text": ["Hello there !"] * n}`。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="train", streaming=True)
>>> def add_prefix(example):
...     example["text"] = "Review: " + example["text"]
...     return example
>>> ds = ds.map(add_prefix)
>>> list(ds.take(3))
[{'label': 1,
 'text': 'Review: the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},
 {'label': 1,
 'text': 'Review: the gorgeously elaborate continuation of " the lord of the rings " trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},
 {'label': 1, 'text': 'Review: effective but too-tepid biopic'}]
```

#### `rename_column`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1939)

```py
( original_column_name: str new_column_name: str ) → export const metadata = 'undefined';IterableDataset
```

参数

+   `original_column_name` (`str`) — 要重命名的列的名称。

+   `new_column_name` (`str`) — 列的新名称。

返回

`IterableDataset`

具有重命名列的数据集副本。

重命名数据集中的列，并将原始列关联的特征移动到新列名下。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="train", streaming=True)
>>> next(iter(ds))
{'label': 1,
 'text': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}
>>> ds = ds.rename_column("text", "movie_review")
>>> next(iter(ds))
{'label': 1,
 'movie_review': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}
```

#### `filter`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1693)

```py
( function: Optional = None with_indices = False input_columns: Union = None batched: bool = False batch_size: Optional = 1000 fn_kwargs: Optional = None )
```

参数

+   `function` (`Callable`) — 具有以下签名之一的可调用函数：

    +   如果`with_indices=False, batched=False`，则`function(example: Dict[str, Any]) -> bool`

    +   如果`with_indices=True, batched=False`，则`function(example: Dict[str, Any], indices: int) -> bool`

    +   如果`with_indices=False, batched=True`，则`function(example: Dict[str, List]) -> List[bool]`

    +   如果`with_indices=True, batched=True`，则`function(example: Dict[str, List], indices: List[int]) -> List[bool]`

    如果未提供函数，则默认为始终为True的函数：`lambda x: True`。

+   `with_indices` (`bool`, 默认为 `False`) — 向`function`提供示例索引。请注意，在这种情况下，`function`的签名应为`def function(example, idx): ...`。

+   `input_columns` (`str` 或 `List[str]`, *optional*) — 要作为位置参数传递给`function`的列。如果为`None`，则将映射到所有格式化列的字典作为一个参数传递。

+   `batched` (`bool`, 默认为 `False`) — 向`function`提供示例批次。

+   `batch_size` (`int`, *optional*, default `1000`) — 如果`batched=True`，则提供给`function`的每个批次的示例数量。

+   `fn_kwargs` (`Dict`, *optional*, default `None`) — 要传递给`function`的关键字参数。

对所有元素应用过滤函数，使数据集仅包含符合过滤函数的示例。在迭代数据集时，过滤是即时进行的。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="train", streaming=True)
>>> ds = ds.filter(lambda x: x["label"] == 0)
>>> list(ds.take(3))
[{'label': 0, 'movie_review': 'simplistic , silly and tedious .'},
 {'label': 0,
 'movie_review': "it's so laddish and juvenile , only teenage boys could possibly find it funny ."},
 {'label': 0,
 'movie_review': 'exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .'}]
```

#### `shuffle`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1771)

```py
( seed = None generator: Optional = None buffer_size: int = 1000 )
```

参数

+   `seed` (`int`, *optional*, 默认为 `None`) — 用于洗牌数据集的随机种子。它用于从洗牌缓冲区中抽样，也用于对数据分片进行洗牌。

+   `generator` (`numpy.random.Generator`, *optional*) — 用于计算数据集行的排列的Numpy随机生成器。如果`generator=None`（默认值），则使用`np.random.default_rng`（NumPy的默认BitGenerator（PCG64））。

+   `buffer_size` (`int`, 默认为 `1000`) — 缓冲区的大小。

随机打乱此数据集的元素。

此数据集填充一个包含`buffer_size`元素的缓冲区，然后从该缓冲区随机抽取元素，用新元素替换所选元素。为了完全洗牌，需要一个大于或等于数据集完整大小的缓冲区大小。

例如，如果数据集包含10,000个元素，但`buffer_size`设置为1000，则`shuffle`将最初从缓冲区中的前1000个元素中随机选择一个元素。一旦选择了一个元素，它在缓冲区中的位置将被下一个（即第1,001个）元素替换，保持1000个元素的缓冲区。

如果数据集由多个分片组成，则还会对分片的顺序进行洗牌。但是，如果使用[skip()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.skip)或[take()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.take)固定了顺序，则分片的顺序保持不变。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="train", streaming=True)
>>> list(ds.take(3))
[{'label': 1,
 'text': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},
 {'label': 1,
 'text': 'the gorgeously elaborate continuation of " the lord of the rings " trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},
 {'label': 1, 'text': 'effective but too-tepid biopic'}]
>>> shuffled_ds = ds.shuffle(seed=42)
>>> list(shuffled_ds.take(3))
[{'label': 1,
 'text': "a sports movie with action that's exciting on the field and a story you care about off it ."},
 {'label': 1,
 'text': 'at its best , the good girl is a refreshingly adult take on adultery . . .'},
 {'label': 1,
 'text': "sam jones became a very lucky filmmaker the day wilco got dropped from their record label , proving that one man's ruin may be another's fortune ."}]
```

#### `skip`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1841)

```py
( n )
```

参数

+   `n`（`int`）— 要跳过的元素数量。

创建一个跳过前`n`个元素的新[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="train", streaming=True)
>>> list(ds.take(3))
[{'label': 1,
 'text': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},
 {'label': 1,
 'text': 'the gorgeously elaborate continuation of " the lord of the rings " trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},
 {'label': 1, 'text': 'effective but too-tepid biopic'}]
>>> ds = ds.skip(1)
>>> list(ds.take(3))
[{'label': 1,
 'text': 'the gorgeously elaborate continuation of " the lord of the rings " trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},
 {'label': 1, 'text': 'effective but too-tepid biopic'},
 {'label': 1,
 'text': 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .'}]
```

#### `take`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/iterable_dataset.py#L1880)

```py
( n )
```

参数

+   `n`（`int`）— 要获取的元素数量。

创建一个只包含前`n`个元素的新[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="train", streaming=True)
>>> small_ds = ds.take(2)
>>> list(small_ds)
[{'label': 1,
 'text': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},
 {'label': 1,
 'text': 'the gorgeously elaborate continuation of " the lord of the rings " trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'}]
```

#### `info`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L159)

```py
( )
```

包含数据集中所有元数据的[DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)对象。

#### `split`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L164)

```py
( )
```

对应于命名数据集分割的[NamedSplit](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.NamedSplit)对象。

#### `builder_name`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L169)

```py
( )
```

#### `citation`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L173)

```py
( )
```

#### `config_name`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L177)

```py
( )
```

#### `dataset_size`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L181)

```py
( )
```

#### `description`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L185)

```py
( )
```

#### `download_checksums`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L189)

```py
( )
```

#### `download_size`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L193)

```py
( )
```

#### `features`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L197)

```py
( )
```

#### `homepage`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L201)

```py
( )
```

#### `license`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L205)

```py
( )
```

#### `size_in_bytes`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L209)

```py
( )
```

#### `supervised_keys`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L213)

```py
( )
```

#### `version`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_dataset.py#L221)

```py
( )
```

## IterableDatasetDict

字典，以分割名称为键（例如‘train’、‘test’），以`IterableDataset`对象为值。

### `class datasets.IterableDatasetDict`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1864)

```py
( )
```

#### `map`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1899)

```py
( function: Optional = None with_indices: bool = False input_columns: Union = None batched: bool = False batch_size: int = 1000 drop_last_batch: bool = False remove_columns: Union = None fn_kwargs: Optional = None )
```

参数

+   `function` (`Callable`, *optional*, defaults to `None`) — 在迭代数据集时即时应用于示例的函数。它必须具有以下签名之一：

    +   `function(example: Dict[str, Any]) -> Dict[str, Any]` 如果`batched=False`且`with_indices=False`

    +   `function(example: Dict[str, Any], idx: int) -> Dict[str, Any]` 如果`batched=False`且`with_indices=True`

    +   `function(batch: Dict[str, List]) -> Dict[str, List]` 如果`batched=True`且`with_indices=False`

    +   `function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List]` 如果`batched=True`且`with_indices=True`

    对于高级用法，函数还可以返回`pyarrow.Table`。此外，如果您的函数返回`None`，则`map`将运行您的函数并返回未更改的数据集。如果未提供函数，则默认为恒等函数：`lambda x: x`。

+   `with_indices` (`bool`, defaults to `False`) — 将示例索引提供给`function`。请注意，在这种情况下，`function`的签名应为`def function(example, idx[, rank]): ...`。

+   `input_columns` (`[Union[str, List[str]]]`, *optional*, defaults to `None`) — 要作为位置参数传递给`function`的列。如果为`None`，则将映射到所有格式化列的字典作为一个参数传递。

+   `batched` (`bool`, defaults to `False`) — 将示例批次提供给`function`。

+   `batch_size` (`int`, *optional*, defaults to `1000`) — 如果`batched=True`，则提供给`function`的每批示例数。

+   `drop_last_batch` (`bool`, defaults to `False`) — 是否应删除小于`batch_size`的最后一批而不是由函数处理。

+   `remove_columns` (`[List[str]]`, *optional*, defaults to `None`) — 在映射时删除一些列。在使用`function`的输出更新示例之前将删除这些列，即如果`function`添加具有`remove_columns`中名称的列，则这些列将被保留。

+   `fn_kwargs` (`Dict`, *optional*, defaults to `None`) — 要传递给`function`的关键字参数

将函数应用于可迭代数据集中的所有示例（单独或批量），并更新它们。如果您的函数返回一个已经存在的列，则会覆盖它。在迭代数据集时，函数会即时应用于示例。转换将应用于数据集字典的所有数据集。

您可以使用`batched`参数指定函数是否应该批处理。

+   如果`batched`为`False`，则函数接收1个示例并应返回1个示例。示例是一个字典，例如`{"text": "Hello there !"}`。

+   如果`batched`为`True`且`batch_size`为1，则函数将以1个示例批次作为输入，并可以返回包含1个或多个示例的批次。批次是一个字典，例如，1个示例批次是`{"text": ["Hello there !"]}`。

+   如果`batched`为`True`且`batch_size`为`n` > 1，则函数接收`n`个示例的批次作为输入，并可以返回包含`n`个示例或任意数量示例的批次。请注意，最后一批可能少于`n`个示例。批次是一个字典，例如，`n`个示例的批次是`{"text": ["Hello there !"] * n}`。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", streaming=True)
>>> def add_prefix(example):
...     example["text"] = "Review: " + example["text"]
...     return example
>>> ds = ds.map(add_prefix)
>>> next(iter(ds["train"]))
{'label': 1,
 'text': 'Review: the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}
```

#### `filter`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1988)

```py
( function: Optional = None with_indices = False input_columns: Union = None batched: bool = False batch_size: Optional = 1000 fn_kwargs: Optional = None )
```

参数

+   `function` (`Callable`) — 具有以下签名之一的可调用函数：

    +   `function(example: Dict[str, Any]) -> bool` 如果`with_indices=False, batched=False`

    +   `function(example: Dict[str, Any], indices: int) -> bool` 如果`with_indices=True, batched=False`

    +   `function(example: Dict[str, List]) -> List[bool]` 如果`with_indices=False, batched=True`

    +   `function(example: Dict[str, List], indices: List[int]) -> List[bool]` 如果`with_indices=True, batched=True`

    如果未提供函数，则默认为始终为True的函数：`lambda x: True`。

+   `with_indices` (`bool`, 默认为`False`) — 向`function`提供示例索引。请注意，在这种情况下，`function`的签名应为`def function(example, idx): ...`。

+   `input_columns` (`str` or `List[str]`, *optional*) — 要作为位置参数传递给`function`的列。如果为`None`，则传递一个映射到所有格式化列的字典作为一个参数。

+   `batched` (`bool`, 默认为`False`) — 提供示例批次给`function`

+   `batch_size` (`int`, *optional*, defaults to `1000`) — 如果`batched=True`，则每个批次提供给`function`的示例数量。

+   `fn_kwargs` (`Dict`, *optional*, 默认为`None`) — 要传递给`function`的关键字参数

对所有元素应用过滤函数，以便数据集仅包含根据过滤函数的示例。在迭代数据集时，过滤是即时进行的。过滤应用于数据集字典的所有数据集。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", streaming=True)
>>> ds = ds.filter(lambda x: x["label"] == 0)
>>> list(ds["train"].take(3))
[{'label': 0, 'text': 'Review: simplistic , silly and tedious .'},
 {'label': 0,
 'text': "Review: it's so laddish and juvenile , only teenage boys could possibly find it funny ."},
 {'label': 0,
 'text': 'Review: exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .'}]
```

#### `shuffle`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2051)

```py
( seed = None generator: Optional = None buffer_size: int = 1000 )
```

参数

+   `seed` (`int`, *optional*, 默认为`None`) — 将用于打乱数据集的随机种子。它用于从洗牌缓冲区中抽样，也用于对数据分片进行洗牌。

+   `generator` (`numpy.random.Generator`, *optional*) — 用于计算数据集行的排列的Numpy随机生成器。如果`generator=None`（默认值），则使用`np.random.default_rng`（NumPy的默认BitGenerator（PCG64））。

+   `buffer_size` (`int`, 默认为`1000`) — 缓冲区的大小。

随机打乱此数据集的元素。打乱应用于数据集字典的所有数据集。

此数据集使用buffer_size个元素填充缓冲区，然后从该缓冲区随机抽取元素，用新元素替换所选元素。为了完全打乱，需要一个大于或等于数据集完整大小的缓冲区大小。

例如，如果您的数据集包含10,000个元素，但`buffer_size`设置为1000，则`shuffle`将最初仅从缓冲区中的前1000个元素中选择一个随机元素。选择元素后，缓冲区中的空间将被下一个（即第1001个）元素替换，保持1000个元素的缓冲区。

如果数据集由多个分片组成，则还会对分片的顺序进行`shuffle`。但是，如果使用[skip()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.skip)或[take()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.take)固定了顺序，则分片的顺序保持不变。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", streaming=True)
>>> list(ds["train"].take(3))
[{'label': 1,
 'text': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},
 {'label': 1,
 'text': 'the gorgeously elaborate continuation of " the lord of the rings " trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},
 {'label': 1, 'text': 'effective but too-tepid biopic'}]
>>> ds = ds.shuffle(seed=42)
>>> list(ds["train"].take(3))
[{'label': 1,
 'text': "a sports movie with action that's exciting on the field and a story you care about off it ."},
 {'label': 1,
 'text': 'at its best , the good girl is a refreshingly adult take on adultery . . .'},
 {'label': 1,
 'text': "sam jones became a very lucky filmmaker the day wilco got dropped from their record label , proving that one man's ruin may be another's fortune ."}]
```

#### `with_format`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L1870)

```py
( type: Optional = None )
```

参数

+   `type` (`str`, *optional*, 默认为`None`) — 如果设置为“torch”，返回的数据集将是`torch.utils.data.IterableDataset`的子类，可用于`DataLoader`中使用。

返回具有指定格式的数据集。此方法目前仅支持“torch”格式。格式设置为数据集字典的所有数据集。 

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", streaming=True)
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
>>> def encode(example):
...     return tokenizer(examples["text"], truncation=True, padding="max_length")
>>> ds = ds.map(encode, batched=True, remove_columns=["text"])
>>> ds = ds.with_format("torch")
```

#### `cast`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2253)

```py
( features: Features ) → export const metadata = 'undefined';IterableDatasetDict
```

参数

+   `features` (`Features`) — 要将数据集转换为的新特征。特征中字段的名称必须与当前列名匹配。数据的类型也必须可以从一种类型转换为另一种类型。对于非平凡的转换，例如`string` <-> `ClassLabel`，您应该使用`map`来更新数据集。

返回

[IterableDatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDatasetDict)

具有转换特征的数据集的副本。

将数据集转换为新的特征集。类型转换应用于数据集字典的所有数据集。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", streaming=True)
>>> ds["train"].features
{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),
 'text': Value(dtype='string', id=None)}
>>> new_features = ds["train"].features.copy()
>>> new_features['label'] = ClassLabel(names=['bad', 'good'])
>>> new_features['text'] = Value('large_string')
>>> ds = ds.cast(new_features)
>>> ds["train"].features
{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),
 'text': Value(dtype='large_string', id=None)}
```

#### `cast_column`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2222)

```py
( column: str feature: Union )
```

参数

+   `column` (`str`) — 列名。

+   `feature` (`Feature`) — 目标特征。

将列转换为特征以进行解码。类型转换应用于数据集字典中的所有数据集。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", streaming=True)
>>> ds["train"].features
{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),
 'text': Value(dtype='string', id=None)}
>>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))
>>> ds["train"].features
{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),
 'text': Value(dtype='string', id=None)}
```

#### `remove_columns`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2170)

```py
( column_names: Union ) → export const metadata = 'undefined';IterableDatasetDict
```

参数

+   `column_names` (`Union[str, List[str]]`) — 要移除的列的名称。

返回

[IterableDatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDatasetDict)

一个不包含要移除列的数据集对象副本。

在数据集中移除一个或多个列以及与它们相关联的特征。在迭代数据集时，移除是即时进行的。移除应用于数据集字典中的所有数据集。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", streaming=True)
>>> ds = ds.remove_columns("label")
>>> next(iter(ds["train"]))
{'text': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}
```

#### `rename_column`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2109)

```py
( original_column_name: str new_column_name: str ) → export const metadata = 'undefined';IterableDatasetDict
```

参数

+   `original_column_name` (`str`) — 要重命名的列的名称。

+   `new_column_name` (`str`) — 列的新名称。

返回

[IterableDatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDatasetDict)

一个重命名列后的数据集副本。

重命名数据集中的一个列，并将原始列相关联的特征移动到新列名下。重命名应用于数据集字典中的所有数据集。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", streaming=True)
>>> ds = ds.rename_column("text", "movie_review")
>>> next(iter(ds["train"]))
{'label': 1,
 'movie_review': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}
```

#### `rename_columns`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2142)

```py
( column_mapping: Dict ) → export const metadata = 'undefined';IterableDatasetDict
```

参数

+   `column_mapping` (`Dict[str, str]`) — 列重命名为它们的新名称的映射。

返回

[IterableDatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDatasetDict)

一个重命名列后的数据集副本

重命名数据集中的多个列，并将原始列相关联的特征移动到新列名下。重命名应用于数据集字典中的所有数据集。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", streaming=True)
>>> ds = ds.rename_columns({"text": "movie_review", "label": "rating"})
>>> next(iter(ds["train"]))
{'movie_review': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',
 'rating': 1}
```

#### `select_columns`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/dataset_dict.py#L2196)

```py
( column_names: Union ) → export const metadata = 'undefined';IterableDatasetDict
```

参数

+   `column_names` (`Union[str, List[str]]`) — 要保留的列的名称。

返回

[IterableDatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDatasetDict)

一个仅包含选定列的数据集对象副本。

在数据集中选择一个或多个列以及与它们相关联的特征。在迭代数据集时，选择是即时进行的。选择应用于数据集字典中的所有数据集。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", streaming=True)
>>> ds = ds.select("text")
>>> next(iter(ds["train"]))
{'text': 'the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}
```

## 特征

### `class datasets.Features`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1582)

```py
( *args **kwargs )
```

一个特殊的字典，定义了数据集的内部结构。

使用类型为 `dict[str, FieldType]` 的字典实例化，其中键是所需的列名，值是该列的类型。

`FieldType` 可以是以下之一：

+   一个[Value](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Value)特征指定一个单一类型的值，例如 `int64` 或 `string`。

+   一个[ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)特征指定一个具有预定义类集的字段，这些类可以有与之关联的标签，并将存储为整数在数据集中。

+   一个Python `dict`，指定字段是包含子字段到子字段特征映射的嵌套字段。可以以任意方式嵌套字段的嵌套字段。

+   一个 python `list` 或一个 [Sequence](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Sequence) 指定该字段包含对象列表。应该提供一个单一子特征作为此列表中托管的特征类型的示例。

    带有内部字典特征的 [Sequence](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Sequence) 将自动转换为列表字典。此行为实现了与 TensorFlow Datasets 库的兼容性层，但在某些情况下可能不需要。如果不希望这种行为，可以使用 python `list` 而不是 [Sequence](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Sequence)。

+   一个 [Array2D](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Array2D)、[Array3D](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Array3D)、[Array4D](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Array4D) 或 [Array5D](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Array5D) 特征，用于多维数组。

+   一个 [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio) 特征，用于存储音频文件的绝对路径或一个包含音频文件相对路径（“path”键）和其字节内容（“bytes”键）的字典。此特征提取音频数据。

+   一个 [Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image) 特征，用于存储图像文件的绝对路径，一个 `np.ndarray` 对象，一个 `PIL.Image.Image` 对象或一个包含图像文件相对路径（“path”键）和其字节内容（“bytes”键）的字典。此特征提取图像数据。

+   [Translation](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Translation) 和 [TranslationVariableLanguages](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.TranslationVariableLanguages)，机器翻译特有的两个特征。

#### `copy`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1993)

```py
( )
```

制作 [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features) 的深层副本。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="train")
>>> copy_of_features = ds.features.copy()
>>> copy_of_features
{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),
 'text': Value(dtype='string', id=None)}
```

#### `decode_batch`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1966)

```py
( batch: dict token_per_repo_id: Optional = None )
```

参数

+   `batch` (`dict[str, list[Any]]`) — 数据集批处理数据。

+   `token_per_repo_id` (`dict`, *optional*) — 要访问和解码 Hub 上私有存储库中的音频或图像文件，可以传递一个字典 repo_id (str) -> token (bool or str)

使用自定义特征解码解码批处理。

#### `decode_column`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1948)

```py
( column: list column_name: str )
```

参数

+   `column` (`list[Any]`) — 数据集列数据。

+   `column_name` (`str`) — 数据集列名称。

使用自定义特征解码解码列。

#### `decode_example`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1925)

```py
( example: dict token_per_repo_id: Optional = None )
```

参数

+   `example` (`dict[str, Any]`) — 数据集行数据。

+   `token_per_repo_id` (`dict`, *optional*) — 要访问和解码 Hub 上私有存储库中的音频或图像文件，可以传递一个字典 `repo_id (str) -> token (bool or str)`。

使用自定义特征解码的解码示例。

#### `encode_batch`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1906)

```py
( batch )
```

参数

+   `batch` (`dict[str, list[Any]]`) — 数据在数据集批处理中。

将批处理编码为 Arrow 格式。

#### `encode_column`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1890)

```py
( column column_name: str )
```

参数

+   `column` (`list[Any]`) — 数据集列中的数据。

+   `column_name` (`str`) — 数据集列名称。

将列编码为 Arrow 格式。

#### `encode_example`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1876)

```py
( example )
```

参数

+   `example` (`dict[str, Any]`) — 数据集行中的数据。

将示例编码为Arrow格式。

#### `flatten`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L2080)

```py
( max_depth = 16 ) → export const metadata = 'undefined';Features
```

返回

[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)

扁平化的特征。

展平特征。每个字典列都被移除，并被它包含的所有子字段所取代。新字段的命名方式是通过连接原始列的名称和子字段名称来命名的，如下所示：<original>.<subfield>。

如果列包含嵌套字典，则所有较低级别的子字段名称也会连接在一起形成新列：<original>.<subfield>.<subsubfield>，等等。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("squad", split="train")
>>> ds.features.flatten()
{'answers.answer_start': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),
 'answers.text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),
 'context': Value(dtype='string', id=None),
 'id': Value(dtype='string', id=None),
 'question': Value(dtype='string', id=None),
 'title': Value(dtype='string', id=None)}
```

#### `from_arrow_schema`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1657)

```py
( pa_schema: Schema )
```

参数

+   `pa_schema` (`pyarrow.Schema`) — Arrow Schema。

从Arrow Schema构造[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)。它还检查Hugging Face数据集特征的模式元数据。不支持非空字段，并设置为可空。

#### `from_dict`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1688)

```py
( dic ) → export const metadata = 'undefined';Features
```

参数

+   `dic` (*dict[str, Any]*) — Python字典。

返回

*Features*

从字典构造[*Features*]。

从反序列化的字典重新生成嵌套特征对象。我们使用*_type*键来推断特征*FieldType*的数据类名称。

它允许使用方便的构造函数语法从反序列化的JSON字典定义特征。特别是在将转储为JSON对象的[*DatasetInfo*]反序列化时使用此函数。这充当了[*Features.from_arrow_schema*]的类似物，并处理递归的逐字段实例化，但不需要任何映射到/从pyarrow的操作，除了它利用了[*Value*]自动执行的pyarrow原始dtype的映射。

示例：

```py
>>> Features.from_dict({'_type': {'dtype': 'string', 'id': None, '_type': 'Value'}})
{'_type': Value(dtype='string', id=None)}
```

#### `reorder_fields_as`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L2013)

```py
( other: Features )
```

参数

+   `other`（[*Features*]） — 与之对齐的其他[*Features*]。

重新排序Features字段以匹配其他[*Features*]的字段顺序。

字段的顺序很重要，因为它对底层的箭头数据很重要。重新排序字段允许使底层的箭头数据类型匹配。

示例：

```py
>>> from datasets import Features, Sequence, Value
>>> # let's say we have to features with a different order of nested fields (for a and b for example)
>>> f1 = Features({"root": Sequence({"a": Value("string"), "b": Value("string")})})
>>> f2 = Features({"root": {"b": Sequence(Value("string")), "a": Sequence(Value("string"))}})
>>> assert f1.type != f2.type
>>> # re-ordering keeps the base structure (here Sequence is defined at the root level), but make the fields order match
>>> f1.reorder_fields_as(f2)
{'root': Sequence(feature={'b': Value(dtype='string', id=None), 'a': Value(dtype='string', id=None)}, length=-1, id=None)}
>>> assert f1.reorder_fields_as(f2).type == f2.type
```

### `class datasets.Sequence`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1128)

```py
( feature: Any length: int = -1 id: Optional = None )
```

参数

+   `length` (`int`) — 序列的长度。

从单一类型或类型字典构造特征列表。主要用于与tfds兼容。

示例：

```py
>>> from datasets import Features, Sequence, Value, ClassLabel
>>> features = Features({'post': Sequence(feature={'text': Value(dtype='string'), 'upvotes': Value(dtype='int32'), 'label': ClassLabel(num_classes=2, names=['hot', 'cold'])})})
>>> features
{'post': Sequence(feature={'text': Value(dtype='string', id=None), 'upvotes': Value(dtype='int32', id=None), 'label': ClassLabel(num_classes=2, names=['hot', 'cold'], id=None)}, length=-1, id=None)}
```

### `class datasets.ClassLabel`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L930)

```py
( num_classes: dataclasses.InitVar[typing.Optional[int]] = None names: List = None names_file: dataclasses.InitVar[typing.Optional[str]] = None id: Optional = None )
```

参数

+   `num_classes` (`int`, *可选*) — 类的数量。所有标签必须小于`num_classes`。

+   `names` (`list` of `str`, *可选*) — 整数类的字符串名称。保留提供名称的顺序。

+   `names_file` (`str`, *可选*) — 包含整数类名称的文件路径，每行一个。

整数类标签的特征类型。

有3种方法来定义`ClassLabel`，对应于3个参数：

+   `num_classes`：创建0到(num_classes-1)标签。

+   `names`：标签字符串列表。

+   `names_file`：包含标签列表的文件。

在内部，标签以整数形式存储。您可以使用负整数表示未知/缺失的标签。

示例：

```py
>>> from datasets import Features
>>> features = Features({'label': ClassLabel(num_classes=3, names=['bad', 'ok', 'good'])})
>>> features
{'label': ClassLabel(num_classes=3, names=['bad', 'ok', 'good'], id=None)}
```

#### `cast_storage`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1096)

```py
( storage: Union ) → export const metadata = 'undefined';pa.Int64Array
```

参数

+   `storage` (`Union[pa.StringArray, pa.IntegerArray]`) — 要转换的PyArrow数组。

返回

`pa.Int64Array`

在`ClassLabel`箭头存储类型中的数组。

将Arrow数组转换为`ClassLabel`箭头存储类型。可以转换为`ClassLabel` pyarrow存储类型的Arrow类型包括：

+   `pa.string()`

+   `pa.int()`

#### `int2str`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1050)

```py
( values: Union )
```

转换`integer` => 类名`string`。

关于未知/缺失标签：传递负整数会引发`ValueError`。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="train")
>>> ds.features["label"].int2str(0)
'neg'
```

#### `str2int`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L1005)

```py
( values: Union )
```

转换类名`string` => `integer`。

示例：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("rotten_tomatoes", split="train")
>>> ds.features["label"].str2int('neg')
0
```

### `class datasets.Value`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L452)

```py
( dtype: str id: Optional = None )
```

`Value` dtypes如下：

+   `null`

+   `bool`

+   `int8`

+   `int16`

+   `int32`

+   `int64`

+   `uint8`

+   `uint16`

+   `uint32`

+   `uint64`

+   `float16`

+   `float32`（别名float）

+   `float64`（别名double）

+   `time32[(s|ms)]`

+   `time64[(us|ns)]`

+   `timestamp[(s|ms|us|ns)]`

+   `timestamp[(s|ms|us|ns), tz=(tzstring)]`

+   `date32`

+   `date64`

+   `duration[(s|ms|us|ns)]`

+   `decimal128(precision, scale)`

+   `decimal256(precision, scale)`

+   `binary`

+   `large_binary`

+   `string`

+   `large_string`

示例：

```py
>>> from datasets import Features
>>> features = Features({'stars': Value(dtype='int32')})
>>> features
{'stars': Value(dtype='int32', id=None)}
```

### `class datasets.Translation`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/translation.py#L11)

```py
( languages: List id: Optional = None )
```

参数

+   `languages`（`dict`）— 一个字典，每个示例将字符串语言代码映射到字符串翻译。

用于每个示例具有固定语言的翻译的`FeatureConnector`。这里是为了与tfds兼容。

示例：

```py
>>> # At construction time:
>>> datasets.features.Translation(languages=['en', 'fr', 'de'])
>>> # During data generation:
>>> yield {
...         'en': 'the cat',
...         'fr': 'le chat',
...         'de': 'die katze'
... }
```

#### `flatten`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/translation.py#L44)

```py
( )
```

将Translation特征展平为字典。

### `class datasets.TranslationVariableLanguages`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/translation.py#L51)

```py
( languages: Optional = None num_languages: Optional = None id: Optional = None ) → export const metadata = 'undefined';
language or translation (variable-length 1D tf.Tensor of tf.string)
```

参数

+   `languages`（`dict`）— 一个字典，每个示例将字符串语言代码映射到一个或多个字符串翻译。每个示例中存在的语言可能会有所不同。

返回

+   `language`或`translation`（可变长度1D `tf.Tensor` of `tf.string`）

语言代码按升序排列或纯文本翻译，排序以与语言代码对齐。

用于每个示例具有可变语言的翻译的`FeatureConnector`。这里是为了与tfds兼容。

示例：

```py
>>> # At construction time:
>>> datasets.features.TranslationVariableLanguages(languages=['en', 'fr', 'de'])
>>> # During data generation:
>>> yield {
...         'en': 'the cat',
...         'fr': ['le chat', 'la chatte,']
...         'de': 'die katze'
... }
>>> # Tensor returned :
>>> {
...         'language': ['en', 'de', 'fr', 'fr'],
...         'translation': ['the cat', 'die katze', 'la chatte', 'le chat'],
... }
```

#### `flatten`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/translation.py#L122)

```py
( )
```

将TranslationVariableLanguages特征展平为字典。

### `class datasets.Array2D`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L535)

```py
( shape: tuple dtype: str id: Optional = None )
```

参数

+   `shape`（`tuple`）— 每个维度的大小。

+   `dtype` (`str`) — 数据类型的值。

创建一个二维数组。

示例：

```py
>>> from datasets import Features
>>> features = Features({'x': Array2D(shape=(1, 3), dtype='int32')})
```

### `class datasets.Array3D`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L560)

```py
( shape: tuple dtype: str id: Optional = None )
```

参数

+   `shape`（`tuple`）— 每个维度的大小。

+   `dtype` (`str`) — 数据类型的值。

创建一个三维数组。

示例：

```py
>>> from datasets import Features
>>> features = Features({'x': Array3D(shape=(1, 2, 3), dtype='int32')})
```

### `class datasets.Array4D`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L585)

```py
( shape: tuple dtype: str id: Optional = None )
```

参数

+   `shape`（`tuple`）— 每个维度的大小。

+   `dtype` (`str`) — 数据类型的值。

创建一个四维数组。

示例：

```py
>>> from datasets import Features
>>> features = Features({'x': Array4D(shape=(1, 2, 2, 3), dtype='int32')})
```

### `class datasets.Array5D`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/features.py#L610)

```py
( shape: tuple dtype: str id: Optional = None )
```

参数

+   `shape`（`tuple`）— 每个维度的大小。

+   `dtype` (`str`) — 数据类型的值。

创建一个五维数组。

示例：

```py
>>> from datasets import Features
>>> features = Features({'x': Array5D(shape=(1, 2, 2, 3, 3), dtype='int32')})
```

### `class datasets.Audio`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L20)

```py
( sampling_rate: Optional = None mono: bool = True decode: bool = True id: Optional = None )
```

参数

+   `sampling_rate`（`int`，*可选*）— 目标采样率。如果为`None`，则使用本机采样率。

+   `mono`（`bool`，默认为`True`）— 是否通过对通道间的样本进行平均来将音频信号转换为单声道。

+   `decode`（`bool`，默认为`True`）— 是否解码音频数据。如果为`False`，则以`{"path": audio_path, "bytes": audio_bytes}`格式返回基础字典。

从音频文件中提取音频数据的音频`Feature`。

输入：音频特征接受以下输入：

+   一个`str`：音频文件的绝对路径（即允许随机访问）。

+   一个带有键的`dict`：

    +   `path`：相对于存档文件的音频文件路径的字符串。

    +   `bytes`：音频文件的字节内容。

    这对于具有顺序访问的存档文件非常有用。

+   一个带有键的`dict`：

    +   `path`：音频文件相对于存档文件的相对路径。

    +   `array`：包含音频样本的数组。

    +   `sampling_rate`：音频样本的采样率对应的整数。

    这对于具有顺序访问的存档文件非常有用。

示例：

```py
>>> from datasets import load_dataset, Audio
>>> ds = load_dataset("PolyAI/minds14", name="en-US", split="train")
>>> ds = ds.cast_column("audio", Audio(sampling_rate=16000))
>>> ds[0]["audio"]
{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,
     3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
 'sampling_rate': 16000}
```

#### `cast_storage`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L209)

```py
( storage: Union ) → export const metadata = 'undefined';pa.StructArray
```

参数

+   `storage`（`Union[pa.StringArray, pa.StructArray]`）— 要转换的PyArrow数组。

返回

`pa.StructArray`

音频箭头存储类型中的数组，即`pa.struct({"bytes": pa.binary(), "path": pa.string()})`

将Arrow数组转换为音频箭头存储类型。可以转换为音频pyarrow存储类型的Arrow类型包括：

+   `pa.string()` - 必须包含“path”数据

+   `pa.binary()` - 必须包含音频字节

+   `pa.struct({"bytes": pa.binary()})`

+   `pa.struct({"path": pa.string()})`

+   `pa.struct({"bytes": pa.binary(), "path": pa.string()})` - 顺序无关紧要

#### `decode_example`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L126)

```py
( value: dict token_per_repo_id: Optional = None ) → export const metadata = 'undefined';dict
```

参数

+   `value`（`dict`）— 一个带有键的字典：

    +   `path`：相对音频文件路径的字符串。

    +   `bytes`：音频文件的字节。

+   `token_per_repo_id`（`dict`，*可选*）— 要从Hub上的私有存储库访问和解码音频文件，可以传递一个字典`repo_id`（`str`）-> `token`（`bool`或`str`）

返回

`字典`

解码示例音频文件为音频数据。

#### `embed_storage`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L247)

```py
( storage: StructArray ) → export const metadata = 'undefined';pa.StructArray
```

参数

+   `storage`（`pa.StructArray`）— 要嵌入的PyArrow数组。

返回

`pa.StructArray`

音频箭头存储类型中的数组，即`pa.struct({"bytes": pa.binary(), "path": pa.string()})`。

将音频文件嵌入到Arrow数组中。

#### `encode_example`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L77)

```py
( value: Union ) → export const metadata = 'undefined';dict
```

参数

+   `value`（`str`或`dict`）— 作为输入传递给音频特征的数据。

返回

`字典`

将示例编码为Arrow格式。

#### `flatten`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/audio.py#L198)

```py
( )
```

如果处于可解码状态，则引发错误，否则将特征展平为字典。

### `class datasets.Image`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L46)

```py
( decode: bool = True id: Optional = None )
```

参数

+   `decode`（`bool`，默认为`True`）— 是否解码图像数据。如果为`False`，则以`{"path": image_path, "bytes": image_bytes}`格式返回基础字典。

从图像文件中读取图像数据的`Feature`。

输入：图像特征接受以下输入：

+   一个`str`：图像文件的绝对路径（即允许随机访问）。

+   一个带有键的`dict`：

    +   `path`：图像文件相对路径。

    +   `bytes`：图像文件的字节。

    这对于具有顺序访问的存档文件非常有用。

+   一个`np.ndarray`：表示图像的NumPy数组。

+   一个`PIL.Image.Image`：PIL图像对象。

示例：

```py
>>> from datasets import load_dataset, Image
>>> ds = load_dataset("beans", split="train")
>>> ds.features["image"]
Image(decode=True, id=None)
>>> ds[0]["image"]
<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500 at 0x15E52E7F0>
>>> ds = ds.cast_column('image', Image(decode=False))
{'bytes': None,
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/b0a21163f78769a2cf11f58dfc767fb458fc7cea5c05dccc0144a2c0f0bc1292/train/healthy/healthy_train.85.jpg'}
```

#### `cast_storage`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L201)

```py
( storage: Union ) → export const metadata = 'undefined';pa.StructArray
```

参数

+   `storage`（`Union[pa.StringArray, pa.StructArray, pa.ListArray]`）— 要转换的PyArrow数组。

返回

`pa.StructArray`

数组在Image箭头存储类型中，即`pa.struct({"bytes": pa.binary(), "path": pa.string()})`。

将Arrow数组转换为Image箭头存储类型。可以转换为Image pyarrow存储类型的Arrow类型有：

+   `pa.string()` - 它必须包含“path”数据

+   `pa.binary()` - 它必须包含图像字节

+   `pa.struct({"bytes": pa.binary()})`

+   `pa.struct({"path": pa.string()})`

+   `pa.struct({"bytes": pa.binary(), "path": pa.string()})` - 顺序无关紧要

+   `pa.list(*)` - 它必须包含图像数组数据

#### `decode_example`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L131)

```py
( value: dict token_per_repo_id = None )
```

参数

+   `value`（`str`或`dict`）— 一个包含绝对图像文件路径的字符串，一个带有键的字典：

    +   `path`: 绝对或相对图像文件路径的字符串。

    +   `bytes`：图像文件的字节。

+   `token_per_repo_id`（`dict`，*可选*）— 要访问和解码Hub上私有存储库中的图像文件，可以传递一个字典repo_id（`str`）-> token（`bool`或`str`）。

解码示例图像文件为图像数据。

#### `embed_storage`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L247)

```py
( storage: StructArray ) → export const metadata = 'undefined';pa.StructArray
```

参数

+   `storage`（`pa.StructArray`）— 要嵌入的PyArrow数组。

返回

`pa.StructArray`

数组在Image箭头存储类型中，即`pa.struct({"bytes": pa.binary(), "path": pa.string()})`。

将图像文件嵌入Arrow数组中。

#### `encode_example`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L92)

```py
( value: Union )
```

参数

+   `value`（`str`，`np.ndarray`，`PIL.Image.Image`或`dict`）— 作为输入传递给Image特征的数据。

将示例编码为Arrow格式。

#### `flatten`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/features/image.py#L188)

```py
( )
```

如果处于可解码状态，则返回特征本身，否则将特征展平为字典。

## MetricInfo

### `class datasets.MetricInfo`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L510)

```py
( description: str citation: str features: Features inputs_description: str = <factory> homepage: str = <factory> license: str = <factory> codebase_urls: List = <factory> reference_urls: List = <factory> streamable: bool = False format: Optional = None metric_name: Optional = None config_name: Optional = None experiment_id: Optional = None )
```

有关度量的信息。

`MetricInfo`记录了一个度量标准的信息，包括其名称、版本和特征。查看构造函数参数和属性以获取完整列表。

注意：并非所有字段在构建时都已知，可能稍后更新。

#### `from_directory`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L566)

```py
( metric_info_dir )
```

从`metric_info_dir`中的JSON文件创建MetricInfo。

示例：

```py
>>> from datasets import MetricInfo
>>> metric_info = MetricInfo.from_directory("/path/to/directory/")
```

#### `write_to_directory`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/info.py#L546)

```py
( metric_info_dir pretty_print = False )
```

将`MetricInfo`写入JSON到`metric_info_dir`。还将许可证单独保存在LICENCE中。如果`pretty_print`为True，则JSON将以缩进级别4进行漂亮打印。

示例：

```py
>>> from datasets import load_metric
>>> metric = load_metric("accuracy")
>>> metric.info.write_to_directory("/path/to/directory/")
```

## Metric

基类`Metric`实现了一个由一个或多个[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)支持的Metric。

### `class datasets.Metric`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/metric.py#L147)

```py
( config_name: Optional = None keep_in_memory: bool = False cache_dir: Optional = None num_process: int = 1 process_id: int = 0 seed: Optional = None experiment_id: Optional = None max_concurrent_cache_files: int = 10000 timeout: Union = 100 **kwargs )
```

参数

+   `config_name`（`str`）— 这用于定义与度量计算脚本特定的哈希，并防止在修改度量加载脚本时覆盖度量的数据。

+   `keep_in_memory`（`bool`）— 保留所有预测和参考数据在内存中。在分布式设置中不可能。

+   `cache_dir`（`str`）— 用于存储临时预测/参考数据的目录路径。数据目录应位于分布式设置中的共享文件系统上。

+   `num_process`（`int`）— 指定分布式设置中的节点总数。这对于在分布式设置中计算指标很有用（特别是像F1这样的非加法指标）。

+   `process_id` (`int`) — 指定分布式设置中当前进程的 id（在 0 和 num_process-1 之间）。这对于在分布式设置中计算指标很有用（特别是像 F1 这样的非加法指标）。

+   `seed` (`int`, optional) — 如果指定，当运行 [datasets.Metric.compute()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric.compute) 时，这将临时设置 numpy 的随机种子。

+   `experiment_id` (`str`) — 特定的实验 id。这在多个分布式评估共享同一文件系统时使用。这对于在分布式设置中计算指标很有用（特别是像 F1 这样的非加法指标）。

+   `max_concurrent_cache_files` (`int`) — 最大并发度量缓存文件数（默认为 10000）。

+   `timeout` (`Union[int, float]`) — 分布式设置同步的超时时间（秒）。

Metric 是所有指标的基类和通用 API。

在 2.5.0 中已弃用

使用新的库 🤗 评估而不是：[https://huggingface.co/docs/evaluate](https://huggingface.co/docs/evaluate)

#### `add`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/metric.py#L522)

```py
( prediction = None reference = None **kwargs )
```

参数

+   `prediction` (list/array/tensor, optional) — 预测。

+   `reference` (list/array/tensor, optional) — 参考。

添加一个预测和参考到指标的堆栈中。

示例：

```py
>>> from datasets import load_metric
>>> metric = load_metric("accuracy")
>>> metric.add(predictions=model_predictions, references=labels)
```

#### `add_batch`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/metric.py#L475)

```py
( predictions = None references = None **kwargs )
```

参数

+   `predictions` (list/array/tensor, optional) — 预测。

+   `references` (list/array/tensor, optional) — 参考。

添加一批预测和参考到指标的堆栈中。

示例：

```py
>>> from datasets import load_metric
>>> metric = load_metric("accuracy")
>>> metric.add_batch(predictions=model_prediction, references=labels)
```

#### `compute`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/metric.py#L404)

```py
( predictions = None references = None **kwargs )
```

参数

+   `predictions` (list/array/tensor, optional) — 预测。

+   `references` (list/array/tensor, optional) — 参考。

+   *`*kwargs` (optional) — 将转发给指标 `_compute` 方法的关键字参数（详细信息请参阅文档字符串）。

计算指标。

不允许使用位置参数以防止错误。

示例：

```py
>>> from datasets import load_metric
>>> metric = load_metric("accuracy")
>>> accuracy = metric.compute(predictions=model_prediction, references=labels)
```

#### `download_and_prepare`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/metric.py#L605)

```py
( download_config: Optional = None dl_manager: Optional = None )
```

参数

+   `download_config` ([DownloadConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadConfig), optional) — 特定的下载配置参数。

+   `dl_manager` ([DownloadManager](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager), optional) — 要使用的特定下载管理器。

下载并准备数据集以供阅读。

## 文件系统

### `class datasets.filesystems.S3FileSystem`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/filesystems/s3filesystem.py#L6)

```py
( *args **kwargs )
```

参数

+   `anon` (`bool`, 默认为 `False`) — 是否使用匿名连接（仅限公共存储桶）。如果为 `False`，则使用给定的密钥/秘密，或 boto 的凭证解析器（client_kwargs, 环境变量, 变量, 配置文件, EC2 IAM 服务器，按顺序）。

+   `key` (`str`) — 如果不是匿名的，使用这个访问密钥 ID，如果指定的话。

+   `secret` (`str`) — 如果不是匿名的，使用这个秘密访问密钥，如果指定的话。

+   `token` (`str`) — 如果不是匿名的，使用这个安全令牌，如果指定的话。

+   `use_ssl` (`bool`, 默认为 `True`) — 是否在与 S3 的连接中使用 SSL；可能不使用会更快，但不安全。如果 `client_kwargs` 中也设置了 `use_ssl`，则以 `client_kwargs` 中设置的值为准。

+   `s3_additional_kwargs` (`dict`) — 在调用 S3 API 方法时使用的参数。通常用于诸如 ServerSideEncryption 之类的事情。

+   `client_kwargs` (`dict`) — botocore 客户端的参数。

+   `requester_pays` (`bool`, 默认为 `False`) — 是否支持 `RequesterPays` 存储桶。

+   `default_block_size` (`int`) — 如果给定，用于 `open()` 的默认块大小值，如果没有在任何时候给定特定值。内置默认值为 5MB。

+   `default_fill_cache`（`bool`，默认为`True`）— 是否默认使用open进行缓存填充。参考`S3File.open`。

+   `default_cache_type`（`str`，默认为`bytes`）— 如果提供，则用于`open()`的默认`cache_type`值。如果不需要缓存，设置为`none`。查看fsspec的文档获取其他可用的`cache_type`值。

+   `version_aware`（`bool`，默认为`False`）— 是否支持存储桶版本控制。如果启用此功能，用户将需要具有处理版本化对象所需的IAM权限。

+   `cache_regions`（`bool`，默认为`False`）— 是否缓存存储桶区域。每当使用新存储桶时，它将首先找出它属于哪个区域，然后使用该区域的客户端。

+   `asynchronous`（`bool`，默认为`False`）— 是否从协程内部使用此实例。

+   `config_kwargs`（`dict`）— 传递给`botocore.client.Config`的参数。**kwargs — 核心会话的其他参数。

+   `session`（`aiobotocore.session.AioSession`）— 用于所有连接的会话。此会话将用于替代在S3FileSystem内部创建新会话。例如：`aiobotocore.session.AioSession(profile='test_user')`。

+   `skip_instance_cache`（`bool`）— 控制实例的重用。传递给`fsspec`。

+   `use_listings_cache`（`bool`）— 控制目录列表的重用。传递给`fsspec`。

+   `listings_expiry_time`（`int`或`float`）— 控制目录列表的重用。传递给`fsspec`。

+   `max_paths`（`int`）— 控制目录列表的重用。传递给`fsspec`。

`datasets.filesystems.S3FileSystem`是[`s3fs.S3FileSystem`](https://s3fs.readthedocs.io/en/latest/api.html)的子类。

用户可以使用这个类来访问S3，就像访问文件系统一样。它在S3存储之上提供了类似文件系统的API（ls、cp、open等）。可以明确提供凭据（`key=`、`secret=`）或使用boto的凭据方法。查看botocore文档获取更多信息。如果没有可用的凭据，可以使用`anon=True`。

示例：

从公共S3存储桶中列出文件。

```py
>>> import datasets
>>> s3 = datasets.filesystems.S3FileSystem(anon=True)
>>> s3.ls('public-datasets/imdb/train')
['dataset_info.json.json','dataset.arrow','state.json']
```

使用`aws_access_key_id`和`aws_secret_access_key`从私有S3存储桶中列出文件。

```py
>>> import datasets
>>> s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)
>>> s3.ls('my-private-datasets/imdb/train')
['dataset_info.json.json','dataset.arrow','state.json']
```

使用`S3Filesystem`与`botocore.session.Session`和自定义`aws_profile`。

```py
>>> import botocore
>>> from datasets.filesystems import S3Filesystem

>>> s3_session = botocore.session.Session(profile_name='my_profile_name')
>>> s3 = S3FileSystem(session=s3_session)
```

使用`S3Filesystem`和[load_from_disk()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_from_disk)从S3加载数据集。

```py
>>> from datasets import load_from_disk
>>> from datasets.filesystems import S3Filesystem

>>> s3 = S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)
>>> dataset = load_from_disk('s3://my-private-datasets/imdb/train', storage_options=s3.storage_options)
>>> print(len(dataset))
25000
```

使用`S3Filesystem`和[Dataset.save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk)将数据集保存到S3。

```py
>>> from datasets import load_dataset
>>> from datasets.filesystems import S3Filesystem

>>> dataset = load_dataset("imdb")
>>> s3 = S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)
>>> dataset.save_to_disk('s3://my-private-datasets/imdb/train', storage_options=s3.storage_options)
```

#### `datasets.filesystems.extract_path_from_uri`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/filesystems.py#L35)

```py
( dataset_path: str )
```

参数

+   `dataset_path`（`str`）— 数据集目录的路径（例如`dataset/train`）或远程uri（例如`s3://my-bucket/dataset/train`）。

预处理`dataset_path`并移除远程文件系统（例如移除`s3://`）。

#### `datasets.filesystems.is_remote_filesystem`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/filesystems/__init__.py#L51)

```py
( fs: AbstractFileSystem )
```

参数

+   `fs`（`fsspec.spec.AbstractFileSystem`）— 用于pythonic文件系统的抽象超类，例如`fsspec.filesystem('file')`或[datasets.filesystems.S3FileSystem](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.filesystems.S3FileSystem)。

检查`fs`是否是远程文件系统。

## 指纹

### `class datasets.fingerprint.Hasher`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/fingerprint.py#L205)

```py
( )
```

接受python对象作为输入的哈希器。
