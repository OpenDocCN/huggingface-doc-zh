- en: Text to speech
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°è¯­éŸ³
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/text-to-speech](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/text-to-speech)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/text-to-speech](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/text-to-speech)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-speech (TTS) is the task of creating natural-sounding speech from text,
    where the speech can be generated in multiple languages and for multiple speakers.
    Several text-to-speech models are currently available in ğŸ¤— Transformers, such
    as [Bark](../model_doc/bark), [MMS](../model_doc/mms), [VITS](../model_doc/vits)
    and [SpeechT5](../model_doc/speecht5).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ˜¯ä»æ–‡æœ¬åˆ›å»ºè‡ªç„¶è¯­éŸ³çš„ä»»åŠ¡ï¼Œè¯­éŸ³å¯ä»¥ç”¨å¤šç§è¯­è¨€å’Œå¤šä¸ªè¯´è¯è€…ç”Ÿæˆã€‚ç›®å‰åœ¨ğŸ¤— Transformersä¸­æœ‰å‡ ç§æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ï¼Œå¦‚[Bark](../model_doc/bark)ã€[MMS](../model_doc/mms)ã€[VITS](../model_doc/vits)å’Œ[SpeechT5](../model_doc/speecht5)ã€‚
- en: 'You can easily generate audio using the `"text-to-audio"` pipeline (or its
    alias - `"text-to-speech"`). Some models, like Bark, can also be conditioned to
    generate non-verbal communications such as laughing, sighing and crying, or even
    add music. Hereâ€™s an example of how you would use the `"text-to-speech"` pipeline
    with Bark:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥è½»æ¾ä½¿ç”¨`"text-to-audio"`æµæ°´çº¿ï¼ˆæˆ–å…¶åˆ«å`"text-to-speech"`ï¼‰ç”ŸæˆéŸ³é¢‘ã€‚ä¸€äº›æ¨¡å‹ï¼Œå¦‚Barkï¼Œè¿˜å¯ä»¥è¢«è°ƒèŠ‚ä»¥ç”Ÿæˆéè¯­è¨€äº¤æµï¼Œå¦‚ç¬‘å£°ã€å¹æ¯å’Œå“­æ³£ï¼Œç”šè‡³æ·»åŠ éŸ³ä¹ã€‚ä»¥ä¸‹æ˜¯æ‚¨å¦‚ä½•ä½¿ç”¨`"text-to-speech"`æµæ°´çº¿ä¸Barkçš„ç¤ºä¾‹ï¼š
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Hereâ€™s a code snippet you can use to listen to the resulting audio in a notebook:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒåœ¨ç¬”è®°æœ¬ä¸­å¬å–ç”Ÿæˆçš„éŸ³é¢‘ï¼š
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For more examples on what Bark and other pretrained TTS models can do, refer
    to our [Audio course](https://huggingface.co/learn/audio-course/chapter6/pre-trained_models).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³Barkå’Œå…¶ä»–é¢„è®­ç»ƒTTSæ¨¡å‹çš„æ›´å¤šç¤ºä¾‹ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„[éŸ³é¢‘è¯¾ç¨‹](https://huggingface.co/learn/audio-course/chapter6/pre-trained_models)ã€‚
- en: If you are looking to fine-tune a TTS model, the only text-to-speech models
    currently available in ğŸ¤— Transformers are [SpeechT5](model_doc/speecht5) and [FastSpeech2Conformer](model_doc/fastspeech2_conformer),
    though more will be added in the future. SpeechT5 is pre-trained on a combination
    of speech-to-text and text-to-speech data, allowing it to learn a unified space
    of hidden representations shared by both text and speech. This means that the
    same pre-trained model can be fine-tuned for different tasks. Furthermore, SpeechT5
    supports multiple speakers through x-vector speaker embeddings.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³è¦å¾®è°ƒTTSæ¨¡å‹ï¼Œç›®å‰åœ¨ğŸ¤— Transformersä¸­å”¯ä¸€å¯ç”¨çš„æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹æ˜¯[SpeechT5](model_doc/speecht5)å’Œ[FastSpeech2Conformer](model_doc/fastspeech2_conformer)ï¼Œæœªæ¥å°†ä¼šæ·»åŠ æ›´å¤šã€‚SpeechT5åœ¨æ–‡æœ¬åˆ°è¯­éŸ³å’Œè¯­éŸ³åˆ°æ–‡æœ¬æ•°æ®çš„ç»„åˆä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ æ–‡æœ¬å’Œè¯­éŸ³å…±äº«çš„éšè—è¡¨ç¤ºç©ºé—´ã€‚è¿™æ„å‘³ç€ç›¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥ç”¨äºä¸åŒçš„ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒSpeechT5é€šè¿‡x-vectorè¯´è¯è€…åµŒå…¥æ”¯æŒå¤šä¸ªè¯´è¯è€…ã€‚
- en: 'The remainder of this guide illustrates how to:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—çš„å…¶ä½™éƒ¨åˆ†å°†è¯´æ˜å¦‚ä½•ï¼š
- en: Fine-tune [SpeechT5](../model_doc/speecht5) that was originally trained on English
    speech on the Dutch (`nl`) language subset of the [VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli)
    dataset.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¾®è°ƒ[SpeechT5](../model_doc/speecht5)ï¼Œè¯¥æ¨¡å‹æœ€åˆæ˜¯åœ¨è‹±è¯­è¯­éŸ³ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œåœ¨[VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli)æ•°æ®é›†çš„è·å…°è¯­ï¼ˆ`nl`ï¼‰è¯­è¨€å­é›†ä¸Šã€‚
- en: 'Use your refined model for inference in one of two ways: using a pipeline or
    directly.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨ç²¾ç‚¼çš„æ¨¡å‹è¿›è¡Œæ¨ç†çš„ä¸¤ç§æ–¹å¼ä¹‹ä¸€ï¼šä½¿ç”¨æµæ°´çº¿æˆ–ç›´æ¥ã€‚
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Install ğŸ¤—Transformers from source as not all the SpeechT5 features have been
    merged into an official release yet:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æºä»£ç å®‰è£…ğŸ¤—Transformersï¼Œå› ä¸ºå¹¶éæ‰€æœ‰SpeechT5åŠŸèƒ½éƒ½å·²åˆå¹¶åˆ°å®˜æ–¹å‘å¸ƒä¸­ï¼š
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To follow this guide you will need a GPU. If youâ€™re working in a notebook,
    run the following line to check if a GPU is available:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æŒ‰ç…§æœ¬æŒ‡å—æ“ä½œï¼Œæ‚¨å°†éœ€è¦ä¸€ä¸ªGPUã€‚å¦‚æœæ‚¨åœ¨ç¬”è®°æœ¬ä¸­å·¥ä½œï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨ï¼š
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'or alternatively for AMD GPUs:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…é€‚ç”¨äºAMD GPUï¼š
- en: '[PRE5]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We encourage you to log in to your Hugging Face account to upload and share
    your model with the community. When prompted, enter your token to log in:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„Hugging Faceè´¦æˆ·ï¼Œå°†æ‚¨çš„æ¨¡å‹ä¸Šä¼ å¹¶ä¸ç¤¾åŒºåˆ†äº«ã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š
- en: '[PRE6]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Load the dataset
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½æ•°æ®é›†
- en: '[VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli) is a large-scale
    multilingual speech corpus consisting of data sourced from 2009-2020 European
    Parliament event recordings. It contains labelled audio-transcription data for
    15 European languages. In this guide, we are using the Dutch language subset,
    feel free to pick another subset.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli)æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè¯­éŸ³è¯­æ–™åº“ï¼ŒåŒ…å«2009-2020å¹´æ¬§æ´²è®®ä¼šæ´»åŠ¨å½•éŸ³çš„æ•°æ®ã€‚å®ƒåŒ…å«äº†15ç§æ¬§æ´²è¯­è¨€çš„å¸¦æ ‡ç­¾éŸ³é¢‘è½¬å½•æ•°æ®ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨è·å…°è¯­å­é›†ï¼Œå¯ä»¥éšæ„é€‰æ‹©å…¶ä»–å­é›†ã€‚'
- en: Note that VoxPopuli or any other automated speech recognition (ASR) dataset
    may not be the most suitable option for training TTS models. The features that
    make it beneficial for ASR, such as excessive background noise, are typically
    undesirable in TTS. However, finding top-quality, multilingual, and multi-speaker
    TTS datasets can be quite challenging.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼ŒVoxPopuliæˆ–ä»»ä½•å…¶ä»–è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ•°æ®é›†å¯èƒ½ä¸æ˜¯è®­ç»ƒTTSæ¨¡å‹çš„æœ€ä½³é€‰æ‹©ã€‚å¯¹äºASRæœ‰ç›Šçš„ç‰¹æ€§ï¼Œå¦‚è¿‡å¤šçš„èƒŒæ™¯å™ªéŸ³ï¼Œåœ¨TTSä¸­é€šå¸¸æ˜¯ä¸å¸Œæœ›çš„ã€‚ç„¶è€Œï¼Œæ‰¾åˆ°é«˜è´¨é‡ã€å¤šè¯­è¨€å’Œå¤šè¯´è¯è€…çš„TTSæ•°æ®é›†å¯èƒ½ä¼šéå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚
- en: 'Letâ€™s load the data:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åŠ è½½æ•°æ®ï¼š
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '20968 examples should be sufficient for fine-tuning. SpeechT5 expects audio
    data to have a sampling rate of 16 kHz, so make sure the examples in the dataset
    meet this requirement:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 20968ä¸ªç¤ºä¾‹åº”è¯¥è¶³å¤Ÿè¿›è¡Œå¾®è°ƒã€‚SpeechT5æœŸæœ›éŸ³é¢‘æ•°æ®çš„é‡‡æ ·ç‡ä¸º16 kHzï¼Œå› æ­¤è¯·ç¡®ä¿æ•°æ®é›†ä¸­çš„ç¤ºä¾‹ç¬¦åˆæ­¤è¦æ±‚ï¼š
- en: '[PRE8]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Preprocess the data
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†æ•°æ®
- en: 'Letâ€™s begin by defining the model checkpoint to use and loading the appropriate
    processor:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆå®šä¹‰è¦ä½¿ç”¨çš„æ¨¡å‹æ£€æŸ¥ç‚¹å¹¶åŠ è½½é€‚å½“çš„å¤„ç†å™¨ï¼š
- en: '[PRE9]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Text cleanup for SpeechT5 tokenization
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SpeechT5åˆ†è¯çš„æ–‡æœ¬æ¸…ç†
- en: 'Start by cleaning up the text data. Youâ€™ll need the tokenizer part of the processor
    to process the text:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆæ¸…ç†æ–‡æœ¬æ•°æ®ã€‚æ‚¨å°†éœ€è¦å¤„ç†æ–‡æœ¬çš„åˆ†è¯å™¨éƒ¨åˆ†ï¼š
- en: '[PRE10]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The dataset examples contain `raw_text` and `normalized_text` features. When
    deciding which feature to use as the text input, consider that the SpeechT5 tokenizer
    doesnâ€™t have any tokens for numbers. In `normalized_text` the numbers are written
    out as text. Thus, it is a better fit, and we recommend using `normalized_text`
    as input text.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†ç¤ºä¾‹åŒ…å«`raw_text`å’Œ`normalized_text`ç‰¹å¾ã€‚åœ¨å†³å®šä½¿ç”¨å“ªä¸ªç‰¹å¾ä½œä¸ºæ–‡æœ¬è¾“å…¥æ—¶ï¼Œè¯·è€ƒè™‘SpeechT5åˆ†è¯å™¨æ²¡æœ‰ä»»ä½•æ•°å­—æ ‡è®°ã€‚åœ¨`normalized_text`ä¸­ï¼Œæ•°å­—è¢«å†™æˆæ–‡æœ¬ã€‚å› æ­¤ï¼Œå®ƒæ›´é€‚åˆï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨`normalized_text`ä½œä¸ºè¾“å…¥æ–‡æœ¬ã€‚
- en: Because SpeechT5 was trained on the English language, it may not recognize certain
    characters in the Dutch dataset. If left as is, these characters will be converted
    to `<unk>` tokens. However, in Dutch, certain characters like `Ã ` are used to
    stress syllables. In order to preserve the meaning of the text, we can replace
    this character with a regular `a`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºSpeechT5æ˜¯åœ¨è‹±è¯­ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œå¯èƒ½æ— æ³•è¯†åˆ«è·å…°æ•°æ®é›†ä¸­çš„æŸäº›å­—ç¬¦ã€‚å¦‚æœä¿æŒåŸæ ·ï¼Œè¿™äº›å­—ç¬¦å°†è¢«è½¬æ¢ä¸º`<unk>`æ ‡è®°ã€‚ç„¶è€Œï¼Œåœ¨è·å…°è¯­ä¸­ï¼Œåƒ`Ã `è¿™æ ·çš„ç‰¹å®šå­—ç¬¦ç”¨äºå¼ºè°ƒéŸ³èŠ‚ã€‚ä¸ºäº†ä¿ç•™æ–‡æœ¬çš„å«ä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ™®é€šçš„`a`æ›¿æ¢è¿™ä¸ªå­—ç¬¦ã€‚
- en: To identify unsupported tokens, extract all unique characters in the dataset
    using the `SpeechT5Tokenizer` which works with characters as tokens. To do this,
    write the `extract_all_chars` mapping function that concatenates the transcriptions
    from all examples into one string and converts it to a set of characters. Make
    sure to set `batched=True` and `batch_size=-1` in `dataset.map()` so that all
    transcriptions are available at once for the mapping function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯†åˆ«ä¸æ”¯æŒçš„æ ‡è®°ï¼Œä½¿ç”¨`SpeechT5Tokenizer`æå–æ•°æ®é›†ä¸­çš„æ‰€æœ‰å”¯ä¸€å­—ç¬¦ï¼Œè¯¥åˆ†è¯å™¨ä½¿ç”¨å­—ç¬¦ä½œä¸ºæ ‡è®°ã€‚ä¸ºæ­¤ï¼Œç¼–å†™`extract_all_chars`æ˜ å°„å‡½æ•°ï¼Œå°†æ‰€æœ‰ç¤ºä¾‹çš„è½¬å½•è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå­—ç¬¦é›†ã€‚ç¡®ä¿åœ¨`dataset.map()`ä¸­è®¾ç½®`batched=True`å’Œ`batch_size=-1`ï¼Œä»¥ä¾¿æ‰€æœ‰è½¬å½•éƒ½å¯ä»¥ä¸€æ¬¡æ€§ç”¨äºæ˜ å°„å‡½æ•°ã€‚
- en: '[PRE11]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now you have two sets of characters: one with the vocabulary from the dataset
    and one with the vocabulary from the tokenizer. To identify any unsupported characters
    in the dataset, you can take the difference between these two sets. The resulting
    set will contain the characters that are in the dataset but not in the tokenizer.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨æœ‰ä¸¤ç»„å­—ç¬¦ï¼šä¸€ç»„æ¥è‡ªæ•°æ®é›†çš„è¯æ±‡è¡¨ï¼Œå¦ä¸€ç»„æ¥è‡ªåˆ†è¯å™¨çš„è¯æ±‡è¡¨ã€‚ä¸ºäº†è¯†åˆ«æ•°æ®é›†ä¸­çš„ä»»ä½•ä¸æ”¯æŒçš„å­—ç¬¦ï¼Œæ‚¨å¯ä»¥å–è¿™ä¸¤ç»„ä¹‹é—´çš„å·®é›†ã€‚ç»“æœé›†å°†åŒ…å«æ•°æ®é›†ä¸­å­˜åœ¨ä½†ä¸åœ¨åˆ†è¯å™¨ä¸­çš„å­—ç¬¦ã€‚
- en: '[PRE12]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To handle the unsupported characters identified in the previous step, define
    a function that maps these characters to valid tokens. Note that spaces are already
    replaced by `â–` in the tokenizer and donâ€™t need to be handled separately.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¤„ç†å‰ä¸€æ­¥éª¤ä¸­è¯†åˆ«å‡ºçš„ä¸æ”¯æŒçš„å­—ç¬¦ï¼Œå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå°†è¿™äº›å­—ç¬¦æ˜ å°„åˆ°æœ‰æ•ˆçš„æ ‡è®°ã€‚è¯·æ³¨æ„ï¼Œåˆ†è¯å™¨ä¸­çš„ç©ºæ ¼å·²ç»è¢«æ›¿æ¢ä¸º`â–`ï¼Œä¸éœ€è¦å•ç‹¬å¤„ç†ã€‚
- en: '[PRE13]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that you have dealt with special characters in the text, itâ€™s time to shift
    focus to the audio data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å·²ç»å¤„ç†äº†æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œæ˜¯æ—¶å€™å°†é‡ç‚¹è½¬ç§»åˆ°éŸ³é¢‘æ•°æ®ä¸Šäº†ã€‚
- en: Speakers
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å‘è¨€è€…
- en: The VoxPopuli dataset includes speech from multiple speakers, but how many speakers
    are represented in the dataset? To determine this, we can count the number of
    unique speakers and the number of examples each speaker contributes to the dataset.
    With a total of 20,968 examples in the dataset, this information will give us
    a better understanding of the distribution of speakers and examples in the data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: VoxPopuliæ•°æ®é›†åŒ…å«å¤šä½å‘è¨€è€…çš„è®²è¯ï¼Œä½†æ•°æ®é›†ä¸­ä»£è¡¨äº†å¤šå°‘ä½å‘è¨€è€…ï¼Ÿä¸ºäº†ç¡®å®šè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ç‹¬ç‰¹å‘è¨€è€…çš„æ•°é‡ä»¥åŠæ¯ä½å‘è¨€è€…å¯¹æ•°æ®é›†çš„è´¡çŒ®ç¤ºä¾‹æ•°é‡ã€‚åœ¨æ•°æ®é›†ä¸­å…±æœ‰20,968ä¸ªç¤ºä¾‹ï¼Œè¿™äº›ä¿¡æ¯å°†å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°äº†è§£æ•°æ®ä¸­å‘è¨€è€…å’Œç¤ºä¾‹çš„åˆ†å¸ƒã€‚
- en: '[PRE14]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: By plotting a histogram you can get a sense of how much data there is for each
    speaker.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ç»˜åˆ¶ç›´æ–¹å›¾ï¼Œæ‚¨å¯ä»¥äº†è§£æ¯ä½å‘è¨€è€…çš„æ•°æ®é‡ã€‚
- en: '[PRE15]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Speakers histogram](../Images/6da9c92a3752085ee6b533eafde281f1.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![å‘è¨€è€…ç›´æ–¹å›¾](../Images/6da9c92a3752085ee6b533eafde281f1.png)'
- en: The histogram reveals that approximately one-third of the speakers in the dataset
    have fewer than 100 examples, while around ten speakers have more than 500 examples.
    To improve training efficiency and balance the dataset, we can limit the data
    to speakers with between 100 and 400 examples.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´æ–¹å›¾æ˜¾ç¤ºï¼Œæ•°æ®é›†ä¸­å¤§çº¦ä¸‰åˆ†ä¹‹ä¸€çš„å‘è¨€è€…æ‹¥æœ‰å°‘äº100ä¸ªç¤ºä¾‹ï¼Œè€Œå¤§çº¦æœ‰åä½å‘è¨€è€…æ‹¥æœ‰è¶…è¿‡500ä¸ªç¤ºä¾‹ã€‚ä¸ºäº†æé«˜è®­ç»ƒæ•ˆç‡å¹¶å¹³è¡¡æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ•°æ®é™åˆ¶åœ¨å…·æœ‰100åˆ°400ä¸ªç¤ºä¾‹ä¹‹é—´çš„å‘è¨€è€…ã€‚
- en: '[PRE16]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Letâ€™s check how many speakers remain:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ£€æŸ¥è¿˜å‰©ä¸‹å¤šå°‘å‘è¨€è€…ï¼š
- en: '[PRE17]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Letâ€™s see how many examples are left:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹è¿˜å‰©ä¸‹å¤šå°‘ç¤ºä¾‹ï¼š
- en: '[PRE18]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You are left with just under 10,000 examples from approximately 40 unique speakers,
    which should be sufficient.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ç°åœ¨å‰©ä¸‹å¤§çº¦40ä½ç‹¬ç‰¹å‘è¨€è€…çš„ä¸åˆ°10,000ä¸ªç¤ºä¾‹ï¼Œè¿™åº”è¯¥è¶³å¤Ÿäº†ã€‚
- en: Note that some speakers with few examples may actually have more audio available
    if the examples are long. However, determining the total amount of audio for each
    speaker requires scanning through the entire dataset, which is a time-consuming
    process that involves loading and decoding each audio file. As such, we have chosen
    to skip this step here.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä¸€äº›ç¤ºä¾‹è¾ƒå°‘çš„å‘è¨€è€…å®é™…ä¸Šå¯èƒ½æœ‰æ›´å¤šçš„éŸ³é¢‘å¯ç”¨ï¼Œå¦‚æœç¤ºä¾‹å¾ˆé•¿ã€‚ç„¶è€Œï¼Œç¡®å®šæ¯ä½å‘è¨€è€…çš„æ€»éŸ³é¢‘é‡éœ€è¦æ‰«ææ•´ä¸ªæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªè€—æ—¶çš„è¿‡ç¨‹ï¼Œæ¶‰åŠåŠ è½½å’Œè§£ç æ¯ä¸ªéŸ³é¢‘æ–‡ä»¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€‰æ‹©è·³è¿‡è¿™ä¸€æ­¥éª¤ã€‚
- en: Speaker embeddings
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å‘è¨€è€…åµŒå…¥
- en: To enable the TTS model to differentiate between multiple speakers, youâ€™ll need
    to create a speaker embedding for each example. The speaker embedding is an additional
    input into the model that captures a particular speakerâ€™s voice characteristics.
    To generate these speaker embeddings, use the pre-trained [spkrec-xvect-voxceleb](https://huggingface.co/speechbrain/spkrec-xvect-voxceleb)
    model from SpeechBrain.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿TTSæ¨¡å‹èƒ½å¤ŸåŒºåˆ†å¤šä¸ªå‘è¨€è€…ï¼Œæ‚¨éœ€è¦ä¸ºæ¯ä¸ªç¤ºä¾‹åˆ›å»ºä¸€ä¸ªå‘è¨€è€…åµŒå…¥ã€‚å‘è¨€è€…åµŒå…¥æ˜¯æ¨¡å‹çš„å¦ä¸€ä¸ªè¾“å…¥ï¼Œæ•æ‰ç‰¹å®šå‘è¨€è€…çš„è¯­éŸ³ç‰¹å¾ã€‚ä¸ºäº†ç”Ÿæˆè¿™äº›å‘è¨€è€…åµŒå…¥ï¼Œä½¿ç”¨SpeechBrainä¸­çš„é¢„è®­ç»ƒ[spkrec-xvect-voxceleb](https://huggingface.co/speechbrain/spkrec-xvect-voxceleb)æ¨¡å‹ã€‚
- en: Create a function `create_speaker_embedding()` that takes an input audio waveform
    and outputs a 512-element vector containing the corresponding speaker embedding.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªåä¸º`create_speaker_embedding()`çš„å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥å—è¾“å…¥éŸ³é¢‘æ³¢å½¢ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªåŒ…å«ç›¸åº”å‘è¨€è€…åµŒå…¥çš„512å…ƒç´ å‘é‡ã€‚
- en: '[PRE19]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Itâ€™s important to note that the `speechbrain/spkrec-xvect-voxceleb` model was
    trained on English speech from the VoxCeleb dataset, whereas the training examples
    in this guide are in Dutch. While we believe that this model will still generate
    reasonable speaker embeddings for our Dutch dataset, this assumption may not hold
    true in all cases.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: For optimal results, we recommend training an X-vector model on the target speech
    first. This will ensure that the model is better able to capture the unique voice
    characteristics present in the Dutch language.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Processing the dataset
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, letâ€™s process the data into the format the model expects. Create a
    `prepare_dataset` function that takes in a single example and uses the `SpeechT5Processor`
    object to tokenize the input text and load the target audio into a log-mel spectrogram.
    It should also add the speaker embeddings as an additional input.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Verify the processing is correct by looking at a single example:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Speaker embeddings should be a 512-element vector:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The labels should be a log-mel spectrogram with 80 mel bins.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![Log-mel spectrogram with 80 mel bins](../Images/68b429200bca2c16fafb5b391302d4d7.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: 'Side note: If you find this spectrogram confusing, it may be due to your familiarity
    with the convention of placing low frequencies at the bottom and high frequencies
    at the top of a plot. However, when plotting spectrograms as an image using the
    matplotlib library, the y-axis is flipped and the spectrograms appear upside down.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Now apply the processing function to the entire dataset. This will take between
    5 and 10 minutes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Youâ€™ll see a warning saying that some examples in the dataset are longer than
    the maximum input length the model can handle (600 tokens). Remove those examples
    from the dataset. Here we go even further and to allow for larger batch sizes
    we remove anything over 200 tokens.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, create a basic train/test split:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Data collator
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to combine multiple examples into a batch, you need to define a custom
    data collator. This collator will pad shorter sequences with padding tokens, ensuring
    that all examples have the same length. For the spectrogram labels, the padded
    portions are replaced with the special value `-100`. This special value instructs
    the model to ignore that part of the spectrogram when calculating the spectrogram
    loss.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In SpeechT5, the input to the decoder part of the model is reduced by a factor
    2\. In other words, it throws away every other timestep from the target sequence.
    The decoder then predicts a sequence that is twice as long. Since the original
    target sequence length may be odd, the data collator makes sure to round the maximum
    length of the batch down to be a multiple of 2.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Train the model
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Load the pre-trained model from the same checkpoint as you used for loading
    the processor:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `use_cache=True` option is incompatible with gradient checkpointing. Disable
    it for training.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Define the training arguments. Here we are not computing any evaluation metrics
    during the training process. Instead, weâ€™ll only look at the loss:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Instantiate the `Trainer` object and pass the model, dataset, and data collator
    to it.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: And with that, youâ€™re ready to start training! Training will take several hours.
    Depending on your GPU, it is possible that you will encounter a CUDA â€œout-of-memoryâ€
    error when you start training. In this case, you can reduce the `per_device_train_batch_size`
    incrementally by factors of 2 and increase `gradient_accumulation_steps` by 2x
    to compensate.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To be able to use your checkpoint with a pipeline, make sure to save the processor
    with the checkpoint:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Push the final model to the ğŸ¤— Hub:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Inference
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inference with a pipeline
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Great, now that youâ€™ve fine-tuned a model, you can use it for inference! First,
    letâ€™s see how you can use it with a corresponding pipeline. Letâ€™s create a `"text-to-speech"`
    pipeline with your checkpoint:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Pick a piece of text in Dutch youâ€™d like narrated, e.g.:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'To use SpeechT5 with the pipeline, youâ€™ll need a speaker embedding. Letâ€™s get
    it from an example in the test dataset:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨ç®¡é“ä¸­ä½¿ç”¨SpeechT5ï¼Œæ‚¨éœ€è¦ä¸€ä¸ªè¯´è¯è€…åµŒå…¥ã€‚è®©æˆ‘ä»¬ä»æµ‹è¯•æ•°æ®é›†ä¸­çš„ä¸€ä¸ªç¤ºä¾‹ä¸­è·å–å®ƒï¼š
- en: '[PRE38]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now you can pass the text and speaker embeddings to the pipeline, and it will
    take care of the rest:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥å°†æ–‡æœ¬å’Œè¯´è¯è€…åµŒå…¥ä¼ é€’ç»™ç®¡é“ï¼Œå®ƒä¼šå¤„ç†å‰©ä¸‹çš„éƒ¨åˆ†ï¼š
- en: '[PRE39]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You can then listen to the result:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ‚¨å¯ä»¥å¬ç»“æœï¼š
- en: '[PRE40]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Run inference manually
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‰‹åŠ¨è¿è¡Œæ¨æ–­
- en: You can achieve the same inference results without using the pipeline, however,
    more steps will be required.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨ä¸ä½¿ç”¨ç®¡é“çš„æƒ…å†µä¸‹å®ç°ç›¸åŒçš„æ¨æ–­ç»“æœï¼Œä½†æ˜¯éœ€è¦æ›´å¤šçš„æ­¥éª¤ã€‚
- en: 'Load the model from the ğŸ¤— Hub:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ğŸ¤— HubåŠ è½½æ¨¡å‹ï¼š
- en: '[PRE41]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Pick an example from the test dataset obtain a speaker embedding.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æµ‹è¯•æ•°æ®é›†ä¸­é€‰æ‹©ä¸€ä¸ªç¤ºä¾‹è·å–è¯´è¯è€…åµŒå…¥ã€‚
- en: '[PRE42]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Define the input text and tokenize it.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰è¾“å…¥æ–‡æœ¬å¹¶å¯¹å…¶è¿›è¡Œæ ‡è®°åŒ–ã€‚
- en: '[PRE43]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Create a spectrogram with your model:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨çš„æ¨¡å‹åˆ›å»ºä¸€ä¸ªé¢‘è°±å›¾ï¼š
- en: '[PRE44]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Visualize the spectrogram, if youâ€™d like to:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ„¿æ„ï¼Œå¯è§†åŒ–é¢‘è°±å›¾ï¼š
- en: '[PRE45]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![Generated log-mel spectrogram](../Images/8bcf491c8356ebfa61722c3c271cd0f7.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: ï¼[ç”Ÿæˆçš„å¯¹æ•°æ¢…å°”é¢‘è°±å›¾]ï¼ˆ../Images/8bcf491c8356ebfa61722c3c271cd0f7.pngï¼‰
- en: Finally, use the vocoder to turn the spectrogram into sound.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä½¿ç”¨å£°ç å™¨å°†é¢‘è°±å›¾è½¬æ¢ä¸ºå£°éŸ³ã€‚
- en: '[PRE46]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: In our experience, obtaining satisfactory results from this model can be challenging.
    The quality of the speaker embeddings appears to be a significant factor. Since
    SpeechT5 was pre-trained with English x-vectors, it performs best when using English
    speaker embeddings. If the synthesized speech sounds poor, try using a different
    speaker embedding.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼Œä»è¿™ä¸ªæ¨¡å‹è·å¾—ä»¤äººæ»¡æ„çš„ç»“æœå¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¯´è¯è€…åµŒå…¥çš„è´¨é‡ä¼¼ä¹æ˜¯ä¸€ä¸ªé‡è¦å› ç´ ã€‚ç”±äºSpeechT5æ˜¯ç”¨è‹±è¯­x-vectorsé¢„è®­ç»ƒçš„ï¼Œå› æ­¤åœ¨ä½¿ç”¨è‹±è¯­è¯´è¯è€…åµŒå…¥æ—¶è¡¨ç°æœ€ä½³ã€‚å¦‚æœåˆæˆçš„è¯­éŸ³å¬èµ·æ¥å¾ˆå·®ï¼Œå°è¯•ä½¿ç”¨ä¸åŒçš„è¯´è¯è€…åµŒå…¥ã€‚
- en: Increasing the training duration is also likely to enhance the quality of the
    results. Even so, the speech clearly is Dutch instead of English, and it does
    capture the voice characteristics of the speaker (compare to the original audio
    in the example). Another thing to experiment with is the modelâ€™s configuration.
    For example, try using `config.reduction_factor = 1` to see if this improves the
    results.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å¢åŠ è®­ç»ƒæŒç»­æ—¶é—´ä¹Ÿå¯èƒ½ä¼šæé«˜ç»“æœçš„è´¨é‡ã€‚å³ä½¿å¦‚æ­¤ï¼Œè¯­éŸ³æ˜æ˜¾æ˜¯è·å…°è¯­è€Œä¸æ˜¯è‹±è¯­ï¼Œå¹¶ä¸”å®ƒæ•æ‰åˆ°è¯´è¯è€…çš„å£°éŸ³ç‰¹å¾ï¼ˆä¸ç¤ºä¾‹ä¸­çš„åŸå§‹éŸ³é¢‘è¿›è¡Œæ¯”è¾ƒï¼‰ã€‚å¦ä¸€ä¸ªè¦å°è¯•çš„æ˜¯æ¨¡å‹çš„é…ç½®ã€‚ä¾‹å¦‚ï¼Œå°è¯•ä½¿ç”¨`config.reduction_factor
    = 1`ï¼Œçœ‹çœ‹æ˜¯å¦ä¼šæ”¹å–„ç»“æœã€‚
- en: Finally, it is essential to consider ethical considerations. Although TTS technology
    has numerous useful applications, it may also be used for malicious purposes,
    such as impersonating someoneâ€™s voice without their knowledge or consent. Please
    use TTS judiciously and responsibly.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œé‡è¦çš„æ˜¯è€ƒè™‘é“å¾·è€ƒé‡ã€‚å°½ç®¡TTSæŠ€æœ¯æœ‰è®¸å¤šæœ‰ç”¨çš„åº”ç”¨ï¼Œä½†ä¹Ÿå¯èƒ½è¢«ç”¨äºæ¶æ„ç›®çš„ï¼Œä¾‹å¦‚æœªç»ä»–ä»¬çš„çŸ¥è¯†æˆ–åŒæ„å†’å……æŸäººçš„å£°éŸ³ã€‚è¯·æ˜æ™ºå’Œè´Ÿè´£ä»»åœ°ä½¿ç”¨TTSã€‚
