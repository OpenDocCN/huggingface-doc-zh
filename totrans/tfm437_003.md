# 快速导览

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/quicktour`](https://huggingface.co/docs/transformers/v4.37.2/en/quicktour)

快速上手 🤗 Transformers！无论您是开发人员还是日常用户，这个快速导览将帮助您入门，展示如何使用 pipeline()进行推理，加载一个预训练模型和预处理器与 AutoClass，并快速使用 PyTorch 或 TensorFlow 训练模型。如果您是初学者，我们建议您查看我们的教程或[课程](https://huggingface.co/course/chapter1/1)以获取更深入的解释。

在开始之前，请确保您已安装所有必要的库：

```py
!pip install transformers datasets
```

您还需要安装您喜欢的机器学习框架：

Pytorch 隐藏 Pytorch 内容

```py
pip install torch
```

TensorFlow 隐藏 TensorFlow 内容

```py
pip install tensorflow
```

## 管道

[`www.youtube-nocookie.com/embed/tiZFewofSLM`](https://www.youtube-nocookie.com/embed/tiZFewofSLM)

pipeline()是使用预训练模型进行推理的最简单和最快速的方法。您可以直接使用 pipeline()来处理许多不同模态的任务，其中一些显示在下表中：

要查看所有可用任务的完整列表，请查看 pipeline API 参考。

| **任务** | **描述** | **模态** | **管道标识符** |
| --- | --- | --- | --- |
| 文本分类 | 为给定的文本序列分配一个标签 | NLP | pipeline(task=“sentiment-analysis”) |
| 文本生成 | 根据提示生成文本 | NLP | pipeline(task=“text-generation”) |
| 摘要 | 生成文本或文档序列的摘要 | NLP | pipeline(task=“summarization”) |
| 图像分类 | 为图像分配一个标签 | 计算机视觉 | pipeline(task=“image-classification”) |
| 图像分割 | 为图像的每个像素分配一个标签（支持语义、全景和实例分割） | 计算机视觉 | pipeline(task=“image-segmentation”) |
| 物体检测 | 预测图像中物体的边界框和类别 | 计算机视觉 | pipeline(task=“object-detection”) |
| 音频分类 | 为一些音频数据分配一个标签 | 音频 | pipeline(task=“audio-classification”) |
| 自动语音识别 | 将语音转录为文本 | 音频 | pipeline(task=“automatic-speech-recognition”) |
| 视觉问答 | 回答关于图像的问题，给定一个图像和一个问题 | 多模态 | pipeline(task=“vqa”) |
| 文档问答 | 回答关于文档的问题，给定一个文档和一个问题 | 多模态 | pipeline(task=“document-question-answering”) |
| 图像字幕 | 为给定图像生成字幕 | 多模态 | pipeline(task=“image-to-text”) |

首先创建一个 pipeline()实例，并指定要用它进行的任务。在本指南中，您将使用 pipeline()进行情感分析作为示例：

```py
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis")
```

pipeline()会下载并缓存一个默认的[预训练模型](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)和情感分析的分词器。现在您可以在目标文本上使用`classifier`：

```py
>>> classifier("We are very happy to show you the 🤗 Transformers library.")
[{'label': 'POSITIVE', 'score': 0.9998}]
```

如果您有多个输入，请将输入作为列表传递给 pipeline()以返回一个字典列表：

```py
>>> results = classifier(["We are very happy to show you the 🤗 Transformers library.", "We hope you don't hate it."])
>>> for result in results:
...     print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.5309
```

pipeline()还可以迭代处理任何您喜欢的任务的整个数据集。在此示例中，让我们选择自动语音识别作为我们的任务：

```py
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")
```

加载音频数据集（有关更多详细信息，请参阅🤗数据集[快速入门](https://huggingface.co/docs/datasets/quickstart#audio)）。例如，加载[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)数据集：

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
```

您需要确保数据集的采样率与[`facebook/wav2vec2-base-960h`](https://huggingface.co/facebook/wav2vec2-base-960h)训练时的采样率匹配：

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
```

在调用`"audio"`列时，音频文件将自动加载并重新采样。从前 4 个样本中提取原始波形数组，并将其作为列表传递给管道：

```py
>>> result = speech_recognizer(dataset[:4]["audio"])
>>> print([d["text"] for d in result])
['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', "FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE", "I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS", 'HOW DO I FURN A JOINA COUT']
```

对于输入数据量较大的情况（比如语音或视觉），您将希望传递一个生成器而不是列表，以将所有输入加载到内存中。查看 pipeline API 参考以获取更多信息。

### 在管道中使用另一个模型和分词器

pipeline()可以适应[Hub](https://huggingface.co/models)中的任何模型，从而可以轻松地调整 pipeline()以适应其他用例。例如，如果您需要一个能够处理法语文本的模型，请使用 Hub 上的标签来过滤适当的模型。顶部过滤结果返回一个针对情感分析进行微调的多语言[BERT 模型](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)，您可以用于法语文本：

```py
>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
```

Pytorch 隐藏 Pytorch 内容

使用 AutoModelForSequenceClassification 和 AutoTokenizer 来加载预训练模型及其关联的分词器（关于`AutoClass`的更多信息请参见下一节）：

```py
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

TensorFlow 隐藏 TensorFlow 内容

使用 TFAutoModelForSequenceClassification 和 AutoTokenizer 来加载预训练模型及其关联的分词器（关于`TFAutoClass`的更多信息请参见下一节）：

```py
>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

在 pipeline()中指定模型和分词器，现在您可以在法语文本上应用`classifier`：

```py
>>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
>>> classifier("Nous sommes très heureux de vous présenter la bibliothèque 🤗 Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

如果找不到适合您用例的模型，您需要在您的数据上对预训练模型进行微调。查看我们的微调教程以了解如何操作。最后，在微调预训练模型后，请考虑在 Hub 上共享该模型，以使机器学习民主化！🤗

## AutoClass

[`www.youtube-nocookie.com/embed/AhChOFRegn4`](https://www.youtube-nocookie.com/embed/AhChOFRegn4)

在幕后，AutoModelForSequenceClassification 和 AutoTokenizer 类共同驱动您上面使用的 pipeline()。AutoClass 是一个快捷方式，可以根据预训练模型的名称或路径自动检索架构。您只需要为您的任务选择适当的`AutoClass`及其关联的预处理类。

让我们回到前一节的示例，看看如何使用`AutoClass`来复制 pipeline()的结果。

### AutoTokenizer

分词器负责将文本预处理为输入模型的数字数组。有多个规则管理标记化过程，包括如何拆分单词以及单词应该在什么级别拆分（在分词器摘要中了解更多关于分词的信息）。最重要的是要记住，您需要使用相同模型名称实例化分词器，以确保您使用与模型预训练时相同的标记化规则。

使用 AutoTokenizer 加载分词器：

```py
>>> from transformers import AutoTokenizer

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

将文本传递给分词器：

```py
>>> encoding = tokenizer("We are very happy to show you the 🤗 Transformers library.")
>>> print(encoding)
{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

分词器返回一个包含的字典：

+   input_ids：您的标记的数值表示。

+   attention_mask：指示应该关注哪些标记。

分词器还可以接受输入列表，并填充和截断文本以返回具有统一长度的批处理：

Pytorch 隐藏 Pytorch 内容

```py
>>> pt_batch = tokenizer(
...     ["We are very happy to show you the 🤗 Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="pt",
... )
```

TensorFlow 隐藏 TensorFlow 内容

```py
>>> tf_batch = tokenizer(
...     ["We are very happy to show you the 🤗 Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="tf",
... )
```

查看预处理教程，了解有关分词以及如何使用 AutoImageProcessor、AutoFeatureExtractor 和 AutoProcessor 预处理图像、音频和多模态输入的更多详细信息。

### AutoModel

Pytorch 隐藏 Pytorch 内容

🤗 Transformers 提供了一种简单而统一的方式来加载预训练实例。这意味着您可以加载一个 AutoModel，就像加载 AutoTokenizer 一样。唯一的区别是选择正确的 AutoModel 用于任务。对于文本（或序列）分类，您应该加载 AutoModelForSequenceClassification：

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

查看任务摘要以了解 AutoModel 类支持的任务。

现在直接将预处理的输入批次传递给模型。您只需通过添加`**`来解包字典：

```py
>>> pt_outputs = pt_model(**pt_batch)
```

模型在`logits`属性中输出最终激活值。将 softmax 函数应用于`logits`以检索概率：

```py
>>> from torch import nn

>>> pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
>>> print(pt_predictions)
tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
```

TensorFlow 隐藏 TensorFlow 内容

🤗 Transformers 提供了一种简单而统一的方式来加载预训练实例。这意味着您可以加载一个 TFAutoModel，就像加载 AutoTokenizer 一样。唯一的区别是选择正确的 TFAutoModel 用于任务。对于文本（或序列）分类，您应该加载 TFAutoModelForSequenceClassification：

```py
>>> from transformers import TFAutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
```

查看任务摘要以了解 AutoModel 类支持的任务。

现在直接将预处理的输入批次传递给模型。您可以直接传递张量：

```py
>>> tf_outputs = tf_model(tf_batch)
```

模型在`logits`属性中输出最终激活值。将 softmax 函数应用于`logits`以检索概率：

```py
>>> import tensorflow as tf

>>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
>>> tf_predictions
```

所有🤗 Transformers 模型（PyTorch 或 TensorFlow）在最终激活函数（如 softmax）之前输出张量，因为最终激活函数通常与损失融合在一起。模型输出是特殊的数据类，因此在 IDE 中可以自动完成其属性。模型输出的行为类似于元组或字典（可以使用整数、切片或字符串进行索引），在这种情况下，空属性将被忽略。

### 保存模型

Pytorch 隐藏 Pytorch 内容

一旦您的模型微调完成，您可以使用 PreTrainedModel.save_pretrained()保存模型及其分词器：

```py
>>> pt_save_directory = "./pt_save_pretrained"
>>> tokenizer.save_pretrained(pt_save_directory)
>>> pt_model.save_pretrained(pt_save_directory)
```

当您准备再次使用模型时，请使用 PreTrainedModel.from_pretrained()重新加载它：

```py
>>> pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")
```

TensorFlow 隐藏 TensorFlow 内容

一旦您的模型微调完成，您可以使用 TFPreTrainedModel.save_pretrained()保存模型及其分词器：

```py
>>> tf_save_directory = "./tf_save_pretrained"
>>> tokenizer.save_pretrained(tf_save_directory)
>>> tf_model.save_pretrained(tf_save_directory)
```

当您准备再次使用模型时，请使用 TFPreTrainedModel.from_pretrained()重新加载它：

```py
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")
```

🤗 Transformers 的一个特别酷的功能是能够将模型保存并重新加载为 PyTorch 或 TensorFlow 模型。`from_pt` 或 `from_tf` 参数可以将模型从一个框架转换为另一个框架：

Pytorch 隐藏 Pytorch 内容

```py
>>> from transformers import AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)
```

TensorFlow 隐藏 TensorFlow 内容

```py
>>> from transformers import TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)
```

## 自定义模型构建

您可以修改模型的配置类以更改模型的构建方式。配置指定模型的属性，例如隐藏层或注意力头的数量。当您从自定义配置类初始化模型时，您将从头开始。模型属性是随机初始化的，您需要在使用它以获得有意义的结果之前对模型进行训练。

首先导入 AutoConfig，然后加载您想要修改的预训练模型。在 AutoConfig.from_pretrained()中，您可以指定要更改的属性，比如注意力头的数量：

```py
>>> from transformers import AutoConfig

>>> my_config = AutoConfig.from_pretrained("distilbert-base-uncased", n_heads=12)
```

Pytorch 隐藏 Pytorch 内容

使用 AutoModel.from_config()从您的自定义配置创建模型：

```py
>>> from transformers import AutoModel

>>> my_model = AutoModel.from_config(my_config)
```

TensorFlow 隐藏 TensorFlow 内容

使用 TFAutoModel.from_config()从您的自定义配置创建模型：

```py
>>> from transformers import TFAutoModel

>>> my_model = TFAutoModel.from_config(my_config)
```

查看创建自定义架构指南，了解有关构建自定义配置的更多信息。

## Trainer - 一个 PyTorch 优化的训练循环

所有模型都是标准的[`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)，因此您可以在任何典型的训练循环中使用它们。虽然您可以编写自己的训练循环，🤗 Transformers 提供了一个用于 PyTorch 的 Trainer 类，其中包含基本的训练循环，并添加了额外的功能，如分布式训练、混合精度等。

根据您的任务，通常会将以下参数传递给 Trainer：

1.  您将从 PreTrainedModel 或[`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)开始：

    ```py
    >>> from transformers import AutoModelForSequenceClassification

    >>> model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
    ```

1.  TrainingArguments 包含您可以更改的模型超参数，如学习率、批量大小和训练的时代数。如果您不指定任何训练参数，将使用默认值：

    ```py
    >>> from transformers import TrainingArguments

    >>> training_args = TrainingArguments(
    ...     output_dir="path/to/save/folder/",
    ...     learning_rate=2e-5,
    ...     per_device_train_batch_size=8,
    ...     per_device_eval_batch_size=8,
    ...     num_train_epochs=2,
    ... )
    ```

1.  加载一个预处理类，比如分词器、图像处理器、特征提取器或处理器：

    ```py
    >>> from transformers import AutoTokenizer

    >>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
    ```

1.  加载数据集：

    ```py
    >>> from datasets import load_dataset

    >>> dataset = load_dataset("rotten_tomatoes")  # doctest: +IGNORE_RESULT
    ```

1.  创建一个函数来对数据集进行分词：

    ```py
    >>> def tokenize_dataset(dataset):
    ...     return tokenizer(dataset["text"])
    ```

    然后在整个数据集上应用它，使用[map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)：

    ```py
    >>> dataset = dataset.map(tokenize_dataset, batched=True)
    ```

1.  一个 DataCollatorWithPadding 来从您的数据集中创建一批示例：

    ```py
    >>> from transformers import DataCollatorWithPadding

    >>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    ```

现在将所有这些类聚集在 Trainer 中：

```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=dataset["train"],
...     eval_dataset=dataset["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
... )  # doctest: +SKIP
```

当你准备好时，调用 train()开始训练：

```py
>>> trainer.train()
```

对于使用序列到序列模型的任务，比如翻译或摘要，使用 Seq2SeqTrainer 和 Seq2SeqTrainingArguments 类。

您可以通过对 Trainer 中的方法进行子类化来自定义训练循环行为。这样可以自定义特性，如损失函数、优化器和调度器。查看 Trainer 参考，了解哪些方法可以被子类化。

另一种自定义训练循环的方法是使用 Callbacks。您可以使用回调函数与其他库集成，并检查训练循环以报告进度或提前停止训练。回调函数不会修改训练循环本身。要自定义像损失函数这样的东西，您需要对 Trainer 进行子类化。

## 使用 TensorFlow 进行训练

所有模型都是标准的[`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)，因此它们可以在 TensorFlow 中使用[Keras](https://keras.io/) API 进行训练。🤗 Transformers 提供了 prepare_tf_dataset()方法，可以轻松将数据集加载为`tf.data.Dataset`，这样您就可以立即开始使用 Keras 的[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)和[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)方法进行训练。

1.  您将从 TFPreTrainedModel 或[`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)开始：

    ```py
    >>> from transformers import TFAutoModelForSequenceClassification

    >>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
    ```

1.  加载一个预处理类，比如分词器、图像处理器、特征提取器或处理器：

    ```py
    >>> from transformers import AutoTokenizer

    >>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
    ```

1.  创建一个函数来对数据集进行分词：

    ```py
    >>> def tokenize_dataset(dataset):
    ...     return tokenizer(dataset["text"])  # doctest: +SKIP
    ```

1.  使用[map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)在整个数据集上应用分词器，然后将数据集和分词器传递给 prepare_tf_dataset()。如果需要，您还可以在这里更改批量大小和对数据集进行洗牌：

    ```py
    >>> dataset = dataset.map(tokenize_dataset)  # doctest: +SKIP
    >>> tf_dataset = model.prepare_tf_dataset(
    ...     dataset["train"], batch_size=16, shuffle=True, tokenizer=tokenizer
    ... )  # doctest: +SKIP
    ```

1.  当您准备好时，您可以调用`compile`和`fit`开始训练。请注意，Transformers 模型都有一个默认的与任务相关的损失函数，所以除非您想要，否则不需要指定一个：

    ```py
    >>> from tensorflow.keras.optimizers import Adam

    >>> model.compile(optimizer=Adam(3e-5))  # No loss argument!
    >>> model.fit(tf_dataset)  # doctest: +SKIP
    ```

## 接下来是什么？

现在您已经完成了🤗 Transformers 的快速导览，请查看我们的指南，学习如何做更具体的事情，比如编写自定义模型，为任务微调模型，以及如何使用脚本训练模型。如果您对学习更多关于🤗 Transformers 核心概念感兴趣，请拿杯咖啡，看看我们的概念指南！
