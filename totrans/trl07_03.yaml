- en: Quickstart
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/trl/quickstart](https://huggingface.co/docs/trl/quickstart)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fine-tuning a language model via PPO consists of roughly three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rollout**: The language model generates a response or continuation based
    on a query which could be the start of a sentence.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluation**: The query and response are evaluated with a function, model,
    human feedback, or some combination of them. The important thing is that this
    process should yield a scalar value for each query/response pair. The optimization
    will aim at maximizing this value.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Optimization**: This is the most complex part. In the optimisation step the
    query/response pairs are used to calculate the log-probabilities of the tokens
    in the sequences. This is done with the model that is trained and a reference
    model, which is usually the pre-trained model before fine-tuning. The KL-divergence
    between the two outputs is used as an additional reward signal to make sure the
    generated responses donâ€™t deviate too far from the reference language model. The
    active language model is then trained with PPO.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The full process is illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00db2b272395f1f726efb5b3cd2a73ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Minimal example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following code illustrates the steps above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In general, you would run steps 3-6 in a for-loop and run it on many diverse
    queries. You can find more realistic examples in the examples section.
  prefs: []
  type: TYPE_NORMAL
- en: How to use a trained model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After training a `AutoModelForCausalLMWithValueHead`, you can directly use the
    model in `transformers`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can also load your model with `AutoModelForCausalLMWithValueHead` if you
    want to use the value head, for example to continue training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
