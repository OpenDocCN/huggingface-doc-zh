- en: Migrating your code to ğŸ¤— Accelerate
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å°†æ‚¨çš„ä»£ç è¿ç§»åˆ° ğŸ¤— Accelerate
- en: 'Original text: [https://huggingface.co/docs/accelerate/basic_tutorials/migration](https://huggingface.co/docs/accelerate/basic_tutorials/migration)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/accelerate/basic_tutorials/migration](https://huggingface.co/docs/accelerate/basic_tutorials/migration)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial will detail how to easily convert existing PyTorch code to use
    ğŸ¤— Accelerate! Youâ€™ll see that by just changing a few lines of code, ğŸ¤— Accelerate
    can perform its magic and get you on your way toward running your code on distributed
    systems with ease!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹å°†è¯¦ç»†ä»‹ç»å¦‚ä½•è½»æ¾å°†ç°æœ‰çš„ PyTorch ä»£ç è½¬æ¢ä¸ºä½¿ç”¨ ğŸ¤— Accelerateï¼æ‚¨å°†çœ‹åˆ°ï¼Œé€šè¿‡ä»…æ›´æ”¹å‡ è¡Œä»£ç ï¼ŒğŸ¤— Accelerate å°±å¯ä»¥å‘æŒ¥å…¶é­”åŠ›ï¼Œè®©æ‚¨è½»æ¾åœ°å°†ä»£ç è¿è¡Œåœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸Šï¼
- en: The base training loop
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŸºæœ¬è®­ç»ƒå¾ªç¯
- en: To begin, write out a very basic PyTorch training loop.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œç¼–å†™ä¸€ä¸ªéå¸¸åŸºæœ¬çš„ PyTorch è®­ç»ƒå¾ªç¯ã€‚
- en: We are under the presumption that `training_dataloader`, `model`, `optimizer`,
    `scheduler`, and `loss_function` have been defined beforehand.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‡è®¾ `training_dataloader`ã€`model`ã€`optimizer`ã€`scheduler` å’Œ `loss_function`
    å·²ç»äº‹å…ˆå®šä¹‰å¥½ã€‚
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Add in ğŸ¤— Accelerate
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·»åŠ  ğŸ¤— Accelerate
- en: 'To start using ğŸ¤— Accelerate, first import and create an [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    instance:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¼€å§‹ä½¿ç”¨ ğŸ¤— Accelerateï¼Œè¯·é¦–å…ˆå¯¼å…¥å¹¶åˆ›å»ºä¸€ä¸ª [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    å®ä¾‹ï¼š
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    is the main force behind utilizing all the possible options for distributed training!'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    æ˜¯åˆ©ç”¨åˆ†å¸ƒå¼è®­ç»ƒçš„æ‰€æœ‰å¯èƒ½é€‰é¡¹çš„ä¸»è¦åŠ›é‡ï¼'
- en: Setting the right device
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¾ç½®æ­£ç¡®çš„è®¾å¤‡
- en: 'The [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    class knows the right device to move any PyTorch object to at any time, so you
    should change the definition of `device` to come from [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    ç±»çŸ¥é“ä»»ä½•æ—¶å€™å°†ä»»ä½• PyTorch å¯¹è±¡ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡ï¼Œå› æ­¤æ‚¨åº”è¯¥å°† `device` çš„å®šä¹‰æ›´æ”¹ä¸ºæ¥è‡ª [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)ï¼š'
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Preparing your objects
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å‡†å¤‡æ‚¨çš„å¯¹è±¡
- en: 'Next, you need to pass all of the important objects related to training into
    [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare).
    ğŸ¤— Accelerate will make sure everything is setup in the current environment for
    you to start training:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦å°†æ‰€æœ‰ä¸è®­ç»ƒç›¸å…³çš„é‡è¦å¯¹è±¡ä¼ é€’ç»™ [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)ã€‚ğŸ¤—
    Accelerate å°†ç¡®ä¿ä¸€åˆ‡éƒ½åœ¨å½“å‰ç¯å¢ƒä¸­è®¾ç½®å¥½ï¼Œä»¥ä¾¿æ‚¨å¼€å§‹è®­ç»ƒï¼š
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These objects are returned in the same order they were sent in. By default when
    using `device_placement=True`, all of the objects that can be sent to the right
    device will be. If you need to work with data that isnâ€™t passed to [~Accelerator.prepare]
    but should be on the active device, you should pass in the `device` you made earlier.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å¯¹è±¡ä»¥å‘é€é¡ºåºè¿”å›ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå½“ä½¿ç”¨ `device_placement=True` æ—¶ï¼Œæ‰€æœ‰å¯ä»¥å‘é€åˆ°æ­£ç¡®è®¾å¤‡çš„å¯¹è±¡éƒ½ä¼šè¢«å‘é€ã€‚å¦‚æœæ‚¨éœ€è¦å¤„ç†æœªä¼ é€’ç»™
    [~Accelerator.prepare] ä½†åº”è¯¥åœ¨æ´»åŠ¨è®¾å¤‡ä¸Šçš„æ•°æ®ï¼Œæ‚¨åº”è¯¥ä¼ å…¥ä¹‹å‰åˆ›å»ºçš„ `device`ã€‚
- en: Accelerate will only prepare objects that inherit from their respective PyTorch
    classes (such as `torch.optim.Optimizer`).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Accelerate ä»…ä¼šå‡†å¤‡ä»å„è‡ªçš„ PyTorch ç±»ç»§æ‰¿çš„å¯¹è±¡ï¼ˆå¦‚ `torch.optim.Optimizer`ï¼‰ã€‚
- en: Modifying the training loop
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¿®æ”¹è®­ç»ƒå¾ªç¯
- en: 'Finally, three lines of code need to be changed in the training loop. ğŸ¤— Accelerateâ€™s
    DataLoader classes will automatically handle the device placement by default,
    and [backward()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.backward)
    should be used for performing the backward pass:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåœ¨è®­ç»ƒå¾ªç¯ä¸­éœ€è¦æ›´æ”¹ä¸‰è¡Œä»£ç ã€‚ğŸ¤— Accelerate çš„ DataLoader ç±»å°†é»˜è®¤è‡ªåŠ¨å¤„ç†è®¾å¤‡æ”¾ç½®ï¼Œå¹¶ä¸”åº”ä½¿ç”¨ [backward()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.backward)
    æ¥æ‰§è¡Œåå‘ä¼ æ’­ï¼š
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: With that, your training loop is now ready to use ğŸ¤— Accelerate!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†è¿™äº›ï¼Œæ‚¨çš„è®­ç»ƒå¾ªç¯ç°åœ¨å·²å‡†å¤‡å¥½ä½¿ç”¨ ğŸ¤— Accelerateï¼
- en: The finished code
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®Œæˆçš„ä»£ç 
- en: 'Below is the final version of the converted code:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯è½¬æ¢åä»£ç çš„æœ€ç»ˆç‰ˆæœ¬ï¼š
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: More Resources
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ›´å¤šèµ„æº
- en: To check out more ways on how to migrate to ğŸ¤— Accelerate, check out our [interactive
    migration tutorial](https://huggingface.co/docs/accelerate/usage_guides/explore)
    which showcases other items that need to be watched for when using Accelerate
    and how to do so quickly.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤šå…³äºå¦‚ä½•è¿ç§»åˆ° ğŸ¤— Accelerate çš„æ–¹æ³•ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„ [äº¤äº’å¼è¿ç§»æ•™ç¨‹](https://huggingface.co/docs/accelerate/usage_guides/explore)ï¼Œå±•ç¤ºäº†åœ¨ä½¿ç”¨
    Accelerate æ—¶éœ€è¦æ³¨æ„çš„å…¶ä»–é¡¹ç›®ä»¥åŠå¦‚ä½•å¿«é€Ÿå®ç°ã€‚
