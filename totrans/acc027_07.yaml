- en: Migrating your code to ðŸ¤— Accelerate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/accelerate/basic_tutorials/migration](https://huggingface.co/docs/accelerate/basic_tutorials/migration)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/accelerate/v0.27.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/start.6e0fb178.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/scheduler.69131cc3.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/singletons.ac467c20.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/paths.b2f3aeca.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/app.67e11fc0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/index.e1f30d73.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/0.bfeed9f0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/4.2659a012.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Tip.22e79575.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/CodeBlock.30cef355.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Heading.0aab6758.js">
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial will detail how to easily convert existing PyTorch code to use
    ðŸ¤— Accelerate! Youâ€™ll see that by just changing a few lines of code, ðŸ¤— Accelerate
    can perform its magic and get you on your way toward running your code on distributed
    systems with ease!
  prefs: []
  type: TYPE_NORMAL
- en: The base training loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To begin, write out a very basic PyTorch training loop.
  prefs: []
  type: TYPE_NORMAL
- en: We are under the presumption that `training_dataloader`, `model`, `optimizer`,
    `scheduler`, and `loss_function` have been defined beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Add in ðŸ¤— Accelerate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start using ðŸ¤— Accelerate, first import and create an [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    is the main force behind utilizing all the possible options for distributed training!'
  prefs: []
  type: TYPE_NORMAL
- en: Setting the right device
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    class knows the right device to move any PyTorch object to at any time, so you
    should change the definition of `device` to come from [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Preparing your objects
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, you need to pass all of the important objects related to training into
    [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare).
    ðŸ¤— Accelerate will make sure everything is setup in the current environment for
    you to start training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: These objects are returned in the same order they were sent in. By default when
    using `device_placement=True`, all of the objects that can be sent to the right
    device will be. If you need to work with data that isnâ€™t passed to [~Accelerator.prepare]
    but should be on the active device, you should pass in the `device` you made earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Accelerate will only prepare objects that inherit from their respective PyTorch
    classes (such as `torch.optim.Optimizer`).
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the training loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, three lines of code need to be changed in the training loop. ðŸ¤— Accelerateâ€™s
    DataLoader classes will automatically handle the device placement by default,
    and [backward()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.backward)
    should be used for performing the backward pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: With that, your training loop is now ready to use ðŸ¤— Accelerate!
  prefs: []
  type: TYPE_NORMAL
- en: The finished code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Below is the final version of the converted code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: More Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To check out more ways on how to migrate to ðŸ¤— Accelerate, check out our [interactive
    migration tutorial](https://huggingface.co/docs/accelerate/usage_guides/explore)
    which showcases other items that need to be watched for when using Accelerate
    and how to do so quickly.
  prefs: []
  type: TYPE_NORMAL
