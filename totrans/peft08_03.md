# 快速导览

> 原文链接：[https://huggingface.co/docs/peft/quicktour](https://huggingface.co/docs/peft/quicktour)

PEFT提供了用于微调大型预训练模型的参数高效方法。传统范式是为每个下游任务微调模型的所有参数，但由于当今模型中参数数量巨大，这种方法变得极其昂贵和不切实际。相反，更有效的方法是训练较少数量的提示参数或使用低秩适应（LoRA）等重新参数化方法来减少可训练参数的数量。

这个快速导览将展示PEFT的主要特点，以及您如何训练或在通常无法在消费者设备上访问的大型模型上运行推理。

## 训练

每个PEFT方法都由一个[PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)类定义，该类存储构建[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)所需的所有重要参数。例如，要使用LoRA进行训练，请加载并创建一个[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)类，并指定以下参数：

+   `task_type`：要进行训练的任务（在本例中为序列到序列语言建模）

+   `inference_mode`：您是否在使用模型进行推理

+   `r`：低秩矩阵的维度

+   `lora_alpha`：低秩矩阵的缩放因子

+   `lora_dropout`：LoRA层的dropout概率

```py
from peft import LoraConfig, TaskType

peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)
```

有关其他可调整的参数的更多详细信息，请参阅[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)参考。

一旦[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)设置好，使用[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)函数创建一个[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)。它需要一个基础模型 - 您可以从Transformers库中加载 - 和包含如何配置模型以使用LoRA进行训练的[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)。

加载您想要微调的基础模型。

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/mt0-large")
```

使用[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)函数将基础模型和`peft_config`封装起来，创建一个[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)。使用`print_trainable_parameters`方法来了解模型中可训练参数的数量。

```py
from peft import get_peft_model

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
"output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282"
```

在[bigscience/mt0-large](https://huggingface.co/bigscience/mt0-large)的12亿参数中，您只训练了其中的0.19%！

就是这样🎉！现在您可以使用Transformers的[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)、Accelerate或任何自定义的PyTorch训练循环来训练模型。

例如，要使用[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类进行训练，请设置一个带有一些训练超参数的[TrainingArguments](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)类。

```py
training_args = TrainingArguments(
    output_dir="your-name/bigscience/mt0-large-lora",
    learning_rate=1e-3,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=2,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)
```

将模型、训练参数、数据集、分词器和其他必要组件传递给[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)，并调用[train](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)开始训练。

```py
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```

### 保存模型

在模型训练完成后，您可以使用[save_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)函数将模型保存到目录中。

```py
model.save_pretrained("output_dir")
```

您还可以使用[push_to_hub](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)函数将模型保存到Hub（请确保您首先登录到您的Hugging Face帐户）。

```py
from huggingface_hub import notebook_login

notebook_login()
model.push_to_hub("your-name/bigscience/mt0-large-lora")
```

这两种方法只保存经过训练的额外PEFT权重，这意味着存储、传输和加载非常高效。例如，使用LoRA训练的这个[facebook/opt-350m](https://huggingface.co/ybelkada/opt-350m-lora)模型只包含两个文件：`adapter_config.json`和`adapter_model.safetensors`。`adapter_model.safetensors`文件只有6.3MB！

![](../Images/5e34ae8912ca7fcb5554d98cb511bc58.png)

与模型权重的完整大小相比，存储在Hub上的opt-350m模型的适配器权重仅为约6MB，后者可能为约700MB。

## 推理

查看[AutoPeftModel](package_reference/auto_class) API参考，了解可用的`AutoPeftModel`类的完整列表。

使用[AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)类和[from_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法轻松加载任何经过PEFT训练的模型进行推理：

```py
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer
import torch

model = AutoPeftModelForCausalLM.from_pretrained("ybelkada/opt-350m-lora")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

model = model.to("cuda")
model.eval()
inputs = tokenizer("Preheat the oven to 350 degrees and place the cookie dough", return_tensors="pt")

outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=50)
print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])

"Preheat the oven to 350 degrees and place the cookie dough in the center of the oven. In a large bowl, combine the flour, baking powder, baking soda, salt, and cinnamon. In a separate bowl, combine the egg yolks, sugar, and vanilla."
```

对于没有明确支持的任务，如自动语音识别等，您仍然可以使用基本的[AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)类加载用于该任务的模型。

```py
from peft import AutoPeftModel

model = AutoPeftModel.from_pretrained("smangrul/openai-whisper-large-v2-LORA-colab")
```

## 下一步

现在您已经看到如何使用PEFT方法之一训练模型，我们鼓励您尝试一些其他方法，如提示调整。这些步骤与快速入门中显示的步骤非常相似：

1.  为PEFT方法准备一个[PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)

1.  使用[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)方法从配置和基本模型创建[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)

然后您可以按照自己的喜好进行训练！要加载用于推理的PEFT模型，您可以使用[AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)类。

如果您有兴趣使用其他PEFT方法训练模型，例如语义分割、多语言自动语音识别、DreamBooth、标记分类等，请随时查看任务指南。
