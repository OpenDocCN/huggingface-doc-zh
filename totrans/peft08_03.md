# å¿«é€Ÿå¯¼è§ˆ

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/peft/quicktour](https://huggingface.co/docs/peft/quicktour)

PEFTæä¾›äº†ç”¨äºå¾®è°ƒå¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°é«˜æ•ˆæ–¹æ³•ã€‚ä¼ ç»ŸèŒƒå¼æ˜¯ä¸ºæ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œä½†ç”±äºå½“ä»Šæ¨¡å‹ä¸­å‚æ•°æ•°é‡å·¨å¤§ï¼Œè¿™ç§æ–¹æ³•å˜å¾—æå…¶æ˜‚è´µå’Œä¸åˆ‡å®é™…ã€‚ç›¸åï¼Œæ›´æœ‰æ•ˆçš„æ–¹æ³•æ˜¯è®­ç»ƒè¾ƒå°‘æ•°é‡çš„æç¤ºå‚æ•°æˆ–ä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ç­‰é‡æ–°å‚æ•°åŒ–æ–¹æ³•æ¥å‡å°‘å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚

è¿™ä¸ªå¿«é€Ÿå¯¼è§ˆå°†å±•ç¤ºPEFTçš„ä¸»è¦ç‰¹ç‚¹ï¼Œä»¥åŠæ‚¨å¦‚ä½•è®­ç»ƒæˆ–åœ¨é€šå¸¸æ— æ³•åœ¨æ¶ˆè´¹è€…è®¾å¤‡ä¸Šè®¿é—®çš„å¤§å‹æ¨¡å‹ä¸Šè¿è¡Œæ¨ç†ã€‚

## è®­ç»ƒ

æ¯ä¸ªPEFTæ–¹æ³•éƒ½ç”±ä¸€ä¸ª[PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)ç±»å®šä¹‰ï¼Œè¯¥ç±»å­˜å‚¨æ„å»º[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)æ‰€éœ€çš„æ‰€æœ‰é‡è¦å‚æ•°ã€‚ä¾‹å¦‚ï¼Œè¦ä½¿ç”¨LoRAè¿›è¡Œè®­ç»ƒï¼Œè¯·åŠ è½½å¹¶åˆ›å»ºä¸€ä¸ª[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)ç±»ï¼Œå¹¶æŒ‡å®šä»¥ä¸‹å‚æ•°ï¼š

+   `task_type`ï¼šè¦è¿›è¡Œè®­ç»ƒçš„ä»»åŠ¡ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸ºåºåˆ—åˆ°åºåˆ—è¯­è¨€å»ºæ¨¡ï¼‰

+   `inference_mode`ï¼šæ‚¨æ˜¯å¦åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†

+   `r`ï¼šä½ç§©çŸ©é˜µçš„ç»´åº¦

+   `lora_alpha`ï¼šä½ç§©çŸ©é˜µçš„ç¼©æ”¾å› å­

+   `lora_dropout`ï¼šLoRAå±‚çš„dropoutæ¦‚ç‡

```py
from peft import LoraConfig, TaskType

peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)
```

æœ‰å…³å…¶ä»–å¯è°ƒæ•´çš„å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)å‚è€ƒã€‚

ä¸€æ—¦[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)è®¾ç½®å¥½ï¼Œä½¿ç”¨[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)å‡½æ•°åˆ›å»ºä¸€ä¸ª[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)ã€‚å®ƒéœ€è¦ä¸€ä¸ªåŸºç¡€æ¨¡å‹ - æ‚¨å¯ä»¥ä»Transformersåº“ä¸­åŠ è½½ - å’ŒåŒ…å«å¦‚ä½•é…ç½®æ¨¡å‹ä»¥ä½¿ç”¨LoRAè¿›è¡Œè®­ç»ƒçš„[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)ã€‚

åŠ è½½æ‚¨æƒ³è¦å¾®è°ƒçš„åŸºç¡€æ¨¡å‹ã€‚

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/mt0-large")
```

ä½¿ç”¨[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)å‡½æ•°å°†åŸºç¡€æ¨¡å‹å’Œ`peft_config`å°è£…èµ·æ¥ï¼Œåˆ›å»ºä¸€ä¸ª[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)ã€‚ä½¿ç”¨`print_trainable_parameters`æ–¹æ³•æ¥äº†è§£æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚

```py
from peft import get_peft_model

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
"output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282"
```

åœ¨[bigscience/mt0-large](https://huggingface.co/bigscience/mt0-large)çš„12äº¿å‚æ•°ä¸­ï¼Œæ‚¨åªè®­ç»ƒäº†å…¶ä¸­çš„0.19%ï¼

å°±æ˜¯è¿™æ ·ğŸ‰ï¼ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨Transformersçš„[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ã€Accelerateæˆ–ä»»ä½•è‡ªå®šä¹‰çš„PyTorchè®­ç»ƒå¾ªç¯æ¥è®­ç»ƒæ¨¡å‹ã€‚

ä¾‹å¦‚ï¼Œè¦ä½¿ç”¨[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ç±»è¿›è¡Œè®­ç»ƒï¼Œè¯·è®¾ç½®ä¸€ä¸ªå¸¦æœ‰ä¸€äº›è®­ç»ƒè¶…å‚æ•°çš„[TrainingArguments](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ç±»ã€‚

```py
training_args = TrainingArguments(
    output_dir="your-name/bigscience/mt0-large-lora",
    learning_rate=1e-3,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=2,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)
```

å°†æ¨¡å‹ã€è®­ç»ƒå‚æ•°ã€æ•°æ®é›†ã€åˆ†è¯å™¨å’Œå…¶ä»–å¿…è¦ç»„ä»¶ä¼ é€’ç»™[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼Œå¹¶è°ƒç”¨[train](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)å¼€å§‹è®­ç»ƒã€‚

```py
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```

### ä¿å­˜æ¨¡å‹

åœ¨æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[save_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)å‡½æ•°å°†æ¨¡å‹ä¿å­˜åˆ°ç›®å½•ä¸­ã€‚

```py
model.save_pretrained("output_dir")
```

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨[push_to_hub](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)å‡½æ•°å°†æ¨¡å‹ä¿å­˜åˆ°Hubï¼ˆè¯·ç¡®ä¿æ‚¨é¦–å…ˆç™»å½•åˆ°æ‚¨çš„Hugging Faceå¸æˆ·ï¼‰ã€‚

```py
from huggingface_hub import notebook_login

notebook_login()
model.push_to_hub("your-name/bigscience/mt0-large-lora")
```

è¿™ä¸¤ç§æ–¹æ³•åªä¿å­˜ç»è¿‡è®­ç»ƒçš„é¢å¤–PEFTæƒé‡ï¼Œè¿™æ„å‘³ç€å­˜å‚¨ã€ä¼ è¾“å’ŒåŠ è½½éå¸¸é«˜æ•ˆã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨LoRAè®­ç»ƒçš„è¿™ä¸ª[facebook/opt-350m](https://huggingface.co/ybelkada/opt-350m-lora)æ¨¡å‹åªåŒ…å«ä¸¤ä¸ªæ–‡ä»¶ï¼š`adapter_config.json`å’Œ`adapter_model.safetensors`ã€‚`adapter_model.safetensors`æ–‡ä»¶åªæœ‰6.3MBï¼

![](../Images/5e34ae8912ca7fcb5554d98cb511bc58.png)

ä¸æ¨¡å‹æƒé‡çš„å®Œæ•´å¤§å°ç›¸æ¯”ï¼Œå­˜å‚¨åœ¨Hubä¸Šçš„opt-350mæ¨¡å‹çš„é€‚é…å™¨æƒé‡ä»…ä¸ºçº¦6MBï¼Œåè€…å¯èƒ½ä¸ºçº¦700MBã€‚

## æ¨ç†

æŸ¥çœ‹[AutoPeftModel](package_reference/auto_class) APIå‚è€ƒï¼Œäº†è§£å¯ç”¨çš„`AutoPeftModel`ç±»çš„å®Œæ•´åˆ—è¡¨ã€‚

ä½¿ç”¨[AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)ç±»å’Œ[from_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•è½»æ¾åŠ è½½ä»»ä½•ç»è¿‡PEFTè®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼š

```py
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer
import torch

model = AutoPeftModelForCausalLM.from_pretrained("ybelkada/opt-350m-lora")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

model = model.to("cuda")
model.eval()
inputs = tokenizer("Preheat the oven to 350 degrees and place the cookie dough", return_tensors="pt")

outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=50)
print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])

"Preheat the oven to 350 degrees and place the cookie dough in the center of the oven. In a large bowl, combine the flour, baking powder, baking soda, salt, and cinnamon. In a separate bowl, combine the egg yolks, sugar, and vanilla."
```

å¯¹äºæ²¡æœ‰æ˜ç¡®æ”¯æŒçš„ä»»åŠ¡ï¼Œå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç­‰ï¼Œæ‚¨ä»ç„¶å¯ä»¥ä½¿ç”¨åŸºæœ¬çš„[AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)ç±»åŠ è½½ç”¨äºè¯¥ä»»åŠ¡çš„æ¨¡å‹ã€‚

```py
from peft import AutoPeftModel

model = AutoPeftModel.from_pretrained("smangrul/openai-whisper-large-v2-LORA-colab")
```

## ä¸‹ä¸€æ­¥

ç°åœ¨æ‚¨å·²ç»çœ‹åˆ°å¦‚ä½•ä½¿ç”¨PEFTæ–¹æ³•ä¹‹ä¸€è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬é¼“åŠ±æ‚¨å°è¯•ä¸€äº›å…¶ä»–æ–¹æ³•ï¼Œå¦‚æç¤ºè°ƒæ•´ã€‚è¿™äº›æ­¥éª¤ä¸å¿«é€Ÿå…¥é—¨ä¸­æ˜¾ç¤ºçš„æ­¥éª¤éå¸¸ç›¸ä¼¼ï¼š

1.  ä¸ºPEFTæ–¹æ³•å‡†å¤‡ä¸€ä¸ª[PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)

1.  ä½¿ç”¨[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)æ–¹æ³•ä»é…ç½®å’ŒåŸºæœ¬æ¨¡å‹åˆ›å»º[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)

ç„¶åæ‚¨å¯ä»¥æŒ‰ç…§è‡ªå·±çš„å–œå¥½è¿›è¡Œè®­ç»ƒï¼è¦åŠ è½½ç”¨äºæ¨ç†çš„PEFTæ¨¡å‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)ç±»ã€‚

å¦‚æœæ‚¨æœ‰å…´è¶£ä½¿ç”¨å…¶ä»–PEFTæ–¹æ³•è®­ç»ƒæ¨¡å‹ï¼Œä¾‹å¦‚è¯­ä¹‰åˆ†å‰²ã€å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€DreamBoothã€æ ‡è®°åˆ†ç±»ç­‰ï¼Œè¯·éšæ—¶æŸ¥çœ‹ä»»åŠ¡æŒ‡å—ã€‚
