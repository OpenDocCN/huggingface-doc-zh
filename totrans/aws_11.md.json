["```py\nPUBLIC_DNS=\"\" # IP address, e.g. ec2-3-80-....\nKEY_PATH=\"\" # local path to key, e.g. ssh/trn.pem\n\nssh -L 8080:localhost:8080 -i ${KEY_NAME}.pem ubuntu@$PUBLIC_DNS\n```", "```py\ngit clone https://github.com/huggingface/optimum-neuron.git\n```", "```py\n# change directory\ncd optimum-neuron/notebooks/text-generation\n# launch jupyter\npython -m notebook --allow-root --port=8080\n```", "```py\n!huggingface-cli login --token YOUR_TOKEN\n```", "```py\n{\n  \"instruction\": \"What is world of warcraft\",\n  \"context\": \"\",\n  \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n}\n```", "```py\nfrom datasets import load_dataset\nfrom random import randrange\n\n# Load dataset from the hub\ndataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n\nprint(f\"dataset size: {len(dataset)}\")\nprint(dataset[randrange(len(dataset))])\n# dataset size: 15011\n\n```", "```py\ndef format_dolly(sample):\n    instruction = f\"### Instruction\\n{sample['instruction']}\"\n    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n    response = f\"### Answer\\n{sample['response']}\"\n    # join all the parts together\n    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n    return prompt\n\n```", "```py\nfrom random import randrange\n\nprint(format_dolly(dataset[randrange(len(dataset))]))\n```", "```py\nfrom transformers import AutoTokenizer\n\n# Hugging Face model id\nmodel_id = \"philschmid/Llama-2-7b-hf\" # ungated\n# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n```", "```py\nfrom random import randint\n# add utils method to path for loading dataset\nimport sys\nsys.path.append(\"./scripts/utils\") # make sure you change this to the correct path\nfrom pack_dataset import pack_dataset\n\n# template dataset to add prompt to each sample\ndef template_dataset(sample):\n    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n    return sample\n\n# apply prompt template per sample\ndataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n# print random sample\nprint(dataset[randint(0, len(dataset))][\"text\"])\n\n# tokenize dataset\ndataset = dataset.map(\n    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n)\n\n# chunk dataset\nlm_dataset = pack_dataset(dataset, chunk_length=2048) # We use 2048 as the maximum length for packing\n```", "```py\n# save train_dataset to disk\ndataset_path = \"tokenized_dolly\"\nlm_dataset.save_to_disk(dataset_path)\n```", "```py\n# precompilation command\n!MALLOC_ARENA_MAX=64 neuron_parallel_compile torchrun --nproc_per_node=32 scripts/run_clm.py \\\n --model_id {model_id} \\\n --dataset_path {dataset_path} \\\n --bf16 True \\\n --learning_rate 5e-5 \\\n --output_dir dolly_llama \\\n --overwrite_output_dir True \\\n --per_device_train_batch_size 1 \\\n --gradient_checkpointing True \\\n --tensor_parallel_size 8 \\\n --max_steps 10 \\\n --logging_steps 10 \\\n --gradient_accumulation_steps 16\n```", "```py\n# remove dummy artifacts which are created by the precompilation command\n!rm -rf dolly_llama\n```", "```py\n!MALLOC_ARENA_MAX=64 torchrun --nproc_per_node=32 scripts/run_clm.py \\\n --model_id {model_id} \\\n --dataset_path {dataset_path} \\\n --bf16 True \\\n --learning_rate 5e-5 \\\n --output_dir dolly_llama \\\n --overwrite_output_dir True \\\n --skip_cache_push True \\\n --per_device_train_batch_size 1 \\\n --gradient_checkpointing True \\\n --tensor_parallel_size 8 \\\n --num_train_epochs 3 \\\n --logging_steps 10 \\\n --gradient_accumulation_steps 16\n```", "```py\n!optimum-cli neuron consolidate dolly_llama/tensor_parallel_shards dolly_llama\n```", "```py\n!rm -rf dolly_llama/tensor_parallel_shards\n```", "```py\nfrom optimum.neuron import NeuronModelForCausalLM\nfrom transformers import AutoTokenizer\n\ncompiler_args = {\"num_cores\": 2, \"auto_cast_type\": 'fp16'}\ninput_shapes = {\"batch_size\": 1, \"sequence_length\": 2048}\n\ntokenizer = AutoTokenizer.from_pretrained(\"dolly_llama\")\nmodel = NeuronModelForCausalLM.from_pretrained(\n        \"dolly_llama\",\n        export=True,\n        **compiler_args,\n        **input_shapes)\n\n```", "```py\n# COMMENT IN if you want to save the compiled model\n# model.save_pretrained(\"compiled_dolly_llama\")\n```", "```py\ndef format_dolly_inference(sample):\n    instruction = f\"### Instruction\\n{sample['instruction']}\"\n    context = f\"### Context\\n{sample['context']}\" if \"context\" in sample else None\n    response = f\"### Answer\\n\"\n    # join all the parts together\n    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n    return prompt\n\ndef generate(sample):\n    prompt = format_dolly_inference(sample)\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    outputs = model.generate(**inputs,\n                         max_new_tokens=512,\n                         do_sample=True,\n                         temperature=0.9,\n                         top_k=50,\n                         top_p=0.9)\n    return tokenizer.decode(outputs[0], skip_special_tokens=False)[len(prompt):]\n```", "```py\nprompt = {\n  \"instruction\": \"Can you tell me something about AWS?\"\n}\nres = generate(prompt)\n\nprint(res)\n```", "```py\nprompt = {\n  \"instruction\": \"How can I train models on AWS Trainium?\",\n  \"context\": \"\ud83e\udd17 Optimum Neuron is the interface between the \ud83e\udd17 Transformers library and AWS Accelerators including [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls) and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/?nc1=h_ls). It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks.\"\n}\nres = generate(prompt)\n\nprint(res)\n```"]