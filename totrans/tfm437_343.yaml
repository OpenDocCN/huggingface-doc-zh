- en: DePlot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/deplot](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/deplot)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/107.b761b3b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DePlot was proposed in the paper [DePlot: One-shot visual language reasoning
    by plot-to-table translation](https://arxiv.org/abs/2212.10505) from Fangyu Liu,
    Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton
    Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract of the paper states the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Visual language such as charts and plots is ubiquitous in the human world.
    Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art
    (SOTA) models require at least tens of thousands of training examples and their
    reasoning capabilities are still much limited, especially on complex human-written
    queries. This paper presents the first one-shot solution to visual language reasoning.
    We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text
    translation, and (2) reasoning over the translated text. The key in this method
    is a modality conversion module, named as DePlot, which translates the image of
    a plot or chart to a linearized table. The output of DePlot can then be directly
    used to prompt a pretrained large language model (LLM), exploiting the few-shot
    reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table
    task by establishing unified task formats and metrics, and train DePlot end-to-end
    on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play
    fashion. Compared with a SOTA model finetuned on more than >28k data points, DePlot+LLM
    with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA
    on human-written queries from the task of chart QA.*'
  prefs: []
  type: TYPE_NORMAL
- en: DePlot is a model that is trained using `Pix2Struct` architecture. You can find
    more information about `Pix2Struct` in the [Pix2Struct documentation](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct).
    DePlot is a Visual Question Answering subset of `Pix2Struct` architecture. It
    renders the input question on the image and predicts the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Usage example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Currently one checkpoint is available for DePlot:'
  prefs: []
  type: TYPE_NORMAL
- en: '`google/deplot`: DePlot fine-tuned on ChartQA dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To fine-tune DePlot, refer to the pix2struct [fine-tuning notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb).
    For `Pix2Struct` models, we have found out that fine-tuning the model with Adafactor
    and cosine learning rate scheduler leads to faster convergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: DePlot is a model trained using `Pix2Struct` architecture. For API reference,
    see [`Pix2Struct` documentation](pix2struct).
  prefs: []
  type: TYPE_NORMAL
