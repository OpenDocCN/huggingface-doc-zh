- en: UNet
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UNet
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/loaders/unet](https://huggingface.co/docs/diffusers/api/loaders/unet)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/loaders/unet](https://huggingface.co/docs/diffusers/api/loaders/unet)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Some training methods - like LoRA and Custom Diffusion - typically target the
    UNet’s attention layers, but these training methods can also target other non-attention
    layers. Instead of training all of a model’s parameters, only a subset of the
    parameters are trained, which is faster and more efficient. This class is useful
    if you’re *only* loading weights into a UNet. If you need to load weights into
    the text encoder or a text encoder and UNet, try using the [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    function instead.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一些训练方法 - 如LoRA和自定义扩散 - 通常针对UNet的注意力层，但这些训练方法也可以针对其他非注意力层。与训练模型的所有参数不同，只训练参数的子集，这样更快更高效。如果您*只*需要将权重加载到UNet中，则此类非常有用。如果您需要将权重加载到文本编码器或文本编码器和UNet中，请尝试使用[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)函数。
- en: The `UNet2DConditionLoadersMixin` class provides functions for loading and saving
    weights, fusing and unfusing LoRAs, disabling and enabling LoRAs, and setting
    and deleting adapters.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '`UNet2DConditionLoadersMixin`类提供了用于加载和保存权重、融合和解除LoRA、禁用和启用LoRA以及设置和删除适配器的函数。'
- en: To learn more about how to load LoRA weights, see the [LoRA](../../using-diffusers/loading_adapters#lora)
    loading guide.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何加载LoRA权重的更多信息，请参阅[LoRA](../../using-diffusers/loading_adapters#lora)加载指南。
- en: UNet2DConditionLoadersMixin
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UNet2DConditionLoadersMixin
- en: '### `class diffusers.loaders.UNet2DConditionLoadersMixin`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.loaders.UNet2DConditionLoadersMixin`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/unet.py#L64)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/unet.py#L64)'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Load LoRA layers into a `UNet2DCondtionModel`.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 将LoRA层加载到`UNet2DCondtionModel`中。
- en: '#### `delete_adapters`'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `delete_adapters`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/unet.py#L661)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/unet.py#L661)'
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`adapter_names` (`Union[List[str], str]`) — The names (single string or list
    of strings) of the adapter to delete.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_names`（`Union[List[str], str]`）—要删除的适配器的名称（单个字符串或字符串列表）。'
- en: Delete an adapter’s LoRA layers from the UNet.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从UNet中删除适配器的LoRA层。
- en: 'Example:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `disable_lora`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_lora`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/unet.py#L615)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/unet.py#L615)'
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Disable the UNet’s active LoRA layers.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用UNet的活动LoRA层。
- en: 'Example:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#### `enable_lora`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_lora`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/unet.py#L638)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/unet.py#L638)'
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Enable the UNet’s active LoRA layers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 启用UNet的活动LoRA层。
- en: 'Example:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#### `load_attn_procs`'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_attn_procs`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/unet.py#L72)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/unet.py#L72)'
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pretrained_model_name_or_path_or_dict` (`str` or `os.PathLike` or `dict`)
    — Can be either:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name_or_path_or_dict`（`str`或`os.PathLike`或`dict`）—可以是以下之一：'
- en: A string, the model id (for example `google/ddpm-celebahq-256`) of a pretrained
    model hosted on the Hub.
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字符串，托管在Hub上的预训练模型的模型ID（例如`google/ddpm-celebahq-256`）。
- en: A path to a directory (for example `./my_model_directory`) containing the model
    weights saved with [ModelMixin.save_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.save_pretrained).
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个目录路径（例如`./my_model_directory`），其中包含使用[ModelMixin.save_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.save_pretrained)保存的模型权重。
- en: A [torch state dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个[torch状态字典](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)。
- en: '`cache_dir` (`Union[str, os.PathLike]`, *optional*) — Path to a directory where
    a downloaded pretrained model configuration is cached if the standard cache is
    not used.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache_dir`（`Union[str, os.PathLike]`，*可选*）—下载的预训练模型配置缓存在其中的目录路径，如果未使用标准缓存。'
- en: '`force_download` (`bool`, *optional*, defaults to `False`) — Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`force_download`（`bool`，*可选*，默认为`False`）—是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。'
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) — Whether or not
    to resume downloading the model weights and configuration files. If set to `False`,
    any incompletely downloaded files are deleted.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resume_download`（`bool`，*可选*，默认为`False`）—是否恢复下载模型权重和配置文件。如果设置为`False`，则删除任何未完全下载的文件。'
- en: '`proxies` (`Dict[str, str]`, *optional*) — A dictionary of proxy servers to
    use by protocol or endpoint, for example, `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`proxies`（`Dict[str, str]`，*可选*）—要使用的代理服务器的协议或端点的字典，例如，`{''http'': ''foo.bar:3128'',
    ''http://hostname'': ''foo.bar:4012''}`。每个请求都会使用代理。'
- en: '`local_files_only` (`bool`, *optional*, defaults to `False`) — Whether to only
    load local model weights and configuration files or not. If set to `True`, the
    model won’t be downloaded from the Hub.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_files_only`（`bool`，*可选*，默认为`False`）—是否仅加载本地模型权重和配置文件。如果设置为`True`，则模型不会从Hub下载。'
- en: '`token` (`str` or *bool*, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, the token generated from `diffusers-cli login` (stored
    in `~/.huggingface`) is used.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token`（`str`或*bool*，*可选*）—用作远程文件的HTTP bearer授权的令牌。如果为`True`，则使用从`diffusers-cli
    login`生成的令牌（存储在`~/.huggingface`中）。'
- en: '`low_cpu_mem_usage` (`bool`, *optional*, defaults to `True` if torch version
    >= 1.9.0 else `False`) — Speed up model loading only loading the pretrained weights
    and not initializing the weights. This also tries to not use more than 1x model
    size in CPU memory (including peak memory) while loading the model. Only supported
    for PyTorch >= 1.9.0\. If you are using an older version of PyTorch, setting this
    argument to `True` will raise an error.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_cpu_mem_usage` (`bool`, *optional*, defaults to `True` if torch version
    >= 1.9.0 else `False`) — 加速模型加载，仅加载预训练权重而不初始化权重。在加载模型时，尝试不使用超过1倍模型大小的CPU内存（包括峰值内存）。仅支持PyTorch
    >= 1.9.0。如果您使用较旧版本的PyTorch，将此参数设置为`True`将引发错误。'
- en: '`revision` (`str`, *optional*, defaults to `"main"`) — The specific model version
    to use. It can be a branch name, a tag name, a commit id, or any identifier allowed
    by Git.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision` (`str`, *optional*, defaults to `"main"`) — 要使用的特定模型版本。可以是分支名称、标签名称、提交ID或Git允许的任何标识符。'
- en: '`subfolder` (`str`, *optional*, defaults to `""`) — The subfolder location
    of a model file within a larger model repository on the Hub or locally.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subfolder` (`str`, *optional*, defaults to `""`) — 在Hub或本地的较大模型存储库中，模型文件的子文件夹位置。'
- en: '`mirror` (`str`, *optional*) — Mirror source to resolve accessibility issues
    if you’re downloading a model in China. We do not guarantee the timeliness or
    safety of the source, and you should refer to the mirror site for more information.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mirror` (`str`, *optional*) — 如果您在中国下载模型时遇到访问问题，可以使用镜像源解决。我们不保证源的及时性或安全性，您应参考镜像站点获取更多信息。'
- en: Load pretrained attention processor layers into [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel).
    Attention processor layers have to be defined in [`attention_processor.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)
    and be a `torch.nn.Module` class.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将预训练的注意力处理器层加载到[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)中。注意力处理器层必须在[`attention_processor.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义，并且必须是`torch.nn.Module`类。
- en: 'Example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#### `save_attn_procs`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_attn_procs`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/unet.py#L413)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/unet.py#L413)'
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`save_directory` (`str` or `os.PathLike`) — Directory to save an attention
    processor to (will be created if it doesn’t exist).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory` (`str` or `os.PathLike`) — 保存注意力处理器的目录（如果不存在将被创建）。'
- en: '`is_main_process` (`bool`, *optional*, defaults to `True`) — Whether the process
    calling this is the main process or not. Useful during distributed training and
    you need to call this function on all processes. In this case, set `is_main_process=True`
    only on the main process to avoid race conditions.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_main_process` (`bool`, *optional*, defaults to `True`) — 调用此函数的进程是否为主进程。在分布式训练期间很有用，当您需要在所有进程上调用此函数时。在这种情况下，仅在主进程上设置`is_main_process=True`以避免竞争条件。'
- en: '`save_function` (`Callable`) — The function to use to save the state dictionary.
    Useful during distributed training when you need to replace `torch.save` with
    another method. Can be configured with the environment variable `DIFFUSERS_SAVE_MODE`.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_function` (`Callable`) — 用于保存状态字典的函数。在分布式训练期间很有用，当您需要用另一种方法替换`torch.save`时。可以使用环境变量`DIFFUSERS_SAVE_MODE`进行配置。'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) — Whether to
    save the model using `safetensors` or with `pickle`.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization` (`bool`, *optional*, defaults to `True`) — 是否使用`safetensors`或`pickle`保存模型。'
- en: Save attention processor layers to a directory so that it can be reloaded with
    the [load_attn_procs()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.load_attn_procs)
    method.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将注意力处理器层保存到目录中，以便可以使用[load_attn_procs()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.load_attn_procs)方法重新加载。
- en: 'Example:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#### `set_adapters`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_adapters`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/unet.py#L567)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/unet.py#L567)'
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`adapter_names` (`List[str]` or `str`) — The names of the adapters to use.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_names` (`List[str]` or `str`) — 要使用的适配器的名称。'
- en: '`adapter_weights` (`Union[List[float], float]`, *optional*) — The adapter(s)
    weights to use with the UNet. If `None`, the weights are set to `1.0` for all
    the adapters.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_weights` (`Union[List[float], float]`, *optional*) — 用于UNet的适配器权重。如果为`None`，则所有适配器的权重设置为`1.0`。'
- en: Set the currently active adapters for use in the UNet.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 设置UNet中当前活动的适配器。
- en: 'Example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
