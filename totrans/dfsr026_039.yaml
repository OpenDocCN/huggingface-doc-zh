- en: Stable Diffusion XL Turbo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/sdxl_turbo](https://huggingface.co/docs/diffusers/using-diffusers/sdxl_turbo)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: SDXL Turbo is an adversarial time-distilled [Stable Diffusion XL](https://huggingface.co/papers/2307.01952)
    (SDXL) model capable of running inference in as little as 1 step.
  prefs: []
  type: TYPE_NORMAL
- en: This guide will show you how to use SDXL-Turbo for text-to-image and image-to-image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you begin, make sure you have the following libraries installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Load model checkpoints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Model weights may be stored in separate subfolders on the Hub or locally, in
    which case, you should use the [from_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained)
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the [from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file)
    method to load a model checkpoint stored in a single file format (`.ckpt` or `.safetensors`)
    from the Hub or locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Text-to-image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For text-to-image, pass a text prompt. By default, SDXL Turbo generates a 512x512
    image, and that resolution gives the best results. You can try setting the `height`
    and `width` parameters to 768x768 or 1024x1024, but you should expect quality
    degradations when doing so.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to set `guidance_scale` to 0.0 to disable, as the model was trained
    without it. A single inference step is enough to generate high quality images.
    Increasing the number of steps to 2, 3 or 4 should improve image quality.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![generated image of a racoon in a robe](../Images/792b5784d47e39b90a447d2316d5179c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image-to-image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For image-to-image generation, make sure that `num_inference_steps * strength`
    is larger or equal to 1. The image-to-image pipeline will run for `int(num_inference_steps
    * strength)` steps, e.g. `0.5 * 2.0 = 1` step in our example below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Image-to-image generation sample using SDXL Turbo](../Images/5f9d53ee5b337acb14785101d7b6149f.png)'
  prefs: []
  type: TYPE_IMG
- en: Speed-up SDXL Turbo even more
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Compile the UNet if you are using PyTorch version 2 or better. The first inference
    run will be very slow, but subsequent ones will be much faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When using the default VAE, keep it in `float32` to avoid costly `dtype` conversions
    before and after each generation. You only need to do this one before your first
    generation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As an alternative, you can also use a [16-bit VAE](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix)
    created by community member [`@madebyollin`](https://huggingface.co/madebyollin)
    that does not need to be upcasted to `float32`.
  prefs: []
  type: TYPE_NORMAL
