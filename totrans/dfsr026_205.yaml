- en: RePaintScheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/schedulers/repaint](https://huggingface.co/docs/diffusers/api/schedulers/repaint)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: '`RePaintScheduler` is a DDPM-based inpainting scheduler for unsupervised inpainting
    with extreme masks. It is designed to be used with the `RePaintPipeline`, and
    it is based on the paper [RePaint: Inpainting using Denoising Diffusion Probabilistic
    Models](https://huggingface.co/papers/2201.09865) by Andreas Lugmayr et al.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Free-form inpainting is the task of adding new content to an image in the
    regions specified by an arbitrary binary mask. Most existing approaches train
    for a certain distribution of masks, which limits their generalization capabilities
    to unseen mask types. Furthermore, training with pixel-wise and perceptual losses
    often leads to simple textural extensions towards the missing areas instead of
    semantically meaningful generation. In this work, we propose RePaint: A Denoising
    Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable
    to even extreme masks. We employ a pretrained unconditional DDPM as the generative
    prior. To condition the generation process, we only alter the reverse diffusion
    iterations by sampling the unmasked regions using the given image information.
    Since this technique does not modify or condition the original DDPM network itself,
    the model produces high-quality and diverse output images for any inpainting form.
    We validate our method for both faces and general-purpose image inpainting using
    standard and extreme masks. RePaint outperforms state-of-the-art Autoregressive,
    and GAN approaches for at least five out of six mask distributions. GitHub Repository:
    [this http URL](http://git.io/RePaint).*'
  prefs: []
  type: TYPE_NORMAL
- en: The original implementation can be found at [andreas128/RePaint](https://github.com/andreas128/).
  prefs: []
  type: TYPE_NORMAL
- en: RePaintScheduler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.RePaintScheduler`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_repaint.py#L91)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`num_train_timesteps` (`int`, defaults to 1000) — The number of diffusion steps
    to train the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta_start` (`float`, defaults to 0.0001) — The starting `beta` value of inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta_end` (`float`, defaults to 0.02) — The final `beta` value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta_schedule` (`str`, defaults to `"linear"`) — The beta schedule, a mapping
    from a beta range to a sequence of betas for stepping the model. Choose from `linear`,
    `scaled_linear`, `squaredcos_cap_v2`, or `sigmoid`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eta` (`float`) — The weight of noise for added noise in diffusion step. If
    its value is between 0.0 and 1.0 it corresponds to the DDIM scheduler, and if
    its value is between -0.0 and 1.0 it corresponds to the DDPM scheduler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trained_betas` (`np.ndarray`, *optional*) — Pass an array of betas directly
    to the constructor to bypass `beta_start` and `beta_end`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_sample` (`bool`, defaults to `True`) — Clip the predicted sample between
    -1 and 1 for numerical stability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RePaintScheduler` is a scheduler for DDPM inpainting inside a given mask.'
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)
    and [ConfigMixin](/docs/diffusers/v0.26.3/en/api/configuration#diffusers.ConfigMixin).
    Check the superclass documentation for the generic methods the library implements
    for all schedulers such as loading and saving.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `scale_model_input`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_repaint.py#L163)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sample` (`torch.FloatTensor`) — The input sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timestep` (`int`, *optional*) — The current timestep in the diffusion chain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: A scaled input sample.
  prefs: []
  type: TYPE_NORMAL
- en: Ensures interchangeability with schedulers that need to scale the denoising
    model input depending on the current timestep.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_timesteps`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_repaint.py#L180)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`) — The number of diffusion steps used when generating
    samples with a pre-trained model. If used, `timesteps` must be `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jump_length` (`int`, defaults to 10) — The number of steps taken forward in
    time before going backward in time for a single jump (“j” in RePaint paper). Take
    a look at Figure 9 and 10 in the paper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jump_n_sample` (`int`, defaults to 10) — The number of times to make a forward
    time jump for a given chosen time sample. Take a look at Figure 9 and 10 in the
    paper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`str` or `torch.device`, *optional*) — The device to which the timesteps
    should be moved to. If `None`, the timesteps are not moved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sets the discrete timesteps used for the diffusion chain (to be run before inference).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_repaint.py#L246)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model_output` (`torch.FloatTensor`) — The direct output from learned diffusion
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timestep` (`int`) — The current discrete timestep in the diffusion chain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample` (`torch.FloatTensor`) — A current instance of a sample created by
    the diffusion process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`original_image` (`torch.FloatTensor`) — The original image to inpaint on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask` (`torch.FloatTensor`) — The mask where a value of 0.0 indicates which
    part of the original image to inpaint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator`, *optional*) — A random number generator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [RePaintSchedulerOutput](/docs/diffusers/v0.26.3/en/api/schedulers/repaint#diffusers.schedulers.scheduling_repaint.RePaintSchedulerOutput)
    or `tuple`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[RePaintSchedulerOutput](/docs/diffusers/v0.26.3/en/api/schedulers/repaint#diffusers.schedulers.scheduling_repaint.RePaintSchedulerOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If return_dict is `True`, [RePaintSchedulerOutput](/docs/diffusers/v0.26.3/en/api/schedulers/repaint#diffusers.schedulers.scheduling_repaint.RePaintSchedulerOutput)
    is returned, otherwise a tuple is returned where the first element is the sample
    tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Predict the sample from the previous timestep by reversing the SDE. This function
    propagates the diffusion process from the learned model outputs (most often the
    predicted noise).
  prefs: []
  type: TYPE_NORMAL
- en: RePaintSchedulerOutput
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.schedulers.scheduling_repaint.RePaintSchedulerOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_repaint.py#L28)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prev_sample` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)` for images) — Computed sample (x_{t-1}) of previous timestep. `prev_sample`
    should be used as next model input in the denoising loop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pred_original_sample` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)` for images) — The predicted denoised sample (x_{0}) based on the
    model output from the current timestep. `pred_original_sample` can be used to
    preview progress or for guidance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output class for the scheduler’s step function output.
  prefs: []
  type: TYPE_NORMAL
