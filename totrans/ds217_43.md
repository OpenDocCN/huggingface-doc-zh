# å¤„ç†æ–‡æœ¬æ•°æ®

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/datasets/nlp_process](https://huggingface.co/docs/datasets/nlp_process)

æœ¬æŒ‡å—å±•ç¤ºäº†å¤„ç†æ–‡æœ¬æ•°æ®é›†çš„å…·ä½“æ–¹æ³•ã€‚å­¦ä¹ å¦‚ä½•ï¼š

+   ä½¿ç”¨ [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map) å¯¹æ•°æ®é›†è¿›è¡Œæ ‡è®°åŒ–ã€‚

+   å°†æ•°æ®é›†æ ‡ç­¾ä¸ NLI æ•°æ®é›†çš„æ ‡ç­¾ id å¯¹é½ã€‚

æœ‰å…³å¦‚ä½•å¤„ç†ä»»ä½•ç±»å‹çš„æ•°æ®é›†çš„æŒ‡å—ï¼Œè¯·æŸ¥çœ‹[é€šç”¨å¤„ç†æŒ‡å—](./process)ã€‚

## æ˜ å°„

[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map) å‡½æ•°æ”¯æŒä¸€æ¬¡å¤„ç†æ‰¹é‡ç¤ºä¾‹ï¼Œä»è€ŒåŠ å¿«æ ‡è®°åŒ–é€Ÿåº¦ã€‚

ä» ğŸ¤— [Transformers](https://huggingface.co/transformers/) åŠ è½½æ ‡è®°å™¨ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

åœ¨ [map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map) å‡½æ•°ä¸­å°† `batched` å‚æ•°è®¾ç½®ä¸º `True`ï¼Œä»¥å°†æ ‡è®°å™¨åº”ç”¨äºæ‰¹é‡ç¤ºä¾‹ï¼š

```py
>>> dataset = dataset.map(lambda examples: tokenizer(examples["text"]), batched=True)
>>> dataset[0]
{'text': 'the rock is destined to be the 21st century\'s new " conan " and that he\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 
 'label': 1, 
 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1000, 16608, 1000, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map) å‡½æ•°å°†è¿”å›çš„å€¼è½¬æ¢ä¸º PyArrow æ”¯æŒçš„æ ¼å¼ã€‚ä½†æ˜¯ï¼Œå°†å¼ é‡æ˜¾å¼è¿”å›ä¸º NumPy æ•°ç»„æ›´å¿«ï¼Œå› ä¸ºå®ƒæ˜¯æœ¬æœºæ”¯æŒçš„ PyArrow æ ¼å¼ã€‚åœ¨æ ‡è®°æ–‡æœ¬æ—¶è®¾ç½® `return_tensors="np"`ï¼š

```py
>>> dataset = dataset.map(lambda examples: tokenizer(examples["text"], return_tensors="np"), batched=True)
```

## å¯¹é½

[align_labels_with_mapping()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.align_labels_with_mapping) å‡½æ•°å°†æ•°æ®é›†æ ‡ç­¾ id ä¸æ ‡ç­¾åç§°å¯¹é½ã€‚å¹¶éæ‰€æœ‰ ğŸ¤— Transformers æ¨¡å‹éƒ½éµå¾ªåŸå§‹æ•°æ®é›†çš„è§„å®šæ ‡ç­¾æ˜ å°„ï¼Œç‰¹åˆ«æ˜¯å¯¹äº NLI æ•°æ®é›†ã€‚ä¾‹å¦‚ï¼Œ[MNLI](https://huggingface.co/datasets/glue) æ•°æ®é›†ä½¿ç”¨ä»¥ä¸‹æ ‡ç­¾æ˜ å°„ï¼š

```py
>>> label2id = {"entailment": 0, "neutral": 1, "contradiction": 2}
```

ä¸ºäº†å°†æ•°æ®é›†æ ‡ç­¾æ˜ å°„ä¸æ¨¡å‹ä½¿ç”¨çš„æ˜ å°„å¯¹é½ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«æ ‡ç­¾åç§°å’Œ id çš„å­—å…¸è¿›è¡Œå¯¹é½ï¼š

```py
>>> label2id = {"contradiction": 0, "neutral": 1, "entailment": 2}
```

å°†æ ‡ç­¾æ˜ å°„çš„å­—å…¸ä¼ é€’ç»™ [align_labels_with_mapping()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.align_labels_with_mapping) å‡½æ•°ï¼Œå¹¶æŒ‡å®šè¦å¯¹é½çš„åˆ—ï¼š

```py
>>> from datasets import load_dataset

>>> mnli = load_dataset("glue", "mnli", split="train")
>>> mnli_aligned = mnli.align_labels_with_mapping(label2id, "label")
```

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨æ­¤å‡½æ•°å°†æ ‡ç­¾æ˜ å°„åˆ° id çš„è‡ªå®šä¹‰æ˜ å°„ã€‚
