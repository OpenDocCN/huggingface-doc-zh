# Kandinsky 2.2

> 原始文本：[`huggingface.co/docs/diffusers/api/pipelines/kandinsky_v22`](https://huggingface.co/docs/diffusers/api/pipelines/kandinsky_v22)

Kandinsky 2.2 由 [Arseniy Shakhmatov](https://github.com/cene555)、[Anton Razzhigaev](https://github.com/razzant)、[Aleksandr Nikolich](https://github.com/AlexWortega)、[Vladimir Arkhipkin](https://github.com/oriBetelgeuse)、[Igor Pavlov](https://github.com/boomb0om)、[Andrey Kuznetsov](https://github.com/kuznetsoffandrey) 和 [Denis Dimitrov](https://github.com/denndimitrov) 创建。

其 GitHub 页面的描述为：

*Kandinsky 2.2 在其前身 Kandinsky 2.1 的基础上带来了实质性的改进，引入了一个新的、更强大的图像编码器 - CLIP-ViT-G 和 ControlNet 支持。将 CLIP-ViT-G 作为图像编码器显著增加了模型生成更美观图片和更好理解文本的能力，从而提升了模型的整体性能。ControlNet 机制的添加使模型能够有效地控制生成图像的过程。这导致更准确和视觉上吸引人的输出，并为文本引导的图像操作开辟了新的可能性。*

原始代码库可在 [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2) 找到。

查看 Hub 上的 [Kandinsky Community](https://huggingface.co/kandinsky-community) 组织，获取官方模型检查点，用于文本到图像、图像到图像和修补等任务。

请确保查看调度器的 指南 以了解如何探索调度器速度和质量之间的权衡，并查看 跨管道重用组件 部分，以了解如何有效地将相同组件加载到多个管道中。

## KandinskyV22PriorPipeline

### `class diffusers.KandinskyV22PriorPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_prior.py#L84)

```py
( prior: PriorTransformer image_encoder: CLIPVisionModelWithProjection text_encoder: CLIPTextModelWithProjection tokenizer: CLIPTokenizer scheduler: UnCLIPScheduler image_processor: CLIPImageProcessor )
```

参数

+   `prior` (PriorTransformer) — 用于从文本嵌入近似图像嵌入的标准 unCLIP 先验。

+   `image_encoder` (`CLIPVisionModelWithProjection`) — 冻结的图像编码器。

+   `text_encoder` (`CLIPTextModelWithProjection`) — 冻结的文本编码器。

+   `tokenizer` (`CLIPTokenizer`) — 类 [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer) 的分词器。

+   `scheduler` (`UnCLIPScheduler`) — 与 `prior` 结合使用的调度器，用于生成图像嵌入。

+   `image_processor` (`CLIPImageProcessor`) — 用于从 clip 预处理图像的 image_processor。

用于生成 Kandinsky 图像先验的管道

此模型继承自 DiffusionPipeline。查看超类文档以了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_prior.py#L370)

```py
( prompt: Union negative_prompt: Union = None num_images_per_prompt: int = 1 num_inference_steps: int = 25 generator: Union = None latents: Optional = None guidance_scale: float = 4.0 output_type: Optional = 'pt' return_dict: bool = True callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] ) → export const metadata = 'undefined';KandinskyPriorPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`) — 用于引导图像生成的提示或提示。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用于引导图像生成的提示或提示。如果不使用引导（即如果 `guidance_scale` 小于 `1`），则忽略。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `num_inference_steps` (`int`, *可选*, 默认为 100) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个 [torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html) 用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *可选*) — 预先生成的嘈杂潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，则将使用提供的随机 `generator` 进行采样生成潜变量张量。

+   `guidance_scale` (`float`, *可选*, 默认为 4.0) — 如 [无分类器扩散引导](https://arxiv.org/abs/2207.12598) 中定义的引导比例。`guidance_scale` 定义为 [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf) 中方程式 2 的 `w`。通过设置 `guidance_scale > 1` 启用引导比例。更高的引导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。

+   `output_type` (`str`, *可选*, 默认为 `"pt"`) — 生成图像的输出格式。可选择 `"np"` (`np.array`) 或 `"pt"` (`torch.Tensor`)。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回 ImagePipelineOutput 而不是普通的元组。

+   `callback_on_step_end` (`Callable`, *可选*) — 在推断过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs` 将包括由 `callback_on_step_end_tensor_inputs` 指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *可选*) — `callback_on_step_end` 函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包含在管道类的 `._callback_tensor_inputs` 属性中列出的变量。

返回

`KandinskyPriorPipelineOutput` 或 `tuple`

调用管道进行生成时调用的函数。

示例:

```py
>>> from diffusers import KandinskyV22Pipeline, KandinskyV22PriorPipeline
>>> import torch

>>> pipe_prior = KandinskyV22PriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-prior")
>>> pipe_prior.to("cuda")
>>> prompt = "red cat, 4k photo"
>>> image_emb, negative_image_emb = pipe_prior(prompt).to_tuple()

>>> pipe = KandinskyV22Pipeline.from_pretrained("kandinsky-community/kandinsky-2-2-decoder")
>>> pipe.to("cuda")
>>> image = pipe(
...     image_embeds=image_emb,
...     negative_image_embeds=negative_image_emb,
...     height=768,
...     width=768,
...     num_inference_steps=50,
... ).images
>>> image[0].save("cat.png")
```

#### `interpolate`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_prior.py#L131)

```py
( images_and_prompts: List weights: List num_images_per_prompt: int = 1 num_inference_steps: int = 25 generator: Union = None latents: Optional = None negative_prior_prompt: Optional = None negative_prompt: str = '' guidance_scale: float = 4.0 device = None ) → export const metadata = 'undefined';KandinskyPriorPipelineOutput or tuple
```

参数

+   `images_and_prompts` (`List[Union[str, PIL.Image.Image, torch.FloatTensor]]`) — 用于引导图像生成的提示和图像列表。权重 — (`List[float]`): `images_and_prompts` 中每个条件的权重列表

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `num_inference_steps` (`int`, *可选*, 默认为 100) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但推断速度较慢。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个 [torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html) 用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *可选*) — 预先生成的嘈杂潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，则将使用提供的随机 `generator` 进行采样生成潜变量张量。

+   `negative_prior_prompt` (`str`, *可选*) — 不用来引导先前扩散过程的提示。如果不使用引导（即如果 `guidance_scale` 小于 `1`，则忽略）。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用来引导图像生成的提示。如果不使用引导（即如果 `guidance_scale` 小于 `1`，则忽略）。

+   `guidance_scale`（`float`，*可选*，默认为 4.0）— 在[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。 `guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程式 2 的`w`。通过设置`guidance_scale > 1`启用引导比例。更高的引导比例鼓励生成与文本`prompt`紧密相关的图像，通常以降低图像质量为代价。

返回

`KandinskyPriorPipelineOutput`或`tuple`

在使用先前的管道进行插值时调用的函数。

示例：

```py
>>> from diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline
>>> from diffusers.utils import load_image
>>> import PIL
>>> import torch
>>> from torchvision import transforms

>>> pipe_prior = KandinskyV22PriorPipeline.from_pretrained(
...     "kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16
... )
>>> pipe_prior.to("cuda")
>>> img1 = load_image(
...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
...     "/kandinsky/cat.png"
... )
>>> img2 = load_image(
...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
...     "/kandinsky/starry_night.jpeg"
... )
>>> images_texts = ["a cat", img1, img2]
>>> weights = [0.3, 0.3, 0.4]
>>> out = pipe_prior.interpolate(images_texts, weights)
>>> pipe = KandinskyV22Pipeline.from_pretrained(
...     "kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16
... )
>>> pipe.to("cuda")
>>> image = pipe(
...     image_embeds=out.image_embeds,
...     negative_image_embeds=out.negative_image_embeds,
...     height=768,
...     width=768,
...     num_inference_steps=50,
... ).images[0]
>>> image.save("starry_cat.png")
```

## KandinskyV22Pipeline

### `class diffusers.KandinskyV22Pipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2.py#L64)

```py
( unet: UNet2DConditionModel scheduler: DDPMScheduler movq: VQModel )
```

参数

+   `scheduler`（Union[`DDIMScheduler`,`DDPMScheduler`]）— 与`unet`结合使用的调度器以生成图像潜在特征。

+   `unet`（UNet2DConditionModel）— 用于降噪图像嵌入的条件 U-Net 架构。

+   `movq`（VQModel）— MoVQ 解码器，用于从潜在特征生成图像。

用于使用 Kandinsky 进行文本到图像生成的管道

此模型继承自 DiffusionPipeline。查看超类文档以了解库为所有管道实现的通用方法（如下载或保存，运行在特定设备上等）。

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2.py#L122)

```py
( image_embeds: Union negative_image_embeds: Union height: int = 512 width: int = 512 num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `image_embeds`（`torch.FloatTensor`或`List[torch.FloatTensor]`）— 用于文本提示的剪辑图像嵌入，将用于条件图像生成。

+   `negative_image_embeds`（`torch.FloatTensor`或`List[torch.FloatTensor]`）— 用于负文本提示的剪辑图像嵌入，将用于条件图像生成。

+   `height`（`int`，*可选*，默认为 512）— 生成图像的高度（以像素为单位）。

+   `width`（`int`，*可选*，默认为 512）— 生成图像的宽度（以像素为单位）。

+   `num_inference_steps`（`int`，*可选*，默认为 100）— 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale`（`float`，*可选*，默认为 4.0）— 在[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。 `guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程式 2 的`w`。通过设置`guidance_scale > 1`启用引导比例。更高的引导比例鼓励生成与文本`prompt`紧密相关的图像，通常以降低图像质量为代价。

+   `num_images_per_prompt`（`int`，*可选*，默认为 1）— 每个提示生成的图像数量。

+   `generator`（`torch.Generator`或`List[torch.Generator]`，*可选*）— 一个或多个[torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents`（`torch.FloatTensor`，*可选*）— 预先生成的噪声潜在特征，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机`generator`进行采样生成潜在特征。

+   `output_type`（`str`，*可选*，默认为`"pil"`）— 生成图像的输出格式。可选择：`"pil"`（`PIL.Image.Image`），`"np"`（`np.array`）或`"pt"`（`torch.Tensor`）。

+   `return_dict`（`bool`，*可选*，默认为`True`）— 是否返回 ImagePipelineOutput 而不是普通元组。

+   `callback_on_step_end` (`Callable`, *可选*) — 在推断期间每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs` 将包含由 `callback_on_step_end_tensor_inputs` 指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *可选*) — `callback_on_step_end` 函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包含在管道类的 `._callback_tensor_inputs` 属性中列出的变量。

返回

ImagePipelineOutput 或 `tuple`

在调用生成管道时调用的函数。

示例:

```py
>>> from diffusers import KandinskyV22Pipeline, KandinskyV22PriorPipeline
>>> import torch

>>> pipe_prior = KandinskyV22PriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-prior")
>>> pipe_prior.to("cuda")
>>> prompt = "red cat, 4k photo"
>>> out = pipe_prior(prompt)
>>> image_emb = out.image_embeds
>>> zero_image_emb = out.negative_image_embeds
>>> pipe = KandinskyV22Pipeline.from_pretrained("kandinsky-community/kandinsky-2-2-decoder")
>>> pipe.to("cuda")
>>> image = pipe(
...     image_embeds=image_emb,
...     negative_image_embeds=zero_image_emb,
...     height=768,
...     width=768,
...     num_inference_steps=50,
... ).images
>>> image[0].save("cat.png")
```

## KandinskyV22CombinedPipeline

### `class diffusers.KandinskyV22CombinedPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_combined.py#L107)

```py
( unet: UNet2DConditionModel scheduler: DDPMScheduler movq: VQModel prior_prior: PriorTransformer prior_image_encoder: CLIPVisionModelWithProjection prior_text_encoder: CLIPTextModelWithProjection prior_tokenizer: CLIPTokenizer prior_scheduler: UnCLIPScheduler prior_image_processor: CLIPImageProcessor )
```

参数

+   `scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) — 与 `unet` 结合使用的调度器，用于生成图像潜变量。

+   `unet` (UNet2DConditionModel) — 用于去噪图像嵌入的条件 U-Net 架构。

+   `movq` (VQModel) — 用于从潜变量生成图像的 MoVQ 解码器。

+   `prior_prior` (PriorTransformer) — 用于从文本嵌入逼近图像嵌入的标准 unCLIP 先验。

+   `prior_image_encoder` (`CLIPVisionModelWithProjection`) — 冻结的图像编码器。

+   `prior_text_encoder` (`CLIPTextModelWithProjection`) — 冻结的文本编码器。

+   `prior_tokenizer` (`CLIPTokenizer`) — 类 [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer) 的分词器。

+   `prior_scheduler` (`UnCLIPScheduler`) — 与 `prior` 结合使用的调度器，用于生成图像嵌入。

+   `prior_image_processor` (`CLIPImageProcessor`) — 用于从 clip 预处理图像的图像处理器。

使用 Kandinsky 进行文本到图像生成的组合管道

此模型继承自 DiffusionPipeline。查看超类文档以了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_combined.py#L201)

```py
( prompt: Union negative_prompt: Union = None num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 height: int = 512 width: int = 512 prior_guidance_scale: float = 4.0 prior_num_inference_steps: int = 25 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True prior_callback_on_step_end: Optional = None prior_callback_on_step_end_tensor_inputs: List = ['latents'] callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt` (`str` or `List[str]`) — 用于指导图像生成的提示或提示。

+   `negative_prompt` (`str` or `List[str]`, *可选*) — 不用于指导图像生成的提示或提示。如果不使用指导（即如果 `guidance_scale` 小于 `1`），则忽略。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `num_inference_steps` (`int`, *可选*, 默认为 100) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推断速度。

+   `height` (`int`, *可选*, 默认为 512) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为 512) — 生成图像的像素宽度。

+   `prior_guidance_scale` (`float`, *optional*, 默认为 4.0) — 在[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`guidance_scale` 定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程 2 的 `w`。通过设置 `guidance_scale > 1` 启用引导比例。更高的引导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。

+   `prior_num_inference_steps` (`int`, *optional*, 默认为 100) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但推理速度会变慢。

+   `guidance_scale` (`float`, *optional*, 默认为 4.0) — 在[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`guidance_scale` 定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程 2 的 `w`。通过设置 `guidance_scale > 1` 启用引导比例。更高的引导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 一个或多个[torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *optional*) — 预生成的噪声潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示微调相同的生成。如果未提供，将使用提供的随机 `generator` 进行采样生成潜变量张量。

+   `output_type` (`str`, *optional*, 默认为 `"pil"`) — 生成图像的输出格式。可选择 `"pil"` (`PIL.Image.Image`)、`"np"` (`np.array`) 或 `"pt"` (`torch.Tensor`)。

+   `return_dict` (`bool`, *optional*, 默认为 `True`) — 是否返回 ImagePipelineOutput 而不是普通元组。

+   `prior_callback_on_step_end` (`Callable`, *optional*) — 在先验管道推理期间的每个降噪步骤结束时调用的函数。该函数将使用以下参数调用：`prior_callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。

+   `prior_callback_on_step_end_tensor_inputs` (`List`, *optional*) — `prior_callback_on_step_end` 函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包含在您的先验管道类的 `._callback_tensor_inputs` 属性中列出的变量。

+   `callback_on_step_end` (`Callable`, *optional*) — 在解码器管道推理期间的每个降噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs` 将包括由 `callback_on_step_end_tensor_inputs` 指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) — `callback_on_step_end` 函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包含在您的管道类的 `._callback_tensor_inputs` 属性中列出的变量。

返回

ImagePipelineOutput 或 `tuple`

调用管道生成时调用的函数。

示例：

```py
from diffusers import AutoPipelineForText2Image
import torch

pipe = AutoPipelineForText2Image.from_pretrained(
    "kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16
)
pipe.enable_model_cpu_offload()

prompt = "A lion in galaxies, spirals, nebulae, stars, smoke, iridescent, intricate detail, octane render, 8k"

image = pipe(prompt=prompt, num_inference_steps=25).images[0]
```

#### `enable_sequential_cpu_offload`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_combined.py#L181)

```py
( gpu_id = 0 )
```

使用加速器将所有模型转移到 CPU，显著减少内存使用。调用时，unet、text_encoder、vae 和 safety checker 的状态字典被保存到 CPU，然后移动到`torch.device('meta')，仅在特定子模块的`forward`方法被调用时加载到 GPU。请注意，转移是基于子模块的。与`enable_model_cpu_offload`相比，内存节省更多，但性能较低。

## KandinskyV22ControlnetPipeline

### `class diffusers.KandinskyV22ControlnetPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_controlnet.py#L106)

```py
( unet: UNet2DConditionModel scheduler: DDPMScheduler movq: VQModel )
```

参数

+   `scheduler` (DDIMScheduler) — 与`unet`一起使用的调度器，用于生成图像潜变量。

+   `unet` (UNet2DConditionModel) — 用于降噪图像嵌入的条件 U-Net 架构。

+   `movq` (VQModel) — MoVQ 解码器，用于从潜变量生成图像。

使用 Kandinsky 进行文本到图像生成的流水线

该模型继承自 DiffusionPipeline。查看超类文档以了解库为所有流水线实现的通用方法（如下载或保存，运行在特定设备上等）。

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_controlnet.py#L151)

```py
( image_embeds: Union negative_image_embeds: Union hint: FloatTensor height: int = 512 width: int = 512 num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt` (`str`或`List[str]`) — 用于指导图像生成的提示或提示。

+   `hint` (`torch.FloatTensor`) — 控制网络条件。

+   `image_embeds` (`torch.FloatTensor`或`List[torch.FloatTensor]`) — 用于条件图像生成的 clip 图像嵌入。

+   `negative_image_embeds` (`torch.FloatTensor`或`List[torch.FloatTensor]`) — 用于负文本提示的 clip 图像嵌入，将用于条件图像生成。

+   `negative_prompt` (`str`或`List[str]`, *可选*) — 不用于指导图像生成的提示或提示。如果不使用指导（即如果`guidance_scale`小于`1`，则忽略）。

+   `height` (`int`, *可选*，默认为 512) — 生成图像的像素高度。

+   `width` (`int`, *可选*，默认为 512) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *可选*，默认为 100) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *可选*，默认为 4.0) — 在[Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)中定义的指导比例。`guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程 2 的`w`。通过设置`guidance_scale > 1`来启用指导比例。更高的指导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `num_images_per_prompt` (`int`, *可选*，默认为 1) — 每个提示生成的图像数量。

+   `generator` (`torch.Generator`或`List[torch.Generator]`, *可选*) — 一个或多个[torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，使生成具有确定性。

+   `latents` (`torch.FloatTensor`, *可选*) — 预先生成的嘈杂潜变量，从高斯分布中采样，用作图像生成的输入。可以用不同的提示调整相同的生成。如果未提供，将使用提供的随机`generator`进行采样生成潜变量张量。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。可选择的值包括：`"pil"`（`PIL.Image.Image`）、`"np"`（`np.array`）或`"pt"`（`torch.Tensor`）。

+   `callback` (`Callable`, *可选*) — 在推理过程中每`callback_steps`步调用的函数。该函数使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *可选*, 默认为 1) — 调用`callback`函数的频率。如果未指定，则在每一步调用回调。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回 ImagePipelineOutput 而不是普通元组。

返回

ImagePipelineOutput 或 `tuple`

调用生成流水线时调用的函数。

示例:

## KandinskyV22PriorEmb2EmbPipeline

### `class diffusers.KandinskyV22PriorEmb2EmbPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_prior_emb2emb.py#L102)

```py
( prior: PriorTransformer image_encoder: CLIPVisionModelWithProjection text_encoder: CLIPTextModelWithProjection tokenizer: CLIPTokenizer scheduler: UnCLIPScheduler image_processor: CLIPImageProcessor )
```

参数

+   `prior` (PriorTransformer) — 用于从文本嵌入近似图像嵌入的标准 unCLIP 先验。

+   `image_encoder` (`CLIPVisionModelWithProjection`) — 冻结的图像编码器。

+   `text_encoder` (`CLIPTextModelWithProjection`) — 冻结的文本编码器。

+   `tokenizer` (`CLIPTokenizer`) — 类[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)的分词器。

+   `scheduler` (`UnCLIPScheduler`) — 与`prior`结合使用以生成图像嵌入的调度器。

用于生成 Kandinsky 图像先验的流水线

该模型继承自 DiffusionPipeline。查看超类文档以了解库为所有流水线实现的通用方法（如下载或保存，运行在特定设备上等）。

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_prior_emb2emb.py#L396)

```py
( prompt: Union image: Union strength: float = 0.3 negative_prompt: Union = None num_images_per_prompt: int = 1 num_inference_steps: int = 25 generator: Union = None guidance_scale: float = 4.0 output_type: Optional = 'pt' return_dict: bool = True ) → export const metadata = 'undefined';KandinskyPriorPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`) — 用于指导图像生成的提示或提示。

+   `strength` (`float`, *可选*, 默认为 0.8) — 在概念上，指示要转换参考`emb`的程度。必须介于 0 和 1 之间。`image`将被用作起点，添加更多噪音会使`strength`越大。降噪步骤的数量取决于最初添加的噪音量。

+   `emb` (`torch.FloatTensor`) — 图像嵌入。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用于指导图像生成的提示或提示。如果不使用指导（即，如果`guidance_scale`小于`1`，则忽略）。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `num_inference_steps` (`int`, *可选*, 默认为 100) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个[torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，以使生成过程确定性。

+   `guidance_scale` (`float`, *可选*，默认为 4.0) — 在[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程 2 的`w`。通过设置`guidance_scale > 1`来启用引导比例。更高的引导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `output_type` (`str`，*可选*，默认为`"pt"`) — 生成图像的输出格式。选择在`"np"`（`np.array`）和`"pt"`（`torch.Tensor`）之间。

+   `return_dict` (`bool`，*可选*，默认为`True`) — 是否返回 ImagePipelineOutput 而不是普通元组。

返回

`KandinskyPriorPipelineOutput`或元组

调用管道生成时调用的函数。

示例：

```py
>>> from diffusers import KandinskyV22Pipeline, KandinskyV22PriorEmb2EmbPipeline
>>> import torch

>>> pipe_prior = KandinskyPriorPipeline.from_pretrained(
...     "kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16
... )
>>> pipe_prior.to("cuda")

>>> prompt = "red cat, 4k photo"
>>> img = load_image(
...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
...     "/kandinsky/cat.png"
... )
>>> image_emb, nagative_image_emb = pipe_prior(prompt, image=img, strength=0.2).to_tuple()

>>> pipe = KandinskyPipeline.from_pretrained(
...     "kandinsky-community/kandinsky-2-2-decoder, torch_dtype=torch.float16"
... )
>>> pipe.to("cuda")

>>> image = pipe(
...     image_embeds=image_emb,
...     negative_image_embeds=negative_image_emb,
...     height=768,
...     width=768,
...     num_inference_steps=100,
... ).images

>>> image[0].save("cat.png")
```

#### `插值`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_prior_emb2emb.py#L155)

```py
( images_and_prompts: List weights: List num_images_per_prompt: int = 1 num_inference_steps: int = 25 generator: Union = None latents: Optional = None negative_prior_prompt: Optional = None negative_prompt: str = '' guidance_scale: float = 4.0 device = None ) → export const metadata = 'undefined';KandinskyPriorPipelineOutput or tuple
```

参数

+   `images_and_prompts` (`List[Union[str, PIL.Image.Image, torch.FloatTensor]]`) — 用于指导图像生成的提示和图像列表。权重 — (`List[float]`): `images_and_prompts`中每个条件的权重列表

+   `num_images_per_prompt` (`int`，*可选*，默认为 1) — 每个提示生成的图像数量。

+   `num_inference_steps` (`int`，*可选*，默认为 100) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `generator` (`torch.Generator`或`List[torch.Generator]`，*可选*) — 一个或多个[torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`，*可选*) — 预先生成的嘈杂潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，则将通过使用提供的随机`generator`进行采样生成潜变量张量。

+   `negative_prior_prompt` (`str`，*可选*) — 不指导先前扩散过程的提示。如果不使用引导（即，如果`guidance_scale`小于`1`，则忽略）。

+   `negative_prompt` (`str`或`List[str]`，*可选*) — 不指导图像生成的提示。如果不使用引导（即，如果`guidance_scale`小于`1`，则忽略）。

+   `guidance_scale` (`float`，*可选*，默认为 4.0) — 在[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程 2 的`w`。通过设置`guidance_scale > 1`来启用引导比例。更高的引导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

返回

`KandinskyPriorPipelineOutput`或元组

在使用先前管道进行插值时调用的函数。

示例：

```py
>>> from diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22Pipeline
>>> from diffusers.utils import load_image
>>> import PIL

>>> import torch
>>> from torchvision import transforms

>>> pipe_prior = KandinskyV22PriorPipeline.from_pretrained(
...     "kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16
... )
>>> pipe_prior.to("cuda")

>>> img1 = load_image(
...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
...     "/kandinsky/cat.png"
... )

>>> img2 = load_image(
...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
...     "/kandinsky/starry_night.jpeg"
... )

>>> images_texts = ["a cat", img1, img2]
>>> weights = [0.3, 0.3, 0.4]
>>> image_emb, zero_image_emb = pipe_prior.interpolate(images_texts, weights)

>>> pipe = KandinskyV22Pipeline.from_pretrained(
...     "kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16
... )
>>> pipe.to("cuda")

>>> image = pipe(
...     image_embeds=image_emb,
...     negative_image_embeds=zero_image_emb,
...     height=768,
...     width=768,
...     num_inference_steps=150,
... ).images[0]

>>> image.save("starry_cat.png")
```

## KandinskyV22Img2ImgPipeline

### `class diffusers.KandinskyV22Img2ImgPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_img2img.py#L92)

```py
( unet: UNet2DConditionModel scheduler: DDPMScheduler movq: VQModel )
```

参数

+   `scheduler` (DDIMScheduler) — 与`unet`结合使用以生成图像潜变量的调度器。

+   `unet` (UNet2DConditionModel) — 用于去噪图像嵌入的条件 U-Net 架构。

+   `movq` (VQModel) — MoVQ 解码器，用于从潜变量生成图像。

使用 Kandinsky 进行图像到图像生成的管道

此模型继承自 DiffusionPipeline。查看超类文档以了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

#### `__call__`

[< 源代码 >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_img2img.py#L190)

```py
( image_embeds: Union image: Union negative_image_embeds: Union height: int = 512 width: int = 512 num_inference_steps: int = 100 guidance_scale: float = 4.0 strength: float = 0.3 num_images_per_prompt: int = 1 generator: Union = None output_type: Optional = 'pil' return_dict: bool = True callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `image_embeds` (`torch.FloatTensor` 或 `List[torch.FloatTensor]`) — 用于文本提示的剪辑图像嵌入，将用于条件图像生成。

+   `image` (`torch.FloatTensor`、`PIL.Image.Image`、`np.ndarray`、`List[torch.FloatTensor]`、`List[PIL.Image.Image]` 或 `List[np.ndarray]`) — 作为过程起点使用的 `Image` 或表示图像批次的张量。如果直接传递潜在变量，则不会再次编码。

+   `strength` (`float`, *可选*, 默认为 0.8) — 在概念上，指示要转换参考 `image` 的程度。必须介于 0 和 1 之间。`image` 将被用作起点，添加的噪声越大，`strength` 越大。降噪步骤的数量取决于最初添加的噪声量。当 `strength` 为 1 时，添加的噪声将达到最大值，并且降噪过程将运行指定的 `num_inference_steps` 的全部迭代次数。因此，值为 1 的情况下，基本上忽略了 `image`。

+   `negative_image_embeds` (`torch.FloatTensor` 或 `List[torch.FloatTensor]`) — 用于负文本提示的剪辑图像嵌入，将用于条件图像生成。

+   `height` (`int`, *可选*, 默认为 512) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为 512) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *可选*, 默认为 100) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *可选*, 默认为 4.0) — 在 [无分类器扩散引导](https://arxiv.org/abs/2207.12598) 中定义的引导比例。`guidance_scale` 定义为 [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf) 中方程式 2 的 `w`。通过设置 `guidance_scale > 1` 启用引导比例。更高的引导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个 [torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html) 以使生成结果确定性。

+   `output_type` (`str`, *可选*, 默认为 `"pil"`) — 生成图像的输出格式。可选择 `"pil"` (`PIL.Image.Image`)、`"np"` (`np.array`) 或 `"pt"` (`torch.Tensor`)。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回一个 ImagePipelineOutput 而不是一个普通元组。

+   `callback_on_step_end` (`Callable`, *可选*) — 在推理期间每个降噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs` 将包含由 `callback_on_step_end_tensor_inputs` 指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *可选*) — `callback_on_step_end` 函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包含在您的管道类的 `._callback_tensor_inputs` 属性中列出的变量。

返回值

ImagePipelineOutput 或 `tuple`

调用管道以进行生成时调用的函数。

示例：

## KandinskyV22Img2ImgCombinedPipeline

### `class diffusers.KandinskyV22Img2ImgCombinedPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_combined.py#L334)

```py
( unet: UNet2DConditionModel scheduler: DDPMScheduler movq: VQModel prior_prior: PriorTransformer prior_image_encoder: CLIPVisionModelWithProjection prior_text_encoder: CLIPTextModelWithProjection prior_tokenizer: CLIPTokenizer prior_scheduler: UnCLIPScheduler prior_image_processor: CLIPImageProcessor )
```

参数

+   `scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) — 用于与`unet`结合使用以生成图像潜变量的调度器。

+   `unet` (UNet2DConditionModel) — 用于去噪图像嵌入的条件 U-Net 架构。

+   `movq` (VQModel) — 用于从潜变量生成图像的 MoVQ 解码器。

+   `prior_prior` (PriorTransformer) — 用于从文本嵌入近似图像嵌入的规范 unCLIP 先验。

+   `prior_image_encoder` (`CLIPVisionModelWithProjection`) — 冻结的图像编码器。

+   `prior_text_encoder` (`CLIPTextModelWithProjection`) — 冻结的文本编码器。

+   `prior_tokenizer` (`CLIPTokenizer`) — 类[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)的分词器。

+   `prior_scheduler` (`UnCLIPScheduler`) — 用于与`prior`结合使用以生成图像嵌入的调度器。

+   `prior_image_processor` (`CLIPImageProcessor`) — 用于从 clip 预处理图像的图像处理器。

使用 Kandinsky 进行图像生成的组合管道

该模型继承自 DiffusionPipeline。查看超类文档以了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_combined.py#L438)

```py
( prompt: Union image: Union negative_prompt: Union = None num_inference_steps: int = 100 guidance_scale: float = 4.0 strength: float = 0.3 num_images_per_prompt: int = 1 height: int = 512 width: int = 512 prior_guidance_scale: float = 4.0 prior_num_inference_steps: int = 25 generator: Union = None latents: Optional = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True prior_callback_on_step_end: Optional = None prior_callback_on_step_end_tensor_inputs: List = ['latents'] callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt` (`str` or `List[str]`) — 用于指导图像生成的提示。

+   `image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, or `List[np.ndarray]`) — 作为过程起点使用的`Image`或表示图像批次的张量。如果直接传递潜变量，则不会再次编码。

+   `negative_prompt` (`str` or `List[str]`, *可选*) — 不用来指导图像生成的提示。如果不使用指导（即如果`guidance_scale`小于`1`），则忽略。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `guidance_scale` (`float`, *可选*, 默认为 4.0) — 如[Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)中定义的指导比例。`guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程 2 的`w`。通过设置`guidance_scale > 1`启用指导比例。更高的指导比例鼓励生成与文本`prompt`紧密相关的图像，通常以降低图像质量为代价。

+   `strength` (`float`, *可选*, 默认为 0.3) — 在概念上，指示要转换参考`image`的程度。必须介于 0 和 1 之间。`image`将被用作起点，添加的噪声越大，`strength`越大。去噪步骤的数量取决于最初添加的噪声量。当`strength`为 1 时，添加的噪声将是最大的，去噪过程将运行指定的`num_inference_steps`的完整迭代次数。因此，值为 1 基本上忽略了`image`。

+   `num_inference_steps` (`int`, *optional*, 默认为 100) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `height` (`int`, *optional*, 默认为 512) — 生成图像的像素高度。

+   `width` (`int`, *optional*, 默认为 512) — 生成图像的像素宽度。

+   `prior_guidance_scale` (`float`, *optional*, 默认为 4.0) — 如[Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)中定义的指导比例。`guidance_scale` 定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程 2 中的`w`。通过设置 `guidance_scale > 1` 来启用指导比例。更高的指导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `prior_num_inference_steps` (`int`, *optional*, 默认为 100) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 一个或多个[torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)列表，使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *optional*) — 预先生成的嘈杂潜在变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，则将使用提供的随机`generator`进行采样生成潜在变量张量。

+   `output_type` (`str`, *optional*, 默认为`"pil"`) — 生成图像的输出格式。可选择：`"pil"` (`PIL.Image.Image`)，`"np"` (`np.array`) 或 `"pt"` (`torch.Tensor`)。

+   `callback` (`Callable`, *optional*) — 在推理过程中每`callback_steps`步调用的函数。该函数调用以下参数：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *optional*, 默认为 1) — 调用`callback`函数的频率。如果未指定，则在每一步调用回调函数。

+   `return_dict` (`bool`, *optional*, 默认为`True`) — 是否返回 ImagePipelineOutput 而不是普通元组。

返回

ImagePipelineOutput 或 `tuple`

调用生成管道时调用的函数。

示例：

```py
from diffusers import AutoPipelineForImage2Image
import torch
import requests
from io import BytesIO
from PIL import Image
import os

pipe = AutoPipelineForImage2Image.from_pretrained(
    "kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16
)
pipe.enable_model_cpu_offload()

prompt = "A fantasy landscape, Cinematic lighting"
negative_prompt = "low quality, bad quality"

url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"

response = requests.get(url)
image = Image.open(BytesIO(response.content)).convert("RGB")
image.thumbnail((768, 768))

image = pipe(prompt=prompt, image=original_image, num_inference_steps=25).images[0]
```

#### `enable_model_cpu_offload`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_combined.py#L408)

```py
( gpu_id = 0 )
```

使用加速将所有模型转移到 CPU，减少内存使用量，对性能影响较小。与`enable_sequential_cpu_offload`相比，此方法在调用其`forward`方法时将整个模型一次性移至 GPU，并且模型保持在 GPU 中，直到下一个模型运行。与`enable_sequential_cpu_offload`相比，内存节省较少，但由于`unet`的迭代执行，性能要好得多。

#### `enable_sequential_cpu_offload`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_combined.py#L418)

```py
( gpu_id = 0 )
```

使用加速将所有模型转移到 CPU，显著减少内存使用量。调用时，`unet`、`text_encoder`、`vae`和`safety checker`的状态字典保存到 CPU，然后移至`torch.device('meta')`，仅在其特定子模块调用`forward`方法时加载到 GPU。请注意，卸载是基于子模块的。与`enable_model_cpu_offload`相比，内存节省更高，但性能较低。

## KandinskyV22ControlnetImg2ImgPipeline

### `class diffusers.KandinskyV22ControlnetImg2ImgPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_controlnet_img2img.py#L120)

```py
( unet: UNet2DConditionModel scheduler: DDPMScheduler movq: VQModel )
```

参数

+   `scheduler` (DDIMScheduler) — 用于与`unet`结合使用以生成图像潜变量的调度器。

+   `unet` (UNet2DConditionModel) — 用于去噪图像嵌入的条件 U-Net 架构。

+   `movq` (VQModel) — 用于从潜变量生成图像的 MoVQ 解码器。

使用 Kandinsky 进行图像生成的流程

该模型继承自 DiffusionPipeline。查看超类文档以了解库为所有流程实现的通用方法（如下载或保存、在特定设备上运行等）。

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_controlnet_img2img.py#L206)

```py
( image_embeds: Union image: Union negative_image_embeds: Union hint: FloatTensor height: int = 512 width: int = 512 num_inference_steps: int = 100 guidance_scale: float = 4.0 strength: float = 0.3 num_images_per_prompt: int = 1 generator: Union = None output_type: Optional = 'pil' callback: Optional = None callback_steps: int = 1 return_dict: bool = True ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `image_embeds` (`torch.FloatTensor` 或 `List[torch.FloatTensor]`) — 用于文本提示的剪辑图像嵌入，将用于条件图像生成。

+   `image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, 或 `List[np.ndarray]`) — 作为过程起点使用的`Image`或表示图像批次的张量。如果直接传递潜变量作为`image`，则不会再次编码。

+   `strength` (`float`, *可选*, 默认为 0.8) — 在概念上，指示要转换参考`image`的程度。必须在 0 和 1 之间。`image`将被用作起点，添加的噪音越大，`strength`越大。去噪步骤的数量取决于最初添加的噪音量。当`strength`为 1 时，添加的噪音将达到最大值，并且去噪过程将运行指定的`num_inference_steps`的全部迭代次数。因此，值为 1 基本上忽略了`image`。

+   `hint` (`torch.FloatTensor`) — 控制网络条件。

+   `negative_image_embeds` (`torch.FloatTensor` 或 `List[torch.FloatTensor]`) — 用于负文本提示的剪辑图像嵌入，将用于条件图像生成。

+   `height` (`int`, *可选*, 默认为 512) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为 512) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *可选*, 默认为 100) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *可选*, 默认为 4.0) — 在[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程式 2 的`w`。通过设置`guidance_scale > 1`启用引导比例。更高的引导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个[torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。可选择：`"pil"` (`PIL.Image.Image`)，`"np"` (`np.array`) 或 `"pt"` (`torch.Tensor`)。

+   `callback` (`Callable`, *optional*) — 在推理过程中每隔 `callback_steps` 步调用一次的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *optional*, 默认为 1) — 调用 `callback` 函数的频率。如果未指定，则在每一步都会调用回调函数。

+   `return_dict` (`bool`, *optional*, 默认为 `True`) — 是否返回一个 ImagePipelineOutput 而不是一个普通的元组。

返回

ImagePipelineOutput 或 `tuple`

调用管道生成时调用的函数。

示例：

## KandinskyV22InpaintPipeline

### `class diffusers.KandinskyV22InpaintPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_inpainting.py#L235)

```py
( unet: UNet2DConditionModel scheduler: DDPMScheduler movq: VQModel )
```

参数

+   `scheduler` (DDIMScheduler) — 用于与 `unet` 结合使用以生成图像潜在空间的调度器。

+   `unet` (UNet2DConditionModel) — 用于降噪图像嵌入的条件 U-Net 架构。

+   `movq` (VQModel) — MoVQ 解码器，用于从潜在空间生成图像。

使用 Kandinsky2.1 进行文本引导图像修复的管道

该模型继承自 DiffusionPipeline。查看超类文档以了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_inpainting.py#L294)

```py
( image_embeds: Union image: Union mask_image: Union negative_image_embeds: Union height: int = 512 width: int = 512 num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 generator: Union = None latents: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `image_embeds` (`torch.FloatTensor` 或 `List[torch.FloatTensor]`) — 用于文本提示的剪辑图像嵌入，将用于条件图像生成。

+   `image` (`PIL.Image.Image`) — `Image`，或表示图像批次的张量，将被修复，即图像的部分将被用 `mask_image` 掩盖并根据 `prompt` 重新绘制。

+   `mask_image` (`np.array`) — 表示图像批次的张量，用于遮罩 `image`。遮罩中的白色像素将被重新绘制，而黑色像素将被保留。如果 `mask_image` 是一个 PIL 图像，它将在使用之前转换为单通道（亮度）。如果它是一个张量，则应该包含一个颜色通道（L）而不是 3 个，因此预期的形状将是 `(B, H, W, 1)`。

+   `negative_image_embeds` (`torch.FloatTensor` 或 `List[torch.FloatTensor]`) — 用于负文本提示的剪辑图像嵌入，将用于条件图像生成。

+   `height` (`int`, *optional*, 默认为 512) — 生成图像的像素高度。

+   `width` (`int`, *optional*, 默认为 512) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *optional*, 默认为 100) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *optional*, 默认为 4.0) — 在 [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598) 中定义的指导尺度。`guidance_scale` 被定义为 [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf) 中方程式 2 的 `w`。通过设置 `guidance_scale > 1` 来启用指导尺度。更高的指导尺度鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。

+   `num_images_per_prompt` (`int`, *optional*, 默认为 1) — 每个提示生成的图像数量。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 一个或多个[torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *optional*) — 预生成的嘈杂潜在变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，将使用提供的随机`generator`进行采样生成潜在变量张量。

+   `output_type` (`str`, *optional*, 默认为`"pil"`) — 生成图像的输出格式。可选择`"pil"` (`PIL.Image.Image`)、`"np"` (`np.array`) 或`"pt"` (`torch.Tensor`)。

+   `return_dict` (`bool`, *optional*, 默认为`True`) — 是否返回 ImagePipelineOutput 而不是普通元组。

+   `callback_on_step_end` (`Callable`, *optional*) — 在推断过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) — 用于`callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在管道类的`._callback_tensor_inputs`属性中列出的变量。

返回

ImagePipelineOutput 或 `tuple`

调用管道进行生成时调用的函数。

示例：

## KandinskyV22InpaintCombinedPipeline

### `class diffusers.KandinskyV22InpaintCombinedPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_combined.py#L582)

```py
( unet: UNet2DConditionModel scheduler: DDPMScheduler movq: VQModel prior_prior: PriorTransformer prior_image_encoder: CLIPVisionModelWithProjection prior_text_encoder: CLIPTextModelWithProjection prior_tokenizer: CLIPTokenizer prior_scheduler: UnCLIPScheduler prior_image_processor: CLIPImageProcessor )
```

参数

+   `scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) — 用于与`unet`结合使用的调度器以生成图像潜在变量。

+   `unet` (UNet2DConditionModel) — 用于去噪图像嵌入的条件 U-Net 架构。

+   `movq` (VQModel) — 用于从潜在变量生成图像的 MoVQ 解码器。

+   `prior_prior` (PriorTransformer) — 用于从文本嵌入近似图像嵌入的标准 unCLIP 先验。

+   `prior_image_encoder` (`CLIPVisionModelWithProjection`) — 冻结的图像编码器。

+   `prior_text_encoder` (`CLIPTextModelWithProjection`) — 冻结的文本编码器。

+   `prior_tokenizer` (`CLIPTokenizer`) — 类[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)的分词器。

+   `prior_scheduler` (`UnCLIPScheduler`) — 用于与`prior`结合使用的调度器以生成图像嵌入。

+   `prior_image_processor` (`CLIPImageProcessor`) — 用于预处理来自 clip 的图像的图像处理器。

使用 Kandinsky 进行修复生成的组合管道

此模型继承自 DiffusionPipeline。查看超类文档以了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_combined.py#L676)

```py
( prompt: Union image: Union mask_image: Union negative_prompt: Union = None num_inference_steps: int = 100 guidance_scale: float = 4.0 num_images_per_prompt: int = 1 height: int = 512 width: int = 512 prior_guidance_scale: float = 4.0 prior_num_inference_steps: int = 25 generator: Union = None latents: Optional = None output_type: Optional = 'pil' return_dict: bool = True prior_callback_on_step_end: Optional = None prior_callback_on_step_end_tensor_inputs: List = ['latents'] callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`) — 用于指导图像生成的提示或提示。

+   `image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, 或 `List[np.ndarray]`) — 代表图像批次的`Image`或张量，将被用作过程的起点。如果直接传递潜在图像，则也可以接受图像潜在作为`image`，那么它将不会再次被编码。

+   `mask_image` (`np.array`) — 代表图像批次的张量，用于遮罩`image`。遮罩中的白色像素将被重新绘制，而黑色像素将被保留。如果`mask_image`是 PIL 图像，则在使用之前将其转换为单通道（亮度）。如果它是张量，则应该包含一个颜色通道（L）而不是 3，因此预期形状将是`(B, H, W, 1)`。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用来引导图像生成的提示或提示。当不使用引导时被忽略（即如果`guidance_scale`小于`1`，则被忽略）。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `guidance_scale` (`float`, *可选*, 默认为 4.0) — 如[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程 2 的`w`。通过设置`guidance_scale > 1`启用引导比例。更高的引导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `num_inference_steps` (`int`, *可选*, 默认为 100) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但推理速度较慢。

+   `height` (`int`, *可选*, 默认为 512) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为 512) — 生成图像的像素宽度。

+   `prior_guidance_scale` (`float`, *可选*, 默认为 4.0) — 如[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程 2 的`w`。通过设置`guidance_scale > 1`启用引导比例。更高的引导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `prior_num_inference_steps` (`int`, *可选*, 默认为 100) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但推理速度较慢。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个[torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *可选*) — 预先生成的噪声潜在图像，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，则将使用提供的随机`generator`进行采样生成潜在张量。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。可选择：`"pil"`（`PIL.Image.Image`）、`"np"`（`np.array`）或`"pt"`（`torch.Tensor`）。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回 ImagePipelineOutput 而不是普通元组。

+   `prior_callback_on_step_end` (`Callable`, *可选*) — 在推理过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`prior_callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。

+   `prior_callback_on_step_end_tensor_inputs` (`List`, *optional*) — `prior_callback_on_step_end` 函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包含在您的管道类的 `._callback_tensor_inputs` 属性中列出的变量。

+   `callback_on_step_end` (`Callable`, *optional*) — 在推理过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs` 将包含由 `callback_on_step_end_tensor_inputs` 指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) — `callback_on_step_end` 函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包含在您的管道类的 `._callback_tensor_inputs` 属性中列出的变量。

返回

ImagePipelineOutput 或 `tuple`

在调用生成管道时调用的函数。

示例：

```py
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image
import torch
import numpy as np

pipe = AutoPipelineForInpainting.from_pretrained(
    "kandinsky-community/kandinsky-2-2-decoder-inpaint", torch_dtype=torch.float16
)
pipe.enable_model_cpu_offload()

prompt = "A fantasy landscape, Cinematic lighting"
negative_prompt = "low quality, bad quality"

original_image = load_image(
    "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main" "/kandinsky/cat.png"
)

mask = np.zeros((768, 768), dtype=np.float32)
# Let's mask out an area above the cat's head
mask[:250, 250:-250] = 1

image = pipe(prompt=prompt, image=original_image, mask_image=mask, num_inference_steps=25).images[0]
```

#### `enable_sequential_cpu_offload`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_combined.py#L656)

```py
( gpu_id = 0 )
```

使用加速器将所有模型转移到 CPU，显着减少内存使用。调用时，unet、text_encoder、vae 和 safety checker 的状态字典将保存到 CPU，然后移动到 `torch.device('meta')`，仅在它们特定的子模块调用`forward`方法时才加载到 GPU。请注意，卸载是基于子模块的。与`enable_model_cpu_offload` 相比，内存节省更高，但性能较低。
