- en: Audio Spectrogram Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/59.098541c6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/PipelineTag.44585822.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Audio Spectrogram Transformer model was proposed in [AST: Audio Spectrogram
    Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James
    Glass. The Audio Spectrogram Transformer applies a [Vision Transformer](vit) to
    audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art
    results for audio classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*In the past decade, convolutional neural networks (CNNs) have been widely
    adopted as the main building block for end-to-end audio classification models,
    which aim to learn a direct mapping from audio spectrograms to corresponding labels.
    To better capture long-range global context, a recent trend is to add a self-attention
    mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it
    is unclear whether the reliance on a CNN is necessary, and if neural networks
    purely based on attention are sufficient to obtain good performance in audio classification.
    In this paper, we answer the question by introducing the Audio Spectrogram Transformer
    (AST), the first convolution-free, purely attention-based model for audio classification.
    We evaluate AST on various audio classification benchmarks, where it achieves
    new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50,
    and 98.1% accuracy on Speech Commands V2.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![drawing](../Images/9126a01e9659aadbacaabc0894d706f6.png) Audio Spectrogram
    Transformer architecture. Taken from the [original paper](https://arxiv.org/abs/2104.01778).'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code can be found [here](https://github.com/YuanGongND/ast).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When fine-tuning the Audio Spectrogram Transformer (AST) on your own dataset,
    it‚Äôs recommended to take care of the input normalization (to make sure the input
    has mean of 0 and std of 0.5). [ASTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor)
    takes care of this. Note that it uses the AudioSet mean and std by default. You
    can check [`ast/src/get_norm_stats.py`](https://github.com/YuanGongND/ast/blob/master/src/get_norm_stats.py)
    to see how the authors compute the stats for a downstream dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the AST needs a low learning rate (the authors use a 10 times smaller
    learning rate compared to their CNN model proposed in the [PSLA paper](https://arxiv.org/abs/2102.01243))
    and converges quickly, so please search for a suitable learning rate and learning
    rate scheduler for your task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A list of official Hugging Face and community (indicated by üåé) resources to
    help you get started with the Audio Spectrogram Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Audio Classification
  prefs: []
  type: TYPE_NORMAL
- en: A notebook illustrating inference with AST for audio classification can be found
    [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/AST).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ASTForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also: [Audio classification](../tasks/audio_classification).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you‚Äôre interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we‚Äôll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  prefs: []
  type: TYPE_NORMAL
- en: ASTConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ASTConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/configuration_audio_spectrogram_transformer.py#L31)'
  prefs: []
  type: TYPE_NORMAL
- en: ( hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size
    = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob
    = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-12 patch_size = 16 qkv_bias
    = True frequency_stride = 10 time_stride = 10 max_length = 1024 num_mel_bins =
    128 **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**hidden_size** (`int`, *optional*, defaults to 768) ‚Äî Dimensionality of the
    encoder layers and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_hidden_layers** (`int`, *optional*, defaults to 12) ‚Äî Number of hidden
    layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_attention_heads** (`int`, *optional*, defaults to 12) ‚Äî Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**intermediate_size** (`int`, *optional*, defaults to 3072) ‚Äî Dimensionality
    of the ‚Äúintermediate‚Äù (i.e., feed-forward) layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_act** (`str` or `function`, *optional*, defaults to `"gelu"`) ‚Äî The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_dropout_prob** (`float`, *optional*, defaults to 0.0) ‚Äî The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_probs_dropout_prob** (`float`, *optional*, defaults to 0.0) ‚Äî The
    dropout ratio for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initializer_range** (`float`, *optional*, defaults to 0.02) ‚Äî The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**layer_norm_eps** (`float`, *optional*, defaults to 1e-12) ‚Äî The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**patch_size** (`int`, *optional*, defaults to 16) ‚Äî The size (resolution)
    of each patch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**qkv_bias** (`bool`, *optional*, defaults to `True`) ‚Äî Whether to add a bias
    to the queries, keys and values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**frequency_stride** (`int`, *optional*, defaults to 10) ‚Äî Frequency stride
    to use when patchifying the spectrograms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**time_stride** (`int`, *optional*, defaults to 10) ‚Äî Temporal stride to use
    when patchifying the spectrograms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_length** (`int`, *optional*, defaults to 1024) ‚Äî Temporal dimension of
    the spectrograms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_mel_bins** (`int`, *optional*, defaults to 128) ‚Äî Frequency dimension
    of the spectrograms (number of Mel-frequency bins).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [ASTModel](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel).
    It is used to instantiate an AST model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the AST [MIT/ast-finetuned-audioset-10-10-0.4593](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ASTFeatureExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ASTFeatureExtractor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py#L39)'
  prefs: []
  type: TYPE_NORMAL
- en: ( feature_size = 1 sampling_rate = 16000 num_mel_bins = 128 max_length = 1024
    padding_value = 0.0 do_normalize = True mean = -4.2677393 std = 4.5689974 return_attention_mask
    = False **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**feature_size** (`int`, *optional*, defaults to 1) ‚Äî The feature dimension
    of the extracted features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sampling_rate** (`int`, *optional*, defaults to 16000) ‚Äî The sampling rate
    at which the audio files should be digitalized expressed in hertz (Hz).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_mel_bins** (`int`, *optional*, defaults to 128) ‚Äî Number of Mel-frequency
    bins.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_length** (`int`, *optional*, defaults to 1024) ‚Äî Maximum length to which
    to pad/truncate the extracted features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_normalize** (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not
    to normalize the log-Mel features using `mean` and `std`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mean** (`float`, *optional*, defaults to -4.2677393) ‚Äî The mean value used
    to normalize the log-Mel features. Uses the AudioSet mean by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**std** (`float`, *optional*, defaults to 4.5689974) ‚Äî The standard deviation
    value used to normalize the log-Mel features. Uses the AudioSet standard deviation
    by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_attention_mask** (`bool`, *optional*, defaults to `False`) ‚Äî Whether
    or not [**call**()](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__)
    should return `attention_mask`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a Audio Spectrogram Transformer (AST) feature extractor.
  prefs: []
  type: TYPE_NORMAL
- en: This feature extractor inherits from [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: This class extracts mel-filter bank features from raw speech using TorchAudio
    if installed or using numpy otherwise, pads/truncates them to a fixed length and
    normalizes them using a mean and standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py#L161)'
  prefs: []
  type: TYPE_NORMAL
- en: '( raw_speech: Union sampling_rate: Optional = None return_tensors: Union =
    None **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**raw_speech** (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`)
    ‚Äî The sequence or batch of sequences to be padded. Each sequence can be a numpy
    array, a list of float values, a list of numpy arrays or a list of list of float
    values. Must be mono channel audio, not stereo, i.e. single float per timestep.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sampling_rate** (`int`, *optional*) ‚Äî The sampling rate at which the `raw_speech`
    input was sampled. It is strongly recommended to pass `sampling_rate` at the forward
    call to prevent silent errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_tensors** (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) ‚Äî If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to featurize and prepare for the model one or several sequence(s).
  prefs: []
  type: TYPE_NORMAL
- en: ASTModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ASTModel'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L430)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: ASTConfig )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig))
    ‚Äî Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare AST Model transformer outputting raw hidden-states without any specific
    head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L458)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_values: Optional = None head_mask: Optional = None output_attentions:
    Optional = None output_hidden_states: Optional = None return_dict: Optional =
    None ) ‚Üí [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_values** (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`)
    ‚Äî Float values mel features extracted from the raw audio waveform. Raw audio waveform
    can be obtained by loading a `.flac` or `.wav` audio file into an array of type
    `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install
    soundfile`). To prepare the array into `input_features`, the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the mel features, padding and conversion into a
    tensor of type `torch.FloatTensor`. See [**call**()](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) ‚Äî Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) ‚Äî Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) ‚Äî Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) ‚Äî Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) ‚Äî Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pooler_output** (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    ‚Äî Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ASTModel](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ASTForAudioClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ASTForAudioClassification'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L527)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: ASTConfig )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig))
    ‚Äî Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio Spectrogram Transformer model with an audio classification head on top
    (a linear layer on top of the pooled output) e.g. for datasets like AudioSet,
    Speech Commands v2.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L547)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_values: Optional = None head_mask: Optional = None labels: Optional
    = None output_attentions: Optional = None output_hidden_states: Optional = None
    return_dict: Optional = None ) ‚Üí [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_values** (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`)
    ‚Äî Float values mel features extracted from the raw audio waveform. Raw audio waveform
    can be obtained by loading a `.flac` or `.wav` audio file into an array of type
    `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install
    soundfile`). To prepare the array into `input_features`, the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the mel features, padding and conversion into a
    tensor of type `torch.FloatTensor`. See [**call**()](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) ‚Äî Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) ‚Äî Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) ‚Äî Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) ‚Äî Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size,)`, *optional*) ‚Äî Labels
    for computing the audio classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) ‚Äî Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`)
    ‚Äî Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ASTForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
