- en: Audio Spectrogram Transformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: éŸ³é¢‘é¢‘è°±å˜æ¢å™¨
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: 'The Audio Spectrogram Transformer model was proposed in [AST: Audio Spectrogram
    Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James
    Glass. The Audio Spectrogram Transformer applies a [Vision Transformer](vit) to
    audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art
    results for audio classification.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'éŸ³é¢‘é¢‘è°±å˜æ¢å™¨æ¨¡å‹æ˜¯ç”± Yuan Gongã€Yu-An Chungã€James Glass åœ¨ [AST: éŸ³é¢‘é¢‘è°±å˜æ¢å™¨](https://arxiv.org/abs/2104.01778)
    ä¸­æå‡ºçš„ã€‚éŸ³é¢‘é¢‘è°±å˜æ¢å™¨å°†è§†è§‰å˜æ¢å™¨åº”ç”¨äºéŸ³é¢‘ï¼Œé€šè¿‡å°†éŸ³é¢‘è½¬æ¢ä¸ºå›¾åƒï¼ˆé¢‘è°±å›¾ï¼‰ã€‚è¯¥æ¨¡å‹åœ¨éŸ³é¢‘åˆ†ç±»æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*In the past decade, convolutional neural networks (CNNs) have been widely
    adopted as the main building block for end-to-end audio classification models,
    which aim to learn a direct mapping from audio spectrograms to corresponding labels.
    To better capture long-range global context, a recent trend is to add a self-attention
    mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it
    is unclear whether the reliance on a CNN is necessary, and if neural networks
    purely based on attention are sufficient to obtain good performance in audio classification.
    In this paper, we answer the question by introducing the Audio Spectrogram Transformer
    (AST), the first convolution-free, purely attention-based model for audio classification.
    We evaluate AST on various audio classification benchmarks, where it achieves
    new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50,
    and 98.1% accuracy on Speech Commands V2.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨è¿‡å»çš„åå¹´ä¸­ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å·²è¢«å¹¿æ³›é‡‡ç”¨ä½œä¸ºç«¯åˆ°ç«¯éŸ³é¢‘åˆ†ç±»æ¨¡å‹çš„ä¸»è¦æ„å»ºæ¨¡å—ï¼Œæ—¨åœ¨å­¦ä¹ ä»éŸ³é¢‘é¢‘è°±åˆ°ç›¸åº”æ ‡ç­¾çš„ç›´æ¥æ˜ å°„ã€‚ä¸ºäº†æ›´å¥½åœ°æ•è·é•¿è·ç¦»å…¨å±€ä¸Šä¸‹æ–‡ï¼Œæœ€è¿‘çš„è¶‹åŠ¿æ˜¯åœ¨
    CNN ä¹‹ä¸Šæ·»åŠ è‡ªæ³¨æ„æœºåˆ¶ï¼Œå½¢æˆ CNN-æ³¨æ„æ··åˆæ¨¡å‹ã€‚ç„¶è€Œï¼Œç›®å‰å°šä¸æ¸…æ¥šæ˜¯å¦ä¾èµ–äº CNN æ˜¯å¿…è¦çš„ï¼Œä»¥åŠåŸºäºæ³¨æ„åŠ›çš„ç¥ç»ç½‘ç»œæ˜¯å¦è¶³ä»¥åœ¨éŸ³é¢‘åˆ†ç±»ä¸­è·å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥éŸ³é¢‘é¢‘è°±å˜æ¢å™¨ï¼ˆASTï¼‰æ¥å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ— å·ç§¯ã€çº¯æ³¨æ„åŠ›çš„éŸ³é¢‘åˆ†ç±»æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨å„ç§éŸ³é¢‘åˆ†ç±»åŸºå‡†ä¸Šè¯„ä¼°äº†
    ASTï¼Œåœ¨è¿™äº›åŸºå‡†ä¸Šå–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼šåœ¨ AudioSet ä¸Šçš„ 0.485 mAPï¼Œåœ¨ ESC-50 ä¸Šçš„ 95.6% å‡†ç¡®ç‡ï¼Œä»¥åŠåœ¨ Speech
    Commands V2 ä¸Šçš„ 98.1% å‡†ç¡®ç‡ã€‚*'
- en: '![drawing](../Images/9126a01e9659aadbacaabc0894d706f6.png) Audio Spectrogram
    Transformer architecture. Taken from the [original paper](https://arxiv.org/abs/2104.01778).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![drawing](../Images/9126a01e9659aadbacaabc0894d706f6.png) éŸ³é¢‘é¢‘è°±å˜æ¢å™¨æ¶æ„ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2104.01778)ã€‚'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code can be found [here](https://github.com/YuanGongND/ast).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç”± [nielsr](https://huggingface.co/nielsr) è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/YuanGongND/ast)æ‰¾åˆ°ã€‚
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: When fine-tuning the Audio Spectrogram Transformer (AST) on your own dataset,
    itâ€™s recommended to take care of the input normalization (to make sure the input
    has mean of 0 and std of 0.5). [ASTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor)
    takes care of this. Note that it uses the AudioSet mean and std by default. You
    can check [`ast/src/get_norm_stats.py`](https://github.com/YuanGongND/ast/blob/master/src/get_norm_stats.py)
    to see how the authors compute the stats for a downstream dataset.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šå¾®è°ƒéŸ³é¢‘é¢‘è°±å˜æ¢å™¨ï¼ˆASTï¼‰æ—¶ï¼Œå»ºè®®è¿›è¡Œè¾“å…¥å½’ä¸€åŒ–å¤„ç†ï¼ˆç¡®ä¿è¾“å…¥çš„å‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 0.5ï¼‰ã€‚[ASTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor)
    è´Ÿè´£æ­¤æ“ä½œã€‚è¯·æ³¨æ„ï¼Œé»˜è®¤æƒ…å†µä¸‹å®ƒä½¿ç”¨ AudioSet çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚æ‚¨å¯ä»¥æŸ¥çœ‹[`ast/src/get_norm_stats.py`](https://github.com/YuanGongND/ast/blob/master/src/get_norm_stats.py)æ¥æŸ¥çœ‹ä½œè€…å¦‚ä½•è®¡ç®—ä¸‹æ¸¸æ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯ã€‚
- en: Note that the AST needs a low learning rate (the authors use a 10 times smaller
    learning rate compared to their CNN model proposed in the [PSLA paper](https://arxiv.org/abs/2102.01243))
    and converges quickly, so please search for a suitable learning rate and learning
    rate scheduler for your task.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼ŒAST éœ€è¦ä¸€ä¸ªè¾ƒä½çš„å­¦ä¹ ç‡ï¼ˆä½œè€…ä½¿ç”¨æ¯”ä»–ä»¬åœ¨ [PSLA è®ºæ–‡](https://arxiv.org/abs/2102.01243) ä¸­æå‡ºçš„
    CNN æ¨¡å‹å° 10 å€çš„å­¦ä¹ ç‡ï¼‰ï¼Œå¹¶ä¸”æ”¶æ•›é€Ÿåº¦å¾ˆå¿«ï¼Œå› æ­¤è¯·ä¸ºæ‚¨çš„ä»»åŠ¡æœç´¢ä¸€ä¸ªåˆé€‚çš„å­¦ä¹ ç‡å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
- en: Resources
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with the Audio Spectrogram Transformer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä»½å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”± ğŸŒ è¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨éŸ³é¢‘é¢‘è°±å˜æ¢å™¨ã€‚
- en: Audio Classification
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: éŸ³é¢‘åˆ†ç±»
- en: A notebook illustrating inference with AST for audio classification can be found
    [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/AST).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/AST)æ‰¾åˆ°ç”¨äºéŸ³é¢‘åˆ†ç±»çš„
    AST æ¨ç†çš„ç¬”è®°æœ¬ã€‚
- en: '[ASTForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ASTForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification)
    å—åˆ°è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)çš„æ”¯æŒã€‚'
- en: 'See also: [Audio classification](../tasks/audio_classification).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦è¯·å‚é˜…ï¼š[éŸ³é¢‘åˆ†ç±»](../tasks/audio_classification)ã€‚
- en: If youâ€™re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æäº¤æ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: ASTConfig
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ASTConfig
- en: '### `class transformers.ASTConfig`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ASTConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/configuration_audio_spectrogram_transformer.py#L31)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/configuration_audio_spectrogram_transformer.py#L31)'
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`hidden_size` (`int`, *optional*, defaults to 768) â€” Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, é»˜è®¤ä¸º768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å™¨å±‚çš„ç»´åº¦ã€‚'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Number of hidden
    layers in the Transformer encoder.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, é»˜è®¤ä¸º12) â€” Transformerç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) â€” Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, é»˜è®¤ä¸º12) â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°é‡ã€‚'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) â€” Dimensionality
    of the â€œintermediateâ€ (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, é»˜è®¤ä¸º3072) â€” Transformerç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`æˆ–`function`, *optional*, é»˜è®¤ä¸º`"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"selu"`å’Œ`"gelu_new"`ã€‚'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, é»˜è®¤ä¸º0.0) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” The
    dropout ratio for the attention probabilities.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, é»˜è®¤ä¸º0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¼ƒæ¯”ä¾‹ã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, é»˜è®¤ä¸º0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” The epsilon used
    by the layer normalization layers.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, é»˜è®¤ä¸º1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚'
- en: '`patch_size` (`int`, *optional*, defaults to 16) â€” The size (resolution) of
    each patch.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *optional*, é»˜è®¤ä¸º16) â€” æ¯ä¸ªå—çš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚'
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) â€” Whether to add a bias
    to the queries, keys and values.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_bias` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦ä¸ºæŸ¥è¯¢ã€é”®å’Œå€¼æ·»åŠ åç½®ã€‚'
- en: '`frequency_stride` (`int`, *optional*, defaults to 10) â€” Frequency stride to
    use when patchifying the spectrograms.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frequency_stride` (`int`, *optional*, é»˜è®¤ä¸º10) â€” åœ¨åˆ¶ä½œé¢‘è°±å›¾å—æ—¶ä½¿ç”¨çš„é¢‘ç‡æ­¥å¹…ã€‚'
- en: '`time_stride` (`int`, *optional*, defaults to 10) â€” Temporal stride to use
    when patchifying the spectrograms.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_stride` (`int`, *optional*, é»˜è®¤ä¸º10) â€” åœ¨åˆ¶ä½œé¢‘è°±å›¾å—æ—¶ä½¿ç”¨çš„æ—¶é—´æ­¥å¹…ã€‚'
- en: '`max_length` (`int`, *optional*, defaults to 1024) â€” Temporal dimension of
    the spectrograms.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *optional*, é»˜è®¤ä¸º1024) â€” é¢‘è°±å›¾çš„æ—¶é—´ç»´åº¦ã€‚'
- en: '`num_mel_bins` (`int`, *optional*, defaults to 128) â€” Frequency dimension of
    the spectrograms (number of Mel-frequency bins).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_mel_bins` (`int`, *optional*, é»˜è®¤ä¸º128) â€” é¢‘è°±å›¾çš„é¢‘ç‡ç»´åº¦ï¼ˆMelé¢‘ç‡ç®±çš„æ•°é‡ï¼‰ã€‚'
- en: This is the configuration class to store the configuration of a [ASTModel](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel).
    It is used to instantiate an AST model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the AST [MIT/ast-finetuned-audioset-10-10-0.4593](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)
    architecture.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨[ASTModel](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel)é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ASTæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºAST
    [MIT/ast-finetuned-audioset-10-10-0.4593](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)
    æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ASTFeatureExtractor
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ASTFeatureExtractor
- en: '### `class transformers.ASTFeatureExtractor`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ASTFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py#L39)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py#L39)'
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`feature_size` (`int`, *optional*, defaults to 1) â€” The feature dimension of
    the extracted features.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_size` (`int`, *optional*, é»˜è®¤ä¸º1) â€” æå–ç‰¹å¾çš„ç‰¹å¾ç»´åº¦ã€‚'
- en: '`sampling_rate` (`int`, *optional*, defaults to 16000) â€” The sampling rate
    at which the audio files should be digitalized expressed in hertz (Hz).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate` (`int`, *optional*, é»˜è®¤ä¸º16000) â€” éŸ³é¢‘æ–‡ä»¶åº”æ•°å­—åŒ–çš„é‡‡æ ·ç‡ï¼Œä»¥èµ«å…¹ï¼ˆHzï¼‰è¡¨ç¤ºã€‚'
- en: '`num_mel_bins` (`int`, *optional*, defaults to 128) â€” Number of Mel-frequency
    bins.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_mel_bins` (`int`, *optional*, é»˜è®¤ä¸º128) â€” Melé¢‘ç‡ç®±çš„æ•°é‡ã€‚'
- en: '`max_length` (`int`, *optional*, defaults to 1024) â€” Maximum length to which
    to pad/truncate the extracted features.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *optional*, é»˜è®¤ä¸º1024) â€” ç”¨äºå¡«å……/æˆªæ–­æå–ç‰¹å¾çš„æœ€å¤§é•¿åº¦ã€‚'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    normalize the log-Mel features using `mean` and `std`.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦å½’ä¸€åŒ–å¯¹æ•°Melç‰¹å¾ä½¿ç”¨`mean`å’Œ`std`ã€‚'
- en: '`mean` (`float`, *optional*, defaults to -4.2677393) â€” The mean value used
    to normalize the log-Mel features. Uses the AudioSet mean by default.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean` (`float`, *optional*, é»˜è®¤ä¸º-4.2677393) â€” ç”¨äºå½’ä¸€åŒ–å¯¹æ•°Melç‰¹å¾çš„å‡å€¼ã€‚é»˜è®¤ä½¿ç”¨AudioSetçš„å‡å€¼ã€‚'
- en: '`std` (`float`, *optional*, defaults to 4.5689974) â€” The standard deviation
    value used to normalize the log-Mel features. Uses the AudioSet standard deviation
    by default.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`std`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º4.5689974ï¼‰â€” ç”¨äºå½’ä¸€åŒ–log-Melç‰¹å¾çš„æ ‡å‡†å·®å€¼ã€‚é»˜è®¤ä½¿ç”¨AudioSetçš„æ ‡å‡†å·®ã€‚'
- en: '`return_attention_mask` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not [`call`()](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__)
    should return `attention_mask`.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦[`call`()](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__)åº”è¿”å›`attention_mask`ã€‚'
- en: Constructs a Audio Spectrogram Transformer (AST) feature extractor.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªéŸ³é¢‘é¢‘è°±å˜æ¢å™¨ï¼ˆASTï¼‰ç‰¹å¾æå–å™¨ã€‚
- en: This feature extractor inherits from [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ç‰¹å¾æå–å™¨ç»§æ‰¿è‡ª[SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)ï¼Œå…¶ä¸­åŒ…å«å¤§éƒ¨åˆ†ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚
- en: This class extracts mel-filter bank features from raw speech using TorchAudio
    if installed or using numpy otherwise, pads/truncates them to a fixed length and
    normalizes them using a mean and standard deviation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç±»ä»åŸå§‹è¯­éŸ³ä¸­æå–mel-filter bankç‰¹å¾ï¼Œå¦‚æœå®‰è£…äº†TorchAudioï¼Œåˆ™ä½¿ç”¨TorchAudioï¼Œå¦åˆ™ä½¿ç”¨numpyï¼Œç„¶åå¯¹å®ƒä»¬è¿›è¡Œå¡«å……/æˆªæ–­åˆ°å›ºå®šé•¿åº¦ï¼Œå¹¶ä½¿ç”¨å‡å€¼å’Œæ ‡å‡†å·®è¿›è¡Œå½’ä¸€åŒ–ã€‚
- en: '#### `__call__`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py#L161)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py#L161)'
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`raw_speech` (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`)
    â€” The sequence or batch of sequences to be padded. Each sequence can be a numpy
    array, a list of float values, a list of numpy arrays or a list of list of float
    values. Must be mono channel audio, not stereo, i.e. single float per timestep.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`raw_speech`ï¼ˆ`np.ndarray`ï¼Œ`List[float]`ï¼Œ`List[np.ndarray]`ï¼Œ`List[List[float]]`ï¼‰â€”
    è¦å¡«å……çš„åºåˆ—æˆ–æ‰¹å¤„ç†åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯ä¸€ä¸ªnumpyæ•°ç»„ï¼Œä¸€ä¸ªæµ®ç‚¹å€¼åˆ—è¡¨ï¼Œä¸€ä¸ªnumpyæ•°ç»„åˆ—è¡¨æˆ–ä¸€ä¸ªæµ®ç‚¹å€¼åˆ—è¡¨çš„åˆ—è¡¨ã€‚å¿…é¡»æ˜¯å•å£°é“éŸ³é¢‘ï¼Œä¸æ˜¯ç«‹ä½“å£°ï¼Œå³æ¯ä¸ªæ—¶é—´æ­¥é•¿ä¸€ä¸ªæµ®ç‚¹æ•°ã€‚'
- en: '`sampling_rate` (`int`, *optional*) â€” The sampling rate at which the `raw_speech`
    input was sampled. It is strongly recommended to pass `sampling_rate` at the forward
    call to prevent silent errors.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” `raw_speech` è¾“å…¥é‡‡æ ·çš„é‡‡æ ·ç‡ã€‚å¼ºçƒˆå»ºè®®åœ¨å‰å‘è°ƒç”¨æ—¶ä¼ é€’`sampling_rate`ä»¥é˜²æ­¢é™é»˜é”™è¯¯ã€‚'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`ï¼ˆ`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)ï¼Œ*å¯é€‰*ï¼‰â€”
    å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯Pythonæ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`ï¼šè¿”å›TensorFlow `tf.constant`å¯¹è±¡ã€‚'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`ï¼šè¿”å›PyTorch `torch.Tensor`å¯¹è±¡ã€‚'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`ï¼šè¿”å›Numpy `np.ndarray`å¯¹è±¡ã€‚'
- en: Main method to featurize and prepare for the model one or several sequence(s).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºå¯¹ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—è¿›è¡Œç‰¹å¾åŒ–å’Œå‡†å¤‡æ¨¡å‹çš„ä¸»è¦æ–¹æ³•ã€‚
- en: ASTModel
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ASTModel
- en: '### `class transformers.ASTModel`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ASTModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L430)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L430)'
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare AST Model transformer outputting raw hidden-states without any specific
    head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸ASTæ¨¡å‹è½¬æ¢å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L458)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L458)'
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`)
    â€” Float values mel features extracted from the raw audio waveform. Raw audio waveform
    can be obtained by loading a `.flac` or `.wav` audio file into an array of type
    `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install
    soundfile`). To prepare the array into `input_features`, the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the mel features, padding and conversion into a
    tensor of type `torch.FloatTensor`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`)
    â€” ä»åŸå§‹éŸ³é¢‘æ³¢å½¢ä¸­æå–çš„æµ®ç‚¹å€¼ mel ç‰¹å¾ã€‚åŸå§‹éŸ³é¢‘æ³¢å½¢å¯ä»¥é€šè¿‡å°† `.flac` æˆ– `.wav` éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ° `List[float]` ç±»å‹çš„æ•°ç»„æˆ–
    `numpy.ndarray` ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡å£°éŸ³æ–‡ä»¶åº“ (`pip install soundfile`)ã€‚è¦å‡†å¤‡å¥½æ•°ç»„ä»¥è·å¾— `input_features`ï¼Œåº”ä½¿ç”¨
    [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    æå– mel ç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸º `torch.FloatTensor` ç±»å‹çš„å¼ é‡ã€‚å‚è§ [`call`()](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨ `[0, 1]`ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 è¡¨ç¤ºå¤´éƒ¨æœªè¢« `æ©ç `ã€‚
- en: 0 indicates the head is `masked`.
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 è¡¨ç¤ºå¤´éƒ¨è¢« `æ©ç `ã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„
    `attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„
    `hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig))
    and inputs.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False`
    æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    â€” Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    â€” ç»è¿‡ç”¨äºè¾…åŠ©é¢„è®­ç»ƒä»»åŠ¡çš„å±‚è¿›ä¸€æ­¥å¤„ç†åï¼Œåºåˆ—ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆåˆ†ç±»æ ‡è®°ï¼‰çš„æœ€åä¸€å±‚éšè—çŠ¶æ€ã€‚ä¾‹å¦‚ï¼Œå¯¹äº BERT ç³»åˆ—æ¨¡å‹ï¼Œè¿™è¿”å›ç»è¿‡çº¿æ€§å±‚å’ŒåŒæ›²æ­£åˆ‡æ¿€æ´»å‡½æ•°å¤„ç†åçš„åˆ†ç±»æ ‡è®°ã€‚çº¿æ€§å±‚çš„æƒé‡æ˜¯åœ¨é¢„è®­ç»ƒæœŸé—´ä»ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰ç›®æ ‡ä¸­è®­ç»ƒçš„ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è‡ªæ³¨æ„åŠ›å¤´éƒ¨ä¸­çš„æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´éƒ¨ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [ASTModel](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel)
    forward method, overrides the `__call__` special method.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[ASTModel](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel)
    çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ASTForAudioClassification
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ASTForAudioClassification
- en: '### `class transformers.ASTForAudioClassification`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ASTForAudioClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L527)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L527)'
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig)ï¼‰â€”
    æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: Audio Spectrogram Transformer model with an audio classification head on top
    (a linear layer on top of the pooled output) e.g. for datasets like AudioSet,
    Speech Commands v2.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é¡¶éƒ¨å¸¦æœ‰éŸ³é¢‘åˆ†ç±»å¤´éƒ¨çš„éŸ³é¢‘é¢‘è°±å˜æ¢å™¨æ¨¡å‹ï¼ˆåœ¨æ± åŒ–è¾“å‡ºçš„é¡¶éƒ¨æœ‰ä¸€ä¸ªçº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºAudioSetã€Speech Commands v2ç­‰æ•°æ®é›†ã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚
- en: '#### `forward`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L547)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L547)'
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`)
    â€” Float values mel features extracted from the raw audio waveform. Raw audio waveform
    can be obtained by loading a `.flac` or `.wav` audio file into an array of type
    `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install
    soundfile`). To prepare the array into `input_features`, the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the mel features, padding and conversion into a
    tensor of type `torch.FloatTensor`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, max_length, num_mel_bins)`çš„`torch.FloatTensor`ï¼‰â€”
    ä»åŸå§‹éŸ³é¢‘æ³¢å½¢ä¸­æå–çš„æµ®ç‚¹å€¼melç‰¹å¾ã€‚åŸå§‹éŸ³é¢‘æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡soundfileåº“ï¼ˆ`pip
    install soundfile`ï¼‰ã€‚è¦å‡†å¤‡æ•°ç»„ä¸º`input_features`ï¼Œåº”ä½¿ç”¨[AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)æ¥æå–melç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚æŸ¥çœ‹[`call`()](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`èŒƒå›´å†…ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æ˜¯`not masked`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨æ˜¯`masked`ã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the audio classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,)`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—éŸ³é¢‘åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels
    > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig))
    and inputs.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[ASTConfig](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€”
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`çš„`torch.FloatTensor`ï¼‰ â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆåœ¨SoftMaxä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [ASTForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[ASTForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE9]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
