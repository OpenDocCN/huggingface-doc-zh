["```py\n( model )\n```", "```py\n( tokens ) \u2192 int\n```", "```py\n( tokens ) \u2192 int\n```", "```py\n( ids skip_special_tokens = True ) \u2192 str\n```", "```py\n( sequences skip_special_tokens = True ) \u2192 List[str]\n```", "```py\n( direction = 'right' pad_id = 0 pad_type_id = 0 pad_token = '[PAD]' length = None pad_to_multiple_of = None )\n```", "```py\n( max_length stride = 0 strategy = 'longest_first' direction = 'right' )\n```", "```py\n( sequence pair = None is_pretokenized = False add_special_tokens = True ) \u2192 Encoding\n```", "```py\nencode(\"A single sequence\")*\nencode(\"A sequence\", \"And its pair\")*\nencode([ \"A\", \"pre\", \"tokenized\", \"sequence\" ], is_pretokenized=True)`\nencode(\n[ \"A\", \"pre\", \"tokenized\", \"sequence\" ], [ \"And\", \"its\", \"pair\" ],\nis_pretokenized=True\n)\n```", "```py\n( input is_pretokenized = False add_special_tokens = True ) \u2192 A List of [`~tokenizers.Encoding\u201c]\n```", "```py\nencode_batch([\n\"A single sequence\",\n(\"A tuple with a sequence\", \"And its pair\"),\n[ \"A\", \"pre\", \"tokenized\", \"sequence\" ],\n([ \"A\", \"pre\", \"tokenized\", \"sequence\" ], \"And its pair\")\n])\n```", "```py\n( buffer ) \u2192 Tokenizer\n```", "```py\n( path ) \u2192 Tokenizer\n```", "```py\n( identifier revision = 'main' auth_token = None ) \u2192 Tokenizer\n```", "```py\n( json ) \u2192 Tokenizer\n```", "```py\n( with_added_tokens = True ) \u2192 Dict[str, int]\n```", "```py\n( with_added_tokens = True ) \u2192 int\n```", "```py\n( id ) \u2192 Optional[str]\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( is_pair )\n```", "```py\n( encoding pair = None add_special_tokens = True ) \u2192 Encoding\n```", "```py\n( path pretty = True )\n```", "```py\n( pretty = False ) \u2192 str\n```", "```py\n( token ) \u2192 Optional[int]\n```", "```py\n( files trainer = None )\n```", "```py\n( iterator trainer = None length = None )\n```"]