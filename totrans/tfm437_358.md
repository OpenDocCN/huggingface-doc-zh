# MatCha

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/matcha](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/matcha)

## 概述

MatCha在论文[MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering](https://arxiv.org/abs/2212.09662)中提出，作者是Fangyu Liu、Francesco Piccinno、Syrine Krichene、Chenxi Pang、Kenton Lee、Mandar Joshi、Yasemin Altun、Nigel Collier、Julian Martin Eisenschlos。

论文摘要如下：

*视觉语言数据，如图表和信息图表，在人类世界中随处可见。然而，最先进的视觉语言模型在这些数据上表现不佳。我们提出MatCha（数学推理和图表解渲染预训练）来增强视觉语言模型在联合建模图表/图表和语言数据方面的能力。具体来说，我们提出了几个预训练任务，涵盖了图表解构和数值推理，这是视觉语言建模中的关键能力。我们从Pix2Struct开始执行MatCha预训练，Pix2Struct是最近提出的图像到文本视觉语言模型。在PlotQA和ChartQA等标准基准测试中，MatCha模型的表现优于最先进的方法近20％。我们还研究了MatCha预训练在诸如屏幕截图、教科书图表和文档图表等领域的转移情况，并观察到整体改进，验证了MatCha预训练在更广泛的视觉语言任务上的有用性。*

## 模型描述

MatCha是使用`Pix2Struct`架构训练的模型。您可以在[Pix2Struct文档](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct)中找到有关`Pix2Struct`的更多信息。MatCha是`Pix2Struct`架构的视觉问答子集。它在图像上呈现输入问题并预测答案。

## 用法

目前有6个MatCha的检查点可用：

+   `google/matcha`：基础MatCha模型，用于在下游任务上微调MatCha

+   `google/matcha-chartqa`：在ChartQA数据集上微调的MatCha模型。可用于回答有关图表的问题。

+   `google/matcha-plotqa-v1`：在PlotQA数据集上微调的MatCha模型。可用于回答有关图表的问题。

+   `google/matcha-plotqa-v2`：在PlotQA数据集上微调的MatCha模型。可用于回答有关图表的问题。

+   `google/matcha-chart2text-statista`：在Statista数据集上微调的MatCha模型。

+   `google/matcha-chart2text-pew`：在Pew数据集上微调的MatCha模型。

在`chart2text-pew`和`chart2text-statista`上微调的模型更适合摘要，而在`plotqa`和`chartqa`上微调的模型更适合回答问题。

您可以按以下方式使用这些模型（在ChatQA数据集上的示例）：

```py
from transformers import AutoProcessor, Pix2StructForConditionalGeneration
import requests
from PIL import Image

model = Pix2StructForConditionalGeneration.from_pretrained("google/matcha-chartqa").to(0)
processor = AutoProcessor.from_pretrained("google/matcha-chartqa")
url = "https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/20294671002019.png"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, text="Is the sum of all 4 places greater than Laos?", return_tensors="pt").to(0)
predictions = model.generate(**inputs, max_new_tokens=512)
print(processor.decode(predictions[0], skip_special_tokens=True))
```

## 微调

要微调MatCha，请参考pix2struct [微调笔记本](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb)。对于`Pix2Struct`模型，我们发现使用Adafactor和余弦学习率调度器微调模型会导致更快的收敛：

```py
from transformers.optimization import Adafactor, get_cosine_schedule_with_warmup

optimizer = Adafactor(self.parameters(), scale_parameter=False, relative_step=False, lr=0.01, weight_decay=1e-05)
scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=1000, num_training_steps=40000)
```

MatCha是使用`Pix2Struct`架构训练的模型。您可以在[Pix2Struct文档](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct)中找到有关`Pix2Struct`的更多信息。
