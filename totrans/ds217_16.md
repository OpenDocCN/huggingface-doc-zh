# åŠ è½½

> åŸæ–‡ï¼š[`huggingface.co/docs/datasets/loading`](https://huggingface.co/docs/datasets/loading)

æ‚¨çš„æ•°æ®å¯ä»¥å­˜å‚¨åœ¨å„ç§ä½ç½®ï¼›å®ƒä»¬å¯ä»¥åœ¨æœ¬åœ°è®¡ç®—æœºçš„ç£ç›˜ä¸Šã€åœ¨ Github å­˜å‚¨åº“ä¸­ä»¥åŠåœ¨ Python å­—å…¸å’Œ Pandas DataFrames ç­‰å†…å­˜æ•°æ®ç»“æ„ä¸­ã€‚æ— è®ºæ•°æ®é›†å­˜å‚¨åœ¨å“ªé‡Œï¼ŒğŸ¤— æ•°æ®é›†éƒ½å¯ä»¥å¸®åŠ©æ‚¨åŠ è½½å®ƒã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä»ä»¥ä¸‹ä½ç½®åŠ è½½æ•°æ®é›†ï¼š

+   æ²¡æœ‰æ•°æ®é›†åŠ è½½è„šæœ¬çš„ Hub

+   æœ¬åœ°åŠ è½½è„šæœ¬

+   æœ¬åœ°æ–‡ä»¶

+   å†…å­˜æ•°æ®

+   ç¦»çº¿

+   åˆ†å‰²çš„ç‰¹å®šåˆ‡ç‰‡

æœ‰å…³åŠ è½½å…¶ä»–æ•°æ®é›†æ¨¡æ€çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹åŠ è½½éŸ³é¢‘æ•°æ®é›†æŒ‡å—ã€åŠ è½½å›¾åƒæ•°æ®é›†æŒ‡å—æˆ–åŠ è½½æ–‡æœ¬æ•°æ®é›†æŒ‡å—ã€‚

## Hugging Face Hub

æ•°æ®é›†æ˜¯ä»ä¸‹è½½å’Œç”Ÿæˆæ•°æ®é›†çš„æ•°æ®é›†åŠ è½½è„šæœ¬ä¸­åŠ è½½çš„ã€‚ä½†æ˜¯ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä» Hub ä¸Šçš„ä»»ä½•æ•°æ®é›†å­˜å‚¨åº“åŠ è½½æ•°æ®é›†è€Œæ— éœ€åŠ è½½è„šæœ¬ï¼é¦–å…ˆåˆ›å»ºä¸€ä¸ªæ•°æ®é›†å­˜å‚¨åº“ï¼Œå¹¶ä¸Šä¼ æ‚¨çš„æ•°æ®æ–‡ä»¶ã€‚ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨ load_dataset()å‡½æ•°æ¥åŠ è½½æ•°æ®é›†ã€‚

ä¾‹å¦‚ï¼Œå°è¯•é€šè¿‡æä¾›å­˜å‚¨åº“å‘½åç©ºé—´å’Œæ•°æ®é›†åç§°ä»è¿™ä¸ª[demo å­˜å‚¨åº“](https://huggingface.co/datasets/lhoestq/demo1)åŠ è½½æ–‡ä»¶ã€‚è¿™ä¸ªæ•°æ®é›†å­˜å‚¨åº“åŒ…å« CSV æ–‡ä»¶ï¼Œä¸‹é¢çš„ä»£ç ä» CSV æ–‡ä»¶åŠ è½½æ•°æ®é›†ï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("lhoestq/demo1")
```

æŸäº›æ•°æ®é›†å¯èƒ½åŸºäº Git æ ‡ç­¾ã€åˆ†æ”¯æˆ–æäº¤æœ‰å¤šä¸ªç‰ˆæœ¬ã€‚ä½¿ç”¨`revision`å‚æ•°æŒ‡å®šè¦åŠ è½½çš„æ•°æ®é›†ç‰ˆæœ¬ï¼š

```py
>>> dataset = load_dataset(
...   "lhoestq/custom_squad",
...   revision="main"  # tag name, or branch name, or commit hash
... )
```

æœ‰å…³å¦‚ä½•åœ¨ Hub ä¸Šåˆ›å»ºæ•°æ®é›†å­˜å‚¨åº“ä»¥åŠå¦‚ä½•ä¸Šä¼ æ•°æ®æ–‡ä»¶çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒä¸Šä¼ æ•°æ®é›†åˆ° Hub æ•™ç¨‹ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ²¡æœ‰åŠ è½½è„šæœ¬çš„æ•°æ®é›†å°†æ‰€æœ‰æ•°æ®åŠ è½½åˆ°`train`åˆ†å‰²ä¸­ã€‚ä½¿ç”¨`data_files`å‚æ•°å°†æ•°æ®æ–‡ä»¶æ˜ å°„åˆ°`train`ã€`validation`å’Œ`test`ç­‰åˆ†å‰²ï¼š

```py
>>> data_files = {"train": "train.csv", "test": "test.csv"}
>>> dataset = load_dataset("namespace/your_dataset_name", data_files=data_files)
```

å¦‚æœæ‚¨æ²¡æœ‰æŒ‡å®šè¦ä½¿ç”¨å“ªäº›æ•°æ®æ–‡ä»¶ï¼Œload_dataset()å°†è¿”å›æ‰€æœ‰æ•°æ®æ–‡ä»¶ã€‚å¦‚æœåŠ è½½åƒ C4 è¿™æ ·çš„å¤§å‹æ•°æ®é›†ï¼Œè¿™å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼ŒC4 å¤§çº¦æœ‰ 13TB çš„æ•°æ®ã€‚

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨`data_files`æˆ–`data_dir`å‚æ•°åŠ è½½ç‰¹å®šçš„æ–‡ä»¶å­é›†ã€‚è¿™äº›å‚æ•°å¯ä»¥æ¥å—ä¸€ä¸ªç›¸å¯¹è·¯å¾„ï¼Œè¯¥è·¯å¾„è§£æä¸ºæ•°æ®é›†åŠ è½½çš„åŸºæœ¬è·¯å¾„ã€‚

```py
>>> from datasets import load_dataset

# load files that match the grep pattern
>>> c4_subset = load_dataset("allenai/c4", data_files="en/c4-train.0000*-of-01024.json.gz")

# load dataset from the en directory on the Hub
>>> c4_subset = load_dataset("allenai/c4", data_dir="en")
```

`split`å‚æ•°è¿˜å¯ä»¥å°†æ•°æ®æ–‡ä»¶æ˜ å°„åˆ°ç‰¹å®šçš„åˆ†å‰²ï¼š

```py
>>> data_files = {"validation": "en/c4-validation.*.json.gz"}
>>> c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation")
```

## æœ¬åœ°åŠ è½½è„šæœ¬

æ‚¨å¯èƒ½åœ¨è®¡ç®—æœºä¸Šæœ¬åœ°æ‹¥æœ‰ä¸€ä¸ªğŸ¤— æ•°æ®é›†åŠ è½½è„šæœ¬ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé€šè¿‡å°†ä»¥ä¸‹è·¯å¾„ä¹‹ä¸€ä¼ é€’ç»™ load_dataset()æ¥åŠ è½½æ•°æ®é›†ï¼š

+   åŠ è½½è„šæœ¬æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„ã€‚

+   åŒ…å«åŠ è½½è„šæœ¬æ–‡ä»¶çš„ç›®å½•çš„æœ¬åœ°è·¯å¾„ï¼ˆä»…å½“è„šæœ¬æ–‡ä»¶ä¸ç›®å½•åŒåæ—¶ï¼‰ã€‚

ä¼ é€’`trust_remote_code=True`ä»¥å…è®¸ğŸ¤— æ•°æ®é›†æ‰§è¡ŒåŠ è½½è„šæœ¬ï¼š

```py
>>> dataset = load_dataset("path/to/local/loading_script/loading_script.py", split="train", trust_remote_code=True)
>>> dataset = load_dataset("path/to/local/loading_script", split="train", trust_remote_code=True)  # equivalent because the file has the same name as the directory
```

### ç¼–è¾‘åŠ è½½è„šæœ¬

æ‚¨è¿˜å¯ä»¥ç¼–è¾‘ Hub ä¸­çš„åŠ è½½è„šæœ¬ä»¥æ·»åŠ è‡ªå·±çš„ä¿®æ”¹ã€‚å°†æ•°æ®é›†å­˜å‚¨åº“ä¸‹è½½åˆ°æœ¬åœ°ï¼Œä»¥ä¾¿åŠ è½½è„šæœ¬ä¸­ç›¸å¯¹è·¯å¾„å¼•ç”¨çš„ä»»ä½•æ•°æ®æ–‡ä»¶éƒ½å¯ä»¥è¢«åŠ è½½ï¼š

```py
git clone https://huggingface.co/datasets/eli5
```

ç¼–è¾‘åŠ è½½è„šæœ¬ï¼Œç„¶åé€šè¿‡å°†å…¶æœ¬åœ°è·¯å¾„ä¼ é€’ç»™ load_dataset()æ¥åŠ è½½å®ƒï¼š

```py
>>> from datasets import load_dataset
>>> eli5 = load_dataset("path/to/local/eli5")
```

## æœ¬åœ°å’Œè¿œç¨‹æ–‡ä»¶

æ•°æ®é›†å¯ä»¥ä»å­˜å‚¨åœ¨è®¡ç®—æœºä¸Šçš„æœ¬åœ°æ–‡ä»¶å’Œè¿œç¨‹æ–‡ä»¶åŠ è½½ã€‚è¿™äº›æ•°æ®é›†å¾ˆå¯èƒ½å­˜å‚¨ä¸º`csv`ã€`json`ã€`txt`æˆ–`parquet`æ–‡ä»¶ã€‚load_dataset()å‡½æ•°å¯ä»¥åŠ è½½æ¯ç§æ–‡ä»¶ç±»å‹ã€‚

### CSV

ğŸ¤— æ•°æ®é›†å¯ä»¥è¯»å–ç”±ä¸€ä¸ªæˆ–å¤šä¸ª CSV æ–‡ä»¶ç»„æˆçš„æ•°æ®é›†ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå°† CSV æ–‡ä»¶ä½œä¸ºåˆ—è¡¨ä¼ é€’ï¼‰ï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("csv", data_files="my_file.csv")
```

æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹å¦‚ä½•ä» CSV æ–‡ä»¶åŠ è½½è¡¨æ ¼æ•°æ®é›†æŒ‡å—ã€‚

### JSON

JSON æ–‡ä»¶ç›´æ¥ä½¿ç”¨ load_dataset()åŠ è½½ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("json", data_files="my_file.json")
```

JSON æ–‡ä»¶æœ‰å„ç§æ ¼å¼ï¼Œä½†æˆ‘ä»¬è®¤ä¸ºæœ€æœ‰æ•ˆçš„æ ¼å¼æ˜¯å…·æœ‰å¤šä¸ª JSON å¯¹è±¡ï¼›æ¯è¡Œä»£è¡¨ä¸€ä¸ªæ•°æ®è¡Œã€‚ä¾‹å¦‚ï¼š

```py
{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true}
```

æ‚¨å¯èƒ½ä¼šé‡åˆ°çš„å¦ä¸€ç§ JSON æ ¼å¼æ˜¯åµŒå¥—å­—æ®µï¼Œè¿™ç§æƒ…å†µä¸‹æ‚¨éœ€è¦åƒä¸‹é¢è¿™æ ·æŒ‡å®š`field`å‚æ•°ï¼š

```py
{"version": "0.1.0",
 "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
          {"a": 4, "b": -5.5, "c": null, "d": true}]
}

>>> from datasets import load_dataset
>>> dataset = load_dataset("json", data_files="my_file.json", field="data")
```

é€šè¿‡ HTTP åŠ è½½è¿œç¨‹ JSON æ–‡ä»¶ï¼Œä¼ é€’ URLsï¼š

```py
>>> base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
>>> dataset = load_dataset("json", data_files={"train": base_url + "train-v1.1.json", "validation": base_url + "dev-v1.1.json"}, field="data")
```

è™½ç„¶è¿™äº›æ˜¯æœ€å¸¸è§çš„ JSON æ ¼å¼ï¼Œä½†æ‚¨ä¼šçœ‹åˆ°å…¶ä»–æ ¼å¼ä¸åŒçš„æ•°æ®é›†ã€‚ğŸ¤—æ•°æ®é›†è¯†åˆ«è¿™äº›å…¶ä»–æ ¼å¼ï¼Œå¹¶å°†ç›¸åº”åœ°å›é€€åˆ° Python JSON åŠ è½½æ–¹æ³•æ¥å¤„ç†å®ƒä»¬ã€‚

### Parquet

Parquet æ–‡ä»¶ä»¥åˆ—æ ¼å¼å­˜å‚¨ï¼Œä¸åŒäºåƒ CSV è¿™æ ·çš„åŸºäºè¡Œçš„æ–‡ä»¶ã€‚ç”±äºå…¶æ›´é«˜æ•ˆå’Œæ›´å¿«é€Ÿåœ°è¿”å›æŸ¥è¯¢ç»“æœï¼Œå¤§å‹æ•°æ®é›†å¯èƒ½å­˜å‚¨åœ¨ Parquet æ–‡ä»¶ä¸­ã€‚

åŠ è½½ Parquet æ–‡ä»¶ï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'})
```

é€šè¿‡ HTTP åŠ è½½è¿œç¨‹ Parquet æ–‡ä»¶ï¼Œä¼ é€’ URLsï¼š

```py
>>> base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
>>> data_files = {"train": base_url + "wikipedia-train.parquet"}
>>> wiki = load_dataset("parquet", data_files=data_files, split="train")
```

### Arrow

Arrow æ–‡ä»¶ä»¥å†…å­˜ä¸­çš„åˆ—æ ¼å¼å­˜å‚¨ï¼Œä¸åŒäºåŸºäºè¡Œçš„æ ¼å¼å¦‚ CSV å’Œæœªå‹ç¼©æ ¼å¼å¦‚ Parquetã€‚

åŠ è½½ Arrow æ–‡ä»¶ï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("arrow", data_files={'train': 'train.arrow', 'test': 'test.arrow'})
```

é€šè¿‡ HTTP åŠ è½½è¿œç¨‹ Arrow æ–‡ä»¶ï¼Œä¼ é€’ URLsï¼š

```py
>>> base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
>>> data_files = {"train": base_url + "wikipedia-train.arrow"}
>>> wiki = load_dataset("arrow", data_files=data_files, split="train")
```

Arrow æ˜¯ğŸ¤—æ•°æ®é›†åº•å±‚ä½¿ç”¨çš„æ–‡ä»¶æ ¼å¼ï¼Œå› æ­¤æ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨ Dataset.from_file()åŠ è½½æœ¬åœ° Arrow æ–‡ä»¶ï¼š

```py
>>> from datasets import Dataset
>>> dataset = Dataset.from_file("data.arrow")
```

ä¸ load_dataset()ä¸åŒï¼ŒDataset.from_file()ä¼šåœ¨ä¸å‡†å¤‡ç¼“å­˜ä¸­çš„æ•°æ®é›†çš„æƒ…å†µä¸‹å†…å­˜æ˜ å°„ Arrow æ–‡ä»¶ï¼ŒèŠ‚çœç£ç›˜ç©ºé—´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”¨äºå­˜å‚¨ä¸­é—´å¤„ç†ç»“æœçš„ç¼“å­˜ç›®å½•å°†æ˜¯ Arrow æ–‡ä»¶ç›®å½•ã€‚

ç›®å‰ä»…æ”¯æŒ Arrow æµæ ¼å¼ã€‚ä¸æ”¯æŒ Arrow IPC æ–‡ä»¶æ ¼å¼ï¼ˆä¹Ÿç§°ä¸º Feather V2ï¼‰ã€‚

### SQL

é€šè¿‡æŒ‡å®šè¿æ¥åˆ°æ•°æ®åº“çš„ URI ä½¿ç”¨ from_sql()è¯»å–æ•°æ®åº“å†…å®¹ã€‚æ‚¨å¯ä»¥è¯»å–è¡¨åå’ŒæŸ¥è¯¢ï¼š

```py
>>> from datasets import Dataset
# load entire table
>>> dataset = Dataset.from_sql("data_table_name", con="sqlite:///sqlite_file.db")
# load from query
>>> dataset = Dataset.from_sql("SELECT text FROM table WHERE length(text) > 100 LIMIT 10", con="sqlite:///sqlite_file.db")
```

æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹å¦‚ä½•ä» SQL æ•°æ®åº“åŠ è½½è¡¨æ ¼æ•°æ®é›†æŒ‡å—ã€‚

### WebDataset

[WebDataset](https://github.com/webdataset/webdataset)æ ¼å¼åŸºäº TAR å­˜æ¡£ï¼Œé€‚ç”¨äºå¤§å‹å›¾åƒæ•°æ®é›†ã€‚ç”±äºå…¶å¤§å°ï¼ŒWebDatasets é€šå¸¸ä»¥æµæ¨¡å¼åŠ è½½ï¼ˆä½¿ç”¨`streaming=True`ï¼‰ã€‚

æ‚¨å¯ä»¥è¿™æ ·åŠ è½½ WebDatasetï¼š

```py
>>> from datasets import load_dataset
>>>
>>> path = "path/to/train/*.tar"
>>> dataset = load_dataset("webdataset", data_files={"train": path}, split="train", streaming=True)
```

é€šè¿‡ HTTP åŠ è½½è¿œç¨‹ WebDatasetsï¼Œä¼ é€’ URLsï¼š

```py
>>> from datasets import load_dataset
>>>
>>> base_url = "https://huggingface.co/datasets/lhoestq/small-publaynet-wds/resolve/main/publaynet-train-{i:06d}.tar"
>>> urls = [base_url.format(i=i) for i in range(4)]
>>> dataset = load_dataset("webdataset", data_files={"train": urls}, split="train", streaming=True)
```

## å¤šè¿›ç¨‹

å½“æ•°æ®é›†ç”±å¤šä¸ªæ–‡ä»¶ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œåˆ†ç‰‡â€ï¼‰ç»„æˆæ—¶ï¼Œå¯ä»¥æ˜¾è‘—åŠ å¿«æ•°æ®é›†ä¸‹è½½å’Œå‡†å¤‡æ­¥éª¤ã€‚

æ‚¨å¯ä»¥é€‰æ‹©ä½¿ç”¨`num_proc`å¹¶è¡Œå‡†å¤‡æ•°æ®é›†çš„è¿›ç¨‹æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªè¿›ç¨‹å°†è·å¾—ä¸€ç»„åˆ†ç‰‡æ¥å‡†å¤‡ï¼š

```py
from datasets import load_dataset

imagenet = load_dataset("imagenet-1k", num_proc=8)
ml_librispeech_spanish = load_dataset("facebook/multilingual_librispeech", "spanish", num_proc=8)
```

## å†…å­˜æ•°æ®

ğŸ¤—æ•°æ®é›†è¿˜å…è®¸æ‚¨ç›´æ¥ä»å†…å­˜æ•°æ®ç»“æ„ï¼ˆå¦‚ Python å­—å…¸å’Œ Pandas DataFramesï¼‰åˆ›å»º Datasetã€‚

### Python å­—å…¸

ä½¿ç”¨ from_dict()åŠ è½½ Python å­—å…¸ï¼š

```py
>>> from datasets import Dataset
>>> my_dict = {"a": [1, 2, 3]}
>>> dataset = Dataset.from_dict(my_dict)
```

### Python å­—å…¸åˆ—è¡¨

ä½¿ç”¨`from_list()`åŠ è½½ Python å­—å…¸åˆ—è¡¨ï¼š

```py
>>> from datasets import Dataset
>>> my_list = [{"a": 1}, {"a": 2}, {"a": 3}]
>>> dataset = Dataset.from_list(my_list)
```

### Python ç”Ÿæˆå™¨

ä½¿ç”¨ from_generator()ä» Python ç”Ÿæˆå™¨åˆ›å»ºæ•°æ®é›†ï¼š

```py
>>> from datasets import Dataset
>>> def my_gen():
...     for i in range(1, 4):
...         yield {"a": i}
...
>>> dataset = Dataset.from_generator(my_gen)
```

è¿™ç§æ–¹æ³•æ”¯æŒåŠ è½½å¤§äºå¯ç”¨å†…å­˜çš„æ•°æ®ã€‚

æ‚¨è¿˜å¯ä»¥é€šè¿‡å°†åˆ—è¡¨ä¼ é€’ç»™`gen_kwargs`æ¥å®šä¹‰åˆ†ç‰‡æ•°æ®é›†ï¼š

```py
>>> def gen(shards):
...     for shard in shards:
...         with open(shard) as f:
...             for line in f:
...                 yield {"line": line}
...
>>> shards = [f"data{i}.txt" for i in range(32)]
>>> ds = IterableDataset.from_generator(gen, gen_kwargs={"shards": shards})
>>> ds = ds.shuffle(seed=42, buffer_size=10_000)  # shuffles the shards order + uses a shuffle buffer
>>> from torch.utils.data import DataLoader
>>> dataloader = DataLoader(ds.with_format("torch"), num_workers=4)  # give each worker a subset of 32/4=8 shards
```

### Pandas DataFrame

ä½¿ç”¨ from_pandas()åŠ è½½ Pandas æ•°æ®æ¡†ï¼š

```py
>>> from datasets import Dataset
>>> import pandas as pd
>>> df = pd.DataFrame({"a": [1, 2, 3]})
>>> dataset = Dataset.from_pandas(df)
```

æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹å¦‚ä½•ä» Pandas æ•°æ®æ¡†åŠ è½½è¡¨æ ¼æ•°æ®é›†æŒ‡å—ã€‚

## ç¦»çº¿

å³ä½¿æ²¡æœ‰äº’è”ç½‘è¿æ¥ï¼Œä»ç„¶å¯ä»¥åŠ è½½æ•°æ®é›†ã€‚åªè¦ä¹‹å‰ä» Hub å­˜å‚¨åº“ä¸‹è½½è¿‡æ•°æ®é›†ï¼Œå®ƒåº”è¯¥å·²è¢«ç¼“å­˜ã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥ä»ç¼“å­˜ä¸­é‡æ–°åŠ è½½æ•°æ®é›†å¹¶ç¦»çº¿ä½¿ç”¨ã€‚

å¦‚æœæ‚¨çŸ¥é“è‡ªå·±å°†æ— æ³•è®¿é—®äº’è”ç½‘ï¼Œå¯ä»¥åœ¨å®Œå…¨ç¦»çº¿æ¨¡å¼ä¸‹è¿è¡ŒğŸ¤—æ•°æ®é›†ã€‚è¿™æ ·åšå¯ä»¥èŠ‚çœæ—¶é—´ï¼Œå› ä¸ºğŸ¤—æ•°æ®é›†ä¸ä¼šç­‰å¾…æ•°æ®é›†æ„å»ºå™¨ä¸‹è½½è¶…æ—¶ï¼Œè€Œæ˜¯ç›´æ¥æŸ¥æ‰¾ç¼“å­˜ã€‚å°†ç¯å¢ƒå˜é‡`HF_DATASETS_OFFLINE`è®¾ç½®ä¸º`1`ä»¥å¯ç”¨å®Œå…¨ç¦»çº¿æ¨¡å¼ã€‚

## åˆ‡ç‰‡æ‹†åˆ†

æ‚¨è¿˜å¯ä»¥é€‰æ‹©ä»…åŠ è½½æ‹†åˆ†çš„ç‰¹å®šåˆ‡ç‰‡ã€‚æœ‰ä¸¤ç§åˆ‡åˆ†æ‹†åˆ†çš„é€‰é¡¹ï¼šä½¿ç”¨å­—ç¬¦ä¸²æˆ– ReadInstruction APIã€‚å¯¹äºç®€å•æƒ…å†µï¼Œå­—ç¬¦ä¸²æ›´ç´§å‡‘ä¸”æ˜“è¯»ï¼Œè€Œ ReadInstruction æ›´å®¹æ˜“ä½¿ç”¨å…·æœ‰å¯å˜åˆ‡ç‰‡å‚æ•°ã€‚

é€šè¿‡è¿æ¥`train`å’Œ`test`æ‹†åˆ†æ¥è¿æ¥ï¼š

```py
>>> train_test_ds = datasets.load_dataset("bookcorpus", split="train+test") Select specific rows of the `train` split:  

```

>>> train_10_20_ds = datasets.load_dataset("bookcorpus", split="train[10:20]") æˆ–é€‰æ‹©æ‹†åˆ†çš„ç™¾åˆ†æ¯”ï¼š

```py
>>> train_10pct_ds = datasets.load_dataset("bookcorpus", split="train[:10%]") Select a combination of percentages from each split:  

```

>>> train_10_80pct_ds = datasets.load_dataset("bookcorpus", split="train[:10%]+train[-80%:]") æœ€åï¼Œæ‚¨ç”šè‡³å¯ä»¥åˆ›å»ºäº¤å‰éªŒè¯æ‹†åˆ†ã€‚ä¸‹é¢çš„ç¤ºä¾‹åˆ›å»ºäº† 10 æŠ˜äº¤å‰éªŒè¯æ‹†åˆ†ã€‚æ¯ä¸ªéªŒè¯æ•°æ®é›†æ˜¯ 10%çš„å—ï¼Œè®­ç»ƒæ•°æ®é›†å å‰©ä½™çš„è¡¥å…… 90%çš„å—ï¼š

```py
>>> val_ds = datasets.load_dataset("bookcorpus", split=[f"train[{k}%:{k+10}%]" for k in range(0, 100, 10)])
>>> train_ds = datasets.load_dataset("bookcorpus", split=[f"train[:{k}%]+train[{k+10}%:]" for k in range(0, 100, 10)])    Percent slicing and rounding The default behavior is to round the boundaries to the nearest integer for datasets where the requested slice boundaries do not divide evenly by 100\. As shown below, some slices may contain more examples than others. For instance, if the following train split includes 999 records, then:  

```

# 19 æ¡è®°å½•ï¼Œä» 500ï¼ˆåŒ…æ‹¬ï¼‰åˆ° 519ï¼ˆä¸åŒ…æ‹¬ï¼‰ã€‚

>>> train_50_52_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%]")

# 20 æ¡è®°å½•ï¼Œä» 519ï¼ˆåŒ…æ‹¬ï¼‰åˆ° 539ï¼ˆä¸åŒ…æ‹¬ï¼‰ã€‚

>>> train_52_54_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%]")

```py

If you want equal sized splits, use `pct1_dropremainder` rounding instead. This treats the specified percentage boundaries as multiples of 1%.

```

# 18 æ¡è®°å½•ï¼Œä» 450ï¼ˆåŒ…æ‹¬ï¼‰åˆ° 468ï¼ˆä¸åŒ…æ‹¬ï¼‰ã€‚

>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train", from_=50, to=52, unit="%", rounding="pct1_dropremainder"))

# 18 æ¡è®°å½•ï¼Œä» 468ï¼ˆåŒ…æ‹¬ï¼‰åˆ° 486ï¼ˆä¸åŒ…æ‹¬ï¼‰ã€‚

>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train",from_=52, to=54, unit="%", rounding="pct1_dropremainder"))

# æˆ–è€…ç­‰æ•ˆåœ°ï¼š

>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split="train50%:52%")

>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split="train52%:54%")

```py

`pct1_dropremainder` rounding may truncate the last examples in a dataset if the number of examples in your dataset donâ€™t divide evenly by 100.

##   Troubleshooting

Sometimes, you may get unexpected results when you load a dataset. Two of the most common issues you may encounter are manually downloading a dataset and specifying features of a dataset.

###   Manual download

Certain datasets require you to manually download the dataset files due to licensing incompatibility or if the files are hidden behind a login page. This causes load_dataset() to throw an `AssertionError`. But ğŸ¤— Datasets provides detailed instructions for downloading the missing files. After youâ€™ve downloaded the files, use the `data_dir` argument to specify the path to the files you just downloaded.

For example, if you try to download a configuration from the [MATINF](https://huggingface.co/datasets/matinf) dataset:

```

>>> æ•°æ®é›† = load_dataset("matinf", "summarization")

ä¸‹è½½å’Œå‡†å¤‡æ•°æ®é›† matinf/summarizationï¼ˆä¸‹è½½ï¼šæœªçŸ¥å¤§å°ï¼Œç”Ÿæˆï¼š246.89 MiBï¼Œåå¤„ç†ï¼šæœªçŸ¥å¤§å°ï¼Œæ€»å…±ï¼š246.89 MiBï¼‰åˆ°/root/.cache/huggingface/datasets/matinf/summarization/1.0.0/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...

æ–­è¨€é”™è¯¯ï¼šé…ç½® summarization çš„æ•°æ®é›† matinf éœ€è¦æ‰‹åŠ¨æ•°æ®ã€‚

è¯·æŒ‰ç…§æ‰‹åŠ¨ä¸‹è½½è¯´æ˜æ“ä½œï¼šè¦ä½¿ç”¨ MATINFï¼Œæ‚¨å¿…é¡»æ‰‹åŠ¨ä¸‹è½½ã€‚è¯·å¡«å†™æ­¤ google è¡¨æ ¼ï¼ˆhttps://forms.gle/nkH4LVE4iNQeDzsc9ï¼‰ã€‚å®Œæˆè¡¨æ ¼åï¼Œæ‚¨å°†æ”¶åˆ°ä¸‹è½½é“¾æ¥å’Œå¯†ç ã€‚è¯·å°†æ‰€æœ‰æ–‡ä»¶æå–åˆ°ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œå¹¶ä½¿ç”¨ä»¥ä¸‹æ–¹å¼åŠ è½½æ•°æ®é›†ï¼š*datasets.load_dataset('matinf', data_dir='path/to/folder/folder_name')*ã€‚

å¯ä»¥ä½¿ç”¨`datasets.load_dataset(matinf, data_dir='<path/to/manual/data>')`åŠ è½½æ‰‹åŠ¨æ•°æ®

```py

If youâ€™ve already downloaded a dataset from the *Hub with a loading script* to your computer, then you need to pass an absolute path to the `data_dir` or `data_files` parameter to load that dataset. Otherwise, if you pass a relative path, load_dataset() will load the directory from the repository on the Hub instead of the local directory.

###   Specify features

When you create a dataset from local files, the Features are automatically inferred by [Apache Arrow](https://arrow.apache.org/docs/). However, the datasetâ€™s features may not always align with your expectations, or you may want to define the features yourself. The following example shows how you can add custom labels with the ClassLabel feature.

Start by defining your own labels with the Features class:

```

>>> class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]

>>> æƒ…æ„Ÿç‰¹å¾ = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})

```py

Next, specify the `features` parameter in load_dataset() with the features you just created:

```

>>> dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)

```py

Now when you look at your dataset features, you can see it uses the custom labels you defined:

```

>>> dataset['train'].features

{'text': Value(dtype='string', id=None),

'label': ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], names_file=None, id=None)}

```py

##   Metrics

Metrics is deprecated in ğŸ¤— Datasets. To learn more about how to use metrics, take a look at the library ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)! In addition to metrics, you can find more tools for evaluating models and datasets.

When the metric you want to use is not supported by ğŸ¤— Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:

```

ä»æ•°æ®é›†å¯¼å…¥åŠ è½½åº¦é‡

åº¦é‡=load_metric('PATH/TO/MY/METRIC/SCRIPT')

# å…¸å‹ç”¨æ³•ç¤ºä¾‹

å¯¹äºæ•°æ®é›†ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡ï¼š

è¾“å…¥ã€å‚è€ƒ=æ‰¹æ¬¡

é¢„æµ‹=model(inputs)

åº¦é‡.add_batch(predictions=predictions, references=references)

å¾—åˆ†=åº¦é‡è®¡ç®—()

```py

See the Metrics guide for more details on how to write your own metric loading script.

###   Load configurations

It is possible for a metric to have different configurations. The configurations are stored in the `config_name` parameter in MetricInfo attribute. When you load a metric, provide the configuration name as shown in the following:

```

ä»æ•°æ®é›†å¯¼å…¥åŠ è½½åº¦é‡

åº¦é‡=load_metric('bleurt', name='bleurt-base-128')

åº¦é‡=load_metric('bleurt', name='bleurt-base-512')

```py

###   Distributed setup

When working in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. ğŸ¤— Datasets supports distributed usage with a few additional arguments when you load a metric.

For example, imagine you are training and evaluating on eight parallel processes. Hereâ€™s how you would load a metric in this distributed setting:

1.  Define the total number of processes with the `num_process` argument.

2.  Set the process `rank` as an integer between zero and `num_process - 1`.

3.  Load your metric with load_metric() with these arguments:

```

ä»æ•°æ®é›†å¯¼å…¥åŠ è½½åº¦é‡

åº¦é‡=load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)

```py

Once youâ€™ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, Metric.compute() gathers all the predictions and references from the nodes, and computes the final metric.

In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an `experiment_id` to distinguish the separate evaluations:

```

ä»æ•°æ®é›†å¯¼å…¥åŠ è½½åº¦é‡

åº¦é‡=load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id, experiment_id="My_experiment_10")

```py

```

```py

```

```py

```
