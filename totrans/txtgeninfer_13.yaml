- en: Non-core Model Serving
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: éæ ¸å¿ƒæ¨¡å‹æœåŠ¡
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models](https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models](https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: TGI supports various LLM architectures (see full list [here](../supported_models)).
    If you wish to serve a model that is not one of the supported models, TGI will
    fallback to the `transformers` implementation of that model. This means you will
    be unable to use some of the features introduced by TGI, such as tensor-parallel
    sharding or flash attention. However, you can still get many benefits of TGI,
    such as continuous batching or streaming outputs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: TGIæ”¯æŒå„ç§LLMæ¶æ„ï¼ˆè¯·å‚è§å®Œæ•´åˆ—è¡¨[æ­¤å¤„](../supported_models)ï¼‰ã€‚å¦‚æœæ‚¨å¸Œæœ›æä¾›çš„æ¨¡å‹ä¸æ˜¯å—æ”¯æŒçš„æ¨¡å‹ä¹‹ä¸€ï¼ŒTGIå°†é€€å›åˆ°è¯¥æ¨¡å‹çš„`transformers`å®ç°ã€‚è¿™æ„å‘³ç€æ‚¨å°†æ— æ³•ä½¿ç”¨TGIå¼•å…¥çš„ä¸€äº›åŠŸèƒ½ï¼Œä¾‹å¦‚å¼ é‡å¹¶è¡Œåˆ†ç‰‡æˆ–é—ªå…‰æ³¨æ„åŠ›ã€‚ä½†æ˜¯ï¼Œæ‚¨ä»ç„¶å¯ä»¥è·å¾—TGIçš„è®¸å¤šå¥½å¤„ï¼Œä¾‹å¦‚è¿ç»­æ‰¹å¤„ç†æˆ–æµå¼è¾“å‡ºã€‚
- en: You can serve these models using the same Docker command-line invocation as
    with fully supported models ğŸ‘‡
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨ä¸å®Œå…¨æ”¯æŒçš„æ¨¡å‹ç›¸åŒçš„Dockerå‘½ä»¤è¡Œè°ƒç”¨æ¥æä¾›è¿™äº›æ¨¡å‹ğŸ‘‡
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If the model you wish to serve is a custom transformers model, and its weights
    and implementation are available in the Hub, you can still serve the model by
    passing the `--trust-remote-code` flag to the `docker run` command like below
    ğŸ‘‡
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å¸Œæœ›æä¾›çš„æ¨¡å‹æ˜¯è‡ªå®šä¹‰çš„transformersæ¨¡å‹ï¼Œå¹¶ä¸”å…¶æƒé‡å’Œå®ç°åœ¨Hubä¸Šå¯ç”¨ï¼Œæ‚¨ä»ç„¶å¯ä»¥é€šè¿‡å‘`docker run`å‘½ä»¤ä¼ é€’`--trust-remote-code`æ ‡å¿—æ¥æä¾›æ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºğŸ‘‡
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Finally, if the model is not on Hugging Face Hub but on your local, you can
    pass the path to the folder that contains your model like below ğŸ‘‡
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå¦‚æœæ¨¡å‹ä¸åœ¨Hugging Face Hubä¸Šï¼Œè€Œæ˜¯åœ¨æœ¬åœ°ï¼Œæ‚¨å¯ä»¥åƒä¸‹é¢è¿™æ ·ä¼ é€’åŒ…å«æ‚¨çš„æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„ğŸ‘‡
- en: '[PRE2]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can refer to [transformers docs on custom models](https://huggingface.co/docs/transformers/main/en/custom_models)
    for more information.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥å‚è€ƒ[transformersæ–‡æ¡£ä¸Šçš„è‡ªå®šä¹‰æ¨¡å‹](https://huggingface.co/docs/transformers/main/en/custom_models)è·å–æ›´å¤šä¿¡æ¯ã€‚
