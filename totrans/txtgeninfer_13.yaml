- en: Non-core Model Serving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models](https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: TGI supports various LLM architectures (see full list [here](../supported_models)).
    If you wish to serve a model that is not one of the supported models, TGI will
    fallback to the `transformers` implementation of that model. This means you will
    be unable to use some of the features introduced by TGI, such as tensor-parallel
    sharding or flash attention. However, you can still get many benefits of TGI,
    such as continuous batching or streaming outputs.
  prefs: []
  type: TYPE_NORMAL
- en: You can serve these models using the same Docker command-line invocation as
    with fully supported models ðŸ‘‡
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If the model you wish to serve is a custom transformers model, and its weights
    and implementation are available in the Hub, you can still serve the model by
    passing the `--trust-remote-code` flag to the `docker run` command like below
    ðŸ‘‡
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Finally, if the model is not on Hugging Face Hub but on your local, you can
    pass the path to the folder that contains your model like below ðŸ‘‡
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can refer to [transformers docs on custom models](https://huggingface.co/docs/transformers/main/en/custom_models)
    for more information.
  prefs: []
  type: TYPE_NORMAL
