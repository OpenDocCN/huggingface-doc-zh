- en: Non-core Model Serving
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非核心模型服务
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models](https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models](https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: TGI supports various LLM architectures (see full list [here](../supported_models)).
    If you wish to serve a model that is not one of the supported models, TGI will
    fallback to the `transformers` implementation of that model. This means you will
    be unable to use some of the features introduced by TGI, such as tensor-parallel
    sharding or flash attention. However, you can still get many benefits of TGI,
    such as continuous batching or streaming outputs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: TGI支持各种LLM架构（请参见完整列表[此处](../supported_models)）。如果您希望提供的模型不是受支持的模型之一，TGI将退回到该模型的`transformers`实现。这意味着您将无法使用TGI引入的一些功能，例如张量并行分片或闪光注意力。但是，您仍然可以获得TGI的许多好处，例如连续批处理或流式输出。
- en: You can serve these models using the same Docker command-line invocation as
    with fully supported models 👇
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用与完全支持的模型相同的Docker命令行调用来提供这些模型👇
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If the model you wish to serve is a custom transformers model, and its weights
    and implementation are available in the Hub, you can still serve the model by
    passing the `--trust-remote-code` flag to the `docker run` command like below
    👇
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望提供的模型是自定义的transformers模型，并且其权重和实现在Hub上可用，您仍然可以通过向`docker run`命令传递`--trust-remote-code`标志来提供模型，如下所示👇
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Finally, if the model is not on Hugging Face Hub but on your local, you can
    pass the path to the folder that contains your model like below 👇
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果模型不在Hugging Face Hub上，而是在本地，您可以像下面这样传递包含您的模型的文件夹路径👇
- en: '[PRE2]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can refer to [transformers docs on custom models](https://huggingface.co/docs/transformers/main/en/custom_models)
    for more information.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考[transformers文档上的自定义模型](https://huggingface.co/docs/transformers/main/en/custom_models)获取更多信息。
