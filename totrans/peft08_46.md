# 前缀调整

> 原文：[`huggingface.co/docs/peft/package_reference/prefix_tuning`](https://huggingface.co/docs/peft/package_reference/prefix_tuning)

[前缀调整](https://hf.co/papers/2101.00190)在输入序列中添加了一系列任务特定向量的前缀，这些向量可以在保持预训练模型冻结的同时学习。前缀参数被插入到所有模型层中。

论文摘要如下：

*微调是利用大型预训练语言模型执行下游任务的事实方式。然而，它修改了所有语言模型参数，因此需要为每个任务存储完整副本。在本文中，我们提出了前缀调整，这是一种用于自然语言生成任务的轻量级替代微调的方法，它保持语言模型参数冻结，但优化一个小的连续任务特定向量（称为前缀）。前缀调整受提示的启发，允许后续标记关注这个前缀，就像它是“虚拟标记”一样。我们将前缀调整应用于 GPT-2 进行表格到文本生成，以及应用于 BART 进行摘要。我们发现，通过仅学习 0.1\%的参数，前缀调整在完整数据设置中获得可比较的性能，在低数据设置中优于微调，并且对于训练期间未见过的主题的示例具有更好的外推能力*。

## PrefixTuningConfig

### `class peft.PrefixTuningConfig`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/prefix_tuning/config.py#L21)

```py
( peft_type: Union = None auto_mapping: Optional = None base_model_name_or_path: Optional = None revision: Optional = None task_type: Union = None inference_mode: bool = False num_virtual_tokens: int = None token_dim: int = None num_transformer_submodules: Optional = None num_attention_heads: Optional = None num_layers: Optional = None encoder_hidden_size: int = None prefix_projection: bool = False )
```

参数

+   `encoder_hidden_size` (`int`) — 提示编码器的隐藏大小。

+   `prefix_projection` (`bool`) — 是否投影前缀嵌入。

这是配置类，用于存储 PrefixEncoder 的配置。

## PrefixEncoder

### `class peft.PrefixEncoder`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/prefix_tuning/model.py#L20)

```py
( config )
```

参数

+   `config` (PrefixTuningConfig) — 前缀编码器的配置。

用于编码前缀的`torch.nn`模型。

示例：

```py
>>> from peft import PrefixEncoder, PrefixTuningConfig

>>> config = PrefixTuningConfig(
...     peft_type="PREFIX_TUNING",
...     task_type="SEQ_2_SEQ_LM",
...     num_virtual_tokens=20,
...     token_dim=768,
...     num_transformer_submodules=1,
...     num_attention_heads=12,
...     num_layers=12,
...     encoder_hidden_size=768,
... )
>>> prefix_encoder = PrefixEncoder(config)
```

**属性**：

+   `embedding` (`torch.nn.Embedding`) — 前缀编码器的嵌入层。

+   `transform` (`torch.nn.Sequential`) — 如果`prefix_projection`为`True`，则用于转换前缀嵌入的两层 MLP。

+   `prefix_projection` (`bool`) — 是否投影前缀嵌入。

输入形状：(`batch_size`, `num_virtual_tokens`)

输出形状：(`batch_size`, `num_virtual_tokens`, `2*layers*hidden`)
