- en: SeamlessM4T
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/seamless_m4t](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/seamless_m4t)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/241.2d5a9a73.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The SeamlessM4T model was proposed in [SeamlessM4T — Massively Multilingual
    & Multimodal Machine Translation](https://dl.fbaipublicfiles.com/seamless/seamless_m4t_paper.pdf)
    by the Seamless Communication team from Meta AI.
  prefs: []
  type: TYPE_NORMAL
- en: This is the **version 1** release of the model. For the updated **version 2**
    release, refer to the [Seamless M4T v2 docs](https://huggingface.co/docs/transformers/main/model_doc/seamless_m4t_v2).
  prefs: []
  type: TYPE_NORMAL
- en: SeamlessM4T is a collection of models designed to provide high quality translation,
    allowing people from different linguistic communities to communicate effortlessly
    through speech and text.
  prefs: []
  type: TYPE_NORMAL
- en: 'SeamlessM4T enables multiple tasks without relying on separate models:'
  prefs: []
  type: TYPE_NORMAL
- en: Speech-to-speech translation (S2ST)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech-to-text translation (S2TT)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text-to-speech translation (T2ST)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text-to-text translation (T2TT)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic speech recognition (ASR)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel)
    can perform all the above tasks, but each task also has its own dedicated sub-model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*What does it take to create the Babel Fish, a tool that can help individuals
    translate speech between any two languages? While recent breakthroughs in text-based
    models have pushed machine translation coverage beyond 200 languages, unified
    speech-to-speech translation models have yet to achieve similar strides. More
    specifically, conventional speech-to-speech translation systems rely on cascaded
    systems that perform translation progressively, putting high-performing unified
    systems out of reach. To address these gaps, we introduce SeamlessM4T, a single
    model that supports speech-to-speech translation, speech-to-text translation,
    text-to-speech translation, text-to-text translation, and automatic speech recognition
    for up to 100 languages. To build this, we used 1 million hours of open speech
    audio data to learn self-supervised speech representations with w2v-BERT 2.0\.
    Subsequently, we created a multimodal corpus of automatically aligned speech translations.
    Filtered and combined with human-labeled and pseudo-labeled data, we developed
    the first multilingual system capable of translating from and into English for
    both speech and text. On FLEURS, SeamlessM4T sets a new standard for translations
    into multiple target languages, achieving an improvement of 20% BLEU over the
    previous SOTA in direct speech-to-text translation. Compared to strong cascaded
    models, SeamlessM4T improves the quality of into-English translation by 1.3 BLEU
    points in speech-to-text and by 2.6 ASR-BLEU points in speech-to-speech. Tested
    for robustness, our system performs better against background noises and speaker
    variations in speech-to-text tasks compared to the current SOTA model. Critically,
    we evaluated SeamlessM4T on gender bias and added toxicity to assess translation
    safety. Finally, all contributions in this work are open-sourced and accessible
    at [https://github.com/facebookresearch/seamless_communication](https://github.com/facebookresearch/seamless_communication)*'
  prefs: []
  type: TYPE_NORMAL
- en: Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, load the processor and a checkpoint of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can seamlessly use this model on text or on audio, to generated either translated
    text or translated audio.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how to use the processor to process text and audio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Speech
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel)
    can *seamlessly* generate text or speech with few or no changes. Let’s target
    Russian voice translation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: With basically the same code, I’ve translated English text and Arabic speech
    to Russian speech samples.
  prefs: []
  type: TYPE_NORMAL
- en: Text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similarly, you can generate translated text from audio files or from text with
    the same model. You only have to pass `generate_speech=False` to [SeamlessM4TModel.generate()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel.generate).
    This time, let’s translate to French.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Tips
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1\. Use dedicated models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel)
    is transformers top level model to generate speech and text, but you can also
    use dedicated models that perform the task without additional components, thus
    reducing the memory footprint. For example, you can replace the audio-to-audio
    generation snippet with the model dedicated to the S2ST task, the rest is exactly
    the same code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Or you can replace the text-to-text generation snippet with the model dedicated
    to the T2TT task, you only have to remove `generate_speech=False`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Feel free to try out [SeamlessM4TForSpeechToText](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToText)
    and [SeamlessM4TForTextToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToSpeech)
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Change the speaker identity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You have the possibility to change the speaker used for speech synthesis with
    the `spkr_id` argument. Some `spkr_id` works better than other for some languages!
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Change the generation strategy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can use different [generation strategies](./generation_strategies) for speech
    and text generation, e.g `.generate(input_ids=input_ids, text_num_beams=4, speech_do_sample=True)`
    which will successively perform beam-search decoding on the text model, and multinomial
    sampling on the speech model.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Generate speech and text at the same time
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use `return_intermediate_token_ids=True` with [SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel)
    to return both speech and text !
  prefs: []
  type: TYPE_NORMAL
- en: Model architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SeamlessM4T features a versatile architecture that smoothly handles the sequential
    generation of text and speech. This setup comprises two sequence-to-sequence (seq2seq)
    models. The first model translates the input modality into translated text, while
    the second model generates speech tokens, known as “unit tokens,” from the translated
    text.
  prefs: []
  type: TYPE_NORMAL
- en: Each modality has its own dedicated encoder with a unique architecture. Additionally,
    for speech output, a vocoder inspired by the [HiFi-GAN](https://arxiv.org/abs/2010.05646)
    architecture is placed on top of the second seq2seq model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how the generation process works:'
  prefs: []
  type: TYPE_NORMAL
- en: Input text or speech is processed through its specific encoder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A decoder creates text tokens in the desired language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If speech generation is required, the second seq2seq model, following a standard
    encoder-decoder structure, generates unit tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These unit tokens are then passed through the final vocoder to produce the actual
    speech.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The
    original code can be found [here](https://github.com/facebookresearch/seamless_communication).
  prefs: []
  type: TYPE_NORMAL
- en: SeamlessM4TModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SeamlessM4TModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3927)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`current_modality` (`str`, *optional*, defaults to `"text"`) — Default modality.
    Used to initialize the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original SeamlessM4T Model transformer which can be used for every tasks
    available (S2ST, S2TT, T2TT, T2ST). This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L4141)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [SeamlessM4TTokenizer](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer)
    or [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_banks)`, *optional*) — Input audio features. This should be returnes by the
    [SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)
    class or the [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)
    class. See [SeamlessM4TFeatureExtractor.`call`()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_intermediate_token_ids` (`bool`, *optional*) — If `True`, also returns
    the intermediate generated text and unit tokens. Set to `True` if you also want
    to get translated text alongside the audio. Note that if `generate_speech=True`,
    this parameter will be ignored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tgt_lang` (`str`, *optional*) — The language to use as target language for
    translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spkr_id` (`int`, *optional*, defaults to 0) — The id of the speaker used for
    speech synthesis. Must be lower than `config.vocoder_num_spkrs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generate_speech` (`bool`, *optional*, defaults to `True`) — If `False`, will
    only returns the text tokens and won’t generate speech.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments that will
    be passed to [GenerationMixin.generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate).
    Keyword arguments are of two types:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without a prefix, they will be entered as `**kwargs` for the `generate` method
    of each sub-model, except for `decoder_input_ids` which will only be passed through
    the text components.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With a *text_* or *speech_* prefix, they will be input for the `generate` method
    of the text model and speech model respectively. It has the priority over the
    keywords without a prefix.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This means you can, for example, specify a generation strategy for one generation
    but not for the other.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Union[SeamlessM4TGenerationOutput, Tuple[Tensor], ModelOutput]`'
  prefs: []
  type: TYPE_NORMAL
- en: If `generate_speech` and `return_intermediate_token_ids`, returns `SeamlessM4TGenerationOutput`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `generate_speech` and not `return_intermediate_token_ids`, returns a tuple
    composed of waveforms of shape `(batch_size, sequence_length)`and and `waveform_lengths`
    which gives the length of each sample.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `generate_speech=False`, it will returns `ModelOutput`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates translated token ids and/or translated audio waveforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'This method successively calls the `.generate` function of two different sub-models.
    You can specify keyword arguments at two different levels: general arguments that
    will be passed to both models, or prefixed arguments that will be passed to one
    of them.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, calling `.generate(input_ids=input_ids, num_beams=4, speech_do_sample=True)`
    will successively perform beam-search decoding on the text model, and multinomial
    beam-search sampling on the speech model.
  prefs: []
  type: TYPE_NORMAL
- en: For an overview of generation strategies and code examples, check out the [following
    guide](./generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: SeamlessM4TForTextToSpeech
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SeamlessM4TForTextToSpeech`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3214)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The text-to-speech SeamlessM4T Model transformer which can be used for T2ST.
    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3366)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [SeamlessM4TTokenizer](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer)
    or [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_intermediate_token_ids` (`bool`, *optional*) — If `True`, also returns
    the intermediate generated text and unit tokens. Set to `True` if you also want
    to get translated text alongside the audio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tgt_lang` (`str`, *optional*) — The language to use as target language for
    translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spkr_id` (`int`, *optional*, defaults to 0) — The id of the speaker used for
    speech synthesis. Must be lower than `config.vocoder_num_spkrs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments that will
    be passed to [GenerationMixin.generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate).
    Keyword arguments are of two types:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without a prefix, they will be entered as `**kwargs` for the `generate` method
    of each sub-model, except for `decoder_input_ids` which will only be passed through
    the text components.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With a *text_* or *speech_* prefix, they will be input for the `generate` method
    of the text model and speech model respectively. It has the priority over the
    keywords without a prefix.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This means you can, for example, specify a generation strategy for one generation
    but not for the other.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]`'
  prefs: []
  type: TYPE_NORMAL
- en: If `return_intermediate_token_ids`, returns `SeamlessM4TGenerationOutput`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If not `return_intermediate_token_ids`, returns a tuple composed of waveforms
    of shape `(batch_size, sequence_length)`and and `waveform_lengths` which gives
    the length of each sample.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates translated audio waveforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'This method successively calls the `.generate` function of two different sub-models.
    You can specify keyword arguments at two different levels: general arguments that
    will be passed to both models, or prefixed arguments that will be passed to one
    of them.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, calling `.generate(input_ids, num_beams=4, speech_do_sample=True)`
    will successively perform beam-search decoding on the text model, and multinomial
    beam-search sampling on the speech model.
  prefs: []
  type: TYPE_NORMAL
- en: For an overview of generation strategies and code examples, check out the [following
    guide](./generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: SeamlessM4TForSpeechToSpeech
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SeamlessM4TForSpeechToSpeech`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3566)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The speech-to-speech SeamlessM4T Model transformer which can be used for S2ST.
    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3721)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_banks)`) — Input audio features. This should be returnes by the [SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)
    class or the [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)
    class. See [SeamlessM4TFeatureExtractor.`call`()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_intermediate_token_ids` (`bool`, *optional*) — If `True`, also returns
    the intermediate generated text and unit tokens. Set to `True` if you also want
    to get translated text alongside the audio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tgt_lang` (`str`, *optional*) — The language to use as target language for
    translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spkr_id` (`int`, *optional*, defaults to 0) — The id of the speaker used for
    speech synthesis. Must be lower than `config.vocoder_num_spkrs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments that will
    be passed to [GenerationMixin.generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate).
    Keyword arguments are of two types:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without a prefix, they will be entered as `**kwargs` for the `generate` method
    of each sub-model, except for `decoder_input_ids` which will only be passed through
    the text components.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With a *text_* or *speech_* prefix, they will be input for the `generate` method
    of the text model and speech model respectively. It has the priority over the
    keywords without a prefix.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This means you can, for example, specify a generation strategy for one generation
    but not for the other.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]`'
  prefs: []
  type: TYPE_NORMAL
- en: If `return_intermediate_token_ids`, returns `SeamlessM4TGenerationOutput`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If not `return_intermediate_token_ids`, returns a tuple composed of waveforms
    of shape `(batch_size, sequence_length)`and and `waveform_lengths` which gives
    the length of each sample.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates translated audio waveforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'This method successively calls the `.generate` function of two different sub-models.
    You can specify keyword arguments at two different levels: general arguments that
    will be passed to both models, or prefixed arguments that will be passed to one
    of them.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, calling `.generate(input_features, num_beams=4, speech_do_sample=True)`
    will successively perform beam-search decoding on the text model, and multinomial
    beam-search sampling on the speech model.
  prefs: []
  type: TYPE_NORMAL
- en: For an overview of generation strategies and code examples, check out the [following
    guide](./generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: SeamlessM4TForTextToText
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SeamlessM4TForTextToText`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2637)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The text-to-text SeamlessM4T Model transformer which can be used for T2TT. This
    model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2689)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [SeamlessM4TTokenizer](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer)
    or [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation.
    If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For translation and summarization training, `decoder_input_ids` should be provided.
    If no `decoder_input_ids` is provided, the model will create this tensor by shifting
    the `input_ids` to the right for denoising pre-training following the paper.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `modeling_bart._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape`(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds`
    takes the value of `inputs_embeds`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should be in
    `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices
    set to `-100` are ignored (masked), the loss is only computed for the tokens with
    labels in `[0, ..., config.vocab_size]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [SeamlessM4TForTextToText](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToText)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2781)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.Tensor` of varying shape depending on the modality, *optional*)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [SeamlessM4TTokenizer](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer)
    or [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`tgt_lang` (`str`, *optional*) — The language to use as target language for
    translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — Custom logits processors
    that complement the default logits processors built from arguments and generation
    config. If a logit processor is passed that is already created with the arguments
    or a generation config an error is thrown. This feature is intended for advanced
    users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — Custom stopping
    criteria that complement the default stopping criteria built from arguments and
    a generation config. If a stopping criteria is passed that is already created
    with the arguments or a generation config an error is thrown. This feature is
    intended for advanced users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`, *optional*)
    — If provided, this function constraints the beam search to allowed tokens only
    at each step. If not provided no constraint is applied. This function takes 2
    arguments: the batch ID `batch_id` and `input_ids`. It has to return a list with
    the allowed tokens for the next generation step conditioned on the batch ID `batch_id`
    and the previously generated tokens `inputs_ids`. This argument is useful for
    constrained generation conditioned on the prefix, as described in [Autoregressive
    Entity Retrieval](https://arxiv.org/abs/2010.00904).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Ad hoc parametrization of `generate_config`
    and/or additional model-specific kwargs that will be forwarded to the `forward`
    function of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    or `torch.LongTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    (if `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`)
    or a `torch.FloatTensor`. The possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates sequences of token ids.
  prefs: []
  type: TYPE_NORMAL
- en: Most generation-controlling parameters are set in `generation_config` which,
    if not passed, will be set to the model’s default generation configuration. You
    can override any `generation_config` by passing the corresponding parameters to
    generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.
  prefs: []
  type: TYPE_NORMAL
- en: For an overview of generation strategies and code examples, check out the [following
    guide](./generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: SeamlessM4TForSpeechToText
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SeamlessM4TForSpeechToText`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2924)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The speech-to-text SeamlessM4T Model transformer which can be used for S2TT.
    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2971)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_banks)`) — Input audio features. This should be returnes by the [SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)
    class or the [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)
    class. See [SeamlessM4TFeatureExtractor.`call`()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation.
    If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For translation and summarization training, `decoder_input_ids` should be provided.
    If no `decoder_input_ids` is provided, the model will create this tensor by shifting
    the `input_ids` to the right for denoising pre-training following the paper.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `modeling_bart._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape`(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds`
    takes the value of `inputs_embeds`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should be in
    `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices
    set to `-100` are ignored (masked), the loss is only computed for the tokens with
    labels in `[0, ..., config.vocab_size]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [SeamlessM4TForSpeechToText](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToText)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L3070)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_banks)`) — Input audio features. This should be returnes by the [SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)
    class or the [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)
    class. See [SeamlessM4TFeatureExtractor.`call`()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tgt_lang` (`str`, *optional*) — The language to use as target language for
    translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — Custom logits processors
    that complement the default logits processors built from arguments and generation
    config. If a logit processor is passed that is already created with the arguments
    or a generation config an error is thrown. This feature is intended for advanced
    users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — Custom stopping
    criteria that complement the default stopping criteria built from arguments and
    a generation config. If a stopping criteria is passed that is already created
    with the arguments or a generation config an error is thrown. This feature is
    intended for advanced users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`, *optional*)
    — If provided, this function constraints the beam search to allowed tokens only
    at each step. If not provided no constraint is applied. This function takes 2
    arguments: the batch ID `batch_id` and `input_ids`. It has to return a list with
    the allowed tokens for the next generation step conditioned on the batch ID `batch_id`
    and the previously generated tokens `inputs_ids`. This argument is useful for
    constrained generation conditioned on the prefix, as described in [Autoregressive
    Entity Retrieval](https://arxiv.org/abs/2010.00904).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Ad hoc parametrization of `generate_config`
    and/or additional model-specific kwargs that will be forwarded to the `forward`
    function of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    or `torch.LongTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    (if `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`)
    or a `torch.FloatTensor`. The possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates sequences of token ids.
  prefs: []
  type: TYPE_NORMAL
- en: Most generation-controlling parameters are set in `generation_config` which,
    if not passed, will be set to the model’s default generation configuration. You
    can override any `generation_config` by passing the corresponding parameters to
    generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.
  prefs: []
  type: TYPE_NORMAL
- en: For an overview of generation strategies and code examples, check out the [following
    guide](./generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: SeamlessM4TConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SeamlessM4TConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/configuration_seamless_m4t.py#L29)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 256102) — Vocabulary size of the
    SeamlessM4T model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [~SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel),
    [~SeamlessM4TForTextToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToSpeech)
    or [~SeamlessM4TForTextToText](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToText).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t2u_vocab_size` (`int`, *optional*, defaults to 10082) — Unit vocabulary size
    of the SeamlessM4T model. Defines the number of different unit tokens that can
    be represented by the `inputs_ids` passed when calling the Text-To-Units sub-model
    of [~SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel),
    [~SeamlessM4TForSpeechToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToSpeech)
    or [~SeamlessM4TForTextToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToSpeech).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameters shared across sub-models
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 1024) — Dimensionality of the
    “intermediate” layers in the architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 1024) — The maximum
    sequence length that this model text encoder and decoder might ever be used with.
    Typically set this to something large just in case (e.g., 512 or 1024 or 2048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_encoder_decoder` (`bool`, *optional*, defaults to `True`) — Whether the
    model is used as an encoder/decoder or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.05) — The LayerDrop
    probability for the encoders. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.05) — The LayerDrop
    probability for the decoders. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation_function` (`str` or `function`, *optional*, defaults to `"relu"`)
    — The non-linear activation function (function or string) in the decoder and feed-forward
    layers. If string, `"gelu"`, `"relu"`, `"selu"`, `"swish"` and `"gelu_new"` are
    supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, decoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all attention layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for all activation layers in the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scale_embedding` (`bool`, *optional*, defaults to `True`) — Scale embeddings
    by diving by sqrt(d_model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text encoder and text decoder specific parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`encoder_layers` (`int`, *optional*, defaults to 24) — Number of hidden layers
    in the Transformer text encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Dimension of the
    “intermediate” (i.e., feed-forward) layer in the Transformer text encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_attention_heads` (`int`, *optional*, defaults to 16) — Number of attention
    heads for each attention layer in the Transformer text encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_layers` (`int`, *optional*, defaults to 24) — Number of hidden layers
    in the Transformer text decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Dimension of the
    “intermediate” (i.e., feed-forward) layer in the Transformer text decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_attention_heads` (`int`, *optional*, defaults to 16) — Number of attention
    heads for each attention layer in the Transformer text decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_start_token_id` (`int`, *optional*, defaults to 3) — If an encoder-decoder
    model starts decoding with a different token than *bos*, the id of that token.
    Only applied in the text decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_new_tokens` (`int`, *optional*, defaults to 256) — The maximum numbers
    of text tokens to generate, ignoring the number of tokens in the prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*, defaults to 0) — The id of the *padding*
    text token. Only applied to the text-decoder model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token_id` (`int`, *optional*, defaults to 2) — The id of the *beginning-of-stream*
    text token. Only applied to the text-decoder model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`int`, *optional*, defaults to 3) — The id of the *end-of-stream*
    text token. Only applied to the text-decoder model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech encoder specific parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`speech_encoder_layers` (`int`, *optional*, defaults to 24) — Number of hidden
    layers in the Transformer speech encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_encoder_attention_heads` (`int`, *optional*, defaults to 16) — Number
    of attention heads for each attention layer in the Transformer speech encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_encoder_intermediate_size` (`int`, *optional*, defaults to 4096) —
    Dimension of the “intermediate” (i.e., feed-forward) layer in the Transformer
    speech encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_encoder_hidden_act` (`str` or `function`, *optional*, defaults to `"swish"`)
    — The non-linear activation function (function or string) in the speech encoder.
    If string, `"gelu"`, `"relu"`, `"selu"`, `"swish"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_encoder_dropout` (`float`, *optional*, defaults to 0.0) — The dropout
    probability for all layers in the speech encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_adapter` (`bool`, *optional*, defaults to `True`) — Add an adapter layer
    on top of the speech encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_encoder_layerdrop` (`float`, *optional*, defaults to 0.1) — The LayerDrop
    probability for the speech encoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature_projection_input_dim` (`int`, *optional*, defaults to 160) — Input
    dimension of the input feature projection of the speech encoder, i.e the dimension
    after processing input audios with [SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — Number of
    convolutional positional embeddings. Defines the kernel size of 1D convolutional
    positional embeddings layer of the speech encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — Number
    of groups of 1D convolutional positional embeddings layer of the speech encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adaptor_kernel_size` (`int`, *optional*, defaults to 8) — Kernel size of the
    convolutional layers in the adapter network. Only relevant if `add_adapter is
    True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adaptor_stride` (`int`, *optional*, defaults to 8) — Stride of the convolutional
    layers in the adapter network. Only relevant if `add_adapter is True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adaptor_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all layers in the speech adapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_adapter_layers` (`int`, *optional*, defaults to 1) — Number of convolutional
    layers that should be used in the adapter network. Only relevant if `add_adapter
    is True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`position_embeddings_type` (`str`, *optional*, defaults to `"relative"`) —
    Can be specified to `relative` or `rotary` for relative or rotary position embeddings
    respectively. If left `None` no relative position embedding is applied. Only applied
    to the speech encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rotary_embedding_base` (`int`, *optional*, defaults to 10000) — If `"rotary"`
    position embeddings are used, defines the size of the embedding base. Only applied
    to the speech encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_source_positions` (`int`, *optional*, defaults to 4096) — if `"relative"`
    position embeddings are used, defines the maximum source input positions. Only
    applied to the speech encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conv_depthwise_kernel_size` (`int`, *optional*, defaults to 31) — Kernel size
    of convolutional depthwise 1D layer in Conformer blocks. Only applied to the speech
    encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text-To-Unit (t2u) model specific parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`t2u_bos_token_id` (`int`, *optional*, defaults to 0) — The id of the *beginning-of-stream*
    unit token. Only applied to the text-to-unit seq2seq model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t2u_pad_token_id` (`int`, *optional*, defaults to 1) — The id of the *padding*
    unit token. Only applied to the text-to-unit seq2seq model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t2u_eos_token_id` (`int`, *optional*, defaults to 2) — The id of the *end-of-stream*
    unit token. Only applied to the text-to-unit seq2seq model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t2u_decoder_start_token_id` (`int`, *optional*, defaults to 2) — If an encoder-decoder
    model starts decoding with a different token than *bos*, the id of that token.
    Only applied to the text-to-unit seq2seq model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t2u_max_new_tokens` (`int`, *optional*, defaults to 1024) — The maximum numbers
    of unit tokens to generate, ignoring the number of tokens in the prompt. Only
    applied to the text-to-unit seq2seq model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t2u_encoder_layers` (`int`, *optional*, defaults to 6) — Number of hidden
    layers in the Transformer text-to-unit encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t2u_encoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Dimension of
    the “intermediate” (i.e., feed-forward) layer in the Transformer text-to-unit
    encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t2u_encoder_attention_heads` (`int`, *optional*, defaults to 16) — Number
    of attention heads for each attention layer in the Transformer text-to-unit encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t2u_decoder_layers` (`int`, *optional*, defaults to 6) — Number of hidden
    layers in the Transformer text-to-unit decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t2u_decoder_ffn_dim` (`int`, *optional*, defaults to 8192) — Dimension of
    the “intermediate” (i.e., feed-forward) layer in the Transformer text-to-unit
    decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t2u_decoder_attention_heads` (`int`, *optional*, defaults to 16) — Number
    of attention heads for each attention layer in the Transformer text-to-unit decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t2u_max_position_embeddings` (`int`, *optional*, defaults to 2048) — The maximum
    sequence length that this model text-to-unit component might ever be used with.
    Typically set this to something large just in case (e.g., 512 or 1024 or 2048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hifi-Gan Vocoder specific parameters
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*, defaults to 16000) — The sampling rate
    at which the output audio will be generated, expressed in hertz (Hz).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`upsample_initial_channel` (`int`, *optional*, defaults to 512) — The number
    of input channels into the hifi-gan upsampling network. Applies to the vocoder
    only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`upsample_rates` (`Tuple[int]` or `List[int]`, *optional*, defaults to `[5,
    4, 4, 2, 2]`) — A tuple of integers defining the stride of each 1D convolutional
    layer in the vocoder upsampling network. The length of *upsample_rates* defines
    the number of convolutional layers and has to match the length of *upsample_kernel_sizes*.
    Applies to the vocoder only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`upsample_kernel_sizes` (`Tuple[int]` or `List[int]`, *optional*, defaults
    to `[11, 8, 8, 4, 4]`) — A tuple of integers defining the kernel size of each
    1D convolutional layer in the vocoder upsampling network. The length of *upsample_kernel_sizes*
    defines the number of convolutional layers and has to match the length of *upsample_rates*.
    Applies to the vocoder only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resblock_kernel_sizes` (`Tuple[int]` or `List[int]`, *optional*, defaults
    to `[3, 7, 11]`) — A tuple of integers defining the kernel sizes of the vocoder
    1D convolutional layers in the multi-receptive field fusion (MRF) module. Applies
    to the vocoder only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resblock_dilation_sizes` (`Tuple[Tuple[int]]` or `List[List[int]]`, *optional*,
    defaults to `[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`) — A nested tuple of integers
    defining the dilation rates of the vocoder dilated 1D convolutional layers in
    the multi-receptive field fusion (MRF) module. Applies to the vocoder only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`leaky_relu_slope` (`float`, *optional*, defaults to 0.1) — The angle of the
    negative slope used by the leaky ReLU activation in the vocoder. Applies to the
    vocoder only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unit_hifi_gan_vocab_size` (`int`, *optional*, defaults to 10000) — Vocabulary
    size of the SeamlessM4T vocoder. Defines the number of different unit tokens that
    can be represented by the `inputs_ids` passed when calling the vocoder of [~SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel),
    [~SeamlessM4TForSpeechToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToSpeech)
    or [~SeamlessM4TForTextToSpeech](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToSpeech).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unit_embed_dim` (`int`, *optional*, defaults to 1280) — The projection dimension
    of the input ids given to the hifi-gan vocoder. Applies to the vocoder only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lang_embed_dim` (`int`, *optional*, defaults to 256) — The projection dimension
    of the target language given to the hifi-gan vocoder. Applies to the vocoder only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spkr_embed_dim` (`int`, *optional*, defaults to 256) — The projection dimension
    of the speaker id given to the hifi-gan vocoder. Applies to the vocoder only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocoder_num_langs` (`int`, *optional*, defaults to 36) — Number of langs supported
    by the vocoder. Might be different from `t2u_num_langs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocoder_num_spkrs` (`int`, *optional*, defaults to 200) — Number of speakers
    supported by the vocoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`variance_predictor_kernel_size` (`int`, *optional*, defaults to 3) — Kernel
    size of the duration predictor. Applies to the vocoder only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`var_pred_dropout` (`float`, *optional*, defaults to 0.5) — The dropout probabilitiy
    of the duration predictor. Applies to the vocoder only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocoder_offset` (`int`, *optional*, defaults to 4) — Offset the unit token
    ids by this number to account for symbol tokens. Applies to the vocoder only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [~SeamlessM4TModel](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel).
    It is used to instantiate an SeamlessM4T model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the SeamlessM4T [“facebook/hf-seamless-m4t-medium”](https://huggingface.co/%22facebook/hf-seamless-m4t-medium%22)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: SeamlessM4TTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SeamlessM4TTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L54)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer_file` (`str`, *optional*) — The path to a tokenizer file to use
    instead of the vocab file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`src_lang` (`str`, *optional*, defaults to `"eng"`) — The language to use as
    source language for translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tgt_lang` (`str`, *optional*, defaults to `"fra"`) — The language to use as
    target language for translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sp_model_kwargs` (`Dict[str, Any]`, *optional*) — Additional keyword arguments
    to pass to the model initialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`additional_special_tokens` (tuple or list of `str` or `tokenizers.AddedToken`,
    *optional*) — A tuple or a list of additional special tokens. Can be used to specify
    the list of languages that will be supported by the tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a SeamlessM4T tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: Adapted from [RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)
    and [XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer).
    Based on [SentencePiece](https://github.com/google/sentencepiece).
  prefs: []
  type: TYPE_NORMAL
- en: The tokenization method is `<language code> <tokens> <eos>` for source language
    documents, and `<eos> <language code> <tokens> <eos>` for target language documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L217)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence or
    batch of sequences to be encoded. Each sequence can be a string or a list of strings
    (pretokenized string). If the sequences are provided as list of strings (pretokenized),
    you must set `is_split_into_words=True` (to lift the ambiguity with a batch of
    sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded. Each sequence can be a string or a list of
    strings (pretokenized string). If the sequences are provided as list of strings
    (pretokenized), you must set `is_split_into_words=True` (to lift the ambiguity
    with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded as target texts. Each sequence can be a string
    or a list of strings (pretokenized string). If the sequences are provided as list
    of strings (pretokenized), you must set `is_split_into_words=True` (to lift the
    ambiguity with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The
    sequence or batch of sequences to be encoded as target texts. Each sequence can
    be a string or a list of strings (pretokenized string). If the sequences are provided
    as list of strings (pretokenized), you must set `is_split_into_words=True` (to
    lift the ambiguity with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `True`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`src_lang` (`str`, *optional*) — A string representing the source language.
    If not specified, the last `src_lang` specified (either during initialization
    or when calling this tokenizer) will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tgt_lang` (`str`, *optional*) — A string representing the target language.
    If not specified, the last `tgt_lang` specified (either during initialization
    or when calling this tokenizer) will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments that will
    be passed to [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `build_inputs_with_special_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L347)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. An NLLB sequence has the following
    format, where `X` represents the sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (for encoder) `X [eos, src_lang_code]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_input_ids`: (for decoder) `X [eos, tgt_lang_code]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BOS is never used. Pairs of sequences are not the expected use case, but they
    will be handled without a separator.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_special_tokens_mask`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L316)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `create_token_type_ids_from_sequences`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L375)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of zeros.
  prefs: []
  type: TYPE_NORMAL
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. nllb does not make use of token type ids, therefore a list of zeros is returned.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_vocabulary`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py#L498)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: SeamlessM4TTokenizerFast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SeamlessM4TTokenizerFast`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t_fast.py#L54)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`, *optional*) — Path to the vocabulary file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer_file` (`str`, *optional*) — The path to a tokenizer file to use
    instead of the vocab file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`src_lang` (`str`, *optional*, defaults to `"eng"`) — The language to use as
    source language for translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tgt_lang` (`str`, *optional*, defaults to `"fra"`) — The language to use as
    target language for translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`additional_special_tokens` (tuple or list of `str` or `tokenizers.AddedToken`,
    *optional*) — A tuple or a list of additional special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a “fast” SeamlessM4T tokenizer (backed by HuggingFace’s *tokenizers*
    library). Based on [BPE](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models).
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: The tokenization method is `<language code> <tokens> <eos>` for source language
    documents, and `<eos> <language code> <tokens> <eos>` for target language documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/tokenization_seamless_m4t_fast.py#L390)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence or
    batch of sequences to be encoded. Each sequence can be a string or a list of strings
    (pretokenized string). If the sequences are provided as list of strings (pretokenized),
    you must set `is_split_into_words=True` (to lift the ambiguity with a batch of
    sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded. Each sequence can be a string or a list of
    strings (pretokenized string). If the sequences are provided as list of strings
    (pretokenized), you must set `is_split_into_words=True` (to lift the ambiguity
    with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded as target texts. Each sequence can be a string
    or a list of strings (pretokenized string). If the sequences are provided as list
    of strings (pretokenized), you must set `is_split_into_words=True` (to lift the
    ambiguity with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The
    sequence or batch of sequences to be encoded as target texts. Each sequence can
    be a string or a list of strings (pretokenized string). If the sequences are provided
    as list of strings (pretokenized), you must set `is_split_into_words=True` (to
    lift the ambiguity with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `True`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`src_lang` (`str`, *optional*) — A string representing the source language.
    If not specified, the last `src_lang` specified (either during initialization
    or when calling this tokenizer) will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tgt_lang` (`str`, *optional*) — A string representing the target language.
    If not specified, the last `tgt_lang` specified (either during initialization
    or when calling this tokenizer) will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments that will
    be passed to [PreTrainedTokenizerFast.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SeamlessM4TFeatureExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SeamlessM4TFeatureExtractor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/feature_extraction_seamless_m4t.py#L38)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_size` (`int`, *optional*, defaults to 80) — The feature dimension
    of the extracted features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*, defaults to 16000) — The sampling rate
    at which the audio files should be digitalized expressed in hertz (Hz).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_mel_bins` (`int`, *optional*, defaults to 80) — Number of Mel-frequency
    bins.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding_value` (`float`, *optional*, defaults to 0.0) — The value that is
    used to fill the padding vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stride` (`int`, *optional*, defaults to 2) — Stride used to reshape audios
    from shape (batch_size,num_frames,num_mel_bins) to (batch_size,num_frames//stride,num_mel_bins*stride).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a SeamlessM4T feature extractor.
  prefs: []
  type: TYPE_NORMAL
- en: This feature extractor inherits from [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: This class extracts mel-filter bank features from raw speech.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/feature_extraction_seamless_m4t.py#L144)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`raw_speech` (`np.ndarray`, `torch.Tensor`, `List[float]`, `List[np.ndarray]`,
    `List[torch.Tensor]`, —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`List[List[float]],` `List[List[List[float]]]`) — The sequence or batch of
    sequences to be padded. Each sequence can be a numpy array, a torch tensor, a
    list of float values, a list of numpy arrays, a list of torch tensors, a list
    of list of float values or a list of a list of list of float values. If `raw_speech`
    is a one-dimensional `np.ndarray`, `torch.Tensor` or a `List[float]`, `raw_speech`
    is considered a single-channel, single-sample sound. In all other cases, the first
    dimension of `raw_speech`, whether from an `np.ndarray`, a `torch.Tensor` or a
    `List[...]`, corresponds to the number of samples in the batch, and the number
    of channels (i.e. mono or stereo character) is derived from the other dimensions
    (1D -> single-channel waveform batches; 2D-> stereo-channel waveform batches).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `True`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*, defaults to 2) — If set will pad the
    sequence to a multiple of the provided value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta), or on TPUs which benefit from having
    sequence lengths be a multiple of 128.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — Maximum length of the returned list and
    optionally padding length (see above).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`bool`) — Activates truncation to cut input sequences longer
    than *max_length* to *max_length*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific feature_extractor’s default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For SeamlessM4T models, `attention_mask` should always be passed for batched
    inference, to avoid subtle bugs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*) — The sampling rate at which the `raw_speech`
    input was sampled. It is strongly recommended to pass `sampling_rate` at the forward
    call to prevent silent errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_normalize_per_mel_bins` (`bool`, *optional*, defaults to `True`) — Whether
    or not to zero-mean unit-variance normalize the input per mel-channel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments that will
    be passed to the tokenizer or the feature extractor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to featurize and prepare for the model one or several sequence(s).
  prefs: []
  type: TYPE_NORMAL
- en: SeamlessM4TProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SeamlessM4TProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/processing_seamless_m4t.py#L22)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_extractor` ([SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor))
    — The audio processor is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([SeamlessM4TTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizerFast))
    — The tokenizer is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a SeamlessM4T processor which wraps a SeamlessM4T feature extractor
    and a SeamlessM4T tokenizer into a single processor.
  prefs: []
  type: TYPE_NORMAL
- en: '[SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor)
    offers all the functionalities of [SeamlessM4TFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor)
    and [SeamlessM4TTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizerFast).
    See the [**call**()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor.__call__)
    and `decode()` for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/processing_seamless_m4t.py#L44)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text` (`str`, `List[str]`, `List[List[str]]`) — The sequence or batch of sequences
    to be encoded. Each sequence can be a string or a list of strings (pretokenized
    string). If the sequences are provided as list of strings (pretokenized), you
    must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audios` (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`)
    — The audio or batch of audios to be prepared. Each audio can be NumPy array or
    PyTorch tensor. In case of a NumPy array/PyTorch tensor, each audio should be
    of shape (C, T), where C is a number of channels, and T the sample length of the
    audio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`src_lang` (`str`, *optional*) — The language code of the input texts/audios.
    If not specified, the last `src_lang` specified will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tgt_lang` (`str`, *optional*) — The code of the target language. If not specified,
    the last `tgt_lang` specified will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (*optional*) — Remaining dictionary of keyword arguments that will
    be passed to the feature extractor and/or the tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` — List of token ids to be fed to a model. Returned when `text`
    is not `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names` and if `text` is not `None`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_features` — Audio input features to be fed to a model. Returned when
    `audios` is not `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to prepare for the model one or several sequences(s) and audio(s).
    This method forwards the `text` and `kwargs` arguments to SeamlessM4TTokenizerFast’s
    [**call**()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizerFast.__call__)
    if `text` is not `None` to encode the text. To prepare the audio(s), this method
    forwards the `audios` and `kwrags` arguments to SeamlessM4TFeatureExtractor’s
    [**call**()](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor.__call__)
    if `audios` is not `None`. Please refer to the doctsring of the above two methods
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: SeamlessM4TCodeHifiGan
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SeamlessM4TCodeHifiGan`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2477)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code HiFi-GAN vocoder as described in this [repository](https://github.com/facebookresearch/speech-resynthesis).
    This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2557)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [SeamlessM4TTextToUnitForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTextToUnitForConditionalGeneration).
    [What are input IDs?](../glossary#input-ids)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`spkr_id` (`int`, *optional*) — The id of the speaker used for speech synthesis.
    Must be lower than `config.vocoder_num_spkrs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tgt_lang` (`str`, *optional*) — The language id to use as target language
    for translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SeamlessM4THifiGan
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SeamlessM4THifiGan`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2405)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2440)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`spectrogram` (`torch.FloatTensor`) — Tensor containing the log-mel spectrograms.
    Can be batched and of shape `(batch_size, sequence_length, model_in_dim)`, or
    un-batched and of shape `(sequence_length, model_in_dim)`. Note that `model_in_dim`
    is the sum of `config.unit_embed_dim`, `config.lang_embed_dim` and `config.spkr_embed_dim`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: Tensor containing the speech waveform. If the input spectrogram is batched,
    will be of shape `(batch_size, num_frames,)`. If un-batched, will be of shape
    `(num_frames,)`.
  prefs: []
  type: TYPE_NORMAL
- en: Converts a log-mel spectrogram into a speech waveform. Passing a batch of log-mel
    spectrograms returns a batch of speech waveforms. Passing a single, un-batched
    log-mel spectrogram returns a single, un-batched speech waveform.
  prefs: []
  type: TYPE_NORMAL
- en: SeamlessM4TTextToUnitModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SeamlessM4TTextToUnitModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2039)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embed_tokens_decoder` (`nn.Embedding`, *optional*) — input embedding of the
    decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer bare text-to-unit encoder-decoder. The encoder is a `SeamlessM4TEncoder`
    without embeddings and the decoder is a `SeamlessM4TDecoder`. This model is a
    PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: SeamlessM4TTextToUnitForConditionalGeneration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SeamlessM4TTextToUnitForConditionalGeneration`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2128)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([~SeamlessM4TConfig](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embed_tokens_decoder` (`nn.Embedding`, *optional*) — input embedding of the
    decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer text-to-unit encoder-decoder with a language model head. The base
    encoder-decoder model is a `SeamlessM4TTextToUnit`. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py#L2181)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [SeamlessM4TTokenizer](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer)
    or [SeamlessM4TProcessor](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation.
    If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For translation and summarization training, `decoder_input_ids` should be provided.
    If no `decoder_input_ids` is provided, the model will create this tensor by shifting
    the `input_ids` to the right for denoising pre-training following the paper.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `modeling_bart._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape`(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds`
    takes the value of `inputs_embeds`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should be in
    `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices
    set to `-100` are ignored (masked), the loss is only computed for the tokens with
    labels in `[0, ..., config.vocab_size]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [SeamlessM4TTextToUnitForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/seamless_m4t#transformers.SeamlessM4TTextToUnitForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
