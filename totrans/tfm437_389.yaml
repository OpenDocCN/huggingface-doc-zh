- en: Graphormer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/graphormer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/graphormer)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/147.9dec8615.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Graphormer model was proposed in [Do Transformers Really Perform Bad for
    Graph Representation?](https://arxiv.org/abs/2106.05234) by Chengxuan Ying, Tianle
    Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen and Tie-Yan Liu.
    It is a Graph Transformer model, modified to allow computations on graphs instead
    of text sequences by generating embeddings and features of interest during preprocessing
    and collation, then using a modified attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Transformer architecture has become a dominant choice in many domains,
    such as natural language processing and computer vision. Yet, it has not achieved
    competitive performance on popular leaderboards of graph-level prediction compared
    to mainstream GNN variants. Therefore, it remains a mystery how Transformers could
    perform well for graph representation learning. In this paper, we solve this mystery
    by presenting Graphormer, which is built upon the standard Transformer architecture,
    and could attain excellent results on a broad range of graph representation learning
    tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to
    utilizing Transformer in the graph is the necessity of effectively encoding the
    structural information of a graph into the model. To this end, we propose several
    simple yet effective structural encoding methods to help Graphormer better model
    graph-structured data. Besides, we mathematically characterize the expressive
    power of Graphormer and exhibit that with our ways of encoding the structural
    information of graphs, many popular GNN variants could be covered as the special
    cases of Graphormer.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [clefourrier](https://huggingface.co/clefourrier).
    The original code can be found [here](https://github.com/microsoft/Graphormer).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This model will not work well on large graphs (more than 100 nodes/edges), as
    it will make the memory explode. You can reduce the batch size, increase your
    RAM, or decrease the `UNREACHABLE_NODE_DISTANCE` parameter in algos_graphormer.pyx,
    but it will be hard to go above 700 nodes/edges.
  prefs: []
  type: TYPE_NORMAL
- en: This model does not use a tokenizer, but instead a special collator during training.
  prefs: []
  type: TYPE_NORMAL
- en: GraphormerConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.GraphormerConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/configuration_graphormer.py#L30)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`num_classes` (`int`, *optional*, defaults to 1) — Number of target classes
    or labels, set to n for binary classification of n tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_atoms` (`int`, *optional*, defaults to 512*9) — Number of node types in
    the graphs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_edges` (`int`, *optional*, defaults to 512*3) — Number of edges types
    in the graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_in_degree` (`int`, *optional*, defaults to 512) — Number of in degrees
    types in the input graphs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_out_degree` (`int`, *optional*, defaults to 512) — Number of out degrees
    types in the input graphs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_edge_dis` (`int`, *optional*, defaults to 128) — Number of edge dis in
    the input graphs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multi_hop_max_dist` (`int`, *optional*, defaults to 20) — Maximum distance
    of multi hop edges between two nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spatial_pos_max` (`int`, *optional*, defaults to 1024) — Maximum distance
    between nodes in the graph attention bias matrices, used during preprocessing
    and collation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`edge_type` (`str`, *optional*, defaults to multihop) — Type of edge relation
    chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_nodes` (`int`, *optional*, defaults to 512) — Maximum number of nodes
    which can be parsed for the input graphs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`share_input_output_embed` (`bool`, *optional*, defaults to `False`) — Shares
    the embedding layer between encoder and decoder - careful, True is not implemented.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_layers` (`int`, *optional*, defaults to 12) — Number of layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embedding_dim` (`int`, *optional*, defaults to 768) — Dimension of the embedding
    layer in encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ffn_embedding_dim` (`int`, *optional*, defaults to 768) — Dimension of the
    “intermediate” (often named feed-forward) layer in encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 32) — Number of attention
    heads in the encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self_attention` (`bool`, *optional*, defaults to `True`) — Model is self attentive
    (False not implemented).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`)
    — The non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the attention weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the activation of the linear transformer layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layerdrop` (`float`, *optional*, defaults to 0.0) — The LayerDrop probability
    for the encoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias` (`bool`, *optional*, defaults to `True`) — Uses bias in the attention
    module - unsupported at the moment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embed_scale(float,` *optional*, defaults to None) — Scaling factor for the
    node embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_trans_layers_to_freeze` (`int`, *optional*, defaults to 0) — Number of
    transformer layers to freeze.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_normalize_before` (`bool`, *optional*, defaults to `False`) — Normalize
    features before encoding the graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pre_layernorm` (`bool`, *optional*, defaults to `False`) — Apply layernorm
    before self attention and the feed forward network. Without this, post layernorm
    will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`apply_graphormer_init` (`bool`, *optional*, defaults to `False`) — Apply a
    custom graphormer initialisation to the model before training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`freeze_embeddings` (`bool`, *optional*, defaults to `False`) — Freeze the
    embedding layer, or train it along the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_normalize_before` (`bool`, *optional*, defaults to `False`) — Apply
    the layer norm before each encoder block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`q_noise` (`float`, *optional*, defaults to 0.0) — Amount of quantization noise
    (see “Training with Quantization Noise for Extreme Model Compression”). (For more
    detail, see fairseq’s documentation on quant_noise).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qn_block_size` (`int`, *optional*, defaults to 8) — Size of the blocks for
    subsequent quantization with iPQ (see q_noise).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kdim` (`int`, *optional*, defaults to None) — Dimension of the key in the
    attention, if different from the other values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vdim` (`int`, *optional*, defaults to None) — Dimension of the value in the
    attention, if different from the other values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`traceable` (`bool`, *optional*, defaults to `False`) — Changes return value
    of the encoder’s inner_state to stacked tensors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example —
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [~GraphormerModel](/docs/transformers/v4.37.2/en/model_doc/graphormer#transformers.GraphormerModel).
    It is used to instantiate an Graphormer model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Graphormer [graphormer-base-pcqm4mv1](https://huggingface.co/graphormer-base-pcqm4mv1)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: GraphormerModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.GraphormerModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/modeling_graphormer.py#L775)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The Graphormer model is a graph-encoder model.
  prefs: []
  type: TYPE_NORMAL
- en: It goes from a graph to its representation. If you want to use the model for
    a downstream classification task, use GraphormerForGraphClassification instead.
    For any other downstream task, feel free to add a new class, or combine this model
    with a downstream model of your choice, following the example in GraphormerForGraphClassification.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/modeling_graphormer.py#L804)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: GraphormerForGraphClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.GraphormerForGraphClassification`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/modeling_graphormer.py#L846)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This model can be used for graph-level classification or regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: It can be trained on
  prefs: []
  type: TYPE_NORMAL
- en: regression (by setting config.num_classes to 1); there should be one float-type
    label per graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: one task classification (by setting config.num_classes to the number of classes);
    there should be one integer label per graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: binary multi-task classification (by setting config.num_classes to the number
    of labels); there should be a list of integer labels for each graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/modeling_graphormer.py#L869)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
