- en: MGP-STR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mgp-str](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mgp-str)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The MGP-STR model was proposed in [Multi-Granularity Prediction for Scene Text
    Recognition](https://arxiv.org/abs/2209.03592) by Peng Wang, Cheng Da, and Cong
    Yao. MGP-STR is a conceptually **simple** yet **powerful** vision Scene Text Recognition
    (STR) model, which is built upon the [Vision Transformer (ViT)](vit). To integrate
    linguistic knowledge, Multi-Granularity Prediction (MGP) strategy is proposed
    to inject information from the language modality into the model in an implicit
    way.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Scene text recognition (STR) has been an active research topic in computer
    vision for years. To tackle this challenging problem, numerous innovative methods
    have been successively proposed and incorporating linguistic knowledge into STR
    models has recently become a prominent trend. In this work, we first draw inspiration
    from the recent progress in Vision Transformer (ViT) to construct a conceptually
    simple yet powerful vision STR model, which is built upon ViT and outperforms
    previous state-of-the-art models for scene text recognition, including both pure
    vision models and language-augmented methods. To integrate linguistic knowledge,
    we further propose a Multi-Granularity Prediction strategy to inject information
    from the language modality into the model in an implicit way, i.e. , subword representations
    (BPE and WordPiece) widely-used in NLP are introduced into the output space, in
    addition to the conventional character level representation, while no independent
    language model (LM) is adopted. The resultant algorithm (termed MGP-STR) is able
    to push the performance envelop of STR to an even higher level. Specifically,
    it achieves an average recognition accuracy of 93.35% on standard benchmarks.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![drawing](../Images/f0c22cdfb9f67b7ee77ac9d0883362b6.png) MGP-STR architecture.
    Taken from the [original paper](https://arxiv.org/abs/2209.03592).'
  prefs: []
  type: TYPE_NORMAL
- en: MGP-STR is trained on two synthetic datasets [MJSynth]((http://www.robots.ox.ac.uk/~vgg/data/text/))
    (MJ) and SynthText([http://www.robots.ox.ac.uk/~vgg/data/scenetext/](http://www.robots.ox.ac.uk/~vgg/data/scenetext/))
    (ST) without fine-tuning on other datasets. It achieves state-of-the-art results
    on six standard Latin scene text benchmarks, including 3 regular text datasets
    (IC13, SVT, IIIT) and 3 irregular ones (IC15, SVTP, CUTE). This model was contributed
    by [yuekun](https://huggingface.co/yuekun). The original code can be found [here](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR).
  prefs: []
  type: TYPE_NORMAL
- en: Inference example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[MgpstrModel](/docs/transformers/v4.37.2/en/model_doc/mgp-str#transformers.MgpstrModel)
    accepts images as input and generates three types of predictions, which represent
    textual information at different granularities. The three types of predictions
    are fused to give the final prediction result.'
  prefs: []
  type: TYPE_NORMAL
- en: The [ViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor)
    class is responsible for preprocessing the input image and [MgpstrTokenizer](/docs/transformers/v4.37.2/en/model_doc/mgp-str#transformers.MgpstrTokenizer)
    decodes the generated character tokens to the target string. The [MgpstrProcessor](/docs/transformers/v4.37.2/en/model_doc/mgp-str#transformers.MgpstrProcessor)
    wraps [ViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor)
    and [MgpstrTokenizer](/docs/transformers/v4.37.2/en/model_doc/mgp-str#transformers.MgpstrTokenizer)
    into a single instance to both extract the input features and decode the predicted
    token ids.
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-step Optical Character Recognition (OCR)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: MgpstrConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.MgpstrConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mgp_str/configuration_mgp_str.py#L28)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`image_size` (`List[int]`, *optional*, defaults to `[32, 128]`) — The size
    (resolution) of each image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_size` (`int`, *optional*, defaults to 4) — The size (resolution) of
    each patch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_token_length` (`int`, *optional*, defaults to 27) — The max number of
    output tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_character_labels` (`int`, *optional*, defaults to 38) — The number of
    classes for character head .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_bpe_labels` (`int`, *optional*, defaults to 50257) — The number of classes
    for bpe head .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_wordpiece_labels` (`int`, *optional*, defaults to 30522) — The number
    of classes for wordpiece head .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — The embedding dimension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlp_ratio` (`float`, *optional*, defaults to 4.0) — The ratio of mlp hidden
    dim to embedding dim.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — Whether to add a bias
    to the queries, keys and values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distilled` (`bool`, *optional*, defaults to `False`) — Model includes a distillation
    token and head as in DeiT models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_rate` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for all fully connected layers in the embeddings, encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attn_drop_rate` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_path_rate` (`float`, *optional*, defaults to 0.0) — The stochastic depth
    rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_a3_attentions` (`bool`, *optional*, defaults to `False`) — Whether
    or not the model should returns A^3 module attentions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of an [MgpstrModel](/docs/transformers/v4.37.2/en/model_doc/mgp-str#transformers.MgpstrModel).
    It is used to instantiate an MGP-STR model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the MGP-STR [alibaba-damo/mgp-str-base](https://huggingface.co/alibaba-damo/mgp-str-base)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: MgpstrTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.MgpstrTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mgp_str/tokenization_mgp_str.py#L38)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"[GO]"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token` (`str`, *optional*, defaults to `"[GO]"`) — The beginning of sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token` (`str`, *optional*, defaults to `"[s]"`) — The end of sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token` (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"[GO]"`)
    — A special token used to make arrays of tokens the same size for batching purpose.
    Will then be ignored by attention mechanisms or loss computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a MGP-STR char tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_vocabulary`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mgp_str/tokenization_mgp_str.py#L100)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: MgpstrProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.MgpstrProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mgp_str/processing_mgp_str.py#L39)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`image_processor` (`ViTImageProcessor`, *optional*) — An instance of `ViTImageProcessor`.
    The image processor is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([MgpstrTokenizer](/docs/transformers/v4.37.2/en/model_doc/mgp-str#transformers.MgpstrTokenizer),
    *optional*) — The tokenizer is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a MGP-STR processor which wraps an image processor and MGP-STR tokenizers
    into a single
  prefs: []
  type: TYPE_NORMAL
- en: '[MgpstrProcessor](/docs/transformers/v4.37.2/en/model_doc/mgp-str#transformers.MgpstrProcessor)
    offers all the functionalities of `ViTImageProcessor`] and [MgpstrTokenizer](/docs/transformers/v4.37.2/en/model_doc/mgp-str#transformers.MgpstrTokenizer).
    See the [**call**()](/docs/transformers/v4.37.2/en/model_doc/mgp-str#transformers.MgpstrProcessor.__call__)
    and [batch_decode()](/docs/transformers/v4.37.2/en/model_doc/mgp-str#transformers.MgpstrProcessor.batch_decode)
    for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mgp_str/processing_mgp_str.py#L79)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: When used in normal mode, this method forwards all its arguments to ViTImageProcessor’s
    [**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    and returns its output. This method also forwards the `text` and `kwargs` arguments
    to MgpstrTokenizer’s [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    if `text` is not `None` to encode the text. Please refer to the doctsring of the
    above methods for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `batch_decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mgp_str/processing_mgp_str.py#L102)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`torch.Tensor`) — List of tokenized input ids.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict[str, any]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dictionary of all the outputs of the decoded results. generated_text (`List[str]`):
    The final results after fusion of char, bpe, and wp. scores (`List[float]`): The
    final scores after fusion of char, bpe, and wp. char_preds (`List[str]`): The
    list of character decoded sentences. bpe_preds (`List[str]`): The list of bpe
    decoded sentences. wp_preds (`List[str]`): The list of wp decoded sentences.'
  prefs: []
  type: TYPE_NORMAL
- en: Convert a list of lists of token ids into a list of strings by calling decode.
  prefs: []
  type: TYPE_NORMAL
- en: This method forwards all its arguments to PreTrainedTokenizer’s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  prefs: []
  type: TYPE_NORMAL
- en: MgpstrModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.MgpstrModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mgp_str/modeling_mgp_str.py#L364)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([MgpstrConfig](/docs/transformers/v4.37.2/en/model_doc/mgp-str#transformers.MgpstrConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare MGP-STR Model transformer outputting raw hidden-states without any
    specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mgp_str/modeling_mgp_str.py#L378)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [MgpstrModel](/docs/transformers/v4.37.2/en/model_doc/mgp-str#transformers.MgpstrModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: MgpstrForSceneTextRecognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.MgpstrForSceneTextRecognition`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mgp_str/modeling_mgp_str.py#L413)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([MgpstrConfig](/docs/transformers/v4.37.2/en/model_doc/mgp-str#transformers.MgpstrConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MGP-STR Model transformer with three classification heads on top (three A^3
    modules and three linear layer on top of the transformer encoder output) for scene
    text recognition (STR) .
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mgp_str/modeling_mgp_str.py#L438)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_a3_attentions` (`bool`, *optional*) — Whether or not to return the
    attentions tensors of a3 modules. See `a3_attentions` under returned tensors for
    more detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.mgp_str.modeling_mgp_str.MgpstrModelOutput` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.mgp_str.modeling_mgp_str.MgpstrModelOutput` or a tuple
    of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.mgp_str.configuration_mgp_str.MgpstrConfig'>`)
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`logits` (`tuple(torch.FloatTensor)` of shape `(batch_size, config.num_character_labels)`)
    — Tuple of `torch.FloatTensor` (one for the output of character of shape `(batch_size,
    config.max_token_length, config.num_character_labels)`, + one for the output of
    bpe of shape `(batch_size, config.max_token_length, config.num_bpe_labels)`, +
    one for the output of wordpiece of shape `(batch_size, config.max_token_length,
    config.num_wordpiece_labels)`) .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification scores (before SoftMax) of character, bpe and wordpiece.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, config.max_token_length, sequence_length,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`a3_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_a3_attentions=True`
    is passed or when `config.output_a3_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for the attention of character, + one for the attention of bpe`, + one for
    the attention of wordpiece) of shape` (batch_size, config.max_token_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [MgpstrForSceneTextRecognition](/docs/transformers/v4.37.2/en/model_doc/mgp-str#transformers.MgpstrForSceneTextRecognition)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
