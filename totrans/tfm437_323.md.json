["```py\n( vocab_size = 32 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout = 0.1 activation_dropout = 0.1 attention_dropout = 0.1 feat_proj_dropout = 0.0 feat_quantizer_dropout = 0.0 final_dropout = 0.1 layerdrop = 0.1 initializer_range = 0.02 layer_norm_eps = 1e-05 feat_extract_norm = 'group' feat_extract_activation = 'gelu' conv_dim = (512, 512, 512, 512, 512, 512, 512) conv_stride = (5, 2, 2, 2, 2, 2, 2) conv_kernel = (10, 3, 3, 3, 3, 2, 2) conv_bias = False num_conv_pos_embeddings = 128 num_conv_pos_embedding_groups = 16 do_stable_layer_norm = False apply_spec_augment = True mask_time_prob = 0.05 mask_time_length = 10 mask_time_min_masks = 2 mask_feature_prob = 0.0 mask_feature_length = 10 mask_feature_min_masks = 0 num_codevectors_per_group = 320 num_codevector_groups = 2 contrastive_logits_temperature = 0.1 num_negatives = 100 codevector_dim = 256 proj_codevector_dim = 256 diversity_loss_weight = 0.1 ctc_loss_reduction = 'sum' ctc_zero_infinity = False use_weighted_layer_sum = False classifier_proj_size = 256 tdnn_dim = (512, 512, 512, 512, 1500) tdnn_kernel = (5, 3, 3, 1, 1) tdnn_dilation = (1, 2, 3, 1, 1) xvector_output_dim = 512 pad_token_id = 0 bos_token_id = 1 eos_token_id = 2 add_adapter = False adapter_kernel_size = 3 adapter_stride = 2 num_adapter_layers = 3 output_hidden_size = None adapter_attn_dim = None **kwargs )\n```", "```py\n>>> from transformers import Wav2Vec2Config, Wav2Vec2Model\n\n>>> # Initializing a Wav2Vec2 facebook/wav2vec2-base-960h style configuration\n>>> configuration = Wav2Vec2Config()\n\n>>> # Initializing a model (with random weights) from the facebook/wav2vec2-base-960h style configuration\n>>> model = Wav2Vec2Model(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file bos_token = '<s>' eos_token = '</s>' unk_token = '<unk>' pad_token = '<pad>' word_delimiter_token = '|' replace_word_delimiter_char = ' ' do_lower_case = False target_lang = None **kwargs )\n```", "```py\n( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( token_ids: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None output_char_offsets: bool = False output_word_offsets: bool = False **kwargs ) \u2192 export const metadata = 'undefined';str or Wav2Vec2CTCTokenizerOutput\n```", "```py\n>>> # Let's see how to retrieve time steps for a model\n>>> from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC\n>>> from datasets import load_dataset\n>>> import datasets\n>>> import torch\n\n>>> # import model, feature extractor, tokenizer\n>>> model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n\n>>> # load first sample of English common_voice\n>>> dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"train\", streaming=True)\n>>> dataset = dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n>>> dataset_iter = iter(dataset)\n>>> sample = next(dataset_iter)\n\n>>> # forward sample through model to get greedily predicted transcription ids\n>>> input_values = feature_extractor(sample[\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n>>> logits = model(input_values).logits[0]\n>>> pred_ids = torch.argmax(logits, axis=-1)\n\n>>> # retrieve word stamps (analogous commands for `output_char_offsets`)\n>>> outputs = tokenizer.decode(pred_ids, output_word_offsets=True)\n>>> # compute `time_offset` in seconds as product of downsampling ratio and sampling_rate\n>>> time_offset = model.config.inputs_to_logits_ratio / feature_extractor.sampling_rate\n\n>>> word_offsets = [\n...     {\n...         \"word\": d[\"word\"],\n...         \"start_time\": round(d[\"start_offset\"] * time_offset, 2),\n...         \"end_time\": round(d[\"end_offset\"] * time_offset, 2),\n...     }\n...     for d in outputs.word_offsets\n... ]\n>>> # compare word offsets with audio `en_train_0/common_voice_en_19121553.mp3` online on the dataset viewer:\n>>> # https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0/viewer/en\n>>> word_offsets[:3]\n[{'word': 'THE', 'start_time': 0.7, 'end_time': 0.78}, {'word': 'TRICK', 'start_time': 0.88, 'end_time': 1.08}, {'word': 'APPEARS', 'start_time': 1.2, 'end_time': 1.64}]\n```", "```py\n( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None output_char_offsets: bool = False output_word_offsets: bool = False **kwargs ) \u2192 export const metadata = 'undefined';List[str] or Wav2Vec2CTCTokenizerOutput\n```", "```py\n( target_lang: str )\n```", "```py\n( feature_size = 1 sampling_rate = 16000 padding_value = 0.0 return_attention_mask = False do_normalize = True **kwargs )\n```", "```py\n( raw_speech: Union padding: Union = False max_length: Optional = None truncation: bool = False pad_to_multiple_of: Optional = None return_attention_mask: Optional = None return_tensors: Union = None sampling_rate: Optional = None **kwargs )\n```", "```py\n( feature_extractor tokenizer )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( pretrained_model_name_or_path **kwargs )\n```", "```py\n( save_directory push_to_hub: bool = False **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( feature_extractor: FeatureExtractionMixin tokenizer: PreTrainedTokenizerBase decoder: BeamSearchDecoderCTC )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( pretrained_model_name_or_path **kwargs )\n```", "```py\n( save_directory )\n```", "```py\n( logits: ndarray pool: Optional = None num_processes: Optional = None beam_width: Optional = None beam_prune_logp: Optional = None token_min_logp: Optional = None hotwords: Optional = None hotword_weight: Optional = None alpha: Optional = None beta: Optional = None unk_score_offset: Optional = None lm_score_boundary: Optional = None output_word_offsets: bool = False n_best: int = 1 )\n```", "```py\n( logits: ndarray beam_width: Optional = None beam_prune_logp: Optional = None token_min_logp: Optional = None hotwords: Optional = None hotword_weight: Optional = None alpha: Optional = None beta: Optional = None unk_score_offset: Optional = None lm_score_boundary: Optional = None output_word_offsets: bool = False n_best: int = 1 )\n```", "```py\n>>> # Let's see how to retrieve time steps for a model\n>>> from transformers import AutoTokenizer, AutoProcessor, AutoModelForCTC\n>>> from datasets import load_dataset\n>>> import datasets\n>>> import torch\n\n>>> # import model, feature extractor, tokenizer\n>>> model = AutoModelForCTC.from_pretrained(\"patrickvonplaten/wav2vec2-base-100h-with-lm\")\n>>> processor = AutoProcessor.from_pretrained(\"patrickvonplaten/wav2vec2-base-100h-with-lm\")\n\n>>> # load first sample of English common_voice\n>>> dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"train\", streaming=True)\n>>> dataset = dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n>>> dataset_iter = iter(dataset)\n>>> sample = next(dataset_iter)\n\n>>> # forward sample through model to get greedily predicted transcription ids\n>>> input_values = processor(sample[\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n>>> with torch.no_grad():\n...     logits = model(input_values).logits[0].cpu().numpy()\n\n>>> # retrieve word stamps (analogous commands for `output_char_offsets`)\n>>> outputs = processor.decode(logits, output_word_offsets=True)\n>>> # compute `time_offset` in seconds as product of downsampling ratio and sampling_rate\n>>> time_offset = model.config.inputs_to_logits_ratio / processor.feature_extractor.sampling_rate\n\n>>> word_offsets = [\n...     {\n...         \"word\": d[\"word\"],\n...         \"start_time\": round(d[\"start_offset\"] * time_offset, 2),\n...         \"end_time\": round(d[\"end_offset\"] * time_offset, 2),\n...     }\n...     for d in outputs.word_offsets\n... ]\n>>> # compare word offsets with audio `en_train_0/common_voice_en_19121553.mp3` online on the dataset viewer:\n>>> # https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0/viewer/en\n>>> word_offsets[:4]\n[{'word': 'THE', 'start_time': 0.68, 'end_time': 0.78}, {'word': 'TRACK', 'start_time': 0.88, 'end_time': 1.1}, {'word': 'APPEARS', 'start_time': 1.18, 'end_time': 1.66}, {'word': 'ON', 'start_time': 1.86, 'end_time': 1.92}]\n```", "```py\n>>> # Let's see how to use a user-managed pool for batch decoding multiple audios\n>>> from multiprocessing import get_context\n>>> from transformers import AutoTokenizer, AutoProcessor, AutoModelForCTC\n>>> from datasets import load_dataset\n>>> import datasets\n>>> import torch\n\n>>> # import model, feature extractor, tokenizer\n>>> model = AutoModelForCTC.from_pretrained(\"patrickvonplaten/wav2vec2-base-100h-with-lm\").to(\"cuda\")\n>>> processor = AutoProcessor.from_pretrained(\"patrickvonplaten/wav2vec2-base-100h-with-lm\")\n\n>>> # load example dataset\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> dataset = dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n\n>>> def map_to_array(batch):\n...     batch[\"speech\"] = batch[\"audio\"][\"array\"]\n...     return batch\n\n>>> # prepare speech data for batch inference\n>>> dataset = dataset.map(map_to_array, remove_columns=[\"audio\"])\n\n>>> def map_to_pred(batch, pool):\n...     inputs = processor(batch[\"speech\"], sampling_rate=16_000, padding=True, return_tensors=\"pt\")\n...     inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n\n...     with torch.no_grad():\n...         logits = model(**inputs).logits\n\n...     transcription = processor.batch_decode(logits.cpu().numpy(), pool).text\n...     batch[\"transcription\"] = transcription\n...     return batch\n\n>>> # note: pool should be instantiated *after* `Wav2Vec2ProcessorWithLM`.\n>>> #       otherwise, the LM won't be available to the pool's sub-processes\n>>> # select number of processes and batch_size based on number of CPU cores available and on dataset size\n>>> with get_context(\"fork\").Pool(processes=2) as pool:\n...     result = dataset.map(\n...         map_to_pred, batched=True, batch_size=2, fn_kwargs={\"pool\": pool}, remove_columns=[\"speech\"]\n...     )\n\n>>> result[\"transcription\"][:2]\n['MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL', \"NOR IS MISTER COULTER'S MANNER LESS INTERESTING THAN HIS MATTER\"]\n```", "```py\n( text: Union logit_score: Union = None lm_score: Union = None word_offsets: Union = None )\n```", "```py\n( last_hidden_state: FloatTensor = None extract_features: FloatTensor = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( loss: Optional = None projected_states: FloatTensor = None projected_quantized_states: FloatTensor = None codevector_perplexity: FloatTensor = None hidden_states: Optional = None attentions: Optional = None contrastive_loss: Optional = None diversity_loss: Optional = None )\n```", "```py\n( last_hidden_state: Array = None extract_features: Array = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( **updates )\n```", "```py\n( projected_states: Array = None projected_quantized_states: Array = None codevector_perplexity: Array = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( **updates )\n```", "```py\n( config: Wav2Vec2Config )\n```", "```py\n( input_values: Optional attention_mask: Optional = None mask_time_indices: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Wav2Vec2BaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, Wav2Vec2Model\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n>>> model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 292, 768]\n```", "```py\n( config target_lang: Optional = None )\n```", "```py\n( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, Wav2Vec2ForCTC\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n>>> model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n>>> predicted_ids = torch.argmax(logits, dim=-1)\n\n>>> # transcribe speech\n>>> transcription = processor.batch_decode(predicted_ids)\n>>> transcription[0]\n'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\n\n>>> inputs[\"labels\"] = processor(text=dataset[0][\"text\"], return_tensors=\"pt\").input_ids\n\n>>> # compute loss\n>>> loss = model(**inputs).loss\n>>> round(loss.item(), 2)\n53.48\n```", "```py\n( target_lang: str force_load = True **kwargs )\n```", "```py\n>>> from transformers import Wav2Vec2ForCTC, AutoProcessor\n\n>>> ckpt = \"facebook/mms-1b-all\"\n>>> processor = AutoProcessor.from_pretrained(ckpt)\n>>> model = Wav2Vec2ForCTC.from_pretrained(ckpt, target_lang=\"eng\")\n>>> # set specific language\n>>> processor.tokenizer.set_target_lang(\"spa\")\n>>> model.load_adapter(\"spa\")\n```", "```py\n( config )\n```", "```py\n( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoFeatureExtractor, Wav2Vec2ForSequenceClassification\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-ks\")\n>>> model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-ks\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.argmax(logits, dim=-1).item()\n>>> predicted_label = model.config.id2label[predicted_class_ids]\n>>> predicted_label\n'_unknown_'\n\n>>> # compute loss - target_label is e.g. \"down\"\n>>> target_label = model.config.id2label[0]\n>>> inputs[\"labels\"] = torch.tensor([model.config.label2id[target_label]])\n>>> loss = model(**inputs).loss\n>>> round(loss.item(), 2)\n6.54\n```", "```py\n( config )\n```", "```py\n( input_values: Optional attention_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoFeatureExtractor, Wav2Vec2ForAudioFrameClassification\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"anton-l/wav2vec2-base-superb-sd\")\n>>> model = Wav2Vec2ForAudioFrameClassification.from_pretrained(\"anton-l/wav2vec2-base-superb-sd\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], return_tensors=\"pt\", sampling_rate=sampling_rate)\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> probabilities = torch.sigmoid(logits[0])\n>>> # labels is a one-hot array of shape (num_frames, num_speakers)\n>>> labels = (probabilities > 0.5).long()\n>>> labels[0].tolist()\n[0, 0]\n```", "```py\n( config )\n```", "```py\n( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.XVectorOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoFeatureExtractor, Wav2Vec2ForXVector\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"anton-l/wav2vec2-base-superb-sv\")\n>>> model = Wav2Vec2ForXVector.from_pretrained(\"anton-l/wav2vec2-base-superb-sv\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = feature_extractor(\n...     [d[\"array\"] for d in dataset[:2][\"audio\"]], sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True\n... )\n>>> with torch.no_grad():\n...     embeddings = model(**inputs).embeddings\n\n>>> embeddings = torch.nn.functional.normalize(embeddings, dim=-1).cpu()\n\n>>> # the resulting embeddings can be used for cosine similarity-based retrieval\n>>> cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n>>> similarity = cosine_sim(embeddings[0], embeddings[1])\n>>> threshold = 0.7  # the optimal threshold is dataset-dependent\n>>> if similarity < threshold:\n...     print(\"Speakers are not the same!\")\n>>> round(similarity.item(), 2)\n0.98\n```", "```py\n( config: Wav2Vec2Config )\n```", "```py\n( input_values: Optional attention_mask: Optional = None mask_time_indices: Optional = None sampled_negative_indices: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoFeatureExtractor, Wav2Vec2ForPreTraining\n>>> from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n>>> from datasets import load_dataset\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n>>> model = Wav2Vec2ForPreTraining.from_pretrained(\"facebook/wav2vec2-base\")\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> input_values = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values  # Batch size 1\n\n>>> # compute masked indices\n>>> batch_size, raw_sequence_length = input_values.shape\n>>> sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length).item()\n>>> mask_time_indices = _compute_mask_indices(\n...     shape=(batch_size, sequence_length), mask_prob=0.2, mask_length=2\n... )\n>>> sampled_negative_indices = _sample_negative_indices(\n...     features_shape=(batch_size, sequence_length),\n...     num_negatives=model.config.num_negatives,\n...     mask_time_indices=mask_time_indices,\n... )\n>>> mask_time_indices = torch.tensor(data=mask_time_indices, device=input_values.device, dtype=torch.long)\n>>> sampled_negative_indices = torch.tensor(\n...     data=sampled_negative_indices, device=input_values.device, dtype=torch.long\n... )\n\n>>> with torch.no_grad():\n...     outputs = model(input_values, mask_time_indices=mask_time_indices)\n\n>>> # compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)\n>>> cosine_sim = torch.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states, dim=-1)\n\n>>> # show that cosine similarity is much higher than random\n>>> cosine_sim[mask_time_indices.to(torch.bool)].mean() > 0.5\ntensor(True)\n\n>>> # for contrastive loss training model should be put into train mode\n>>> model = model.train()\n>>> loss = model(\n...     input_values, mask_time_indices=mask_time_indices, sampled_negative_indices=sampled_negative_indices\n... ).loss\n```", "```py\n( config: Wav2Vec2Config *inputs **kwargs )\n```", "```py\n( input_values: tf.Tensor attention_mask: tf.Tensor | None = None token_type_ids: tf.Tensor | None = None position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None inputs_embeds: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoProcessor, TFWav2Vec2Model\n>>> from datasets import load_dataset\n>>> import soundfile as sf\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n>>> model = TFWav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n\n>>> def map_to_array(batch):\n...     speech, _ = sf.read(batch[\"file\"])\n...     batch[\"speech\"] = speech\n...     return batch\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> ds = ds.map(map_to_array)\n\n>>> input_values = processor(ds[\"speech\"][0], return_tensors=\"tf\").input_values  # Batch size 1\n>>> hidden_states = model(input_values).last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_values: tf.Tensor attention_mask: tf.Tensor | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None return_dict: bool | None = None labels: tf.Tensor | None = None training: bool = False )\n```", "```py\n( config: Wav2Vec2Config *inputs **kwargs )\n```", "```py\n( input_values: tf.Tensor attention_mask: tf.Tensor | None = None token_type_ids: tf.Tensor | None = None position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None inputs_embeds: tf.Tensor | None = None output_attentions: Optional[bool] = None labels: tf.Tensor | None = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFCausalLMOutput or tuple(tf.Tensor)\n```", "```py\n>>> import tensorflow as tf\n>>> from transformers import AutoProcessor, TFWav2Vec2ForCTC\n>>> from datasets import load_dataset\n>>> import soundfile as sf\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n>>> model = TFWav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n\n>>> def map_to_array(batch):\n...     speech, _ = sf.read(batch[\"file\"])\n...     batch[\"speech\"] = speech\n...     return batch\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> ds = ds.map(map_to_array)\n\n>>> input_values = processor(ds[\"speech\"][0], return_tensors=\"tf\").input_values  # Batch size 1\n>>> logits = model(input_values).logits\n>>> predicted_ids = tf.argmax(logits, axis=-1)\n\n>>> transcription = processor.decode(predicted_ids[0])\n\n>>> # compute loss\n>>> target_transcription = \"A MAN SAID TO THE UNIVERSE SIR I EXIST\"\n\n>>> # Pass transcription as `text` to encode labels\n>>> labels = processor(text=transcription, return_tensors=\"tf\").input_ids\n\n>>> loss = model(input_values, labels=labels).loss\n```", "```py\n( config: Wav2Vec2Config input_shape: Tuple = (1, 1024) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_values attention_mask = None mask_time_indices = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None freeze_feature_encoder: bool = False return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, FlaxWav2Vec2Model\n>>> from datasets import load_dataset\n>>> import soundfile as sf\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-large-lv60\")\n>>> model = FlaxWav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-lv60\")\n\n>>> def map_to_array(batch):\n...     speech, _ = sf.read(batch[\"file\"])\n...     batch[\"speech\"] = speech\n...     return batch\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> ds = ds.map(map_to_array)\n\n>>> input_values = processor(\n...     ds[\"speech\"][0], sampling_rate=16_000, return_tensors=\"np\"\n... ).input_values  # Batch size 1\n>>> hidden_states = model(input_values).last_hidden_state\n```", "```py\n( config: Wav2Vec2Config input_shape: Tuple = (1, 1024) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_values attention_mask = None mask_time_indices = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None freeze_feature_encoder: bool = False return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxMaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import jax.numpy as jnp\n>>> from transformers import AutoProcessor, FlaxWav2Vec2ForCTC\n>>> from datasets import load_dataset\n>>> import soundfile as sf\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60\")\n>>> model = FlaxWav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h-lv60\")\n\n>>> def map_to_array(batch):\n...     speech, _ = sf.read(batch[\"file\"])\n...     batch[\"speech\"] = speech\n...     return batch\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> ds = ds.map(map_to_array)\n\n>>> input_values = processor(\n...     ds[\"speech\"][0], sampling_rate=16_000, return_tensors=\"np\"\n... ).input_values  # Batch size 1\n>>> logits = model(input_values).logits\n>>> predicted_ids = jnp.argmax(logits, axis=-1)\n\n>>> transcription = processor.decode(predicted_ids[0])\n>>> # should give:  \"A MAN SAID TO THE UNIVERSE SIR I EXIST\"\n```", "```py\n( config: Wav2Vec2Config input_shape: Tuple = (1, 1024) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_values attention_mask = None mask_time_indices = None gumbel_temperature: int = 1 params: dict = None dropout_rng: PRNGKey = None gumbel_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None freeze_feature_encoder: bool = False return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import optax\n>>> import numpy as np\n>>> import jax.numpy as jnp\n>>> from transformers import AutoFeatureExtractor, FlaxWav2Vec2ForPreTraining\n>>> from transformers.models.wav2vec2.modeling_flax_wav2vec2 import _compute_mask_indices\n>>> from datasets import load_dataset\n>>> import soundfile as sf\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-lv60\")\n>>> model = FlaxWav2Vec2ForPreTraining.from_pretrained(\"facebook/wav2vec2-large-lv60\")\n\n>>> def map_to_array(batch):\n...     speech, _ = sf.read(batch[\"file\"])\n...     batch[\"speech\"] = speech\n...     return batch\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> ds = ds.map(map_to_array)\n\n>>> input_values = feature_extractor(ds[\"speech\"][0], return_tensors=\"np\").input_values  # Batch size 1\n\n>>> # compute masked indices\n>>> batch_size, raw_sequence_length = input_values.shape\n>>> sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length)\n>>> mask_time_indices = _compute_mask_indices((batch_size, sequence_length), mask_prob=0.2, mask_length=2)\n\n>>> outputs = model(input_values, mask_time_indices=mask_time_indices)\n\n>>> # compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)\n>>> cosine_sim = optax.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states)\n\n>>> # show that cosine similarity is much higher than random\n>>> assert np.asarray(cosine_sim)[mask_time_indices].mean() > 0.5\n```"]