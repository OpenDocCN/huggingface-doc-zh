- en: LLM prompting guide
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/prompting](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/prompting)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/353.89e0200a.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/DocNotebookDropdown.3e6b3817.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models such as Falcon, LLaMA, etc. are pretrained transformer
    models initially trained to predict the next token given some input text. They
    typically have billions of parameters and have been trained on trillions of tokens
    for an extended period of time. As a result, these models become quite powerful
    and versatile, and you can use them to solve multiple NLP tasks out of the box
    by instructing the models with natural language prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Designing such prompts to ensure the optimal output is often called ‚Äúprompt
    engineering‚Äù. Prompt engineering is an iterative process that requires a fair
    amount of experimentation. Natural languages are much more flexible and expressive
    than programming languages, however, they can also introduce some ambiguity. At
    the same time, prompts in natural language are quite sensitive to changes. Even
    minor modifications in prompts can lead to wildly different outputs.
  prefs: []
  type: TYPE_NORMAL
- en: While there is no exact recipe for creating prompts to match all cases, researchers
    have worked out a number of best practices that help to achieve optimal results
    more consistently.
  prefs: []
  type: TYPE_NORMAL
- en: 'This guide covers the prompt engineering best practices to help you craft better
    LLM prompts and solve various NLP tasks. You‚Äôll learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Basics of prompting](#basic-prompts)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Best practices of LLM prompting](#best-practices-of-llm-prompting)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Advanced prompting techniques: few-shot prompting and chain-of-thought](#advanced-prompting-techniques)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[When to fine-tune instead of prompting](#prompting-vs-fine-tuning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt engineering is only a part of the LLM output optimization process. Another
    essential component is choosing the optimal text generation strategy. You can
    customize how your LLM selects each of the subsequent tokens when generating the
    text without modifying any of the trainable parameters. By tweaking the text generation
    parameters, you can reduce repetition in the generated text and make it more coherent
    and human-sounding. Text generation strategies and parameters are out of scope
    for this guide, but you can learn more about these topics in the following guides:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Generation with LLMs](../llm_tutorial)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Text generation strategies](../generation_strategies)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basics of prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Types of models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The majority of modern LLMs are decoder-only transformers. Some examples include:
    [LLaMA](../model_doc/llama), [Llama2](../model_doc/llama2), [Falcon](../model_doc/falcon),
    [GPT2](../model_doc/gpt2). However, you may encounter encoder-decoder transformer
    LLMs as well, for instance, [Flan-T5](../model_doc/flan-t5) and [BART](../model_doc/bart).'
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-decoder-style models are typically used in generative tasks where the
    output **heavily** relies on the input, for example, in translation and summarization.
    The decoder-only models are used for all other types of generative tasks.
  prefs: []
  type: TYPE_NORMAL
- en: When using a pipeline to generate text with an LLM, it‚Äôs important to know what
    type of LLM you are using, because they use different pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run inference with decoder-only models with the `text-generation` pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To run inference with an encoder-decoder, use the `text2text-generation` pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Base vs instruct/chat models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most of the recent LLM checkpoints available on ü§ó Hub come in two versions:
    base and instruct (or chat). For example, [`tiiuae/falcon-7b`](https://huggingface.co/tiiuae/falcon-7b)
    and [`tiiuae/falcon-7b-instruct`](https://huggingface.co/tiiuae/falcon-7b-instruct).'
  prefs: []
  type: TYPE_NORMAL
- en: Base models are excellent at completing the text when given an initial prompt,
    however, they are not ideal for NLP tasks where they need to follow instructions,
    or for conversational use. This is where the instruct (chat) versions come in.
    These checkpoints are the result of further fine-tuning of the pre-trained base
    versions on instructions and conversational data. This additional fine-tuning
    makes them a better choice for many NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs illustrate some simple prompts that you can use with [`tiiuae/falcon-7b-instruct`](https://huggingface.co/tiiuae/falcon-7b-instruct)
    to solve some common NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: NLP tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, let‚Äôs set up the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let‚Äôs load the model with the appropriate pipeline (`"text-generation"`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that Falcon models were trained using the `bfloat16` datatype, so we recommend
    you use the same. This requires a recent version of CUDA and works best on modern
    cards.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the model loaded via the pipeline, let‚Äôs explore how you can
    use prompts to solve NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the most common forms of text classification is sentiment analysis,
    which assigns a label like ‚Äúpositive‚Äù, ‚Äúnegative‚Äù, or ‚Äúneutral‚Äù to a sequence
    of text. Let‚Äôs write a prompt that instructs the model to classify a given text
    (a movie review). We‚Äôll start by giving the instruction, and then specifying the
    text to classify. Note that instead of leaving it at that, we‚Äôre also adding the
    beginning of the response - `"Sentiment: "`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As a result, the output contains a classification label from the list we have
    provided in the instructions, and it is a correct one!
  prefs: []
  type: TYPE_NORMAL
- en: You may notice that in addition to the prompt, we pass a `max_new_tokens` parameter.
    It controls the number of tokens the model shall generate, and it is one of the
    many text generation parameters that you can learn about in [Text generation strategies](../generation_strategies)
    guide.
  prefs: []
  type: TYPE_NORMAL
- en: Named Entity Recognition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Named Entity Recognition (NER) is a task of finding named entities in a piece
    of text, such as a person, location, or organization. Let‚Äôs modify the instructions
    in the prompt to make the LLM perform this task. Here, let‚Äôs also set `return_full_text
    = False` so that output doesn‚Äôt contain the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the model correctly identified two named entities from the given
    text.
  prefs: []
  type: TYPE_NORMAL
- en: Translation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another task LLMs can perform is translation. You can choose to use encoder-decoder
    models for this task, however, here, for the simplicity of the examples, we‚Äôll
    keep using Falcon-7b-instruct, which does a decent job. Once again, here‚Äôs how
    you can write a basic prompt to instruct a model to translate a piece of text
    from English to Italian:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here we‚Äôve added a `do_sample=True` and `top_k=10` to allow the model to be
    a bit more flexible when generating output.
  prefs: []
  type: TYPE_NORMAL
- en: Text summarization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to the translation, text summarization is another generative task where
    the output **heavily** relies on the input, and encoder-decoder models can be
    a better choice. However, decoder-style models can be used for this task as well.
    Previously, we have placed the instructions at the very beginning of the prompt.
    However, the very end of the prompt can also be a suitable location for instructions.
    Typically, it‚Äôs better to place the instruction on one of the extreme ends.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Question answering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For question answering task we can structure the prompt into the following
    logical components: instructions, context, question, and the leading word or phrase
    (`"Answer:"`) to nudge the model to start generating the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Reasoning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reasoning is one of the most difficult tasks for LLMs, and achieving good results
    often requires applying advanced prompting techniques, like [Chain-of-though](#chain-of-thought).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs try if we can make a model reason about a simple arithmetics task with
    a basic prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Correct! Let‚Äôs increase the complexity a little and see if we can still get
    away with a basic prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is a wrong answer, it should be 12\. In this case, this can be due to the
    prompt being too basic, or due to the choice of model, after all we‚Äôve picked
    the smallest version of Falcon. Reasoning is difficult for models of all sizes,
    but larger models are likely to perform better.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices of LLM prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section of the guide we have compiled a list of best practices that
    tend to improve the prompt results:'
  prefs: []
  type: TYPE_NORMAL
- en: When choosing the model to work with, the latest and most capable models are
    likely to perform better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start with a simple and short prompt, and iterate from there.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Put the instructions at the beginning of the prompt, or at the very end. When
    working with large context, models apply various optimizations to prevent Attention
    complexity from scaling quadratically. This may make a model more attentive to
    the beginning or end of a prompt than the middle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clearly separate instructions from the text they apply to - more on this in
    the next section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be specific and descriptive about the task and the desired outcome - its format,
    length, style, language, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid ambiguous descriptions and instructions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Favor instructions that say ‚Äúwhat to do‚Äù instead of those that say ‚Äúwhat not
    to do‚Äù.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‚ÄúLead‚Äù the output in the right direction by writing the first word (or even
    begin the first sentence for the model).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use advanced techniques like [Few-shot prompting](#few-shot-prompting) and [Chain-of-thought](#chain-of-thought)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test your prompts with different models to assess their robustness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Version and track the performance of your prompts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced prompting techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Few-shot prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The basic prompts in the sections above are the examples of ‚Äúzero-shot‚Äù prompts,
    meaning, the model has been given instructions and context, but no examples with
    solutions. LLMs that have been fine-tuned on instruction datasets, generally perform
    well on such ‚Äúzero-shot‚Äù tasks. However, you may find that your task has more
    complexity or nuance, and, perhaps, you have some requirements for the output
    that the model doesn‚Äôt catch on just from the instructions. In this case, you
    can try the technique called few-shot prompting.
  prefs: []
  type: TYPE_NORMAL
- en: In few-shot prompting, we provide examples in the prompt giving the model more
    context to improve the performance. The examples condition the model to generate
    the output following the patterns in the examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the above code snippet we used a single example to demonstrate the desired
    output to the model, so this can be called a ‚Äúone-shot‚Äù prompting. However, depending
    on the task complexity you may need to use more than one example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Limitations of the few-shot prompting technique:'
  prefs: []
  type: TYPE_NORMAL
- en: While LLMs can pick up on the patterns in the examples, these technique doesn‚Äôt
    work well on complex reasoning tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Few-shot prompting requires creating lengthy prompts. Prompts with large number
    of tokens can increase computation and latency. There‚Äôs also a limit to the length
    of the prompts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes when given a number of examples, models can learn patterns that you
    didn‚Äôt intend them to learn, e.g. that the third movie review is always negative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chain-of-thought
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Chain-of-thought (CoT) prompting is a technique that nudges a model to produce
    intermediate reasoning steps thus improving the results on complex reasoning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways of steering a model to producing the reasoning steps:'
  prefs: []
  type: TYPE_NORMAL
- en: few-shot prompting by illustrating examples with detailed answers to questions,
    showing the model how to work through a problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: by instructing the model to reason by adding phrases like ‚ÄúLet‚Äôs think step
    by step‚Äù or ‚ÄúTake a deep breath and work through the problem step by step.‚Äù
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we apply the CoT technique to the muffins example from the [reasoning section](#reasoning)
    and use a larger model, such as (`tiiuae/falcon-180B-chat`) which you can play
    with in the [HuggingChat](https://huggingface.co/chat/), we‚Äôll get a significant
    improvement on the reasoning result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Prompting vs fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can achieve great results by optimizing your prompts, however, you may
    still ponder whether fine-tuning a model would work better for your case. Here
    are some scenarios when fine-tuning a smaller model may be a preferred option:'
  prefs: []
  type: TYPE_NORMAL
- en: Your domain is wildly different from what LLMs were pre-trained on and extensive
    prompt optimization did not yield sufficient results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need your model to work well in a low-resource language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need the model to be trained on sensitive data that is under strict regulations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have to use a small model due to cost, privacy, infrastructure or other
    limitations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In all of the above examples, you will need to make sure that you either already
    have or can easily obtain a large enough domain-specific dataset at a reasonable
    cost to fine-tune a model. You will also need to have enough time and resources
    to fine-tune a model.
  prefs: []
  type: TYPE_NORMAL
- en: If the above examples are not the case for you, optimizing prompts can prove
    to be more beneficial.
  prefs: []
  type: TYPE_NORMAL
