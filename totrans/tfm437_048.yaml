- en: LLM prompting guide
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/prompting](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/prompting)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models such as Falcon, LLaMA, etc. are pretrained transformer
    models initially trained to predict the next token given some input text. They
    typically have billions of parameters and have been trained on trillions of tokens
    for an extended period of time. As a result, these models become quite powerful
    and versatile, and you can use them to solve multiple NLP tasks out of the box
    by instructing the models with natural language prompts.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Designing such prompts to ensure the optimal output is often called ‚Äúprompt
    engineering‚Äù. Prompt engineering is an iterative process that requires a fair
    amount of experimentation. Natural languages are much more flexible and expressive
    than programming languages, however, they can also introduce some ambiguity. At
    the same time, prompts in natural language are quite sensitive to changes. Even
    minor modifications in prompts can lead to wildly different outputs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: While there is no exact recipe for creating prompts to match all cases, researchers
    have worked out a number of best practices that help to achieve optimal results
    more consistently.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'This guide covers the prompt engineering best practices to help you craft better
    LLM prompts and solve various NLP tasks. You‚Äôll learn:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[Basics of prompting](#basic-prompts)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Best practices of LLM prompting](#best-practices-of-llm-prompting)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Advanced prompting techniques: few-shot prompting and chain-of-thought](#advanced-prompting-techniques)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[When to fine-tune instead of prompting](#prompting-vs-fine-tuning)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt engineering is only a part of the LLM output optimization process. Another
    essential component is choosing the optimal text generation strategy. You can
    customize how your LLM selects each of the subsequent tokens when generating the
    text without modifying any of the trainable parameters. By tweaking the text generation
    parameters, you can reduce repetition in the generated text and make it more coherent
    and human-sounding. Text generation strategies and parameters are out of scope
    for this guide, but you can learn more about these topics in the following guides:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[Generation with LLMs](../llm_tutorial)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Text generation strategies](../generation_strategies)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basics of prompting
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Types of models
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The majority of modern LLMs are decoder-only transformers. Some examples include:
    [LLaMA](../model_doc/llama), [Llama2](../model_doc/llama2), [Falcon](../model_doc/falcon),
    [GPT2](../model_doc/gpt2). However, you may encounter encoder-decoder transformer
    LLMs as well, for instance, [Flan-T5](../model_doc/flan-t5) and [BART](../model_doc/bart).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-decoder-style models are typically used in generative tasks where the
    output **heavily** relies on the input, for example, in translation and summarization.
    The decoder-only models are used for all other types of generative tasks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: When using a pipeline to generate text with an LLM, it‚Äôs important to know what
    type of LLM you are using, because they use different pipelines.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Run inference with decoder-only models with the `text-generation` pipeline:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To run inference with an encoder-decoder, use the `text2text-generation` pipeline:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Base vs instruct/chat models
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most of the recent LLM checkpoints available on ü§ó Hub come in two versions:
    base and instruct (or chat). For example, [`tiiuae/falcon-7b`](https://huggingface.co/tiiuae/falcon-7b)
    and [`tiiuae/falcon-7b-instruct`](https://huggingface.co/tiiuae/falcon-7b-instruct).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Base models are excellent at completing the text when given an initial prompt,
    however, they are not ideal for NLP tasks where they need to follow instructions,
    or for conversational use. This is where the instruct (chat) versions come in.
    These checkpoints are the result of further fine-tuning of the pre-trained base
    versions on instructions and conversational data. This additional fine-tuning
    makes them a better choice for many NLP tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs illustrate some simple prompts that you can use with [`tiiuae/falcon-7b-instruct`](https://huggingface.co/tiiuae/falcon-7b-instruct)
    to solve some common NLP tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: NLP tasks
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, let‚Äôs set up the environment:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, let‚Äôs load the model with the appropriate pipeline (`"text-generation"`):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that Falcon models were trained using the `bfloat16` datatype, so we recommend
    you use the same. This requires a recent version of CUDA and works best on modern
    cards.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the model loaded via the pipeline, let‚Äôs explore how you can
    use prompts to solve NLP tasks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Text classification
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the most common forms of text classification is sentiment analysis,
    which assigns a label like ‚Äúpositive‚Äù, ‚Äúnegative‚Äù, or ‚Äúneutral‚Äù to a sequence
    of text. Let‚Äôs write a prompt that instructs the model to classify a given text
    (a movie review). We‚Äôll start by giving the instruction, and then specifying the
    text to classify. Note that instead of leaving it at that, we‚Äôre also adding the
    beginning of the response - `"Sentiment: "`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As a result, the output contains a classification label from the list we have
    provided in the instructions, and it is a correct one!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: You may notice that in addition to the prompt, we pass a `max_new_tokens` parameter.
    It controls the number of tokens the model shall generate, and it is one of the
    many text generation parameters that you can learn about in [Text generation strategies](../generation_strategies)
    guide.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Named Entity Recognition
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Named Entity Recognition (NER) is a task of finding named entities in a piece
    of text, such as a person, location, or organization. Let‚Äôs modify the instructions
    in the prompt to make the LLM perform this task. Here, let‚Äôs also set `return_full_text
    = False` so that output doesn‚Äôt contain the prompt:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, the model correctly identified two named entities from the given
    text.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Translation
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another task LLMs can perform is translation. You can choose to use encoder-decoder
    models for this task, however, here, for the simplicity of the examples, we‚Äôll
    keep using Falcon-7b-instruct, which does a decent job. Once again, here‚Äôs how
    you can write a basic prompt to instruct a model to translate a piece of text
    from English to Italian:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here we‚Äôve added a `do_sample=True` and `top_k=10` to allow the model to be
    a bit more flexible when generating output.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Text summarization
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to the translation, text summarization is another generative task where
    the output **heavily** relies on the input, and encoder-decoder models can be
    a better choice. However, decoder-style models can be used for this task as well.
    Previously, we have placed the instructions at the very beginning of the prompt.
    However, the very end of the prompt can also be a suitable location for instructions.
    Typically, it‚Äôs better to place the instruction on one of the extreme ends.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Question answering
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For question answering task we can structure the prompt into the following
    logical components: instructions, context, question, and the leading word or phrase
    (`"Answer:"`) to nudge the model to start generating the answer:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Reasoning
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reasoning is one of the most difficult tasks for LLMs, and achieving good results
    often requires applying advanced prompting techniques, like [Chain-of-though](#chain-of-thought).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs try if we can make a model reason about a simple arithmetics task with
    a basic prompt:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Correct! Let‚Äôs increase the complexity a little and see if we can still get
    away with a basic prompt:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This is a wrong answer, it should be 12\. In this case, this can be due to the
    prompt being too basic, or due to the choice of model, after all we‚Äôve picked
    the smallest version of Falcon. Reasoning is difficult for models of all sizes,
    but larger models are likely to perform better.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ËøôÊòØ‰∏Ä‰∏™ÈîôËØØÁ≠îÊ°àÔºåÂ∫îËØ•ÊòØ12„ÄÇÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåËøôÂèØËÉΩÊòØÂõ†‰∏∫ÊèêÁ§∫Ëøá‰∫éÂü∫Á°ÄÔºåÊàñËÄÖÊòØÂõ†‰∏∫Ê®°ÂûãÈÄâÊã©‰∏çÂΩìÔºåÊØïÁ´üÊàë‰ª¨ÈÄâÊã©‰∫ÜFalconÁöÑÊúÄÂ∞èÁâàÊú¨„ÄÇÂØπ‰∫éÊâÄÊúâÂ§ßÂ∞èÁöÑÊ®°ÂûãÊù•ËØ¥ÔºåÊé®ÁêÜÈÉΩÊòØÂõ∞ÈöæÁöÑÔºå‰ΩÜÊõ¥Â§ßÁöÑÊ®°ÂûãÂèØËÉΩË°®Áé∞Êõ¥Â•Ω„ÄÇ
- en: Best practices of LLM prompting
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMÊèêÁ§∫ÁöÑÊúÄ‰Ω≥ÂÆûË∑µ
- en: 'In this section of the guide we have compiled a list of best practices that
    tend to improve the prompt results:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Âú®Êú¨ÊåáÂçóÁöÑËøô‰∏ÄÈÉ®ÂàÜ‰∏≠ÔºåÊàë‰ª¨ÁºñÂà∂‰∫Ü‰∏Ä‰ªΩÂÄæÂêë‰∫éÊîπÂñÑÊèêÁ§∫ÁªìÊûúÁöÑÊúÄ‰Ω≥ÂÆûË∑µÊ∏ÖÂçïÔºö
- en: When choosing the model to work with, the latest and most capable models are
    likely to perform better.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Âú®ÈÄâÊã©Ë¶Å‰ΩøÁî®ÁöÑÊ®°ÂûãÊó∂ÔºåÊúÄÊñ∞ÂíåÊúÄÊúâËÉΩÂäõÁöÑÊ®°ÂûãÂèØËÉΩË°®Áé∞Êõ¥Â•Ω„ÄÇ
- en: Start with a simple and short prompt, and iterate from there.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‰ªé‰∏Ä‰∏™ÁÆÄÂçïËÄåÁü≠ÁöÑÊèêÁ§∫ÂºÄÂßãÔºåÁÑ∂ÂêéÈÄêÊ≠•Ëø≠‰ª£„ÄÇ
- en: Put the instructions at the beginning of the prompt, or at the very end. When
    working with large context, models apply various optimizations to prevent Attention
    complexity from scaling quadratically. This may make a model more attentive to
    the beginning or end of a prompt than the middle.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Â∞ÜÊåá‰ª§ÊîæÂú®ÊèêÁ§∫ÁöÑÂºÄÂ§¥ÊàñÊúÄÂêé„ÄÇÂú®Â§ÑÁêÜÂ§ßÈáè‰∏ä‰∏ãÊñáÊó∂ÔºåÊ®°Âûã‰ºöÂ∫îÁî®ÂêÑÁßç‰ºòÂåñÊé™ÊñΩÔºå‰ª•Èò≤Ê≠¢Ê≥®ÊÑèÂäõÂ§çÊùÇÂ∫¶Âëà‰∫åÊ¨°ÊñπÂ¢ûÈïø„ÄÇËøôÂèØËÉΩ‰ºö‰ΩøÊ®°ÂûãÊõ¥Âä†ÂÖ≥Ê≥®ÊèêÁ§∫ÁöÑÂºÄÂ§¥ÊàñÁªìÂ∞æÔºåËÄå‰∏çÊòØ‰∏≠Èó¥ÈÉ®ÂàÜ„ÄÇ
- en: Clearly separate instructions from the text they apply to - more on this in
    the next section.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Â∞ÜÊåá‰ª§‰∏éÂÖ∂ÈÄÇÁî®ÁöÑÊñáÊú¨Ê∏ÖÊô∞ÂàÜÂºÄ-Êõ¥Â§öÂÜÖÂÆπËØ∑ÂèÇËßÅ‰∏ã‰∏ÄËäÇ„ÄÇ
- en: Be specific and descriptive about the task and the desired outcome - its format,
    length, style, language, etc.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÂØπ‰ªªÂä°ÂíåÊúüÊúõÁªìÊûúËøõË°åÂÖ∑‰ΩìÂíåÊèèËø∞ÊÄßÁöÑËØ¥Êòé-ÂÖ∂Ê†ºÂºè„ÄÅÈïøÂ∫¶„ÄÅÈ£éÊ†º„ÄÅËØ≠Ë®ÄÁ≠â„ÄÇ
- en: Avoid ambiguous descriptions and instructions.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÈÅøÂÖçÊ®°Ê£±‰∏§ÂèØÁöÑÊèèËø∞ÂíåÊåá‰ª§„ÄÇ
- en: Favor instructions that say ‚Äúwhat to do‚Äù instead of those that say ‚Äúwhat not
    to do‚Äù.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Êõ¥ÂÄæÂêë‰∫éËØ¥‚ÄúË¶ÅÂÅö‰ªÄ‰πà‚ÄùËÄå‰∏çÊòØËØ¥‚Äú‰∏çË¶ÅÂÅö‰ªÄ‰πà‚ÄùÁöÑÊåá‰ª§„ÄÇ
- en: ‚ÄúLead‚Äù the output in the right direction by writing the first word (or even
    begin the first sentence for the model).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÈÄöËøáÁºñÂÜôÁ¨¨‰∏Ä‰∏™ÂçïËØçÔºàÁîöËá≥ÂºÄÂßãÁ¨¨‰∏Ä‰∏™Âè•Â≠êÔºâÊù•‚ÄúÂºïÂØº‚ÄùËæìÂá∫ÊúùÁùÄÊ≠£Á°ÆÊñπÂêëÂèëÂ±ï„ÄÇ
- en: Use advanced techniques like [Few-shot prompting](#few-shot-prompting) and [Chain-of-thought](#chain-of-thought)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‰ΩøÁî®È´òÁ∫ßÊäÄÊúØÔºåÂ¶Ç[Â∞ëÊ†∑Êú¨ÊèêÁ§∫](#Â∞ëÊ†∑Êú¨ÊèêÁ§∫)Âíå[ÊÄùÁª¥Èìæ](#ÊÄùÁª¥Èìæ)
- en: Test your prompts with different models to assess their robustness.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‰ΩøÁî®‰∏çÂêåÊ®°ÂûãÊµãËØïÊÇ®ÁöÑÊèêÁ§∫Ôºå‰ª•ËØÑ‰º∞ÂÖ∂Á®≥ÂÅ•ÊÄß„ÄÇ
- en: Version and track the performance of your prompts.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÁâàÊú¨ÂíåË∑üË∏™ÊèêÁ§∫ÁöÑÊÄßËÉΩ„ÄÇ
- en: Advanced prompting techniques
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: È´òÁ∫ßÊèêÁ§∫ÊäÄÊúØ
- en: Few-shot prompting
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Â∞ëÊ†∑Êú¨ÊèêÁ§∫
- en: The basic prompts in the sections above are the examples of ‚Äúzero-shot‚Äù prompts,
    meaning, the model has been given instructions and context, but no examples with
    solutions. LLMs that have been fine-tuned on instruction datasets, generally perform
    well on such ‚Äúzero-shot‚Äù tasks. However, you may find that your task has more
    complexity or nuance, and, perhaps, you have some requirements for the output
    that the model doesn‚Äôt catch on just from the instructions. In this case, you
    can try the technique called few-shot prompting.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ‰∏äËø∞ÈÉ®ÂàÜÁöÑÂü∫Êú¨ÊèêÁ§∫ÊòØ‚ÄúÈõ∂Ê†∑Êú¨‚ÄùÊèêÁ§∫ÁöÑÁ§∫‰æãÔºåËøôÊÑèÂë≥ÁùÄÊ®°ÂûãÂ∑≤ÁªèËé∑Âæó‰∫ÜÊåá‰ª§Âíå‰∏ä‰∏ãÊñáÔºå‰ΩÜÊ≤°ÊúâÂ∏¶ÊúâËß£ÂÜ≥ÊñπÊ°àÁöÑÁ§∫‰æã„ÄÇÈÄöÂ∏∏Âú®Êåá‰ª§Êï∞ÊçÆÈõÜ‰∏äËøõË°åÂæÆË∞ÉÁöÑLLMÂú®ËøôÁßç‚ÄúÈõ∂Ê†∑Êú¨‚Äù‰ªªÂä°‰∏äË°®Áé∞ËâØÂ•Ω„ÄÇÁÑ∂ËÄåÔºåÊÇ®ÂèØËÉΩ‰ºöÂèëÁé∞ÊÇ®ÁöÑ‰ªªÂä°Êõ¥Âä†Â§çÊùÇÊàñÂæÆÂ¶ôÔºå‰πüËÆ∏ÊÇ®ÂØπÊ®°ÂûãÊ≤°Êúâ‰ªéÊåá‰ª§‰∏≠ÊçïÊçâÂà∞ÁöÑËæìÂá∫Êúâ‰∏Ä‰∫õË¶ÅÊ±Ç„ÄÇÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÊÇ®ÂèØ‰ª•Â∞ùËØïÁß∞‰∏∫Â∞ëÊ†∑Êú¨ÊèêÁ§∫ÁöÑÊäÄÊúØ„ÄÇ
- en: In few-shot prompting, we provide examples in the prompt giving the model more
    context to improve the performance. The examples condition the model to generate
    the output following the patterns in the examples.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Âú®Â∞ëÊ†∑Êú¨ÊèêÁ§∫‰∏≠ÔºåÊàë‰ª¨Âú®ÊèêÁ§∫‰∏≠Êèê‰æõÁ§∫‰æãÔºå‰∏∫Ê®°ÂûãÊèê‰æõÊõ¥Â§ö‰∏ä‰∏ãÊñá‰ª•ÊèêÈ´òÊÄßËÉΩ„ÄÇËøô‰∫õÁ§∫‰æã‰ºöËÆ©Ê®°ÂûãÁîüÊàêÈÅµÂæ™Á§∫‰æãÊ®°ÂºèÁöÑËæìÂá∫„ÄÇ
- en: 'Here‚Äôs an example:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ËøôÈáåÊúâ‰∏Ä‰∏™‰æãÂ≠êÔºö
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the above code snippet we used a single example to demonstrate the desired
    output to the model, so this can be called a ‚Äúone-shot‚Äù prompting. However, depending
    on the task complexity you may need to use more than one example.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Âú®‰∏äÈù¢ÁöÑ‰ª£Á†ÅÁâáÊÆµ‰∏≠ÔºåÊàë‰ª¨‰ΩøÁî®‰∫Ü‰∏Ä‰∏™Á§∫‰æãÊù•ÂêëÊ®°ÂûãÂ±ïÁ§∫ÊâÄÈúÄÁöÑËæìÂá∫ÔºåÂõ†Ê≠§ËøôÂèØ‰ª•Áß∞‰∏∫‚Äú‰∏ÄÊ¨°ÊÄß‚ÄùÊèêÁ§∫„ÄÇÁÑ∂ËÄåÔºåÊ†πÊçÆ‰ªªÂä°ÁöÑÂ§çÊùÇÊÄßÔºåÊÇ®ÂèØËÉΩÈúÄË¶Å‰ΩøÁî®Â§ö‰∏™Á§∫‰æã„ÄÇ
- en: 'Limitations of the few-shot prompting technique:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Â∞ëÊ†∑Êú¨ÊèêÁ§∫ÊäÄÊúØÁöÑÂ±ÄÈôêÊÄßÔºö
- en: While LLMs can pick up on the patterns in the examples, these technique doesn‚Äôt
    work well on complex reasoning tasks
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ËôΩÁÑ∂LLMÂèØ‰ª•ÊçïÊçâÂà∞Á§∫‰æã‰∏≠ÁöÑÊ®°ÂºèÔºå‰ΩÜËøô‰∫õÊäÄÊúØÂú®Â§çÊùÇÁöÑÊé®ÁêÜ‰ªªÂä°‰∏äÊïàÊûú‰∏ç‰Ω≥
- en: Few-shot prompting requires creating lengthy prompts. Prompts with large number
    of tokens can increase computation and latency. There‚Äôs also a limit to the length
    of the prompts.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Â∞ëÊ†∑Êú¨ÊèêÁ§∫ÈúÄË¶ÅÂàõÂª∫ËæÉÈïøÁöÑÊèêÁ§∫„ÄÇÂÖ∑ÊúâÂ§ßÈáèÊ†áËÆ∞ÁöÑÊèêÁ§∫ÂèØËÉΩ‰ºöÂ¢ûÂä†ËÆ°ÁÆóÂíåÂª∂Ëøü„ÄÇÊèêÁ§∫ÁöÑÈïøÂ∫¶‰πüÊúâÈôêÂà∂„ÄÇ
- en: Sometimes when given a number of examples, models can learn patterns that you
    didn‚Äôt intend them to learn, e.g. that the third movie review is always negative.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÊúâÊó∂ÔºåÂΩìÁªôÂÆöÂ§ö‰∏™Á§∫‰æãÊó∂ÔºåÊ®°ÂûãÂèØËÉΩ‰ºöÂ≠¶‰π†ÊÇ®Âπ∂ÈùûÊâìÁÆóËÆ©ÂÆÉÂ≠¶‰π†ÁöÑÊ®°ÂºèÔºå‰æãÂ¶ÇÁ¨¨‰∏â‰∏™ÁîµÂΩ±ËØÑËÆ∫ÊÄªÊòØË¥üÈù¢ÁöÑ„ÄÇ
- en: Chain-of-thought
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ÊÄùÁª¥Èìæ
- en: Chain-of-thought (CoT) prompting is a technique that nudges a model to produce
    intermediate reasoning steps thus improving the results on complex reasoning tasks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ÊÄùÁª¥ÈìæÔºàCoTÔºâÊèêÁ§∫ÊòØ‰∏ÄÁßçÊäÄÊúØÔºåÂÆÉ‰øÉ‰ΩøÊ®°Âûã‰∫ßÁîü‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§Ôºå‰ªéËÄåÊèêÈ´òÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°ÁöÑÁªìÊûú„ÄÇ
- en: 'There are two ways of steering a model to producing the reasoning steps:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Êúâ‰∏§ÁßçÊñπÊ≥ïÂèØ‰ª•ÂºïÂØºÊ®°Âûã‰∫ßÁîüÊé®ÁêÜÊ≠•È™§Ôºö
- en: few-shot prompting by illustrating examples with detailed answers to questions,
    showing the model how to work through a problem.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÈÄöËøáÁî®ËØ¶ÁªÜÁ≠îÊ°àËØ¥ÊòéÁ§∫‰æãÊù•ËøõË°åÂ∞ëÊ†∑Êú¨ÊèêÁ§∫ÔºåÂêëÊ®°ÂûãÂ±ïÁ§∫Â¶Ç‰ΩïËß£ÂÜ≥ÈóÆÈ¢ò„ÄÇ
- en: by instructing the model to reason by adding phrases like ‚ÄúLet‚Äôs think step
    by step‚Äù or ‚ÄúTake a deep breath and work through the problem step by step.‚Äù
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÈÄöËøáÊ∑ªÂä†Áü≠ËØ≠ÔºåÂ¶Ç‚ÄúËÆ©Êàë‰ª¨‰∏ÄÊ≠•‰∏ÄÊ≠•Âú∞ÊÄùËÄÉ‚ÄùÊàñ‚ÄúÊ∑±ÂëºÂê∏Ôºå‰∏ÄÊ≠•‰∏ÄÊ≠•Âú∞Ëß£ÂÜ≥ÈóÆÈ¢ò‚ÄùÔºåÊåáÂØºÊ®°ÂûãËøõË°åÊé®ÁêÜ„ÄÇ
- en: 'If we apply the CoT technique to the muffins example from the [reasoning section](#reasoning)
    and use a larger model, such as (`tiiuae/falcon-180B-chat`) which you can play
    with in the [HuggingChat](https://huggingface.co/chat/), we‚Äôll get a significant
    improvement on the reasoning result:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Â¶ÇÊûúÊàë‰ª¨Â∞ÜCoTÊäÄÊúØÂ∫îÁî®‰∫é[Êé®ÁêÜÈÉ®ÂàÜ](#Êé®ÁêÜ)‰∏≠ÁöÑÊùæÈ•ºÁ§∫‰æãÔºåÂπ∂‰ΩøÁî®Êõ¥Â§ßÁöÑÊ®°ÂûãÔºå‰æãÂ¶ÇÔºà`tiiuae/falcon-180B-chat`ÔºâÔºåÊÇ®ÂèØ‰ª•Âú®[HuggingChat](https://huggingface.co/chat/)‰∏≠Â∞ùËØïÔºåÊàë‰ª¨Â∞ÜÂú®Êé®ÁêÜÁªìÊûú‰∏äËé∑ÂæóÊòæËëóÁöÑÊîπËøõÔºö
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Prompting vs fine-tuning
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ÊèêÁ§∫ vs ÂæÆË∞É
- en: 'You can achieve great results by optimizing your prompts, however, you may
    still ponder whether fine-tuning a model would work better for your case. Here
    are some scenarios when fine-tuning a smaller model may be a preferred option:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ÈÄöËøá‰ºòÂåñÊÇ®ÁöÑÊèêÁ§∫ÔºåÊÇ®ÂèØ‰ª•ÂèñÂæóÂá∫Ëâ≤ÁöÑÁªìÊûúÔºå‰ΩÜÊòØÊÇ®ÂèØËÉΩ‰ªçÁÑ∂Âú®ËÄÉËôëÊòØÂê¶ÂæÆË∞ÉÊ®°ÂûãÂØπÊÇ®ÁöÑÊÉÖÂÜµÊõ¥ÊúâÊïà„ÄÇ‰ª•‰∏ãÊòØ‰∏Ä‰∫õÂæÆË∞ÉËæÉÂ∞èÊ®°ÂûãÂèØËÉΩÊòØÈ¶ñÈÄâÁöÑÊÉÖÂÜµÔºö
- en: Your domain is wildly different from what LLMs were pre-trained on and extensive
    prompt optimization did not yield sufficient results.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÊÇ®ÁöÑÈ¢ÜÂüü‰∏éLLMsÈ¢ÑÂÖàËÆ≠ÁªÉÁöÑÈ¢ÜÂüüÂ§ßÁõ∏ÂæÑÂ∫≠ÔºåÂπøÊ≥õÁöÑÊèêÁ§∫‰ºòÂåñÂπ∂Êú™‰∫ßÁîüË∂≥Â§üÁöÑÁªìÊûú„ÄÇ
- en: You need your model to work well in a low-resource language.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÊÇ®ÈúÄË¶ÅÊÇ®ÁöÑÊ®°ÂûãÂú®ËµÑÊ∫êÁ®ÄÁº∫ÁöÑËØ≠Ë®Ä‰∏≠Ë°®Áé∞ËâØÂ•Ω„ÄÇ
- en: You need the model to be trained on sensitive data that is under strict regulations.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÊÇ®ÈúÄË¶ÅËÆ≠ÁªÉÊ®°ÂûãÁöÑÊï∞ÊçÆÊòØÂèó‰∏•Ê†ºÁõëÁÆ°ÁöÑÊïèÊÑüÊï∞ÊçÆ„ÄÇ
- en: You have to use a small model due to cost, privacy, infrastructure or other
    limitations.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Áî±‰∫éÊàêÊú¨„ÄÅÈöêÁßÅ„ÄÅÂü∫Á°ÄËÆæÊñΩÊàñÂÖ∂‰ªñÈôêÂà∂ÔºåÊÇ®ÂøÖÈ°ª‰ΩøÁî®Â∞èÂûãÊ®°Âûã„ÄÇ
- en: In all of the above examples, you will need to make sure that you either already
    have or can easily obtain a large enough domain-specific dataset at a reasonable
    cost to fine-tune a model. You will also need to have enough time and resources
    to fine-tune a model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Âú®‰∏äËø∞ÊâÄÊúâÁ§∫‰æã‰∏≠ÔºåÊÇ®ÈúÄË¶ÅÁ°Æ‰øùÊÇ®Â∑≤ÁªèÊã•ÊúâÊàñÂèØ‰ª•ËΩªÊùæËé∑ÂæóË∂≥Â§üÂ§ßÁöÑÈ¢ÜÂüüÁâπÂÆöÊï∞ÊçÆÈõÜÔºå‰ª•ÂêàÁêÜÁöÑÊàêÊú¨Êù•ÂæÆË∞ÉÊ®°Âûã„ÄÇÊÇ®ËøòÈúÄË¶ÅÊúâË∂≥Â§üÁöÑÊó∂Èó¥ÂíåËµÑÊ∫êÊù•ÂæÆË∞ÉÊ®°Âûã„ÄÇ
- en: If the above examples are not the case for you, optimizing prompts can prove
    to be more beneficial.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Â¶ÇÊûú‰∏äËø∞Á§∫‰æã‰∏çÈÄÇÁî®‰∫éÊÇ®Ôºå‰ºòÂåñÊèêÁ§∫ÂèØËÉΩ‰ºöÊõ¥ÊúâÁõä„ÄÇ
