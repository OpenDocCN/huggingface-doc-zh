- en: LLM prompting guide
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM提示指南
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/prompting](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/prompting)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/prompting](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/prompting)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models such as Falcon, LLaMA, etc. are pretrained transformer
    models initially trained to predict the next token given some input text. They
    typically have billions of parameters and have been trained on trillions of tokens
    for an extended period of time. As a result, these models become quite powerful
    and versatile, and you can use them to solve multiple NLP tasks out of the box
    by instructing the models with natural language prompts.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 像Falcon、LLaMA等大型语言模型是预训练的变压器模型，最初训练用于预测给定一些输入文本的下一个标记。它们通常具有数十亿个参数，并且已经在长时间内训练了数万亿个标记。因此，这些模型变得非常强大和多功能，您可以通过用自然语言提示指导模型来解决多个NLP任务。
- en: Designing such prompts to ensure the optimal output is often called “prompt
    engineering”. Prompt engineering is an iterative process that requires a fair
    amount of experimentation. Natural languages are much more flexible and expressive
    than programming languages, however, they can also introduce some ambiguity. At
    the same time, prompts in natural language are quite sensitive to changes. Even
    minor modifications in prompts can lead to wildly different outputs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 设计这样的提示以确保最佳输出通常被称为“提示工程”。提示工程是一个需要大量实验的迭代过程。自然语言比编程语言更加灵活和表达丰富，但也可能引入一些歧义。同时，自然语言中的提示对变化非常敏感。即使提示中进行轻微修改也可能导致截然不同的输出。
- en: While there is no exact recipe for creating prompts to match all cases, researchers
    have worked out a number of best practices that help to achieve optimal results
    more consistently.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然没有确切的配方可以创建适用于所有情况的提示，但研究人员已经制定出一些最佳实践，有助于更一致地实现最佳结果。
- en: 'This guide covers the prompt engineering best practices to help you craft better
    LLM prompts and solve various NLP tasks. You’ll learn:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南涵盖了提示工程的最佳实践，以帮助您制作更好的LLM提示并解决各种NLP任务。您将学到：
- en: '[Basics of prompting](#basic-prompts)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提示的基础知识](#basic-prompts)'
- en: '[Best practices of LLM prompting](#best-practices-of-llm-prompting)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLM提示的最佳实践](#best-practices-of-llm-prompting)'
- en: '[Advanced prompting techniques: few-shot prompting and chain-of-thought](#advanced-prompting-techniques)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[高级提示技术：少样本提示和思维链](#advanced-prompting-techniques)'
- en: '[When to fine-tune instead of prompting](#prompting-vs-fine-tuning)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[何时进行微调而不是提示](#prompting-vs-fine-tuning)'
- en: 'Prompt engineering is only a part of the LLM output optimization process. Another
    essential component is choosing the optimal text generation strategy. You can
    customize how your LLM selects each of the subsequent tokens when generating the
    text without modifying any of the trainable parameters. By tweaking the text generation
    parameters, you can reduce repetition in the generated text and make it more coherent
    and human-sounding. Text generation strategies and parameters are out of scope
    for this guide, but you can learn more about these topics in the following guides:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程仅是LLM输出优化过程的一部分。另一个重要组成部分是选择最佳的文本生成策略。您可以自定义LLM在生成文本时如何选择每个后续标记，而无需修改任何可训练参数。通过调整文本生成参数，您可以减少生成文本中的重复，并使其更连贯和更具人类声音。文本生成策略和参数超出了本指南的范围，但您可以在以下指南中了解更多相关主题：
- en: '[Generation with LLMs](../llm_tutorial)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用LLM进行生成](../llm_tutorial)'
- en: '[Text generation strategies](../generation_strategies)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本生成策略](../generation_strategies)'
- en: Basics of prompting
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示的基础知识
- en: Types of models
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型类型
- en: 'The majority of modern LLMs are decoder-only transformers. Some examples include:
    [LLaMA](../model_doc/llama), [Llama2](../model_doc/llama2), [Falcon](../model_doc/falcon),
    [GPT2](../model_doc/gpt2). However, you may encounter encoder-decoder transformer
    LLMs as well, for instance, [Flan-T5](../model_doc/flan-t5) and [BART](../model_doc/bart).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现代LLM大多数是仅解码器的变压器。一些例子包括：[LLaMA](../model_doc/llama), [Llama2](../model_doc/llama2),
    [Falcon](../model_doc/falcon), [GPT2](../model_doc/gpt2)。但是，您也可能遇到编码器-解码器变压器LLM，例如[Flan-T5](../model_doc/flan-t5)和[BART](../model_doc/bart)。
- en: Encoder-decoder-style models are typically used in generative tasks where the
    output **heavily** relies on the input, for example, in translation and summarization.
    The decoder-only models are used for all other types of generative tasks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器风格的模型通常用于生成任务，其中输出**严重**依赖于输入，例如翻译和总结。解码器模型用于所有其他类型的生成任务。
- en: When using a pipeline to generate text with an LLM, it’s important to know what
    type of LLM you are using, because they use different pipelines.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用管道生成LLM文本时，了解您正在使用的LLM类型很重要，因为它们使用不同的管道。
- en: 'Run inference with decoder-only models with the `text-generation` pipeline:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`text-generation`管道运行仅解码器模型的推理：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To run inference with an encoder-decoder, use the `text2text-generation` pipeline:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用编码器-解码器进行推理，请使用`text2text-generation`管道：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Base vs instruct/chat models
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础版 vs 指导/聊天版模型
- en: 'Most of the recent LLM checkpoints available on 🤗 Hub come in two versions:
    base and instruct (or chat). For example, [`tiiuae/falcon-7b`](https://huggingface.co/tiiuae/falcon-7b)
    and [`tiiuae/falcon-7b-instruct`](https://huggingface.co/tiiuae/falcon-7b-instruct).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Hub上提供的大多数最新LLM检查点都有两个版本：基础版和指导版（或聊天版）。例如，[`tiiuae/falcon-7b`](https://huggingface.co/tiiuae/falcon-7b)
    和 [`tiiuae/falcon-7b-instruct`](https://huggingface.co/tiiuae/falcon-7b-instruct)。
- en: Base models are excellent at completing the text when given an initial prompt,
    however, they are not ideal for NLP tasks where they need to follow instructions,
    or for conversational use. This is where the instruct (chat) versions come in.
    These checkpoints are the result of further fine-tuning of the pre-trained base
    versions on instructions and conversational data. This additional fine-tuning
    makes them a better choice for many NLP tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型在给定初始提示时完成文本的能力非常出色，但是它们并不适合需要遵循指令或用于对话的NLP任务。这就是指导（聊天）版本的用武之地。这些检查点是在预训练基础版本上进一步微调指令和对话数据的结果。这种额外的微调使它们成为许多NLP任务的更好选择。
- en: Let’s illustrate some simple prompts that you can use with [`tiiuae/falcon-7b-instruct`](https://huggingface.co/tiiuae/falcon-7b-instruct)
    to solve some common NLP tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举例说明一些简单的提示，您可以使用[`tiiuae/falcon-7b-instruct`](https://huggingface.co/tiiuae/falcon-7b-instruct)来解决一些常见的NLP任务。
- en: NLP tasks
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然语言处理任务
- en: 'First, let’s set up the environment:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们设置环境：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, let’s load the model with the appropriate pipeline (`"text-generation"`):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用适当的管道（`"text-generation"`）加载模型：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that Falcon models were trained using the `bfloat16` datatype, so we recommend
    you use the same. This requires a recent version of CUDA and works best on modern
    cards.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Falcon模型是使用`bfloat16`数据类型训练的，因此我们建议您也使用相同的数据类型。这需要一个最新版本的CUDA，并且在现代显卡上效果最佳。
- en: Now that we have the model loaded via the pipeline, let’s explore how you can
    use prompts to solve NLP tasks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经通过管道加载了模型，让我们探讨如何使用提示来解决NLP任务。
- en: Text classification
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 文本分类
- en: 'One of the most common forms of text classification is sentiment analysis,
    which assigns a label like “positive”, “negative”, or “neutral” to a sequence
    of text. Let’s write a prompt that instructs the model to classify a given text
    (a movie review). We’ll start by giving the instruction, and then specifying the
    text to classify. Note that instead of leaving it at that, we’re also adding the
    beginning of the response - `"Sentiment: "`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类中最常见的形式之一是情感分析，它为一段文本分配一个标签，比如“积极”、“消极”或“中性”。让我们编写一个提示，指示模型对给定的文本（电影评论）进行分类。我们将从给出指令开始，然后指定要分类的文本。请注意，我们不仅仅止步于此，还添加了响应的开头
    - `"情感："`：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As a result, the output contains a classification label from the list we have
    provided in the instructions, and it is a correct one!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，输出包含了我们在指令中提供的列表中的一个分类标签，而且是正确的！
- en: You may notice that in addition to the prompt, we pass a `max_new_tokens` parameter.
    It controls the number of tokens the model shall generate, and it is one of the
    many text generation parameters that you can learn about in [Text generation strategies](../generation_strategies)
    guide.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能注意到，除了提示之外，我们还传递了一个`max_new_tokens`参数。它控制模型应该生成的标记数量，这是您可以在[文本生成策略](../generation_strategies)指南中了解的许多文本生成参数之一。
- en: Named Entity Recognition
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: 'Named Entity Recognition (NER) is a task of finding named entities in a piece
    of text, such as a person, location, or organization. Let’s modify the instructions
    in the prompt to make the LLM perform this task. Here, let’s also set `return_full_text
    = False` so that output doesn’t contain the prompt:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）是在文本中找到命名实体的任务，比如人物、地点或组织。让我们修改提示中的指令，让LLM执行这个任务。在这里，我们还设置`return_full_text
    = False`，这样输出就不包含提示了：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, the model correctly identified two named entities from the given
    text.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，模型正确识别了给定文本中的两个命名实体。
- en: Translation
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 翻译
- en: 'Another task LLMs can perform is translation. You can choose to use encoder-decoder
    models for this task, however, here, for the simplicity of the examples, we’ll
    keep using Falcon-7b-instruct, which does a decent job. Once again, here’s how
    you can write a basic prompt to instruct a model to translate a piece of text
    from English to Italian:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: LLM可以执行的另一个任务是翻译。您可以选择使用编码器-解码器模型来执行此任务，但是在这里，为了简化示例，我们将继续使用Falcon-7b-instruct，它做得相当不错。再次，这是您如何编写一个基本提示，指示模型将一段文本从英语翻译成意大利语：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here we’ve added a `do_sample=True` and `top_k=10` to allow the model to be
    a bit more flexible when generating output.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们添加了`do_sample=True`和`top_k=10`，以允许模型在生成输出时更加灵活。
- en: Text summarization
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 文本摘要
- en: Similar to the translation, text summarization is another generative task where
    the output **heavily** relies on the input, and encoder-decoder models can be
    a better choice. However, decoder-style models can be used for this task as well.
    Previously, we have placed the instructions at the very beginning of the prompt.
    However, the very end of the prompt can also be a suitable location for instructions.
    Typically, it’s better to place the instruction on one of the extreme ends.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 与翻译类似，文本摘要是另一个生成任务，输出**严重**依赖于输入，编码器-解码器模型可能是更好的选择。然而，解码器风格的模型也可以用于这个任务。以前，我们将指令放在提示的开头。然而，提示的最后也可以是一个合适的位置来放置指令。通常，最好将指令放在两端之一。
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Question answering
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问答
- en: 'For question answering task we can structure the prompt into the following
    logical components: instructions, context, question, and the leading word or phrase
    (`"Answer:"`) to nudge the model to start generating the answer:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于问答任务，我们可以将提示结构化为以下逻辑组件：指令、上下文、问题和引导词或短语（`"Answer:"`），以促使模型开始生成答案：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Reasoning
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理
- en: Reasoning is one of the most difficult tasks for LLMs, and achieving good results
    often requires applying advanced prompting techniques, like [Chain-of-though](#chain-of-thought).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 推理是LLM中最困难的任务之一，要取得良好的结果通常需要应用高级提示技术，比如[思维链](#chain-of-thought)。
- en: 'Let’s try if we can make a model reason about a simple arithmetics task with
    a basic prompt:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试看看我们是否可以让模型通过一个基本提示来推理一个简单的算术任务：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Correct! Let’s increase the complexity a little and see if we can still get
    away with a basic prompt:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正确！让我们稍微增加一点复杂性，看看我们是否仍然可以通过一个基本提示来完成：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This is a wrong answer, it should be 12\. In this case, this can be due to the
    prompt being too basic, or due to the choice of model, after all we’ve picked
    the smallest version of Falcon. Reasoning is difficult for models of all sizes,
    but larger models are likely to perform better.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个错误答案，应该是12。在这种情况下，这可能是因为提示过于基础，或者是因为模型选择不当，毕竟我们选择了Falcon的最小版本。对于所有大小的模型来说，推理都是困难的，但更大的模型可能表现更好。
- en: Best practices of LLM prompting
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM提示的最佳实践
- en: 'In this section of the guide we have compiled a list of best practices that
    tend to improve the prompt results:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本指南的这一部分中，我们编制了一份倾向于改善提示结果的最佳实践清单：
- en: When choosing the model to work with, the latest and most capable models are
    likely to perform better.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选择要使用的模型时，最新和最有能力的模型可能表现更好。
- en: Start with a simple and short prompt, and iterate from there.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一个简单而短的提示开始，然后逐步迭代。
- en: Put the instructions at the beginning of the prompt, or at the very end. When
    working with large context, models apply various optimizations to prevent Attention
    complexity from scaling quadratically. This may make a model more attentive to
    the beginning or end of a prompt than the middle.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将指令放在提示的开头或最后。在处理大量上下文时，模型会应用各种优化措施，以防止注意力复杂度呈二次方增长。这可能会使模型更加关注提示的开头或结尾，而不是中间部分。
- en: Clearly separate instructions from the text they apply to - more on this in
    the next section.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将指令与其适用的文本清晰分开-更多内容请参见下一节。
- en: Be specific and descriptive about the task and the desired outcome - its format,
    length, style, language, etc.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对任务和期望结果进行具体和描述性的说明-其格式、长度、风格、语言等。
- en: Avoid ambiguous descriptions and instructions.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免模棱两可的描述和指令。
- en: Favor instructions that say “what to do” instead of those that say “what not
    to do”.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更倾向于说“要做什么”而不是说“不要做什么”的指令。
- en: “Lead” the output in the right direction by writing the first word (or even
    begin the first sentence for the model).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过编写第一个单词（甚至开始第一个句子）来“引导”输出朝着正确方向发展。
- en: Use advanced techniques like [Few-shot prompting](#few-shot-prompting) and [Chain-of-thought](#chain-of-thought)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用高级技术，如[少样本提示](#少样本提示)和[思维链](#思维链)
- en: Test your prompts with different models to assess their robustness.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同模型测试您的提示，以评估其稳健性。
- en: Version and track the performance of your prompts.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本和跟踪提示的性能。
- en: Advanced prompting techniques
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级提示技术
- en: Few-shot prompting
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 少样本提示
- en: The basic prompts in the sections above are the examples of “zero-shot” prompts,
    meaning, the model has been given instructions and context, but no examples with
    solutions. LLMs that have been fine-tuned on instruction datasets, generally perform
    well on such “zero-shot” tasks. However, you may find that your task has more
    complexity or nuance, and, perhaps, you have some requirements for the output
    that the model doesn’t catch on just from the instructions. In this case, you
    can try the technique called few-shot prompting.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 上述部分的基本提示是“零样本”提示的示例，这意味着模型已经获得了指令和上下文，但没有带有解决方案的示例。通常在指令数据集上进行微调的LLM在这种“零样本”任务上表现良好。然而，您可能会发现您的任务更加复杂或微妙，也许您对模型没有从指令中捕捉到的输出有一些要求。在这种情况下，您可以尝试称为少样本提示的技术。
- en: In few-shot prompting, we provide examples in the prompt giving the model more
    context to improve the performance. The examples condition the model to generate
    the output following the patterns in the examples.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在少样本提示中，我们在提示中提供示例，为模型提供更多上下文以提高性能。这些示例会让模型生成遵循示例模式的输出。
- en: 'Here’s an example:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the above code snippet we used a single example to demonstrate the desired
    output to the model, so this can be called a “one-shot” prompting. However, depending
    on the task complexity you may need to use more than one example.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码片段中，我们使用了一个示例来向模型展示所需的输出，因此这可以称为“一次性”提示。然而，根据任务的复杂性，您可能需要使用多个示例。
- en: 'Limitations of the few-shot prompting technique:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本提示技术的局限性：
- en: While LLMs can pick up on the patterns in the examples, these technique doesn’t
    work well on complex reasoning tasks
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然LLM可以捕捉到示例中的模式，但这些技术在复杂的推理任务上效果不佳
- en: Few-shot prompting requires creating lengthy prompts. Prompts with large number
    of tokens can increase computation and latency. There’s also a limit to the length
    of the prompts.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 少样本提示需要创建较长的提示。具有大量标记的提示可能会增加计算和延迟。提示的长度也有限制。
- en: Sometimes when given a number of examples, models can learn patterns that you
    didn’t intend them to learn, e.g. that the third movie review is always negative.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时，当给定多个示例时，模型可能会学习您并非打算让它学习的模式，例如第三个电影评论总是负面的。
- en: Chain-of-thought
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 思维链
- en: Chain-of-thought (CoT) prompting is a technique that nudges a model to produce
    intermediate reasoning steps thus improving the results on complex reasoning tasks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 思维链（CoT）提示是一种技术，它促使模型产生中间推理步骤，从而提高复杂推理任务的结果。
- en: 'There are two ways of steering a model to producing the reasoning steps:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以引导模型产生推理步骤：
- en: few-shot prompting by illustrating examples with detailed answers to questions,
    showing the model how to work through a problem.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过用详细答案说明示例来进行少样本提示，向模型展示如何解决问题。
- en: by instructing the model to reason by adding phrases like “Let’s think step
    by step” or “Take a deep breath and work through the problem step by step.”
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过添加短语，如“让我们一步一步地思考”或“深呼吸，一步一步地解决问题”，指导模型进行推理。
- en: 'If we apply the CoT technique to the muffins example from the [reasoning section](#reasoning)
    and use a larger model, such as (`tiiuae/falcon-180B-chat`) which you can play
    with in the [HuggingChat](https://huggingface.co/chat/), we’ll get a significant
    improvement on the reasoning result:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将CoT技术应用于[推理部分](#推理)中的松饼示例，并使用更大的模型，例如（`tiiuae/falcon-180B-chat`），您可以在[HuggingChat](https://huggingface.co/chat/)中尝试，我们将在推理结果上获得显著的改进：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Prompting vs fine-tuning
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示 vs 微调
- en: 'You can achieve great results by optimizing your prompts, however, you may
    still ponder whether fine-tuning a model would work better for your case. Here
    are some scenarios when fine-tuning a smaller model may be a preferred option:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过优化您的提示，您可以取得出色的结果，但是您可能仍然在考虑是否微调模型对您的情况更有效。以下是一些微调较小模型可能是首选的情况：
- en: Your domain is wildly different from what LLMs were pre-trained on and extensive
    prompt optimization did not yield sufficient results.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的领域与LLMs预先训练的领域大相径庭，广泛的提示优化并未产生足够的结果。
- en: You need your model to work well in a low-resource language.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要您的模型在资源稀缺的语言中表现良好。
- en: You need the model to be trained on sensitive data that is under strict regulations.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要训练模型的数据是受严格监管的敏感数据。
- en: You have to use a small model due to cost, privacy, infrastructure or other
    limitations.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于成本、隐私、基础设施或其他限制，您必须使用小型模型。
- en: In all of the above examples, you will need to make sure that you either already
    have or can easily obtain a large enough domain-specific dataset at a reasonable
    cost to fine-tune a model. You will also need to have enough time and resources
    to fine-tune a model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述所有示例中，您需要确保您已经拥有或可以轻松获得足够大的领域特定数据集，以合理的成本来微调模型。您还需要有足够的时间和资源来微调模型。
- en: If the above examples are not the case for you, optimizing prompts can prove
    to be more beneficial.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果上述示例不适用于您，优化提示可能会更有益。
