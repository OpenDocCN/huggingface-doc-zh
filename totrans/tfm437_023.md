# 翻译

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/translation](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/translation)

[https://www.youtube-nocookie.com/embed/1JvfrvZgi6c](https://www.youtube-nocookie.com/embed/1JvfrvZgi6c)

翻译将一个语言的文本序列转换为另一种语言。它是您可以将其制定为序列到序列问题的几个任务之一，这是一个从输入返回某些输出的强大框架，如翻译或摘要。翻译系统通常用于不同语言文本之间的翻译，但也可以用于语音或文本到语音或语音到文本之间的某种组合。

本指南将向您展示如何：

1.  在[OPUS Books](https://huggingface.co/datasets/opus_books)数据集的英语-法语子集上微调[T5](https://huggingface.co/t5-small)以将英语文本翻译成法语。

1.  使用您微调的模型进行推理。

本教程中演示的任务由以下模型架构支持：

[BART](../model_doc/bart), [BigBird-Pegasus](../model_doc/bigbird_pegasus), [Blenderbot](../model_doc/blenderbot), [BlenderbotSmall](../model_doc/blenderbot-small), [Encoder decoder](../model_doc/encoder-decoder), [FairSeq Machine-Translation](../model_doc/fsmt), [GPTSAN-japanese](../model_doc/gptsan-japanese), [LED](../model_doc/led), [LongT5](../model_doc/longt5), [M2M100](../model_doc/m2m_100), [Marian](../model_doc/marian), [mBART](../model_doc/mbart), [MT5](../model_doc/mt5), [MVP](../model_doc/mvp), [NLLB](../model_doc/nllb), [NLLB-MOE](../model_doc/nllb-moe), [Pegasus](../model_doc/pegasus), [PEGASUS-X](../model_doc/pegasus_x), [PLBart](../model_doc/plbart), [ProphetNet](../model_doc/prophetnet), [SeamlessM4T](../model_doc/seamless_m4t), [SeamlessM4Tv2](../model_doc/seamless_m4t_v2), [SwitchTransformers](../model_doc/switch_transformers), [T5](../model_doc/t5), [UMT5](../model_doc/umt5), [XLM-ProphetNet](../model_doc/xlm-prophetnet)

在开始之前，请确保您已安装所有必要的库：

```py
pip install transformers datasets evaluate sacrebleu
```

我们鼓励您登录您的Hugging Face帐户，这样您就可以上传和与社区共享您的模型。在提示时，输入您的令牌以登录：

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## 加载OPUS Books数据集

首先加载🤗数据集库中[OPUS Books](https://huggingface.co/datasets/opus_books)数据集的英语-法语子集：

```py
>>> from datasets import load_dataset

>>> books = load_dataset("opus_books", "en-fr")
```

使用[train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)方法将数据集分割为训练集和测试集：

```py
>>> books = books["train"].train_test_split(test_size=0.2)
```

然后看一个例子：

```py
>>> books["train"][0]
{'id': '90560',
 'translation': {'en': 'But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.',
  'fr': 'Mais ce plateau élevé ne mesurait que quelques toises, et bientôt nous fûmes rentrés dans notre élément.'}}
```

`translation`：文本的英语和法语翻译。

## 预处理

[https://www.youtube-nocookie.com/embed/XAR8jnZZuUs](https://www.youtube-nocookie.com/embed/XAR8jnZZuUs)

下一步是加载T5标记器以处理英语-法语语言对：

```py
>>> from transformers import AutoTokenizer

>>> checkpoint = "t5-small"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

您要创建的预处理函数需要：

1.  在输入前加上提示，以便T5知道这是一个翻译任务。一些能够执行多个NLP任务的模型需要为特定任务提供提示。

1.  将输入（英语）和目标（法语）分别进行标记化，因为无法使用在英语词汇上预训练的标记器对法语文本进行标记化。

1.  将序列截断为`max_length`参数设置的最大长度。

```py
>>> source_lang = "en"
>>> target_lang = "fr"
>>> prefix = "translate English to French: "

>>> def preprocess_function(examples):
...     inputs = [prefix + example[source_lang] for example in examples["translation"]]
...     targets = [example[target_lang] for example in examples["translation"]]
...     model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)
...     return model_inputs
```

要在整个数据集上应用预处理函数，请使用🤗数据集[map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)方法。您可以通过设置`batched=True`来加速`map`函数，以一次处理数据集的多个元素：

```py
>>> tokenized_books = books.map(preprocess_function, batched=True)
```

现在使用[DataCollatorForSeq2Seq](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq)创建一批示例。在整理过程中，将句子动态填充到批次中的最长长度，而不是将整个数据集填充到最大长度，这样更有效。

Pytorch隐藏Pytorch内容

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```

TensorFlow隐藏TensorFlow内容

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors="tf")
```

## 评估

在训练过程中包含一个指标通常有助于评估模型的性能。您可以使用🤗[Evaluate](https://huggingface.co/docs/evaluate/index)库快速加载评估方法。对于这个任务，加载[SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu)指标（查看🤗Evaluate [快速入门](https://huggingface.co/docs/evaluate/a_quick_tour)以了解如何加载和计算指标）：

```py
>>> import evaluate

>>> metric = evaluate.load("sacrebleu")
```

然后创建一个函数，将您的预测和标签传递给`compute`以计算SacreBLEU分数：

```py
>>> import numpy as np

>>> def postprocess_text(preds, labels):
...     preds = [pred.strip() for pred in preds]
...     labels = [[label.strip()] for label in labels]

...     return preds, labels

>>> def compute_metrics(eval_preds):
...     preds, labels = eval_preds
...     if isinstance(preds, tuple):
...         preds = preds[0]
...     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

...     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

...     result = metric.compute(predictions=decoded_preds, references=decoded_labels)
...     result = {"bleu": result["score"]}

...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
...     result["gen_len"] = np.mean(prediction_lens)
...     result = {k: round(v, 4) for k, v in result.items()}
...     return result
```

您的`compute_metrics`函数现在已经准备就绪，当您设置训练时会返回到它。

## 训练

Pytorch隐藏Pytorch内容

如果您不熟悉如何使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)微调模型，请查看这里的基本教程！

现在您已经准备好开始训练您的模型了！使用[AutoModelForSeq2SeqLM](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM)加载T5：

```py
>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

此时，只剩下三个步骤：

1.  在[Seq2SeqTrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments)中定义您的训练超参数。唯一必需的参数是`output_dir`，指定保存模型的位置。通过设置`push_to_hub=True`将此模型推送到Hub（您需要登录Hugging Face才能上传模型）。在每个epoch结束时，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)将评估SacreBLEU指标并保存训练检查点。

1.  将训练参数传递给[Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)，同时还包括模型、数据集、分词器、数据整理器和`compute_metrics`函数。

1.  调用[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)来微调您的模型。

```py
>>> training_args = Seq2SeqTrainingArguments(
...     output_dir="my_awesome_opus_books_model",
...     evaluation_strategy="epoch",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     weight_decay=0.01,
...     save_total_limit=3,
...     num_train_epochs=2,
...     predict_with_generate=True,
...     fp16=True,
...     push_to_hub=True,
... )

>>> trainer = Seq2SeqTrainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_books["train"],
...     eval_dataset=tokenized_books["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

训练完成后，使用[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)方法将模型共享到Hub，以便每个人都可以使用您的模型：

```py
>>> trainer.push_to_hub()
```

TensorFlow隐藏TensorFlow内容

如果您不熟悉如何使用Keras微调模型，请查看这里的基本教程！

要在TensorFlow中微调模型，请首先设置优化器函数、学习率调度和一些训练超参数：

```py
>>> from transformers import AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

然后您可以使用[TFAutoModelForSeq2SeqLM](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForSeq2SeqLM)加载T5：

```py
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

使用[prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset)将数据集转换为`tf.data.Dataset`格式：

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_books["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     tokenized_books["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

使用[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)配置模型进行训练。请注意，Transformers模型都有一个默认的与任务相关的损失函数，因此除非您想要指定一个，否则不需要：

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)  # No loss argument!
```

在开始训练之前，还有最后两件事要设置：从预测中计算SacreBLEU指标，并提供一种将模型推送到Hub的方法。这两个都可以通过使用[Keras回调](../main_classes/keras_callbacks)来完成。

将您的`compute_metrics`函数传递给[KerasMetricCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.KerasMetricCallback)：

```py
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)
```

指定将模型和分词器推送到[PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback)的位置：

```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_opus_books_model",
...     tokenizer=tokenizer,
... )
```

然后将您的回调捆绑在一起：

```py
>>> callbacks = [metric_callback, push_to_hub_callback]
```

最后，你已经准备好开始训练你的模型了！调用[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)与你的训练和验证数据集，时代的数量，以及你的回调来微调模型：

```py
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)
```

一旦训练完成，你的模型会自动上传到Hub，这样每个人都可以使用它！

有关如何为翻译微调模型的更深入示例，请查看相应的[PyTorch笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)或[TensorFlow笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)。

## 推理

很好，现在你已经微调了一个模型，你可以用它进行推理！

想出一些你想要翻译成另一种语言的文本。对于T5，你需要根据你正在处理的任务来添加前缀。对于从英语翻译成法语，你应该像下面所示添加前缀：

```py
>>> text = "translate English to French: Legumes share resources with nitrogen-fixing bacteria."
```

尝试使用微调模型进行推理的最简单方法是在[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)中使用它。为翻译实例化一个`pipeline`与您的模型，并将文本传递给它：

```py
>>> from transformers import pipeline

>>> translator = pipeline("translation", model="my_awesome_opus_books_model")
>>> translator(text)
[{'translation_text': 'Legumes partagent des ressources avec des bactéries azotantes.'}]
```

如果你愿意，你也可以手动复制`pipeline`的结果：

Pytorch隐藏Pytorch内容

对文本进行标记化，并将`input_ids`返回为PyTorch张量：

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_opus_books_model")
>>> inputs = tokenizer(text, return_tensors="pt").input_ids
```

使用[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)方法创建翻译。有关不同文本生成策略和控制生成的参数的更多详细信息，请查看[Text Generation](../main_classes/text_generation) API。

```py
>>> from transformers import AutoModelForSeq2SeqLM

>>> model = AutoModelForSeq2SeqLM.from_pretrained("my_awesome_opus_books_model")
>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)
```

将生成的标记ID解码回文本：

```py
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'Les lignées partagent des ressources avec des bactéries enfixant l'azote.'
```

TensorFlow隐藏TensorFlow内容

对文本进行标记化，并将`input_ids`返回为TensorFlow张量：

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_opus_books_model")
>>> inputs = tokenizer(text, return_tensors="tf").input_ids
```

使用[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin.generate)方法创建翻译。有关不同文本生成策略和控制生成的参数的更多详细信息，请查看[Text Generation](../main_classes/text_generation) API。

```py
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained("my_awesome_opus_books_model")
>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)
```

将生成的标记ID解码回文本：

```py
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'Les lugumes partagent les ressources avec des bactéries fixatrices d'azote.'
```
