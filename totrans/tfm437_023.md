# ÁøªËØë

> ÂéüÂßãÊñáÊú¨Ôºö[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/translation](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/translation)

[https://www.youtube-nocookie.com/embed/1JvfrvZgi6c](https://www.youtube-nocookie.com/embed/1JvfrvZgi6c)

ÁøªËØëÂ∞Ü‰∏Ä‰∏™ËØ≠Ë®ÄÁöÑÊñáÊú¨Â∫èÂàóËΩ¨Êç¢‰∏∫Âè¶‰∏ÄÁßçËØ≠Ë®Ä„ÄÇÂÆÉÊòØÊÇ®ÂèØ‰ª•Â∞ÜÂÖ∂Âà∂ÂÆö‰∏∫Â∫èÂàóÂà∞Â∫èÂàóÈóÆÈ¢òÁöÑÂá†‰∏™‰ªªÂä°‰πã‰∏ÄÔºåËøôÊòØ‰∏Ä‰∏™‰ªéËæìÂÖ•ËøîÂõûÊüê‰∫õËæìÂá∫ÁöÑÂº∫Â§ßÊ°ÜÊû∂ÔºåÂ¶ÇÁøªËØëÊàñÊëòË¶Å„ÄÇÁøªËØëÁ≥ªÁªüÈÄöÂ∏∏Áî®‰∫é‰∏çÂêåËØ≠Ë®ÄÊñáÊú¨‰πãÈó¥ÁöÑÁøªËØëÔºå‰ΩÜ‰πüÂèØ‰ª•Áî®‰∫éËØ≠Èü≥ÊàñÊñáÊú¨Âà∞ËØ≠Èü≥ÊàñËØ≠Èü≥Âà∞ÊñáÊú¨‰πãÈó¥ÁöÑÊüêÁßçÁªÑÂêà„ÄÇ

Êú¨ÊåáÂçóÂ∞ÜÂêëÊÇ®Â±ïÁ§∫Â¶Ç‰ΩïÔºö

1.  Âú®[OPUS Books](https://huggingface.co/datasets/opus_books)Êï∞ÊçÆÈõÜÁöÑËã±ËØ≠-Ê≥ïËØ≠Â≠êÈõÜ‰∏äÂæÆË∞É[T5](https://huggingface.co/t5-small)‰ª•Â∞ÜËã±ËØ≠ÊñáÊú¨ÁøªËØëÊàêÊ≥ïËØ≠„ÄÇ

1.  ‰ΩøÁî®ÊÇ®ÂæÆË∞ÉÁöÑÊ®°ÂûãËøõË°åÊé®ÁêÜ„ÄÇ

Êú¨ÊïôÁ®ã‰∏≠ÊºîÁ§∫ÁöÑ‰ªªÂä°Áî±‰ª•‰∏ãÊ®°ÂûãÊû∂ÊûÑÊîØÊåÅÔºö

[BART](../model_doc/bart), [BigBird-Pegasus](../model_doc/bigbird_pegasus), [Blenderbot](../model_doc/blenderbot), [BlenderbotSmall](../model_doc/blenderbot-small), [Encoder decoder](../model_doc/encoder-decoder), [FairSeq Machine-Translation](../model_doc/fsmt), [GPTSAN-japanese](../model_doc/gptsan-japanese), [LED](../model_doc/led), [LongT5](../model_doc/longt5), [M2M100](../model_doc/m2m_100), [Marian](../model_doc/marian), [mBART](../model_doc/mbart), [MT5](../model_doc/mt5), [MVP](../model_doc/mvp), [NLLB](../model_doc/nllb), [NLLB-MOE](../model_doc/nllb-moe), [Pegasus](../model_doc/pegasus), [PEGASUS-X](../model_doc/pegasus_x), [PLBart](../model_doc/plbart), [ProphetNet](../model_doc/prophetnet), [SeamlessM4T](../model_doc/seamless_m4t), [SeamlessM4Tv2](../model_doc/seamless_m4t_v2), [SwitchTransformers](../model_doc/switch_transformers), [T5](../model_doc/t5), [UMT5](../model_doc/umt5), [XLM-ProphetNet](../model_doc/xlm-prophetnet)

Âú®ÂºÄÂßã‰πãÂâçÔºåËØ∑Á°Æ‰øùÊÇ®Â∑≤ÂÆâË£ÖÊâÄÊúâÂøÖË¶ÅÁöÑÂ∫ìÔºö

```py
pip install transformers datasets evaluate sacrebleu
```

Êàë‰ª¨ÈºìÂä±ÊÇ®ÁôªÂΩïÊÇ®ÁöÑHugging FaceÂ∏êÊà∑ÔºåËøôÊ†∑ÊÇ®Â∞±ÂèØ‰ª•‰∏ä‰º†Âíå‰∏éÁ§æÂå∫ÂÖ±‰∫´ÊÇ®ÁöÑÊ®°Âûã„ÄÇÂú®ÊèêÁ§∫Êó∂ÔºåËæìÂÖ•ÊÇ®ÁöÑ‰ª§Áâå‰ª•ÁôªÂΩïÔºö

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## Âä†ËΩΩOPUS BooksÊï∞ÊçÆÈõÜ

È¶ñÂÖàÂä†ËΩΩü§óÊï∞ÊçÆÈõÜÂ∫ì‰∏≠[OPUS Books](https://huggingface.co/datasets/opus_books)Êï∞ÊçÆÈõÜÁöÑËã±ËØ≠-Ê≥ïËØ≠Â≠êÈõÜÔºö

```py
>>> from datasets import load_dataset

>>> books = load_dataset("opus_books", "en-fr")
```

‰ΩøÁî®[train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)ÊñπÊ≥ïÂ∞ÜÊï∞ÊçÆÈõÜÂàÜÂâ≤‰∏∫ËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜÔºö

```py
>>> books = books["train"].train_test_split(test_size=0.2)
```

ÁÑ∂ÂêéÁúã‰∏Ä‰∏™‰æãÂ≠êÔºö

```py
>>> books["train"][0]
{'id': '90560',
 'translation': {'en': 'But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.',
  'fr': 'Mais ce plateau √©lev√© ne mesurait que quelques toises, et bient√¥t nous f√ªmes rentr√©s dans notre √©l√©ment.'}}
```

`translation`ÔºöÊñáÊú¨ÁöÑËã±ËØ≠ÂíåÊ≥ïËØ≠ÁøªËØë„ÄÇ

## È¢ÑÂ§ÑÁêÜ

[https://www.youtube-nocookie.com/embed/XAR8jnZZuUs](https://www.youtube-nocookie.com/embed/XAR8jnZZuUs)

‰∏ã‰∏ÄÊ≠•ÊòØÂä†ËΩΩT5Ê†áËÆ∞Âô®‰ª•Â§ÑÁêÜËã±ËØ≠-Ê≥ïËØ≠ËØ≠Ë®ÄÂØπÔºö

```py
>>> from transformers import AutoTokenizer

>>> checkpoint = "t5-small"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

ÊÇ®Ë¶ÅÂàõÂª∫ÁöÑÈ¢ÑÂ§ÑÁêÜÂáΩÊï∞ÈúÄË¶ÅÔºö

1.  Âú®ËæìÂÖ•ÂâçÂä†‰∏äÊèêÁ§∫Ôºå‰ª•‰æøT5Áü•ÈÅìËøôÊòØ‰∏Ä‰∏™ÁøªËØë‰ªªÂä°„ÄÇ‰∏Ä‰∫õËÉΩÂ§üÊâßË°åÂ§ö‰∏™NLP‰ªªÂä°ÁöÑÊ®°ÂûãÈúÄË¶Å‰∏∫ÁâπÂÆö‰ªªÂä°Êèê‰æõÊèêÁ§∫„ÄÇ

1.  Â∞ÜËæìÂÖ•ÔºàËã±ËØ≠ÔºâÂíåÁõÆÊ†áÔºàÊ≥ïËØ≠ÔºâÂàÜÂà´ËøõË°åÊ†áËÆ∞ÂåñÔºåÂõ†‰∏∫Êó†Ê≥ï‰ΩøÁî®Âú®Ëã±ËØ≠ËØçÊ±á‰∏äÈ¢ÑËÆ≠ÁªÉÁöÑÊ†áËÆ∞Âô®ÂØπÊ≥ïËØ≠ÊñáÊú¨ËøõË°åÊ†áËÆ∞Âåñ„ÄÇ

1.  Â∞ÜÂ∫èÂàóÊà™Êñ≠‰∏∫`max_length`ÂèÇÊï∞ËÆæÁΩÆÁöÑÊúÄÂ§ßÈïøÂ∫¶„ÄÇ

```py
>>> source_lang = "en"
>>> target_lang = "fr"
>>> prefix = "translate English to French: "

>>> def preprocess_function(examples):
...     inputs = [prefix + example[source_lang] for example in examples["translation"]]
...     targets = [example[target_lang] for example in examples["translation"]]
...     model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)
...     return model_inputs
```

Ë¶ÅÂú®Êï¥‰∏™Êï∞ÊçÆÈõÜ‰∏äÂ∫îÁî®È¢ÑÂ§ÑÁêÜÂáΩÊï∞ÔºåËØ∑‰ΩøÁî®ü§óÊï∞ÊçÆÈõÜ[map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)ÊñπÊ≥ï„ÄÇÊÇ®ÂèØ‰ª•ÈÄöËøáËÆæÁΩÆ`batched=True`Êù•Âä†ÈÄü`map`ÂáΩÊï∞Ôºå‰ª•‰∏ÄÊ¨°Â§ÑÁêÜÊï∞ÊçÆÈõÜÁöÑÂ§ö‰∏™ÂÖÉÁ¥†Ôºö

```py
>>> tokenized_books = books.map(preprocess_function, batched=True)
```

Áé∞Âú®‰ΩøÁî®[DataCollatorForSeq2Seq](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq)ÂàõÂª∫‰∏ÄÊâπÁ§∫‰æã„ÄÇÂú®Êï¥ÁêÜËøáÁ®ã‰∏≠ÔºåÂ∞ÜÂè•Â≠êÂä®ÊÄÅÂ°´ÂÖÖÂà∞ÊâπÊ¨°‰∏≠ÁöÑÊúÄÈïøÈïøÂ∫¶ÔºåËÄå‰∏çÊòØÂ∞ÜÊï¥‰∏™Êï∞ÊçÆÈõÜÂ°´ÂÖÖÂà∞ÊúÄÂ§ßÈïøÂ∫¶ÔºåËøôÊ†∑Êõ¥ÊúâÊïà„ÄÇ

PytorchÈöêËóèPytorchÂÜÖÂÆπ

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```

TensorFlowÈöêËóèTensorFlowÂÜÖÂÆπ

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors="tf")
```

## ËØÑ‰º∞

Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂåÖÂê´‰∏Ä‰∏™ÊåáÊ†áÈÄöÂ∏∏ÊúâÂä©‰∫éËØÑ‰º∞Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÊÇ®ÂèØ‰ª•‰ΩøÁî®ü§ó[Evaluate](https://huggingface.co/docs/evaluate/index)Â∫ìÂø´ÈÄüÂä†ËΩΩËØÑ‰º∞ÊñπÊ≥ï„ÄÇÂØπ‰∫éËøô‰∏™‰ªªÂä°ÔºåÂä†ËΩΩ[SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu)ÊåáÊ†áÔºàÊü•Áúãü§óEvaluate [Âø´ÈÄüÂÖ•Èó®](https://huggingface.co/docs/evaluate/a_quick_tour)‰ª•‰∫ÜËß£Â¶Ç‰ΩïÂä†ËΩΩÂíåËÆ°ÁÆóÊåáÊ†áÔºâÔºö

```py
>>> import evaluate

>>> metric = evaluate.load("sacrebleu")
```

ÁÑ∂ÂêéÂàõÂª∫‰∏Ä‰∏™ÂáΩÊï∞ÔºåÂ∞ÜÊÇ®ÁöÑÈ¢ÑÊµãÂíåÊ†áÁ≠æ‰º†ÈÄíÁªô`compute`‰ª•ËÆ°ÁÆóSacreBLEUÂàÜÊï∞Ôºö

```py
>>> import numpy as np

>>> def postprocess_text(preds, labels):
...     preds = [pred.strip() for pred in preds]
...     labels = [[label.strip()] for label in labels]

...     return preds, labels

>>> def compute_metrics(eval_preds):
...     preds, labels = eval_preds
...     if isinstance(preds, tuple):
...         preds = preds[0]
...     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

...     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

...     result = metric.compute(predictions=decoded_preds, references=decoded_labels)
...     result = {"bleu": result["score"]}

...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
...     result["gen_len"] = np.mean(prediction_lens)
...     result = {k: round(v, 4) for k, v in result.items()}
...     return result
```

ÊÇ®ÁöÑ`compute_metrics`ÂáΩÊï∞Áé∞Âú®Â∑≤ÁªèÂáÜÂ§áÂ∞±Áª™ÔºåÂΩìÊÇ®ËÆæÁΩÆËÆ≠ÁªÉÊó∂‰ºöËøîÂõûÂà∞ÂÆÉ„ÄÇ

## ËÆ≠ÁªÉ

PytorchÈöêËóèPytorchÂÜÖÂÆπ

Â¶ÇÊûúÊÇ®‰∏çÁÜüÊÇâÂ¶Ç‰Ωï‰ΩøÁî®[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ÂæÆË∞ÉÊ®°ÂûãÔºåËØ∑Êü•ÁúãËøôÈáåÁöÑÂü∫Êú¨ÊïôÁ®ãÔºÅ

Áé∞Âú®ÊÇ®Â∑≤ÁªèÂáÜÂ§áÂ•ΩÂºÄÂßãËÆ≠ÁªÉÊÇ®ÁöÑÊ®°Âûã‰∫ÜÔºÅ‰ΩøÁî®[AutoModelForSeq2SeqLM](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM)Âä†ËΩΩT5Ôºö

```py
>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

Ê≠§Êó∂ÔºåÂè™Ââ©‰∏ã‰∏â‰∏™Ê≠•È™§Ôºö

1.  Âú®[Seq2SeqTrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments)‰∏≠ÂÆö‰πâÊÇ®ÁöÑËÆ≠ÁªÉË∂ÖÂèÇÊï∞„ÄÇÂîØ‰∏ÄÂøÖÈúÄÁöÑÂèÇÊï∞ÊòØ`output_dir`ÔºåÊåáÂÆö‰øùÂ≠òÊ®°ÂûãÁöÑ‰ΩçÁΩÆ„ÄÇÈÄöËøáËÆæÁΩÆ`push_to_hub=True`Â∞ÜÊ≠§Ê®°ÂûãÊé®ÈÄÅÂà∞HubÔºàÊÇ®ÈúÄË¶ÅÁôªÂΩïHugging FaceÊâçËÉΩ‰∏ä‰º†Ê®°ÂûãÔºâ„ÄÇÂú®ÊØè‰∏™epochÁªìÊùüÊó∂Ôºå[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)Â∞ÜËØÑ‰º∞SacreBLEUÊåáÊ†áÂπ∂‰øùÂ≠òËÆ≠ÁªÉÊ£ÄÊü•ÁÇπ„ÄÇ

1.  Â∞ÜËÆ≠ÁªÉÂèÇÊï∞‰º†ÈÄíÁªô[Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)ÔºåÂêåÊó∂ËøòÂåÖÊã¨Ê®°Âûã„ÄÅÊï∞ÊçÆÈõÜ„ÄÅÂàÜËØçÂô®„ÄÅÊï∞ÊçÆÊï¥ÁêÜÂô®Âíå`compute_metrics`ÂáΩÊï∞„ÄÇ

1.  Ë∞ÉÁî®[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)Êù•ÂæÆË∞ÉÊÇ®ÁöÑÊ®°Âûã„ÄÇ

```py
>>> training_args = Seq2SeqTrainingArguments(
...     output_dir="my_awesome_opus_books_model",
...     evaluation_strategy="epoch",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     weight_decay=0.01,
...     save_total_limit=3,
...     num_train_epochs=2,
...     predict_with_generate=True,
...     fp16=True,
...     push_to_hub=True,
... )

>>> trainer = Seq2SeqTrainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_books["train"],
...     eval_dataset=tokenized_books["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

ËÆ≠ÁªÉÂÆåÊàêÂêéÔºå‰ΩøÁî®[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)ÊñπÊ≥ïÂ∞ÜÊ®°ÂûãÂÖ±‰∫´Âà∞HubÔºå‰ª•‰æøÊØè‰∏™‰∫∫ÈÉΩÂèØ‰ª•‰ΩøÁî®ÊÇ®ÁöÑÊ®°ÂûãÔºö

```py
>>> trainer.push_to_hub()
```

TensorFlowÈöêËóèTensorFlowÂÜÖÂÆπ

Â¶ÇÊûúÊÇ®‰∏çÁÜüÊÇâÂ¶Ç‰Ωï‰ΩøÁî®KerasÂæÆË∞ÉÊ®°ÂûãÔºåËØ∑Êü•ÁúãËøôÈáåÁöÑÂü∫Êú¨ÊïôÁ®ãÔºÅ

Ë¶ÅÂú®TensorFlow‰∏≠ÂæÆË∞ÉÊ®°ÂûãÔºåËØ∑È¶ñÂÖàËÆæÁΩÆ‰ºòÂåñÂô®ÂáΩÊï∞„ÄÅÂ≠¶‰π†ÁéáË∞ÉÂ∫¶Âíå‰∏Ä‰∫õËÆ≠ÁªÉË∂ÖÂèÇÊï∞Ôºö

```py
>>> from transformers import AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

ÁÑ∂ÂêéÊÇ®ÂèØ‰ª•‰ΩøÁî®[TFAutoModelForSeq2SeqLM](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForSeq2SeqLM)Âä†ËΩΩT5Ôºö

```py
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

‰ΩøÁî®[prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset)Â∞ÜÊï∞ÊçÆÈõÜËΩ¨Êç¢‰∏∫`tf.data.Dataset`Ê†ºÂºèÔºö

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_books["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     tokenized_books["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

‰ΩøÁî®[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)ÈÖçÁΩÆÊ®°ÂûãËøõË°åËÆ≠ÁªÉ„ÄÇËØ∑Ê≥®ÊÑèÔºåTransformersÊ®°ÂûãÈÉΩÊúâ‰∏Ä‰∏™ÈªòËÆ§ÁöÑ‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÂõ†Ê≠§Èô§ÈùûÊÇ®ÊÉ≥Ë¶ÅÊåáÂÆö‰∏Ä‰∏™ÔºåÂê¶Âàô‰∏çÈúÄË¶ÅÔºö

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)  # No loss argument!
```

Âú®ÂºÄÂßãËÆ≠ÁªÉ‰πãÂâçÔºåËøòÊúâÊúÄÂêé‰∏§‰ª∂‰∫ãË¶ÅËÆæÁΩÆÔºö‰ªéÈ¢ÑÊµã‰∏≠ËÆ°ÁÆóSacreBLEUÊåáÊ†áÔºåÂπ∂Êèê‰æõ‰∏ÄÁßçÂ∞ÜÊ®°ÂûãÊé®ÈÄÅÂà∞HubÁöÑÊñπÊ≥ï„ÄÇËøô‰∏§‰∏™ÈÉΩÂèØ‰ª•ÈÄöËøá‰ΩøÁî®[KerasÂõûË∞É](../main_classes/keras_callbacks)Êù•ÂÆåÊàê„ÄÇ

Â∞ÜÊÇ®ÁöÑ`compute_metrics`ÂáΩÊï∞‰º†ÈÄíÁªô[KerasMetricCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.KerasMetricCallback)Ôºö

```py
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)
```

ÊåáÂÆöÂ∞ÜÊ®°ÂûãÂíåÂàÜËØçÂô®Êé®ÈÄÅÂà∞[PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback)ÁöÑ‰ΩçÁΩÆÔºö

```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_opus_books_model",
...     tokenizer=tokenizer,
... )
```

ÁÑ∂ÂêéÂ∞ÜÊÇ®ÁöÑÂõûË∞ÉÊçÜÁªëÂú®‰∏ÄËµ∑Ôºö

```py
>>> callbacks = [metric_callback, push_to_hub_callback]
```

ÊúÄÂêéÔºå‰Ω†Â∑≤ÁªèÂáÜÂ§áÂ•ΩÂºÄÂßãËÆ≠ÁªÉ‰Ω†ÁöÑÊ®°Âûã‰∫ÜÔºÅË∞ÉÁî®[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)‰∏é‰Ω†ÁöÑËÆ≠ÁªÉÂíåÈ™åËØÅÊï∞ÊçÆÈõÜÔºåÊó∂‰ª£ÁöÑÊï∞ÈáèÔºå‰ª•Âèä‰Ω†ÁöÑÂõûË∞ÉÊù•ÂæÆË∞ÉÊ®°ÂûãÔºö

```py
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)
```

‰∏ÄÊó¶ËÆ≠ÁªÉÂÆåÊàêÔºå‰Ω†ÁöÑÊ®°Âûã‰ºöËá™Âä®‰∏ä‰º†Âà∞HubÔºåËøôÊ†∑ÊØè‰∏™‰∫∫ÈÉΩÂèØ‰ª•‰ΩøÁî®ÂÆÉÔºÅ

ÊúâÂÖ≥Â¶Ç‰Ωï‰∏∫ÁøªËØëÂæÆË∞ÉÊ®°ÂûãÁöÑÊõ¥Ê∑±ÂÖ•Á§∫‰æãÔºåËØ∑Êü•ÁúãÁõ∏Â∫îÁöÑ[PyTorchÁ¨îËÆ∞Êú¨](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)Êàñ[TensorFlowÁ¨îËÆ∞Êú¨](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)„ÄÇ

## Êé®ÁêÜ

ÂæàÂ•ΩÔºåÁé∞Âú®‰Ω†Â∑≤ÁªèÂæÆË∞É‰∫Ü‰∏Ä‰∏™Ê®°ÂûãÔºå‰Ω†ÂèØ‰ª•Áî®ÂÆÉËøõË°åÊé®ÁêÜÔºÅ

ÊÉ≥Âá∫‰∏Ä‰∫õ‰Ω†ÊÉ≥Ë¶ÅÁøªËØëÊàêÂè¶‰∏ÄÁßçËØ≠Ë®ÄÁöÑÊñáÊú¨„ÄÇÂØπ‰∫éT5Ôºå‰Ω†ÈúÄË¶ÅÊ†πÊçÆ‰Ω†Ê≠£Âú®Â§ÑÁêÜÁöÑ‰ªªÂä°Êù•Ê∑ªÂä†ÂâçÁºÄ„ÄÇÂØπ‰∫é‰ªéËã±ËØ≠ÁøªËØëÊàêÊ≥ïËØ≠Ôºå‰Ω†Â∫îËØ•ÂÉè‰∏ãÈù¢ÊâÄÁ§∫Ê∑ªÂä†ÂâçÁºÄÔºö

```py
>>> text = "translate English to French: Legumes share resources with nitrogen-fixing bacteria."
```

Â∞ùËØï‰ΩøÁî®ÂæÆË∞ÉÊ®°ÂûãËøõË°åÊé®ÁêÜÁöÑÊúÄÁÆÄÂçïÊñπÊ≥ïÊòØÂú®[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)‰∏≠‰ΩøÁî®ÂÆÉ„ÄÇ‰∏∫ÁøªËØëÂÆû‰æãÂåñ‰∏Ä‰∏™`pipeline`‰∏éÊÇ®ÁöÑÊ®°ÂûãÔºåÂπ∂Â∞ÜÊñáÊú¨‰º†ÈÄíÁªôÂÆÉÔºö

```py
>>> from transformers import pipeline

>>> translator = pipeline("translation", model="my_awesome_opus_books_model")
>>> translator(text)
[{'translation_text': 'Legumes partagent des ressources avec des bact√©ries azotantes.'}]
```

Â¶ÇÊûú‰Ω†ÊÑøÊÑèÔºå‰Ω†‰πüÂèØ‰ª•ÊâãÂä®Â§çÂà∂`pipeline`ÁöÑÁªìÊûúÔºö

PytorchÈöêËóèPytorchÂÜÖÂÆπ

ÂØπÊñáÊú¨ËøõË°åÊ†áËÆ∞ÂåñÔºåÂπ∂Â∞Ü`input_ids`ËøîÂõû‰∏∫PyTorchÂº†ÈáèÔºö

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_opus_books_model")
>>> inputs = tokenizer(text, return_tensors="pt").input_ids
```

‰ΩøÁî®[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)ÊñπÊ≥ïÂàõÂª∫ÁøªËØë„ÄÇÊúâÂÖ≥‰∏çÂêåÊñáÊú¨ÁîüÊàêÁ≠ñÁï•ÂíåÊéßÂà∂ÁîüÊàêÁöÑÂèÇÊï∞ÁöÑÊõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑Êü•Áúã[Text Generation](../main_classes/text_generation) API„ÄÇ

```py
>>> from transformers import AutoModelForSeq2SeqLM

>>> model = AutoModelForSeq2SeqLM.from_pretrained("my_awesome_opus_books_model")
>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)
```

Â∞ÜÁîüÊàêÁöÑÊ†áËÆ∞IDËß£Á†ÅÂõûÊñáÊú¨Ôºö

```py
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'Les lign√©es partagent des ressources avec des bact√©ries enfixant l'azote.'
```

TensorFlowÈöêËóèTensorFlowÂÜÖÂÆπ

ÂØπÊñáÊú¨ËøõË°åÊ†áËÆ∞ÂåñÔºåÂπ∂Â∞Ü`input_ids`ËøîÂõû‰∏∫TensorFlowÂº†ÈáèÔºö

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_opus_books_model")
>>> inputs = tokenizer(text, return_tensors="tf").input_ids
```

‰ΩøÁî®[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin.generate)ÊñπÊ≥ïÂàõÂª∫ÁøªËØë„ÄÇÊúâÂÖ≥‰∏çÂêåÊñáÊú¨ÁîüÊàêÁ≠ñÁï•ÂíåÊéßÂà∂ÁîüÊàêÁöÑÂèÇÊï∞ÁöÑÊõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑Êü•Áúã[Text Generation](../main_classes/text_generation) API„ÄÇ

```py
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained("my_awesome_opus_books_model")
>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)
```

Â∞ÜÁîüÊàêÁöÑÊ†áËÆ∞IDËß£Á†ÅÂõûÊñáÊú¨Ôºö

```py
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'Les lugumes partagent les ressources avec des bact√©ries fixatrices d'azote.'
```
