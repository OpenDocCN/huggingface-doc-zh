- en: Iterative Trainer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/trl/iterative_sft_trainer](https://huggingface.co/docs/trl/iterative_sft_trainer)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Iterative fine-tuning is a training method that enables to perform custom actions
    (generation and filtering for example) between optimization steps. In TRL we provide
    an easy-to-use API to fine-tune your models in an iterative way in just a few
    lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get started quickly, instantiate an instance a model, and a tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You have the choice to either provide a list of strings or a list of tensors
    to the step function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a list of tensors as input:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Using a list of strings as input:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For causal language models, labels will automatically be created from input_ids
    or from texts. When using sequence to sequence models you will have to provide
    your own labels or text_labels.
  prefs: []
  type: TYPE_NORMAL
- en: IterativeTrainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class trl.IterativeSFTTrainer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/iterative_sft_trainer.py#L39)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '*`*model**` (`PreTrainedModel`) — Model to be optimized, either an ‘AutoModelForCausalLM’
    or an ‘AutoModelForSeq2SeqLM’. — Check the documentation of `PreTrainedModel`
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*args**` (`transformers.TrainingArguments`) — — The arguments to use for
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*tokenizer**` (`PreTrainedTokenizerBase`) — Tokenizer to be used for encoding
    the — data. Check the documentation of `transformers.PreTrainedTokenizer` and
    `transformers.PreTrainedTokenizerFast` for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*optimizers**` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`)
    — — The optimizer and scheduler to use for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*data_collator**` (Union[DataCollatorForLanguageModeling, DataCollatorForSeq2Seq],
    *optional*) — Data collator to be used for training and — passed along the dataloader.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*eval_dataset**` (`datasets.Dataset`) — The dataset to use for evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*max_length**` (`int`, defaults to `None`) — — The maximum length of the
    input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*truncation_mode**` (`str`, defaults to `keep_end`) — — The truncation mode
    to use, either `keep_end` or `keep_start`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*preprocess_logits_for_metrics**` (`Callable[[torch.Tensor, torch.Tensor],
    torch.Tensor]`) — — The function to use to preprocess the logits before computing
    the metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*compute_metrics**` (`Callable[[EvalPrediction], Dict]`, *optional*) — —
    The function to use to compute the metrics. Must take a `EvalPrediction` and return
    a dictionary string to metric values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*optimize_device_cache` * *(`bool`,* optional*, defaults to `False`) — Optimize
    CUDA cache for slightly more memory-efficient training. —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The IterativeSFTTrainer can be used to finetune models with methods that requires
    some steps between optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/iterative_sft_trainer.py#L229)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (List`torch.LongTensor`) — List of tensors containing the input_ids
    (if not provided, text will be used)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (List`torch.LongTensor`, , *optional*) — List of tensors containing
    the attention_mask'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (List`torch.FloatTensor`, *optional*) — List of tensors containing
    the labels (if set to None, will default to input_ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`texts` (List`str`, *optional*) — List of strings containing the text input
    (if not provided, input_ids will directly be used)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`texts_labels` (List`str`, *optional*) — List of strings containing the text
    labels (if set to None, will default to text)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`dict[str, Any]`'
  prefs: []
  type: TYPE_NORMAL
- en: A summary of the training statistics
  prefs: []
  type: TYPE_NORMAL
- en: Run an optimisation step given a list of input_ids, attention_mask, and labels
    or a list of text and text_labels.
  prefs: []
  type: TYPE_NORMAL
