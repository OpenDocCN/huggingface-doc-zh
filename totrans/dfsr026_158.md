# Shap-E

> 原始文本：[`huggingface.co/docs/diffusers/api/pipelines/shap_e`](https://huggingface.co/docs/diffusers/api/pipelines/shap_e)

Shap-E 模型是由 Alex Nichol 和 Heewoo Jun 提出的，来自 OpenAI。

该论文的摘要为：

*我们提出了 Shap-E，一个用于 3D 资产的条件生成模型。与最近关于 3D 生成模型的工作不同，这些模型产生单个输出表示，Shap-E 直接生成可以呈现为纹理网格和神经辐射场的隐式函数的参数。我们分两个阶段训练 Shap-E：首先，我们训练一个编码器，将 3D 资产确定性地映射到隐式函数的参数；其次，我们在编码器的输出上训练一个条件扩散模型。当在大型配对的 3D 和文本数据集上训练时，我们得到的模型能够在几秒钟内生成复杂和多样化的 3D 资产。与 Point-E 相比，Point-E 是一个点云上的显式生成模型，Shap-E 收敛更快，尽管对建模更高维度、多表示输出空间，但达到了可比较或更好的样本质量。*

原始代码库可以在[openai/shap-e](https://github.com/openai/shap-e)找到。

查看跨管道重用组件部分，了解如何有效地将相同组件加载到多个管道中。

## ShapEPipeline

### `class diffusers.ShapEPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/shap_e/pipeline_shap_e.py#L79)

```py
( prior: PriorTransformer text_encoder: CLIPTextModelWithProjection tokenizer: CLIPTokenizer scheduler: HeunDiscreteScheduler shap_e_renderer: ShapERenderer )
```

参数

+   `prior`（PriorTransformer）— 用于从文本嵌入近似图像嵌入的规范 unCLIP 先验。

+   `text_encoder`（[CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection)）— 冻结的文本编码器。

+   `tokenizer`（[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)）— 用于对文本进行标记化的`CLIPTokenizer`。

+   `scheduler`（HeunDiscreteScheduler）— 与`prior`模型结合使用的调度器，用于生成图像嵌入。

+   `shap_e_renderer`（`ShapERenderer`）— Shap-E 渲染器将生成的潜变量投影到 MLP 的参数中，以使用 NeRF 渲染方法创建 3D 对象。

用于生成 3D 资产的潜在表示并使用 NeRF 方法进行渲染的管道。

该模型继承自 DiffusionPipeline。查看超类文档以了解所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/shap_e/pipeline_shap_e.py#L182)

```py
( prompt: str num_images_per_prompt: int = 1 num_inference_steps: int = 25 generator: Union = None latents: Optional = None guidance_scale: float = 4.0 frame_size: int = 64 output_type: Optional = 'pil' return_dict: bool = True ) → export const metadata = 'undefined';ShapEPipelineOutput or tuple
```

参数

+   `prompt`（`str`或`List[str]`）— 用于指导图像生成的提示或提示。

+   `num_images_per_prompt`（`int`，*可选*，默认为 1）— 每个提示生成的图像数量。

+   `num_inference_steps`（`int`，*可选*，默认为 25）— 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `generator`（`torch.Generator`或`List[torch.Generator]`，*可选*）— 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成的嘈杂潜变量，用作图像生成的输入。可用于使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成一个潜变量张量。

+   `guidance_scale` (`float`, *可选*, 默认为 4.0) — 更高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。

+   `frame_size` (`int`, *可选*, 默认为 64) — 生成的 3D 输出的每个图像帧的宽度和高度。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。在`"pil"`（`PIL.Image.Image`）、`"np"`（`np.array`）、`"latent"`（`torch.Tensor`）或网格（`MeshDecoderOutput`）之间进行选择。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回 ShapEPipelineOutput 而不是普通元组。

返回

ShapEPipelineOutput 或`tuple`

如果`return_dict`为`True`，则返回 ShapEPipelineOutput，否则返回一个`tuple`，其中第一个元素是包含生成图像的列表。

用于生成的管道的调用函数。

示例:

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from diffusers.utils import export_to_gif

>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

>>> repo = "openai/shap-e"
>>> pipe = DiffusionPipeline.from_pretrained(repo, torch_dtype=torch.float16)
>>> pipe = pipe.to(device)

>>> guidance_scale = 15.0
>>> prompt = "a shark"

>>> images = pipe(
...     prompt,
...     guidance_scale=guidance_scale,
...     num_inference_steps=64,
...     frame_size=256,
... ).images

>>> gif_path = export_to_gif(images[0], "shark_3d.gif")
```

## ShapEImg2ImgPipeline

### `class diffusers.ShapEImg2ImgPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/shap_e/pipeline_shap_e_img2img.py#L80)

```py
( prior: PriorTransformer image_encoder: CLIPVisionModel image_processor: CLIPImageProcessor scheduler: HeunDiscreteScheduler shap_e_renderer: ShapERenderer )
```

参数

+   `prior` (PriorTransformer) — 用于从文本嵌入近似图像嵌入的经典 unCLIP 先验。

+   `image_encoder` ([CLIPVisionModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel)) — 冻结的图像编码器。

+   `image_processor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) — 用于处理图像的`CLIPImageProcessor`。

+   `scheduler` (HeunDiscreteScheduler) — 与`prior`模型结合使用以生成图像嵌入的调度器。

+   `shap_e_renderer` (`ShapERenderer`) — Shap-E 渲染器将生成的潜变量投影到 MLP 的参数中，以使用 NeRF 渲染方法创建 3D 对象。

用于生成 3D 资产的潜在表示并使用 NeRF 方法从图像渲染的管道。

此模型继承自 DiffusionPipeline。查看超类文档以获取为所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/shap_e/pipeline_shap_e_img2img.py#L164)

```py
( image: Union num_images_per_prompt: int = 1 num_inference_steps: int = 25 generator: Union = None latents: Optional = None guidance_scale: float = 4.0 frame_size: int = 64 output_type: Optional = 'pil' return_dict: bool = True ) → export const metadata = 'undefined';ShapEPipelineOutput or tuple
```

参数

+   `image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, 或 `List[np.ndarray]`) — 代表要用作起点的图像批次的`Image`或张量。也可以接受图像潜变量作为图像，但如果直接传递潜变量，则不会再次编码。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示要生成的图像数量。

+   `num_inference_steps` (`int`, *可选*, 默认为 25) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `generator` (`torch.Generator`或`List[torch.Generator]`, *可选*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中抽样的预生成噪声潜变量，用作图像生成的输入。可以用来使用不同提示调整相同的生成。如果未提供，则通过使用提供的随机`generator`进行抽样生成潜变量张量。

+   `guidance_scale` (`float`, *可选*, 默认为 4.0) — 更高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。

+   `frame_size` (`int`, *可选*, 默认为 64) — 生成的 3D 输出中每个图像帧的宽度和高度。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。选择在`"pil"` (`PIL.Image.Image`)、`"np"` (`np.array`)、`"latent"` (`torch.Tensor`)或网格 (`MeshDecoderOutput`)之间。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回一个 ShapEPipelineOutput 而不是一个普通的元组。

返回

ShapEPipelineOutput 或 `tuple`

如果`return_dict`为`True`，则返回 ShapEPipelineOutput，否则返回一个元组，其中第一个元素是包含生成图像的列表。

用于生成的管道的调用函数。

示例:

```py
>>> from PIL import Image
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from diffusers.utils import export_to_gif, load_image

>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

>>> repo = "openai/shap-e-img2img"
>>> pipe = DiffusionPipeline.from_pretrained(repo, torch_dtype=torch.float16)
>>> pipe = pipe.to(device)

>>> guidance_scale = 3.0
>>> image_url = "https://hf.co/datasets/diffusers/docs-images/resolve/main/shap-e/corgi.png"
>>> image = load_image(image_url).convert("RGB")

>>> images = pipe(
...     image,
...     guidance_scale=guidance_scale,
...     num_inference_steps=64,
...     frame_size=256,
... ).images

>>> gif_path = export_to_gif(images[0], "corgi_3d.gif")
```

## ShapEPipelineOutput

### `class diffusers.pipelines.shap_e.pipeline_shap_e.ShapEPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/shap_e/pipeline_shap_e.py#L66)

```py
( images: Union )
```

参数

+   `images` (`torch.FloatTensor`) — 用于 3D 渲染的图像列表。

ShapEPipeline 和 ShapEImg2ImgPipeline 的输出类。
