- en: SwitchTransformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SwitchTransformers
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/switch_transformers](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/switch_transformers)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '原文链接: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/switch_transformers](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/switch_transformers)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The SwitchTransformers model was proposed in [Switch Transformers: Scaling
    to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)
    by William Fedus, Barret Zoph, Noam Shazeer.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'SwitchTransformers模型是由William Fedus、Barret Zoph和Noam Shazeer在[Switch Transformers:
    Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)中提出的。'
- en: The Switch Transformer model uses a sparse T5 encoder-decoder architecture,
    where the MLP are replaced by a Mixture of Experts (MoE). A routing mechanism
    (top 1 in this case) associates each token to one of the expert, where each expert
    is a dense MLP. While switch transformers have a lot more weights than their equivalent
    dense models, the sparsity allows better scaling and better finetuning performance
    at scale. During a forward pass, only a fraction of the weights are used. The
    routing mechanism allows the model to select relevant weights on the fly which
    increases the model capacity without increasing the number of operations.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Switch Transformer模型使用了稀疏的T5编码器-解码器架构，其中MLP被Mixture of Experts（MoE）替换。一个路由机制（在这种情况下是前1个）将每个标记与一个专家关联起来，其中每个专家都是一个密集的MLP。虽然开关变压器比其等效的密集模型有更多的权重，但稀疏性允许更好的扩展和更好的规模微调性能。在前向传递过程中，只使用了一小部分权重。路由机制允许模型动态选择相关权重，从而增加模型容量而不增加操作数量。
- en: 'The abstract from the paper is the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*In deep learning, models typically reuse the same parameters for all inputs.
    Mixture of Experts (MoE) defies this and instead selects different parameters
    for each incoming example. The result is a sparsely-activated model — with outrageous
    numbers of parameters — but a constant computational cost. However, despite several
    notable successes of MoE, widespread adoption has been hindered by complexity,
    communication costs and training instability — we address these with the Switch
    Transformer. We simplify the MoE routing algorithm and design intuitive improved
    models with reduced communication and computational costs. Our proposed training
    techniques help wrangle the instabilities and we show large sparse models may
    be trained, for the first time, with lower precision (bfloat16) formats. We design
    models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training
    speed with the same computational resources. These improvements extend into multilingual
    settings where we measure gains over the mT5-Base version across all 101 languages.
    Finally, we advance the current scale of language models by pre-training up to
    trillion parameter models on the “Colossal Clean Crawled Corpus” and achieve a
    4x speedup over the T5-XXL model.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*在深度学习中，模型通常对所有输入重复使用相同的参数。专家混合（MoE）违背了这一点，而是为每个传入的示例选择不同的参数。结果是一个稀疏激活的模型 -
    具有惊人数量的参数 - 但是计算成本恒定。然而，尽管MoE取得了几个显著的成功，但广泛采用受到了复杂性、通信成本和训练不稳定性的阻碍 - 我们通过Switch
    Transformer来解决这些问题。我们简化了MoE路由算法，并设计了直观的改进模型，减少了通信和计算成本。我们提出的训练技术有助于解决不稳定性问题，并且我们展示了大型稀疏模型可以首次使用更低精度（bfloat16）格式进行训练。我们基于T5-Base和T5-Large设计模型，利用相同的计算资源实现了高达7倍的预训练速度提升。这些改进延伸到多语言设置，我们在所有101种语言中测量了对mT5-Base版本的增益。最后，我们通过在“Colossal
    Clean Crawled Corpus”上预训练高达万亿参数模型，将语言模型的当前规模提升，并实现了T5-XXL模型的4倍加速。*'
- en: This model was contributed by [Younes Belkada](https://huggingface.co/ybelkada)
    and [Arthur Zucker](https://huggingface.co/ArthurZ). The original code can be
    found [here](https://github.com/google/flaxformer/tree/main/flaxformer/architectures/moe).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[Younes Belkada](https://huggingface.co/ybelkada)和[Arthur Zucker](https://huggingface.co/ArthurZ)贡献。原始代码可以在[这里](https://github.com/google/flaxformer/tree/main/flaxformer/architectures/moe)找到。
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: SwitchTransformers uses the [T5Tokenizer](/docs/transformers/v4.37.2/en/model_doc/mt5#transformers.T5Tokenizer),
    which can be loaded directly from each model’s repository.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SwitchTransformers使用[T5Tokenizer](/docs/transformers/v4.37.2/en/model_doc/mt5#transformers.T5Tokenizer)，可以直接从每个模型的存储库中加载。
- en: The released weights are pretrained on English [Masked Language Modeling](https://moon-ci-docs.huggingface.co/docs/transformers/pr_19323/en/glossary#general-terms)
    task, and should be finetuned.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布的权重是在英语[遮蔽语言建模](https://moon-ci-docs.huggingface.co/docs/transformers/pr_19323/en/glossary#general-terms)任务上预训练的，应进行微调。
- en: Resources
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Translation task guide](../tasks/translation)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[翻译任务指南](../tasks/translation)'
- en: '[Summarization task guide](../tasks/summarization)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[总结任务指南](../tasks/summarization)'
- en: SwitchTransformersConfig
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SwitchTransformersConfig
- en: '### `class transformers.SwitchTransformersConfig`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SwitchTransformersConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/configuration_switch_transformers.py#L27)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/configuration_switch_transformers.py#L27)'
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 32128) — Vocabulary size of the
    SwitchTransformers model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [SwitchTransformersModel](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersModel).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`（`int`，*可选*，默认为32128） - SwitchTransformers模型的词汇量。定义了在调用[SwitchTransformersModel](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersModel)时可以表示的不同标记的数量。'
- en: '`d_model` (`int`, *optional*, defaults to 768) — Size of the encoder layers
    and the pooler layer.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_model`（`int`，*可选*，默认为768） - 编码器层和池化层的大小。'
- en: '`d_kv` (`int`, *optional*, defaults to 64) — Size of the key, query, value
    projections per attention head. `d_kv` has to be equal to `d_model // num_heads`.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_kv`（`int`，*可选*，默认为64） - 每个注意力头的键、查询、值投影的大小。`d_kv`必须等于`d_model // num_heads`。'
- en: '`d_ff` (`int`, *optional*, defaults to 2048) — Size of the intermediate feed
    forward layer in each `SwitchTransformersBlock`.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_ff` (`int`, *optional*, defaults to 2048) — 每个`SwitchTransformersBlock`中间级前馈层的大小。'
- en: '`expert_capacity` (`int`, *optional*, defaults to 64) — Number of tokens that
    can be stored in each expert. If set to 1, the model will behave like a regular
    Transformer.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`expert_capacity` (`int`, *optional*, defaults to 64) — 每个专家可以存储的令牌数量。如果设置为1，则模型将表现得像一个常规Transformer。'
- en: '`num_layers` (`int`, *optional*, defaults to 12) — Number of dense hidden layers
    in the Transformer encoder layer.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers` (`int`, *optional*, defaults to 12) — Transformer编码器层中的稠密隐藏层数量。'
- en: '`num_sparse_encoder_layers` (`int`, *optional*, defaults to 3) — Number of
    sparse (MoE) dense hidden layers in the Transformer encoder layer.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_sparse_encoder_layers` (`int`, *optional*, defaults to 3) — Transformer编码器层中稀疏（MoE）稠密隐藏层的数量。'
- en: '`num_decoder_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer decoder. Will use the same value as `num_layers` if
    not set.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_decoder_layers` (`int`, *optional*, defaults to 12) — Transformer解码器中的隐藏层数量。如果未设置，将使用与`num_layers`相同的值。'
- en: '`num_sparse_decoder_layers` (`int`, *optional*, defaults to 3) — Number of
    sparse (MoE) dense hidden layers in the Transformer decoder layer.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_sparse_decoder_layers` (`int`, *optional*, defaults to 3) — Transformer解码器层中稀疏（MoE）稠密隐藏层的数量。'
- en: '`num_heads` (`int`, *optional*, defaults to 12) — Number of attention heads
    for each attention layer in the Transformer encoder.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_heads` (`int`, *optional*, defaults to 12) — 每个注意力层中的注意力头数。'
- en: '`num_experts` (`int`, *optional*, defaults to 8) — Number of experts for each
    SwitchTransformer layer.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_experts` (`int`, *optional*, defaults to 8) — 每个SwitchTransformer层的专家数量。'
- en: '`router_bias` (`bool`, *optional*, defaults to `False`) — Whether to add a
    bias to the router.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_bias` (`bool`, *optional*, defaults to `False`) — 是否向路由器添加偏置。'
- en: '`router_jitter_noise` (`float`, *optional*, defaults to 0.01) — Amount of noise
    to add to the router.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_jitter_noise` (`float`, *optional*, defaults to 0.01) — 添加到路由器的噪音量。'
- en: '`router_dtype` (`str`, *optional*, default to `"float32"`) — The `dtype` used
    for the routers. It is preferable to keep the `dtype` to `"float32"` as specified
    in the *selective precision* discussion in [the paper](https://arxiv.org/abs/2101.03961).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_dtype` (`str`, *optional*, default to `"float32"`) — 用于路由器的`dtype`。最好将`dtype`保持为`"float32"`，如[论文](https://arxiv.org/abs/2101.03961)中的*选择性精度*讨论中所指定的。'
- en: '`router_ignore_padding_tokens` (`bool`, *optional*, defaults to `False`) —
    Whether to ignore padding tokens when routing.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_ignore_padding_tokens` (`bool`, *optional*, defaults to `False`) —
    在路由时是否忽略填充标记。'
- en: '`relative_attention_num_buckets` (`int`, *optional*, defaults to 32) — The
    number of buckets to use for each attention layer.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`relative_attention_num_buckets` (`int`, *optional*, defaults to 32) — 每个注意力层使用的桶数。'
- en: '`relative_attention_max_distance` (`int`, *optional*, defaults to 128) — The
    maximum distance of the longer sequences for the bucket separation.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`relative_attention_max_distance` (`int`, *optional*, defaults to 128) — 用于桶分离的较长序列的最大距离。'
- en: '`dropout_rate` (`float`, *optional*, defaults to 0.1) — The ratio for all dropout
    layers.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout_rate` (`float`, *optional*, defaults to 0.1) — 所有dropout层的比率。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-6) — The epsilon used
    by the layer normalization layers.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-6) — 层归一化层使用的epsilon。'
- en: '`router_z_loss_coef` (`float`, *optional*, defaults to 0.001) — The z loss
    factor for the total loss.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_z_loss_coef` (`float`, *optional*, defaults to 0.001) — 总损失的z损失因子。'
- en: '`router_aux_loss_coef` (`float`, *optional*, defaults to 0.001) — The aux loss
    factor for the total loss.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_aux_loss_coef` (`float`, *optional*, defaults to 0.001) — 总损失的辅助损失因子。'
- en: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — A factor for
    initializing all weight matrices (should be kept to 1, used internally for initialization
    testing).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — 初始化所有权重矩阵的因子（应保持为1，用于内部初始化测试）。'
- en: '`dense_act_fn` (`string`, *optional*, defaults to `"relu"`) — Type of feed
    forward layer to be used. Should be one of `"relu"` or `"gated-gelu"`. SwitchTransformersv1.1
    uses the `"gated-gelu"` feed forward projection. Original SwitchTransformers uses
    `"relu"`.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dense_act_fn` (`string`, *optional*, defaults to `"relu"`) — 要使用的前馈层类型。应为`"relu"`或`"gated-gelu"`之一。SwitchTransformersv1.1使用`"gated-gelu"`前馈投影。原始SwitchTransformers使用`"relu"`。'
- en: '`add_router_probs` (`bool`, *optional*, defaults to `False`) — Whether to output
    router probabilities to compute router auxiliary loss.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_router_probs` (`bool`, *optional*, defaults to `False`) — 是否输出路由器概率以计算路由器辅助损失。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。'
- en: This is the configuration class to store the configuration of a [SwitchTransformersModel](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersModel).
    It is used to instantiate a SwitchTransformers model according to the specified
    arguments, defining the model architecture. Instantiating a configuration with
    the defaults will yield a similar configuration to that of the SwitchTransformers
    [google/switch-base-8](https://huggingface.co/google/switch-base-8) architecture.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[SwitchTransformersModel](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersModel)的配置。根据指定的参数实例化SwitchTransformers模型，定义模型架构。使用默认值实例化配置将产生类似于SwitchTransformers
    [google/switch-base-8](https://huggingface.co/google/switch-base-8)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: SwitchTransformersTop1Router
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SwitchTransformersTop1Router
- en: '### `class transformers.SwitchTransformersTop1Router`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SwitchTransformersTop1Router`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L130)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L130)'
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Router using tokens choose top-1 experts assignment.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用令牌选择顶级专家分配的路由器。
- en: 'This router uses the same mechanism as in Switch Transformer ([https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961))
    and V-MoE ([https://arxiv.org/abs/2106.05974](https://arxiv.org/abs/2106.05974)):
    tokens choose their top experts. Items are sorted by router_probs and then routed
    to their choice of expert until the expert’s expert_capacity is reached. **There
    is no guarantee that each token is processed by an expert**, or that each expert
    receives at least one token.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 该路由器使用与Switch Transformer ([https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961))和V-MoE
    ([https://arxiv.org/abs/2106.05974](https://arxiv.org/abs/2106.05974))相同的机制：令牌选择其顶级专家。项目按`router_probs`排序，然后路由到其选择的专家，直到达到专家的`expert_capacity`。**不能保证每个令牌都由专家处理**，也不能保证每个专家至少收到一个令牌。
- en: '#### `_compute_router_probabilities`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `_compute_router_probabilities`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L150)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L150)'
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_states` (`torch.Tensor`) — (batch_size, sequence_length, hidden_dim)
    from which router probabilities are computed.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`torch.Tensor`) — (batch_size, sequence_length, hidden_dim)
    从中计算路由器概率的隐藏状态。'
- en: Returns
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: router_probabilities (`torch.Tensor`)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: router_probabilities (`torch.Tensor`)
- en: 'Tensor of shape (batch_size, sequence_length, num_experts) corresponding to
    the probabilities for each token and expert. Used for routing tokens to experts.
    router_logits (`torch.Tensor`): Logits tensor of shape (batch_size, sequence_length,
    num_experts) corresponding to raw router logits. This is used later for computing
    router z-loss.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '形状为(batch_size, sequence_length, num_experts)的张量，对应于每个令牌和专家的概率。用于将令牌路由到专家。router_logits
    (`torch.Tensor`): 形状为(batch_size, sequence_length, num_experts)的对应原始路由器logits的对数张量。稍后用于计算路由器z-loss。'
- en: Computes router probabilities from input hidden states.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入隐藏状态计算路由器概率。
- en: '#### `forward`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L191)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L191)'
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_states` (`torch.Tensor`) — [num_groups, tokens_per_group, hidden_dim]
    inputs to send to experts.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`torch.Tensor`) — [num_groups, tokens_per_group, hidden_dim]
    发送给专家的输入。'
- en: Generic forward function for every Router class. Each Router expects to have
    the same input hidden states (`hidden_states`) corresponding to the hidden states
    for each token, the `expert_capacity` corresponding to the number of tokens the
    Router will send to each expert, some Routers can send up to few tokens to each
    expert.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每个路由器类的通用前向函数。每个路由器都期望具有相同的输入隐藏状态(`hidden_states`)，对应于每个令牌的隐藏状态，`expert_capacity`对应于路由器将发送到每个专家的令牌数量，一些路由器可以将少量令牌发送给每个专家。
- en: 'Each Router works as the following: it expects the hidden states for each token,
    gets the `router_probs` and `router_logits` from the `router_weights`. This will
    assign for each token, the raw probability to be assigned to an expert. Then each
    Router class will have to define its own `_compute_routing_instructions`.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 每个路由器的工作方式如下：它期望每个令牌的隐藏状态，从`router_weights`获取`router_probs`和`router_logits`。这将为每个令牌分配原始概率以分配给专家。然后每个路由器类将不得不定义自己的`_compute_routing_instructions`。
- en: SwitchTransformersSparseMLP
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SwitchTransformersSparseMLP
- en: '### `class transformers.SwitchTransformersSparseMLP`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SwitchTransformersSparseMLP`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L275)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L275)'
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Implementation of the Switch Transformers Sparse MLP module.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Switch Transformers Sparse MLP模块的实现。
- en: '#### `forward`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L290)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L290)'
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Hold on, this will be slightly tricky to understand In the correct order, a
    MoE layer does the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 稍等，这可能有点难以理解。按正确顺序，MoE层执行以下操作：
- en: '1- Gets the `router_mask` from the router. The shape of the mask is `(batch_size,
    sequence_length, num_expert)` and corresponds to the argmax of the `router_probs`.
    The probabilities are needed in the computation of the hidden states : they are
    broadcasted to the hidden states values (can be interpreted as a scaling factor).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 1- 从路由器获取`router_mask`。掩码的形状为(batch_size, sequence_length, num_expert)，对应于`router_probs`的argmax。在计算隐藏状态时需要这些概率：它们被广播到隐藏状态值（可以解释为缩放因子）。
- en: 2- Dispatch the tokens to its associated experts. We do a classic for loop over
    the experts and assign for each expert the corresponding hidden states.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 2- 将令牌分派给其关联的专家。我们对专家进行经典的for循环，并为每个专家分配相应的隐藏状态。
- en: SwitchTransformersModel
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SwitchTransformersModel
- en: '### `class transformers.SwitchTransformersModel`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SwitchTransformersModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L1287)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L1287)'
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([SwitchTransformersConfig](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[SwitchTransformersConfig](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare SWITCH_TRANSFORMERS Model transformer outputting raw hidden-states
    without any specific head on top.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的SWITCH_TRANSFORMERS模型变压器输出原始隐藏状态，没有特定的头部。
- en: 'The SWITCH_TRANSFORMERS model was proposed in [Switch Transformers: Scaling
    to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)
    by [William Fedus](https://arxiv.org/search/cs?searchtype=author&query=Fedus%2C+W),
    [Barret Zoph](https://arxiv.org/search/cs?searchtype=author&query=Zoph%2C+B),
    and [Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&query=Shazeer%2C+N).
    It’s an encoder-decoder T5-like model with sparse Feed Forward that stands for
    Mixture of Experts (MoE) architecture.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 'SWITCH_TRANSFORMERS模型是由[Switch Transformers: Scaling to Trillion Parameter
    Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)中提出的，作者是[William
    Fedus](https://arxiv.org/search/cs?searchtype=author&query=Fedus%2C+W)、[Barret
    Zoph](https://arxiv.org/search/cs?searchtype=author&query=Zoph%2C+B)和[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&query=Shazeer%2C+N)。它是一个编码器-解码器T5样式的模型，具有稀疏前馈，代表专家混合（MoE）架构。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L1342)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L1342)'
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. SWITCH_TRANSFORMERS is a model
    with relative position embeddings so you should be able to pad the inputs on both
    the right and the left.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。SWITCH_TRANSFORMERS是一个具有相对位置嵌入的模型，因此您应该能够在右侧和左侧填充输入。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for detail.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: To know more on how to prepare `input_ids` for pretraining take a look a [SWITCH_TRANSFORMERS
    Training](./switch_transformers#training).
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要了解有关如何为预训练准备`input_ids`的更多信息，请查看[SWITCH_TRANSFORMERS Training](./switch_transformers#training)。
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于避免在填充标记索引上执行注意力的掩码。选择在`[0, 1]`中的掩码值：'
- en: 1 for tokens that are `not masked`,
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记为1。
- en: 0 for tokens that are `masked`.
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）-
    词汇表中解码器输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是解码器输入ID？](../glossary#decoder-input-ids)'
- en: SWITCH_TRANSFORMERS uses the `pad_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SWITCH_TRANSFORMERS使用`pad_token_id`作为`decoder_input_ids`生成的起始标记。如果使用`past_key_values`，则可选择仅输入最后的`decoder_input_ids`（请参阅`past_key_values`）。
- en: To know more on how to prepare `decoder_input_ids` for pretraining take a look
    at [SWITCH_TRANSFORMERS Training](./switch_transformers#training).
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要了解有关如何为预训练准备`decoder_input_ids`的更多信息，请查看[SWITCH_TRANSFORMERS Training](./switch_transformers#training)。
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）-
    默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules in
    the encoder. Mask values selected in `[0, 1]`:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）-
    用于使编码器中自注意力模块的选定头部失效的掩码。掩码值选定在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部`未被掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`掩码`。
- en: '`decoder_head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules in the decoder. Mask values selected in `[0, 1]`:'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）-
    用于在解码器中使自注意力模块的选定头部失效的掩码。掩码值选定在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部`未被掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`掩码`。
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the cross-attention
    modules in the decoder. Mask values selected in `[0, 1]`:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.Tensor`，*可选*）-
    用于使解码器中交叉注意力模块的选定头部失效的掩码。掩码值选定在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部`未被掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`掩码`。
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, `optional`: *hidden_states*, `optional`: *attentions*)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` is a
    sequence of hidden states at the output of the last layer of the encoder. Used
    in the cross-attention of the decoder.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）- 元组包括（`last_hidden_state`，*可选*：*hidden_states*，*可选*：*attentions*）`last_hidden_state`的形状为`(batch_size,
    sequence_length, hidden_size)`，是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden states
    of the attention blocks. Can be used to speed up decoding.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（长度为`config.n_layers`的`tuple(tuple(torch.FloatTensor))`，每个元组包含4个形状为`(batch_size,
    num_heads, sequence_length - 1, embed_size_per_head)`的张量）- 包含注意力块的预计算键和值隐藏状态。可用于加速解码。'
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，用户可以选择仅输入形状为`(batch_size, 1)`的最后一个`decoder_input_ids`（这些没有将它们的过去键值状态提供给此模型）而不是形状为`(batch_size,
    sequence_length)`的所有`decoder_input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds`（形状为`(batch_size, target_sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    可选地，可以直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用了`past_key_values`，则只需输入最后一个`decoder_inputs_embeds`（参见`past_key_values`）。如果您想要更多控制权，以便将`decoder_input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵。'
- en: If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds`
    takes the value of `inputs_embeds`.
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*）- 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`output_router_logits` (`bool`, *optional*) — Whether or not to return the
    logits of all the routers. They are useful for computing the router loss, and
    should not be returned during inference.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_router_logits`（`bool`，*可选*）- 是否返回所有路由器的对数。它们对于计算路由器损失很有用，在推理期间不应返回。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是一个普通元组。'
- en: Returns
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.modeling_outputs.Seq2SeqMoEModelOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_outputs.Seq2SeqMoEModelOutput` 或 `tuple(torch.FloatTensor)`'
- en: A `transformers.modeling_outputs.Seq2SeqMoEModelOutput` or a tuple of `torch.FloatTensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration ([SwitchTransformersConfig](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig))
    and inputs.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `transformers.modeling_outputs.Seq2SeqMoEModelOutput` 或一个 `torch.FloatTensor`
    元组（如果传递 `return_dict=False` 或 `config.return_dict=False`）包含根据配置（[SwitchTransformersConfig](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig)）和输入的不同元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the decoder of the model.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — 模型解码器最后一层的隐藏状态序列。'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果仅使用 `past_key_values`，则输出形状为 `(batch_size, 1, hidden_size)` 的序列的最后一个隐藏状态。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递 `use_cache=True`
    或 `config.use_cache=True` 时返回) — 长度为 `config.n_layers` 的 `tuple(torch.FloatTensor)`
    元组，每个元组有两个形状为 `(batch_size, num_heads, sequence_length, embed_size_per_head)`
    的张量，以及两个额外的形状为 `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`
    的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见 `past_key_values` 输入）。
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_hidden_states=True`
    或 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组。'
- en: Hidden-states of the decoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器每一层的隐藏状态以及可选的初始嵌入输出。
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_attentions=True`
    或 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`decoder_router_logits` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_router_logits=True` is passed or when `config.add_router_probs=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length,
    num_experts)`.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_router_logits` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_router_logits=True`
    或 `config.add_router_probs=True` 时返回) — 形状为 `(batch_size, sequence_length, num_experts)`
    的 `torch.FloatTensor` 元组（每层一个）。'
- en: Router logits of the decoder model, useful to compute the auxiliary loss for
    Mixture of Experts models.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器模型的路由器logits，用于计算混合专家模型的辅助损失。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_attentions=True`
    或 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_hidden_states=True`
    或 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组。'
- en: Hidden-states of the encoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器每一层的隐藏状态以及可选的初始嵌入输出。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`encoder_router_logits` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_router_logits=True` is passed or when `config.add_router_probs=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length,
    num_experts)`.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_router_logits` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_router_logits=True`或`config.add_router_probs=True`时返回)
    — 形状为`(batch_size, sequence_length, num_experts)`的`torch.FloatTensor`元组（每层一个）。'
- en: Router logits of the encoder model, useful to compute the auxiliary loss and
    the z_loss for the sparse modules.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器模型的路由器logits，用于计算辅助损失和稀疏模块的z_loss。
- en: The [SwitchTransformersModel](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersModel)
    forward method, overrides the `__call__` special method.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[SwitchTransformersModel](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersModel)的forward方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: SwitchTransformersForConditionalGeneration
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SwitchTransformersForConditionalGeneration
- en: '### `class transformers.SwitchTransformersForConditionalGeneration`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SwitchTransformersForConditionalGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L1461)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L1461)'
- en: '[PRE9]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([SwitchTransformersConfig](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([SwitchTransformersConfig](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: SWITCH_TRANSFORMERS Model with a `language modeling` head on top.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: SWITCH_TRANSFORMERS模型在顶部有一个`语言建模`头。
- en: 'The SWITCH_TRANSFORMERS model was proposed in [Switch Transformers: Scaling
    to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)
    by [William Fedus](https://arxiv.org/search/cs?searchtype=author&query=Fedus%2C+W),
    [Barret Zoph](https://arxiv.org/search/cs?searchtype=author&query=Zoph%2C+B),
    and [Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&query=Shazeer%2C+N).
    It’s an encoder-decoder T5-like model with sparse Feed Forward that stands for
    Mixture of Experts (MoE) architecture.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 'SWITCH_TRANSFORMERS模型是由[William Fedus](https://arxiv.org/search/cs?searchtype=author&query=Fedus%2C+W)、[Barret
    Zoph](https://arxiv.org/search/cs?searchtype=author&query=Zoph%2C+B)和[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&query=Shazeer%2C+N)提出的，其在[Switch
    Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)中描述。它是一种具有稀疏前馈的编码器-解码器T5-like模型，代表着专家混合（MoE）架构。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L1521)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L1521)'
- en: '[PRE10]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. SWITCH_TRANSFORMERS is a model
    with relative position embeddings so you should be able to pad the inputs on both
    the right and the left.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。SWITCH_TRANSFORMERS是一个具有相对位置嵌入的模型，因此您应该能够在右侧和左侧都填充输入。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for detail.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)来获取索引。详细信息请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: To know more on how to prepare `input_ids` for pretraining take a look a [SWITCH_TRANSFORMERS
    Training](./switch_transformers#training).
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要了解有关如何为预训练准备 `input_ids` 的更多信息，请查看 [SWITCH_TRANSFORMERS Training](./switch_transformers#training)。
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示未被 `掩码` 的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被 `掩码` 的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — 词汇表中解码器输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是解码器输入ID？](../glossary#decoder-input-ids)'
- en: SWITCH_TRANSFORMERS uses the `pad_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SWITCH_TRANSFORMERS 使用 `pad_token_id` 作为 `decoder_input_ids` 生成的起始标记。如果使用 `past_key_values`，则可以选择仅输入最后的
    `decoder_input_ids`（参见 `past_key_values`）。
- en: To know more on how to prepare `decoder_input_ids` for pretraining take a look
    at [SWITCH_TRANSFORMERS Training](./switch_transformers#training).
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要了解有关如何为预训练准备 `decoder_input_ids` 的更多信息，请查看 [SWITCH_TRANSFORMERS Training](./switch_transformers#training)。
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — 默认行为：生成一个张量，忽略 `decoder_input_ids` 中的填充标记。因果掩码也将默认使用。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules in
    the encoder. Mask values selected in `[0, 1]`:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使编码器中的自注意力模块中的选定头部失效的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被 `掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被 `掩码`。
- en: '`decoder_head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules in the decoder. Mask values selected in `[0, 1]`:'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — 用于使解码器中的自注意力模块中的选定头部失效的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被 `掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被 `掩码`。
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the cross-attention
    modules in the decoder. Mask values selected in `[0, 1]`:'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask` (`torch.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — 用于使解码器中的交叉注意力模块中的选定头部失效的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被 `掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被 `掩码`。
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, `optional`: *hidden_states*, `optional`: *attentions*)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` is a
    sequence of hidden states at the output of the last layer of the encoder. Used
    in the cross-attention of the decoder.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组由 (`last_hidden_state`,
    `optional`: *hidden_states*, `optional`: *attentions*) 组成，`last_hidden_state`
    的形状为 `(batch_size, sequence_length, hidden_size)`，是编码器最后一层输出的隐藏状态序列。在解码器的交叉注意力中使用。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden states
    of the attention blocks. Can be used to speed up decoding.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。'
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用 `past_key_values`，用户可以选择仅输入最后的 `decoder_input_ids`（即那些没有将其过去键值状态提供给此模型的）的形状为
    `(batch_size, 1)`，而不是形状为 `(batch_size, sequence_length)` 的所有 `decoder_input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示而不是传递 `input_ids`。如果您想要更多控制权来将 `input_ids`
    索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, target_sequence_length,
    hidden_size)`，*可选*) — 可选地，可以直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用`past_key_values`，则只需输入最后的`decoder_inputs_embeds`（参见`past_key_values`）。如果要更好地控制如何将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds`
    takes the value of `inputs_embeds`.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`，*可选*) — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。'
- en: '`output_router_logits` (`bool`, *optional*) — Whether or not to return the
    logits of all the routers. They are useful for computing the router loss, and
    should not be returned during inference.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_router_logits` (`bool`，*可选*) — 是否返回所有路由器的逻辑。用于计算路由器损失，不应在推理期间返回。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[-100, 0, ..., config.vocab_size - 1]`. All labels set to `-100` are ignored
    (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size,)`，*可选*) — 用于计算序列分类/回归损失的标签。索引应在`[-100,
    0, ..., config.vocab_size - 1]`中。所有设置为`-100`的标签都被忽略（掩码），损失仅计算标签在`[0, ..., config.vocab_size]`中的标签'
- en: Returns
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.modeling_outputs.Seq2SeqMoEOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_outputs.Seq2SeqMoEOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.modeling_outputs.Seq2SeqMoEOutput` or a tuple of `torch.FloatTensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration ([SwitchTransformersConfig](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig))
    and inputs.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.modeling_outputs.Seq2SeqMoEOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`，则根据配置（[SwitchTransformersConfig](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig)）和输入返回不同的元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 语言建模损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。'
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`decoder_router_logits` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_router_logits=True` is passed or when `config.add_router_probs=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length,
    num_experts)`.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_router_logits`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_router_logits=True`或`config.add_router_probs=True`时返回）—
    形状为`(batch_size, sequence_length, num_experts)`的`torch.FloatTensor`元组（每层一个）。'
- en: Router logits of the decoder model, useful to compute the auxiliary loss for
    Mixture of Experts models.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器模型的路由器logits，用于计算混合专家模型的辅助损失。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的输出+每层的输出）。'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层输出的编码器的隐藏状态加上初始嵌入输出。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`encoder_router_logits` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_router_logits=True` is passed or when `config.add_router_probs=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length,
    num_experts)`.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_router_logits`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_router_logits=True`或`config.add_router_probs=True`时返回）—
    形状为`(batch_size, sequence_length, num_experts)`的`torch.FloatTensor`元组（每层一个）。'
- en: Router logits of the encoder model, useful to compute the auxiliary loss and
    z_loss for Mixture of Experts models.
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器模型的路由器logits，用于计算混合专家模型的辅助损失和z_loss。
- en: The [SwitchTransformersForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[SwitchTransformersForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersForConditionalGeneration)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: SwitchTransformersEncoderModel
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SwitchTransformersEncoderModel
- en: '### `class transformers.SwitchTransformersEncoderModel`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SwitchTransformersEncoderModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L1781)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L1781)'
- en: '[PRE12]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([SwitchTransformersConfig](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[SwitchTransformersConfig](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig)）—
    包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare SWITCH_TRANSFORMERS Model transformer outputting encoder’s raw hidden-states
    without any specific head on top.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: SWITCH_TRANSFORMERS模型的基本transformer输出编码器的原始隐藏状态，没有特定的头部。
- en: 'The SWITCH_TRANSFORMERS model was proposed in [Switch Transformers: Scaling
    to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)
    by [William Fedus](https://arxiv.org/search/cs?searchtype=author&query=Fedus%2C+W),
    [Barret Zoph](https://arxiv.org/search/cs?searchtype=author&query=Zoph%2C+B),
    and [Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&query=Shazeer%2C+N).
    It’s an encoder-decoder T5-like model with sparse Feed Forward that stands for
    Mixture of Experts (MoE) architecture.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 'SWITCH_TRANSFORMERS模型是由[William Fedus](https://arxiv.org/search/cs?searchtype=author&query=Fedus%2C+W)、[Barret
    Zoph](https://arxiv.org/search/cs?searchtype=author&query=Zoph%2C+B)和[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&query=Shazeer%2C+N)在[Switch
    Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)中提出的。它是一种编码器-解码器T5样式的模型，具有稀疏的前馈，代表着专家混合（MoE）架构。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L1826)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/switch_transformers/modeling_switch_transformers.py#L1826)'
- en: '[PRE13]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. SWITCH_TRANSFORMERS is a model
    with relative position embeddings so you should be able to pad the inputs on both
    the right and the left.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。SWITCH_TRANSFORMERS是一个具有相对位置嵌入的模型，因此您应该能够在右侧和左侧都填充输入。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for detail.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: To know more on how to prepare `input_ids` for pretraining take a look a [SWITCH_TRANSFORMERS
    Training](./switch_transformers#training).
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要了解有关如何为预训练准备`input_ids`的更多信息，请查看[SWITCH_TRANSFORMERS Training](./switch_transformers#training)。
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*可选*)
    — 用于避免在填充标记索引上执行注意力的掩码。掩码值选定在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示未被`masked`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示被`masked`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*)
    — 用于使自注意力模块的选定头部失效的掩码。掩码值选定在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被`masked`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*)
    — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`output_router_logits` (`bool`, *optional*) — Whether or not to return the
    logits of all the routers. They are useful for computing the router loss, and
    should not be returned during inference.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_router_logits` (`bool`, *可选*) — 是否返回所有路由器的logits。它们对于计算路由器损失很有用，在推断期间不应返回。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: Returns
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.modeling_outputs.MoEModelOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_outputs.MoEModelOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.modeling_outputs.MoEModelOutput` or a tuple of `torch.FloatTensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration ([SwitchTransformersConfig](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig))
    and inputs.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.modeling_outputs.MoEModelOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[SwitchTransformersConfig](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig)）和输入而异的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — `torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出 + 每个层的输出）的形状为`(batch_size, sequence_length,
    hidden_size)`。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — `torch.FloatTensor`元组（每个层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: '`router_probs` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_router_probs=True`
    and `config.add_router_probs=True` is passed or when `config.output_router_probs=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length,
    num_experts)`.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_probs` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_router_probs=True`和`config.add_router_probs=True`时返回或当`config.output_router_probs=True`时返回)
    — `torch.FloatTensor`元组（每个层一个）的形状为`(batch_size, sequence_length, num_experts)`。'
- en: Raw router probabilities that are computed by MoE routers, these terms are used
    to compute the auxiliary loss and the z_loss for Mixture of Experts models.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由MoE路由器计算的原始路由器概率，这些术语用于计算混合专家模型的辅助损失和z损失。
- en: The [SwitchTransformersEncoderModel](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersEncoderModel)
    forward method, overrides the `__call__` special method.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[SwitchTransformersEncoderModel](/docs/transformers/v4.37.2/en/model_doc/switch_transformers#transformers.SwitchTransformersEncoderModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
