- en: MarkupLM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MarkupLM
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/markuplm](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/markuplm)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/markuplm](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/markuplm)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup
    Language for Visually-rich Document Understanding](https://arxiv.org/abs/2110.08518)
    by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but applied to
    HTML pages instead of raw text documents. The model incorporates additional embedding
    layers to improve performance, similar to [LayoutLM](layoutlm).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'MarkupLM模型是由Junlong Li、Yiheng Xu、Lei Cui、Furu Wei在[MarkupLM: Pre-training of
    Text and Markup Language for Visually-rich Document Understanding](https://arxiv.org/abs/2110.08518)中提出的。MarkupLM是BERT，但应用于HTML页面而不是原始文本文档。该模型包含额外的嵌入层以提高性能，类似于[LayoutLM](layoutlm)。'
- en: 'The model can be used for tasks like question answering on web pages or information
    extraction from web pages. It obtains state-of-the-art results on 2 important
    benchmarks:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可用于在网页上进行问答或从网页中提取信息等任务。它在2个重要基准测试上取得了最先进的结果：
- en: '[WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural
    Reading Comprehension (a bit like SQuAD but for web pages)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WebSRC](https://x-lance.github.io/WebSRC/)，这是一个用于基于Web的结构化阅读理解的数据集（有点像SQuAD，但适用于网页）'
- en: '[SWDE](https://www.researchgate.net/publication/221299838_From_one_tree_to_a_forest_a_unified_solution_for_structured_web_data_extraction),
    a dataset for information extraction from web pages (basically named-entity recogntion
    on web pages)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SWDE](https://www.researchgate.net/publication/221299838_From_one_tree_to_a_forest_a_unified_solution_for_structured_web_data_extraction)，这是一个用于从网页中提取信息的数据集（基本上是对网页上的命名实体进行识别）'
- en: 'The abstract from the paper is the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Multimodal pre-training with text, layout, and image has made significant
    progress for Visually-rich Document Understanding (VrDU), especially the fixed-layout
    documents such as scanned document images. While, there are still a large number
    of digital documents where the layout information is not fixed and needs to be
    interactively and dynamically rendered for visualization, making existing layout-based
    pre-training approaches not easy to apply. In this paper, we propose MarkupLM
    for document understanding tasks with markup languages as the backbone such as
    HTML/XML-based documents, where text and markup information is jointly pre-trained.
    Experiment results show that the pre-trained MarkupLM significantly outperforms
    the existing strong baseline models on several document understanding tasks. The
    pre-trained model and code will be publicly available.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*多模态文本、布局和图像的预训练在视觉丰富的文档理解（VrDU）方面取得了显著进展，特别是固定布局文档，如扫描文档图像。然而，仍然有大量数字文档，其中布局信息不固定，需要进行交互和动态呈现以进行可视化，使得现有基于布局的预训练方法不易应用。在本文中，我们提出了MarkupLM，用于具有标记语言作为骨干的文档理解任务，例如基于HTML/XML的文档，其中文本和标记信息是联合预训练的。实验结果表明，预训练的MarkupLM在几个文档理解任务上明显优于现有的强基线模型。预训练模型和代码将公开提供。*'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[nielsr](https://huggingface.co/nielsr)贡献。原始代码可以在[这里](https://github.com/microsoft/unilm/tree/master/markuplm)找到。
- en: Usage tips
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: In addition to `input_ids`, [forward()](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMModel.forward)
    expects 2 additional inputs, namely `xpath_tags_seq` and `xpath_subs_seq`. These
    are the XPATH tags and subscripts respectively for each token in the input sequence.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了`input_ids`，[forward()](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMModel.forward)还需要2个额外的输入，即`xpath_tags_seq`和`xpath_subs_seq`。这些分别是输入序列中每个标记的XPATH标记和下标。
- en: One can use [MarkupLMProcessor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMProcessor)
    to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor)
    for more info.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用[MarkupLMProcessor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMProcessor)来为模型准备所有数据。有关更多信息，请参考[使用指南](#usage-markuplmprocessor)。
- en: '![drawing](../Images/7029d473524b7b733e9ae0c5ae38fd9f.png) MarkupLM architecture.
    Taken from the [original paper.](https://arxiv.org/abs/2110.08518)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7029d473524b7b733e9ae0c5ae38fd9f.png) MarkupLM架构。取自[原始论文。](https://arxiv.org/abs/2110.08518)'
- en: 'Usage: MarkupLMProcessor'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用法：MarkupLMProcessor
- en: The easiest way to prepare data for the model is to use [MarkupLMProcessor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMProcessor),
    which internally combines a feature extractor ([MarkupLMFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMFeatureExtractor))
    and a tokenizer ([MarkupLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizer)
    or [MarkupLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizerFast)).
    The feature extractor is used to extract all nodes and xpaths from the HTML strings,
    which are then provided to the tokenizer, which turns them into the token-level
    inputs of the model (`input_ids` etc.). Note that you can still use the feature
    extractor and tokenizer separately, if you only want to handle one of the two
    tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '为模型准备数据的最简单方法是使用[MarkupLMProcessor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMProcessor)，它内部结合了特征提取器（[MarkupLMFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMFeatureExtractor)）和标记器（[MarkupLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizer)或[MarkupLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizerFast)）。特征提取器用于从HTML字符串中提取所有节点和XPATH，然后提供给标记器，将它们转换为模型的标记级输入（`input_ids`等）。请注意，如果您只想处理其中一个任务，仍然可以分别使用特征提取器和标记器。 '
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In short, one can provide HTML strings (and possibly additional data) to [MarkupLMProcessor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMProcessor),
    and it will create the inputs expected by the model. Internally, the processor
    first uses [MarkupLMFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMFeatureExtractor)
    to get a list of nodes and corresponding xpaths. The nodes and xpaths are then
    provided to [MarkupLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizer)
    or [MarkupLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizerFast),
    which converts them to token-level `input_ids`, `attention_mask`, `token_type_ids`,
    `xpath_subs_seq`, `xpath_tags_seq`. Optionally, one can provide node labels to
    the processor, which are turned into token-level `labels`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，可以将HTML字符串（以及可能的其他数据）提供给 [MarkupLMProcessor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMProcessor)，它将创建模型所需的输入。在内部，处理器首先使用
    [MarkupLMFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMFeatureExtractor)
    获取节点和对应的xpath列表。然后将节点和xpath提供给 [MarkupLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizer)
    或 [MarkupLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizerFast)，将它们转换为标记级别的`input_ids`、`attention_mask`、`token_type_ids`、`xpath_subs_seq`、`xpath_tags_seq`。可选地，可以向处理器提供节点标签，这些标签将转换为标记级别的`labels`。
- en: '[MarkupLMFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMFeatureExtractor)
    uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/),
    a Python library for pulling data out of HTML and XML files, under the hood. Note
    that you can still use your own parsing solution of choice, and provide the nodes
    and xpaths yourself to [MarkupLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizer)
    or [MarkupLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizerFast).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[MarkupLMFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMFeatureExtractor)
    使用 [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)，这是一个用于从HTML和XML文件中提取数据的Python库。请注意，您仍然可以使用自己选择的解析解决方案，并将节点和xpath自己提供给
    [MarkupLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizer)
    或 [MarkupLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizerFast)。'
- en: In total, there are 5 use cases that are supported by the processor. Below,
    we list them all. Note that each of these use cases work for both batched and
    non-batched inputs (we illustrate them for non-batched inputs).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有5个由处理器支持的用例。下面我们列出它们。请注意，这些用例适用于批处理和非批处理输入（我们为非批处理输入进行说明）。
- en: '**Use case 1: web page classification (training, inference) + token classification
    (inference), parse_html = True**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**用例1：网页分类（训练，推理）+ 标记分类（推理），parse_html=True**'
- en: This is the simplest case, in which the processor will use the feature extractor
    to get all nodes and xpaths from the HTML.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单的情况，处理器将使用特征提取器从HTML中获取所有节点和xpath。
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Use case 2: web page classification (training, inference) + token classification
    (inference), parse_html=False**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**用例2：网页分类（训练，推理）+ 标记分类（推理），parse_html=False**'
- en: In case one already has obtained all nodes and xpaths, one doesn’t need the
    feature extractor. In that case, one should provide the nodes and corresponding
    xpaths themselves to the processor, and make sure to set `parse_html` to `False`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果已经获取了所有节点和xpath，就不需要特征提取器。在这种情况下，应该将节点和对应的xpath自己提供给处理器，并确保将`parse_html`设置为`False`。
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Use case 3: token classification (training), parse_html=False**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**用例3：标记分类（训练），parse_html=False**'
- en: For token classification tasks (such as [SWDE](https://paperswithcode.com/dataset/swde)),
    one can also provide the corresponding node labels in order to train a model.
    The processor will then convert these into token-level `labels`. By default, it
    will only label the first wordpiece of a word, and label the remaining wordpieces
    with -100, which is the `ignore_index` of PyTorch’s CrossEntropyLoss. In case
    you want all wordpieces of a word to be labeled, you can initialize the tokenizer
    with `only_label_first_subword` set to `False`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标记分类任务（例如[SWDE](https://paperswithcode.com/dataset/swde)），还可以提供相应的节点标签以训练模型。处理器将把这些转换为标记级别的`labels`。默认情况下，它只会标记一个单词的第一个wordpiece，并用-100标记剩余的wordpieces，这是PyTorch的CrossEntropyLoss的`ignore_index`。如果您希望标记一个单词的所有wordpieces，可以将分词器初始化为`only_label_first_subword`设置为`False`。
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Use case 4: web page question answering (inference), parse_html=True**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**用例4：网页问答（推理），parse_html=True**'
- en: For question answering tasks on web pages, you can provide a question to the
    processor. By default, the processor will use the feature extractor to get all
    nodes and xpaths, and create [CLS] question tokens [SEP] word tokens [SEP].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网页问答任务，您可以向处理器提供一个问题。默认情况下，处理器将使用特征提取器获取所有节点和xpath，并创建[CLS]问题标记[SEP]单词标记[SEP]。
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Use case 5: web page question answering (inference), parse_html=False**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**用例5：网页问答（推理），parse_html=False**'
- en: For question answering tasks (such as WebSRC), you can provide a question to
    the processor. If you have extracted all nodes and xpaths yourself, you can provide
    them directly to the processor. Make sure to set `parse_html` to `False`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于问答任务（例如WebSRC），您可以向处理器提供一个问题。如果您已经自己提取了所有节点和xpath，可以直接提供给处理器。请确保将`parse_html`设置为`False`。
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Resources
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Demo notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[演示笔记本](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM)'
- en: '[Text classification task guide](../tasks/sequence_classification)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本分类任务指南](../tasks/sequence_classification)'
- en: '[Token classification task guide](../tasks/token_classification)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[标记分类任务指南](../tasks/token_classification)'
- en: '[Question answering task guide](../tasks/question_answering)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[问答任务指南](../tasks/question_answering)'
- en: MarkupLMConfig
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MarkupLMConfig
- en: '### `class transformers.MarkupLMConfig`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MarkupLMConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/configuration_markuplm.py#L29)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/configuration_markuplm.py#L29)'
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) — Vocabulary size of the
    MarkupLM model. Defines the different tokens that can be represented by the *inputs_ids*
    passed to the forward method of [MarkupLMModel](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMModel).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 30522) — MarkupLM 模型的词汇表大小。定义了可以由传递给[MarkupLMModel](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMModel)的
    *inputs_ids* 表示的不同标记。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化器层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer 编码器中的隐藏层数。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer 编码器中每个注意力层的注意力头数。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer 编码器中“中间”（即前馈）层的维度。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持
    `"gelu"`, `"relu"`, `"silu"` 和 `"gelu_new"`。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的
    dropout 概率。'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — The
    dropout ratio for the attention probabilities.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — 注意力概率的
    dropout 比率。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — 模型可能使用的最大序列长度。通常将其设置为一个较大的值以防万一（例如512或1024或2048）。'
- en: '`type_vocab_size` (`int`, *optional*, defaults to 2) — The vocabulary size
    of the `token_type_ids` passed into [MarkupLMModel](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMModel).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type_vocab_size` (`int`, *optional*, defaults to 2) — 传递给[MarkupLMModel](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMModel)的
    `token_type_ids` 的词汇表大小。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的 epsilon
    值。'
- en: '`max_tree_id_unit_embeddings` (`int`, *optional*, defaults to 1024) — The maximum
    value that the tree id unit embedding might ever use. Typically set this to something
    large just in case (e.g., 1024).'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_tree_id_unit_embeddings` (`int`, *optional*, defaults to 1024) — 树 id
    单元嵌入可能使用的最大值。通常将其设置为一个较大的值以防万一（例如1024）。'
- en: '`max_xpath_tag_unit_embeddings` (`int`, *optional*, defaults to 256) — The
    maximum value that the xpath tag unit embedding might ever use. Typically set
    this to something large just in case (e.g., 256).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_xpath_tag_unit_embeddings` (`int`, *optional*, defaults to 256) — xpath
    标签单元嵌入可能使用的最大值。通常将其设置为一个较大的值以防万一（例如256）。'
- en: '`max_xpath_subs_unit_embeddings` (`int`, *optional*, defaults to 1024) — The
    maximum value that the xpath subscript unit embedding might ever use. Typically
    set this to something large just in case (e.g., 1024).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_xpath_subs_unit_embeddings` (`int`, *optional*, defaults to 1024) — xpath
    下标单元嵌入可能使用的最大值。通常将其设置为一个较大的值以防万一（例如1024）。'
- en: '`tag_pad_id` (`int`, *optional*, defaults to 216) — The id of the padding token
    in the xpath tags.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tag_pad_id` (`int`, *optional*, defaults to 216) — xpath 标签中填充标记的 id。'
- en: '`subs_pad_id` (`int`, *optional*, defaults to 1001) — The id of the padding
    token in the xpath subscripts.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subs_pad_id` (`int`, *optional*, defaults to 1001) — xpath 下标中填充标记的 id。'
- en: '`xpath_tag_unit_hidden_size` (`int`, *optional*, defaults to 32) — The hidden
    size of each tree id unit. One complete tree index will have (50*xpath_tag_unit_hidden_size)-dim.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xpath_tag_unit_hidden_size` (`int`, *optional*, defaults to 32) — 每个树 id 单元的隐藏大小。一个完整的树索引将具有
    (50*xpath_tag_unit_hidden_size) 维度。'
- en: '`max_depth` (`int`, *optional*, defaults to 50) — The maximum depth in xpath.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth` (`int`, *optional*, defaults to 50) — xpath 中的最大深度。'
- en: This is the configuration class to store the configuration of a [MarkupLMModel](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMModel).
    It is used to instantiate a MarkupLM model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the MarkupLM [microsoft/markuplm-base](https://huggingface.co/microsoft/markuplm-base)
    architecture.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[MarkupLMModel](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMModel)配置的配置类。根据指定的参数实例化一个
    MarkupLM 模型，定义模型架构。使用默认值实例化配置将产生类似于 MarkupLM [microsoft/markuplm-base](https://huggingface.co/microsoft/markuplm-base)
    架构的配置。
- en: Configuration objects inherit from [BertConfig](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertConfig)
    and can be used to control the model outputs. Read the documentation from [BertConfig](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertConfig)
    for more information.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[BertConfig](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertConfig)，可用于控制模型输出。阅读[BertConfig](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertConfig)的文档以获取更多信息。
- en: 'Examples:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: MarkupLMFeatureExtractor
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MarkupLMFeatureExtractor
- en: '### `class transformers.MarkupLMFeatureExtractor`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MarkupLMFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/feature_extraction_markuplm.py#L33)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/feature_extraction_markuplm.py#L33)'
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Constructs a MarkupLM feature extractor. This can be used to get a list of nodes
    and corresponding xpaths from HTML strings.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 构造一个 MarkupLM 特征提取器。这可用于从 HTML 字符串获取节点列表和相应的 XPath。
- en: This feature extractor inherits from `PreTrainedFeatureExtractor()` which contains
    most of the main methods. Users should refer to this superclass for more information
    regarding those methods.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此特征提取器继承自 `PreTrainedFeatureExtractor()`，其中包含大部分主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `__call__`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/feature_extraction_markuplm.py#L99)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/feature_extraction_markuplm.py#L99)'
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`html_strings` (`str`, `List[str]`) — The HTML string or batch of HTML strings
    from which to extract nodes and corresponding xpaths.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`html_strings` (`str`, `List[str]`) — 要提取节点和相应 XPath 的 HTML 字符串或 HTML 字符串批处理。'
- en: Returns
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)'
- en: 'A [BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)
    with the following fields:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一个带有以下字段的 [BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)：
- en: '`nodes` — Nodes.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nodes` — 节点。'
- en: '`xpaths` — Corresponding xpaths.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xpaths` — 相应的 XPath。'
- en: Main method to prepare for the model one or several HTML strings.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为模型准备一个或多个 HTML 字符串的主要方法。
- en: 'Examples:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: MarkupLMTokenizer
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MarkupLMTokenizer
- en: '### `class transformers.MarkupLMTokenizer`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MarkupLMTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm.py#L145)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm.py#L145)'
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇表文件的路径。'
- en: '`merges_file` (`str`) — Path to the merges file.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file` (`str`) — 合并文件的路径。'
- en: '`errors` (`str`, *optional*, defaults to `"replace"`) — Paradigm to follow
    when decoding bytes to UTF-8\. See [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)
    for more information.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`errors` (`str`, *optional*, defaults to `"replace"`) — 解码字节为 UTF-8 时要遵循的范例。有关更多信息，请参阅
    [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — 在预训练期间使用的序列开始标记。可以用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列开头的标记。使用的标记是 `cls_token`。
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列结尾的标记。使用的标记是 `sep_token`。
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — 分隔符标记，在从多个序列构建序列时使用，例如用于序列分类的两个序列或用于问题回答的文本和问题。它还用作使用特殊标记构建的序列的最后一个标记。'
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — 在进行序列分类（对整个序列而不是每个标记进行分类）时使用的分类器标记。在使用特殊标记构建序列时，它是序列的第一个标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为
    ID，而是设置为此标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。'
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — 用于屏蔽值的标记。在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) — Whether or not
    to add an initial space to the input. This allows to treat the leading word just
    as any other word. (RoBERTa tokenizer detect beginning of words by the preceding
    space).'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) — 是否将初始空格添加到输入中。这允许将前导单词视为任何其他单词。（RoBERTa
    分词器通过前面的空格检测单词的开头）。'
- en: Construct a MarkupLM tokenizer. Based on byte-level Byte-Pair-Encoding (BPE).
    [MarkupLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizer)
    can be used to turn HTML strings into to token-level `input_ids`, `attention_mask`,
    `token_type_ids`, `xpath_tags_seq` and `xpath_tags_seq`. This tokenizer inherits
    from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个MarkupLM分词器。基于字节级字节对编码（BPE）。[MarkupLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizer)可用于将HTML字符串转换为标记级`input_ids`、`attention_mask`、`token_type_ids`、`xpath_tags_seq`和`xpath_tags_seq`。此分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm.py#L426)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm.py#L426)'
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 将添加特殊标记的ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 具有适当特殊标记的[input IDs](../glossary#input-ids)列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. A RoBERTa sequence has the following
    format:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记从序列或序列对构建用于序列分类任务的模型输入。RoBERTa序列具有以下格式：
- en: 'single sequence: `<s> X </s>`'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '单个序列: `<s> X </s>`'
- en: 'pair of sequences: `<s> A </s></s> B </s>`'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '序列对: `<s> A </s></s> B </s>`'
- en: '#### `get_special_tokens_mask`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm.py#L465)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm.py#L465)'
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`Retrieve` sequence ids from a token list that has no special tokens added.
    This method is called when adding —'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从没有添加特殊标记的标记列表中检索序列ID。在添加时调用此方法 —
- en: '`special` tokens using the tokenizer `prepare_for_model` method. — token_ids_0
    (`List[int]`): List of IDs. token_ids_1 (`List[int]`, *optional*): Optional second
    list of IDs for sequence pairs. already_has_special_tokens (`bool`, *optional*,
    defaults to `False`): Whether or not the token list is already formatted with
    special tokens for the model.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '使用分词器`prepare_for_model`方法的`special`标记。— token_ids_0 (`List[int]`): ID列表。token_ids_1
    (`List[int]`, *可选*): 序列对的可选第二个ID列表。already_has_special_tokens (`bool`, *可选*, 默认为
    `False`): 标记列表是否已经为模型格式化了特殊标记。'
- en: Returns
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一个整数列表，范围为[0, 1]：1表示特殊标记，0表示序列标记。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm.py#L490)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm.py#L490)'
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of zeros.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 零列表。
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. RoBERTa does not make use of token type ids, therefore a list of zeros is
    returned.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建一个用于序列对分类任务的掩码。RoBERTa不使用标记类型ID，因此返回一个零列表。
- en: '#### `save_vocabulary`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm.py#L389)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm.py#L389)'
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: MarkupLMTokenizerFast
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MarkupLMTokenizerFast
- en: '### `class transformers.MarkupLMTokenizerFast`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MarkupLMTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm_fast.py#L100)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm_fast.py#L100)'
- en: '[PRE16]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇文件的路径。'
- en: '`merges_file` (`str`) — Path to the merges file.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file` (`str`) — 合并文件的路径。'
- en: '`errors` (`str`, *optional*, defaults to `"replace"`) — Paradigm to follow
    when decoding bytes to UTF-8\. See [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)
    for more information.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`errors` (`str`, *可选*, 默认为 `"replace"`) — 解码字节为UTF-8时要遵循的范例。有关更多信息，请参见 [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`, *可选*, 默认为 `"<s>"`) — 在预训练期间使用的序列开始标记。可用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列开头的标记。使用的标记是 `cls_token`。
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *可选*, 默认为 `"</s>"`) — 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列结尾的标记。使用的标记是 `sep_token`。
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — 用于在构建多个序列时使用的分隔符标记，例如用于序列分类的两个序列或用于问题回答的文本和问题。它还用作使用特殊标记构建的序列的最后一个标记。'
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — 在进行序列分类（整个序列的分类而不是每个标记的分类）时使用的分类器标记。在使用特殊标记构建序列时，它是序列的第一个标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。'
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — 用于屏蔽值的标记。这是在使用屏蔽语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) — Whether or not
    to add an initial space to the input. This allows to treat the leading word just
    as any other word. (RoBERTa tokenizer detect beginning of words by the preceding
    space).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) — 是否在输入中添加初始空格。这允许将前导单词视为任何其他单词。（RoBERTa分词器通过前面的空格检测单词的开头）。'
- en: Construct a MarkupLM tokenizer. Based on byte-level Byte-Pair-Encoding (BPE).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个MarkupLM分词器。基于字节级字节对编码（BPE）。
- en: '[MarkupLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizerFast)
    can be used to turn HTML strings into to token-level `input_ids`, `attention_mask`,
    `token_type_ids`, `xpath_tags_seq` and `xpath_tags_seq`. This tokenizer inherits
    from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[MarkupLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizerFast)可用于将HTML字符串转换为标记级`input_ids`、`attention_mask`、`token_type_ids`、`xpath_tags_seq`和`xpath_tags_seq`。此分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。'
- en: Users should refer to this superclass for more information regarding those methods.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `batch_encode_plus`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_encode_plus`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm_fast.py#L440)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm_fast.py#L440)'
- en: '[PRE17]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'add_special_tokens (`bool`, *optional*, defaults to `True`): Whether or not
    to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically. padding (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`): Activates and controls padding. Accepts the
    following values:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 'add_special_tokens (`bool`, *optional*, defaults to `True`): 在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了自动添加到输入ID的标记。如果要自动添加`bos`或`eos`标记，则这很有用。填充（`bool`，`str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*optional*，默认为`False`）：激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` or `''longest''`: 填充到批处理中最长序列的长度（如果只提供一个序列，则不进行填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: 使用参数`max_length`指定的最大长度进行填充，或者如果未提供该参数，则使用模型的最大可接受输入长度进行填充。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths). truncation (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`): Activates and controls truncation. Accepts the
    following values:'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` or `''do_not_pad''`（默认）：不进行填充（即，可以输出具有不同长度序列的批次）。截断（`bool`，`str`或[TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)，*optional*，默认为`False`）：激活和控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` or `''longest_first''`: 截断到使用参数`max_length`指定的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。这将逐标记截断，如果提供了一对序列（或一批对序列），则从该对中最长序列中删除一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`：仅截断到由参数`max_length`指定的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则仅会截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: 仅截断到由参数`max_length`指定的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则仅会截断第二个序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).
    max_length (`int`, *optional*): Controls the maximum length to use by one of the
    truncation/padding parameters.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`或`''do_not_truncate''`（默认）：无截断（即，可以输出具有大于模型最大可接受输入大小的序列长度的批次）。max_length（`int`，*可选*）：由截断/填充参数之一使用的最大长度。'
- en: 'If left unset or set to `None`, this will use the predefined model maximum
    length if a maximum length is required by one of the truncation/padding parameters.
    If the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated. stride (`int`, *optional*, defaults to
    0): If set to a number along with `max_length`, the overflowing tokens returned
    when `return_overflowing_tokens=True` will contain some tokens from the end of
    the truncated sequence returned to provide some overlap between truncated and
    overflowing sequences. The value of this argument defines the number of overlapping
    tokens. is_split_into_words (`bool`, *optional*, defaults to `False`): Whether
    or not the input is already pre-tokenized (e.g., split into words). If set to
    `True`, the tokenizer assumes the input is already split into words (for instance,
    by splitting it on whitespace) which it will tokenize. This is useful for NER
    or token classification. pad_to_multiple_of (`int`, *optional*): If set will pad
    the sequence to a multiple of the provided value. Requires `padding` to be activated.
    This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta). return_tensors (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*): If set, will return tensors instead of list of python integers. Acceptable
    values are:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未设置或设置为`None`，则如果截断/填充参数中的一个需要最大长度，则将使用预定义的模型最大长度。如果模型没有特定的最大输入长度（如XLNet），则将禁用截断/填充到最大长度。stride（`int`，*可选*，默认为0）：如果与`max_length`一起设置为一个数字，则当`return_overflowing_tokens=True`时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义了重叠标记的数量。is_split_into_words（`bool`，*可选*，默认为`False`）：输入是否已经预先分词（例如，已经分成单词）。如果设置为`True`，则分词器会假定输入已经分成单词（例如，通过在空格上分割），然后对其进行标记化。这对于NER或标记分类很有用。pad_to_multiple_of（`int`，*可选*）：如果设置，将填充序列到提供的值的倍数。需要激活`padding`。这对于启用具有计算能力`>=
    7.5`（Volta）的NVIDIA硬件上的Tensor Cores特别有用。return_tensors（`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*）：如果设置，将返回张量而不是Python整数列表。可接受的值为：
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`：返回TensorFlow `tf.constant`对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`：返回PyTorch `torch.Tensor`对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`：返回Numpy `np.ndarray`对象。'
- en: 'add_special_tokens (`bool`, *optional*, defaults to `True`): Whether or not
    to encode the sequences with the special tokens relative to their model. padding
    (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`): Activates and controls padding. Accepts the
    following values:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: add_special_tokens（`bool`，*可选*，默认为`True`）：是否使用相对于其模型的特殊标记对序列进行编码。padding（`bool`，`str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`False`）：激活并控制填充。接受以下值：
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True`或`''longest''`：填充到批次中最长的序列（如果只提供了单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`：填充到由参数`max_length`指定的最大长度，或者如果未提供该参数，则填充到模型的最大可接受输入长度。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths). truncation (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`): Activates and controls truncation. Accepts the
    following values:'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`或`''do_not_pad''`（默认）：无填充（即，可以输出具有不同长度序列的批次）。truncation（`bool`，`str`或[TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)，*可选*，默认为`False`）：激活并控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True`或`''longest_first''`：仅截断到由参数`max_length`指定的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则将逐标记截断，从一对序列中删除最长序列的一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`: 如果提供了参数`max_length`，则截断到指定的最大长度，或者截断到模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列对），则只会截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: 如果提供了参数`max_length`，则截断到指定的最大长度，或者截断到模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列对），则只会截断第二个序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).
    max_length (`int`, *optional*): Controls the maximum length to use by one of the
    truncation/padding parameters. If left unset or set to `None`, this will use the
    predefined model maximum length if a maximum length is required by one of the
    truncation/padding parameters. If the model has no specific maximum input length
    (like XLNet) truncation/padding to a maximum length will be deactivated. stride
    (`int`, *optional*, defaults to 0): If set to a number along with `max_length`,
    the overflowing tokens returned when `return_overflowing_tokens=True` will contain
    some tokens from the end of the truncated sequence returned to provide some overlap
    between truncated and overflowing sequences. The value of this argument defines
    the number of overlapping tokens. pad_to_multiple_of (`int`, *optional*): If set
    will pad the sequence to a multiple of the provided value. This is especially
    useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
    `>= 7.5` (Volta). return_tensors (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*): If set, will return tensors instead of list of python integers. Acceptable
    values are:'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`或`''do_not_truncate''`（默认）: 不截断（即，可以输出长度大于模型最大可接受输入大小的批处理）。max_length（`int`，*可选*）:
    通过截断/填充参数之一控制要使用的最大长度。如果未设置或设置为`None`，则将使用预定义的模型最大长度（如果截断/填充参数需要最大长度）。如果模型没有特定的最大输入长度（如XLNet），则将禁用截断/填充到最大长度。stride（`int`，*可选*，默认为0）:
    如果与`max_length`一起设置为一个数字，则当`return_overflowing_tokens=True`时返回的溢出标记将包含从截断序列末尾返回的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义了重叠标记的数量。pad_to_multiple_of（`int`，*可选*）:
    如果设置，将填充序列到提供的值的倍数。这对于在具有计算能力`>= 7.5`（Volta）的NVIDIA硬件上启用Tensor Cores特别有用。return_tensors（`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*）:
    如果设置，将返回张量而不是Python整数列表。可接受的值为:'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回TensorFlow `tf.constant`对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回PyTorch `torch.Tensor`对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回Numpy `np.ndarray`对象。'
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm_fast.py#L890)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm_fast.py#L890)'
- en: '[PRE18]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）— 将添加特殊标记的ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 带有适当特殊标记的[input IDs](../glossary#input-ids)列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. A RoBERTa sequence has the following
    format:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记从序列或序列对构建用于序列分类任务的模型输入。RoBERTa序列的格式如下：
- en: 'single sequence: `<s> X </s>`'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '单个序列: `<s> X </s>`'
- en: 'pair of sequences: `<s> A </s></s> B </s>`'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '序列对: `<s> A </s></s> B </s>`'
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm_fast.py#L913)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm_fast.py#L913)'
- en: '[PRE19]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）— ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of zeros.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 零列表。
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. RoBERTa does not make use of token type ids, therefore a list of zeros is
    returned.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建一个用于序列对分类任务的掩码。RoBERTa不使用token类型ID，因此返回一个零列表。
- en: '#### `encode_plus`'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_plus`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm_fast.py#L507)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm_fast.py#L507)'
- en: '[PRE20]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`, `List[str]`, `List[List[str]]`) — The first sequence to be encoded.
    This can be a string, a list of strings or a list of list of strings.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text`（`str`，`List[str]`，`List[List[str]]`）— 要编码的第一个序列。这可以是一个字符串，一个字符串列表或一个字符串列表的列表。'
- en: '`text_pair` (`List[str]` or `List[int]`, *optional*) — Optional second sequence
    to be encoded. This can be a list of strings (words of a single example) or a
    list of list of strings (words of a batch of examples).'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair`（`List[str]`或`List[int]`，*可选*）— 要编码的可选第二个序列。这可以是一个字符串列表（单个示例的单词）或一个字符串列表的列表（一批示例的单词）。'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (`bool`, *可选*, 默认为 `True`) — 在编码序列时是否添加特殊标记。这将使用底层的 `PretrainedTokenizerBase.build_inputs_with_special_tokens`
    函数，该函数定义了自动添加到输入 id 的标记。如果要自动添加 `bos` 或 `eos` 标记，则这很有用。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *可选*, 默认为 `False`) — 激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`: 填充到批次中最长的序列（或者如果只提供了单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: 填充到由参数 `max_length` 指定的最大长度，或者填充到模型可接受的最大输入长度（如果未提供该参数）。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）: 不填充（即，可以输出具有不同长度序列的批次）。'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *可选*, 默认为 `False`) — 激活和控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest_first''`: 截断到由参数 `max_length` 指定的最大长度，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列），则将逐个标记截断，从该对中最长的序列中删除一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`: 截断到由参数 `max_length` 指定的最大长度，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列），则仅截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: 截断到由参数 `max_length` 指定的最大长度，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列），则仅截断第二个序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_truncate''`（默认）: 不截断（即，可以输出具有大于模型最大可接受输入大小的序列长度的批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *可选*) — 由截断/填充参数之一使用的最大长度。'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置或设置为 `None`，则如果截断/填充参数中的一个需要最大长度，则将使用预定义的模型最大长度。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *可选*, 默认为 0) — 如果与 `max_length` 一起设置为一个数字，则当 `return_overflowing_tokens=True`
    时，返回的溢出标记将包含从截断序列末尾返回的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义重叠标记的数量。'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words` (`bool`, *可选*, 默认为 `False`) — 输入是否已经预分词化（例如，已分割为单词）。如果设置为
    `True`，则分词器会假定输入已经分割为单词（例如，通过在空格上分割），然后对其进行分词。这对于 NER 或标记分类很有用。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *可选*) — 如果设置，将填充序列到提供的值的倍数。需要激活 `padding`。这对于启用
    NVIDIA 硬件上的 Tensor Cores 特别有用，计算能力 `>= 7.5`（Volta）。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回 TensorFlow `tf.constant` 对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回 PyTorch `torch.Tensor` 对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回 Numpy `np.ndarray` 对象。'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to encode the sequences with the special tokens relative to their model.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (`bool`, *可选*, 默认为 `True`) — 是否对序列进行编码，使用相对于其模型的特殊标记。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *可选*, 默认为 `False`) — 激活和控制填充。接受以下值:'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`: 填充到批次中最长的序列（如果只提供一个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: 填充到使用参数 `max_length` 指定的最大长度，或者如果未提供该参数，则填充到模型的最大可接受输入长度。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）: 不填充（即，可以输出长度不同的序列批次）。'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *可选*, 默认为 `False`) — 激活和控制截断。接受以下值:'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest_first''`: 截断到使用参数 `max_length` 指定的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则将逐标记截断，从一对序列中最长的序列中移除一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`: 截断到使用参数 `max_length` 指定的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则只会截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: 截断到使用参数 `max_length` 指定的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则只会截断第二个序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_truncate''`（默认）: 不截断（即，可以输出长度大于模型最大可接受输入大小的序列批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters. If left unset or set to `None`, this will
    use the predefined model maximum length if a maximum length is required by one
    of the truncation/padding parameters. If the model has no specific maximum input
    length (like XLNet) truncation/padding to a maximum length will be deactivated.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *可选*) — 控制截断/填充参数之一使用的最大长度。如果未设置或设置为 `None`，则将使用预定义的模型最大长度，如果截断/填充参数需要最大长度。如果模型没有特定的最大输入长度（如
    XLNet）则截断/填充到最大长度将被禁用。'
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *可选*, 默认为 0) — 如果设置为一个数字，并且 `max_length` 一起设置，当 `return_overflowing_tokens=True`
    时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义了重叠标记的数量。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *可选*) — 如果设置，将填充序列到提供的值的倍数。这对于启用 NVIDIA 硬件上的 Tensor
    Cores 特别有用，计算能力 `>= 7.5`（Volta）。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为:'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回 TensorFlow `tf.constant` 对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回 PyTorch `torch.Tensor` 对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回 Numpy `np.ndarray` 对象。'
- en: 'Tokenize and prepare for the model a sequence or a pair of sequences. .. warning::
    This method is deprecated, `__call__` should be used instead.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '对一个序列或一对序列进行标记化和准备模型。.. 警告:: 此方法已弃用，应改用 `__call__`。'
- en: '#### `get_xpath_seq`'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_xpath_seq`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm_fast.py#L270)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/tokenization_markuplm_fast.py#L270)'
- en: '[PRE21]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Given the xpath expression of one particular node (like “/html/body/div/li[1]/div/span[2]”),
    return a list of tag IDs and corresponding subscripts, taking into account max
    depth.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个特定节点的 xpath 表达式（如“/html/body/div/li[1]/div/span[2]”），返回一个考虑最大深度的标签 ID 列表和相应的下标。
- en: MarkupLMProcessor
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MarkupLMProcessor
- en: '### `class transformers.MarkupLMProcessor`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MarkupLMProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/processing_markuplm.py#L25)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/processing_markuplm.py#L25)'
- en: '[PRE22]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`feature_extractor` (`MarkupLMFeatureExtractor`) — An instance of [MarkupLMFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMFeatureExtractor).
    The feature extractor is a required input.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor` (`MarkupLMFeatureExtractor`) — [MarkupLMFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMFeatureExtractor)
    的实例。特征提取器是必需的输入。'
- en: '`tokenizer` (`MarkupLMTokenizer` or `MarkupLMTokenizerFast`) — An instance
    of [MarkupLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizer)
    or [MarkupLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizerFast).
    The tokenizer is a required input.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`MarkupLMTokenizer` 或 `MarkupLMTokenizerFast`) — [MarkupLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizer)
    或 [MarkupLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizerFast)
    的实例。分词器是必需的输入。'
- en: '`parse_html` (`bool`, *optional*, defaults to `True`) — Whether or not to use
    `MarkupLMFeatureExtractor` to parse HTML strings into nodes and corresponding
    xpaths.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parse_html` (`bool`, *optional*, 默认为 `True`) — 是否使用 `MarkupLMFeatureExtractor`
    来解析 HTML 字符串为节点和对应的 xpath。'
- en: Constructs a MarkupLM processor which combines a MarkupLM feature extractor
    and a MarkupLM tokenizer into a single processor.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个将 MarkupLM 特征提取器和 MarkupLM 分词器合并为单个处理器的 MarkupLM 处理器。
- en: '[MarkupLMProcessor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMProcessor)
    offers all the functionalities you need to prepare data for the model.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[MarkupLMProcessor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMProcessor)
    提供了准备模型数据所需的所有功能。'
- en: It first uses [MarkupLMFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMFeatureExtractor)
    to extract nodes and corresponding xpaths from one or more HTML strings. Next,
    these are provided to [MarkupLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizer)
    or [MarkupLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizerFast),
    which turns them into token-level `input_ids`, `attention_mask`, `token_type_ids`,
    `xpath_tags_seq` and `xpath_subs_seq`.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 首先使用 [MarkupLMFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMFeatureExtractor)
    从一个或多个 HTML 字符串中提取节点和对应的 xpath。接下来，将它们提供给 [MarkupLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizer)
    或 [MarkupLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMTokenizerFast)，将它们转换为标记级别的
    `input_ids`、`attention_mask`、`token_type_ids`、`xpath_tags_seq` 和 `xpath_subs_seq`。
- en: '#### `__call__`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/processing_markuplm.py#L49)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/processing_markuplm.py#L49)'
- en: '[PRE23]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This method first forwards the `html_strings` argument to [**call**()](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMFeatureExtractor.__call__).
    Next, it passes the `nodes` and `xpaths` along with the additional arguments to
    `__call__()` and returns the output.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法首先将 `html_strings` 参数转发到 [**call**()](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMFeatureExtractor.__call__)。接下来，将
    `nodes` 和 `xpaths` 与其他参数一起传递给 `__call__()` 并返回输出。
- en: Optionally, one can also provide a `text` argument which is passed along as
    first sequence.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，还可以提供一个 `text` 参数，作为第一个序列传递。
- en: Please refer to the docstring of the above two methods for more information.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考上述两个方法的文档字符串以获取更多信息。
- en: MarkupLMModel
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MarkupLMModel
- en: '### `class transformers.MarkupLMModel`'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MarkupLMModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/modeling_markuplm.py#L798)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/modeling_markuplm.py#L798)'
- en: '[PRE24]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([MarkupLMConfig](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([MarkupLMConfig](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The bare MarkupLM Model transformer outputting raw hidden-states without any
    specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的 MarkupLM 模型，输出原始隐藏状态而不带任何特定的头部。该模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/modeling_markuplm.py#L830)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/modeling_markuplm.py#L830)'
- en: '[PRE25]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    输入序列标记在词汇表中的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是输入ID？
- en: '`xpath_tags_seq` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    config.max_depth)`, *optional*) — Tag IDs for each token in the input sequence,
    padded up to config.max_depth.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xpath_tags_seq`（`torch.LongTensor`，形状为`(batch_size, sequence_length, config.max_depth)`，*可选*）—
    输入序列中每个标记的标记ID，填充至config.max_depth。'
- en: '`xpath_subs_seq` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    config.max_depth)`, *optional*) — Subscript IDs for each token in the input sequence,
    padded up to config.max_depth.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xpath_subs_seq`（`torch.LongTensor`，形状为`(batch_size, sequence_length, config.max_depth)`，*可选*）—
    输入序列中每个标记的下标ID，填充至config.max_depth。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`: `1` for tokens that are NOT MASKED, `0` for MASKED
    tokens.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。选择在`[0, 1]`范围内的掩码值：对于未被MASK的标记为`1`，对于MASK标记为`0`。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是注意力掩码？
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`: `0` corresponds to a *sentence A* token,
    `1` corresponds to a *sentence B* token'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）—
    段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：`0`对应于*sentence A*标记，`1`对应于*sentence B*标记'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是标记类型ID？
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。选择范围在`[0, config.max_position_embeddings - 1]`内。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是位置ID？
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`: `1` indicates the head is **not masked**, `0` indicates
    the head is **masked**.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*）—
    用于使自注意力模块的选定头部失效的掩码。选择在`[0, 1]`范围内的掩码值：`1`表示头部**未被掩码**，`0`表示头部**被掩码**。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*）—
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将*input_ids*索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — If set to `True`, the attentions
    tensors of all attention layers are returned. See `attentions` under returned
    tensors for more detail.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 如果设置为`True`，则返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — If set to `True`, the hidden
    states of all layers are returned. See `hidden_states` under returned tensors
    for more detail.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 如果设置为`True`，则返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — If set to `True`, the model will return
    a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 如果设置为`True`，模型将返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: Returns
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions)
    or `tuple(torch.FloatTensor)`'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MarkupLMConfig](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMConfig))
    and inputs.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（[MarkupLMConfig](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMConfig)）和输入。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`）—
    模型最后一层的隐藏状态输出的序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — 经过用于辅助预训练任务的层进一步处理后，序列第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选的*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — `torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出 + 每层的输出），形状为`(batch_size, sequence_length,
    hidden_size)`。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选的*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 每个层的`torch.FloatTensor`元组，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *可选的*，当传递`output_attentions=True`和`config.add_cross_attention=True`或`config.output_attentions=True`时返回）
    — 每个层的`torch.FloatTensor`元组，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and optionally if `config.is_encoder_decoder=True`
    2 additional tensors of shape `(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *可选的*，当传递`use_cache=True`或`config.use_cache=True`时返回）
    — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`还有2个额外的形状为`(batch_size,
    num_heads, encoder_sequence_length, embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and optionally if `config.is_encoder_decoder=True` in the cross-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块中的键和值，以及如果`config.is_encoder_decoder=True`在交叉注意力块中）可用（参见`past_key_values`输入）以加速顺序解码。
- en: The [MarkupLMModel](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMModel)
    forward method, overrides the `__call__` special method.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '[MarkupLMModel](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMModel)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE26]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: MarkupLMForSequenceClassification
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MarkupLMForSequenceClassification
- en: '### `class transformers.MarkupLMForSequenceClassification`'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MarkupLMForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/modeling_markuplm.py#L1200)'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/modeling_markuplm.py#L1200)'
- en: '[PRE27]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([MarkupLMConfig](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([MarkupLMConfig](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: MarkupLM Model transformer with a sequence classification/regression head on
    top (a linear layer on top of the pooled output) e.g. for GLUE tasks.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 带有顶部序列分类/回归头（池化输出顶部的线性层）的MarkupLM模型变换器，例如用于GLUE任务。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/modeling_markuplm.py#L1224)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/modeling_markuplm.py#L1224)'
- en: '[PRE28]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是输入ID？
- en: '`xpath_tags_seq` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    config.max_depth)`, *optional*) — Tag IDs for each token in the input sequence,
    padded up to config.max_depth.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xpath_tags_seq` (`torch.LongTensor`，形状为 `(batch_size, sequence_length, config.max_depth)`，*optional*)
    — 输入序列中每个标记的标记ID，填充至 config.max_depth。'
- en: '`xpath_subs_seq` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    config.max_depth)`, *optional*) — Subscript IDs for each token in the input sequence,
    padded up to config.max_depth.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xpath_subs_seq` (`torch.LongTensor`，形状为 `(batch_size, sequence_length, config.max_depth)`，*optional*)
    — 输入序列中每个标记的下标ID，填充至 config.max_depth。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`: `1` for tokens that are NOT MASKED, `0` for MASKED
    tokens.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length)`，*optional*)
    — 用于避免对填充标记索引执行注意力的掩码。选择的掩码值在 `[0, 1]` 中：`1` 表示**未被掩码**的标记，`0` 表示**被掩码**的标记。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是注意力掩码？
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`: `0` corresponds to a *sentence A* token,
    `1` corresponds to a *sentence B* token'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*optional*)
    — 段标记索引，指示输入的第一部分和第二部分。索引在 `[0, 1]` 中选择：`0` 对应于 *句子A* 标记，`1` 对应于 *句子B* 标记'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是标记类型ID？
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*optional*)
    — 每个输入序列标记的位置索引。在范围 `[0, config.max_position_embeddings - 1]` 中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是位置ID？
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`: `1` indicates the head is **not masked**, `0` indicates
    the head is **masked**.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`，形状为 `(num_heads,)` 或 `(num_layers, num_heads)`，*optional*)
    — 用于使自注意力模块中选择的头部失效的掩码。选择的掩码值在 `[0, 1]` 中：`1` 表示头部**未被掩码**，`0` 表示头部**被掩码**。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, hidden_size)`，*optional*)
    — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您希望更多地控制如何将 *input_ids* 索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`output_attentions` (`bool`, *optional*) — If set to `True`, the attentions
    tensors of all attention layers are returned. See `attentions` under returned
    tensors for more detail.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*optional*) — 如果设置为 `True`，则返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — If set to `True`, the hidden
    states of all layers are returned. See `hidden_states` under returned tensors
    for more detail.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 如果设置为 `True`，则返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — If set to `True`, the model will return
    a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*optional*) — 如果设置为 `True`，模型将返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)，而不是一个普通的元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为 `(batch_size,)`，*optional*) — 用于计算序列分类/回归损失的标签。索引应在
    `[0, ..., config.num_labels - 1]` 范围内。如果 `config.num_labels == 1`，则计算回归损失（均方损失），如果
    `config.num_labels > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MarkupLMConfig](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMConfig))
    and inputs.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False`
    时）包含根据配置（[MarkupLMConfig](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMConfig)）和输入的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`损失` (`torch.FloatTensor` 的形状为 `(1,)`，*可选*，当提供 `labels` 时返回) — 分类（如果 `config.num_labels==1`
    则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` 的形状为 `(batch_size, config.num_labels)`) — 分类（如果
    `config.num_labels==1` 则为回归）分数（SoftMax 之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True`
    或当 `config.output_hidden_states=True` 时返回） — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（如果模型具有嵌入层，则为嵌入的输出 + 每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层模型的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_attentions=True`
    或当 `config.output_attentions=True` 时返回） — 形状为 `(batch_size, num_heads, sequence_length,
    sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自注意力头中用于计算加权平均值的注意力 softmax 之后的注意力权重。
- en: The [MarkupLMForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '[MarkupLMForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMForSequenceClassification)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE29]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: MarkupLMForTokenClassification
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MarkupLMForTokenClassification
- en: '### `class transformers.MarkupLMForTokenClassification`'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MarkupLMForTokenClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/modeling_markuplm.py#L1100)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/modeling_markuplm.py#L1100)'
- en: '[PRE30]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([MarkupLMConfig](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([MarkupLMConfig](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: MarkupLM Model with a `token_classification` head on top. This model is a PyTorch
    [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for
    all matter related to general usage and behavior.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部有一个 `token_classification` 头的 MarkupLM 模型。这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/modeling_markuplm.py#L1117)'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/modeling_markuplm.py#L1117)'
- en: '[PRE31]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Parameters
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` 的形状为 `(batch_size, sequence_length)`） — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`xpath_tags_seq` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    config.max_depth)`, *optional*) — Tag IDs for each token in the input sequence,
    padded up to config.max_depth.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xpath_tags_seq` (`torch.LongTensor` 的形状为 `(batch_size, sequence_length, config.max_depth)`，*可选*)
    — 输入序列中每个标记的标签 ID，填充至 config.max_depth。'
- en: '`xpath_subs_seq` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    config.max_depth)`, *optional*) — Subscript IDs for each token in the input sequence,
    padded up to config.max_depth.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xpath_subs_seq` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    config.max_depth)`, *optional*) — 输入序列中每个标记的下标ID，填充至config.max_depth。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`: `1` for tokens that are NOT MASKED, `0` for MASKED
    tokens.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在填充标记索引上执行注意力的遮罩。遮罩值选择在`[0, 1]`范围内：`1`表示未被遮罩的标记，`0`表示被遮罩的标记。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力遮罩？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`: `0` corresponds to a *sentence A* token,
    `1` corresponds to a *sentence B* token'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`范围内：`0`对应于*句子A*标记，`1`对应于*句子B*标记'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`: `1` indicates the head is **not masked**, `0` indicates
    the head is **masked**.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使自注意力模块中的选定头部失效的遮罩。遮罩值选择在`[0, 1]`范围内：`1`表示头部**未被遮罩**，`0`表示头部**被遮罩**。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您希望更多地控制如何将*input_ids*索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`output_attentions` (`bool`, *optional*) — If set to `True`, the attentions
    tensors of all attention layers are returned. See `attentions` under returned
    tensors for more detail.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 如果设置为`True`，则返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — If set to `True`, the hidden
    states of all layers are returned. See `hidden_states` under returned tensors
    for more detail.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 如果设置为`True`，则返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — If set to `True`, the model will return
    a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 如果设置为`True`，模型将返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)，而不是一个普通的元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the token classification loss. Indices should be in `[0,
    ..., config.num_labels - 1]`.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — 用于计算标记分类损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。'
- en: Returns
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MarkupLMConfig](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMConfig))
    and inputs.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含各种元素，这取决于配置（[MarkupLMConfig](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Masked language modeling (MLM) loss.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) —
    掩码语言建模（MLM）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头部的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层的输出，则为一个，每层的输出为一个）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层模型的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [MarkupLMForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMForTokenClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '[MarkupLMForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMForTokenClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE32]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: MarkupLMForQuestionAnswering
  id: totrans-401
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MarkupLMForQuestionAnswering
- en: '### `class transformers.MarkupLMForQuestionAnswering`'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MarkupLMForQuestionAnswering`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/modeling_markuplm.py#L974)'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/modeling_markuplm.py#L974)'
- en: '[PRE33]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Parameters
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([MarkupLMConfig](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[MarkupLMConfig](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: MarkupLM Model with a span classification head on top for extractive question-answering
    tasks like SQuAD (a linear layers on top of the hidden-states output to compute
    `span start logits` and `span end logits`).
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部带有跨度分类头的MarkupLM模型，用于提取式问答任务，如SQuAD（在隐藏状态输出的顶部进行线性层计算`span start logits`和`span
    end logits`）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/modeling_markuplm.py#L993)'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/markuplm/modeling_markuplm.py#L993)'
- en: '[PRE34]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Parameters
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`）- 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`xpath_tags_seq` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    config.max_depth)`, *optional*) — Tag IDs for each token in the input sequence,
    padded up to config.max_depth.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xpath_tags_seq`（`torch.LongTensor`，形状为`(batch_size, sequence_length, config.max_depth)`，*可选*）-
    输入序列中每个标记的标签ID，填充至config.max_depth。'
- en: '`xpath_subs_seq` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    config.max_depth)`, *optional*) — Subscript IDs for each token in the input sequence,
    padded up to config.max_depth.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xpath_subs_seq`（`torch.LongTensor`，形状为`(batch_size, sequence_length, config.max_depth)`，*可选*）-
    每个输入序列中每个标记的下标ID，填充至config.max_depth。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`: `1` for tokens that are NOT MASKED, `0` for MASKED
    tokens.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*可选*）-
    用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：`1`表示未屏蔽的标记，`0`表示已屏蔽的标记。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`: `0` corresponds to a *sentence A* token,
    `1` corresponds to a *sentence B* token'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）-
    段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：`0`对应于*句子A*标记，`1`对应于*句子B*标记'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）-
    每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`: `1` indicates the head is **not masked**, `0` indicates
    the head is **masked**.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使自注意力模块中的选定头部失效的掩码。在`[0, 1]`中选择的掩码值：`1`表示头部**未被掩码**，`0`表示头部**被掩码**。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将*input_ids*索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — If set to `True`, the attentions
    tensors of all attention layers are returned. See `attentions` under returned
    tensors for more detail.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 如果设置为`True`，则返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — If set to `True`, the hidden
    states of all layers are returned. See `hidden_states` under returned tensors
    for more detail.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 如果设置为`True`，则返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — If set to `True`, the model will return
    a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 如果设置为`True`，模型将返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)，而不是一个普通的元组。'
- en: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Labels for position (index) of the start of the labelled span for computing
    the token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — 用于计算标记跨度开始位置的位置（索引）的标签。位置被夹紧到序列的长度（`sequence_length`）。序列外的位置不会被考虑在内，用于计算损失。'
- en: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    Labels for position (index) of the end of the labelled span for computing the
    token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    用于计算标记跨度结束位置的位置（索引）的标签。位置被夹紧到序列的长度（`sequence_length`）。序列外的位置不会被考虑在内，用于计算损失。'
- en: Returns
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MarkupLMConfig](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMConfig))
    and inputs.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包含各种元素，具体取决于配置（[MarkupLMConfig](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Total span extraction loss is the sum of a Cross-Entropy for the
    start and end positions.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) —
    总跨度提取损失是开始和结束位置的交叉熵之和。'
- en: '`start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-start scores (before SoftMax).'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-start scores (before SoftMax).'
- en: '`end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-end scores (before SoftMax).'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-end scores (before SoftMax).'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层，则为嵌入的输出加上每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [MarkupLMForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '[MarkupLMForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/markuplm#transformers.MarkupLMForQuestionAnswering)的前向方法重写了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者负责运行预处理和后处理步骤，而后者则会默默地忽略它们。
- en: 'Examples:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE35]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
