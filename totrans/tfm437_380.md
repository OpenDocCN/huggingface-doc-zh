# Decision Transformer

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/decision_transformer`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/decision_transformer)

## 概述

Decision Transformer 模型提出于[Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345)

作者为 Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch。

该论文的摘要如下：

*我们引入了一个将强化学习（RL）抽象为序列建模问题的框架。这使我们能够利用 Transformer 架构的简单性和可扩展性，以及与语言建模相关的进展，如 GPT-x 和 BERT。特别是，我们提出了 Decision Transformer，这是一个将 RL 问题转化为条件序列建模的架构。与先前拟合值函数或计算策略梯度的 RL 方法不同，Decision Transformer 通过利用因果屏蔽的 Transformer 简单地输出最佳动作。通过将自回归模型条件于期望的回报（奖励）、过去状态和动作，我们的 Decision Transformer 模型可以生成实现期望回报的未来动作。尽管简单，Decision Transformer 在 Atari、OpenAI Gym 和 Key-to-Door 任务上与最先进的无模型离线 RL 基线模型的性能相匹配或超越。*

该模型版本适用于状态为向量的任务。

该模型由[edbeeching](https://huggingface.co/edbeeching)贡献。原始代码可在[此处](https://github.com/kzl/decision-transformer)找到。

## DecisionTransformerConfig

### `class transformers.DecisionTransformerConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/configuration_decision_transformer.py#L31)

```py
( state_dim = 17 act_dim = 4 hidden_size = 128 max_ep_len = 4096 action_tanh = True vocab_size = 1 n_positions = 1024 n_layer = 3 n_head = 1 n_inner = None activation_function = 'relu' resid_pdrop = 0.1 embd_pdrop = 0.1 attn_pdrop = 0.1 layer_norm_epsilon = 1e-05 initializer_range = 0.02 scale_attn_weights = True use_cache = True bos_token_id = 50256 eos_token_id = 50256 scale_attn_by_inverse_layer_idx = False reorder_and_upcast_attn = False **kwargs )
```

参数

+   `state_dim` (`int`, *optional*, 默认为 17) — RL 环境的状态大小

+   `act_dim` (`int`, *optional*, 默认为 4) — 输出动作空间的大小

+   `hidden_size` (`int`, *optional*, 默认为 128) — 隐藏层的大小

+   `max_ep_len` (`int`, *optional*, 默认为 4096) — 环境中一个 episode 的最大长度

+   `action_tanh` (`bool`, *optional*, 默认为 True) — 是否在动作预测上使用 tanh 激活

+   `vocab_size` (`int`, *optional*, 默认为 50257) — GPT-2 模型的词汇大小。定义了在调用 DecisionTransformerModel 时可以表示的不同标记数量。

+   `n_positions` (`int`, *optional*, 默认为 1024) — 该模型可能会使用的最大序列长度。通常将其设置为较大的值以防万一（例如 512、1024 或 2048）。

+   `n_layer` (`int`, *optional*, 默认为 3) — Transformer 编码器中隐藏层的数量。

+   `n_head` (`int`, *optional*, 默认为 1) — Transformer 编码器中每个注意力层的注意力头数。

+   `n_inner` (`int`, *optional*) — 内部前馈层的维度。如果未设置，将默认为`n_embd`的 4 倍。

+   `activation_function` (`str`, *optional*, 默认为`"gelu"`) — 激活函数，可在列表`["relu", "silu", "gelu", "tanh", "gelu_new"]`中选择。

+   `resid_pdrop` (`float`, *optional*, 默认为 0.1) — 嵌入层、编码器和池化器中所有全连接层的丢失概率。

+   `embd_pdrop` (`int`, *optional*, 默认为 0.1) — 嵌入的丢失比例。

+   `attn_pdrop` (`float`, *optional*, 默认为 0.1) — 注意力的丢失比例。

+   `layer_norm_epsilon` (`float`, *optional*, 默认为 1e-5) — 在层归一化层中使用的 epsilon 值。

+   `initializer_range` (`float`, *可选*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `scale_attn_weights` (`bool`, *可选*, 默认为 `True`) — 通过除以 sqrt(hidden_size)来缩放注意力权重。

+   `use_cache` (`bool`, *可选*, 默认为 `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。

+   `scale_attn_by_inverse_layer_idx` (`bool`, *可选*, 默认为 `False`) — 是否通过`1 / layer_idx + 1`额外缩放注意力权重。

+   `reorder_and_upcast_attn` (`bool`, *可选*, 默认为 `False`) — 是否在计算注意力（点积）之前缩放键（K），并在使用混合精度训练时将注意力点积/softmax 向上转换为 float()。

这是用于存储 DecisionTransformerModel 配置的配置类。它用于根据指定的参数实例化一个决策变换器模型，定义模型架构。使用默认值实例化配置将产生类似于标准 DecisionTransformer 架构的配置。许多配置选项用于实例化作为架构一部分使用的 GPT2 模型。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import DecisionTransformerConfig, DecisionTransformerModel

>>> # Initializing a DecisionTransformer configuration
>>> configuration = DecisionTransformerConfig()

>>> # Initializing a model (with random weights) from the configuration
>>> model = DecisionTransformerModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## DecisionTransformerGPT2Model

### `class transformers.DecisionTransformerGPT2Model`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py#L473)

```py
( config )
```

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py#L503)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

## DecisionTransformerModel

### `class transformers.DecisionTransformerModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py#L783)

```py
( config )
```

参数

+   `config` (~DecisionTransformerConfig) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

决策变换器模型 这个模型是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。

该模型基于 GPT2 架构，在离线 RL 设置中执行动作的自回归预测。更多细节请参考论文：[`arxiv.org/abs/2106.01345`](https://arxiv.org/abs/2106.01345)

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py#L817)

```py
( states: Optional = None actions: Optional = None rewards: Optional = None returns_to_go: Optional = None timesteps: Optional = None attention_mask: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.decision_transformer.modeling_decision_transformer.DecisionTransformerOutput or tuple(torch.FloatTensor)
```

参数

+   `states` (`torch.FloatTensor`，形状为`(batch_size, episode_length, state_dim)`) — 轨迹中每个步骤的状态

+   `actions` (`torch.FloatTensor`，形状为`(batch_size, episode_length, act_dim)`) — “专家”策略对当前状态采取的动作，这些动作用于自回归预测

+   `rewards` (`torch.FloatTensor`，形状为`(batch_size, episode_length, 1)`) — 每个状态、动作的奖励

+   `returns_to_go`（形状为`(batch_size, episode_length, 1)`的`torch.FloatTensor`）- 轨迹中每个状态的回报

+   `timesteps`（形状为`(batch_size, episode_length)`的`torch.LongTensor`）- 轨迹中每一步的时间步长

+   `attention_mask`（形状为`(batch_size, episode_length)`的`torch.FloatTensor`）- 掩码，用于在执行自回归预测时屏蔽动作

返回

`transformers.models.decision_transformer.modeling_decision_transformer.DecisionTransformerOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.decision_transformer.modeling_decision_transformer.DecisionTransformerOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含各种元素，取决于配置（`DecisionTransformerConfig`）和输入。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）- 模型最后一层的隐藏状态序列。

+   `state_preds`（形状为`(batch_size, sequence_length, state_dim)`的`torch.FloatTensor`）- 环境状态预测

+   `action_preds`（形状为`(batch_size, sequence_length, action_dim)`的`torch.FloatTensor`）- 模型动作预测

+   `return_preds`（形状为`(batch_size, sequence_length, 1)`的`torch.FloatTensor`）- 每个状态的预测回报

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

`DecisionTransformerModel`的`forward`方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import DecisionTransformerModel
>>> import torch

>>> model = DecisionTransformerModel.from_pretrained("edbeeching/decision-transformer-gym-hopper-medium")
>>> # evaluation
>>> model = model.to(device)
>>> model.eval()

>>> env = gym.make("Hopper-v3")
>>> state_dim = env.observation_space.shape[0]
>>> act_dim = env.action_space.shape[0]

>>> state = env.reset()
>>> states = torch.from_numpy(state).reshape(1, 1, state_dim).to(device=device, dtype=torch.float32)
>>> actions = torch.zeros((1, 1, act_dim), device=device, dtype=torch.float32)
>>> rewards = torch.zeros(1, 1, device=device, dtype=torch.float32)
>>> target_return = torch.tensor(TARGET_RETURN, dtype=torch.float32).reshape(1, 1)
>>> timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)
>>> attention_mask = torch.zeros(1, 1, device=device, dtype=torch.float32)

>>> # forward pass
>>> with torch.no_grad():
...     state_preds, action_preds, return_preds = model(
...         states=states,
...         actions=actions,
...         rewards=rewards,
...         returns_to_go=target_return,
...         timesteps=timesteps,
...         attention_mask=attention_mask,
...         return_dict=False,
...     )
```
