- en: BLIP-Diffusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/diffusers/api/pipelines/blip_diffusion](https://huggingface.co/docs/diffusers/api/pipelines/blip_diffusion)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/38.96a76f37.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Tip.230e2334.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Docstring.93f6f462.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/CodeBlock.57fe6e13.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/ExampleCodeBlock.658f5cd6.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'BLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation
    for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720).
    It enables zero-shot subject-driven generation and control-guided zero-shot generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Subject-driven text-to-image generation models create novel renditions of
    an input subject based on text prompts. Existing models suffer from lengthy fine-tuning
    and difficulties preserving the subject fidelity. To overcome these limitations,
    we introduce BLIP-Diffusion, a new subject-driven image generation model that
    supports multimodal control which consumes inputs of subject images and text prompts.
    Unlike other subject-driven generation models, BLIP-Diffusion introduces a new
    multimodal encoder which is pre-trained to provide subject representation. We
    first pre-train the multimodal encoder following BLIP-2 to produce visual representation
    aligned with the text. Then we design a subject representation learning task which
    enables a diffusion model to leverage such visual representation and generates
    new subject renditions. Compared with previous methods such as DreamBooth, our
    model enables zero-shot subject-driven generation, and efficient fine-tuning for
    customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion
    can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt
    to enable novel subject-driven generation and editing applications. Project page
    at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).*'
  prefs: []
  type: TYPE_NORMAL
- en: The original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion).
    You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce)
    organization.
  prefs: []
  type: TYPE_NORMAL
- en: '`BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed
    by [`ayushtues`](https://github.com/ayushtues/).'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: BlipDiffusionPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.BlipDiffusionPipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/blip_diffusion/pipeline_blip_diffusion.py#L75)'
  prefs: []
  type: TYPE_NORMAL
- en: '( tokenizer: CLIPTokenizer text_encoder: ContextCLIPTextModel vae: AutoencoderKL
    unet: UNet2DConditionModel scheduler: PNDMScheduler qformer: Blip2QFormerModel
    image_processor: BlipImageProcessor ctx_begin_pos: int = 2 mean: List = None std:
    List = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tokenizer** (`CLIPTokenizer`) — Tokenizer for the text encoder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_encoder** (`ContextCLIPTextModel`) — Text encoder to encode the text
    prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vae** ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — VAE model to map the latents to the image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unet** ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — Conditional U-Net architecture to denoise the image embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scheduler** ([PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler))
    — A scheduler to be used in combination with `unet` to generate image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**qformer** (`Blip2QFormerModel`) — QFormer model to get multi-modal embeddings
    from the text and image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_processor** (`BlipImageProcessor`) — Image Processor to preprocess
    and postprocess the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ctx_begin_pos** (int, `optional`, defaults to 2) — Position of the context
    token in the text encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for Zero-Shot Subject Driven Generation using Blip Diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/blip_diffusion/pipeline_blip_diffusion.py#L186)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt: List reference_image: Image source_subject_category: List target_subject_category:
    List latents: Optional = None guidance_scale: float = 7.5 height: int = 512 width:
    int = 512 num_inference_steps: int = 50 generator: Union = None neg_prompt: Optional
    = '''' prompt_strength: float = 1.0 prompt_reps: int = 20 output_type: Optional
    = ''pil'' return_dict: bool = True ) → [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`List[str]`) — The prompt or prompts to guide the image generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**reference_image** (`PIL.Image.Image`) — The reference image to condition
    the generation on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**source_subject_category** (`List[str]`) — The source subject category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**target_subject_category** (`List[str]`) — The target subject category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**latents** (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by random sampling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**guidance_scale** (`float`, *optional*, defaults to 7.5) — Guidance scale
    as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**height** (`int`, *optional*, defaults to 512) — The height of the generated
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**width** (`int`, *optional*, defaults to 512) — The width of the generated
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_inference_steps** (`int`, *optional*, defaults to 50) — The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** (`torch.Generator` or `List[torch.Generator]`, *optional*) —
    One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**neg_prompt** (`str`, *optional*, defaults to "") — The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_strength** (`float`, *optional*, defaults to 1.0) — The strength of
    the prompt. Specifies the number of times the prompt is repeated along with prompt_reps
    to amplify the prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_reps** (`int`, *optional*, defaults to 20) — The number of times the
    prompt is repeated along with prompt_strength to amplify the prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_type** (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Function invoked when calling the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: BlipDiffusionControlNetPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.BlipDiffusionControlNetPipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_blip_diffusion.py#L82)'
  prefs: []
  type: TYPE_NORMAL
- en: '( tokenizer: CLIPTokenizer text_encoder: ContextCLIPTextModel vae: AutoencoderKL
    unet: UNet2DConditionModel scheduler: PNDMScheduler qformer: Blip2QFormerModel
    controlnet: ControlNetModel image_processor: BlipImageProcessor ctx_begin_pos:
    int = 2 mean: List = None std: List = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tokenizer** (`CLIPTokenizer`) — Tokenizer for the text encoder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_encoder** (`ContextCLIPTextModel`) — Text encoder to encode the text
    prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vae** ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — VAE model to map the latents to the image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unet** ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — Conditional U-Net architecture to denoise the image embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scheduler** ([PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler))
    — A scheduler to be used in combination with `unet` to generate image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**qformer** (`Blip2QFormerModel`) — QFormer model to get multi-modal embeddings
    from the text and image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**controlnet** ([ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel))
    — ControlNet model to get the conditioning image embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_processor** (`BlipImageProcessor`) — Image Processor to preprocess
    and postprocess the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ctx_begin_pos** (int, `optional`, defaults to 2) — Position of the context
    token in the text encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for Canny Edge based Controlled subject-driven generation using Blip
    Diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_blip_diffusion.py#L234)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt: List reference_image: Image condtioning_image: Image source_subject_category:
    List target_subject_category: List latents: Optional = None guidance_scale: float
    = 7.5 height: int = 512 width: int = 512 num_inference_steps: int = 50 generator:
    Union = None neg_prompt: Optional = '''' prompt_strength: float = 1.0 prompt_reps:
    int = 20 output_type: Optional = ''pil'' return_dict: bool = True ) → [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`List[str]`) — The prompt or prompts to guide the image generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**reference_image** (`PIL.Image.Image`) — The reference image to condition
    the generation on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**condtioning_image** (`PIL.Image.Image`) — The conditioning canny edge image
    to condition the generation on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**source_subject_category** (`List[str]`) — The source subject category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**target_subject_category** (`List[str]`) — The target subject category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**latents** (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by random sampling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**guidance_scale** (`float`, *optional*, defaults to 7.5) — Guidance scale
    as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**height** (`int`, *optional*, defaults to 512) — The height of the generated
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**width** (`int`, *optional*, defaults to 512) — The width of the generated
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**seed** (`int`, *optional*, defaults to 42) — The seed to use for random generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_inference_steps** (`int`, *optional*, defaults to 50) — The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** (`torch.Generator` or `List[torch.Generator]`, *optional*) —
    One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**neg_prompt** (`str`, *optional*, defaults to "") — The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_strength** (`float`, *optional*, defaults to 1.0) — The strength of
    the prompt. Specifies the number of times the prompt is repeated along with prompt_reps
    to amplify the prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_reps** (`int`, *optional*, defaults to 20) — The number of times the
    prompt is repeated along with prompt_strength to amplify the prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Function invoked when calling the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
