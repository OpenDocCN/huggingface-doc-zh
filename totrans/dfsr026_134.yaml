- en: BLIP-Diffusion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BLIP-Diffusion
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/blip_diffusion](https://huggingface.co/docs/diffusers/api/pipelines/blip_diffusion)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/blip_diffusion](https://huggingface.co/docs/diffusers/api/pipelines/blip_diffusion)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'BLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation
    for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720).
    It enables zero-shot subject-driven generation and control-guided zero-shot generation.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: BLIP-Diffusion提出于[BLIP-Diffusion：用于可控文本到图像生成和编辑的预训练主题表示](https://arxiv.org/abs/2305.14720)。它实现了零样本主题驱动生成和控制引导的零样本生成。
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要为：
- en: '*Subject-driven text-to-image generation models create novel renditions of
    an input subject based on text prompts. Existing models suffer from lengthy fine-tuning
    and difficulties preserving the subject fidelity. To overcome these limitations,
    we introduce BLIP-Diffusion, a new subject-driven image generation model that
    supports multimodal control which consumes inputs of subject images and text prompts.
    Unlike other subject-driven generation models, BLIP-Diffusion introduces a new
    multimodal encoder which is pre-trained to provide subject representation. We
    first pre-train the multimodal encoder following BLIP-2 to produce visual representation
    aligned with the text. Then we design a subject representation learning task which
    enables a diffusion model to leverage such visual representation and generates
    new subject renditions. Compared with previous methods such as DreamBooth, our
    model enables zero-shot subject-driven generation, and efficient fine-tuning for
    customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion
    can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt
    to enable novel subject-driven generation and editing applications. Project page
    at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*主题驱动的文本到图像生成模型根据文本提示创建输入主题的新版本。现有模型存在着冗长的微调和难以保持主题忠实度的困难。为了克服这些限制，我们引入了BLIP-Diffusion，这是一个新的主题驱动图像生成模型，支持多模态控制，消耗主题图像和文本提示的输入。与其他主题驱动生成模型不同，BLIP-Diffusion引入了一个新的多模态编码器，经过预训练以提供主题表示。我们首先按照BLIP-2预训练多模态编码器，以产生与文本对齐的视觉表示。然后我们设计一个主题表示学习任务，使扩散模型能够利用这种视觉表示并生成新的主题版本。与DreamBooth等先前方法相比，我们的模型实现了零样本主题驱动生成，并且对于自定义主题的高效微调可实现高达20倍的加速。我们还展示了BLIP-Diffusion可以灵活地与现有技术结合，如ControlNet和prompt-to-prompt，以实现新的主题驱动生成和编辑应用。项目页面位于[此https
    URL](https://dxli94.github.io/BLIP-Diffusion-website/)。*'
- en: The original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion).
    You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce)
    organization.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 原始代码库可以在[salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion)找到。您可以在[hf.co/SalesForce](https://hf.co/SalesForce)组织下找到官方BLIP-Diffusion检查点。
- en: '`BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed
    by [`ayushtues`](https://github.com/ayushtues/).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '`BlipDiffusionPipeline`和`BlipDiffusionControlNetPipeline`由[`ayushtues`](https://github.com/ayushtues/)贡献。'
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 请务必查看调度器指南以了解如何在调度器速度和质量之间进行权衡，并查看重用管道中的组件部分，以了解如何有效地将相同组件加载到多个管道中。
- en: BlipDiffusionPipeline
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BlipDiffusionPipeline
- en: '### `class diffusers.BlipDiffusionPipeline`'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.BlipDiffusionPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/blip_diffusion/pipeline_blip_diffusion.py#L75)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/blip_diffusion/pipeline_blip_diffusion.py#L75)'
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tokenizer` (`CLIPTokenizer`) — Tokenizer for the text encoder'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（`CLIPTokenizer`）- 用于文本编码器的分词器'
- en: '`text_encoder` (`ContextCLIPTextModel`) — Text encoder to encode the text prompt'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder`（`ContextCLIPTextModel`）- 用于编码文本提示的文本编码器'
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — VAE model to map the latents to the image'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL））-
    VAE模型，将潜变量映射到图像'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — Conditional U-Net architecture to denoise the image embedding.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel））-
    用于去噪图像嵌入的条件U-Net架构。'
- en: '`scheduler` ([PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler))
    — A scheduler to be used in combination with `unet` to generate image latents.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler`（[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler））-
    与`unet`结合使用的调度器，用于生成图像潜变量。'
- en: '`qformer` (`Blip2QFormerModel`) — QFormer model to get multi-modal embeddings
    from the text and image.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qformer`（`Blip2QFormerModel`）- 用于从文本和图像获取多模态嵌入的QFormer模型。'
- en: '`image_processor` (`BlipImageProcessor`) — Image Processor to preprocess and
    postprocess the image.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor`（`BlipImageProcessor`）- 用于预处理和后处理图像的图像处理器。'
- en: '`ctx_begin_pos` (int, `optional`, defaults to 2) — Position of the context
    token in the text encoder.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctx_begin_pos`（整数，`可选`，默认为2）- 文本编码器中上下文标记的位置。'
- en: Pipeline for Zero-Shot Subject Driven Generation using Blip Diffusion.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Blip Diffusion进行零样本主题驱动生成的管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以了解库为所有管道实现的通用方法（如下载或保存，运行在特定设备上等）。
- en: '#### `__call__`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/blip_diffusion/pipeline_blip_diffusion.py#L186)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/blip_diffusion/pipeline_blip_diffusion.py#L186)'
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`List[str]`) — The prompt or prompts to guide the image generation.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`List[str]`) — 指导图像生成的提示或提示。'
- en: '`reference_image` (`PIL.Image.Image`) — The reference image to condition the
    generation on.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reference_image` (`PIL.Image.Image`) — 用于条件生成的参考图像。'
- en: '`source_subject_category` (`List[str]`) — The source subject category.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`source_subject_category` (`List[str]`) — 源主题类别。'
- en: '`target_subject_category` (`List[str]`) — The target subject category.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_subject_category` (`List[str]`) — 目标主题类别。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by random sampling.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成的噪声潜变量，用作图像生成的输入。可用于使用不同提示微调相同的生成。如果未提供，则将通过随机抽样生成一个潜变量张量。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 7.5) — Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *可选*, 默认为7.5) — 如[无分类器扩散指导](https://arxiv.org/abs/2207.12598)中定义的指导比例。`guidance_scale`被定义为[Imagen
    Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程式2的`w`。通过设置`guidance_scale > 1`启用指导比例。更高的指导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。'
- en: '`height` (`int`, *optional*, defaults to 512) — The height of the generated
    image.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *可选*, 默认为512) — 生成图像的高度。'
- en: '`width` (`int`, *optional*, defaults to 512) — The width of the generated image.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *可选*, 默认为512) — 生成图像的宽度。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) — The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *可选*, 默认为50) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个[torch生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)用于使生成过程确定性。'
- en: '`neg_prompt` (`str`, *optional*, defaults to "") — The prompt or prompts not
    to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neg_prompt` (`str`, *可选*, 默认为"") — 不指导图像生成的提示或提示。当不使用指导时被忽略（即如果`guidance_scale`小于`1`，则被忽略）。'
- en: '`prompt_strength` (`float`, *optional*, defaults to 1.0) — The strength of
    the prompt. Specifies the number of times the prompt is repeated along with prompt_reps
    to amplify the prompt.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_strength` (`float`, *可选*, 默认为1.0) — 提示的强度。指定了提示重复的次数以及prompt_reps以放大提示。'
- en: '`prompt_reps` (`int`, *optional*, defaults to 20) — The number of times the
    prompt is repeated along with prompt_strength to amplify the prompt.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_reps` (`int`, *可选*, 默认为20) — 提示重复的次数，以及prompt_strength以放大提示。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。可选择在`"pil"`（`PIL.Image.Image`）、`"np"`（`np.array`）或`"pt"`（`torch.Tensor`）之间选择。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)而不是普通元组。'
- en: Returns
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    或 `tuple`'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用生成管道时调用的函数。
- en: 'Examples:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: BlipDiffusionControlNetPipeline
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BlipDiffusionControlNetPipeline
- en: '### `class diffusers.BlipDiffusionControlNetPipeline`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.BlipDiffusionControlNetPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_blip_diffusion.py#L82)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_blip_diffusion.py#L82)'
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tokenizer` (`CLIPTokenizer`) — Tokenizer for the text encoder'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`CLIPTokenizer`) — 用于文本编码器的分词器'
- en: '`text_encoder` (`ContextCLIPTextModel`) — Text encoder to encode the text prompt'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`ContextCLIPTextModel`) — 用于编码文本提示的文本编码器'
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — VAE model to map the latents to the image'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — 将潜变量映射到图像的VAE模型'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — Conditional U-Net architecture to denoise the image embedding.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — 用于去噪图像嵌入的条件U-Net架构。'
- en: '`scheduler` ([PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler))
    — A scheduler to be used in combination with `unet` to generate image latents.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` ([PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler))
    — 用于与`unet`结合使用以生成图像潜变量的调度器。'
- en: '`qformer` (`Blip2QFormerModel`) — QFormer model to get multi-modal embeddings
    from the text and image.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qformer` (`Blip2QFormerModel`) — 用于从文本和图像获取多模态嵌入的QFormer模型。'
- en: '`controlnet` ([ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel))
    — ControlNet model to get the conditioning image embedding.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`controlnet` ([ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel))
    — 用于获取条件图像嵌入的ControlNet模型。'
- en: '`image_processor` (`BlipImageProcessor`) — Image Processor to preprocess and
    postprocess the image.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor` (`BlipImageProcessor`) — 用于预处理和后处理图像的图像处理器。'
- en: '`ctx_begin_pos` (int, `optional`, defaults to 2) — Position of the context
    token in the text encoder.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctx_begin_pos` (int, `可选`, 默认为2) — 文本编码器中上下文标记的位置。'
- en: Pipeline for Canny Edge based Controlled subject-driven generation using Blip
    Diffusion.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Canny边缘的受控主体驱动生成的Blip扩散管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档，了解库为所有管道实现的通用方法（如下载或保存，运行在特定设备上等）。
- en: '#### `__call__`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_blip_diffusion.py#L234)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_blip_diffusion.py#L234)'
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`List[str]`) — The prompt or prompts to guide the image generation.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`List[str]`) — 用于指导图像生成的提示或提示。'
- en: '`reference_image` (`PIL.Image.Image`) — The reference image to condition the
    generation on.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reference_image` (`PIL.Image.Image`) — 用于条件生成的参考图像。'
- en: '`condtioning_image` (`PIL.Image.Image`) — The conditioning canny edge image
    to condition the generation on.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`condtioning_image` (`PIL.Image.Image`) — 用于条件生成的条件Canny边缘图像。'
- en: '`source_subject_category` (`List[str]`) — The source subject category.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`source_subject_category` (`List[str]`) — 源主题类别。'
- en: '`target_subject_category` (`List[str]`) — The target subject category.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_subject_category` (`List[str]`) — 目标主题类别。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by random sampling.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *可选*) — 预先生成的嘈杂潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，将通过随机抽样生成潜变量张量。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 7.5) — Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *可选*, 默认为7.5) — 指导比例，如[无分类器扩散指导](https://arxiv.org/abs/2207.12598)中定义。`guidance_scale`定义为[Imagen
    Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程2的`w`。通过设置`guidance_scale > 1`启用指导比例。更高的指导比例鼓励生成与文本`prompt`紧密相关的图像，通常以降低图像质量为代价。'
- en: '`height` (`int`, *optional*, defaults to 512) — The height of the generated
    image.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *可选*, 默认为512) — 生成图像的高度。'
- en: '`width` (`int`, *optional*, defaults to 512) — The width of the generated image.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *可选*, 默认为512) — 生成图像的宽度。'
- en: '`seed` (`int`, *optional*, defaults to 42) — The seed to use for random generation.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed` (`int`, *可选*, 默认为42) — 用于随机生成的种子。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) — The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *可选*, 默认为50) — 降噪步骤的数量。更多的降噪步骤通常会导致图像质量更高，但推理速度较慢。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个[torch生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。'
- en: '`neg_prompt` (`str`, *optional*, defaults to "") — The prompt or prompts not
    to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neg_prompt` (`str`, *可选*, 默认为"") — 不用于指导图像生成的提示或提示。如果不使用指导（即如果`guidance_scale`小于`1`，则忽略）。'
- en: '`prompt_strength` (`float`, *optional*, defaults to 1.0) — The strength of
    the prompt. Specifies the number of times the prompt is repeated along with prompt_reps
    to amplify the prompt.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_strength` (`float`, *可选*, 默认为1.0) — 提示的强度。指定提示重复的次数，以及prompt_reps以放大提示。'
- en: '`prompt_reps` (`int`, *optional*, defaults to 20) — The number of times the
    prompt is repeated along with prompt_strength to amplify the prompt.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_reps` (`int`, *可选*, 默认为20) — 提示重复的次数，以及prompt_strength以放大提示。'
- en: Returns
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    或 `tuple`'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 调用管道进行生成时调用的函数。
- en: 'Examples:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
