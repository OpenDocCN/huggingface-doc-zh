- en: Latent Diffusion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在扩散
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion](https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion](https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Latent Diffusion was proposed in [High-Resolution Image Synthesis with Latent
    Diffusion Models](https://huggingface.co/papers/2112.10752) by Robin Rombach,
    Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Robin Rombach、Andreas Blattmann、Dominik Lorenz、Patrick Esser、Björn Ommer在[使用潜在扩散模型进行高分辨率图像合成](https://huggingface.co/papers/2112.10752)中提出了潜在扩散。
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要是：
- en: '*By decomposing the image formation process into a sequential application of
    denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis
    results on image data and beyond. Additionally, their formulation allows for a
    guiding mechanism to control the image generation process without retraining.
    However, since these models typically operate directly in pixel space, optimization
    of powerful DMs often consumes hundreds of GPU days and inference is expensive
    due to sequential evaluations. To enable DM training on limited computational
    resources while retaining their quality and flexibility, we apply them in the
    latent space of powerful pretrained autoencoders. In contrast to previous work,
    training diffusion models on such a representation allows for the first time to
    reach a near-optimal point between complexity reduction and detail preservation,
    greatly boosting visual fidelity. By introducing cross-attention layers into the
    model architecture, we turn diffusion models into powerful and flexible generators
    for general conditioning inputs such as text or bounding boxes and high-resolution
    synthesis becomes possible in a convolutional manner. Our latent diffusion models
    (LDMs) achieve a new state of the art for image inpainting and highly competitive
    performance on various tasks, including unconditional image generation, semantic
    scene synthesis, and super-resolution, while significantly reducing computational
    requirements compared to pixel-based DMs.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*通过将图像形成过程分解为去噪自动编码器的顺序应用，扩散模型（DMs）在图像数据及其他领域取得了最先进的合成结果。此外，它们的制定允许通过引导机制控制图像生成过程而无需重新训练。然而，由于这些模型通常直接在像素空间中运行，优化强大的DMs通常消耗数百个GPU天，并且由于顺序评估而导致推理成本高昂。为了在有限的计算资源上训练DM并保持其质量和灵活性，我们将它们应用于强大的预训练自动编码器的潜在空间。与以往的工作相比，对这种表示进行扩散模型的训练首次实现了在复杂性减少和细节保留之间达到近乎最佳点，极大地提升了视觉保真度。通过在模型架构中引入交叉注意力层，我们将扩散模型转变为通用条件输入（如文本或边界框）的强大灵活生成器，高分辨率合成变得可能以卷积方式进行。我们的潜在扩散模型（LDMs）在图像修补方面取得了最新的技术水平，并在各种任务上表现出高竞争力，包括无条件图像生成、语义场景合成和超分辨率，同时与基于像素的DM相比显著降低了计算要求。*'
- en: The original codebase can be found at [CompVis/latent-diffusion](https://github.com/CompVis/latent-diffusion).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 原始代码库可在[CompVis/latent-diffusion](https://github.com/CompVis/latent-diffusion)找到。
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 请务必查看调度器[指南](../../using-diffusers/schedulers)，以了解如何探索调度器速度和质量之间的权衡，并查看[跨管道重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分，以了解如何有效地将相同组件加载到多个管道中。
- en: LDMTextToImagePipeline
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LDMTextToImagePipeline
- en: '### `class diffusers.LDMTextToImagePipeline`'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.LDMTextToImagePipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py#L32)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py#L32)'
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vqvae` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    — Vector-quantized (VQ) model to encode and decode images to and from latent representations.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vqvae` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    — 用于将图像编码和解码为潜在表示的矢量量化（VQ）模型。'
- en: '`bert` (`LDMBertModel`) — Text-encoder model based on `BERT`.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bert` (`LDMBertModel`) — 基于`BERT`的文本编码器模型。'
- en: '`tokenizer` ([BertTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer))
    — A `BertTokenizer` to tokenize text.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([BertTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer))
    — 用于对文本进行标记化的`BertTokenizer`。'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — A `UNet2DConditionModel` to denoise the encoded image latents.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — 用于去噪编码图像潜在表示的`UNet2DConditionModel`。'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — 用于与`unet`结合使用以去噪编码图像潜在表示的调度器。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。'
- en: Pipeline for text-to-image generation using latent diffusion.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用潜在扩散进行文本到图像生成的管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取为所有管道实现的通用方法（下载、保存、在特定设备上运行等）。
- en: '#### `__call__`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py#L67)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py#L67)'
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`) — The prompt or prompts to guide the image
    generation.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str`或`List[str]`) — 用于指导图像生成的提示或提示。'
- en: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The height in pixels of the generated image.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`)
    — 生成图像的高度（以像素为单位）。'
- en: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The width in pixels of the generated image.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`)
    — 生成图像的宽度（以像素为单位）。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) — The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *可选*, 默认为50) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 1.0) — A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *可选*, 默认为1.0) — 更高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。
    当`guidance_scale > 1`时启用引导比例。'
- en: '`generator` (`torch.Generator`, *optional*) — A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator`, *可选*) — 一个[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for image generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中预先生成的嘈杂潜在空间，用作图像生成的输入。可以用来调整相同生成过程的不同提示。如果未提供，则通过使用提供的随机`generator`进行采样生成潜在空间张量。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的一个。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)而不是一个普通的tuple。'
- en: Returns
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)或`tuple`'
- en: If `return_dict` is `True`, [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`return_dict`为`True`，则返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)，否则返回一个`tuple`，其中第一个元素是包含生成图像的列表。
- en: The call function to the pipeline for generation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: 'Example:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: LDMSuperResolutionPipeline
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LDMSuperResolutionPipeline
- en: '### `class diffusers.LDMSuperResolutionPipeline`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.LDMSuperResolutionPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion_superresolution.py#L33)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion_superresolution.py#L33)'
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vqvae` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    — Vector-quantized (VQ) model to encode and decode images to and from latent representations.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vqvae` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    — 用于将图像编码和解码为潜在表示的矢量量化（VQ）模型。'
- en: '`unet` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    — A `UNet2DModel` to denoise the encoded image.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    — 用于去噪编码图像的`UNet2DModel`。'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    latens. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    [EulerDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/euler#diffusers.EulerDiscreteScheduler),
    [EulerAncestralDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/euler_ancestral#diffusers.EulerAncestralDiscreteScheduler),
    [DPMSolverMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — 一个调度器，与`unet`结合使用，用于去噪编码图像的潜在空间。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)，[EulerDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/euler#diffusers.EulerDiscreteScheduler)，[EulerAncestralDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/euler_ancestral#diffusers.EulerAncestralDiscreteScheduler)，[DPMSolverMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler)，或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。'
- en: A pipeline for image super-resolution using latent diffusion.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用潜在扩散进行图像超分辨率的管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。
- en: '#### `__call__`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion_superresolution.py#L67)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion_superresolution.py#L67)'
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image` (`torch.Tensor` or `PIL.Image.Image`) — `Image` or tensor representing
    an image batch to be used as the starting point for the process.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`torch.Tensor` 或 `PIL.Image.Image`) — 代表要用作过程起点的图像批次的 `Image` 或张量。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — Number of images to generate.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *可选*, 默认为 1) — 要生成的图像数量。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 100) — The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *可选*, 默认为 100) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。'
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, *可选*, 默认为 0.0) — 对应于 [DDIM](https://arxiv.org/abs/2010.02502)
    论文中的参数 eta (η)。仅适用于 [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中将被忽略。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 用于使生成过程确定性的
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *可选*, 默认为 `"pil"`) — 生成图像的输出格式。选择 `PIL.Image` 或 `np.array`
    之间的一个。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回一个 [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    而不是一个普通的元组。'
- en: Returns
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    或 `tuple`'
- en: If `return_dict` is `True`, [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `return_dict` 为 `True`，将返回 [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)，否则将返回一个
    `tuple`，其中第一个元素是包含生成图像的列表。
- en: The call function to the pipeline for generation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: 'Example:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ImagePipelineOutput
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像管道输出
- en: '### `class diffusers.ImagePipelineOutput`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.ImagePipelineOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L116)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L116)'
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`List[PIL.Image.Image]` or `np.ndarray`) — List of denoised PIL images
    of length `batch_size` or NumPy array of shape `(batch_size, height, width, num_channels)`.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`List[PIL.Image.Image]` 或 `np.ndarray`) — 长度为 `batch_size` 的去噪 PIL
    图像列表或形状为 `(batch_size, height, width, num_channels)` 的 NumPy 数组。'
- en: Output class for image pipelines.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图像管道的输出类。
