- en: VQDiffusionScheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/diffusers/api/schedulers/vq_diffusion](https://huggingface.co/docs/diffusers/api/schedulers/vq_diffusion)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/112.4ec0580a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Docstring.93f6f462.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/CodeBlock.57fe6e13.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/ExampleCodeBlock.658f5cd6.js">
  prefs: []
  type: TYPE_NORMAL
- en: '`VQDiffusionScheduler` converts the transformer model’s output into a sample
    for the unnoised image at the previous diffusion timestep. It was introduced in
    [Vector Quantized Diffusion Model for Text-to-Image Synthesis](https://huggingface.co/papers/2111.14822)
    by Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan,
    Baining Guo.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We present the vector quantized diffusion (VQ-Diffusion) model for text-to-image
    generation. This method is based on a vector quantized variational autoencoder
    (VQ-VAE) whose latent space is modeled by a conditional variant of the recently
    developed Denoising Diffusion Probabilistic Model (DDPM). We find that this latent-space
    method is well-suited for text-to-image generation tasks because it not only eliminates
    the unidirectional bias with existing methods but also allows us to incorporate
    a mask-and-replace diffusion strategy to avoid the accumulation of errors, which
    is a serious problem with existing methods. Our experiments show that the VQ-Diffusion
    produces significantly better text-to-image generation results when compared with
    conventional autoregressive (AR) models with similar numbers of parameters. Compared
    with previous GAN-based text-to-image methods, our VQ-Diffusion can handle more
    complex scenes and improve the synthesized image quality by a large margin. Finally,
    we show that the image generation computation in our method can be made highly
    efficient by reparameterization. With traditional AR methods, the text-to-image
    generation time increases linearly with the output image resolution and hence
    is quite time consuming even for normal size images. The VQ-Diffusion allows us
    to achieve a better trade-off between quality and speed. Our experiments indicate
    that the VQ-Diffusion model with the reparameterization is fifteen times faster
    than traditional AR methods while achieving a better image quality.*'
  prefs: []
  type: TYPE_NORMAL
- en: VQDiffusionScheduler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.VQDiffusionScheduler'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_vq_diffusion.py#L106)'
  prefs: []
  type: TYPE_NORMAL
- en: '( num_vec_classes: int num_train_timesteps: int = 100 alpha_cum_start: float
    = 0.99999 alpha_cum_end: float = 9e-06 gamma_cum_start: float = 9e-06 gamma_cum_end:
    float = 0.99999 )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**num_vec_classes** (`int`) — The number of classes of the vector embeddings
    of the latent pixels. Includes the class for the masked latent pixel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_train_timesteps** (`int`, defaults to 100) — The number of diffusion
    steps to train the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**alpha_cum_start** (`float`, defaults to 0.99999) — The starting cumulative
    alpha value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**alpha_cum_end** (`float`, defaults to 0.00009) — The ending cumulative alpha
    value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gamma_cum_start** (`float`, defaults to 0.00009) — The starting cumulative
    gamma value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gamma_cum_end** (`float`, defaults to 0.99999) — The ending cumulative gamma
    value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A scheduler for vector quantized diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)
    and [ConfigMixin](/docs/diffusers/v0.26.3/en/api/configuration#diffusers.ConfigMixin).
    Check the superclass documentation for the generic methods the library implements
    for all schedulers such as loading and saving.
  prefs: []
  type: TYPE_NORMAL
- en: '#### log_Q_t_transitioning_to_known_class'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_vq_diffusion.py#L356)'
  prefs: []
  type: TYPE_NORMAL
- en: '( t: torch.int32 x_t: LongTensor log_onehot_x_t: FloatTensor cumulative: bool
    ) → `torch.FloatTensor` of shape `(batch size, num classes - 1, num latent pixels)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**t** (`torch.Long`) — The timestep that determines which transition matrix
    is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**x_t** (`torch.LongTensor` of shape `(batch size, num latent pixels)`) — The
    classes of each latent pixel at time `t`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**log_onehot_x_t** (`torch.FloatTensor` of shape `(batch size, num classes,
    num latent pixels)`) — The log one-hot vectors of `x_t`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cumulative** (`bool`) — If cumulative is `False`, the single step transition
    matrix `t-1`->`t` is used. If cumulative is `True`, the cumulative transition
    matrix `0`->`t` is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch size, num classes - 1, num latent pixels)`'
  prefs: []
  type: TYPE_NORMAL
- en: Each *column* of the returned matrix is a *row* of log probabilities of the
    complete probability transition matrix.
  prefs: []
  type: TYPE_NORMAL
- en: When non cumulative, returns `self.num_classes - 1` rows because the initial
    latent pixel cannot be masked.
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '`q_n` is the probability distribution for the forward process of the `n`th
    latent pixel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C_0 is a class of a latent pixel embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C_k is the class of the masked latent pixel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'non-cumulative result (omitting logarithms):'
  prefs: []
  type: TYPE_NORMAL
- en: <codeblock code="{`cV8wKHhfdCUyMCU3QyUyMHhfJTdCdC0xJTdEJTIwJTNEJTIwQ18wKSUyMC4uLiUyMHFfbih4X3QlMjAlN0MlMjB4XyU3QnQtMSU3RCUyMCUzRCUyMENfMCklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAuJTIwJTIwJTIwJTIwJTIwJTIwLiUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMC4lMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAuJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwLiUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMC4lMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAuJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwLiUyMCUyMCUyMCUyMCUyMC4lMEFxXzAoeF90JTIwJTdDJTIweF8lN0J0LTElN0QlMjAlM0QlMjBDX2spJTIwLi4uJTIwcV9uKHhfdCUyMCU3QyUyMHhfJTdCdC0xJTdEJTIwJTNEJTIwQ19rKQ==`}"
    highlighted="{`q<span" class="hljs-constructor">_0(x_t | x_{t-1\} = C_0) ... q_n(x_t
    | x_{t-1\} = C_0) . . . . . . . . . q_0(x_t | x_{t-1\} = C_k) ... q_n(x_t | x_{t-1\}
    = C_k)`} wrap={false} />
  prefs: []
  type: TYPE_NORMAL
- en: 'cumulative result (omitting logarithms):'
  prefs: []
  type: TYPE_NORMAL
- en: <codeblock code="{`cV8wX2N1bXVsYXRpdmUoeF90JTIwJTdDJTIweF8wJTIwJTNEJTIwQ18wKSUyMCUyMCUyMCUyMC4uLiUyMCUyMHFfbl9jdW11bGF0aXZlKHhfdCUyMCU3QyUyMHhfMCUyMCUzRCUyMENfMCklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAuJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwLiUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMC4lMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAuJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwLiUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMC4lMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAuJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwLiUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMC4lMEFxXzBfY3VtdWxhdGl2ZSh4X3QlMjAlN0MlMjB4XzAlMjAlM0QlMjBDXyU3QmstMSU3RCklMjAuLi4lMjBxX25fY3VtdWxhdGl2ZSh4X3QlMjAlN0MlMjB4XzAlMjAlM0QlMjBDXyU3QmstMSU3RCk=`}"
    highlighted="{`q<span" class="hljs-constructor">_0_cumulative(x_t | x_0 = C_0)
    ... q_n_cumulative(x_t | x_0 = C_0) . . . . . . . . . q_0_cumulative(x_t | x_0
    = C_{k-1\}) ... q_n_cumulative(x_t | x_0 = C_{k-1\})`} wrap={false} /></codeblock></codeblock>
  prefs: []
  type: TYPE_NORMAL
- en: Calculates the log probabilities of the rows from the (cumulative or non-cumulative)
    transition matrix for each latent pixel in `x_t`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### q_posterior'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_vq_diffusion.py#L245)'
  prefs: []
  type: TYPE_NORMAL
- en: ( log_p_x_0 x_t t ) → `torch.FloatTensor` of shape `(batch size, num classes,
    num latent pixels)`
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**log_p_x_0** (`torch.FloatTensor` of shape `(batch size, num classes - 1,
    num latent pixels)`) — The log probabilities for the predicted classes of the
    initial latent pixels. Does not include a prediction for the masked class as the
    initial unnoised image cannot be masked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**x_t** (`torch.LongTensor` of shape `(batch size, num latent pixels)`) — The
    classes of each latent pixel at time `t`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**t** (`torch.Long`) — The timestep that determines which transition matrix
    is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch size, num classes, num latent pixels)`'
  prefs: []
  type: TYPE_NORMAL
- en: The log probabilities for the predicted classes of the image at timestep `t-1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculates the log probabilities for the predicted classes of the image at
    timestep `t-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#### set_timesteps'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_vq_diffusion.py#L178)'
  prefs: []
  type: TYPE_NORMAL
- en: '( num_inference_steps: int device: Union = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**num_inference_steps** (`int`) — The number of diffusion steps used when generating
    samples with a pre-trained model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (`str` or `torch.device`, *optional*) — The device to which the
    timesteps and diffusion process parameters (alpha, beta, gamma) should be moved
    to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sets the discrete timesteps used for the diffusion chain (to be run before inference).
  prefs: []
  type: TYPE_NORMAL
- en: '#### step'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_vq_diffusion.py#L200)'
  prefs: []
  type: TYPE_NORMAL
- en: '( model_output: FloatTensor timestep: torch.int64 sample: LongTensor generator:
    Optional = None return_dict: bool = True ) → [VQDiffusionSchedulerOutput](/docs/diffusers/v0.26.3/en/api/schedulers/vq_diffusion#diffusers.schedulers.scheduling_vq_diffusion.VQDiffusionSchedulerOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**t** (`torch.long`) — The timestep that determines which transition matrices
    are used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**x_t** (`torch.LongTensor` of shape `(batch size, num latent pixels)`) — The
    classes of each latent pixel at time `t`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** (`torch.Generator`, or `None`) — A random number generator for
    the noise applied to `p(x_{t-1} | x_t)` before it is sampled from.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [VQDiffusionSchedulerOutput](/docs/diffusers/v0.26.3/en/api/schedulers/vq_diffusion#diffusers.schedulers.scheduling_vq_diffusion.VQDiffusionSchedulerOutput)
    or `tuple`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[VQDiffusionSchedulerOutput](/docs/diffusers/v0.26.3/en/api/schedulers/vq_diffusion#diffusers.schedulers.scheduling_vq_diffusion.VQDiffusionSchedulerOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If return_dict is `True`, [VQDiffusionSchedulerOutput](/docs/diffusers/v0.26.3/en/api/schedulers/vq_diffusion#diffusers.schedulers.scheduling_vq_diffusion.VQDiffusionSchedulerOutput)
    is returned, otherwise a tuple is returned where the first element is the sample
    tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Predict the sample from the previous timestep by the reverse transition distribution.
    See [q_posterior()](/docs/diffusers/v0.26.3/en/api/schedulers/vq_diffusion#diffusers.VQDiffusionScheduler.q_posterior)
    for more details about how the distribution is computer.
  prefs: []
  type: TYPE_NORMAL
- en: VQDiffusionSchedulerOutput
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.schedulers.scheduling_vq_diffusion.VQDiffusionSchedulerOutput'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_vq_diffusion.py#L27)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prev_sample: LongTensor )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prev_sample** (`torch.LongTensor` of shape `(batch size, num latent pixels)`)
    — Computed sample x_{t-1} of previous timestep. `prev_sample` should be used as
    next model input in the denoising loop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output class for the scheduler’s step function output.
  prefs: []
  type: TYPE_NORMAL
