# åˆ›å»ºå›¾åƒæ•°æ®é›†

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/datasets/image_dataset](https://huggingface.co/docs/datasets/image_dataset)

æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥åˆ›å»ºå’Œå…±äº«å›¾åƒæ•°æ®é›†ã€‚æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

+   ä½¿ç”¨`ImageFolder`å’Œä¸€äº›å…ƒæ•°æ®åˆ›å»ºå›¾åƒæ•°æ®é›†ã€‚è¿™æ˜¯ä¸€ä¸ªæ— ä»£ç è§£å†³æ–¹æ¡ˆï¼Œå¯å¿«é€Ÿåˆ›å»ºåŒ…å«æ•°åƒå¼ å›¾åƒçš„å›¾åƒæ•°æ®é›†ã€‚

+   é€šè¿‡ç¼–å†™åŠ è½½è„šæœ¬åˆ›å»ºå›¾åƒæ•°æ®é›†ã€‚è¿™ç§æ–¹æ³•ç¨å¾®å¤æ‚ä¸€äº›ï¼Œä½†æ‚¨å¯ä»¥æ›´çµæ´»åœ°å®šä¹‰ã€ä¸‹è½½å’Œç”Ÿæˆæ•°æ®é›†ï¼Œè¿™å¯¹äºæ›´å¤æ‚æˆ–å¤§è§„æ¨¡çš„å›¾åƒæ•°æ®é›†å¯èƒ½å¾ˆæœ‰ç”¨ã€‚

æ‚¨å¯ä»¥é€šè¿‡è¦æ±‚ç”¨æˆ·é¦–å…ˆå…±äº«å…¶è”ç³»ä¿¡æ¯æ¥æ§åˆ¶å¯¹æ•°æ®é›†çš„è®¿é—®ã€‚æŸ¥çœ‹æœ‰å…³å¦‚ä½•åœ¨Hubä¸Šå¯ç”¨æ­¤åŠŸèƒ½çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[Gated datasets](https://huggingface.co/docs/hub/datasets-gated)æŒ‡å—ã€‚

## ImageFolder

`ImageFolder`æ˜¯ä¸€ä¸ªæ•°æ®é›†æ„å»ºå™¨ï¼Œæ—¨åœ¨å¿«é€ŸåŠ è½½åŒ…å«æ•°åƒå¼ å›¾åƒçš„å›¾åƒæ•°æ®é›†ï¼Œè€Œæ— éœ€ç¼–å†™ä»»ä½•ä»£ç ã€‚

ğŸ’¡æŸ¥çœ‹[Split pattern hierarchy](repository_structure#split-pattern-hierarchy)ä»¥äº†è§£æœ‰å…³`ImageFolder`å¦‚ä½•åŸºäºæ•°æ®é›†å­˜å‚¨åº“ç»“æ„åˆ›å»ºæ•°æ®é›†æ‹†åˆ†çš„æ›´å¤šä¿¡æ¯ã€‚

`ImageFolder`ä¼šæ ¹æ®ç›®å½•åç§°è‡ªåŠ¨æ¨æ–­æ•°æ®é›†çš„ç±»æ ‡ç­¾ã€‚å°†æ•°æ®é›†å­˜å‚¨åœ¨ä»¥ä¸‹ç»“æ„çš„ç›®å½•ä¸­ï¼š

```py
folder/train/dog/golden_retriever.png
folder/train/dog/german_shepherd.png
folder/train/dog/chihuahua.png

folder/train/cat/maine_coon.png
folder/train/cat/bengal.png
folder/train/cat/birman.png
```

ç„¶åç”¨æˆ·å¯ä»¥é€šè¿‡åœ¨[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)ä¸­æŒ‡å®š`imagefolder`å’Œ`data_dir`ä¸­çš„ç›®å½•æ¥åŠ è½½æ•°æ®é›†ï¼š

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("imagefolder", data_dir="/path/to/folder")
```

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨`imagefolder`åŠ è½½æ¶‰åŠå¤šä¸ªæ‹†åˆ†çš„æ•°æ®é›†ã€‚ä¸ºæ­¤ï¼Œæ‚¨çš„æ•°æ®é›†ç›®å½•åº”å…·æœ‰ä»¥ä¸‹ç»“æ„ï¼š

```py
folder/train/dog/golden_retriever.png
folder/train/cat/maine_coon.png
folder/test/dog/german_shepherd.png
folder/test/cat/bengal.png
```

å¦‚æœæ‰€æœ‰å›¾åƒæ–‡ä»¶éƒ½åŒ…å«åœ¨å•ä¸ªç›®å½•ä¸­ï¼Œæˆ–è€…å®ƒä»¬ä¸åœ¨ç›¸åŒçº§åˆ«çš„ç›®å½•ç»“æ„ä¸­ï¼Œ`label`åˆ—å°†ä¸ä¼šè‡ªåŠ¨æ·»åŠ ã€‚å¦‚æœéœ€è¦ï¼Œè¯·æ˜¾å¼è®¾ç½®`drop_labels=False`ã€‚

å¦‚æœæ‚¨æƒ³è¦åŒ…å«æœ‰å…³æ•°æ®é›†çš„å…¶ä»–ä¿¡æ¯ï¼Œå¦‚æ–‡æœ¬æ ‡é¢˜æˆ–è¾¹ç•Œæ¡†ï¼Œè¯·å°†å…¶æ·»åŠ ä¸º`metadata.csv`æ–‡ä»¶æ”¾åœ¨æ‚¨çš„æ–‡ä»¶å¤¹ä¸­ã€‚è¿™æ ·å¯ä»¥å¿«é€Ÿä¸ºä¸åŒçš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡åˆ›å»ºæ•°æ®é›†ï¼Œå¦‚æ–‡æœ¬æ ‡é¢˜æˆ–ç›®æ ‡æ£€æµ‹ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨JSONLæ–‡ä»¶`metadata.jsonl`ã€‚

```py
folder/train/metadata.csv
folder/train/0001.png
folder/train/0002.png
folder/train/0003.png
```

æ‚¨è¿˜å¯ä»¥å‹ç¼©æ‚¨çš„å›¾åƒï¼š

```py
folder/metadata.csv
folder/train.zip
folder/test.zip
folder/valid.zip
```

æ‚¨çš„`metadata.csv`æ–‡ä»¶å¿…é¡»å…·æœ‰ä¸€ä¸ª`file_name`åˆ—ï¼Œå°†å›¾åƒæ–‡ä»¶ä¸å…¶å…ƒæ•°æ®é“¾æ¥èµ·æ¥ï¼š

```py
file_name,additional_feature
0001.png,This is a first value of a text feature you added to your images
0002.png,This is a second value of a text feature you added to your images
0003.png,This is a third value of a text feature you added to your images
```

æˆ–è€…ä½¿ç”¨`metadata.jsonl`ï¼š

```py
{"file_name": "0001.png", "additional_feature": "This is a first value of a text feature you added to your images"}
{"file_name": "0002.png", "additional_feature": "This is a second value of a text feature you added to your images"}
{"file_name": "0003.png", "additional_feature": "This is a third value of a text feature you added to your images"}
```

å¦‚æœå­˜åœ¨å…ƒæ•°æ®æ–‡ä»¶ï¼Œåˆ™é»˜è®¤æƒ…å†µä¸‹åŸºäºç›®å½•åç§°æ¨æ–­çš„æ ‡ç­¾å°†è¢«åˆ é™¤ã€‚è¦åŒ…å«è¿™äº›æ ‡ç­¾ï¼Œè¯·åœ¨`load_dataset`ä¸­è®¾ç½®`drop_labels=False`ã€‚

### å›¾åƒæ ‡é¢˜

å›¾åƒæ ‡é¢˜æ•°æ®é›†åŒ…å«æè¿°å›¾åƒçš„æ–‡æœ¬ã€‚ä¸€ä¸ªç¤ºä¾‹`metadata.csv`å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
file_name,text
0001.png,This is a golden retriever playing with a ball
0002.png,A german shepherd
0003.png,One chihuahua
```

ä½¿ç”¨`ImageFolder`åŠ è½½æ•°æ®é›†ï¼Œå®ƒå°†ä¸ºå›¾åƒæ ‡é¢˜åˆ›å»ºä¸€ä¸ª`text`åˆ—ï¼š

```py
>>> dataset = load_dataset("imagefolder", data_dir="/path/to/folder", split="train")
>>> dataset[0]["text"]
"This is a golden retriever playing with a ball"
```

### ç›®æ ‡æ£€æµ‹

ç›®æ ‡æ£€æµ‹æ•°æ®é›†å…·æœ‰è¾¹ç•Œæ¡†å’Œç±»åˆ«ï¼Œç”¨äºè¯†åˆ«å›¾åƒä¸­çš„å¯¹è±¡ã€‚ä¸€ä¸ªç¤ºä¾‹`metadata.jsonl`å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
{"file_name": "0001.png", "objects": {"bbox": [[302.0, 109.0, 73.0, 52.0]], "categories": [0]}}
{"file_name": "0002.png", "objects": {"bbox": [[810.0, 100.0, 57.0, 28.0]], "categories": [1]}}
{"file_name": "0003.png", "objects": {"bbox": [[160.0, 31.0, 248.0, 616.0], [741.0, 68.0, 202.0, 401.0]], "categories": [2, 2]}}
```

ä½¿ç”¨`ImageFolder`åŠ è½½æ•°æ®é›†ï¼Œå®ƒå°†åˆ›å»ºä¸€ä¸ªå¸¦æœ‰è¾¹ç•Œæ¡†å’Œç±»åˆ«çš„`objects`åˆ—ï¼š

```py
>>> dataset = load_dataset("imagefolder", data_dir="/path/to/folder", split="train")
>>> dataset[0]["objects"]
{"bbox": [[302.0, 109.0, 73.0, 52.0]], "categories": [0]}
```

### å°†æ•°æ®é›†ä¸Šä¼ åˆ°Hub

åˆ›å»ºæ•°æ®é›†åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[push_to_hub()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict.push_to_hub)æ–¹æ³•å°†å…¶å…±äº«åˆ°Hubã€‚ç¡®ä¿æ‚¨å·²å®‰è£…[huggingface_hub](https://huggingface.co/docs/huggingface_hub/index)åº“ï¼Œå¹¶ä¸”å·²ç™»å½•åˆ°æ‚¨çš„Hugging Faceå¸æˆ·ï¼ˆæœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ä½¿ç”¨Pythonä¸Šä¼ æ•™ç¨‹](upload_dataset#upload-with-python)ï¼‰ã€‚

ä½¿ç”¨[push_to_hub()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict.push_to_hub)ä¸Šä¼ æ‚¨çš„æ•°æ®é›†ï¼š

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("imagefolder", data_dir="/path/to/folder", split="train")
>>> dataset.push_to_hub("stevhliu/my-image-captioning-dataset")
```

## WebDataset

[WebDataset](https://github.com/webdataset/webdataset)æ ¼å¼åŸºäºTARå­˜æ¡£ï¼Œé€‚ç”¨äºå¤§å‹å›¾åƒæ•°æ®é›†ã€‚å®é™…ä¸Šï¼Œæ‚¨å¯ä»¥å°†å›¾åƒåˆ†ç»„åœ¨TARå­˜æ¡£ä¸­ï¼ˆä¾‹å¦‚æ¯ä¸ªTARå­˜æ¡£1GBçš„å›¾åƒï¼‰å¹¶æ‹¥æœ‰æ•°åƒä¸ªTARå­˜æ¡£ï¼š

```py
folder/train/00000.tar
folder/train/00001.tar
folder/train/00002.tar
...
```

åœ¨å­˜æ¡£ä¸­ï¼Œæ¯ä¸ªç¤ºä¾‹ç”±å…±äº«ç›¸åŒå‰ç¼€çš„æ–‡ä»¶ç»„æˆï¼š

```py
e39871fd9fd74f55.jpg e39871fd9fd74f55.json f18b91585c4d3f3e.jpg f18b91585c4d3f3e.json ede6e66b2fb59aab.jpg ede6e66b2fb59aab.json ed600d57fcee4f94.jpg ed600d57fcee4f94.json ...
```

æ‚¨å¯ä»¥ä½¿ç”¨JSONæˆ–æ–‡æœ¬æ–‡ä»¶æ”¾ç½®å›¾åƒæ ‡ç­¾/æ ‡é¢˜/è¾¹ç•Œæ¡†ã€‚

æœ‰å…³WebDatasetæ ¼å¼å’ŒPythonåº“çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[WebDatasetæ–‡æ¡£](https://webdataset.github.io/webdataset)ã€‚

åŠ è½½æ‚¨çš„WebDatasetï¼Œå®ƒå°†ä¸ºæ¯ä¸ªæ–‡ä»¶åç¼€åˆ›å»ºä¸€åˆ—ï¼ˆè¿™é‡Œæ˜¯â€œjpgâ€å’Œâ€œjsonâ€ï¼‰ï¼š

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("webdataset", data_dir="/path/to/folder", split="train")
>>> dataset[0]["json"]
{"bbox": [[302.0, 109.0, 73.0, 52.0]], "categories": [0]}
```

## åŠ è½½è„šæœ¬

ç¼–å†™æ•°æ®é›†åŠ è½½è„šæœ¬ä»¥å…±äº«æ•°æ®é›†ã€‚å®ƒå®šä¹‰äº†æ•°æ®é›†çš„æ‹†åˆ†å’Œé…ç½®ï¼Œå¹¶å¤„ç†æ•°æ®é›†çš„ä¸‹è½½å’Œç”Ÿæˆã€‚è„šæœ¬ä½äºä¸æ•°æ®é›†ç›¸åŒçš„æ–‡ä»¶å¤¹æˆ–å­˜å‚¨åº“ä¸­ï¼Œå¹¶ä¸”åº”å…·æœ‰ç›¸åŒçš„åç§°ã€‚

```py
my_dataset/
â”œâ”€â”€ README.md
â”œâ”€â”€ my_dataset.py
â””â”€â”€ data/  # optional, may contain your images or TAR archives
```

è¿™ç§ç»“æ„å…è®¸æ‚¨çš„æ•°æ®é›†åœ¨ä¸€è¡Œä¸­åŠ è½½ï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("path/to/my_dataset")
```

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä¸ºå›¾åƒæ•°æ®é›†åˆ›å»ºæ•°æ®é›†åŠ è½½è„šæœ¬ï¼Œè¿™ä¸[ä¸ºæ–‡æœ¬æ•°æ®é›†åˆ›å»ºåŠ è½½è„šæœ¬](./dataset_script)æœ‰äº›ä¸åŒã€‚æ‚¨å°†å­¦ä¹ å¦‚ä½•ï¼š

+   åˆ›å»ºæ•°æ®é›†æ„å»ºå™¨ç±»ã€‚

+   åˆ›å»ºæ•°æ®é›†é…ç½®ã€‚

+   æ·»åŠ æ•°æ®é›†å…ƒæ•°æ®ã€‚

+   ä¸‹è½½å¹¶å®šä¹‰æ•°æ®é›†æ‹†åˆ†ã€‚

+   ç”Ÿæˆæ•°æ®é›†ã€‚

+   ç”Ÿæˆæ•°æ®é›†å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰ã€‚

+   å°†æ•°æ®é›†ä¸Šä¼ åˆ°Hubã€‚

å­¦ä¹ çš„æœ€ä½³æ–¹æ³•æ˜¯æ‰“å¼€ç°æœ‰çš„å›¾åƒæ•°æ®é›†åŠ è½½è„šæœ¬ï¼Œä¾‹å¦‚[Food-101](https://huggingface.co/datasets/food101/blob/main/food101.py)ï¼Œå¹¶è·Ÿéšæ“ä½œï¼

ä¸ºäº†å¸®åŠ©æ‚¨å…¥é—¨ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŠ è½½è„šæœ¬[æ¨¡æ¿](https://github.com/huggingface/datasets/blob/main/templates/new_dataset_script.py)ï¼Œæ‚¨å¯ä»¥å¤åˆ¶å¹¶ç”¨ä½œèµ·ç‚¹ï¼

### åˆ›å»ºæ•°æ®é›†æ„å»ºå™¨ç±»

[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder) æ˜¯ä»å­—å…¸ç”Ÿæˆçš„æ•°æ®é›†çš„åŸºç±»ã€‚åœ¨è¿™ä¸ªç±»ä¸­ï¼Œæœ‰ä¸‰ç§æ–¹æ³•å¯ä»¥å¸®åŠ©åˆ›å»ºæ‚¨çš„æ•°æ®é›†ï¼š

+   `info` å­˜å‚¨æœ‰å…³æ•°æ®é›†çš„ä¿¡æ¯ï¼Œå¦‚æè¿°ã€è®¸å¯è¯å’Œç‰¹å¾ã€‚

+   `split_generators` ä¸‹è½½æ•°æ®é›†å¹¶å®šä¹‰å…¶æ‹†åˆ†ã€‚

+   `generate_examples` ä¸ºæ¯ä¸ªæ‹†åˆ†ç”Ÿæˆå›¾åƒå’Œæ ‡ç­¾ã€‚

é¦–å…ˆå°†æ‚¨çš„æ•°æ®é›†ç±»åˆ›å»ºä¸º[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder)çš„å­ç±»ï¼Œå¹¶æ·»åŠ è¿™ä¸‰ç§æ–¹æ³•ã€‚æš‚æ—¶ä¸ç”¨æ‹…å¿ƒå¡«å†™è¿™äº›æ–¹æ³•ä¸­çš„æ¯ä¸€ä¸ªï¼Œæ‚¨å°†åœ¨æ¥ä¸‹æ¥çš„å‡ ä¸ªéƒ¨åˆ†ä¸­å¼€å‘è¿™äº›æ–¹æ³•ï¼š

```py
class Food101(datasets.GeneratorBasedBuilder):
    """Food-101 Images dataset"""

    def _info(self):

    def _split_generators(self, dl_manager):

    def _generate_examples(self, images, metadata_path):
```

#### å¤šä¸ªé…ç½®

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ•°æ®é›†å¯èƒ½æœ‰å¤šä¸ªé…ç½®ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æŸ¥çœ‹[Imagenetteæ•°æ®é›†](https://huggingface.co/datasets/frgfm/imagenette)ï¼Œæ‚¨ä¼šæ³¨æ„åˆ°æœ‰ä¸‰ä¸ªå­é›†ã€‚

è¦åˆ›å»ºä¸åŒçš„é…ç½®ï¼Œè¯·ä½¿ç”¨[BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)ç±»ä¸ºæ‚¨çš„æ•°æ®é›†åˆ›å»ºä¸€ä¸ªå­ç±»ã€‚åœ¨`data_url`å’Œ`metadata_urls`ä¸­æä¾›ä¸‹è½½å›¾åƒå’Œæ ‡ç­¾çš„é“¾æ¥ï¼š

```py
class Food101Config(datasets.BuilderConfig):
    """Builder Config for Food-101"""

    def __init__(self, data_url, metadata_urls, **kwargs):
        """BuilderConfig for Food-101.
        Args:
          data_url: `string`, url to download the zip file from.
          metadata_urls: dictionary with keys 'train' and 'validation' containing the archive metadata URLs
          **kwargs: keyword arguments forwarded to super.
        """
        super(Food101Config, self).__init__(version=datasets.Version("1.0.0"), **kwargs)
        self.data_url = data_url
        self.metadata_urls = metadata_urls
```

ç°åœ¨æ‚¨å¯ä»¥åœ¨[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder)çš„é¡¶éƒ¨å®šä¹‰æ‚¨çš„å­é›†ã€‚å‡è®¾æ‚¨æƒ³åœ¨Food-101æ•°æ®é›†ä¸­åŸºäºæ—©é¤æˆ–æ™šé¤é£Ÿç‰©åˆ›å»ºä¸¤ä¸ªå­é›†ã€‚

1.  ä½¿ç”¨`Food101Config`åœ¨`BUILDER_CONFIGS`åˆ—è¡¨ä¸­å®šä¹‰æ‚¨çš„å­é›†ã€‚

1.  å¯¹äºæ¯ä¸ªé…ç½®ï¼Œæä¾›åç§°ã€æè¿°ä»¥åŠä»å“ªé‡Œä¸‹è½½å›¾åƒå’Œæ ‡ç­¾ã€‚

```py
class Food101(datasets.GeneratorBasedBuilder):
    """Food-101 Images dataset"""

    BUILDER_CONFIGS = [
        Food101Config(
            name="breakfast",
            description="Food types commonly eaten during breakfast.",
            data_url="https://link-to-breakfast-foods.zip",
            metadata_urls={
                "train": "https://link-to-breakfast-foods-train.txt", 
                "validation": "https://link-to-breakfast-foods-validation.txt"
            },
        ,
        Food101Config(
            name="dinner",
            description="Food types commonly eaten during dinner.",
            data_url="https://link-to-dinner-foods.zip",
            metadata_urls={
                "train": "https://link-to-dinner-foods-train.txt", 
                "validation": "https://link-to-dinner-foods-validation.txt"
            },
        )...
    ]
```

ç°åœ¨ï¼Œå¦‚æœç”¨æˆ·æƒ³è¦åŠ è½½`breakfast`é…ç½®ï¼Œä»–ä»¬å¯ä»¥ä½¿ç”¨é…ç½®åç§°ï¼š

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("food101", "breakfast", split="train")
```

### æ·»åŠ æ•°æ®é›†å…ƒæ•°æ®

æ·»åŠ æœ‰å…³æ•°æ®é›†çš„ä¿¡æ¯å¯¹äºç”¨æˆ·äº†è§£æ›´å¤šä¿¡æ¯å¾ˆæœ‰ç”¨ã€‚è¿™äº›ä¿¡æ¯å­˜å‚¨åœ¨[DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)ç±»ä¸­ï¼Œè¯¥ç±»ç”±`info`æ–¹æ³•è¿”å›ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¿é—®è¿™äº›ä¿¡æ¯ï¼š

```py
>>> from datasets import load_dataset_builder
>>> ds_builder = load_dataset_builder("food101")
>>> ds_builder.info
```

æ‚¨å¯ä»¥æŒ‡å®šæœ‰å…³æ•°æ®é›†çš„å¤§é‡ä¿¡æ¯ï¼Œä½†ä¸€äº›é‡è¦çš„ä¿¡æ¯åŒ…æ‹¬ï¼š

1.  `description` æä¾›äº†æ•°æ®é›†çš„ç®€æ´æè¿°ã€‚

1.  `features` æŒ‡å®šæ•°æ®é›†åˆ—ç±»å‹ã€‚ç”±äºæ‚¨æ­£åœ¨åˆ›å»ºä¸€ä¸ªå›¾åƒåŠ è½½è„šæœ¬ï¼Œæ‚¨éœ€è¦åŒ…å«[Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)ç‰¹å¾ã€‚

1.  `supervised_keys` æŒ‡å®šè¾“å…¥ç‰¹å¾å’Œæ ‡ç­¾ã€‚

1.  `homepage` æä¾›äº†æŒ‡å‘æ•°æ®é›†ä¸»é¡µçš„é“¾æ¥ã€‚

1.  `citation` æ˜¯æ•°æ®é›†çš„ BibTeX å¼•ç”¨ã€‚

1.  `license` è¡¨æ˜æ•°æ®é›†çš„è®¸å¯è¯ã€‚

æ‚¨ä¼šæ³¨æ„åˆ°å¾ˆå¤šæ•°æ®é›†ä¿¡æ¯åœ¨åŠ è½½è„šæœ¬ä¸­è¾ƒæ—©åœ°å®šä¹‰ï¼Œè¿™ä½¿å¾—é˜…è¯»æ›´åŠ å®¹æ˜“ã€‚è¿˜æœ‰å…¶ä»– `~Datasets.Features` æ‚¨å¯ä»¥è¾“å…¥ï¼Œå› æ­¤è¯·åŠ¡å¿…æŸ¥çœ‹å®Œæ•´åˆ—è¡¨ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

```py
def _info(self):
    return datasets.DatasetInfo(
        description=_DESCRIPTION,
        features=datasets.Features(
            {
                "image": datasets.Image(),
                "label": datasets.ClassLabel(names=_NAMES),
            }
        ),
        supervised_keys=("image", "label"),
        homepage=_HOMEPAGE,
        citation=_CITATION,
        license=_LICENSE,
        task_templates=[ImageClassification(image_column="image", label_column="label")],
    )
```

### ä¸‹è½½å¹¶å®šä¹‰æ•°æ®é›†æ‹†åˆ†

ç°åœ¨æ‚¨å·²ç»æ·»åŠ äº†ä¸€äº›å…³äºæ•°æ®é›†çš„ä¿¡æ¯ï¼Œä¸‹ä¸€æ­¥æ˜¯ä¸‹è½½æ•°æ®é›†å¹¶ç”Ÿæˆæ‹†åˆ†ã€‚

1.  ä½¿ç”¨[DownloadManager.download()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download)æ–¹æ³•ä¸‹è½½æ•°æ®é›†å’Œä»»ä½•å…¶ä»–æ‚¨æƒ³è¦å…³è”çš„å…ƒæ•°æ®ã€‚æ­¤æ–¹æ³•æ¥å—ï¼š

    +   å°†åç§°æ˜ å°„åˆ° Hub æ•°æ®é›†å­˜å‚¨åº“ä¸­çš„æ–‡ä»¶ï¼ˆæ¢å¥è¯è¯´ï¼Œ`data/` æ–‡ä»¶å¤¹ï¼‰

    +   æŒ‡å‘å…¶ä»–åœ°æ–¹æ‰˜ç®¡çš„æ–‡ä»¶çš„URL

    +   æ–‡ä»¶åæˆ–URLçš„åˆ—è¡¨æˆ–å­—å…¸

    åœ¨ Food-101 åŠ è½½è„šæœ¬ä¸­ï¼Œæ‚¨ä¼šå†æ¬¡æ³¨æ„åˆ° URL åœ¨è„šæœ¬ä¸­è¾ƒæ—©åœ°å®šä¹‰äº†ã€‚

1.  ä¸‹è½½æ•°æ®é›†åï¼Œä½¿ç”¨[SplitGenerator](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.SplitGenerator)æ¥ç»„ç»‡æ¯ä¸ªæ‹†åˆ†ä¸­çš„å›¾åƒå’Œæ ‡ç­¾ã€‚ä¸ºæ¯ä¸ªæ‹†åˆ†å‘½åä¸€ä¸ªæ ‡å‡†åç§°ï¼Œå¦‚ï¼š`Split.TRAIN`ã€`Split.TEST` å’Œ `SPLIT.Validation`ã€‚

    åœ¨ `gen_kwargs` å‚æ•°ä¸­ï¼ŒæŒ‡å®šè¦è¿­ä»£å’ŒåŠ è½½çš„ `images` æ–‡ä»¶è·¯å¾„ã€‚å¦‚æœéœ€è¦ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[DownloadManager.iter_archive()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.iter_archive)æ¥è¿­ä»£ TAR å­˜æ¡£ä¸­çš„å›¾åƒã€‚æ‚¨è¿˜å¯ä»¥åœ¨ `metadata_path` ä¸­æŒ‡å®šç›¸å…³æ ‡ç­¾ã€‚`images` å’Œ `metadata_path` å®é™…ä¸Šæ˜¯ä¼ é€’åˆ°ä¸‹ä¸€æ­¥çš„ï¼Œæ‚¨å°†åœ¨é‚£é‡Œå®é™…ç”Ÿæˆæ•°æ®é›†ã€‚

è¦æµå¼ä¼ è¾“ä¸€ä¸ª TAR å­˜æ¡£æ–‡ä»¶ï¼Œæ‚¨éœ€è¦ä½¿ç”¨[DownloadManager.iter_archive()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.iter_archive)ï¼[DownloadManager.download_and_extract()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download_and_extract)å‡½æ•°ä¸æ”¯æŒæµå¼ä¼ è¾“æ¨¡å¼ä¸‹çš„ TAR å­˜æ¡£ã€‚

```py
def _split_generators(self, dl_manager):
    archive_path = dl_manager.download(_BASE_URL)
    split_metadata_paths = dl_manager.download(_METADATA_URLS)
    return [
        datasets.SplitGenerator(
            name=datasets.Split.TRAIN,
            gen_kwargs={
                "images": dl_manager.iter_archive(archive_path),
                "metadata_path": split_metadata_paths["train"],
            },
        ),
        datasets.SplitGenerator(
            name=datasets.Split.VALIDATION,
            gen_kwargs={
                "images": dl_manager.iter_archive(archive_path),
                "metadata_path": split_metadata_paths["test"],
            },
        ),
    ]
```

### ç”Ÿæˆæ•°æ®é›†

[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder) ç±»ä¸­çš„æœ€åä¸€ä¸ªæ–¹æ³•å®é™…ä¸Šç”Ÿæˆäº†æ•°æ®é›†ä¸­çš„å›¾åƒå’Œæ ‡ç­¾ã€‚å®ƒæ ¹æ® `info` æ–¹æ³•ä¸­æŒ‡å®šçš„ `features` ç»“æ„ç”Ÿæˆæ•°æ®é›†ã€‚æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œ`generate_examples` æ¥å—æ¥è‡ªå‰ä¸€ä¸ªæ–¹æ³•çš„ `images` å’Œ `metadata_path` ä½œä¸ºå‚æ•°ã€‚

è¦æµå¼ä¼ è¾“ä¸€ä¸ª TAR å­˜æ¡£æ–‡ä»¶ï¼Œéœ€è¦å…ˆæ‰“å¼€å¹¶è¯»å– `metadata_path`ã€‚TAR æ–‡ä»¶æ˜¯æŒ‰é¡ºåºè®¿é—®å’Œç”Ÿæˆçš„ã€‚è¿™æ„å‘³ç€æ‚¨éœ€è¦å…ˆæŒæ¡å…ƒæ•°æ®ä¿¡æ¯ï¼Œä»¥ä¾¿èƒ½å¤Ÿå°†å…¶ä¸ç›¸åº”çš„å›¾åƒä¸€èµ·ç”Ÿæˆã€‚

ç°åœ¨æ‚¨å¯ä»¥ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥æ‰“å¼€å’ŒåŠ è½½æ•°æ®é›†ä¸­çš„ç¤ºä¾‹ï¼š

```py
def _generate_examples(self, images, metadata_path):
    """Generate images and labels for splits."""
    with open(metadata_path, encoding="utf-8") as f:
        files_to_keep = set(f.read().split("\n"))
    for file_path, file_obj in images:
        if file_path.startswith(_IMAGES_DIR):
            if file_path[len(_IMAGES_DIR) : -len(".jpg")] in files_to_keep:
                label = file_path.split("/")[2]
                yield file_path, {
                    "image": {"path": file_path, "bytes": file_obj.read()},
                    "label": label,
                }
```

### ç”Ÿæˆæ•°æ®é›†å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰

æ•°æ®é›†å…ƒæ•°æ®å¯ä»¥ç”Ÿæˆå¹¶å­˜å‚¨åœ¨æ•°æ®é›†å¡ç‰‡ï¼ˆ`README.md`æ–‡ä»¶ï¼‰ä¸­ã€‚

è¿è¡Œä»¥ä¸‹å‘½ä»¤åœ¨ `README.md` ä¸­ç”Ÿæˆæ•°æ®é›†å…ƒæ•°æ®ï¼Œå¹¶ç¡®ä¿æ‚¨çš„æ–°åŠ è½½è„šæœ¬èƒ½å¤Ÿæ­£å¸¸å·¥ä½œï¼š

```py
datasets-cli test path/to/<your-dataset-loading-script> --save_info --all_configs
```

å¦‚æœæ‚¨çš„åŠ è½½è„šæœ¬é€šè¿‡äº†æµ‹è¯•ï¼Œç°åœ¨æ‚¨åº”è¯¥åœ¨æ•°æ®é›†æ–‡ä»¶å¤¹ä¸­çš„ `README.md` æ–‡ä»¶çš„å¤´éƒ¨æœ‰ `dataset_info` YAML å­—æ®µã€‚

### å°†æ•°æ®é›†ä¸Šä¼ åˆ° Hub

ä¸€æ—¦æ‚¨çš„è„šæœ¬å‡†å¤‡å¥½äº†ï¼Œ[åˆ›å»ºä¸€ä¸ªæ•°æ®é›†å¡ç‰‡](./dataset_card)å¹¶[å°†å…¶ä¸Šä¼ åˆ° Hub](./share)ã€‚

æ­å–œï¼Œæ‚¨ç°åœ¨å¯ä»¥ä» Hub åŠ è½½æ‚¨çš„æ•°æ®é›†äº†ï¼ğŸ¥³

```py
>>> from datasets import load_dataset
>>> load_dataset("<username>/my_dataset")
```
