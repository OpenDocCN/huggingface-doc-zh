# 创建图像数据集

> 原始文本：[https://huggingface.co/docs/datasets/image_dataset](https://huggingface.co/docs/datasets/image_dataset)

有两种方法可以创建和共享图像数据集。本指南将向您展示如何：

+   使用`ImageFolder`和一些元数据创建图像数据集。这是一个无代码解决方案，可快速创建包含数千张图像的图像数据集。

+   通过编写加载脚本创建图像数据集。这种方法稍微复杂一些，但您可以更灵活地定义、下载和生成数据集，这对于更复杂或大规模的图像数据集可能很有用。

您可以通过要求用户首先共享其联系信息来控制对数据集的访问。查看有关如何在Hub上启用此功能的更多信息，请参阅[Gated datasets](https://huggingface.co/docs/hub/datasets-gated)指南。

## ImageFolder

`ImageFolder`是一个数据集构建器，旨在快速加载包含数千张图像的图像数据集，而无需编写任何代码。

💡查看[Split pattern hierarchy](repository_structure#split-pattern-hierarchy)以了解有关`ImageFolder`如何基于数据集存储库结构创建数据集拆分的更多信息。

`ImageFolder`会根据目录名称自动推断数据集的类标签。将数据集存储在以下结构的目录中：

```py
folder/train/dog/golden_retriever.png
folder/train/dog/german_shepherd.png
folder/train/dog/chihuahua.png

folder/train/cat/maine_coon.png
folder/train/cat/bengal.png
folder/train/cat/birman.png
```

然后用户可以通过在[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)中指定`imagefolder`和`data_dir`中的目录来加载数据集：

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("imagefolder", data_dir="/path/to/folder")
```

您还可以使用`imagefolder`加载涉及多个拆分的数据集。为此，您的数据集目录应具有以下结构：

```py
folder/train/dog/golden_retriever.png
folder/train/cat/maine_coon.png
folder/test/dog/german_shepherd.png
folder/test/cat/bengal.png
```

如果所有图像文件都包含在单个目录中，或者它们不在相同级别的目录结构中，`label`列将不会自动添加。如果需要，请显式设置`drop_labels=False`。

如果您想要包含有关数据集的其他信息，如文本标题或边界框，请将其添加为`metadata.csv`文件放在您的文件夹中。这样可以快速为不同的计算机视觉任务创建数据集，如文本标题或目标检测。您还可以使用JSONL文件`metadata.jsonl`。

```py
folder/train/metadata.csv
folder/train/0001.png
folder/train/0002.png
folder/train/0003.png
```

您还可以压缩您的图像：

```py
folder/metadata.csv
folder/train.zip
folder/test.zip
folder/valid.zip
```

您的`metadata.csv`文件必须具有一个`file_name`列，将图像文件与其元数据链接起来：

```py
file_name,additional_feature
0001.png,This is a first value of a text feature you added to your images
0002.png,This is a second value of a text feature you added to your images
0003.png,This is a third value of a text feature you added to your images
```

或者使用`metadata.jsonl`：

```py
{"file_name": "0001.png", "additional_feature": "This is a first value of a text feature you added to your images"}
{"file_name": "0002.png", "additional_feature": "This is a second value of a text feature you added to your images"}
{"file_name": "0003.png", "additional_feature": "This is a third value of a text feature you added to your images"}
```

如果存在元数据文件，则默认情况下基于目录名称推断的标签将被删除。要包含这些标签，请在`load_dataset`中设置`drop_labels=False`。

### 图像标题

图像标题数据集包含描述图像的文本。一个示例`metadata.csv`可能如下所示：

```py
file_name,text
0001.png,This is a golden retriever playing with a ball
0002.png,A german shepherd
0003.png,One chihuahua
```

使用`ImageFolder`加载数据集，它将为图像标题创建一个`text`列：

```py
>>> dataset = load_dataset("imagefolder", data_dir="/path/to/folder", split="train")
>>> dataset[0]["text"]
"This is a golden retriever playing with a ball"
```

### 目标检测

目标检测数据集具有边界框和类别，用于识别图像中的对象。一个示例`metadata.jsonl`可能如下所示：

```py
{"file_name": "0001.png", "objects": {"bbox": [[302.0, 109.0, 73.0, 52.0]], "categories": [0]}}
{"file_name": "0002.png", "objects": {"bbox": [[810.0, 100.0, 57.0, 28.0]], "categories": [1]}}
{"file_name": "0003.png", "objects": {"bbox": [[160.0, 31.0, 248.0, 616.0], [741.0, 68.0, 202.0, 401.0]], "categories": [2, 2]}}
```

使用`ImageFolder`加载数据集，它将创建一个带有边界框和类别的`objects`列：

```py
>>> dataset = load_dataset("imagefolder", data_dir="/path/to/folder", split="train")
>>> dataset[0]["objects"]
{"bbox": [[302.0, 109.0, 73.0, 52.0]], "categories": [0]}
```

### 将数据集上传到Hub

创建数据集后，您可以使用[push_to_hub()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict.push_to_hub)方法将其共享到Hub。确保您已安装[huggingface_hub](https://huggingface.co/docs/huggingface_hub/index)库，并且已登录到您的Hugging Face帐户（有关更多详细信息，请参阅[使用Python上传教程](upload_dataset#upload-with-python)）。

使用[push_to_hub()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict.push_to_hub)上传您的数据集：

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("imagefolder", data_dir="/path/to/folder", split="train")
>>> dataset.push_to_hub("stevhliu/my-image-captioning-dataset")
```

## WebDataset

[WebDataset](https://github.com/webdataset/webdataset)格式基于TAR存档，适用于大型图像数据集。实际上，您可以将图像分组在TAR存档中（例如每个TAR存档1GB的图像）并拥有数千个TAR存档：

```py
folder/train/00000.tar
folder/train/00001.tar
folder/train/00002.tar
...
```

在存档中，每个示例由共享相同前缀的文件组成：

```py
e39871fd9fd74f55.jpg e39871fd9fd74f55.json f18b91585c4d3f3e.jpg f18b91585c4d3f3e.json ede6e66b2fb59aab.jpg ede6e66b2fb59aab.json ed600d57fcee4f94.jpg ed600d57fcee4f94.json ...
```

您可以使用JSON或文本文件放置图像标签/标题/边界框。

有关WebDataset格式和Python库的更多详细信息，请查看[WebDataset文档](https://webdataset.github.io/webdataset)。

加载您的WebDataset，它将为每个文件后缀创建一列（这里是“jpg”和“json”）：

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("webdataset", data_dir="/path/to/folder", split="train")
>>> dataset[0]["json"]
{"bbox": [[302.0, 109.0, 73.0, 52.0]], "categories": [0]}
```

## 加载脚本

编写数据集加载脚本以共享数据集。它定义了数据集的拆分和配置，并处理数据集的下载和生成。脚本位于与数据集相同的文件夹或存储库中，并且应具有相同的名称。

```py
my_dataset/
├── README.md
├── my_dataset.py
└── data/  # optional, may contain your images or TAR archives
```

这种结构允许您的数据集在一行中加载：

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("path/to/my_dataset")
```

本指南将向您展示如何为图像数据集创建数据集加载脚本，这与[为文本数据集创建加载脚本](./dataset_script)有些不同。您将学习如何：

+   创建数据集构建器类。

+   创建数据集配置。

+   添加数据集元数据。

+   下载并定义数据集拆分。

+   生成数据集。

+   生成数据集元数据（可选）。

+   将数据集上传到Hub。

学习的最佳方法是打开现有的图像数据集加载脚本，例如[Food-101](https://huggingface.co/datasets/food101/blob/main/food101.py)，并跟随操作！

为了帮助您入门，我们创建了一个加载脚本[模板](https://github.com/huggingface/datasets/blob/main/templates/new_dataset_script.py)，您可以复制并用作起点！

### 创建数据集构建器类

[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder) 是从字典生成的数据集的基类。在这个类中，有三种方法可以帮助创建您的数据集：

+   `info` 存储有关数据集的信息，如描述、许可证和特征。

+   `split_generators` 下载数据集并定义其拆分。

+   `generate_examples` 为每个拆分生成图像和标签。

首先将您的数据集类创建为[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder)的子类，并添加这三种方法。暂时不用担心填写这些方法中的每一个，您将在接下来的几个部分中开发这些方法：

```py
class Food101(datasets.GeneratorBasedBuilder):
    """Food-101 Images dataset"""

    def _info(self):

    def _split_generators(self, dl_manager):

    def _generate_examples(self, images, metadata_path):
```

#### 多个配置

在某些情况下，数据集可能有多个配置。例如，如果您查看[Imagenette数据集](https://huggingface.co/datasets/frgfm/imagenette)，您会注意到有三个子集。

要创建不同的配置，请使用[BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)类为您的数据集创建一个子类。在`data_url`和`metadata_urls`中提供下载图像和标签的链接：

```py
class Food101Config(datasets.BuilderConfig):
    """Builder Config for Food-101"""

    def __init__(self, data_url, metadata_urls, **kwargs):
        """BuilderConfig for Food-101.
        Args:
          data_url: `string`, url to download the zip file from.
          metadata_urls: dictionary with keys 'train' and 'validation' containing the archive metadata URLs
          **kwargs: keyword arguments forwarded to super.
        """
        super(Food101Config, self).__init__(version=datasets.Version("1.0.0"), **kwargs)
        self.data_url = data_url
        self.metadata_urls = metadata_urls
```

现在您可以在[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder)的顶部定义您的子集。假设您想在Food-101数据集中基于早餐或晚餐食物创建两个子集。

1.  使用`Food101Config`在`BUILDER_CONFIGS`列表中定义您的子集。

1.  对于每个配置，提供名称、描述以及从哪里下载图像和标签。

```py
class Food101(datasets.GeneratorBasedBuilder):
    """Food-101 Images dataset"""

    BUILDER_CONFIGS = [
        Food101Config(
            name="breakfast",
            description="Food types commonly eaten during breakfast.",
            data_url="https://link-to-breakfast-foods.zip",
            metadata_urls={
                "train": "https://link-to-breakfast-foods-train.txt", 
                "validation": "https://link-to-breakfast-foods-validation.txt"
            },
        ,
        Food101Config(
            name="dinner",
            description="Food types commonly eaten during dinner.",
            data_url="https://link-to-dinner-foods.zip",
            metadata_urls={
                "train": "https://link-to-dinner-foods-train.txt", 
                "validation": "https://link-to-dinner-foods-validation.txt"
            },
        )...
    ]
```

现在，如果用户想要加载`breakfast`配置，他们可以使用配置名称：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset("food101", "breakfast", split="train")
```

### 添加数据集元数据

添加有关数据集的信息对于用户了解更多信息很有用。这些信息存储在[DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)类中，该类由`info`方法返回。用户可以通过以下方式访问这些信息：

```py
>>> from datasets import load_dataset_builder
>>> ds_builder = load_dataset_builder("food101")
>>> ds_builder.info
```

您可以指定有关数据集的大量信息，但一些重要的信息包括：

1.  `description` 提供了数据集的简洁描述。

1.  `features` 指定数据集列类型。由于您正在创建一个图像加载脚本，您需要包含[Image](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Image)特征。

1.  `supervised_keys` 指定输入特征和标签。

1.  `homepage` 提供了指向数据集主页的链接。

1.  `citation` 是数据集的 BibTeX 引用。

1.  `license` 表明数据集的许可证。

您会注意到很多数据集信息在加载脚本中较早地定义，这使得阅读更加容易。还有其他 `~Datasets.Features` 您可以输入，因此请务必查看完整列表以获取更多详细信息。

```py
def _info(self):
    return datasets.DatasetInfo(
        description=_DESCRIPTION,
        features=datasets.Features(
            {
                "image": datasets.Image(),
                "label": datasets.ClassLabel(names=_NAMES),
            }
        ),
        supervised_keys=("image", "label"),
        homepage=_HOMEPAGE,
        citation=_CITATION,
        license=_LICENSE,
        task_templates=[ImageClassification(image_column="image", label_column="label")],
    )
```

### 下载并定义数据集拆分

现在您已经添加了一些关于数据集的信息，下一步是下载数据集并生成拆分。

1.  使用[DownloadManager.download()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download)方法下载数据集和任何其他您想要关联的元数据。此方法接受：

    +   将名称映射到 Hub 数据集存储库中的文件（换句话说，`data/` 文件夹）

    +   指向其他地方托管的文件的URL

    +   文件名或URL的列表或字典

    在 Food-101 加载脚本中，您会再次注意到 URL 在脚本中较早地定义了。

1.  下载数据集后，使用[SplitGenerator](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.SplitGenerator)来组织每个拆分中的图像和标签。为每个拆分命名一个标准名称，如：`Split.TRAIN`、`Split.TEST` 和 `SPLIT.Validation`。

    在 `gen_kwargs` 参数中，指定要迭代和加载的 `images` 文件路径。如果需要，您可以使用[DownloadManager.iter_archive()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.iter_archive)来迭代 TAR 存档中的图像。您还可以在 `metadata_path` 中指定相关标签。`images` 和 `metadata_path` 实际上是传递到下一步的，您将在那里实际生成数据集。

要流式传输一个 TAR 存档文件，您需要使用[DownloadManager.iter_archive()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.iter_archive)！[DownloadManager.download_and_extract()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download_and_extract)函数不支持流式传输模式下的 TAR 存档。

```py
def _split_generators(self, dl_manager):
    archive_path = dl_manager.download(_BASE_URL)
    split_metadata_paths = dl_manager.download(_METADATA_URLS)
    return [
        datasets.SplitGenerator(
            name=datasets.Split.TRAIN,
            gen_kwargs={
                "images": dl_manager.iter_archive(archive_path),
                "metadata_path": split_metadata_paths["train"],
            },
        ),
        datasets.SplitGenerator(
            name=datasets.Split.VALIDATION,
            gen_kwargs={
                "images": dl_manager.iter_archive(archive_path),
                "metadata_path": split_metadata_paths["test"],
            },
        ),
    ]
```

### 生成数据集

[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder) 类中的最后一个方法实际上生成了数据集中的图像和标签。它根据 `info` 方法中指定的 `features` 结构生成数据集。正如您所看到的，`generate_examples` 接受来自前一个方法的 `images` 和 `metadata_path` 作为参数。

要流式传输一个 TAR 存档文件，需要先打开并读取 `metadata_path`。TAR 文件是按顺序访问和生成的。这意味着您需要先掌握元数据信息，以便能够将其与相应的图像一起生成。

现在您可以编写一个函数来打开和加载数据集中的示例：

```py
def _generate_examples(self, images, metadata_path):
    """Generate images and labels for splits."""
    with open(metadata_path, encoding="utf-8") as f:
        files_to_keep = set(f.read().split("\n"))
    for file_path, file_obj in images:
        if file_path.startswith(_IMAGES_DIR):
            if file_path[len(_IMAGES_DIR) : -len(".jpg")] in files_to_keep:
                label = file_path.split("/")[2]
                yield file_path, {
                    "image": {"path": file_path, "bytes": file_obj.read()},
                    "label": label,
                }
```

### 生成数据集元数据（可选）

数据集元数据可以生成并存储在数据集卡片（`README.md`文件）中。

运行以下命令在 `README.md` 中生成数据集元数据，并确保您的新加载脚本能够正常工作：

```py
datasets-cli test path/to/<your-dataset-loading-script> --save_info --all_configs
```

如果您的加载脚本通过了测试，现在您应该在数据集文件夹中的 `README.md` 文件的头部有 `dataset_info` YAML 字段。

### 将数据集上传到 Hub

一旦您的脚本准备好了，[创建一个数据集卡片](./dataset_card)并[将其上传到 Hub](./share)。

恭喜，您现在可以从 Hub 加载您的数据集了！🥳

```py
>>> from datasets import load_dataset
>>> load_dataset("<username>/my_dataset")
```
