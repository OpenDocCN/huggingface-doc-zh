- en: The tokenization pipeline
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/tokenizers/pipeline](https://huggingface.co/docs/tokenizers/pipeline)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'When calling `Tokenizer.encode` or `Tokenizer.encode_batch`, the input text(s)
    go through the following pipeline:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '`normalization`'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pre-tokenization`'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model`'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`post-processing`'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weâ€™ll see in details what happens during each of those steps in detail, as well
    as when you want to `decode <decoding>` some token ids, and how the ğŸ¤— Tokenizers
    library allows you to customize each of those steps to your needs. If youâ€™re already
    familiar with those steps and want to learn by seeing some code, jump to `our
    BERT from scratch example <example>`.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'For the examples that require a `Tokenizer` we will use the tokenizer we trained
    in the `quicktour`, which you can load with:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: PythonRustNode
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Normalization
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normalization is, in a nutshell, a set of operations you apply to a raw string
    to make it less random or â€œcleanerâ€. Common operations include stripping whitespace,
    removing accented characters or lowercasing all text. If youâ€™re familiar with
    [Unicode normalization](https://unicode.org/reports/tr15), it is also a very common
    normalization operation applied in most tokenizers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Each normalization operation is represented in the ğŸ¤— Tokenizers library by
    a `Normalizer`, and you can combine several of those by using a `normalizers.Sequence`.
    Here is a normalizer applying NFD Unicode normalization and removing accents as
    an example:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: PythonRustNode
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can manually test that normalizer by applying it to any string:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: PythonRustNode
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When building a `Tokenizer`, you can customize its normalizer by just changing
    the corresponding attribute:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: PythonRustNode
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Of course, if you change the way a tokenizer applies normalization, you should
    probably retrain it from scratch afterward.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Pre-Tokenization
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pre-tokenization is the act of splitting a text into smaller objects that give
    an upper bound to what your tokens will be at the end of training. A good way
    to think of this is that the pre-tokenizer will split your text into â€œwordsâ€ and
    then, your final tokens will be parts of those words.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'An easy way to pre-tokenize inputs is to split on spaces and punctuations,
    which is done by the `pre_tokenizers.Whitespace` pre-tokenizer:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: PythonRustNode
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The output is a list of tuples, with each tuple containing one word and its
    span in the original sentence (which is used to determine the final `offsets`
    of our `Encoding`). Note that splitting on punctuation will split contractions
    like `"I'm"` in this example.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'You can combine together any `PreTokenizer` together. For instance, here is
    a pre-tokenizer that will split on space, punctuation and digits, separating numbers
    in their individual digits:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: PythonRustNode
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As we saw in the `quicktour`, you can customize the pre-tokenizer of a `Tokenizer`
    by just changing the corresponding attribute:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: PythonRustNode
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Of course, if you change the way the pre-tokenizer, you should probably retrain
    your tokenizer from scratch afterward.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Model
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the input texts are normalized and pre-tokenized, the `Tokenizer` applies
    the model on the pre-tokens. This is the part of the pipeline that needs training
    on your corpus (or that has been trained if you are using a pretrained tokenizer).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The role of the model is to split your â€œwordsâ€ into tokens, using the rules
    it has learned. Itâ€™s also responsible for mapping those tokens to their corresponding
    IDs in the vocabulary of the model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'This model is passed along when intializing the `Tokenizer` so you already
    know how to customize this part. Currently, the ğŸ¤— Tokenizers library supports:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '`models.BPE`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models.Unigram`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models.WordLevel`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models.WordPiece`'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more details about each model and its behavior, you can check [here](components#models)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Post-Processing
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Post-processing is the last step of the tokenization pipeline, to perform any
    additional transformation to the `Encoding` before itâ€™s returned, like adding
    potential special tokens.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in the quick tour, we can customize the post processor of a `Tokenizer`
    by setting the corresponding attribute. For instance, here is how we can post-process
    to make the inputs suitable for the BERT model:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨å¿«é€Ÿæµè§ˆä¸­çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½®ç›¸åº”çš„å±æ€§æ¥è‡ªå®šä¹‰`Tokenizer`çš„åå¤„ç†å™¨ã€‚ä¾‹å¦‚ï¼Œè¿™æ˜¯æˆ‘ä»¬å¦‚ä½•è¿›è¡Œåå¤„ç†ä»¥ä½¿è¾“å…¥é€‚åˆBERTæ¨¡å‹ï¼š
- en: PythonRustNode
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that contrarily to the pre-tokenizer or the normalizer, you donâ€™t need
    to retrain a tokenizer after changing its post-processor.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä¸é¢„åˆ†è¯å™¨æˆ–è§„èŒƒåŒ–å™¨ç›¸åï¼Œæ›´æ”¹åå¤„ç†å™¨åä¸éœ€è¦é‡æ–°è®­ç»ƒåˆ†è¯å™¨ã€‚
- en: 'All together: a BERT tokenizer from scratch'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰€æœ‰åœ¨ä¸€èµ·ï¼šä»å¤´å¼€å§‹çš„BERTåˆ†è¯å™¨
- en: 'Letâ€™s put all those pieces together to build a BERT tokenizer. First, BERT
    relies on WordPiece, so we instantiate a new `Tokenizer` with this model:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æŠŠæ‰€æœ‰è¿™äº›éƒ¨åˆ†æ”¾åœ¨ä¸€èµ·æ¥æ„å»ºä¸€ä¸ªBERTåˆ†è¯å™¨ã€‚é¦–å…ˆï¼ŒBERTä¾èµ–äºWordPieceï¼Œå› æ­¤æˆ‘ä»¬ç”¨è¿™ä¸ªæ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªæ–°çš„`Tokenizer`ï¼š
- en: PythonRustNode
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then we know that BERT preprocesses texts by removing accents and lowercasing.
    We also use a unicode normalizer:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬çŸ¥é“BERTé€šè¿‡å»é™¤é‡éŸ³ç¬¦å·å’Œå°å†™æ¥é¢„å¤„ç†æ–‡æœ¬ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨äº†ä¸€ä¸ªUnicodeè§„èŒƒåŒ–å™¨ï¼š
- en: PythonRustNode
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The pre-tokenizer is just splitting on whitespace and punctuation:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„åˆ†è¯å™¨åªæ˜¯åœ¨ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·ä¸Šè¿›è¡Œåˆ†å‰²ï¼š
- en: PythonRustNode
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And the post-processing uses the template we saw in the previous section:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åå¤„ç†ä½¿ç”¨äº†æˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚ä¸­çœ‹åˆ°çš„æ¨¡æ¿ï¼š
- en: PythonRustNode
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can use this tokenizer and train on it on wikitext like in the `quicktour`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªåˆ†è¯å™¨ï¼Œå¹¶åœ¨wikitextä¸Šè¿›è¡Œè®­ç»ƒï¼Œå°±åƒåœ¨`quicktour`ä¸­ä¸€æ ·ï¼š
- en: PythonRustNode
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Decoding
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§£ç 
- en: On top of encoding the input texts, a `Tokenizer` also has an API for decoding,
    that is converting IDs generated by your model back to a text. This is done by
    the methods `Tokenizer.decode` (for one predicted text) and `Tokenizer.decode_batch`
    (for a batch of predictions).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç å¤–ï¼Œ`Tokenizer`è¿˜å…·æœ‰ç”¨äºè§£ç çš„APIï¼Œå³å°†æ¨¡å‹ç”Ÿæˆçš„IDè½¬æ¢å›æ–‡æœ¬ã€‚è¿™æ˜¯é€šè¿‡æ–¹æ³•`Tokenizer.decode`ï¼ˆç”¨äºä¸€ä¸ªé¢„æµ‹æ–‡æœ¬ï¼‰å’Œ`Tokenizer.decode_batch`ï¼ˆç”¨äºä¸€æ‰¹é¢„æµ‹ï¼‰æ¥å®Œæˆçš„ã€‚
- en: 'The `decoder` will first convert the IDs back to tokens (using the tokenizerâ€™s
    vocabulary) and remove all special tokens, then join those tokens with spaces:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`decoder`é¦–å…ˆå°†IDè½¬æ¢å›æ ‡è®°ï¼ˆä½¿ç”¨åˆ†è¯å™¨çš„è¯æ±‡è¡¨ï¼‰ï¼Œç„¶ååˆ é™¤æ‰€æœ‰ç‰¹æ®Šæ ‡è®°ï¼Œç„¶åç”¨ç©ºæ ¼è¿æ¥è¿™äº›æ ‡è®°ï¼š'
- en: PythonRustNode
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If you used a model that added special characters to represent subtokens of
    a given â€œwordâ€ (like the `"##"` in WordPiece) you will need to customize the `decoder`
    to treat them properly. If we take our previous `bert_tokenizer` for instance
    the default decoding will give:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä½¿ç”¨äº†ä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ·»åŠ äº†ç‰¹æ®Šå­—ç¬¦æ¥è¡¨ç¤ºç»™å®šâ€œå•è¯â€çš„å­æ ‡è®°ï¼ˆä¾‹å¦‚WordPieceä¸­çš„`"##"`ï¼‰ï¼Œåˆ™éœ€è¦è‡ªå®šä¹‰`decoder`ä»¥æ­£ç¡®å¤„ç†å®ƒä»¬ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä»¥å‰çš„`bert_tokenizer`ï¼Œé»˜è®¤è§£ç å°†ä¼šç»™å‡ºï¼š
- en: PythonRustNode
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'But by changing it to a proper decoder, we get:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†é€šè¿‡å°†å…¶æ›´æ”¹ä¸ºé€‚å½“çš„è§£ç å™¨ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
- en: PythonRustNode
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
