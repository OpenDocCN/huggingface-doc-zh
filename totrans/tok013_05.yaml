- en: The tokenization pipeline
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记化管道
- en: 'Original text: [https://huggingface.co/docs/tokenizers/pipeline](https://huggingface.co/docs/tokenizers/pipeline)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/tokenizers/pipeline](https://huggingface.co/docs/tokenizers/pipeline)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'When calling `Tokenizer.encode` or `Tokenizer.encode_batch`, the input text(s)
    go through the following pipeline:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用`Tokenizer.encode`或`Tokenizer.encode_batch`时，输入文本将通过以下管道进行处理：
- en: '`normalization`'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`归一化`'
- en: '`pre-tokenization`'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`预分词`'
- en: '`model`'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`模型`'
- en: '`post-processing`'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`后处理`'
- en: We’ll see in details what happens during each of those steps in detail, as well
    as when you want to `decode <decoding>` some token ids, and how the 🤗 Tokenizers
    library allows you to customize each of those steps to your needs. If you’re already
    familiar with those steps and want to learn by seeing some code, jump to `our
    BERT from scratch example <example>`.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细了解每个步骤的详细情况，以及当您想要`解码<解码>`一些标记ID时，以及🤗 Tokenizers库如何允许您根据需要自定义每个步骤。如果您已经熟悉这些步骤，并希望通过查看一些代码来学习，请跳转到`我们的BERT从头开始示例<示例>`。
- en: 'For the examples that require a `Tokenizer` we will use the tokenizer we trained
    in the `quicktour`, which you can load with:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要`Tokenizer`的示例，我们将使用在`快速浏览`中训练的分词器，您可以通过以下方式加载：
- en: PythonRustNode
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Normalization
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 归一化
- en: Normalization is, in a nutshell, a set of operations you apply to a raw string
    to make it less random or “cleaner”. Common operations include stripping whitespace,
    removing accented characters or lowercasing all text. If you’re familiar with
    [Unicode normalization](https://unicode.org/reports/tr15), it is also a very common
    normalization operation applied in most tokenizers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化，简而言之，是对原始字符串应用的一组操作，使其更少随机或“更干净”。常见操作包括去除空格、去除重音字符或将所有文本转换为小写。如果您熟悉[Unicode规范化](https://unicode.org/reports/tr15)，那么大多数分词器中也会应用非常常见的规范化操作。
- en: 'Each normalization operation is represented in the 🤗 Tokenizers library by
    a `Normalizer`, and you can combine several of those by using a `normalizers.Sequence`.
    Here is a normalizer applying NFD Unicode normalization and removing accents as
    an example:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 每个规范化操作在🤗 Tokenizers库中由一个`Normalizer`表示，您可以通过使用`normalizers.Sequence`组合其中的几个。这是一个应用NFD
    Unicode规范化并去除重音的规范化器示例：
- en: PythonRustNode
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can manually test that normalizer by applying it to any string:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将其应用于任何字符串来手动测试该规范化器：
- en: PythonRustNode
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When building a `Tokenizer`, you can customize its normalizer by just changing
    the corresponding attribute:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 构建`Tokenizer`时，您可以通过更改相应属性来自定义其规范化：
- en: PythonRustNode
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Of course, if you change the way a tokenizer applies normalization, you should
    probably retrain it from scratch afterward.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果更改了分词器应用归一化的方式，那么之后可能需要从头开始重新训练它。
- en: Pre-Tokenization
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预分词
- en: Pre-tokenization is the act of splitting a text into smaller objects that give
    an upper bound to what your tokens will be at the end of training. A good way
    to think of this is that the pre-tokenizer will split your text into “words” and
    then, your final tokens will be parts of those words.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 预分词化是将文本分割为较小对象的行为，这些对象给出了在训练结束时您的标记将是什么的上限。一个好的思考方式是，预分词器将把您的文本分割成“单词”，然后，您的最终标记将是这些单词的一部分。
- en: 'An easy way to pre-tokenize inputs is to split on spaces and punctuations,
    which is done by the `pre_tokenizers.Whitespace` pre-tokenizer:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的预分词输入的方法是在空格和标点符号上进行分割，这是由`pre_tokenizers.Whitespace`预分词器完成的：
- en: PythonRustNode
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The output is a list of tuples, with each tuple containing one word and its
    span in the original sentence (which is used to determine the final `offsets`
    of our `Encoding`). Note that splitting on punctuation will split contractions
    like `"I'm"` in this example.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个元组列表，每个元组包含一个单词及其在原始句子中的跨度（用于确定我们的`Encoding`的最终`offsets`）。请注意，根据标点符号分割将分割像`"I'm"`这个例子中的缩写。
- en: 'You can combine together any `PreTokenizer` together. For instance, here is
    a pre-tokenizer that will split on space, punctuation and digits, separating numbers
    in their individual digits:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将任何`PreTokenizer`组合在一起。例如，这是一个预分词器，将在空格、标点符号和数字上进行分割，将数字分隔为各自的数字：
- en: PythonRustNode
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As we saw in the `quicktour`, you can customize the pre-tokenizer of a `Tokenizer`
    by just changing the corresponding attribute:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在`快速浏览`中看到的，您可以通过更改相应属性来自定义`Tokenizer`的预分词器：
- en: PythonRustNode
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Of course, if you change the way the pre-tokenizer, you should probably retrain
    your tokenizer from scratch afterward.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果更改了预分词器的方式，那么之后可能需要从头开始重新训练您的分词器。
- en: Model
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: Once the input texts are normalized and pre-tokenized, the `Tokenizer` applies
    the model on the pre-tokens. This is the part of the pipeline that needs training
    on your corpus (or that has been trained if you are using a pretrained tokenizer).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦输入文本被归一化和预分词，`Tokenizer`将在预分词上应用模型。这是管道中需要在您的语料库上进行训练的部分（或者如果您使用预训练的分词器，则已经训练过）。
- en: The role of the model is to split your “words” into tokens, using the rules
    it has learned. It’s also responsible for mapping those tokens to their corresponding
    IDs in the vocabulary of the model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的作用是根据其学习的规则将您的“单词”分割为标记。它还负责将这些标记映射到模型词汇表中的相应ID。
- en: 'This model is passed along when intializing the `Tokenizer` so you already
    know how to customize this part. Currently, the 🤗 Tokenizers library supports:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型在初始化`Tokenizer`时传递，所以您已经知道如何自定义这部分。目前，🤗 Tokenizers库支持：
- en: '`models.BPE`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`models.BPE`'
- en: '`models.Unigram`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`models.Unigram`'
- en: '`models.WordLevel`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`models.WordLevel`'
- en: '`models.WordPiece`'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`models.WordPiece`'
- en: For more details about each model and its behavior, you can check [here](components#models)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有关每个模型及其行为的更多详细信息，您可以在[此处](components#models)查看
- en: Post-Processing
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后处理
- en: Post-processing is the last step of the tokenization pipeline, to perform any
    additional transformation to the `Encoding` before it’s returned, like adding
    potential special tokens.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理是分词管道的最后一步，用于在返回之前对`Encoding`执行任何额外的转换，例如添加潜在的特殊标记。
- en: 'As we saw in the quick tour, we can customize the post processor of a `Tokenizer`
    by setting the corresponding attribute. For instance, here is how we can post-process
    to make the inputs suitable for the BERT model:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在快速浏览中看到的，我们可以通过设置相应的属性来自定义`Tokenizer`的后处理器。例如，这是我们如何进行后处理以使输入适合BERT模型：
- en: PythonRustNode
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that contrarily to the pre-tokenizer or the normalizer, you don’t need
    to retrain a tokenizer after changing its post-processor.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与预分词器或规范化器相反，更改后处理器后不需要重新训练分词器。
- en: 'All together: a BERT tokenizer from scratch'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 所有在一起：从头开始的BERT分词器
- en: 'Let’s put all those pieces together to build a BERT tokenizer. First, BERT
    relies on WordPiece, so we instantiate a new `Tokenizer` with this model:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把所有这些部分放在一起来构建一个BERT分词器。首先，BERT依赖于WordPiece，因此我们用这个模型实例化一个新的`Tokenizer`：
- en: PythonRustNode
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then we know that BERT preprocesses texts by removing accents and lowercasing.
    We also use a unicode normalizer:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们知道BERT通过去除重音符号和小写来预处理文本。我们还使用了一个Unicode规范化器：
- en: PythonRustNode
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The pre-tokenizer is just splitting on whitespace and punctuation:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 预分词器只是在空格和标点符号上进行分割：
- en: PythonRustNode
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And the post-processing uses the template we saw in the previous section:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理使用了我们在上一节中看到的模板：
- en: PythonRustNode
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can use this tokenizer and train on it on wikitext like in the `quicktour`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个分词器，并在wikitext上进行训练，就像在`quicktour`中一样：
- en: PythonRustNode
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Decoding
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码
- en: On top of encoding the input texts, a `Tokenizer` also has an API for decoding,
    that is converting IDs generated by your model back to a text. This is done by
    the methods `Tokenizer.decode` (for one predicted text) and `Tokenizer.decode_batch`
    (for a batch of predictions).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对输入文本进行编码外，`Tokenizer`还具有用于解码的API，即将模型生成的ID转换回文本。这是通过方法`Tokenizer.decode`（用于一个预测文本）和`Tokenizer.decode_batch`（用于一批预测）来完成的。
- en: 'The `decoder` will first convert the IDs back to tokens (using the tokenizer’s
    vocabulary) and remove all special tokens, then join those tokens with spaces:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`decoder`首先将ID转换回标记（使用分词器的词汇表），然后删除所有特殊标记，然后用空格连接这些标记：'
- en: PythonRustNode
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If you used a model that added special characters to represent subtokens of
    a given “word” (like the `"##"` in WordPiece) you will need to customize the `decoder`
    to treat them properly. If we take our previous `bert_tokenizer` for instance
    the default decoding will give:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用了一个模型，该模型添加了特殊字符来表示给定“单词”的子标记（例如WordPiece中的`"##"`），则需要自定义`decoder`以正确处理它们。例如，如果我们以前的`bert_tokenizer`，默认解码将会给出：
- en: PythonRustNode
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'But by changing it to a proper decoder, we get:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 但通过将其更改为适当的解码器，我们得到：
- en: PythonRustNode
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
