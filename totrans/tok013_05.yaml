- en: The tokenization pipeline
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/tokenizers/pipeline](https://huggingface.co/docs/tokenizers/pipeline)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'When calling `Tokenizer.encode` or `Tokenizer.encode_batch`, the input text(s)
    go through the following pipeline:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '`normalization`'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pre-tokenization`'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model`'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`post-processing`'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll see in details what happens during each of those steps in detail, as well
    as when you want to `decode <decoding>` some token ids, and how the 🤗 Tokenizers
    library allows you to customize each of those steps to your needs. If you’re already
    familiar with those steps and want to learn by seeing some code, jump to `our
    BERT from scratch example <example>`.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'For the examples that require a `Tokenizer` we will use the tokenizer we trained
    in the `quicktour`, which you can load with:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: PythonRustNode
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Normalization
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normalization is, in a nutshell, a set of operations you apply to a raw string
    to make it less random or “cleaner”. Common operations include stripping whitespace,
    removing accented characters or lowercasing all text. If you’re familiar with
    [Unicode normalization](https://unicode.org/reports/tr15), it is also a very common
    normalization operation applied in most tokenizers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Each normalization operation is represented in the 🤗 Tokenizers library by
    a `Normalizer`, and you can combine several of those by using a `normalizers.Sequence`.
    Here is a normalizer applying NFD Unicode normalization and removing accents as
    an example:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: PythonRustNode
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can manually test that normalizer by applying it to any string:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: PythonRustNode
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When building a `Tokenizer`, you can customize its normalizer by just changing
    the corresponding attribute:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: PythonRustNode
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Of course, if you change the way a tokenizer applies normalization, you should
    probably retrain it from scratch afterward.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Pre-Tokenization
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pre-tokenization is the act of splitting a text into smaller objects that give
    an upper bound to what your tokens will be at the end of training. A good way
    to think of this is that the pre-tokenizer will split your text into “words” and
    then, your final tokens will be parts of those words.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'An easy way to pre-tokenize inputs is to split on spaces and punctuations,
    which is done by the `pre_tokenizers.Whitespace` pre-tokenizer:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: PythonRustNode
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The output is a list of tuples, with each tuple containing one word and its
    span in the original sentence (which is used to determine the final `offsets`
    of our `Encoding`). Note that splitting on punctuation will split contractions
    like `"I'm"` in this example.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'You can combine together any `PreTokenizer` together. For instance, here is
    a pre-tokenizer that will split on space, punctuation and digits, separating numbers
    in their individual digits:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: PythonRustNode
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As we saw in the `quicktour`, you can customize the pre-tokenizer of a `Tokenizer`
    by just changing the corresponding attribute:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: PythonRustNode
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Of course, if you change the way the pre-tokenizer, you should probably retrain
    your tokenizer from scratch afterward.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Model
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the input texts are normalized and pre-tokenized, the `Tokenizer` applies
    the model on the pre-tokens. This is the part of the pipeline that needs training
    on your corpus (or that has been trained if you are using a pretrained tokenizer).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The role of the model is to split your “words” into tokens, using the rules
    it has learned. It’s also responsible for mapping those tokens to their corresponding
    IDs in the vocabulary of the model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'This model is passed along when intializing the `Tokenizer` so you already
    know how to customize this part. Currently, the 🤗 Tokenizers library supports:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '`models.BPE`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models.Unigram`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models.WordLevel`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models.WordPiece`'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more details about each model and its behavior, you can check [here](components#models)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Post-Processing
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Post-processing is the last step of the tokenization pipeline, to perform any
    additional transformation to the `Encoding` before it’s returned, like adding
    potential special tokens.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in the quick tour, we can customize the post processor of a `Tokenizer`
    by setting the corresponding attribute. For instance, here is how we can post-process
    to make the inputs suitable for the BERT model:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在快速浏览中看到的，我们可以通过设置相应的属性来自定义`Tokenizer`的后处理器。例如，这是我们如何进行后处理以使输入适合BERT模型：
- en: PythonRustNode
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that contrarily to the pre-tokenizer or the normalizer, you don’t need
    to retrain a tokenizer after changing its post-processor.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与预分词器或规范化器相反，更改后处理器后不需要重新训练分词器。
- en: 'All together: a BERT tokenizer from scratch'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 所有在一起：从头开始的BERT分词器
- en: 'Let’s put all those pieces together to build a BERT tokenizer. First, BERT
    relies on WordPiece, so we instantiate a new `Tokenizer` with this model:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把所有这些部分放在一起来构建一个BERT分词器。首先，BERT依赖于WordPiece，因此我们用这个模型实例化一个新的`Tokenizer`：
- en: PythonRustNode
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then we know that BERT preprocesses texts by removing accents and lowercasing.
    We also use a unicode normalizer:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们知道BERT通过去除重音符号和小写来预处理文本。我们还使用了一个Unicode规范化器：
- en: PythonRustNode
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The pre-tokenizer is just splitting on whitespace and punctuation:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 预分词器只是在空格和标点符号上进行分割：
- en: PythonRustNode
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And the post-processing uses the template we saw in the previous section:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理使用了我们在上一节中看到的模板：
- en: PythonRustNode
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can use this tokenizer and train on it on wikitext like in the `quicktour`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个分词器，并在wikitext上进行训练，就像在`quicktour`中一样：
- en: PythonRustNode
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Decoding
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码
- en: On top of encoding the input texts, a `Tokenizer` also has an API for decoding,
    that is converting IDs generated by your model back to a text. This is done by
    the methods `Tokenizer.decode` (for one predicted text) and `Tokenizer.decode_batch`
    (for a batch of predictions).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对输入文本进行编码外，`Tokenizer`还具有用于解码的API，即将模型生成的ID转换回文本。这是通过方法`Tokenizer.decode`（用于一个预测文本）和`Tokenizer.decode_batch`（用于一批预测）来完成的。
- en: 'The `decoder` will first convert the IDs back to tokens (using the tokenizer’s
    vocabulary) and remove all special tokens, then join those tokens with spaces:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`decoder`首先将ID转换回标记（使用分词器的词汇表），然后删除所有特殊标记，然后用空格连接这些标记：'
- en: PythonRustNode
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If you used a model that added special characters to represent subtokens of
    a given “word” (like the `"##"` in WordPiece) you will need to customize the `decoder`
    to treat them properly. If we take our previous `bert_tokenizer` for instance
    the default decoding will give:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用了一个模型，该模型添加了特殊字符来表示给定“单词”的子标记（例如WordPiece中的`"##"`），则需要自定义`decoder`以正确处理它们。例如，如果我们以前的`bert_tokenizer`，默认解码将会给出：
- en: PythonRustNode
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'But by changing it to a proper decoder, we get:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 但通过将其更改为适当的解码器，我们得到：
- en: PythonRustNode
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: PythonRustNode
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
