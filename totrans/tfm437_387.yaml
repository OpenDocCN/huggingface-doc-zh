- en: Time Series Transformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列Transformer
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/time_series_transformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/time_series_transformer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/time_series_transformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/time_series_transformer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The Time Series Transformer model is a vanilla encoder-decoder Transformer for
    time series forecasting. This model was contributed by [kashif](https://huggingface.co/kashif).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列Transformer模型是用于时间序列预测的基本编码器-解码器Transformer。此模型由[kashif](https://huggingface.co/kashif)贡献。
- en: Usage tips
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: Similar to other models in the library, [TimeSeriesTransformerModel](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerModel)
    is the raw Transformer without any head on top, and [TimeSeriesTransformerForPrediction](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerForPrediction)
    adds a distribution head on top of the former, which can be used for time-series
    forecasting. Note that this is a so-called probabilistic forecasting model, not
    a point forecasting model. This means that the model learns a distribution, from
    which one can sample. The model doesn’t directly output values.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与库中其他模型类似，[TimeSeriesTransformerModel](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerModel)是没有顶部头部的原始Transformer，而[TimeSeriesTransformerForPrediction](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerForPrediction)在前者的顶部添加了一个分布头部，可用于时间序列预测。请注意，这是一种所谓的概率预测模型，而不是点预测模型。这意味着模型学习一个分布，可以从中进行采样。该模型不直接输出值。
- en: '[TimeSeriesTransformerForPrediction](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerForPrediction)
    consists of 2 blocks: an encoder, which takes a `context_length` of time series
    values as input (called `past_values`), and a decoder, which predicts a `prediction_length`
    of time series values into the future (called `future_values`). During training,
    one needs to provide pairs of (`past_values` and `future_values`) to the model.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TimeSeriesTransformerForPrediction](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerForPrediction)由2个模块组成：编码器，接受时间序列值的`context_length`作为输入（称为`past_values`），解码器，预测未来的`prediction_length`时间序列值（称为`future_values`）。在训练过程中，需要向模型提供（`past_values`和`future_values`）的配对数据。'
- en: 'In addition to the raw (`past_values` and `future_values`), one typically provides
    additional features to the model. These can be the following:'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了原始的（`past_values`和`future_values`）之外，通常还向模型提供其他特征。这些可以是以下内容：
- en: '`past_time_features`: temporal features which the model will add to `past_values`.
    These serve as “positional encodings” for the Transformer encoder. Examples are
    “day of the month”, “month of the year”, etc. as scalar values (and then stacked
    together as a vector). e.g. if a given time-series value was obtained on the 11th
    of August, then one could have [11, 8] as time feature vector (11 being “day of
    the month”, 8 being “month of the year”).'
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_time_features`：模型将添加到`past_values`的时间特征。这些作为Transformer编码器的“位置编码”。例如，“月份的日期”，“年份的月份”等作为标量值（然后堆叠在一起形成向量）。例如，如果给定的时间序列值是在8月11日获得的，则可以将[11,
    8]作为时间特征向量（11代表“月份的日期”，8代表“年份的月份”）。'
- en: '`future_time_features`: temporal features which the model will add to `future_values`.
    These serve as “positional encodings” for the Transformer decoder. Examples are
    “day of the month”, “month of the year”, etc. as scalar values (and then stacked
    together as a vector). e.g. if a given time-series value was obtained on the 11th
    of August, then one could have [11, 8] as time feature vector (11 being “day of
    the month”, 8 being “month of the year”).'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_time_features`：模型将添加到`future_values`的时间特征。这些作为Transformer解码器的“位置编码”。例如，“月份的日期”，“年份的月份”等作为标量值（然后堆叠在一起形成向量）。例如，如果给定的时间序列值是在8月11日获得的，则可以将[11,
    8]作为时间特征向量（11代表“月份的日期”，8代表“年份的月份”）。'
- en: '`static_categorical_features`: categorical features which are static over time
    (i.e., have the same value for all `past_values` and `future_values`). An example
    here is the store ID or region ID that identifies a given time-series. Note that
    these features need to be known for ALL data points (also those in the future).'
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_categorical_features`：随时间保持不变的分类特征（即所有`past_values`和`future_values`具有相同的值）。一个例子是标识给定时间序列的商店ID或地区ID。请注意，这些特征需要对所有数据点（包括未来的数据点）都是已知的。'
- en: '`static_real_features`: real-valued features which are static over time (i.e.,
    have the same value for all `past_values` and `future_values`). An example here
    is the image representation of the product for which you have the time-series
    values (like the [ResNet](resnet) embedding of a “shoe” picture, if your time-series
    is about the sales of shoes). Note that these features need to be known for ALL
    data points (also those in the future).'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_real_features`：随时间保持不变的实值特征（即所有`past_values`和`future_values`具有相同的值）。一个例子是产品的图像表示，您拥有该产品的时间序列值（比如关于鞋子销售的时间序列的[ResNet](resnet)嵌入的“鞋子”图片）。请注意，这些特征需要对所有数据点（包括未来的数据点）都是已知的。'
- en: The model is trained using “teacher-forcing”, similar to how a Transformer is
    trained for machine translation. This means that, during training, one shifts
    the `future_values` one position to the right as input to the decoder, prepended
    by the last value of `past_values`. At each time step, the model needs to predict
    the next target. So the set-up of training is similar to a GPT model for language,
    except that there’s no notion of `decoder_start_token_id` (we just use the last
    value of the context as initial input for the decoder).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型使用“teacher-forcing”进行训练，类似于Transformer用于机器翻译的训练方式。这意味着在训练过程中，将`future_values`向右移动一个位置作为解码器的输入，前面加上`past_values`的最后一个值。在每个时间步骤，模型需要预测下一个目标。因此，训练的设置类似于用于语言的GPT模型，只是没有`decoder_start_token_id`的概念（我们只使用上下文的最后一个值作为解码器的初始输入）。
- en: At inference time, we give the final value of the `past_values` as input to
    the decoder. Next, we can sample from the model to make a prediction at the next
    time step, which is then fed to the decoder in order to make the next prediction
    (also called autoregressive generation).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推断时，我们将`past_values`的最终值作为输入传递给解码器。接下来，我们可以从模型中进行采样，以在下一个时间步骤进行预测，然后将其馈送给解码器以进行下一个预测（也称为自回归生成）。
- en: Resources
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started. If you’re interested in submitting a resource to be included
    here, please feel free to open a Pull Request and we’ll review it! The resource
    should ideally demonstrate something new instead of duplicating an existing resource.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列官方Hugging Face和社区（由🌎表示）资源，可帮助您入门。如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将进行审查！资源应该展示一些新内容，而不是重复现有资源。
- en: 'Check out the Time Series Transformer blog-post in HuggingFace blog: [Probabilistic
    Time Series Forecasting with 🤗 Transformers](https://huggingface.co/blog/time-series-transformers)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在HuggingFace博客中查看时间序列Transformer博文：[使用🤗 Transformers进行概率时间序列预测](https://huggingface.co/blog/time-series-transformers)
- en: TimeSeriesTransformerConfig
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TimeSeriesTransformerConfig
- en: '### `class transformers.TimeSeriesTransformerConfig`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TimeSeriesTransformerConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/time_series_transformer/configuration_time_series_transformer.py#L33)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/time_series_transformer/configuration_time_series_transformer.py#L33)'
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prediction_length` (`int`) — The prediction length for the decoder. In other
    words, the prediction horizon of the model. This value is typically dictated by
    the dataset and we recommend to set it appropriately.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_length` (`int`) — 解码器的预测长度。换句话说，模型的预测范围。此值通常由数据集决定，我们建议适当设置。'
- en: '`context_length` (`int`, *optional*, defaults to `prediction_length`) — The
    context length for the encoder. If `None`, the context length will be the same
    as the `prediction_length`.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_length` (`int`, *可选*, 默认为`prediction_length`) — 编码器的上下文长度。如果为`None`，上下文长度将与`prediction_length`相同。'
- en: '`distribution_output` (`string`, *optional*, defaults to `"student_t"`) — The
    distribution emission head for the model. Could be either “student_t”, “normal”
    or “negative_binomial”.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distribution_output` (`string`, *可选*, 默认为`"student_t"`) — 模型的分布发射头。可以是“student_t”、“normal”或“negative_binomial”之一。'
- en: '`loss` (`string`, *optional*, defaults to `"nll"`) — The loss function for
    the model corresponding to the `distribution_output` head. For parametric distributions
    it is the negative log likelihood (nll) - which currently is the only supported
    one.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`string`, *可选*, 默认为`"nll"`) — 与`distribution_output`头对应的模型损失函数。对于参数分布，它是负对数似然（nll）-
    目前是唯一支持的损失函数。'
- en: '`input_size` (`int`, *optional*, defaults to 1) — The size of the target variable
    which by default is 1 for univariate targets. Would be > 1 in case of multivariate
    targets.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_size` (`int`, *可选*, 默认为1) — 目标变量的大小，默认情况下对于单变量目标为1。对于多变量目标，将大于1。'
- en: '`scaling` (`string` or `bool`, *optional* defaults to `"mean"`) — Whether to
    scale the input targets via “mean” scaler, “std” scaler or no scaler if `None`.
    If `True`, the scaler is set to “mean”.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaling` (`string`或`bool`, *可选*, 默认为`"mean"`) — 是否通过“mean”缩放器、“std”缩放器或如果为`None`则不进行缩放来缩放输入目标。如果为`True`，则缩放器设置为“mean”。'
- en: '`lags_sequence` (`list[int]`, *optional*, defaults to `[1, 2, 3, 4, 5, 6, 7]`)
    — The lags of the input time series as covariates often dictated by the frequency
    of the data. Default is `[1, 2, 3, 4, 5, 6, 7]` but we recommend to change it
    based on the dataset appropriately.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lags_sequence` (`list[int]`, *可选*, 默认为`[1, 2, 3, 4, 5, 6, 7]`) — 输入时间序列的滞后作为协变量的滞后，通常由数据的频率决定。默认为`[1,
    2, 3, 4, 5, 6, 7]`，但我们建议根据数据集适当地进行更改。'
- en: '`num_time_features` (`int`, *optional*, defaults to 0) — The number of time
    features in the input time series.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_time_features` (`int`, *可选*, 默认为0) — 输入时间序列中的时间特征数量。'
- en: '`num_dynamic_real_features` (`int`, *optional*, defaults to 0) — The number
    of dynamic real valued features.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_dynamic_real_features` (`int`, *可选*, 默认为0) — 动态实值特征的数量。'
- en: '`num_static_categorical_features` (`int`, *optional*, defaults to 0) — The
    number of static categorical features.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_static_categorical_features` (`int`, *可选*, 默认为0) — 静态分类特征的数量。'
- en: '`num_static_real_features` (`int`, *optional*, defaults to 0) — The number
    of static real valued features.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_static_real_features` (`int`, *可选*, 默认为0) — 静态实值特征的数量。'
- en: '`cardinality` (`list[int]`, *optional*) — The cardinality (number of different
    values) for each of the static categorical features. Should be a list of integers,
    having the same length as `num_static_categorical_features`. Cannot be `None`
    if `num_static_categorical_features` is > 0.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cardinality` (`list[int]`, *可选*) — 每个静态分类特征的基数（不同值的数量）。应该是一个整数列表，长度与`num_static_categorical_features`相同。如果`num_static_categorical_features`大于0，则不能为`None`。'
- en: '`embedding_dimension` (`list[int]`, *optional*) — The dimension of the embedding
    for each of the static categorical features. Should be a list of integers, having
    the same length as `num_static_categorical_features`. Cannot be `None` if `num_static_categorical_features`
    is > 0.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding_dimension` (`list[int]`, *可选*) — 每个静态分类特征的嵌入维度。应该是一个整数列表，长度与`num_static_categorical_features`相同。如果`num_static_categorical_features`大于0，则不能为`None`。'
- en: '`d_model` (`int`, *optional*, defaults to 64) — Dimensionality of the transformer
    layers.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_model` (`int`, *可选*, 默认为64) — Transformer层的维度。'
- en: '`encoder_layers` (`int`, *optional*, defaults to 2) — Number of encoder layers.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layers` (`int`, *可选*, 默认为2) — 编码器层数。'
- en: '`decoder_layers` (`int`, *optional*, defaults to 2) — Number of decoder layers.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layers` (`int`, *可选*, 默认为2) — 解码器层数。'
- en: '`encoder_attention_heads` (`int`, *optional*, defaults to 2) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attention_heads` (`int`, *可选*, 默认为2) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`decoder_attention_heads` (`int`, *optional*, defaults to 2) — Number of attention
    heads for each attention layer in the Transformer decoder.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_heads` (`int`, *optional*, defaults to 2) — Transformer解码器中每个注意力层的注意力头数。'
- en: '`encoder_ffn_dim` (`int`, *optional*, defaults to 32) — Dimension of the “intermediate”
    (often named feed-forward) layer in encoder.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_ffn_dim` (`int`, *optional*, defaults to 32) — 编码器中“中间”（通常称为前馈）层的维度。'
- en: '`decoder_ffn_dim` (`int`, *optional*, defaults to 32) — Dimension of the “intermediate”
    (often named feed-forward) layer in decoder.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ffn_dim` (`int`, *optional*, defaults to 32) — 解码器中“中间”（通常称为前馈）层的维度。'
- en: '`activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`)
    — The non-linear activation function (function or string) in the encoder and decoder.
    If string, `"gelu"` and `"relu"` are supported.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`)
    — 编码器和解码器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`和`"relu"`。'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the encoder, and decoder.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, defaults to 0.1) — 编码器和解码器中所有全连接层的dropout概率。'
- en: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the attention and fully connected layers for each encoder layer.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.1) — 每个编码器层的注意力和全连接层的dropout概率。'
- en: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the attention and fully connected layers for each decoder layer.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.1) — 每个解码器层的注意力和全连接层的dropout概率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the attention probabilities.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力概率的dropout概率。'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    used between the two layers of the feed-forward networks.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — 在前馈网络的两个层之间使用的dropout概率。'
- en: '`num_parallel_samples` (`int`, *optional*, defaults to 100) — The number of
    samples to generate in parallel for each time step of inference.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_parallel_samples` (`int`, *optional*, defaults to 100) — 每个推理时间步生成的并行样本数。'
- en: '`init_std` (`float`, *optional*, defaults to 0.02) — The standard deviation
    of the truncated normal weight initialization distribution.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_std` (`float`, *optional*, defaults to 0.02) — 截断正态权重初始化分布的标准差。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether to use the past
    key/values attentions (if applicable to the model) to speed up decoding.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, defaults to `True`) — 是否使用过去的键/值注意力（如果适用于模型）以加速解码。'
- en: Example —
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例 —
- en: This is the configuration class to store the configuration of a [TimeSeriesTransformerModel](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerModel).
    It is used to instantiate a Time Series Transformer model according to the specified
    arguments, defining the model architecture. Instantiating a configuration with
    the defaults will yield a similar configuration to that of the Time Series Transformer
    [huggingface/time-series-transformer-tourism-monthly](https://huggingface.co/huggingface/time-series-transformer-tourism-monthly)
    architecture.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[TimeSeriesTransformerModel](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerModel)配置的配置类。根据指定的参数实例化一个时间序列Transformer模型，定义模型架构。使用默认值实例化配置将产生类似于Time
    Series Transformer [huggingface/time-series-transformer-tourism-monthly](https://huggingface.co/huggingface/time-series-transformer-tourism-monthly)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的配置对象可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: TimeSeriesTransformerModel
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TimeSeriesTransformerModel
- en: '### `class transformers.TimeSeriesTransformerModel`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TimeSeriesTransformerModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/time_series_transformer/modeling_time_series_transformer.py#L1182)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/time_series_transformer/modeling_time_series_transformer.py#L1182)'
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([TimeSeriesTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([TimeSeriesTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare Time Series Transformer Model outputting raw hidden-states without
    any specific head on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的时间序列Transformer模型，在顶部没有特定的头输出原始隐藏状态。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/time_series_transformer/modeling_time_series_transformer.py#L1324)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/time_series_transformer/modeling_time_series_transformer.py#L1324)'
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`past_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`
    or `(batch_size, sequence_length, input_size)`) — Past values of the time series,
    that serve as context in order to predict the future. The sequence size of this
    tensor must be larger than the `context_length` of the model, since the model
    will use the larger size to construct lag features, i.e. additional values from
    the past which are added in order to serve as “extra context”.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_values`（形状为`(batch_size, sequence_length)`或`(batch_size, sequence_length,
    input_size)`的`torch.FloatTensor`）- 时间序列的过去值，作为上下文以预测未来。这个张量的序列大小必须大于模型的`context_length`，因为模型将使用更大的大小来构建滞后特征，即从过去添加的额外值，以充当“额外上下文”。'
- en: The `sequence_length` here is equal to `config.context_length` + `max(config.lags_sequence)`,
    which if no `lags_sequence` is configured, is equal to `config.context_length`
    + 7 (as by default, the largest look-back index in `config.lags_sequence` is 7).
    The property `_past_length` returns the actual length of the past.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的`sequence_length`等于`config.context_length` + `max(config.lags_sequence)`，如果没有配置`lags_sequence`，则等于`config.context_length`
    + 7（因为默认情况下，`config.lags_sequence`中最大的回溯索引是7）。属性`_past_length`返回过去的实际长度。
- en: The `past_values` is what the Transformer encoder gets as input (with optional
    additional features, such as `static_categorical_features`, `static_real_features`,
    `past_time_features` and lags).
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`past_values` 是Transformer编码器的输入（可选的附加特征，如`static_categorical_features`、`static_real_features`、`past_time_features`和lags）。'
- en: Optionally, missing values need to be replaced with zeros and indicated via
    the `past_observed_mask`.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可选地，缺失值需要用零替换，并通过`past_observed_mask`指示。
- en: For multivariate time series, the `input_size` > 1 dimension is required and
    corresponds to the number of variates in the time series per time step.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于多变量时间序列，`input_size` > 1维是必需的，并且对应于每个时间步中时间序列中的变量数。
- en: '`past_time_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_features)`) — Required time features, which the model internally will add
    to `past_values`. These could be things like “month of year”, “day of the month”,
    etc. encoded as vectors (for instance as Fourier features). These could also be
    so-called “age” features, which basically help the model know “at which point
    in life” a time-series is. Age features have small values for distant past time
    steps and increase monotonically the more we approach the current time step. Holiday
    features are also a good example of time features.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_time_features`（形状为`(batch_size, sequence_length, num_features)`的`torch.FloatTensor`）-
    模型内部将添加到`past_values`中的必需时间特征。这些可能是像“年份的月份”、“月份的日期”等编码为向量（例如傅立叶特征）的东西。这些也可以是所谓的“年龄”特征，基本上帮助模型知道时间序列处于“生活中的哪个时刻”。年龄特征对于远处的过去时间步具有较小的值，并且随着我们接近当前时间步而单调增加。假期特征也是时间特征的一个很好的例子。'
- en: These features serve as the “positional encodings” of the inputs. So contrary
    to a model like BERT, where the position encodings are learned from scratch internally
    as parameters of the model, the Time Series Transformer requires to provide additional
    time features. The Time Series Transformer only learns additional embeddings for
    `static_categorical_features`.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些特征作为输入的“位置编码”。因此，与像BERT这样的模型不同，BERT的位置编码是从头开始内部作为模型的参数学习的，时间序列Transformer需要提供额外的时间特征。时间序列Transformer仅为`static_categorical_features`学习额外的嵌入。
- en: Additional dynamic real covariates can be concatenated to this tensor, with
    the caveat that these features must but known at prediction time.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 额外的动态实数协变量可以连接到这个张量中，但这些特征必须在预测时已知。
- en: The `num_features` here is equal to `config.`num_time_features`+`config.num_dynamic_real_features`.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的`num_features`等于`config.num_time_features`+`config.num_dynamic_real_features`。
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`
    or `(batch_size, sequence_length, input_size)`, *optional*) — Boolean mask to
    indicate which `past_values` were observed and which were missing. Mask values
    selected in `[0, 1]`:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_observed_mask`（形状为`(batch_size, sequence_length)`或`(batch_size, sequence_length,
    input_size)`的`torch.BoolTensor`，*可选*）- 用于指示哪些`past_values`是观察到的，哪些是缺失的布尔掩码。掩码值选择在`[0,
    1]`中：'
- en: 1 for values that are `observed`,
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`observed`的值为1，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`missing`的值（即用零替换的NaN值），为0。
- en: '`static_categorical_features` (`torch.LongTensor` of shape `(batch_size, number
    of static categorical features)`, *optional*) — Optional static categorical features
    for which the model will learn an embedding, which it will add to the values of
    the time series.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_categorical_features`（形状为`(batch_size, number of static categorical
    features)`的`torch.LongTensor`，*可选*）- 模型将学习一个嵌入，将其添加到时间序列值中的可选静态分类特征。'
- en: Static categorical features are features which have the same value for all time
    steps (static over time).
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态分类特征是所有时间步长上具有相同值的特征（随时间保持不变）。
- en: A typical example of a static categorical feature is a time series ID.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态分类特征的典型示例是时间序列ID。
- en: '`static_real_features` (`torch.FloatTensor` of shape `(batch_size, number of
    static real features)`, *optional*) — Optional static real features which the
    model will add to the values of the time series.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_real_features`（形状为`(batch_size, number of static real features)`的`torch.FloatTensor`，*可选*）-
    模型将添加到时间序列值中的可选静态实数特征。'
- en: Static real features are features which have the same value for all time steps
    (static over time).
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态实数特征是所有时间步长上具有相同值的特征（随时间保持不变）。
- en: A typical example of a static real feature is promotion information.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态实际特征的典型示例是促销信息。
- en: '`future_values` (`torch.FloatTensor` of shape `(batch_size, prediction_length)`
    or `(batch_size, prediction_length, input_size)`, *optional*) — Future values
    of the time series, that serve as labels for the model. The `future_values` is
    what the Transformer needs during training to learn to output, given the `past_values`.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_values`（形状为`(batch_size, prediction_length)`或`(batch_size, prediction_length,
    input_size)`的`torch.FloatTensor`，*可选*）— 时间序列的未来值，用作模型的标签。`future_values`是Transformer在训练期间需要学习输出的内容，给定`past_values`。'
- en: The sequence length here is equal to `prediction_length`.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的序列长度等于`prediction_length`。
- en: See the demo notebook and code snippets for details.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关详细信息，请参阅演示笔记本和代码片段。
- en: Optionally, during training any missing values need to be replaced with zeros
    and indicated via the `future_observed_mask`.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练期间，任何缺失值都需要用零替换，并通过`future_observed_mask`指示。
- en: For multivariate time series, the `input_size` > 1 dimension is required and
    corresponds to the number of variates in the time series per time step.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于多变量时间序列，需要`input_size` > 1维，并且对应于时间序列中每个时间步的变量数量。
- en: '`future_time_features` (`torch.FloatTensor` of shape `(batch_size, prediction_length,
    num_features)`) — Required time features for the prediction window, which the
    model internally will add to `future_values`. These could be things like “month
    of year”, “day of the month”, etc. encoded as vectors (for instance as Fourier
    features). These could also be so-called “age” features, which basically help
    the model know “at which point in life” a time-series is. Age features have small
    values for distant past time steps and increase monotonically the more we approach
    the current time step. Holiday features are also a good example of time features.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_time_features`（形状为`(batch_size, prediction_length, num_features)`的`torch.FloatTensor`）—
    预测窗口所需的时间特征，模型内部将这些特征添加到`future_values`中。这些特征可以是诸如“年份月份”、“每月日期”等的向量编码（例如傅立叶特征）。这些也可以是所谓的“年龄”特征，基本上帮助模型了解时间序列处于“生命周期的哪个阶段”。年龄特征对于遥远的过去时间步具有较小的值，并且随着我们接近当前时间步，值会单调增加。假期特征也是时间特征的一个很好的例子。'
- en: These features serve as the “positional encodings” of the inputs. So contrary
    to a model like BERT, where the position encodings are learned from scratch internally
    as parameters of the model, the Time Series Transformer requires to provide additional
    time features. The Time Series Transformer only learns additional embeddings for
    `static_categorical_features`.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些特征作为输入的“位置编码”。与BERT等模型不同，BERT的位置编码是从头开始内部作为模型的参数学习的，时间序列Transformer需要提供额外的时间特征。时间序列Transformer仅为`static_categorical_features`学习额外的嵌入。
- en: Additional dynamic real covariates can be concatenated to this tensor, with
    the caveat that these features must but known at prediction time.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以将额外的动态实际协变量连接到此张量中，但必须在预测时了解这些特征。
- en: The `num_features` here is equal to `config.`num_time_features`+`config.num_dynamic_real_features`.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的`num_features`等于`config.num_time_features`+`config.num_dynamic_real_features`。
- en: '`future_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`
    or `(batch_size, sequence_length, input_size)`, *optional*) — Boolean mask to
    indicate which `future_values` were observed and which were missing. Mask values
    selected in `[0, 1]`:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_observed_mask`（形状为`(batch_size, sequence_length)`或`(batch_size, sequence_length,
    input_size)`的`torch.BoolTensor`，*可选*）— 布尔蒙版，指示哪些`future_values`被观察到，哪些是缺失的。蒙版值选在`[0,
    1]`范围内：'
- en: 1 for values that are `observed`,
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示值被观察到，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于值为“missing”（即被零替换的NaN）的情况。
- en: This mask is used to filter out missing values for the final loss calculation.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此蒙版用于过滤最终损失计算中的缺失值。
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on certain token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 用于避免在某些标记索引上执行注意力的蒙版。蒙版值选在`[0,
    1]`范围内：'
- en: 1 for tokens that are `not masked`,
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被蒙版的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被蒙版的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力蒙版？](../glossary#attention-mask)'
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Mask to avoid performing attention on certain token indices. By
    default, a causal mask will be used, to make sure the model can only look at previous
    inputs in order to predict the future.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）—
    用于避免在某些标记索引上执行注意力的蒙版。默认情况下，将使用因果蒙版，以确保模型只能查看以前的输入以预测未来。'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(encoder_layers, encoder_attention_heads)`的`torch.Tensor`，*可选*）—
    用于使编码器中注意力模块的特定头部失效的蒙版。蒙版值选在`[0, 1]`范围内：'
- en: 1 indicates the head is `not masked`,
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被蒙版，
- en: 0 indicates the head is `masked`.
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被蒙版。
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）—
    用于使解码器中注意力模块的特定头部失效的蒙版。蒙版值选在`[0, 1]`范围内：'
- en: 1 indicates the head is `not masked`,
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被蒙版，
- en: 0 indicates the head is `masked`.
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被蒙版。
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）—
    用于使交叉注意力模块的特定头部失效的蒙版。蒙版值选在`[0, 1]`范围内：'
- en: 1 indicates the head is `not masked`,
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被蒙版，
- en: 0 indicates the head is `masked`.
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被蒙版。
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组包含`last_hidden_state`、`hidden_states`（*可选*）和`attentions`（*可选*）`last_hidden_state`的形状为`(batch_size,
    sequence_length, hidden_size)`（*可选*），是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回，类型为`tuple(tuple(torch.FloatTensor))`，长度为`config.n_layers`，每个元组包含2个形状为`(batch_size,
    num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads,
    encoder_sequence_length, embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，用户可以选择仅输入形状为`(batch_size, 1)`的最后一个`decoder_input_ids`（这些没有将其过去的键值状态提供给此模型）而不是形状为`(batch_size,
    sequence_length)`的所有`decoder_input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*）
    — 可选地，您可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.Seq2SeqTSModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Seq2SeqTSModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSModelOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.Seq2SeqTSModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TimeSeriesTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig))
    and inputs.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Seq2SeqTSModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSModelOutput)
    或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（[TimeSeriesTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig)）和输入的不同元素。'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the decoder of the model.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型解码器最后一层的隐藏状态序列。'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果仅使用`past_key_values`，则输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — 当传递`use_cache=True`或`config.use_cache=True`时返回，类型为`tuple(tuple(torch.FloatTensor))`，长度为`config.n_layers`，每个元组包含2个形状为`(batch_size,
    num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads,
    encoder_sequence_length, embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入层的输出+每层的输出）。'
- en: Hidden-states of the decoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器在每一层输出的隐藏状态加上可选的初始嵌入输出。
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`,
    *可选*) — 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入层的输出+每层的输出）。'
- en: Hidden-states of the encoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器在每一层输出的隐藏状态加上可选的初始嵌入输出。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`loc` (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`,
    *optional*) — Shift values of each time series’ context window which is used to
    give the model inputs of the same magnitude and then used to shift back to the
    original magnitude.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loc` (`形状为`(batch_size,)`或`(batch_size, input_size)`的`torch.FloatTensor`,
    *可选*) — 每个时间序列的上下文窗口的偏移值，用于给模型输入相同数量级的输入，然后用于将其偏移回原始数量级。'
- en: '`scale` (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`,
    *optional*) — Scaling values of each time series’ context window which is used
    to give the model inputs of the same magnitude and then used to rescale back to
    the original magnitude.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale` (`形状为`(batch_size,)`或`(batch_size, input_size)`的`torch.FloatTensor`,
    *可选*) — 每个时间序列的上下文窗口的缩放值，用于给模型输入相同数量级的输入，然后用于将其重新缩放回原始数量级。'
- en: '`static_features` (`torch.FloatTensor` of shape `(batch_size, feature size)`,
    *optional*) — Static features of each time series’ in a batch which are copied
    to the covariates at inference time.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_features` (`形状为`(batch_size, feature size)`的`torch.FloatTensor`, *可选*)
    — 每个时间序列的静态特征，在推断时复制到协变量中。'
- en: The [TimeSeriesTransformerModel](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerModel)
    forward method, overrides the `__call__` special method.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[TimeSeriesTransformerModel](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: TimeSeriesTransformerForPrediction
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TimeSeriesTransformerForPrediction
- en: '### `class transformers.TimeSeriesTransformerForPrediction`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`class transformers.TimeSeriesTransformerForPrediction`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/time_series_transformer/modeling_time_series_transformer.py#L1443)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/time_series_transformer/modeling_time_series_transformer.py#L1443)'
- en: '[PRE5]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([TimeSeriesTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([TimeSeriesTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig))
    — 模型的配置类，包含所有模型的参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The Time Series Transformer Model with a distribution head on top for time-series
    forecasting. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 带有时间序列预测分布头的时间序列变换器模型。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/time_series_transformer/modeling_time_series_transformer.py#L1487)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/time_series_transformer/modeling_time_series_transformer.py#L1487)'
- en: '[PRE6]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`past_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`
    or `(batch_size, sequence_length, input_size)`) — Past values of the time series,
    that serve as context in order to predict the future. The sequence size of this
    tensor must be larger than the `context_length` of the model, since the model
    will use the larger size to construct lag features, i.e. additional values from
    the past which are added in order to serve as “extra context”.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_values`（形状为`(batch_size, sequence_length)`或`(batch_size, sequence_length,
    input_size)`的`torch.FloatTensor`） — 时间序列的过去值，用作上下文以预测未来。此张量的序列大小必须大于模型的`context_length`，因为模型将使用较大的大小来构建滞后特征，即从过去添加的额外值，以充当“额外上下文”。'
- en: The `sequence_length` here is equal to `config.context_length` + `max(config.lags_sequence)`,
    which if no `lags_sequence` is configured, is equal to `config.context_length`
    + 7 (as by default, the largest look-back index in `config.lags_sequence` is 7).
    The property `_past_length` returns the actual length of the past.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的`sequence_length`等于`config.context_length` + `max(config.lags_sequence)`，如果没有配置`lags_sequence`，则等于`config.context_length`
    + 7（默认情况下，`config.lags_sequence`中最大的回顾索引为7）。属性`_past_length`返回过去的实际长度。
- en: The `past_values` is what the Transformer encoder gets as input (with optional
    additional features, such as `static_categorical_features`, `static_real_features`,
    `past_time_features` and lags).
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`past_values`是Transformer编码器作为输入的内容（带有可选的附加特征，如`static_categorical_features`、`static_real_features`、`past_time_features`和滞后）。'
- en: Optionally, missing values need to be replaced with zeros and indicated via
    the `past_observed_mask`.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可选地，缺失值需要用零替换，并通过`past_observed_mask`指示。
- en: For multivariate time series, the `input_size` > 1 dimension is required and
    corresponds to the number of variates in the time series per time step.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于多变量时间序列，需要`input_size` > 1维，并对应于每个时间步长中时间序列的变量数量。
- en: '`past_time_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_features)`) — Required time features, which the model internally will add
    to `past_values`. These could be things like “month of year”, “day of the month”,
    etc. encoded as vectors (for instance as Fourier features). These could also be
    so-called “age” features, which basically help the model know “at which point
    in life” a time-series is. Age features have small values for distant past time
    steps and increase monotonically the more we approach the current time step. Holiday
    features are also a good example of time features.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_time_features`（形状为`(batch_size, sequence_length, num_features)`的`torch.FloatTensor`）
    — 必需的时间特征，模型内部将其添加到`past_values`中。这些可能是诸如“年份中的月份”、“月份中的日期”等编码为向量（例如作为傅立叶特征）的内容。这也可以是所谓的“年龄”特征，基本上帮助模型知道时间序列处于“生活中的哪个阶段”。年龄特征对于远处的过去时间步具有较小的值，并且随着我们接近当前时间步而单调增加。假期特征也是时间特征的一个很好的例子。'
- en: These features serve as the “positional encodings” of the inputs. So contrary
    to a model like BERT, where the position encodings are learned from scratch internally
    as parameters of the model, the Time Series Transformer requires to provide additional
    time features. The Time Series Transformer only learns additional embeddings for
    `static_categorical_features`.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些特征用作输入的“位置编码”。与像BERT这样的模型相反，BERT中的位置编码是从头开始内部作为模型的参数学习的，而时间序列变换器需要提供额外的时间特征。时间序列变换器仅为`static_categorical_features`学习额外的嵌入。
- en: Additional dynamic real covariates can be concatenated to this tensor, with
    the caveat that these features must but known at prediction time.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以将额外的动态实际协变量连接到此张量中，但需要注意的是这些特征必须在预测时已知。
- en: The `num_features` here is equal to `config.`num_time_features`+`config.num_dynamic_real_features`.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的`num_features`等于`config.num_time_features`+`config.num_dynamic_real_features`。
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`
    or `(batch_size, sequence_length, input_size)`, *optional*) — Boolean mask to
    indicate which `past_values` were observed and which were missing. Mask values
    selected in `[0, 1]`:'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`
    or `(batch_size, sequence_length, input_size)`, *optional*) — 布尔掩码，指示哪些`past_values`是观察到的，哪些是缺失的。掩码值选在`[0,
    1]`之间：'
- en: 1 for values that are `observed`,
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示“观察到”的值，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示“缺失”的值（即被零替换的NaN）。
- en: '`static_categorical_features` (`torch.LongTensor` of shape `(batch_size, number
    of static categorical features)`, *optional*) — Optional static categorical features
    for which the model will learn an embedding, which it will add to the values of
    the time series.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_categorical_features` (`torch.LongTensor` of shape `(batch_size, number
    of static categorical features)`, *optional*) — 模型将学习嵌入这些静态分类特征，并将其添加到时间序列的值中。'
- en: Static categorical features are features which have the same value for all time
    steps (static over time).
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态分类特征是所有时间步的值都相同的特征（随时间保持静态）。
- en: A typical example of a static categorical feature is a time series ID.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态分类特征的典型示例是时间序列ID。
- en: '`static_real_features` (`torch.FloatTensor` of shape `(batch_size, number of
    static real features)`, *optional*) — Optional static real features which the
    model will add to the values of the time series.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_real_features` (`torch.FloatTensor` of shape `(batch_size, number of
    static real features)`, *optional*) — 可选的静态实数特征，模型将把这些特征添加到时间序列的值中。'
- en: Static real features are features which have the same value for all time steps
    (static over time).
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态实数特征是所有时间步的值都相同的特征（随时间保持静态）。
- en: A typical example of a static real feature is promotion information.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态实数特征的典型示例是促销信息。
- en: '`future_values` (`torch.FloatTensor` of shape `(batch_size, prediction_length)`
    or `(batch_size, prediction_length, input_size)`, *optional*) — Future values
    of the time series, that serve as labels for the model. The `future_values` is
    what the Transformer needs during training to learn to output, given the `past_values`.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_values` (`torch.FloatTensor` of shape `(batch_size, prediction_length)`
    or `(batch_size, prediction_length, input_size)`, *optional*) — 时间序列的未来值，作为模型的标签。`future_values`是Transformer在训练期间需要学习输出的内容，给定`past_values`。'
- en: The sequence length here is equal to `prediction_length`.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的序列长度等于`prediction_length`。
- en: See the demo notebook and code snippets for details.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关详细信息，请参阅演示笔记本和代码片段。
- en: Optionally, during training any missing values need to be replaced with zeros
    and indicated via the `future_observed_mask`.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练期间，任何缺失值都需要用零替换，并通过`future_observed_mask`指示。
- en: For multivariate time series, the `input_size` > 1 dimension is required and
    corresponds to the number of variates in the time series per time step.
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于多变量时间序列，需要`input_size` > 1维，并且对应于时间序列每个时间步中的变量数量。
- en: '`future_time_features` (`torch.FloatTensor` of shape `(batch_size, prediction_length,
    num_features)`) — Required time features for the prediction window, which the
    model internally will add to `future_values`. These could be things like “month
    of year”, “day of the month”, etc. encoded as vectors (for instance as Fourier
    features). These could also be so-called “age” features, which basically help
    the model know “at which point in life” a time-series is. Age features have small
    values for distant past time steps and increase monotonically the more we approach
    the current time step. Holiday features are also a good example of time features.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_time_features` (`torch.FloatTensor` of shape `(batch_size, prediction_length,
    num_features)`) — 模型在内部将这些特征添加到`future_values`中，这些特征是预测窗口所需的时间特征。这些特征可以是诸如“年份中的月份”、“月份中的日期”等编码为向量（例如傅立叶特征）的内容。这些也可以是所谓的“年龄”特征，基本上帮助模型了解时间序列处于“生命中的哪个阶段”。年龄特征对于远处的过去时间步具有较小的值，并且随着我们接近当前时间步而单调增加。假期特征也是时间特征的一个很好的例子。'
- en: These features serve as the “positional encodings” of the inputs. So contrary
    to a model like BERT, where the position encodings are learned from scratch internally
    as parameters of the model, the Time Series Transformer requires to provide additional
    time features. The Time Series Transformer only learns additional embeddings for
    `static_categorical_features`.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些特征作为输入的“位置编码”。与BERT等模型不同，BERT的位置编码是从头开始内部作为模型的参数学习的，时间序列Transformer需要提供额外的时间特征。时间序列Transformer仅为`static_categorical_features`学习额外的嵌入。
- en: Additional dynamic real covariates can be concatenated to this tensor, with
    the caveat that these features must but known at prediction time.
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以将额外的动态实数协变量连接到这个张量中，但需要注意这些特征必须在预测时已知。
- en: The `num_features` here is equal to `config.`num_time_features`+`config.num_dynamic_real_features`.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的`num_features`等于`config.num_time_features`+`config.num_dynamic_real_features`。
- en: '`future_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`
    or `(batch_size, sequence_length, input_size)`, *optional*) — Boolean mask to
    indicate which `future_values` were observed and which were missing. Mask values
    selected in `[0, 1]`:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`
    or `(batch_size, sequence_length, input_size)`, *optional*) — 布尔掩码，指示哪些`future_values`是观察到的，哪些是缺失的。掩码值选在`[0,
    1]`之间：'
- en: 1 for values that are `observed`,
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示“观察到”的值，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示“缺失”的值（即被零替换的NaN）。
- en: This mask is used to filter out missing values for the final loss calculation.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个掩码用于在最终损失计算中过滤缺失值。
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on certain token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在某些标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示“未被掩码”的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示“被掩码”的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Mask to avoid performing attention on certain token indices. By
    default, a causal mask will be used, to make sure the model can only look at previous
    inputs in order to predict the future.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — 用于避免在某些标记索引上执行注意力的掩码。默认情况下，将使用因果掩码，以确保模型只能查看以前的输入以预测未来。'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — 用于将编码器中注意力模块中选择的头部置零的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被屏蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被屏蔽。
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — 用于将解码器中注意力模块中选择的头部置零的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被屏蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被屏蔽。
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — 用于将交叉注意力模块中选择的头部置零的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被屏蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被屏蔽。
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组包括 `last_hidden_state`、`hidden_states`（*可选*）和
    `attentions`（*可选*）`last_hidden_state` 的形状为 `(batch_size, sequence_length, hidden_size)`（*可选*）是编码器最后一层的输出的隐藏状态序列。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递 `use_cache=True`
    或 `config.use_cache=True` 时返回) — 长度为 `config.n_layers` 的 `tuple(torch.FloatTensor)`
    元组，每个元组包含 2 个形状为 `(batch_size, num_heads, sequence_length, embed_size_per_head)`
    的张量和 2 个额外的形状为 `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`
    的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见 `past_key_values` 输入）。
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了 `past_key_values`，用户可以选择仅输入最后的 `decoder_input_ids`（这些没有将其过去的键值状态提供给此模型）的形状为
    `(batch_size, 1)`，而不是所有形状为 `(batch_size, sequence_length)` 的 `decoder_input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制权，以便将 `input_ids`
    索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为 `True`，则返回 `past_key_values` 键值状态，可用于加速解码（参见
    `past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: Returns
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.Seq2SeqTSModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Seq2SeqTSModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSModelOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.Seq2SeqTSModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TimeSeriesTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig))
    and inputs.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.Seq2SeqTSModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSModelOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（[TimeSeriesTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig)）和输入的不同元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the decoder of the model.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）-
    模型解码器最后一层的隐藏状态序列。'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，则只输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）-
    长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个加上每一层的输出）。'
- en: Hidden-states of the decoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器在每一层的输出的隐藏状态以及可选的初始嵌入输出。
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个加上每一层的输出）。'
- en: Hidden-states of the encoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器在每一层的输出的隐藏状态以及可选的初始嵌入输出。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`loc` (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`,
    *optional*) — Shift values of each time series’ context window which is used to
    give the model inputs of the same magnitude and then used to shift back to the
    original magnitude.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loc`（形状为`(batch_size,)`或`(batch_size, input_size)`的`torch.FloatTensor`，*可选*）-
    用于将每个时间序列的上下文窗口的值移位，以便为模型提供相同数量级的输入，然后用于将其移位回原始数量级。'
- en: '`scale` (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`,
    *optional*) — Scaling values of each time series’ context window which is used
    to give the model inputs of the same magnitude and then used to rescale back to
    the original magnitude.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale`（形状为`(batch_size,)`或`(batch_size, input_size)`的`torch.FloatTensor`，*可选*）-
    用于将每个时间序列的上下文窗口的缩放值移位，以便为模型提供相同数量级的输入，然后用于重新缩放回原始数量级。'
- en: '`static_features` (`torch.FloatTensor` of shape `(batch_size, feature size)`,
    *optional*) — Static features of each time series’ in a batch which are copied
    to the covariates at inference time.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_features`（形状为`(batch_size, feature size)`的`torch.FloatTensor`，*可选*）-
    每个时间序列在批处理中的静态特征，在推断时将复制到协变量中。'
- en: The [TimeSeriesTransformerForPrediction](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerForPrediction)
    forward method, overrides the `__call__` special method.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[TimeSeriesTransformerForPrediction](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerForPrediction)的前向方法重写了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数中定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
