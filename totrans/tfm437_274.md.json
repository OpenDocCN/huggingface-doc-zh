["```py\n( encoder_last_hidden_state: Optional = None pixel_decoder_last_hidden_state: Optional = None transformer_decoder_last_hidden_state: Optional = None encoder_hidden_states: Optional = None pixel_decoder_hidden_states: Optional = None transformer_decoder_hidden_states: Optional = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( loss: Optional = None class_queries_logits: FloatTensor = None masks_queries_logits: FloatTensor = None auxiliary_logits: FloatTensor = None encoder_last_hidden_state: Optional = None pixel_decoder_last_hidden_state: Optional = None transformer_decoder_last_hidden_state: Optional = None encoder_hidden_states: Optional = None pixel_decoder_hidden_states: Optional = None transformer_decoder_hidden_states: Optional = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( fpn_feature_size: int = 256 mask_feature_size: int = 256 no_object_weight: float = 0.1 use_auxiliary_loss: bool = False backbone_config: Optional = None decoder_config: Optional = None init_std: float = 0.02 init_xavier_std: float = 1.0 dice_weight: float = 1.0 cross_entropy_weight: float = 1.0 mask_weight: float = 20.0 output_auxiliary_logits: Optional = None **kwargs )\n```", "```py\n>>> from transformers import MaskFormerConfig, MaskFormerModel\n\n>>> # Initializing a MaskFormer facebook/maskformer-swin-base-ade configuration\n>>> configuration = MaskFormerConfig()\n\n>>> # Initializing a model (with random weights) from the facebook/maskformer-swin-base-ade style configuration\n>>> model = MaskFormerModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( backbone_config: PretrainedConfig decoder_config: PretrainedConfig **kwargs ) \u2192 export const metadata = 'undefined';MaskFormerConfig\n```", "```py\n( do_resize: bool = True size: Dict = None size_divisor: int = 32 resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_factor: float = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None ignore_index: Optional = None do_reduce_labels: bool = False **kwargs )\n```", "```py\n( images: Union segmentation_maps: Union = None instance_id_to_semantic_id: Optional = None do_resize: Optional = None size: Optional = None size_divisor: Optional = None resample: Resampling = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None ignore_index: Optional = None do_reduce_labels: Optional = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( pixel_values_list: List segmentation_maps: Union = None instance_id_to_semantic_id: Union = None ignore_index: Optional = None reduce_labels: bool = False return_tensors: Union = None input_data_format: Union = None ) \u2192 export const metadata = 'undefined';BatchFeature\n```", "```py\n( outputs target_sizes: Optional = None ) \u2192 export const metadata = 'undefined';List[torch.Tensor]\n```", "```py\n( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 target_sizes: Optional = None return_coco_annotation: Optional = False return_binary_maps: Optional = False ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 label_ids_to_fuse: Optional = None target_sizes: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( *args **kwargs )\n```", "```py\n( images segmentation_maps = None **kwargs )\n```", "```py\n( pixel_values_list: List segmentation_maps: Union = None instance_id_to_semantic_id: Union = None ignore_index: Optional = None reduce_labels: bool = False return_tensors: Union = None input_data_format: Union = None ) \u2192 export const metadata = 'undefined';BatchFeature\n```", "```py\n( outputs target_sizes: Optional = None ) \u2192 export const metadata = 'undefined';List[torch.Tensor]\n```", "```py\n( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 target_sizes: Optional = None return_coco_annotation: Optional = False return_binary_maps: Optional = False ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 label_ids_to_fuse: Optional = None target_sizes: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( config: MaskFormerConfig )\n```", "```py\n( pixel_values: Tensor pixel_mask: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.maskformer.modeling_maskformer.MaskFormerModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, MaskFormerModel\n>>> from PIL import Image\n>>> import requests\n\n>>> # load MaskFormer fine-tuned on ADE20k semantic segmentation\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/maskformer-swin-base-ade\")\n>>> model = MaskFormerModel.from_pretrained(\"facebook/maskformer-swin-base-ade\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(**inputs)\n\n>>> # the decoder of MaskFormer outputs hidden states of shape (batch_size, num_queries, hidden_size)\n>>> transformer_decoder_last_hidden_state = outputs.transformer_decoder_last_hidden_state\n>>> list(transformer_decoder_last_hidden_state.shape)\n[1, 100, 256]\n```", "```py\n( config: MaskFormerConfig )\n```", "```py\n( pixel_values: Tensor mask_labels: Optional = None class_labels: Optional = None pixel_mask: Optional = None output_auxiliary_logits: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.maskformer.modeling_maskformer.MaskFormerForInstanceSegmentationOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, MaskFormerForInstanceSegmentation\n>>> from PIL import Image\n>>> import requests\n\n>>> # load MaskFormer fine-tuned on ADE20k semantic segmentation\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/maskformer-swin-base-ade\")\n>>> model = MaskFormerForInstanceSegmentation.from_pretrained(\"facebook/maskformer-swin-base-ade\")\n\n>>> url = (\n...     \"https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\"\n... )\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> # model predicts class_queries_logits of shape `(batch_size, num_queries)`\n>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n>>> class_queries_logits = outputs.class_queries_logits\n>>> masks_queries_logits = outputs.masks_queries_logits\n\n>>> # you can pass them to image_processor for postprocessing\n>>> predicted_semantic_map = image_processor.post_process_semantic_segmentation(\n...     outputs, target_sizes=[image.size[::-1]]\n... )[0]\n\n>>> # we refer to the demo notebooks for visualization (see \"Resources\" section in the MaskFormer docs)\n>>> list(predicted_semantic_map.shape)\n[512, 683]\n```", "```py\n>>> from transformers import AutoImageProcessor, MaskFormerForInstanceSegmentation\n>>> from PIL import Image\n>>> import requests\n\n>>> # load MaskFormer fine-tuned on COCO panoptic segmentation\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/maskformer-swin-base-coco\")\n>>> model = MaskFormerForInstanceSegmentation.from_pretrained(\"facebook/maskformer-swin-base-coco\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> # model predicts class_queries_logits of shape `(batch_size, num_queries)`\n>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n>>> class_queries_logits = outputs.class_queries_logits\n>>> masks_queries_logits = outputs.masks_queries_logits\n\n>>> # you can pass them to image_processor for postprocessing\n>>> result = image_processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n\n>>> # we refer to the demo notebooks for visualization (see \"Resources\" section in the MaskFormer docs)\n>>> predicted_panoptic_map = result[\"segmentation\"]\n>>> list(predicted_panoptic_map.shape)\n[480, 640]\n```"]