# DeepSpeed

> 原文链接: [https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload](https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload)

[DeepSpeed](https://www.deepspeed.ai/) 是一个专为大型模型分布式训练的速度和规模而设计的库，具有数十亿参数。其核心是 Zero Redundancy Optimizer (ZeRO)，它将优化器状态 (ZeRO-1)、梯度 (ZeRO-2) 和参数 (ZeRO-3) 分片到数据并行进程中。这大大减少了内存使用，使您可以将训练扩展到数十亿参数模型。为了进一步提高内存效率，ZeRO-Offload 通过利用 CPU 资源在优化过程中减少 GPU 计算和内存。

这两个功能都受到 🤗 Accelerate 的支持，您可以在 🤗 PEFT 中使用它们。本指南将帮助您学习如何使用我们的 DeepSpeed [训练脚本](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py)。您将配置脚本以使用 ZeRO-3 和 ZeRO-Offload 训练大型条件生成模型。

💡 为了帮助您入门，请查看我们的示例训练脚本，用于[因果语言建模](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_accelerate_ds_zero3_offload.py)和[条件生成](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py)。您可以为自己的应用程序调整这些脚本，甚至在您的任务类似于脚本中的任务时直接使用它们。

## 配置

首先运行以下命令来[创建一个 DeepSpeed 配置文件](https://huggingface.co/docs/accelerate/quicktour#launching-your-distributed-script)与 🤗 Accelerate。`--config_file` 标志允许您将配置文件保存到特定位置，否则它将保存为 `default_config.yaml` 文件在 🤗 Accelerate 缓存中。

配置文件用于在启动训练脚本时设置默认选项。

```py
accelerate config --config_file ds_zero3_cpu.yaml
```

您将被问及有关您的设置的几个问题，并配置以下参数。在此示例中，您将使用 ZeRO-3 和 ZeRO-Offload，请确保选择这些选项。

```py
`zero_stage`: [0] Disabled, [1] optimizer state partitioning, [2] optimizer+gradient state partitioning and [3] optimizer+gradient+parameter partitioning
`gradient_accumulation_steps`: Number of training steps to accumulate gradients before averaging and applying them.
`gradient_clipping`: Enable gradient clipping with value.
`offload_optimizer_device`: [none] Disable optimizer offloading, [cpu] offload optimizer to CPU, [nvme] offload optimizer to NVMe SSD. Only applicable with ZeRO >= Stage-2.
`offload_param_device`: [none] Disable parameter offloading, [cpu] offload parameters to CPU, [nvme] offload parameters to NVMe SSD. Only applicable with ZeRO Stage-3.
`zero3_init_flag`: Decides whether to enable `deepspeed.zero.Init` for constructing massive models. Only applicable with ZeRO Stage-3.
`zero3_save_16bit_model`: Decides whether to save 16-bit model weights when using ZeRO Stage-3.
`mixed_precision`: `no` for FP32 training, `fp16` for FP16 mixed-precision training and `bf16` for BF16 mixed-precision training. 
```

一个[配置文件](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/accelerate_ds_zero3_cpu_offload_config.yaml)可能如下所示。最重要的是要注意 `zero_stage` 设置为 `3`，`offload_optimizer_device` 和 `offload_param_device` 设置为 `cpu`。

```py
compute_environment: LOCAL_MACHINE
deepspeed_config:
  gradient_accumulation_steps: 1
  gradient_clipping: 1.0
  offload_optimizer_device: cpu
  offload_param_device: cpu
  zero3_init_flag: true
  zero3_save_16bit_model: true
  zero_stage: 3
distributed_type: DEEPSPEED
downcast_bf16: 'no'
dynamo_backend: 'NO'
fsdp_config: {}
machine_rank: 0
main_training_function: main
megatron_lm_config: {}
mixed_precision: 'no'
num_machines: 1
num_processes: 1
rdzv_backend: static
same_network: true
use_cpu: false
```

## 重要部分

让我们深入了解脚本，以便您了解正在发生的事情，并理解它是如何工作的。

在 [`main`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py#L103) 函数中，脚本创建了一个 [Accelerator](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator) 类来初始化分布式训练所需的所有必要条件。

💡 随意更改 `main` 函数中的模型和数据集。如果您的数据集格式与脚本中的不同，您可能还需要编写自己的预处理函数。

脚本还为您正在使用的 🤗 PEFT 方法创建配置，本例中为 LoRA。[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig) 指定了任务类型和重要参数，如低秩矩阵的维度、矩阵缩放因子以及 LoRA 层的 dropout 概率。如果您想使用其他 🤗 PEFT 方法，请确保将 `LoraConfig` 替换为适当的 [class](../package_reference/tuners)。

```py
 def main():
+    accelerator = Accelerator()
     model_name_or_path = "facebook/bart-large"
     dataset_name = "twitter_complaints"
+    peft_config = LoraConfig(
         task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
     )
```

在整个脚本中，您将看到[main_process_first](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.main_process_first)和[wait_for_everyone](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.wait_for_everyone)函数，它们有助于控制和同步进程的执行时间。

[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)函数接受一个基础模型和您之前准备的`peft_config`，以创建一个[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)：

```py
  model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
+ model = get_peft_model(model, peft_config)
```

将所有相关的训练对象传递给🤗 Accelerate的[prepare](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)，确保一切准备就绪：

```py
model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler = accelerator.prepare(
    model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler
)
```

接下来的代码段检查`Accelerator`中是否使用了DeepSpeed插件，如果插件存在，则`Accelerator`将根据配置文件中指定的ZeRO-3来使用它：

```py
is_ds_zero_3 = False
if getattr(accelerator.state, "deepspeed_plugin", None):
    is_ds_zero_3 = accelerator.state.deepspeed_plugin.zero_stage == 3
```

在训练循环中，通常的`loss.backward()`被🤗 Accelerate的[backward](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.backward)替换，它根据您的配置使用正确的`backward()`方法：

```py
  for epoch in range(num_epochs):
      with TorchTracemalloc() as tracemalloc:
          model.train()
          total_loss = 0
          for step, batch in enumerate(tqdm(train_dataloader)):
              outputs = model(**batch)
              loss = outputs.loss
              total_loss += loss.detach().float()
+             accelerator.backward(loss)
              optimizer.step()
              lr_scheduler.step()
              optimizer.zero_grad()
```

就这些了！脚本的其余部分处理训练循环、评估，甚至为您将其推送到Hub。

## 训练

运行以下命令启动训练脚本。之前，您将配置文件保存为`ds_zero3_cpu.yaml`，因此您需要通过`--config_file`参数将路径传递给启动器，就像这样：

```py
accelerate launch --config_file ds_zero3_cpu.yaml examples/peft_lora_seq2seq_accelerate_ds_zero3_offload.py
```

您将看到一些跟踪内存使用情况的输出日志，在训练完成后，脚本将返回准确性并将预测与标签进行比较：

```py
GPU Memory before entering the train : 1916
GPU Memory consumed at the end of the train (end-begin): 66
GPU Peak Memory consumed during the train (max-begin): 7488
GPU Total Peak Memory consumed during the train (max): 9404
CPU Memory before entering the train : 19411
CPU Memory consumed at the end of the train (end-begin): 0
CPU Peak Memory consumed during the train (max-begin): 0
CPU Total Peak Memory consumed during the train (max): 19411
epoch=4: train_ppl=tensor(1.0705, device='cuda:0') train_epoch_loss=tensor(0.0681, device='cuda:0')
100%|████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:27<00:00,  3.92s/it]
GPU Memory before entering the eval : 1982
GPU Memory consumed at the end of the eval (end-begin): -66
GPU Peak Memory consumed during the eval (max-begin): 672
GPU Total Peak Memory consumed during the eval (max): 2654
CPU Memory before entering the eval : 19411
CPU Memory consumed at the end of the eval (end-begin): 0
CPU Peak Memory consumed during the eval (max-begin): 0
CPU Total Peak Memory consumed during the eval (max): 19411
accuracy=100.0
eval_preds[:10]=['no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint', 'no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint']
dataset['train'][label_column][:10]=['no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint', 'no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint']
```
