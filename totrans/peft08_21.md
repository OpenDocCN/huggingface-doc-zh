# DeepSpeed

> åŸæ–‡é“¾æ¥: [https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload](https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload)

[DeepSpeed](https://www.deepspeed.ai/) æ˜¯ä¸€ä¸ªä¸“ä¸ºå¤§å‹æ¨¡å‹åˆ†å¸ƒå¼è®­ç»ƒçš„é€Ÿåº¦å’Œè§„æ¨¡è€Œè®¾è®¡çš„åº“ï¼Œå…·æœ‰æ•°åäº¿å‚æ•°ã€‚å…¶æ ¸å¿ƒæ˜¯ Zero Redundancy Optimizer (ZeRO)ï¼Œå®ƒå°†ä¼˜åŒ–å™¨çŠ¶æ€ (ZeRO-1)ã€æ¢¯åº¦ (ZeRO-2) å’Œå‚æ•° (ZeRO-3) åˆ†ç‰‡åˆ°æ•°æ®å¹¶è¡Œè¿›ç¨‹ä¸­ã€‚è¿™å¤§å¤§å‡å°‘äº†å†…å­˜ä½¿ç”¨ï¼Œä½¿æ‚¨å¯ä»¥å°†è®­ç»ƒæ‰©å±•åˆ°æ•°åäº¿å‚æ•°æ¨¡å‹ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å†…å­˜æ•ˆç‡ï¼ŒZeRO-Offload é€šè¿‡åˆ©ç”¨ CPU èµ„æºåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å‡å°‘ GPU è®¡ç®—å’Œå†…å­˜ã€‚

è¿™ä¸¤ä¸ªåŠŸèƒ½éƒ½å—åˆ° ğŸ¤— Accelerate çš„æ”¯æŒï¼Œæ‚¨å¯ä»¥åœ¨ ğŸ¤— PEFT ä¸­ä½¿ç”¨å®ƒä»¬ã€‚æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨å­¦ä¹ å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„ DeepSpeed [è®­ç»ƒè„šæœ¬](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py)ã€‚æ‚¨å°†é…ç½®è„šæœ¬ä»¥ä½¿ç”¨ ZeRO-3 å’Œ ZeRO-Offload è®­ç»ƒå¤§å‹æ¡ä»¶ç”Ÿæˆæ¨¡å‹ã€‚

ğŸ’¡ ä¸ºäº†å¸®åŠ©æ‚¨å…¥é—¨ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„ç¤ºä¾‹è®­ç»ƒè„šæœ¬ï¼Œç”¨äº[å› æœè¯­è¨€å»ºæ¨¡](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_accelerate_ds_zero3_offload.py)å’Œ[æ¡ä»¶ç”Ÿæˆ](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py)ã€‚æ‚¨å¯ä»¥ä¸ºè‡ªå·±çš„åº”ç”¨ç¨‹åºè°ƒæ•´è¿™äº›è„šæœ¬ï¼Œç”šè‡³åœ¨æ‚¨çš„ä»»åŠ¡ç±»ä¼¼äºè„šæœ¬ä¸­çš„ä»»åŠ¡æ—¶ç›´æ¥ä½¿ç”¨å®ƒä»¬ã€‚

## é…ç½®

é¦–å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥[åˆ›å»ºä¸€ä¸ª DeepSpeed é…ç½®æ–‡ä»¶](https://huggingface.co/docs/accelerate/quicktour#launching-your-distributed-script)ä¸ ğŸ¤— Accelerateã€‚`--config_file` æ ‡å¿—å…è®¸æ‚¨å°†é…ç½®æ–‡ä»¶ä¿å­˜åˆ°ç‰¹å®šä½ç½®ï¼Œå¦åˆ™å®ƒå°†ä¿å­˜ä¸º `default_config.yaml` æ–‡ä»¶åœ¨ ğŸ¤— Accelerate ç¼“å­˜ä¸­ã€‚

é…ç½®æ–‡ä»¶ç”¨äºåœ¨å¯åŠ¨è®­ç»ƒè„šæœ¬æ—¶è®¾ç½®é»˜è®¤é€‰é¡¹ã€‚

```py
accelerate config --config_file ds_zero3_cpu.yaml
```

æ‚¨å°†è¢«é—®åŠæœ‰å…³æ‚¨çš„è®¾ç½®çš„å‡ ä¸ªé—®é¢˜ï¼Œå¹¶é…ç½®ä»¥ä¸‹å‚æ•°ã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨ ZeRO-3 å’Œ ZeRO-Offloadï¼Œè¯·ç¡®ä¿é€‰æ‹©è¿™äº›é€‰é¡¹ã€‚

```py
`zero_stage`: [0] Disabled, [1] optimizer state partitioning, [2] optimizer+gradient state partitioning and [3] optimizer+gradient+parameter partitioning
`gradient_accumulation_steps`: Number of training steps to accumulate gradients before averaging and applying them.
`gradient_clipping`: Enable gradient clipping with value.
`offload_optimizer_device`: [none] Disable optimizer offloading, [cpu] offload optimizer to CPU, [nvme] offload optimizer to NVMe SSD. Only applicable with ZeRO >= Stage-2.
`offload_param_device`: [none] Disable parameter offloading, [cpu] offload parameters to CPU, [nvme] offload parameters to NVMe SSD. Only applicable with ZeRO Stage-3.
`zero3_init_flag`: Decides whether to enable `deepspeed.zero.Init` for constructing massive models. Only applicable with ZeRO Stage-3.
`zero3_save_16bit_model`: Decides whether to save 16-bit model weights when using ZeRO Stage-3.
`mixed_precision`: `no` for FP32 training, `fp16` for FP16 mixed-precision training and `bf16` for BF16 mixed-precision training. 
```

ä¸€ä¸ª[é…ç½®æ–‡ä»¶](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/accelerate_ds_zero3_cpu_offload_config.yaml)å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºã€‚æœ€é‡è¦çš„æ˜¯è¦æ³¨æ„ `zero_stage` è®¾ç½®ä¸º `3`ï¼Œ`offload_optimizer_device` å’Œ `offload_param_device` è®¾ç½®ä¸º `cpu`ã€‚

```py
compute_environment: LOCAL_MACHINE
deepspeed_config:
  gradient_accumulation_steps: 1
  gradient_clipping: 1.0
  offload_optimizer_device: cpu
  offload_param_device: cpu
  zero3_init_flag: true
  zero3_save_16bit_model: true
  zero_stage: 3
distributed_type: DEEPSPEED
downcast_bf16: 'no'
dynamo_backend: 'NO'
fsdp_config: {}
machine_rank: 0
main_training_function: main
megatron_lm_config: {}
mixed_precision: 'no'
num_machines: 1
num_processes: 1
rdzv_backend: static
same_network: true
use_cpu: false
```

## é‡è¦éƒ¨åˆ†

è®©æˆ‘ä»¬æ·±å…¥äº†è§£è„šæœ¬ï¼Œä»¥ä¾¿æ‚¨äº†è§£æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ï¼Œå¹¶ç†è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

åœ¨ [`main`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py#L103) å‡½æ•°ä¸­ï¼Œè„šæœ¬åˆ›å»ºäº†ä¸€ä¸ª [Accelerator](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator) ç±»æ¥åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒæ‰€éœ€çš„æ‰€æœ‰å¿…è¦æ¡ä»¶ã€‚

ğŸ’¡ éšæ„æ›´æ”¹ `main` å‡½æ•°ä¸­çš„æ¨¡å‹å’Œæ•°æ®é›†ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†æ ¼å¼ä¸è„šæœ¬ä¸­çš„ä¸åŒï¼Œæ‚¨å¯èƒ½è¿˜éœ€è¦ç¼–å†™è‡ªå·±çš„é¢„å¤„ç†å‡½æ•°ã€‚

è„šæœ¬è¿˜ä¸ºæ‚¨æ­£åœ¨ä½¿ç”¨çš„ ğŸ¤— PEFT æ–¹æ³•åˆ›å»ºé…ç½®ï¼Œæœ¬ä¾‹ä¸­ä¸º LoRAã€‚[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig) æŒ‡å®šäº†ä»»åŠ¡ç±»å‹å’Œé‡è¦å‚æ•°ï¼Œå¦‚ä½ç§©çŸ©é˜µçš„ç»´åº¦ã€çŸ©é˜µç¼©æ”¾å› å­ä»¥åŠ LoRA å±‚çš„ dropout æ¦‚ç‡ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨å…¶ä»– ğŸ¤— PEFT æ–¹æ³•ï¼Œè¯·ç¡®ä¿å°† `LoraConfig` æ›¿æ¢ä¸ºé€‚å½“çš„ [class](../package_reference/tuners)ã€‚

```py
 def main():
+    accelerator = Accelerator()
     model_name_or_path = "facebook/bart-large"
     dataset_name = "twitter_complaints"
+    peft_config = LoraConfig(
         task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
     )
```

åœ¨æ•´ä¸ªè„šæœ¬ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°[main_process_first](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.main_process_first)å’Œ[wait_for_everyone](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.wait_for_everyone)å‡½æ•°ï¼Œå®ƒä»¬æœ‰åŠ©äºæ§åˆ¶å’ŒåŒæ­¥è¿›ç¨‹çš„æ‰§è¡Œæ—¶é—´ã€‚

[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)å‡½æ•°æ¥å—ä¸€ä¸ªåŸºç¡€æ¨¡å‹å’Œæ‚¨ä¹‹å‰å‡†å¤‡çš„`peft_config`ï¼Œä»¥åˆ›å»ºä¸€ä¸ª[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)ï¼š

```py
  model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
+ model = get_peft_model(model, peft_config)
```

å°†æ‰€æœ‰ç›¸å…³çš„è®­ç»ƒå¯¹è±¡ä¼ é€’ç»™ğŸ¤— Accelerateçš„[prepare](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)ï¼Œç¡®ä¿ä¸€åˆ‡å‡†å¤‡å°±ç»ªï¼š

```py
model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler = accelerator.prepare(
    model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler
)
```

æ¥ä¸‹æ¥çš„ä»£ç æ®µæ£€æŸ¥`Accelerator`ä¸­æ˜¯å¦ä½¿ç”¨äº†DeepSpeedæ’ä»¶ï¼Œå¦‚æœæ’ä»¶å­˜åœ¨ï¼Œåˆ™`Accelerator`å°†æ ¹æ®é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šçš„ZeRO-3æ¥ä½¿ç”¨å®ƒï¼š

```py
is_ds_zero_3 = False
if getattr(accelerator.state, "deepspeed_plugin", None):
    is_ds_zero_3 = accelerator.state.deepspeed_plugin.zero_stage == 3
```

åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œé€šå¸¸çš„`loss.backward()`è¢«ğŸ¤— Accelerateçš„[backward](https://huggingface.co/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.backward)æ›¿æ¢ï¼Œå®ƒæ ¹æ®æ‚¨çš„é…ç½®ä½¿ç”¨æ­£ç¡®çš„`backward()`æ–¹æ³•ï¼š

```py
  for epoch in range(num_epochs):
      with TorchTracemalloc() as tracemalloc:
          model.train()
          total_loss = 0
          for step, batch in enumerate(tqdm(train_dataloader)):
              outputs = model(**batch)
              loss = outputs.loss
              total_loss += loss.detach().float()
+             accelerator.backward(loss)
              optimizer.step()
              lr_scheduler.step()
              optimizer.zero_grad()
```

å°±è¿™äº›äº†ï¼è„šæœ¬çš„å…¶ä½™éƒ¨åˆ†å¤„ç†è®­ç»ƒå¾ªç¯ã€è¯„ä¼°ï¼Œç”šè‡³ä¸ºæ‚¨å°†å…¶æ¨é€åˆ°Hubã€‚

## è®­ç»ƒ

è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒè„šæœ¬ã€‚ä¹‹å‰ï¼Œæ‚¨å°†é…ç½®æ–‡ä»¶ä¿å­˜ä¸º`ds_zero3_cpu.yaml`ï¼Œå› æ­¤æ‚¨éœ€è¦é€šè¿‡`--config_file`å‚æ•°å°†è·¯å¾„ä¼ é€’ç»™å¯åŠ¨å™¨ï¼Œå°±åƒè¿™æ ·ï¼š

```py
accelerate launch --config_file ds_zero3_cpu.yaml examples/peft_lora_seq2seq_accelerate_ds_zero3_offload.py
```

æ‚¨å°†çœ‹åˆ°ä¸€äº›è·Ÿè¸ªå†…å­˜ä½¿ç”¨æƒ…å†µçš„è¾“å‡ºæ—¥å¿—ï¼Œåœ¨è®­ç»ƒå®Œæˆåï¼Œè„šæœ¬å°†è¿”å›å‡†ç¡®æ€§å¹¶å°†é¢„æµ‹ä¸æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼š

```py
GPU Memory before entering the train : 1916
GPU Memory consumed at the end of the train (end-begin): 66
GPU Peak Memory consumed during the train (max-begin): 7488
GPU Total Peak Memory consumed during the train (max): 9404
CPU Memory before entering the train : 19411
CPU Memory consumed at the end of the train (end-begin): 0
CPU Peak Memory consumed during the train (max-begin): 0
CPU Total Peak Memory consumed during the train (max): 19411
epoch=4: train_ppl=tensor(1.0705, device='cuda:0') train_epoch_loss=tensor(0.0681, device='cuda:0')
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:27<00:00,  3.92s/it]
GPU Memory before entering the eval : 1982
GPU Memory consumed at the end of the eval (end-begin): -66
GPU Peak Memory consumed during the eval (max-begin): 672
GPU Total Peak Memory consumed during the eval (max): 2654
CPU Memory before entering the eval : 19411
CPU Memory consumed at the end of the eval (end-begin): 0
CPU Peak Memory consumed during the eval (max-begin): 0
CPU Total Peak Memory consumed during the eval (max): 19411
accuracy=100.0
eval_preds[:10]=['no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint', 'no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint']
dataset['train'][label_column][:10]=['no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint', 'no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint']
```
