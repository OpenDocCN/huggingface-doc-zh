- en: Supervised Fine-tuning Trainer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督微调训练器
- en: 'Original text: [https://huggingface.co/docs/trl/sft_trainer](https://huggingface.co/docs/trl/sft_trainer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/trl/sft_trainer](https://huggingface.co/docs/trl/sft_trainer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Supervised fine-tuning (or SFT for short) is a crucial step in RLHF. In TRL
    we provide an easy-to-use API to create your SFT models and train them with few
    lines of code on your dataset.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 监督微调（或简称SFT）是RLHF中的一个关键步骤。在TRL中，我们提供了一个易于使用的API来创建您的SFT模型，并用几行代码在您的数据集上训练它们。
- en: Check out a complete flexible example at [`examples/scripts/sft.py`](https://github.com/huggingface/trl/tree/main/examples/scripts/sft.py).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在[`examples/scripts/sft.py`](https://github.com/huggingface/trl/tree/main/examples/scripts/sft.py)中查看一个完整灵活的示例。
- en: Quickstart
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速开始
- en: 'If you have a dataset hosted on the 🤗 Hub, you can easily fine-tune your SFT
    model using [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer) from TRL.
    Let us assume your dataset is `imdb`, the text you want to predict is inside the
    `text` field of the dataset, and you want to fine-tune the `facebook/opt-350m`
    model. The following code-snippet takes care of all the data pre-processing and
    training for you:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在🤗 Hub上托管了一个数据集，您可以使用[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)从TRL轻松微调您的SFT模型。假设您的数据集是`imdb`，您要预测的文本位于数据集的`text`字段中，您想要微调`facebook/opt-350m`模型。以下代码片段会处理所有的数据预处理和训练工作：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Make sure to pass a correct value for `max_seq_length` as the default value
    will be set to `min(tokenizer.model_max_length, 1024)`.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 确保为`max_seq_length`传递正确的值，因为默认值将设置为`min(tokenizer.model_max_length, 1024)`。
- en: 'You can also construct a model outside of the trainer and pass it as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在训练师之外构建一个模型，并将其传递如下：
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The above snippets will use the default training arguments from the [`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)
    class. If you want to modify that, make sure to create your own `TrainingArguments`
    object and pass it to the [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)
    constructor as it is done on the [`supervised_finetuning.py` script](https://github.com/huggingface/trl/blob/main/examples/stack_llama/scripts/supervised_finetuning.py)
    on the stack-llama example.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段将使用[`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)类中的默认训练参数。如果您想修改它，请确保创建自己的`TrainingArguments`对象并将其传递给[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)构造函数，就像在[`supervised_finetuning.py`脚本](https://github.com/huggingface/trl/blob/main/examples/stack_llama/scripts/supervised_finetuning.py)中的stack-llama示例中所做的那样。
- en: Advanced usage
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级用法
- en: Train on completions only
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 仅在完成上训练
- en: 'You can use the `DataCollatorForCompletionOnlyLM` to train your model on the
    generated prompts only. Note that this works only in the case when `packing=False`.
    To instantiate that collator for instruction data, pass a response template and
    the tokenizer. Here is an example of how it would work to fine-tune `opt-350m`
    on completions only on the CodeAlpaca dataset:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`DataCollatorForCompletionOnlyLM`仅在生成提示上训练您的模型。请注意，这仅在`packing=False`的情况下有效。为了为指令数据实例化该收集器，传递一个响应模板和分词器。以下是在CodeAlpaca数据集上仅对完成进行微调`opt-350m`的工作示例：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To instantiate that collator for assistant style conversation data, pass a
    response template, an instruction template and the tokenizer. Here is an example
    of how it would work to fine-tune `opt-350m` on assistant completions only on
    the Open Assistant Guanaco dataset:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为助手风格对话数据实例化该收集器时，请传递一个响应模板、一个指令模板和分词器。以下是在Open Assistant Guanaco数据集上仅对助手完成进行微调`opt-350m`的工作示例：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Make sure to have a `pad_token_id` which is different from `eos_token_id` which
    can result in the model not properly predicting EOS (End of Sentence) tokens during
    generation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 确保有一个`pad_token_id`，它与`eos_token_id`不同，这可以防止模型在生成过程中无法正确预测EOS（句子结束）标记。
- en: Using token_ids directly for response_template
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 直接使用token_ids作为response_template
- en: 'Some tokenizers like Llama 2 (`meta-llama/Llama-2-XXb-hf`) tokenize sequences
    differently depending whether they have context or not. For example:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一些分词器（如Llama 2（`meta-llama/Llama-2-XXb-hf`））根据是否有上下文而不同地对序列进行分词。例如：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this case, and due to lack of context in `response_template`, the same string
    (”### Assistant:”) is tokenized differently:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于`response_template`中缺乏上下文，相同的字符串（“### Assistant:”）被不同地分词：
- en: 'Text (with context): `[2277, 29937, 4007, 22137, 29901]`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本（带上下文）：`[2277, 29937, 4007, 22137, 29901]`
- en: '`response_template` (without context): `[835, 4007, 22137, 29901]`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`response_template`（无上下文）：`[835, 4007, 22137, 29901]`'
- en: 'This will lead to an error when the `DataCollatorForCompletionOnlyLM` does
    not find the `response_template` in the dataset example text:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当`DataCollatorForCompletionOnlyLM`在数据集示例文本中找不到`response_template`时，这将导致错误：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To solve this, you can tokenize the `response_template` with the same context
    than in the dataset, truncate it as needed and pass the `token_ids` directly to
    the `response_template` argument of the `DataCollatorForCompletionOnlyLM` class.
    For example:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，您可以使用与数据集中相同上下文的方式对`response_template`进行分词，根据需要截断它，并将`token_ids`直接传递给`DataCollatorForCompletionOnlyLM`类的`response_template`参数。例如：
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Add Special Tokens for Chat Format
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为聊天格式添加特殊标记
- en: 'Adding special tokens to a language model is crucial for training chat models.
    These tokens are added between the different roles in a conversation, such as
    the user, assistant, and system and help the model recognize the structure and
    flow of a conversation. This setup is essential for enabling the model to generate
    coherent and contextually appropriate responses in a chat environment. The `setup_chat_format()`
    function in `trl` easily sets up a model and tokenizer for conversational AI tasks.
    This function:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为语言模型添加特殊标记对于训练聊天模型至关重要。这些标记被添加在对话中的不同角色之间，如用户、助手和系统，并帮助模型识别对话的结构和流程。这种设置对于使模型能够在聊天环境中生成连贯和上下文适当的响应至关重要。`trl`中的`setup_chat_format()`函数可以轻松为会话AI任务设置模型和分词器。这个函数：
- en: Adds special tokens to the tokenizer, e.g. `<|im_start|>` and `<|im_end|>`,
    to indicate the start and end of a conversation.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向标记化器添加特殊标记，例如`<|im_start|>`和`<|imm_end|>`，以指示会话的开始和结束。
- en: Resizes the model’s embedding layer to accommodate the new tokens.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整模型的嵌入层以容纳新的标记。
- en: Sets the `chat_template` of the tokenizer, which is used to format the input
    data into a chat-like format. The default is `chatml` from OpenAI.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置令牌化器的`chat_template`，用于将输入数据格式化为类似聊天的格式。默认值为OpenAI的`chatml`。
- en: '*optionally* you can pass `resize_to_multiple_of` to resize the embedding layer
    to a multiple of the `resize_to_multiple_of` argument, e.g. 64\. If you want to
    see more formats being supported in the future, please open a GitHub issue on
    [trl](https://github.com/huggingface/trl)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可选*您可以传递`resize_to_multiple_of`来将嵌入层调整为`resize_to_multiple_of`参数的倍数，例如64。如果您希望将来支持更多格式，请在[trl](https://github.com/huggingface/trl)上打开GitHub问题'
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With our model and tokenizer set up, we can now fine-tune our model on a conversational
    dataset. Below is an example of how a dataset can be formatted for fine-tuning.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们的模型和分词器设置，我们现在可以在对话数据集上对模型进行微调。以下是一个格式化数据集进行微调的示例。
- en: Dataset format support
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集格式支持
- en: 'The [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer) supports popular
    dataset formats. This allows you to pass the dataset to the trainer without any
    pre-processing directly. The following formats are supported:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)支持流行的数据集格式。这使您可以直接将数据集传递给训练器，无需任何预处理。支持以下格式：'
- en: conversational format
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对话格式
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: instruction format
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指令格式
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If your dataset uses one of the above formats, you can directly pass it to the
    trainer without pre-processing. The [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)
    will then format the dataset for you using the defined format from the model’s
    tokenizer with the [apply_chat_template](https://huggingface.co/docs/transformers/main/en/chat_templating#templates-for-chat-models)
    method.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据集使用上述格式之一，您可以直接将其传递给训练器而无需预处理。[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)将使用模型的分词器中定义的格式使用[apply_chat_template](https://huggingface.co/docs/transformers/main/en/chat_templating#templates-for-chat-models)方法为您格式化数据集。
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If the dataset is not in one those format you can either preprocess the dataset
    to match the formatting or pass a formatting function to the SFTTrainer to do
    it for you. Let’s have a look.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集不是这些格式之一，您可以预处理数据集以匹配格式，或者将格式化函数传递给SFTTrainer以代替。让我们看看。
- en: Format your input prompts
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 格式化您的输入提示
- en: 'For instruction fine-tuning, it is quite common to have two columns inside
    the dataset: one for the prompt & the other for the response. This allows people
    to format examples like [Stanford-Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
    did as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于指令微调，通常在数据集中有两列：一个用于提示，另一个用于响应。这使人们可以像[Stanford-Alpaca](https://github.com/tatsu-lab/stanford_alpaca)那样格式化示例：
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let us assume your dataset has two fields, `question` and `answer`. Therefore
    you can just run:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的数据集有两个字段，`question`和`answer`。因此您可以直接运行：
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To preperly format your input make sure to process all the examples by looping
    over them and returning a list of processed text. Check out a full example on
    how to use SFTTrainer on alpaca dataset [here](https://github.com/huggingface/trl/pull/444#issue-1760952763)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确格式化您的输入，请确保通过循环处理所有示例并返回处理过的文本列表。查看如何在alpaca数据集上使用SFTTrainer的完整示例[这里](https://github.com/huggingface/trl/pull/444#issue-1760952763)
- en: Packing dataset ( ConstantLengthDataset )
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 打包数据集（ConstantLengthDataset）
- en: '[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer) supports *example
    packing*, where multiple short examples are packed in the same input sequence
    to increase training efficiency. This is done with the `ConstantLengthDataset`
    utility class that returns constant length chunks of tokens from a stream of examples.
    To enable the usage of this dataset class, simply pass `packing=True` to the [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)
    constructor.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)支持*示例打包*，其中多个短示例打包在同一输入序列中以增加训练效率。这是通过`ConstantLengthDataset`实用程序类完成的，该类从示例流中返回常长度的令牌块。要启用此数据集类的使用，只需将`packing=True`传递给[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)构造函数。'
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that if you use a packed dataset and if you pass `max_steps` in the training
    arguments you will probably train your models for more than few epochs, depending
    on the way you have configured the packed dataset and the training protocol. Double
    check that you know and understand what you are doing.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您使用打包数据集，并且在训练参数中传递了`max_steps`，则您可能会训练模型多个周期，具体取决于您如何配置打包数据集和训练协议。请确保您知道并理解您正在做什么。
- en: Customize your prompts using packed dataset
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用打包数据集自定义您的提示
- en: 'If your dataset has several fields that you want to combine, for example if
    the dataset has `question` and `answer` fields and you want to combine them, you
    can pass a formatting function to the trainer that will take care of that. For
    example:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据集有几个字段要合并，例如数据集有`question`和`answer`字段，您想要合并它们，您可以将格式化函数传递给训练器来处理。例如：
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can also customize the `ConstantLengthDataset` much more by directly passing
    the arguments to the [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)
    constructor. Please refer to that class’ signature for more information.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过直接将参数传递给[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)构造函数来更多地定制`ConstantLengthDataset`。请参考该类的签名以获取更多信息。
- en: Control over the pretrained model
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对预训练模型的控制
- en: You can directly pass the kwargs of the `from_pretrained()` method to the [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer).
    For example, if you want to load a model in a different precision, analogous to
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以直接将`from_pretrained()`方法的kwargs传递给[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)。例如，如果要以不同的精度加载模型，类似于
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that all keyword arguments of `from_pretrained()` are supported.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`from_pretrained()`的所有关键字参数都受支持。
- en: Training adapters
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练适配器
- en: We also support a tight integration with 🤗 PEFT library so that any user can
    conveniently train adapters and share them on the Hub instead of training the
    entire model
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还支持与🤗 PEFT库的紧密集成，以便任何用户可以方便地训练适配器并在Hub上共享它们，而不是训练整个模型
- en: '[PRE17]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You can also continue training your `PeftModel`. For that, first load a `PeftModel`
    outside `SFTTrainer` and pass it directly to the trainer without the `peft_config`
    argument being passed.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以继续训练您的`PeftModel`。为此，请首先在`SFTTrainer`之外加载`PeftModel`，并直接将其传递给训练器，而无需传递`peft_config`参数。
- en: Training adapters with base 8 bit models
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用基础8位模型训练适配器
- en: 'For that you need to first load your 8bit model outside the Trainer and pass
    a `PeftConfig` to the trainer. For example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，您需要首先在Trainer之外加载您的8位模型，并将`PeftConfig`传递给训练器。例如：
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Using Flash Attention and Flash Attention 2
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Flash Attention和Flash Attention 2
- en: You can benefit from Flash Attention 1 & 2 using SFTTrainer out of the box with
    minimal changes of code. First, to make sure you have all the latest features
    from transformers, install transformers from source
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用SFTTrainer中的Flash Attention 1和2来获益，只需进行最少的代码更改。首先，为了确保您拥有来自transformers的所有最新功能，请从源代码安装transformers
- en: '[PRE19]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that Flash Attention only works on GPU now and under half-precision regime
    (when using adapters, base model loaded in half-precision) Note also both features
    are perfectly compatible with other tools such as quantization.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Flash Attention现在仅在GPU上运行，并且在半精度制度下运行（当使用适配器时，基础模型以半精度加载）。还请注意，这两个功能与量化等其他工具完全兼容。
- en: Using Flash-Attention 1
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Flash-Attention 1
- en: 'For Flash Attention 1 you can use the `BetterTransformer` API and force-dispatch
    the API to use Flash Attention kernel. First, install the latest optimum package:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Flash Attention 1，您可以使用`BetterTransformer` API并强制分派API以使用Flash Attention内核。首先，安装最新的optimum包：
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Once you have loaded your model, wrap the `trainer.train()` call under the
    `with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):`
    context manager:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 加载模型后，将`trainer.train()`调用包装在`with torch.backends.cuda.sdp_kernel(enable_flash=True,
    enable_math=False, enable_mem_efficient=False):`上下文管理器中：
- en: '[PRE21]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that you cannot train your model using Flash Attention 1 on an arbitrary
    dataset as `torch.scaled_dot_product_attention` does not support training with
    padding tokens if you use Flash Attention kernels. Therefore you can only use
    that feature with `packing=True`. If your dataset contains padding tokens, consider
    switching to Flash Attention 2 integration.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果使用Flash Attention内核，您不能在任意数据集上训练您的模型，因为`torch.scaled_dot_product_attention`不支持使用填充令牌进行训练。因此，您只能在`packing=True`的情况下使用该功能。如果您的数据集包含填充令牌，请考虑切换到Flash
    Attention 2集成。
- en: Below are some numbers you can get in terms of speedup and memory efficiency,
    using Flash Attention 1, on a single NVIDIA-T4 16GB.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在单个NVIDIA-T4 16GB上使用Flash Attention 1可以获得的一些加速和内存效率方面的数字。
- en: '| use_flash_attn_1 | model_name | max_seq_len | batch_size | time per training
    step |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| use_flash_attn_1 | model_name | max_seq_len | batch_size | time per training
    step |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| x | facebook/opt-350m | 2048 | 8 | ~59.1s |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| x | facebook/opt-350m | 2048 | 8 | ~59.1s |'
- en: '|  | facebook/opt-350m | 2048 | 8 | **OOM** |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | facebook/opt-350m | 2048 | 8 | **OOM** |'
- en: '| x | facebook/opt-350m | 2048 | 4 | ~30.3s |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| x | facebook/opt-350m | 2048 | 4 | ~30.3s |'
- en: '|  | facebook/opt-350m | 2048 | 4 | ~148.9s |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | facebook/opt-350m | 2048 | 4 | ~148.9s |'
- en: Using Flash Attention-2
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Flash Attention-2
- en: 'To use Flash Attention 2, first install the latest `flash-attn` package:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Flash Attention 2，首先安装最新的`flash-attn`包：
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'And add `use_flash_attention_2=True` when calling `from_pretrained`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用`from_pretrained`时添加`use_flash_attention_2=True`：
- en: '[PRE23]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If you don’t use quantization, make sure your model is loaded in half-precision
    and dispatch your model on a supported GPU device. After loading your model, you
    can either train it as it is, or attach adapters and train adapters on it in case
    your model is quantized.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不使用量化，请确保您的模型以半精度加载，并将您的模型分派到支持的GPU设备上。加载模型后，您可以按原样训练它，或者在您的模型经过量化的情况下，附加适配器并在其上训练适配器。
- en: In contrary to Flash Attention 1, the integration makes it possible to train
    your model on an arbitrary dataset that also includes padding tokens.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与Flash Attention 1相反，集成使得可以在包含填充令牌的任意数据集上训练您的模型成为可能。
- en: Enhance model’s performances using NEFTune
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用NEFTune增强模型性能
- en: 'NEFTune is a technique to boost the performance of chat models and was introduced
    by the paper [“NEFTune: Noisy Embeddings Improve Instruction Finetuning”](https://arxiv.org/abs/2310.05914)
    from Jain et al. it consists of adding noise to the embedding vectors during training.
    According to the abstract of the paper:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 'NEFTune是一种提高聊天模型性能的技术，由Jain等人在论文["NEFTune: Noisy Embeddings Improve Instruction
    Finetuning"](https://arxiv.org/abs/2310.05914)中介绍。它包括在训练过程中向嵌入向量添加噪声。根据论文的摘要：'
- en: Standard finetuning of LLaMA-2-7B using Alpaca achieves 29.79% on AlpacaEval,
    which rises to 64.69% using noisy embeddings. NEFTune also improves over strong
    baselines on modern instruction datasets. Models trained with Evol-Instruct see
    a 10% improvement, with ShareGPT an 8% improvement, and with OpenPlatypus an 8%
    improvement. Even powerful models further refined with RLHF such as LLaMA-2-Chat
    benefit from additional training with NEFTune.
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用Alpaca对LLaMA-2-7B进行标准微调，在AlpacaEval上达到29.79%，使用嘈杂的嵌入提高到64.69%。NEFTune还改善了现代指令数据集上的强基线。使用Evol-Instruct训练的模型看到了10%的改进，使用ShareGPT看到了8%的改进，使用OpenPlatypus看到了8%的改进。即使是进一步通过RLHF精细调整的强大模型，如LLaMA-2-Chat，也受益于NEFTune的额外训练。
- en: '![](../Images/52cfc6d8e877350238b9e92fa688af9a.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52cfc6d8e877350238b9e92fa688af9a.png)'
- en: To use it in `SFTTrainer` simply pass `neftune_noise_alpha` when creating your
    `SFTTrainer` instance. Note that to avoid any surprising behaviour, NEFTune is
    disabled after training to retrieve back the original behaviour of the embedding
    layer.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要在`SFTTrainer`中使用它，只需在创建`SFTTrainer`实例时传递`neftune_noise_alpha`。请注意，为了避免任何意外行为，在训练后禁用NEFTune以恢复嵌入层的原始行为。
- en: '[PRE24]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We have tested NEFTune by training `mistralai/Mistral-7B-v0.1` on the [OpenAssistant
    dataset](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) and
    validated that using NEFTune led to a performance boost of ~25% on MT Bench.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在[OpenAssistant数据集](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)上训练`mistralai/Mistral-7B-v0.1`来测试NEFTune，并验证使用NEFTune导致MT
    Bench性能提升约25%。
- en: '![](../Images/2041067e08b1835ad3fb89fb4c7255c5.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2041067e08b1835ad3fb89fb4c7255c5.png)'
- en: Note however, that the amount of performance gain is *dataset dependent* and
    in particular, applying NEFTune on synthetic datasets like [UltraChat](https://huggingface.co/datasets/stingning/ultrachat)
    typically produces smaller gains.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 但请注意，性能增益的数量是*数据集相关*的，特别是在合成数据集（如[UltraChat](https://huggingface.co/datasets/stingning/ultrachat)）上应用
    NEFTune 通常会产生较小的增益。
- en: Accelerate fine-tuning 2x using unsloth
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 unsloth 加速微调 2 倍
- en: 'You can further accelerate QLoRA / LoRA (2x faster, 60% less memory) using
    the [`unsloth`](https://github.com/unslothai/unsloth) library that is fully compatible
    with `SFTTrainer`. Currently `unsloth` supports only Llama (Yi, TinyLlama, Qwen,
    Deepseek etc) and Mistral architectures. Some benchmarks on 1x A100 listed below:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用完全兼容`SFTTrainer`的[`unsloth`](https://github.com/unslothai/unsloth)库进一步加速
    QLoRA / LoRA（速度提高 2 倍，内存减少 60%）。目前，`unsloth`仅支持 Llama（Yi、TinyLlama、Qwen、Deepseek
    等）和 Mistral 架构。以下是 1x A100 的一些基准测试：
- en: '| 1 A100 40GB | Dataset | 🤗 | 🤗 + Flash Attention 2 | 🦥 Unsloth | 🦥 VRAM saved
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 1 A100 40GB | 数据集 | 🤗 | 🤗 + 闪存注意力 2 | 🦥 Unsloth | 🦥 VRAM 已保存 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Code Llama 34b | Slim Orca | 1x | 1.01x | **1.94x** | -22.7% |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Code Llama 34b | 瘦鲸鱼 | 1x | 1.01x | **1.94x** | -22.7% |'
- en: '| Llama-2 7b | Slim Orca | 1x | 0.96x | **1.87x** | -39.3% |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2 7b | 瘦鲸鱼 | 1x | 0.96x | **1.87x** | -39.3% |'
- en: '| Mistral 7b | Slim Orca | 1x | 1.17x | **1.88x** | -65.9% |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Mistral 7b | 瘦鲸鱼 | 1x | 1.17x | **1.88x** | -65.9% |'
- en: '| Tiny Llama 1.1b | Alpaca | 1x | 1.55x | **2.74x** | -57.8% |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Tiny Llama 1.1b | 羊驼 | 1x | 1.55x | **2.74x** | -57.8% |'
- en: 'First install `unsloth` according to the [official documentation](https://github.com/unslothai/unsloth).
    Once installed, you can incorporate unsloth into your workflow in a very simple
    manner; instead of loading `AutoModelForCausalLM`, you just need to load a `FastLanguageModel`
    as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 首先根据[官方文档](https://github.com/unslothai/unsloth)安装`unsloth`。安装完成后，您可以非常简单地将
    unsloth 整合到您的工作流程中；只需加载`FastLanguageModel`，而不是加载`AutoModelForCausalLM`，如下所示：
- en: '[PRE25]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The saved model is fully compatible with Hugging Face’s transformers library.
    Learn more about unsloth in their [official repository](https://github.com/unslothai/unsloth).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 保存的模型与 Hugging Face 的 transformers 库完全兼容。在他们的[官方存储库](https://github.com/unslothai/unsloth)中了解更多关于
    unsloth 的信息。
- en: Best practices
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践
- en: 'Pay attention to the following best practices when training a model with that
    trainer:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用该训练器训练模型时，请注意以下最佳实践：
- en: '[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer) always pads by default
    the sequences to the `max_seq_length` argument of the [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer).
    If none is passed, the trainer will retrieve that value from the tokenizer. Some
    tokenizers do not provide default value, so there is a check to retrieve the minimum
    between 2048 and that value. Make sure to check it before training.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer) 默认情况下总是将序列填充到[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)的`max_seq_length`参数。如果没有传递值，训练器将从分词器中检索该值。一些分词器不提供默认值，因此有一个检查以检索
    2048 和该值之间的最小值。在训练之前，请务必检查。'
- en: For training adapters in 8bit, you might need to tweak the arguments of the
    `prepare_model_for_kbit_training` method from PEFT, hence we advise users to use
    `prepare_in_int8_kwargs` field, or create the `PeftModel` outside the [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)
    and pass it.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于在 8 位中训练适配器，您可能需要调整 PEFT 的`prepare_model_for_kbit_training`方法的参数，因此我们建议用户使用`prepare_in_int8_kwargs`字段，或在[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)之外创建`PeftModel`并传递它。
- en: For a more memory-efficient training using adapters, you can load the base model
    in 8bit, for that simply add `load_in_8bit` argument when creating the [SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer),
    or create a base model in 8bit outside the trainer and pass it.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了更节省内存地使用适配器进行训练，您可以在 8 位中加载基础模型，只需在创建[SFTTrainer](/docs/trl/v0.7.10/en/trainer#trl.SFTTrainer)时添加`load_in_8bit`参数，或在训练器之外创建一个
    8 位的基础模型并传递它。
- en: If you create a model outside the trainer, make sure to not pass to the trainer
    any additional keyword arguments that are relative to `from_pretrained()` method.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您在训练器之外创建模型，请确保不要向训练器传递相对于`from_pretrained()`方法的任何额外关键字参数。
- en: GPTQ Conversion
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPTQ 转换
- en: You may experience some issues with GPTQ Quantization after completing training.
    Lowering `gradient_accumulation_steps` to `4` will resolve most issues during
    the quantization process to GPTQ format.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成训练后，您可能会遇到一些 GPTQ 量化的问题。将`gradient_accumulation_steps`降低到`4`将解决大部分在量化过程中到
    GPTQ 格式的问题。
- en: SFTTrainer
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SFTTrainer
- en: '### `class trl.SFTTrainer`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class trl.SFTTrainer`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/sft_trainer.py#L54)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/sft_trainer.py#L54)'
- en: '[PRE26]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (Union[`transformers.PreTrainedModel`, `nn.Module`, `str`]) — The model
    to train, can be a `PreTrainedModel`, a `torch.nn.Module` or a string with the
    model name to load from cache or download. The model can be also converted to
    a `PeftModel` if a `PeftConfig` object is passed to the `peft_config` argument.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（Union[`transformers.PreTrainedModel`, `nn.Module`, `str`）— 要训练的模型，可以是`PreTrainedModel`，`torch.nn.Module`或包含要从缓存加载或下载的模型名称的字符串。如果将`PeftConfig`对象传递给`peft_config`参数，则模型也可以转换为`PeftModel`。'
- en: '`args` (Optional[transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.36.2/en/main_classes/trainer#transformers.TrainingArguments))
    — The arguments to tweak for training. Please refer to the official documentation
    of `transformers.TrainingArguments` for more information.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`args`（Optional[transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.36.2/en/main_classes/trainer#transformers.TrainingArguments)）—
    用于调整训练的参数。请参考`transformers.TrainingArguments`的官方文档以获取更多信息。'
- en: '`data_collator` (Optional`transformers.DataCollator`) — The data collator to
    use for training.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_collator`（可选`transformers.DataCollator`）— 用于训练的数据整理器。'
- en: '`train_dataset` (Optional[datasets.Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset))
    — The dataset to use for training. We recommend users to use `trl.trainer.ConstantLengthDataset`
    to create their dataset.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_dataset` (Optional[datasets.Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset))
    — 用于训练的数据集。我们建议用户使用`trl.trainer.ConstantLengthDataset`来创建他们的数据集。'
- en: '`eval_dataset` (Optional[Union[`datasets.Dataset`, Dict[`str`, `datasets.Dataset`]]])
    — The dataset to use for evaluation. We recommend users to use `trl.trainer.ConstantLengthDataset`
    to create their dataset.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_dataset` (Optional[Union[`datasets.Dataset`, Dict[`str`, `datasets.Dataset`]]])
    — 用于评估的数据集。我们建议用户使用`trl.trainer.ConstantLengthDataset`来创建他们的数据集。'
- en: '`tokenizer` (Optional[transformers.PreTrainedTokenizer](https://huggingface.co/docs/transformers/v4.36.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer to use for training. If not specified, the tokenizer associated
    to the model will be used.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (Optional[transformers.PreTrainedTokenizer](https://huggingface.co/docs/transformers/v4.36.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — 用于训练的分词器。如果未指定，将使用与模型关联的分词器。'
- en: '`model_init` (`Callable[[], transformers.PreTrainedModel]`) — The model initializer
    to use for training. If None is specified, the default model initializer will
    be used.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_init` (`Callable[[], transformers.PreTrainedModel]`) — 用于训练的模型初始化器。如果指定为None，则将使用默认的模型初始化器。'
- en: '`compute_metrics` (`Callable[[transformers.EvalPrediction], Dict]`, *optional*
    defaults to None) — The function used to compute metrics during evaluation. It
    should return a dictionary mapping metric names to metric values. If not specified,
    only the loss will be computed during evaluation.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compute_metrics` (`Callable[[transformers.EvalPrediction], Dict]`, *optional*
    defaults to None) — 用于在评估过程中计算指标的函数。它应该返回一个将指标名称映射到指标值的字典。如果未指定，评估过程中只会计算损失。'
- en: '`callbacks` (`List[transformers.TrainerCallback]`) — The callbacks to use for
    training.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callbacks` (`List[transformers.TrainerCallback]`) — 用于训练的回调函数。'
- en: '`optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`)
    — The optimizer and scheduler to use for training.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`)
    — 用于训练的优化器和调度器。'
- en: '`preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`)
    — The function to use to preprocess the logits before computing the metrics.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`)
    — 用于在计算指标之前预处理logits的函数。'
- en: '`peft_config` (`Optional[PeftConfig]`) — The PeftConfig object to use to initialize
    the PeftModel.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`peft_config` (`Optional[PeftConfig]`) — 用于初始化PeftModel的PeftConfig对象。'
- en: '`dataset_text_field` (`Optional[str]`) — The name of the text field of the
    dataset, in case this is passed by a user, the trainer will automatically create
    a `ConstantLengthDataset` based on the `dataset_text_field` argument.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_text_field` (`Optional[str]`) — 数据集的文本字段的名称，如果用户传递了这个参数，训练器将自动基于`dataset_text_field`参数创建一个`ConstantLengthDataset`。'
- en: '`formatting_func` (`Optional[Callable]`) — The formatting function to be used
    for creating the `ConstantLengthDataset`.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`formatting_func` (`Optional[Callable]`) — 用于创建`ConstantLengthDataset`的格式化函数。'
- en: '`max_seq_length` (`Optional[int]`) — The maximum sequence length to use for
    the `ConstantLengthDataset` and for automatically creating the Dataset. Defaults
    to `512`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_seq_length` (`Optional[int]`) — 用于`ConstantLengthDataset`和自动创建数据集的最大序列长度。默认为`512`。'
- en: '`infinite` (`Optional[bool]`) — Whether to use an infinite dataset or not.
    Defaults to `False`.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`infinite` (`Optional[bool]`) — 是否使用无限数据集。默认为`False`。'
- en: '`num_of_sequences` (`Optional[int]`) — The number of sequences to use for the
    `ConstantLengthDataset`. Defaults to `1024`.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_of_sequences` (`Optional[int]`) — 用于`ConstantLengthDataset`的序列数。默认为`1024`。'
- en: '`chars_per_token` (`Optional[float]`) — The number of characters per token
    to use for the `ConstantLengthDataset`. Defaults to `3.6`. You can check how this
    is computed in the stack-llama example: [https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53](https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chars_per_token` (`Optional[float]`) — 用于`ConstantLengthDataset`的每个标记的字符数。默认为`3.6`。您可以在stack-llama示例中查看如何计算这个值：[https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53](https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53)。'
- en: '`packing` (`Optional[bool]`) — Used only in case `dataset_text_field` is passed.
    This argument is used by the `ConstantLengthDataset` to pack the sequences of
    the dataset.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`packing` (`Optional[bool]`) — 仅在传递了`dataset_text_field`的情况下使用。这个参数由`ConstantLengthDataset`用于打包数据集的序列。'
- en: '`dataset_num_proc` (`Optional[int]`) — The number of workers to use to tokenize
    the data. Only used when `packing=False`. Defaults to None.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_num_proc` (`Optional[int]`) — 用于标记化数据的工作进程数。仅在`packing=False`时使用。默认为None。'
- en: '`dataset_batch_size` (`int`) — The number of examples to tokenize per batch.
    If batch_size <= 0 or batch_size == None, tokenize the full dataset as a single
    batch. Defaults to 1000.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_batch_size` (`int`) — 每批要标记化的示例数。如果batch_size <= 0或batch_size == None，则将整个数据集标记为单个批次。默认为1000。'
- en: '`neftune_noise_alpha` (`Optional[float]`) — If not `None`, this will activate
    NEFTune noise embeddings. This has been proven to drastically improve model performances
    for instruction fine-tuning. Check out the original paper here: [https://arxiv.org/abs/2310.05914](https://arxiv.org/abs/2310.05914)
    and the original code here: [https://github.com/neelsjain/NEFTune](https://github.com/neelsjain/NEFTune)
    model_init_kwargs — (`Optional[Dict]`, *optional*): Dict of Optional kwargs to
    pass when instantiating the model from a string dataset_kwargs — (`Optional[Dict]`,
    *optional*): Dict of Optional kwargs to pass when creating packed or non-packed
    datasets'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neftune_noise_alpha` (`Optional[float]`) — 如果不是 `None`，则会激活 NEFTune 噪声嵌入。这已被证明极大地提高了指令微调模型的性能。查看原始论文：[https://arxiv.org/abs/2310.05914](https://arxiv.org/abs/2310.05914)
    和原始代码：[https://github.com/neelsjain/NEFTune](https://github.com/neelsjain/NEFTune)
    model_init_kwargs — (`Optional[Dict]`, *optional*): 传递给实例化模型时的可选参数字典 dataset_kwargs
    — (`Optional[Dict]`, *optional*): 创建打包或非打包数据集时传递的可选参数字典'
- en: Class definition of the Supervised Finetuning Trainer (SFT Trainer). This class
    is a wrapper around the `transformers.Trainer` class and inherits all of its attributes
    and methods. The trainer takes care of properly initializing the PeftModel in
    case a user passes a `PeftConfig` object.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 监督微调训练器（SFT Trainer）的类定义。这个类是 `transformers.Trainer` 类的包装器，并继承了它的所有属性和方法。训练器负责在用户传递
    `PeftConfig` 对象时正确初始化 PeftModel。
- en: ConstantLengthDataset
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ConstantLengthDataset
- en: '### `class trl.trainer.ConstantLengthDataset`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class trl.trainer.ConstantLengthDataset`'
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/utils.py#L355)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/utils.py#L355)'
- en: '[PRE27]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tokenizer` (`transformers.PreTrainedTokenizer`) — The processor used for processing
    the data.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`transformers.PreTrainedTokenizer`) — 用于处理数据的处理器。'
- en: '`dataset` (`dataset.Dataset`) — Dataset with text files.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset` (`dataset.Dataset`) — 包含文本文件的数据集。'
- en: '`dataset_text_field` (`str`, **optional**) — Name of the field in the dataset
    that contains the text. Used only if `formatting_func` is `None`.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_text_field` (`str`, **optional**) — 数据集中包含文本的字段名称。仅在 `formatting_func`
    为 `None` 时使用。'
- en: '`formatting_func` (`Callable`, **optional**) — Function that formats the text
    before tokenization. Usually it is recommended to have follows a certain pattern
    such as `"### Question: {question} ### Answer: {answer}"`'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`formatting_func` (`Callable`, **optional**) — 在分词之前格式化文本的函数。通常建议遵循特定模式，如 `"###
    问题：{question} ### 答案：{answer}"`'
- en: '`infinite` (`bool`, *optional*, defaults to `False`) — If True the iterator
    is reset after dataset reaches end else stops.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`infinite` (`bool`, *optional*, defaults to `False`) — 如果为 True，则在数据集到达末尾后重置迭代器，否则停止。'
- en: '`seq_length` (`int`, *optional*, defaults to `1024`) — Length of token sequences
    to return.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seq_length` (`int`, *optional*, defaults to `1024`) — 要返回的令牌序列的长度。'
- en: '`num_of_sequences` (`int`, *optional*, defaults to `1024`) — Number of token
    sequences to keep in buffer.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_of_sequences` (`int`, *optional*, defaults to `1024`) — 在缓冲区中保留的令牌序列数。'
- en: '`chars_per_token` (`int`, *optional*, defaults to `3.6`) — Number of characters
    per token used to estimate number of tokens in text buffer.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chars_per_token` (`int`, *optional*, defaults to `3.6`) — 用于估计文本缓冲区中令牌数量的每个令牌的字符数。'
- en: '`eos_token_id` (`int`, *optional*, defaults to `0`) — Id of the end of sequence
    token if the passed tokenizer does not have an EOS token.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`, *optional*, defaults to `0`) — 如果传递的分词器没有 EOS 标记，则为序列结束标记的
    ID。'
- en: '`shuffle` (‘bool’, *optional*, defaults to True) — Shuffle the examples before
    they are returned'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shuffle` (‘bool’, *optional*, defaults to True) — 在返回示例之前对示例进行洗牌'
- en: '`append_concat_token` (‘bool’, *optional*, defaults to True) — If true, appends
    `eos_token_id` at the end of each sample being packed.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`append_concat_token` (‘bool’, *optional*, defaults to True) — 如果为 True，则在每个被打包的样本末尾附加
    `eos_token_id`。'
- en: '`add_special_tokens` (‘bool’, *optional*, defaults to True) — If true, tokenizers
    adds special tokens to each sample being packed.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (‘bool’, *optional*, defaults to True) — 如果为 True，则分词器会为每个被打包的样本添加特殊标记。'
- en: Iterable dataset that returns constant length chunks of tokens from stream of
    text files. The dataset also formats the text before tokenization with a specific
    format that is provided by the user.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 可迭代数据集，从文本文件流中返回恒定长度的令牌块。数据集还会在分词之前使用用户提供的特定格式对文本进行格式化。
