["```py\n( peft_type: Union = None auto_mapping: Optional = None base_model_name_or_path: Optional = None revision: Optional = None task_type: Union = None inference_mode: bool = False num_virtual_tokens: int = None token_dim: int = None num_transformer_submodules: Optional = None num_attention_heads: Optional = None num_layers: Optional = None encoder_hidden_size: int = None prefix_projection: bool = False )\n```", "```py\n( config )\n```", "```py\n>>> from peft import PrefixEncoder, PrefixTuningConfig\n\n>>> config = PrefixTuningConfig(\n...     peft_type=\"PREFIX_TUNING\",\n...     task_type=\"SEQ_2_SEQ_LM\",\n...     num_virtual_tokens=20,\n...     token_dim=768,\n...     num_transformer_submodules=1,\n...     num_attention_heads=12,\n...     num_layers=12,\n...     encoder_hidden_size=768,\n... )\n>>> prefix_encoder = PrefixEncoder(config)\n```"]