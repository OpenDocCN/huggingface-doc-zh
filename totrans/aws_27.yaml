- en: Optimum Neuron Distributed
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Optimum Neuron Distributed
- en: 'Original text: [https://huggingface.co/docs/optimum-neuron/package_reference/distributed](https://huggingface.co/docs/optimum-neuron/package_reference/distributed)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/optimum-neuron/package_reference/distributed](https://huggingface.co/docs/optimum-neuron/package_reference/distributed)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The `optimum.neuron.distributed` module provides a set of tools to perform distributed
    training and inference.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '`optimum.neuron.distributed`模块提供了一组工具来执行分布式训练和推理。'
- en: Parallelization
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行化
- en: The main task in distributed training / inference is being able to shard things
    such as the model weights, the gradient, and/or the optimizer state. We built
    `Parallelizer` classes to handle the sharding.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式训练/推理中的主要任务是能够分片模型权重、梯度和/或优化器状态等内容。我们构建了`Parallelizer`类来处理分片。
- en: Base Parallelizer
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础并行化器
- en: The `Parallelizer` class is the base abstract class being derived for every
    model supporting model parallelism. It provides methods to parallelize the model
    and save and load sharded checkpoints.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '`Parallelizer`类是为支持模型并行性的每个模型派生的基础抽象类。它提供了并行化模型、保存和加载分片检查点的方法。'
- en: '### `class optimum.neuron.distributed.Parallelizer`'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.distributed.Parallelizer`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L135)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L135)'
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Base abstract class that handles model parallelism.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 处理模型并行性的基础抽象类。
- en: '#### `_parallelize`'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`_parallelize`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L238)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L238)'
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (`PreTrainedModel`) — The model to parallelize.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`PreTrainedModel`) — 要并行化的模型。'
- en: '`device` (`Optional[torch.device]`, defaults to `None`) — The device where
    the new parallel layers should be put.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`Optional[torch.device]`，默认为`None`) — 新并行层应该放置的设备。'
- en: '`parallelize_embeddings` (`bool`, defaults to `True`) — Whether or not the
    embeddings should be parallelized. This can be disabled in the case when the TP
    size does not divide the vocabulary size.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parallelize_embeddings` (`bool`，默认为`True`) — 是否应该并行化嵌入。当TP大小不能整除词汇量大小时，可以禁用此选项。'
- en: '`sequence_parallel_enabled` (`bool`, defaults to `False`) — Whether or not
    sequence parallelism is enabled.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_parallel_enabled` (`bool`，默认为`False`) — 是否启用序列并行性。'
- en: Returns
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`PreTrainedModel`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`PreTrainedModel`'
- en: The parallelized model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化后的模型。
- en: Parallelizes the model by transforming regular layer into their parallel counterparts.
    Each concrete class must implement it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将常规层转换为其并行化对应层来并行化模型。每个具体类必须实现它。
- en: '#### `parallelize`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `parallelize`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L264)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L264)'
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (`PreTrainedModel`) — The model to parallelize.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`PreTrainedModel`) — 要并行化的模型。'
- en: '`device` (`Optional[torch.device]`, defaults to `None`) — The device where
    the new parallel layers should be put.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`Optional[torch.device]`，默认为`None`) — 新并行层应该放置的设备。'
- en: '`parallelize_embeddings` (`bool`, defaults to `True`) — Whether or not the
    embeddings should be parallelized. This can be disabled in the case when the TP
    size does not divide the vocabulary size.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parallelize_embeddings` (`bool`，默认为`True`) — 是否应该并行化嵌入。当TP大小不能整除词汇量大小时，可以禁用此选项。'
- en: '`sequence_parallel_enabled` (`bool`, defaults to `False`) — Whether or not
    sequence parallelism is enabled.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_parallel_enabled` (`bool`，默认为`False`) — 是否启用序列并行性。'
- en: '`pipeline_parallel_num_microbatches` (`int`, defaults to 1) — The number of
    microbatches used for pipeline execution.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline_parallel_num_microbatches` (`int`，默认为1) — 用于管道执行的微批次数。'
- en: '`pipeline_parallel_use_zero1_optimizer` (`bool`, defaults to `False`) — When
    zero-1 optimizer is used, set this to True, so the PP model will understand that
    zero-1 optimizer will handle data parallel gradient averaging.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline_parallel_use_zero1_optimizer` (`bool`，默认为`False`) — 当使用zero-1优化器时，将此选项设置为True，这样PP模型将了解zero-1优化器将处理数据并行梯度平均。'
- en: '`checkpoint_dir` (`Optional[Union[str, Path]]`) — Path to a sharded checkpoint.
    If specified, the checkpoint weights will be loaded to the parallelized model.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`checkpoint_dir` (`Optional[Union[str, Path]]`) — 分片检查点的路径。如果指定，检查点权重将被加载到并行化模型中。'
- en: Returns
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`PreTrainedModel`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`PreTrainedModel`'
- en: The parallelized model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化后的模型。
- en: Parallelizes the model by transforming regular layer into their parallel counterparts
    using `cls._parallelize()`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`cls._parallelize()`将常规层转换为其并行化对应层来并行化模型。
- en: It also makes sure that each parameter has loaded its weights or has been initialized
    if there is no pre-trained weights associated to it.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 它还确保每个参数已加载其权重或者如果没有与之关联的预训练权重，则已初始化。
- en: '#### `optimizer_for_mp`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `optimizer_for_mp`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L610)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L610)'
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`optimizer` (`torch.optim.Optimizer`) — The original optimizer.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer` (`torch.optim.Optimizer`) — 原始优化器。'
- en: '`orig_param_to_parallel_param_on_xla` (`Mapping[int, torch.nn.Parameter]`)
    — A mapping (e.g. dict-like) that maps the id of a parameter in `optimizer` to
    the id of its parallelized counterpart on an XLA device.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`orig_param_to_parallel_param_on_xla` (`Mapping[int, torch.nn.Parameter]`)
    — 将`optimizer`中参数的ID映射到其在XLA设备上的并行化对应项的映射（例如类似于字典）。'
- en: Returns
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.optim.Optimizer`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.optim.Optimizer`'
- en: The tensor parallelism ready optimizer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 张量并行准备好的优化器。
- en: Creates an optimizer ready for a parallelized model from an existing optimizer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从现有的优化器创建一个为并行化模型准备好的优化器。
- en: 'There are two cases:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种情况：
- en: The optimizer has been created via a lazy constructor from `optimum.neuron.distributed.utils.make_optimizer_constructor_lazy`,
    it which case the exactly intended optimizer is created for tensor parallelism.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化器是通过`optimum.neuron.distributed.utils.make_optimizer_constructor_lazy`的延迟构造函数创建的，这种情况下，为张量并行性创建了确切的优化器。
- en: The optimizer was created with a regular constructor. In this case the optimizer
    for tensor parallelism is created as close as possible to what was intended but
    that is not guaranteed.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用常规构造函数创建了优化器。在这种情况下，为张量并行性创建的优化器尽可能接近预期，但不能保证完全一致。
- en: '#### `save_model_checkpoint`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_model_checkpoint`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L757)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L757)'
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#### `load_model_checkpoint`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_model_checkpoint`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L789)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L789)'
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Selecting Model-Specific Parallelizer Classes
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择特定于模型的并行器类
- en: Each model that supports parallelization in `optimum-neuron` has its own derived
    `Parallelizer` class. The factory class `ParallelizersManager` allows you to retrieve
    such model-specific `Parallelizer`s easily.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`optimum-neuron` 中支持并行化的每个模型都有自己派生的 `Parallelizer` 类。工厂类 `ParallelizersManager`
    允许您轻松检索这些特定于模型的 `Parallelizer`。'
- en: '### `class optimum.neuron.distributed.ParallelizersManager`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class optimum.neuron.distributed.ParallelizersManager`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/parallelizers_manager.py#L52)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/parallelizers_manager.py#L52)'
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#### `get_supported_model_types`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_supported_model_types`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/parallelizers_manager.py#L65)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/parallelizers_manager.py#L65)'
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Provides the list of supported model types for parallelization.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 提供支持并行化的模型类型列表。
- en: '#### `is_model_supported`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `is_model_supported`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/parallelizers_manager.py#L85)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/parallelizers_manager.py#L85)'
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model_type_or_model` (`Union[str, PreTrainedModel]`) — Either the model type
    or an instance of the model.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_type_or_model` (`Union[str, PreTrainedModel]`) — 模型类型或模型实例。'
- en: Returns `True` if the model can be parallelized, `False` otherwise.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型可以并行化，则返回 `True`，否则返回 `False`。
- en: '#### `parallelizer_for_model`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `parallelizer_for_model`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/parallelizers_manager.py#L97)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/parallelizers_manager.py#L97)'
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model_type_or_model` (`Union[str, PreTrainedModel]`) — Either the model type
    or an instance of the model.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_type_or_model` (`Union[str, PreTrainedModel]`) — 模型类型或模型实例。'
- en: Returns the parallelizer class associated to the model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 返回与模型关联的并行器类。
- en: Utils
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具
- en: Lazy Loading
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 延迟加载
- en: Distributed training / inference is usually needed when the model is too big
    to fit in one device. Tools that allow for lazy loading of model weights and optimizer
    states are thus needed to avoid going out-of-memory before parallelization.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型太大无法适应一个设备时，通常需要分布式训练/推理。因此，需要能够延迟加载模型权重和优化器状态的工具，以避免在并行化之前出现内存不足。
- en: '#### `optimum.neuron.distributed.lazy_load_for_parallelism`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `optimum.neuron.distributed.lazy_load_for_parallelism`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/utils.py#L789)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/utils.py#L789)'
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`tensor_parallel_size` (`int`, defaults to 1) — The tensor parallel size considered.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor_parallel_size` (`int`，默认为1）— 考虑的张量并行大小。'
- en: '`pipeline_parallel_size` (`int`, defaults to 1) — The pipeline parallel size
    considered.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline_parallel_size` (`int`，默认为1）— 考虑的管道并行大小。'
- en: 'Context manager that makes the loading of a model lazy for model parallelism:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文管理器，使模型加载变为延迟加载，用于模型并行性：
- en: Every `torch.nn.Linear` is put on the `torch.device("meta")` device, meaning
    that it takes no memory to instantiate.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 `torch.nn.Linear` 都放在 `torch.device("meta")` 设备上，这意味着实例化时不占用内存。
- en: Every `torch.nn.Embedding` is also put on the `torch.device("meta")` device.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 `torch.nn.Embedding` 也放在 `torch.device("meta")` 设备上。
- en: No state dict is actually loaded, instead a weight map is created and attached
    to the model. For more information, read the `optimum.neuron.distributed.utils.from_pretrained_for_mp`
    docstring.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际上并未加载状态字典，而是创建了一个权重映射并附加到模型上。有关更多信息，请阅读 `optimum.neuron.distributed.utils.from_pretrained_for_mp`
    的文档字符串。
- en: If both `tensor_parallel_size` and `pipeline_parallel_size` are set to 1, no
    lazy loading is performed.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `tensor_parallel_size` 和 `pipeline_parallel_size` 都设置为1，则不执行延迟加载。
- en: '#### `optimum.neuron.distributed.make_optimizer_constructor_lazy`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`optimum.neuron.distributed.make_optimizer_constructor_lazy`'
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/utils.py#L835)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/utils.py#L835)'
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Transforms an optimizer constructor (optimizer class) to make it lazy by not
    initializing the parameters. This makes the optimizer lightweight and usable to
    create a “real” optimizer once the model has been parallelized.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 将优化器构造函数（优化器类）转换为延迟加载，不初始化参数。这使得优化器轻量且可用于在模型并行化后创建“真实”优化器。
