- en: Optimum Neuron Distributed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/optimum-neuron/package_reference/distributed](https://huggingface.co/docs/optimum-neuron/package_reference/distributed)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/optimum.neuron/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/entry/start.abfe5599.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/scheduler.9039eef2.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/singletons.9144bb03.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/paths.e169ac99.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/entry/app.df8ec0a0.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/index.cdcc3d35.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/nodes/0.a52c6f40.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/nodes/15.c00b1ac1.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/Docstring.fff63cfc.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/Heading.96ce3702.js">
  prefs: []
  type: TYPE_NORMAL
- en: The `optimum.neuron.distributed` module provides a set of tools to perform distributed
    training and inference.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main task in distributed training / inference is being able to shard things
    such as the model weights, the gradient, and/or the optimizer state. We built
    `Parallelizer` classes to handle the sharding.
  prefs: []
  type: TYPE_NORMAL
- en: Base Parallelizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `Parallelizer` class is the base abstract class being derived for every
    model supporting model parallelism. It provides methods to parallelize the model
    and save and load sharded checkpoints.
  prefs: []
  type: TYPE_NORMAL
- en: '### class optimum.neuron.distributed.Parallelizer'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L135)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Base abstract class that handles model parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: '#### _parallelize'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L238)'
  prefs: []
  type: TYPE_NORMAL
- en: '( model: PreTrainedModel device: Optional = None parallelize_embeddings: bool
    = True sequence_parallel_enabled: bool = False ) → `PreTrainedModel`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**model** (`PreTrainedModel`) — The model to parallelize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (`Optional[torch.device]`, defaults to `None`) — The device where
    the new parallel layers should be put.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**parallelize_embeddings** (`bool`, defaults to `True`) — Whether or not the
    embeddings should be parallelized. This can be disabled in the case when the TP
    size does not divide the vocabulary size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sequence_parallel_enabled** (`bool`, defaults to `False`) — Whether or not
    sequence parallelism is enabled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`PreTrainedModel`'
  prefs: []
  type: TYPE_NORMAL
- en: The parallelized model.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizes the model by transforming regular layer into their parallel counterparts.
    Each concrete class must implement it.
  prefs: []
  type: TYPE_NORMAL
- en: '#### parallelize'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L264)'
  prefs: []
  type: TYPE_NORMAL
- en: '( model: PreTrainedModel device: Optional = None parallelize_embeddings: bool
    = True sequence_parallel_enabled: bool = False pipeline_parallel_input_names:
    Union = None pipeline_parallel_num_microbatches: int = 1 pipeline_parallel_use_zero1_optimizer:
    bool = False checkpoint_dir: Union = None ) → `PreTrainedModel`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**model** (`PreTrainedModel`) — The model to parallelize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (`Optional[torch.device]`, defaults to `None`) — The device where
    the new parallel layers should be put.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**parallelize_embeddings** (`bool`, defaults to `True`) — Whether or not the
    embeddings should be parallelized. This can be disabled in the case when the TP
    size does not divide the vocabulary size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sequence_parallel_enabled** (`bool`, defaults to `False`) — Whether or not
    sequence parallelism is enabled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pipeline_parallel_num_microbatches** (`int`, defaults to 1) — The number
    of microbatches used for pipeline execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pipeline_parallel_use_zero1_optimizer** (`bool`, defaults to `False`) — When
    zero-1 optimizer is used, set this to True, so the PP model will understand that
    zero-1 optimizer will handle data parallel gradient averaging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**checkpoint_dir** (`Optional[Union[str, Path]]`) — Path to a sharded checkpoint.
    If specified, the checkpoint weights will be loaded to the parallelized model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`PreTrainedModel`'
  prefs: []
  type: TYPE_NORMAL
- en: The parallelized model.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizes the model by transforming regular layer into their parallel counterparts
    using `cls._parallelize()`.
  prefs: []
  type: TYPE_NORMAL
- en: It also makes sure that each parameter has loaded its weights or has been initialized
    if there is no pre-trained weights associated to it.
  prefs: []
  type: TYPE_NORMAL
- en: '#### optimizer_for_mp'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L610)'
  prefs: []
  type: TYPE_NORMAL
- en: '( optimizer: torch.optim.Optimizer orig_param_to_parallel_param_on_xla: Mapping
    ) → `torch.optim.Optimizer`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**optimizer** (`torch.optim.Optimizer`) — The original optimizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**orig_param_to_parallel_param_on_xla** (`Mapping[int, torch.nn.Parameter]`)
    — A mapping (e.g. dict-like) that maps the id of a parameter in `optimizer` to
    the id of its parallelized counterpart on an XLA device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.optim.Optimizer`'
  prefs: []
  type: TYPE_NORMAL
- en: The tensor parallelism ready optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: Creates an optimizer ready for a parallelized model from an existing optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two cases:'
  prefs: []
  type: TYPE_NORMAL
- en: The optimizer has been created via a lazy constructor from `optimum.neuron.distributed.utils.make_optimizer_constructor_lazy`,
    it which case the exactly intended optimizer is created for tensor parallelism.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The optimizer was created with a regular constructor. In this case the optimizer
    for tensor parallelism is created as close as possible to what was intended but
    that is not guaranteed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '#### save_model_checkpoint'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L757)'
  prefs: []
  type: TYPE_NORMAL
- en: '( model: PreTrainedModel output_dir: Union as_regular: bool = False as_sharded:
    bool = True optimizer: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: '#### load_model_checkpoint'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/base.py#L789)'
  prefs: []
  type: TYPE_NORMAL
- en: '( model: PreTrainedModel load_dir: Union )'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting Model-Specific Parallelizer Classes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each model that supports parallelization in `optimum-neuron` has its own derived
    `Parallelizer` class. The factory class `ParallelizersManager` allows you to retrieve
    such model-specific `Parallelizer`s easily.
  prefs: []
  type: TYPE_NORMAL
- en: '### class optimum.neuron.distributed.ParallelizersManager'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/parallelizers_manager.py#L52)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: '#### get_supported_model_types'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/parallelizers_manager.py#L65)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Provides the list of supported model types for parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: '#### is_model_supported'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/parallelizers_manager.py#L85)'
  prefs: []
  type: TYPE_NORMAL
- en: '( model_type_or_model: Union )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**model_type_or_model** (`Union[str, PreTrainedModel]`) — Either the model
    type or an instance of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns `True` if the model can be parallelized, `False` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: '#### parallelizer_for_model'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/parallelizers_manager.py#L97)'
  prefs: []
  type: TYPE_NORMAL
- en: '( model_type_or_model: Union )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**model_type_or_model** (`Union[str, PreTrainedModel]`) — Either the model
    type or an instance of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns the parallelizer class associated to the model.
  prefs: []
  type: TYPE_NORMAL
- en: Utils
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lazy Loading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Distributed training / inference is usually needed when the model is too big
    to fit in one device. Tools that allow for lazy loading of model weights and optimizer
    states are thus needed to avoid going out-of-memory before parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: '#### optimum.neuron.distributed.lazy_load_for_parallelism'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/utils.py#L789)'
  prefs: []
  type: TYPE_NORMAL
- en: '( tensor_parallel_size: int = 1 pipeline_parallel_size: int = 1 )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor_parallel_size** (`int`, defaults to 1) — The tensor parallel size
    considered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pipeline_parallel_size** (`int`, defaults to 1) — The pipeline parallel size
    considered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Context manager that makes the loading of a model lazy for model parallelism:'
  prefs: []
  type: TYPE_NORMAL
- en: Every `torch.nn.Linear` is put on the `torch.device("meta")` device, meaning
    that it takes no memory to instantiate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every `torch.nn.Embedding` is also put on the `torch.device("meta")` device.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No state dict is actually loaded, instead a weight map is created and attached
    to the model. For more information, read the `optimum.neuron.distributed.utils.from_pretrained_for_mp`
    docstring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If both `tensor_parallel_size` and `pipeline_parallel_size` are set to 1, no
    lazy loading is performed.
  prefs: []
  type: TYPE_NORMAL
- en: '#### optimum.neuron.distributed.make_optimizer_constructor_lazy'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/optimum-neuron/blob/main/optimum/neuron/distributed/utils.py#L835)'
  prefs: []
  type: TYPE_NORMAL
- en: '( optimizer_cls: Type )'
  prefs: []
  type: TYPE_NORMAL
- en: Transforms an optimizer constructor (optimizer class) to make it lazy by not
    initializing the parameters. This makes the optimizer lightweight and usable to
    create a “real” optimizer once the model has been parallelized.
  prefs: []
  type: TYPE_NORMAL
