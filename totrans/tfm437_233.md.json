["```py\nimport torch\nfrom transformers import AutoTokenizer, RwkvConfig, RwkvModel\n\nmodel = RwkvModel.from_pretrained(\"sgugger/rwkv-430M-pile\")\ntokenizer = AutoTokenizer.from_pretrained(\"sgugger/rwkv-430M-pile\")\n\ninputs = tokenizer(\"This is an example.\", return_tensors=\"pt\")\n# Feed everything to the model\noutputs = model(inputs[\"input_ids\"])\noutput_whole = outputs.last_hidden_state\n\noutputs = model(inputs[\"input_ids\"][:, :2])\noutput_one = outputs.last_hidden_state\n\n# Using the state computed on the first inputs, we will get the same output\noutputs = model(inputs[\"input_ids\"][:, 2:], state=outputs.state)\noutput_two = outputs.last_hidden_state\n\ntorch.allclose(torch.cat([output_one, output_two], dim=1), output_whole, atol=1e-5)\n```", "```py\nfrom transformers import StoppingCriteria\n\nclass RwkvStoppingCriteria(StoppingCriteria):\n    def __init__(self, eos_sequence = [187,187], eos_token_id = 537):\n        self.eos_sequence = eos_sequence\n        self.eos_token_id = eos_token_id\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        last_2_ids = input_ids[:,-2:].tolist()\n        return self.eos_sequence in last_2_ids\n\noutput = model.generate(inputs[\"input_ids\"], max_new_tokens=64, stopping_criteria = [RwkvStoppingCriteria()])\n```", "```py\n( vocab_size = 50277 context_length = 1024 hidden_size = 4096 num_hidden_layers = 32 attention_hidden_size = None intermediate_size = None layer_norm_epsilon = 1e-05 bos_token_id = 0 eos_token_id = 0 rescale_every = 6 tie_word_embeddings = False use_cache = True **kwargs )\n```", "```py\n>>> from transformers import RwkvConfig, RwkvModel\n\n>>> # Initializing a Rwkv configuration\n>>> configuration = RwkvConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = RwkvModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None inputs_embeds: Optional = None state: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.rwkv.modeling_rwkv.RwkvOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, RwkvModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-169m-pile\")\n>>> model = RwkvModel.from_pretrained(\"RWKV/rwkv-4-169m-pile\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None inputs_embeds: Optional = None state: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.rwkv.modeling_rwkv.RwkvCausalLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, RwkvForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"RWKV/rwkv-4-169m-pile\")\n>>> model = RwkvForCausalLM.from_pretrained(\"RWKV/rwkv-4-169m-pile\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```"]