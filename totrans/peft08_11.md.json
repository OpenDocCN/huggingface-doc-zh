["```py\nfrom datasets import load_dataset\n\nds = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\nds = ds[\"train\"].train_test_split(test_size=0.1)\nds[\"validation\"] = ds[\"test\"]\ndel ds[\"test\"]\n\nclasses = ds[\"train\"].features[\"label\"].names\nds = ds.map(\n    lambda x: {\"text_label\": [classes[label] for label in x[\"label\"]]},\n    batched=True,\n    num_proc=1,\n)\n\nds[\"train\"][0]\n{'sentence': 'It will be operated by Nokia , and supported by its Nokia NetAct network and service management system .',\n 'label': 1,\n 'text_label': 'neutral'}\n```", "```py\nfrom transformers import AutoTokenizer\n\ntext_column = \"sentence\"\nlabel_column = \"text_label\"\nmax_length = 128\n\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/mt0-large\")\n\ndef preprocess_function(examples):\n    inputs = examples[text_column]\n    targets = examples[label_column]\n    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    labels = tokenizer(targets, max_length=3, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    labels = labels[\"input_ids\"]\n    labels[labels == tokenizer.pad_token_id] = -100\n    model_inputs[\"labels\"] = labels\n    return model_inputs\n```", "```py\nprocessed_ds = ds.map(\n    preprocess_function,\n    batched=True,\n    num_proc=1,\n    remove_columns=ds[\"train\"].column_names,\n    load_from_cache_file=False,\n    desc=\"Running tokenizer on dataset\",\n)\n```", "```py\nfrom torch.utils.data import DataLoader\nfrom transformers import default_data_collator\n\ntrain_ds = processed_ds[\"train\"]\neval_ds = processed_ds[\"validation\"]\n\nbatch_size = 8\n\ntrain_dataloader = DataLoader(\n    train_ds, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n)\neval_dataloader = DataLoader(eval_ds, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n```", "```py\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/mt0-large\")\n```", "```py\nfrom peft import IA3Config, get_peft_model\n\npeft_config = IA3Config(task_type=\"SEQ_2_SEQ_LM\")\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\"trainable params: 282,624 || all params: 1,229,863,936 || trainable%: 0.022980103060766553\"\n```", "```py\nimport torch\nfrom transformers import get_linear_schedule_with_warmup\n\nlr = 8e-3\nnum_epochs = 3\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=(len(train_dataloader) * num_epochs),\n)\n```", "```py\nfrom tqdm import tqdm\n\ndevice = \"cuda\"\nmodel = model.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_loss += loss.detach().float()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n    model.eval()\n    eval_loss = 0\n    eval_preds = []\n    for step, batch in enumerate(tqdm(eval_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n        loss = outputs.loss\n        eval_loss += loss.detach().float()\n        eval_preds.extend(\n            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n        )\n\n    eval_epoch_loss = eval_loss / len(eval_dataloader)\n    eval_ppl = torch.exp(eval_epoch_loss)\n    train_epoch_loss = total_loss / len(train_dataloader)\n    train_ppl = torch.exp(train_epoch_loss)\n    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n```", "```py\nfrom huggingface_hub import notebook_login\n\naccount = <your-hf-account-name>\npeft_model_id = f\"{account}/mt0-large-ia3\"\nmodel.push_to_hub(peft_model_id)\n```", "```py\nfrom peft import AutoPeftModelForSeq2SeqLM\n\nmodel = AutoPeftModelForSeq2SeqLM.from_pretrained(\"<your-hf-account-name>/mt0-large-ia3\").to(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/mt0-large\")\n\ni = 15\ninputs = tokenizer(ds[\"validation\"][text_column][i], return_tensors=\"pt\")\nprint(ds[\"validation\"][text_column][i])\n\"The robust growth was the result of the inclusion of clothing chain Lindex in the Group in December 2007 .\"\n```", "```py\nwith torch.no_grad():\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\n['positive']\n```"]