- en: SpeechT5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speecht5](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speecht5)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/250.6a043940.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The SpeechT5 model was proposed in [SpeechT5: Unified-Modal Encoder-Decoder
    Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205)
    by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom
    Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained
    natural language processing models, we propose a unified-modal SpeechT5 framework
    that explores the encoder-decoder pre-training for self-supervised speech/text
    representation learning. The SpeechT5 framework consists of a shared encoder-decoder
    network and six modal-specific (speech/text) pre/post-nets. After preprocessing
    the input speech/text through the pre-nets, the shared encoder-decoder network
    models the sequence-to-sequence transformation, and then the post-nets generate
    the output in the speech/text modality based on the output of the decoder. Leveraging
    large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal
    representation, hoping to improve the modeling capability for both speech and
    text. To align the textual and speech information into this unified semantic space,
    we propose a cross-modal vector quantization approach that randomly mixes up speech/text
    states with latent units as the interface between encoder and decoder. Extensive
    evaluations show the superiority of the proposed SpeechT5 framework on a wide
    variety of spoken language processing tasks, including automatic speech recognition,
    speech synthesis, speech translation, voice conversion, speech enhancement, and
    speaker identification.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [Matthijs](https://huggingface.co/Matthijs). The
    original code can be found [here](https://github.com/microsoft/SpeechT5).
  prefs: []
  type: TYPE_NORMAL
- en: SpeechT5Config
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SpeechT5Config`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/configuration_speecht5.py#L37)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 81) — Vocabulary size of the SpeechT5
    model. Defines the number of different tokens that can be represented by the `inputs_ids`
    passed to the forward method of [SpeechT5Model](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_layers` (`int`, *optional*, defaults to 12) — Number of hidden layers
    in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_ffn_dim` (`int`, *optional*, defaults to 3072) — Dimensionality of
    the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.1) — The LayerDrop
    probability for the encoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_layers` (`int`, *optional*, defaults to 6) — Number of hidden layers
    in the Transformer decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_ffn_dim` (`int`, *optional*, defaults to 3072) — Dimensionality of
    the “intermediate” (often named feed-forward) layer in the Transformer decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.1) — The LayerDrop
    probability for the decoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`positional_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the text position encoding layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for activations inside the fully connected layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-5) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scale_embedding` (`bool`, *optional*, defaults to `False`) — Scale embeddings
    by diving by sqrt(d_model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feat_extract_norm` (`str`, *optional*, defaults to `"group"`) — The norm to
    be applied to 1D convolutional layers in the speech encoder pre-net. One of `"group"`
    for group normalization of only the first 1D convolutional layer or `"layer"`
    for layer normalization of all 1D convolutional layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feat_proj_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for output of the speech encoder pre-net.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feat_extract_activation` (`str,` optional`, defaults to` “gelu”`) -- The non-linear
    activation function (function or string) in the 1D convolutional layers of the
    feature extractor. If string,` “gelu”`,` “relu”`,` “selu”`and`“gelu_new”` are
    supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 512, 512, 512)`) — A tuple of integers defining the number of input
    and output channels of each 1D convolutional layer in the speech encoder pre-net.
    The length of *conv_dim* defines the number of 1D convolutional layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2,
    2, 2, 2, 2, 2)`) — A tuple of integers defining the stride of each 1D convolutional
    layer in the speech encoder pre-net. The length of *conv_stride* defines the number
    of convolutional layers and has to match the length of *conv_dim*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3,
    3, 3, 3, 3, 3)`) — A tuple of integers defining the kernel size of each 1D convolutional
    layer in the speech encoder pre-net. The length of *conv_kernel* defines the number
    of convolutional layers and has to match the length of *conv_dim*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conv_bias` (`bool`, *optional*, defaults to `False`) — Whether the 1D convolutional
    layers have a bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — Number of
    convolutional positional embeddings. Defines the kernel size of 1D convolutional
    positional embeddings layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — Number
    of groups of 1D convolutional positional embeddings layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`apply_spec_augment` (`bool`, *optional*, defaults to `True`) — Whether to
    apply *SpecAugment* data augmentation to the outputs of the speech encoder pre-net.
    For reference see [SpecAugment: A Simple Data Augmentation Method for Automatic
    Speech Recognition](https://arxiv.org/abs/1904.08779).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_time_prob` (`float`, *optional*, defaults to 0.05) — Percentage (between
    0 and 1) of all feature vectors along the time axis which will be masked. The
    masking procecure generates ”mask_time_prob*len(time_axis)/mask_time_length” independent
    masks over the axis. If reasoning from the propability of each feature vector
    to be chosen as the start of the vector span to be masked,* mask_time_prob *should
    be `prob_vector_start*mask_time_length`. Note that overlap may decrease the actual
    percentage of masked vectors. This is only relevant if` apply_spec_augment is
    True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_time_length` (`int`, *optional*, defaults to 10) — Length of vector span
    along the time axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_time_min_masks` (`int`, *optional*, defaults to 2), — The minimum number
    of masks of length `mask_feature_length` generated along the time axis, each time
    step, irrespectively of `mask_feature_prob`. Only relevant if ”mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masks”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_feature_prob` (`float`, *optional*, defaults to 0.0) — Percentage (between
    0 and 1) of all feature vectors along the feature axis which will be masked. The
    masking procecure generates ”mask_feature_prob*len(feature_axis)/mask_time_length”
    independent masks over the axis. If reasoning from the propability of each feature
    vector to be chosen as the start of the vector span to be masked,* mask_feature_prob
    *should be `prob_vector_start*mask_feature_length`. Note that overlap may decrease
    the actual percentage of masked vectors. This is only relevant if` apply_spec_augment
    is True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_feature_length` (`int`, *optional*, defaults to 10) — Length of vector
    span along the feature axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_feature_min_masks` (`int`, *optional*, defaults to 0), — The minimum
    number of masks of length `mask_feature_length` generated along the feature axis,
    each time step, irrespectively of `mask_feature_prob`. Only relevant if ”mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masks”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_mel_bins` (`int`, *optional*, defaults to 80) — Number of mel features
    used per input features. Used by the speech decoder pre-net. Should correspond
    to the value used in the [SpeechT5Processor](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Processor)
    class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_decoder_prenet_layers` (`int`, *optional*, defaults to 2) — Number
    of layers in the speech decoder pre-net.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_decoder_prenet_units` (`int`, *optional*, defaults to 256) — Dimensionality
    of the layers in the speech decoder pre-net.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_decoder_prenet_dropout` (`float`, *optional*, defaults to 0.5) — The
    dropout probability for the speech decoder pre-net layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speaker_embedding_dim` (`int`, *optional*, defaults to 512) — Dimensionality
    of the *XVector* embedding vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_decoder_postnet_layers` (`int`, *optional*, defaults to 5) — Number
    of layers in the speech decoder post-net.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_decoder_postnet_units` (`int`, *optional*, defaults to 256) — Dimensionality
    of the layers in the speech decoder post-net.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_decoder_postnet_kernel` (`int`, *optional*, defaults to 5) — Number
    of convolutional filter channels in the speech decoder post-net.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_decoder_postnet_dropout` (`float`, *optional*, defaults to 0.5) — The
    dropout probability for the speech decoder post-net layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduction_factor` (`int`, *optional*, defaults to 2) — Spectrogram length
    reduction factor for the speech decoder inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_speech_positions` (`int`, *optional*, defaults to 4000) — The maximum
    sequence length of speech features that this model might ever be used with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_text_positions` (`int`, *optional*, defaults to 450) — The maximum sequence
    length of text features that this model might ever be used with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_max_relative_position` (`int`, *optional*, defaults to 160) — Maximum
    distance for relative position embedding in the encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_guided_attention_loss` (`bool`, *optional*, defaults to `True`) — Whether
    to apply guided attention loss while training the TTS model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guided_attention_loss_num_heads` (`int`, *optional*, defaults to 2) — Number
    of attention heads the guided attention loss will be applied to. Use -1 to apply
    this loss to all attention heads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guided_attention_loss_sigma` (`float`, *optional*, defaults to 0.4) — Standard
    deviation for guided attention loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guided_attention_loss_scale` (`float`, *optional*, defaults to 10.0) — Scaling
    coefficient for guided attention loss (also known as lambda).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [SpeechT5Model](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Model).
    It is used to instantiate a SpeechT5 model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the SpeechT5 [microsoft/speecht5_asr](https://huggingface.co/microsoft/speecht5_asr)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: SpeechT5HifiGanConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SpeechT5HifiGanConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/configuration_speecht5.py#L350)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model_in_dim` (`int`, *optional*, defaults to 80) — The number of frequency
    bins in the input log-mel spectrogram.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*, defaults to 16000) — The sampling rate
    at which the output audio will be generated, expressed in hertz (Hz).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`upsample_initial_channel` (`int`, *optional*, defaults to 512) — The number
    of input channels into the upsampling network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`upsample_rates` (`Tuple[int]` or `List[int]`, *optional*, defaults to `[4,
    4, 4, 4]`) — A tuple of integers defining the stride of each 1D convolutional
    layer in the upsampling network. The length of *upsample_rates* defines the number
    of convolutional layers and has to match the length of *upsample_kernel_sizes*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`upsample_kernel_sizes` (`Tuple[int]` or `List[int]`, *optional*, defaults
    to `[8, 8, 8, 8]`) — A tuple of integers defining the kernel size of each 1D convolutional
    layer in the upsampling network. The length of *upsample_kernel_sizes* defines
    the number of convolutional layers and has to match the length of *upsample_rates*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resblock_kernel_sizes` (`Tuple[int]` or `List[int]`, *optional*, defaults
    to `[3, 7, 11]`) — A tuple of integers defining the kernel sizes of the 1D convolutional
    layers in the multi-receptive field fusion (MRF) module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resblock_dilation_sizes` (`Tuple[Tuple[int]]` or `List[List[int]]`, *optional*,
    defaults to `[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`) — A nested tuple of integers
    defining the dilation rates of the dilated 1D convolutional layers in the multi-receptive
    field fusion (MRF) module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.01) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`leaky_relu_slope` (`float`, *optional*, defaults to 0.1) — The angle of the
    negative slope used by the leaky ReLU activation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalize_before` (`bool`, *optional*, defaults to `True`) — Whether or not
    to normalize the spectrogram before vocoding using the vocoder’s learned mean
    and variance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a `SpeechT5HifiGanModel`.
    It is used to instantiate a SpeechT5 HiFi-GAN vocoder model according to the specified
    arguments, defining the model architecture. Instantiating a configuration with
    the defaults will yield a similar configuration to that of the SpeechT5 [microsoft/speecht5_hifigan](https://huggingface.co/microsoft/speecht5_hifigan)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: SpeechT5Tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SpeechT5Tokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/tokenization_speecht5.py#L48)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`) — [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a *.spm* extension) that contains the vocabulary necessary
    to instantiate a tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The begin of sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalize` (`bool`, *optional*, defaults to `False`) — Whether to convert
    numeric quantities in the text to their spelt-out english counterparts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sp_model_kwargs` (`dict`, *optional*) — Will be passed to the `SentencePieceProcessor.__init__()`
    method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python)
    can be used, among other things, to set:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enable_sampling`: Enable subword regularization.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size = {0,1}`: No sampling is performed.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size > 1`: samples from the nbest_size results.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size < 0`: assuming that nbest_size is infinite and samples from the
    all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha`: Smoothing parameter for unigram sampling, and dropout probability
    of merge operations for BPE-dropout.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sp_model` (`SentencePieceProcessor`) — The *SentencePiece* processor that
    is used for every conversion (string, tokens and IDs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a SpeechT5 tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece).
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence or
    batch of sequences to be encoded. Each sequence can be a string or a list of strings
    (pretokenized string). If the sequences are provided as list of strings (pretokenized),
    you must set `is_split_into_words=True` (to lift the ambiguity with a batch of
    sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded. Each sequence can be a string or a list of
    strings (pretokenized string). If the sequences are provided as list of strings
    (pretokenized), you must set `is_split_into_words=True` (to lift the ambiguity
    with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences to be encoded as target texts. Each sequence can be a string
    or a list of strings (pretokenized string). If the sequences are provided as list
    of strings (pretokenized), you must set `is_split_into_words=True` (to lift the
    ambiguity with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — The
    sequence or batch of sequences to be encoded as target texts. Each sequence can
    be a string or a list of strings (pretokenized string). If the sequences are provided
    as list of strings (pretokenized), you must set `is_split_into_words=True` (to
    lift the ambiguity with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_token_type_ids` (`bool`, *optional*) — Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizer’s default, defined by the `return_outputs` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizer’s default, defined by the `return_outputs` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return special tokens mask information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return `(char_start, char_end)` for each token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Python’s tokenizer, this method will raise `NotImplementedError`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_length` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the lengths of the encoded inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose` (`bool`, *optional*, defaults to `True`) — Whether or not to print
    more information and warnings. **kwargs — passed to the `self.tokenize()` method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` — List of token ids to be fed to a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` — List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *“token_type_ids”* is in `self.model_input_names`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`overflowing_tokens` — List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_truncated_tokens` — Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`special_tokens_mask` — List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length` — The length of the inputs (when `return_length=True`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_vocabulary`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/tokenization_speecht5.py#L220)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#### `decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3726)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`)
    — List of tokenized input ids. Can be obtained using the `__call__` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether or
    not to remove special tokens in the decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) — Whether or not to clean
    up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (additional keyword arguments, *optional*) — Will be passed to the
    underlying model specific decode method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`str`'
  prefs: []
  type: TYPE_NORMAL
- en: The decoded sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Converts a sequence of ids in a string, using the tokenizer and vocabulary with
    options to remove special tokens and clean up tokenization spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `batch_decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`)
    — List of tokenized input ids. Can be obtained using the `__call__` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether or
    not to remove special tokens in the decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) — Whether or not to clean
    up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (additional keyword arguments, *optional*) — Will be passed to the
    underlying model specific decode method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[str]`'
  prefs: []
  type: TYPE_NORMAL
- en: The list of decoded sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Convert a list of lists of token ids into a list of strings by calling decode.
  prefs: []
  type: TYPE_NORMAL
- en: SpeechT5FeatureExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SpeechT5FeatureExtractor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/feature_extraction_speecht5.py#L31)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_size` (`int`, *optional*, defaults to 1) — The feature dimension of
    the extracted features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*, defaults to 16000) — The sampling rate
    at which the audio files should be digitalized expressed in hertz (Hz).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding_value` (`float`, *optional*, defaults to 0.0) — The value that is
    used to fill the padding values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_normalize` (`bool`, *optional*, defaults to `False`) — Whether or not to
    zero-mean unit-variance normalize the input. Normalizing can help to significantly
    improve the performance for some models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_mel_bins` (`int`, *optional*, defaults to 80) — The number of mel-frequency
    bins in the extracted spectrogram features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hop_length` (`int`, *optional*, defaults to 16) — Number of ms between windows.
    Otherwise referred to as “shift” in many papers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`win_length` (`int`, *optional*, defaults to 64) — Number of ms per window.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`win_function` (`str`, *optional*, defaults to `"hann_window"`) — Name for
    the window function used for windowing, must be accessible via `torch.{win_function}`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frame_signal_scale` (`float`, *optional*, defaults to 1.0) — Constant multiplied
    in creating the frames before applying DFT. This argument is deprecated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fmin` (`float`, *optional*, defaults to 80) — Minimum mel frequency in Hz.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fmax` (`float`, *optional*, defaults to 7600) — Maximum mel frequency in Hz.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mel_floor` (`float`, *optional*, defaults to 1e-10) — Minimum value of mel
    frequency banks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduction_factor` (`int`, *optional*, defaults to 2) — Spectrogram length
    reduction factor. This argument is deprecated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_attention_mask` (`bool`, *optional*, defaults to `True`) — Whether
    or not [`call`()](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5FeatureExtractor.__call__)
    should return `attention_mask`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a SpeechT5 feature extractor.
  prefs: []
  type: TYPE_NORMAL
- en: This class can pre-process a raw speech signal by (optionally) normalizing to
    zero-mean unit-variance, for use by the SpeechT5 speech encoder prenet.
  prefs: []
  type: TYPE_NORMAL
- en: This class can also extract log-mel filter bank features from raw speech, for
    use by the SpeechT5 speech decoder prenet.
  prefs: []
  type: TYPE_NORMAL
- en: This feature extractor inherits from [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/feature_extraction_speecht5.py#L180)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`audio` (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`,
    *optional*) — The sequence or batch of sequences to be processed. Each sequence
    can be a numpy array, a list of float values, a list of numpy arrays or a list
    of list of float values. This outputs waveform features. Must be mono channel
    audio, not stereo, i.e. single float per timestep.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audio_target` (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`,
    *optional*) — The sequence or batch of sequences to be processed as targets. Each
    sequence can be a numpy array, a list of float values, a list of numpy arrays
    or a list of list of float values. This outputs log-mel spectrogram features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — Maximum length of the returned list and
    optionally padding length (see above).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`bool`) — Activates truncation to cut input sequences longer
    than *max_length* to *max_length*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta), or on TPUs which benefit from having
    sequence lengths be a multiple of 128.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific feature_extractor’s default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*) — The sampling rate at which the `audio`
    or `audio_target` input was sampled. It is strongly recommended to pass `sampling_rate`
    at the forward call to prevent silent errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to featurize and prepare for the model one or several sequence(s).
  prefs: []
  type: TYPE_NORMAL
- en: Pass in a value for `audio` to extract waveform features. Pass in a value for
    `audio_target` to extract log-mel spectrogram features.
  prefs: []
  type: TYPE_NORMAL
- en: SpeechT5Processor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SpeechT5Processor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/processing_speecht5.py#L20)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_extractor` (`SpeechT5FeatureExtractor`) — An instance of [SpeechT5FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5FeatureExtractor).
    The feature extractor is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`SpeechT5Tokenizer`) — An instance of [SpeechT5Tokenizer](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Tokenizer).
    The tokenizer is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a SpeechT5 processor which wraps a feature extractor and a tokenizer
    into a single processor.
  prefs: []
  type: TYPE_NORMAL
- en: '[SpeechT5Processor](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Processor)
    offers all the functionalities of [SpeechT5FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5FeatureExtractor)
    and [SpeechT5Tokenizer](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Tokenizer).
    See the docstring of [**call**()](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Processor.__call__)
    and [decode()](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Processor.decode)
    for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/processing_speecht5.py#L40)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Processes audio and text input, as well as audio and text targets.
  prefs: []
  type: TYPE_NORMAL
- en: You can process audio by using the argument `audio`, or process audio targets
    by using the argument `audio_target`. This forwards the arguments to SpeechT5FeatureExtractor’s
    [**call**()](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5FeatureExtractor.__call__).
  prefs: []
  type: TYPE_NORMAL
- en: You can process text by using the argument `text`, or process text labels by
    using the argument `text_target`. This forwards the arguments to SpeechT5Tokenizer’s
    [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__).
  prefs: []
  type: TYPE_NORMAL
- en: 'Valid input combinations are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`text` only'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audio` only'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_target` only'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audio_target` only'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text` and `audio_target`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audio` and `audio_target`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text` and `text_target`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audio` and `text_target`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please refer to the docstring of the above two methods for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `pad`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/processing_speecht5.py#L111)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Collates the audio and text inputs, as well as their targets, into a padded
    batch.
  prefs: []
  type: TYPE_NORMAL
- en: Audio inputs are padded by SpeechT5FeatureExtractor’s [pad()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad).
    Text inputs are padded by SpeechT5Tokenizer’s [pad()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad).
  prefs: []
  type: TYPE_NORMAL
- en: 'Valid input combinations are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` only'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_values` only'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` only, either log-mel spectrograms or text tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_ids` and log-mel spectrogram `labels`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_values` and text `labels`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please refer to the docstring of the above two methods for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L406)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained_model_name_or_path` (`str` or `os.PathLike`) — This can be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a string, the *model id* of a pretrained feature_extractor hosted inside a model
    repo on huggingface.co. Valid model ids can be located at the root-level, like
    `bert-base-uncased`, or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a path to a *directory* containing a feature extractor file saved using the
    [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)
    method, e.g., `./my_model_directory/`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a path or url to a saved feature extractor JSON *file*, e.g., `./my_model_directory/preprocessor_config.json`.
    **kwargs — Additional keyword arguments passed along to both [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained)
    and `~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate a processor associated with a pretrained model.
  prefs: []
  type: TYPE_NORMAL
- en: This class method is simply calling the feature extractor [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained),
    image processor [ImageProcessingMixin](/docs/transformers/v4.37.2/en/internal/image_processing_utils#transformers.ImageProcessingMixin)
    and the tokenizer `~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`
    methods. Please refer to the docstrings of the methods above for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L167)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`save_directory` (`str` or `os.PathLike`) — Directory where the feature extractor
    JSON file and the tokenizer files will be saved (directory will be created if
    it does not exist).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) — Whether or not to
    push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional key word arguments passed
    along to the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saves the attributes of this processor (feature extractor, tokenizer…) in the
    specified directory so that it can be reloaded using the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained)
    method.
  prefs: []
  type: TYPE_NORMAL
- en: This class method is simply calling [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)
    and [save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained).
    Please refer to the docstrings of the methods above for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `batch_decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/processing_speecht5.py#L171)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This method forwards all its arguments to SpeechT5Tokenizer’s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/processing_speecht5.py#L178)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This method forwards all its arguments to SpeechT5Tokenizer’s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  prefs: []
  type: TYPE_NORMAL
- en: SpeechT5Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SpeechT5Model`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2090)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([SpeechT5Config](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder` (`SpeechT5EncoderWithSpeechPrenet` or `SpeechT5EncoderWithTextPrenet`
    or `None`) — The Transformer encoder module that applies the appropiate speech
    or text encoder prenet. If `None`, `SpeechT5EncoderWithoutPrenet` will be used
    and the `input_values` are assumed to be hidden states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder` (`SpeechT5DecoderWithSpeechPrenet` or `SpeechT5DecoderWithTextPrenet`
    or `None`) — The Transformer decoder module that applies the appropiate speech
    or text decoder prenet. If `None`, `SpeechT5DecoderWithoutPrenet` will be used
    and the `decoder_input_values` are assumed to be hidden states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare SpeechT5 Encoder-Decoder Model outputting raw hidden-states without
    any specific pre- or post-nets. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2136)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, `attention_mask` should `not` be passed to avoid degraded performance
    when doing batched inference. For such models `input_values` should simply be
    padded with 0 and passed without `attention_mask`. Be aware that these models
    also yield slightly different results depending on whether `input_values` is padded
    or not.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_values`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `SpeechT5Decoder._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_head_mask` (`torch.FloatTensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If `past_key_values` are used, the user can optionally input only the last
    `decoder_input_values` (those that don’t have their past key value states given
    to this model) of shape `(batch_size, 1)` instead of all `decoder_input_values`
    of shape `(batch_size, sequence_length)`. decoder_inputs_embeds (`torch.FloatTensor`
    of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*): Optionally,
    instead of passing `decoder_input_values` you can choose to directly pass an embedded
    representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds`
    have to be input (see `past_key_values`). This is useful if you want more control
    over how to convert `decoder_input_values` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_values` (`torch.Tensor` of shape `(batch_size, sequence_length)`) —
    Depending on which encoder is being used, the `input_values` are either: float
    values of the input raw speech waveform, or indices of input sequence tokens in
    the vocabulary, or hidden states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_input_values` (`torch.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Depending on which decoder is being used, the `decoder_input_values`
    are either: float values of log-mel filterbank features extracted from the raw
    speech waveform, or indices of decoder input sequence tokens in the vocabulary,
    or hidden states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speaker_embeddings` (`torch.FloatTensor` of shape `(batch_size, config.speaker_embedding_dim)`,
    *optional*) — Tensor containing the speaker embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.Seq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.Seq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([SpeechT5Config](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Config))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the decoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the decoder at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the encoder at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [SpeechT5Model](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Model)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: SpeechT5ForSpeechToText
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SpeechT5ForSpeechToText`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2238)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([SpeechT5Config](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SpeechT5 Model with a speech encoder and a text decoder. This model inherits
    from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2284)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, `attention_mask` should `not` be passed to avoid degraded performance
    when doing batched inference. For such models `input_values` should simply be
    padded with 0 and passed without `attention_mask`. Be aware that these models
    also yield slightly different results depending on whether `input_values` is padded
    or not.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_values`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `SpeechT5Decoder._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_head_mask` (`torch.FloatTensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If `past_key_values` are used, the user can optionally input only the last
    `decoder_input_values` (those that don’t have their past key value states given
    to this model) of shape `(batch_size, 1)` instead of all `decoder_input_values`
    of shape `(batch_size, sequence_length)`. decoder_inputs_embeds (`torch.FloatTensor`
    of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*): Optionally,
    instead of passing `decoder_input_values` you can choose to directly pass an embedded
    representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds`
    have to be input (see `past_key_values`). This is useful if you want more control
    over how to convert `decoder_input_values` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a *.flac* or *.wav* audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (*pip install soundfile*). To prepare the array
    into `input_values`, the [SpeechT5Processor](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Processor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [SpeechT5Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Processor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [SpeechT5Tokenizer](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Tokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SpeechT5 uses the `eos_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the language modeling loss. Indices should either be in
    `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with
    indices set to `-100` are ignored (masked), the loss is only computed for the
    tokens with labels in `[0, ..., config.vocab_size]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label indices can be obtained using [SpeechT5Tokenizer](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Tokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([SpeechT5Config](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Config))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [SpeechT5ForSpeechToText](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5ForSpeechToText)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: SpeechT5ForTextToSpeech
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SpeechT5ForTextToSpeech`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2602)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([SpeechT5Config](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SpeechT5 Model with a text encoder and a speech decoder. This model inherits
    from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2635)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, `attention_mask` should `not` be passed to avoid degraded performance
    when doing batched inference. For such models `input_values` should simply be
    padded with 0 and passed without `attention_mask`. Be aware that these models
    also yield slightly different results depending on whether `input_values` is padded
    or not.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_values`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `SpeechT5Decoder._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_head_mask` (`torch.FloatTensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If `past_key_values` are used, the user can optionally input only the last
    `decoder_input_values` (those that don’t have their past key value states given
    to this model) of shape `(batch_size, 1)` instead of all `decoder_input_values`
    of shape `(batch_size, sequence_length)`. decoder_inputs_embeds (`torch.FloatTensor`
    of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*): Optionally,
    instead of passing `decoder_input_values` you can choose to directly pass an embedded
    representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds`
    have to be input (see `past_key_values`). This is useful if you want more control
    over how to convert `decoder_input_values` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [SpeechT5Tokenizer](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Tokenizer).
    See [encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.num_mel_bins)`) — Float values of input mel spectrogram.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SpeechT5 uses an all-zero spectrum as the starting token for `decoder_input_values`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_values`
    have to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`speaker_embeddings` (`torch.FloatTensor` of shape `(batch_size, config.speaker_embedding_dim)`,
    *optional*) — Tensor containing the speaker embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_mel_bins)`,
    *optional*) — Float values of target mel spectrogram. Timesteps set to `-100.0`
    are ignored (masked) for the loss computation. Spectrograms can be obtained using
    [SpeechT5Processor](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Processor).
    See [SpeechT5Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Processor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.Seq2SeqSpectrogramOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqSpectrogramOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.Seq2SeqSpectrogramOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqSpectrogramOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([SpeechT5Config](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Config))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Spectrogram generation loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spectrogram` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_bins)`) — The predicted spectrogram.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [SpeechT5ForTextToSpeech](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5ForTextToSpeech)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2755)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [SpeechT5Tokenizer](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Tokenizer).
    See [encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Attention mask from the tokenizer, required for batched inference to signal
    to the model where to ignore padded tokens from the input_ids.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speaker_embeddings` (`torch.FloatTensor` of shape `(batch_size, config.speaker_embedding_dim)`,
    *optional*) — Tensor containing the speaker embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threshold` (`float`, *optional*, defaults to 0.5) — The generated sequence
    ends when the predicted stop token probability exceeds this value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minlenratio` (`float`, *optional*, defaults to 0.0) — Used to calculate the
    minimum required length for the output sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxlenratio` (`float`, *optional*, defaults to 20.0) — Used to calculate the
    maximum allowed length for the output sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocoder` (`nn.Module`, *optional*) — The vocoder that converts the mel spectrogram
    into a speech waveform. If `None`, the output is the mel spectrogram.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_cross_attentions` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the attentions tensors of the decoder’s cross-attention layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_output_lengths` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the concrete spectrogram/waveform lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tuple(torch.FloatTensor)` comprising various elements depending on the inputs'
  prefs: []
  type: TYPE_NORMAL
- en: when `return_output_lengths` is False
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spectrogram` (*optional*, returned when no `vocoder` is provided) `torch.FloatTensor`
    of shape `(output_sequence_length, config.num_mel_bins)` — The predicted log-mel
    spectrogram.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`waveform` (*optional*, returned when a `vocoder` is provided) `torch.FloatTensor`
    of shape `(num_frames,)` — The predicted speech waveform.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attentions` (*optional*, returned when `output_cross_attentions` is
    `True`) `torch.FloatTensor` of shape `(config.decoder_layers, config.decoder_attention_heads,
    output_sequence_length, input_sequence_length)` — The outputs of the decoder’s
    cross-attention layers.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: when `return_output_lengths` is True
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spectrograms` (*optional*, returned when no `vocoder` is provided) `torch.FloatTensor`
    of shape `(batch_size, output_sequence_length, config.num_mel_bins)` — The predicted
    log-mel spectrograms that are padded to the maximum length.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spectrogram_lengths` (*optional*, returned when no `vocoder` is provided)
    `List[Int]` — A list of all the concrete lengths for each spectrogram.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`waveforms` (*optional*, returned when a `vocoder` is provided) `torch.FloatTensor`
    of shape `(batch_size, num_frames)` — The predicted speech waveforms that are
    padded to the maximum length.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`waveform_lengths` (*optional*, returned when a `vocoder` is provided) `List[Int]`
    — A list of all the concrete lengths for each waveform.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attentions` (*optional*, returned when `output_cross_attentions` is
    `True`) `torch.FloatTensor` of shape `(batch_size, config.decoder_layers, config.decoder_attention_heads,
    output_sequence_length, input_sequence_length)` — The outputs of the decoder’s
    cross-attention layers.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Converts a sequence of input tokens into a sequence of mel spectrograms, which
    are subsequently turned into a speech waveform using a vocoder.
  prefs: []
  type: TYPE_NORMAL
- en: SpeechT5ForSpeechToSpeech
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SpeechT5ForSpeechToSpeech`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2944)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([SpeechT5Config](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SpeechT5 Model with a speech encoder and a speech decoder. This model inherits
    from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L2974)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, `attention_mask` should `not` be passed to avoid degraded performance
    when doing batched inference. For such models `input_values` should simply be
    padded with 0 and passed without `attention_mask`. Be aware that these models
    also yield slightly different results depending on whether `input_values` is padded
    or not.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_values`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `SpeechT5Decoder._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_head_mask` (`torch.FloatTensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If `past_key_values` are used, the user can optionally input only the last
    `decoder_input_values` (those that don’t have their past key value states given
    to this model) of shape `(batch_size, 1)` instead of all `decoder_input_values`
    of shape `(batch_size, sequence_length)`. decoder_inputs_embeds (`torch.FloatTensor`
    of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*): Optionally,
    instead of passing `decoder_input_values` you can choose to directly pass an embedded
    representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds`
    have to be input (see `past_key_values`). This is useful if you want more control
    over how to convert `decoder_input_values` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a *.flac* or *.wav* audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (*pip install soundfile*). To prepare the array
    into `input_values`, the [SpeechT5Processor](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Processor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [SpeechT5Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Processor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.num_mel_bins)`) — Float values of input mel spectrogram.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SpeechT5 uses an all-zero spectrum as the starting token for `decoder_input_values`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_values`
    have to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`speaker_embeddings` (`torch.FloatTensor` of shape `(batch_size, config.speaker_embedding_dim)`,
    *optional*) — Tensor containing the speaker embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_mel_bins)`,
    *optional*) — Float values of target mel spectrogram. Spectrograms can be obtained
    using [SpeechT5Processor](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Processor).
    See [SpeechT5Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Processor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.Seq2SeqSpectrogramOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqSpectrogramOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.Seq2SeqSpectrogramOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqSpectrogramOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([SpeechT5Config](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Config))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Spectrogram generation loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spectrogram` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_bins)`) — The predicted spectrogram.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [SpeechT5ForSpeechToSpeech](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5ForSpeechToSpeech)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '#### `generate_speech`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L3088)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values can be obtained by loading a *.flac* or *.wav* audio file into an array
    of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (*pip
    install soundfile*). To prepare the array into `input_values`, the [SpeechT5Processor](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Processor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [SpeechT5Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5Processor.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`speaker_embeddings` (`torch.FloatTensor` of shape `(batch_size, config.speaker_embedding_dim)`,
    *optional*) — Tensor containing the speaker embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`threshold` (`float`, *optional*, defaults to 0.5) — The generated sequence
    ends when the predicted stop token probability exceeds this value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minlenratio` (`float`, *optional*, defaults to 0.0) — Used to calculate the
    minimum required length for the output sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxlenratio` (`float`, *optional*, defaults to 20.0) — Used to calculate the
    maximum allowed length for the output sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocoder` (`nn.Module`, *optional*, defaults to `None`) — The vocoder that
    converts the mel spectrogram into a speech waveform. If `None`, the output is
    the mel spectrogram.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_cross_attentions` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the attentions tensors of the decoder’s cross-attention layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_output_lengths` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the concrete spectrogram/waveform lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tuple(torch.FloatTensor)` comprising various elements depending on the inputs'
  prefs: []
  type: TYPE_NORMAL
- en: when `return_output_lengths` is False
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spectrogram` (*optional*, returned when no `vocoder` is provided) `torch.FloatTensor`
    of shape `(output_sequence_length, config.num_mel_bins)` — The predicted log-mel
    spectrogram.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`waveform` (*optional*, returned when a `vocoder` is provided) `torch.FloatTensor`
    of shape `(num_frames,)` — The predicted speech waveform.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attentions` (*optional*, returned when `output_cross_attentions` is
    `True`) `torch.FloatTensor` of shape `(config.decoder_layers, config.decoder_attention_heads,
    output_sequence_length, input_sequence_length)` — The outputs of the decoder’s
    cross-attention layers.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: when `return_output_lengths` is True
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spectrograms` (*optional*, returned when no `vocoder` is provided) `torch.FloatTensor`
    of shape `(batch_size, output_sequence_length, config.num_mel_bins)` — The predicted
    log-mel spectrograms that are padded to the maximum length.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spectrogram_lengths` (*optional*, returned when no `vocoder` is provided)
    `List[Int]` — A list of all the concrete lengths for each spectrogram.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`waveforms` (*optional*, returned when a `vocoder` is provided) `torch.FloatTensor`
    of shape `(batch_size, num_frames)` — The predicted speech waveforms that are
    padded to the maximum length.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`waveform_lengths` (*optional*, returned when a `vocoder` is provided) `List[Int]`
    — A list of all the concrete lengths for each waveform.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attentions` (*optional*, returned when `output_cross_attentions` is
    `True`) `torch.FloatTensor` of shape `(batch_size, config.decoder_layers, config.decoder_attention_heads,
    output_sequence_length, input_sequence_length)` — The outputs of the decoder’s
    cross-attention layers.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Converts a raw speech waveform into a sequence of mel spectrograms, which are
    subsequently turned back into a speech waveform using a vocoder.
  prefs: []
  type: TYPE_NORMAL
- en: SpeechT5HifiGan
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SpeechT5HifiGan`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L3253)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([SpeechT5HifiGanConfig](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5HifiGanConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HiFi-GAN vocoder. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speecht5/modeling_speecht5.py#L3322)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`spectrogram` (`torch.FloatTensor`) — Tensor containing the log-mel spectrograms.
    Can be batched and of shape `(batch_size, sequence_length, config.model_in_dim)`,
    or un-batched and of shape `(sequence_length, config.model_in_dim)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: Tensor containing the speech waveform. If the input spectrogram is batched,
    will be of shape `(batch_size, num_frames,)`. If un-batched, will be of shape
    `(num_frames,)`.
  prefs: []
  type: TYPE_NORMAL
- en: Converts a log-mel spectrogram into a speech waveform. Passing a batch of log-mel
    spectrograms returns a batch of speech waveforms. Passing a single, un-batched
    log-mel spectrogram returns a single, un-batched speech waveform.
  prefs: []
  type: TYPE_NORMAL
