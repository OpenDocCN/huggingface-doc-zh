- en: Brief introduction to RL documentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/learn/deep-rl-course/unitbonus3/rl-documentation](https://huggingface.co/learn/deep-rl-course/unitbonus3/rl-documentation)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/107.553a57d8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'In this advanced topic, we address the question: **how should we monitor and
    keep track of powerful reinforcement learning agents that we are training in the
    real world and interfacing with humans?**'
  prefs: []
  type: TYPE_NORMAL
- en: As machine learning systems have increasingly impacted modern life, the **call
    for the documentation of these systems has grown**.
  prefs: []
  type: TYPE_NORMAL
- en: Such documentation can cover aspects such as the training data used — where
    it is stored, when it was collected, who was involved, etc. — or the model optimization
    framework — the architecture, evaluation metrics, relevant papers, etc. — and
    more.
  prefs: []
  type: TYPE_NORMAL
- en: Today, model cards and datasheets are becoming increasingly available. For example,
    on the Hub (see documentation [here](https://huggingface.co/docs/hub/model-cards)).
  prefs: []
  type: TYPE_NORMAL
- en: If you click on a [popular model on the Hub](https://huggingface.co/models),
    you can learn about its creation process.
  prefs: []
  type: TYPE_NORMAL
- en: These model and data specific logs are designed to be completed when the model
    or dataset are created, leaving them to go un-updated when these models are built
    into evolving systems in the future. ​
  prefs: []
  type: TYPE_NORMAL
- en: Motivating Reward Reports
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement learning systems are fundamentally designed to optimize based
    on measurements of reward and time. While the notion of a reward function can
    be mapped nicely to many well-understood fields of supervised learning (via a
    loss function), understanding of how machine learning systems evolve over time
    is limited.
  prefs: []
  type: TYPE_NORMAL
- en: To that end, the authors introduce [*Reward Reports for Reinforcement Learning*](https://www.notion.so/Brief-introduction-to-RL-documentation-b8cbda5a6f5242338e0756e6bef72af4)
    (the pithy naming is designed to mirror the popular papers *Model Cards for Model
    Reporting* and *Datasheets for Datasets*). The goal is to propose a type of documentation
    focused on the **human factors of reward** and **time-varying feedback systems**.
  prefs: []
  type: TYPE_NORMAL
- en: Building on the documentation frameworks for [model cards](https://arxiv.org/abs/1810.03993)
    and [datasheets](https://arxiv.org/abs/1803.09010) proposed by Mitchell et al.
    and Gebru et al., we argue the need for Reward Reports for AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reward Reports** are living documents for proposed RL deployments that demarcate
    design choices.'
  prefs: []
  type: TYPE_NORMAL
- en: However, many questions remain about the applicability of this framework to
    different RL applications, roadblocks to system interpretability, and the resonances
    between deployed supervised machine learning systems and the sequential decision-making
    utilized in RL.
  prefs: []
  type: TYPE_NORMAL
- en: At a minimum, Reward Reports are an opportunity for RL practitioners to deliberate
    on these questions and begin the work of deciding how to resolve them in practice.
    ​
  prefs: []
  type: TYPE_NORMAL
- en: Capturing temporal behavior with documentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core piece specific to documentation designed for RL and feedback-driven
    ML systems is a *change-log*. The change-log updates information from the designer
    (changed training parameters, data, etc.) along with noticed changes from the
    user (harmful behavior, unexpected responses, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: The change log is accompanied by update triggers that encourage monitoring these
    effects.
  prefs: []
  type: TYPE_NORMAL
- en: Contributing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some of the most impactful RL-driven systems are multi-stakeholder in nature
    and behind the closed doors of private corporations. These corporations are largely
    without regulation, so the burden of documentation falls on the public.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in contributing, we are building Reward Reports for popular
    machine learning systems on a public record on [GitHub](https://github.com/RewardReports/reward-reports).
    ​ For further reading, you can visit the Reward Reports [paper](https://arxiv.org/abs/2204.10817)
    or look [an example report](https://github.com/RewardReports/reward-reports/tree/main/examples).
  prefs: []
  type: TYPE_NORMAL
- en: Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section was written by [Nathan Lambert](https://twitter.com/natolambert)
  prefs: []
  type: TYPE_NORMAL
