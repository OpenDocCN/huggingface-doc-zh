# 社区

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/community](https://huggingface.co/docs/transformers/v4.37.2/en/community)

此页面汇集了由社区开发的🤗 Transformers周围的资源。

## 社区资源：

| 资源 | 描述 | 作者 |
| :-- | :-- | --: |
| [Hugging Face Transformers词汇表闪卡](https://www.darigovresearch.com/huggingface-transformers-glossary-flashcards) | 基于[Transformers Docs词汇表](glossary)的一套闪卡，已经制作成易于使用[Anki](https://apps.ankiweb.net/)学习/复习的形式，Anki是一款专门设计用于长期知识保留的开源、跨平台应用程序。查看这个[介绍性视频，了解如何使用闪卡](https://www.youtube.com/watch?v=Dji_h7PILrw)。 | [Darigov Research](https://www.darigovresearch.com/) |

## 社区笔记本：

| 笔记本 | 描述 | 作者 |  |
| :-- | :-- | :-- | --: |
| [微调预训练的Transformer以生成歌词](https://github.com/AlekseyKorshuk/huggingartists) | 如何通过微调GPT-2模型生成您最喜爱艺术家风格的歌词 | [Aleksey Korshuk](https://github.com/AlekseyKorshuk) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb) |
| [在Tensorflow 2中训练T5](https://github.com/snapthat/TF-T5-text-to-text) | 如何使用Tensorflow 2为任何任务训练T5。这个笔记本演示了在Tensorflow 2中实现的一个问答任务，使用SQUAD | [Muhammad Harris](https://github.com/HarrisDePerceptron) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb) |
| [在TPU上训练T5](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb) | 如何使用Transformers和Nlp在SQUAD上训练T5 | [Suraj Patil](https://github.com/patil-suraj) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil) |
| [为分类和多项选择微调T5](https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb) | 如何使用PyTorch Lightning以文本-文本格式微调T5以进行分类和多项选择任务 | [Suraj Patil](https://github.com/patil-suraj) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb) |
| [在新数据集和语言上微调DialoGPT](https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb) | 如何在新数据集上微调DialoGPT模型，用于开放对话聊天机器人 | [Nathan Cooper](https://github.com/ncoop57) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb) |
| [使用Reformer进行长序列建模](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb) | 如何使用Reformer在长度为500,000个标记的序列上进行训练 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb) |
| [为摘要微调BART](https://github.com/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb) | 如何使用blurr使用fastai微调BART进行摘要 | [Wayde Gilliam](https://ohmeow.com/) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb) |
| [为任何人的推文微调预训练Transformer](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb) | 如何通过微调GPT-2模型生成您喜爱的Twitter账户风格的推文 | [Boris Dayma](https://github.com/borisdayma) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb) |
| [使用Weights＆Biases优化🤗Hugging Face模型](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb) | 一个完整的教程，展示了W＆B与Hugging Face的集成 | [Boris Dayma](https://github.com/borisdayma) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb) |
| [预训练Longformer](https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb) | 如何构建现有预训练模型的“长”版本 | [Iz Beltagy](https://beltagy.net) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb) |
| [为QA微调Longformer](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb) | 如何为QA任务微调Longformer模型 | [Suraj Patil](https://github.com/patil-suraj) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb) |
| [使用🤗nlp评估模型](https://github.com/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb) | 如何使用`nlp`在TriviaQA上评估Longformer | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1m7eTGlPmLRgoPkkA7rkhQdZ9ydpmsdLE?usp=sharing) |
| [为情感跨度提取微调T5](https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb) | 如何使用PyTorch Lightning以文本到文本格式微调T5进行情感跨度提取 | [Lorenzo Ampil](https://github.com/enzoampil) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb) |
| [为多类分类微调DistilBert](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb) | 如何使用PyTorch微调DistilBert进行多类分类 | [Abhishek Kumar Mishra](https://github.com/abhimishra91) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb) |
| [微调BERT进行多标签分类](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb) | 如何使用PyTorch微调BERT进行多标签分类 | [Abhishek Kumar Mishra](https://github.com/abhimishra91) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb) |
| [微调T5进行摘要](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb) | 如何在PyTorch中微调T5进行摘要，并使用WandB跟踪实验 | [Abhishek Kumar Mishra](https://github.com/abhimishra91) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb) |
| [使用动态填充/分桶加速Transformer中的微调](https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb) | 如何通过动态填充/分桶将微调加速2倍 | [Michael Benesty](https://github.com/pommedeterresautee) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1CBfRU1zbfu7-ijiOqAAQUA-RJaxfcJoO?usp=sharing) |
| [为遮蔽语言建模预训练Reformer](https://github.com/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb) | 如何训练具有双向自注意力层的Reformer模型 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1tzzh0i8PgDQGV3SMFUGxM7_gGae3K-uW?usp=sharing) |
| [扩展和微调Sci-BERT](https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb) | 如何在CORD数据集上增加预训练的SciBERT模型的词汇量并进行流水线处理。 | [Tanmay Thakur](https://github.com/lordtt13) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1rqAR40goxbAfez1xvF3hBJphSCsvXmh8) |
| [使用Trainer API微调BlenderBotSmall进行摘要](https://github.com/lordtt13/transformers-experiments/blob/master/Custom%20Tasks/fine-tune-blenderbot_small-for-summarization.ipynb) | 如何在自定义数据集上使用Trainer API微调BlenderBotSmall进行摘要 | [Tanmay Thakur](https://github.com/lordtt13) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/19Wmupuls7mykSGyRN_Qo6lPQhgp56ymq?usp=sharing) |
| [微调Electra并使用Integrated Gradients进行解释](https://github.com/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb) | 如何微调Electra进行情感分析，并使用Captum Integrated Gradients解释预测 | [Eliza Szczechla](https://elsanns.github.io) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb) |
| [使用Trainer类微调非英语GPT-2模型](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb) | 如何使用Trainer类微调非英语GPT-2模型 | [Philipp Schmid](https://www.philschmid.de) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb) |
| [为多标签分类任务微调DistilBERT模型](https://github.com/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb) | 如何为多标签分类任务微调DistilBERT模型 | [Dhaval Taunk](https://github.com/DhavalTaunk08) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb) |
| [为句对分类微调ALBERT](https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb) | 如何为句对分类任务微调ALBERT模型或其他基于BERT的模型 | [Nadir El Manouzi](https://github.com/NadirEM) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb) |
| [Fine-tune Roberta for sentiment analysis](https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb) | 如何为情感分析微调一个Roberta模型 | [Dhaval Taunk](https://github.com/DhavalTaunk08) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb) |
| [评估问题生成模型](https://github.com/flexudy-pipe/qugeev) | 您的seq2seq变压器模型生成的问题的答案有多准确？ | [Pascal Zoleko](https://github.com/zolekode) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1bpsSqCQU-iw_5nNoRm_crPq6FRuJthq_?usp=sharing) |
| [使用DistilBERT和Tensorflow对文本进行分类](https://github.com/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb) | 如何在TensorFlow中为文本分类微调DistilBERT | [Peter Bayerle](https://github.com/peterbayerle) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb) |
| [利用BERT进行CNN/Dailymail的编码器-解码器摘要](https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb) | 如何使用*bert-base-uncased*检查点对CNN/Dailymail的摘要进行热启动*EncoderDecoderModel* | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb) |
| [利用RoBERTa进行BBC XSum的编码器-解码器摘要](https://github.com/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb) | 如何使用*roberta-base*检查点对BBC/XSum的摘要进行热启动共享*EncoderDecoderModel* | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb) |
| [在顺序问答（SQA）上微调TAPAS](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) | 如何在顺序问答（SQA）数据集上使用*tapas-base*检查点微调*TapasForQuestionAnswering* | [Niels Rogge](https://github.com/nielsrogge) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) |
| [在Table Fact Checking（TabFact）上评估TAPAS](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb) | 如何使用🤗数据集和🤗transformers库的组合评估经过微调的*TapasForSequenceClassification*，使用*tapas-base-finetuned-tabfact*检查点 | [Niels Rogge](https://github.com/nielsrogge) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb) |
| [为翻译微调mBART](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb) | 如何使用Seq2SeqTrainer为印地语到英语翻译微调mBART | [Vasudev Gupta](https://github.com/vasudevgupta7) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb) |
| [在FUNSD上微调LayoutLM（一种表单理解数据集）](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb) | 如何在FUNSD数据集上微调*LayoutLMForTokenClassification*，从扫描文档中提取信息 | [Niels Rogge](https://github.com/nielsrogge) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb) |
| [微调DistilGPT2并生成文本](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb) | 如何微调DistilGPT2并生成文本 | [Aakash Tripathi](https://github.com/tripathiaakash) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb) |
| [在最多8K标记上微调LED](https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb) | 如何在pubmed上微调LED进行长距离摘要 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb) |
| [在Arxiv上评估LED](https://github.com/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb) | 如何有效评估LED进行长距离摘要 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb) |
| [在RVL-CDIP上微调LayoutLM（一种文档图像分类数据集）](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb) | 如何在RVL-CDIP数据集上微调*LayoutLMForSequenceClassification*，用于扫描文档分类 | [Niels Rogge](https://github.com/nielsrogge) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb) |
| - [使用GPT2调整进行Wav2Vec2 CTC解码](https://github.com/voidful/huggingface_notebook/blob/main/xlsr_gpt.ipynb) | 如何使用语言模型调整解码CTC序列 | [Eric Lam](https://github.com/voidful) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1e_z5jQHYbO2YKEaUgzb1ww1WwiAyydAj?usp=sharing) |
| - [使用Trainer类在两种语言中对BART进行摘要微调](https://github.com/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb) | 如何使用Trainer类在两种语言中对BART进行摘要微调 | [Eliza Szczechla](https://github.com/elsanns) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb) |
| - [在Trivia QA上评估Big Bird](https://github.com/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb) | 如何在Trivia QA上评估BigBird在长文档问答上的表现 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb) |
| - [使用Wav2Vec2创建视频字幕](https://github.com/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb) | 如何通过使用Wav2Vec转录音频来从任何视频创建YouTube字幕 | [Niklas Muennighoff](https://github.com/Muennighoff) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb) |
| - [使用PyTorch Lightning在CIFAR-10上微调Vision Transformer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb) | 如何使用HuggingFace Transformers、Datasets和PyTorch Lightning在CIFAR-10上微调Vision Transformer（ViT） | [Niels Rogge](https://github.com/nielsrogge) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb) |
| - [使用🤗 Trainer在CIFAR-10上微调Vision Transformer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb) | 如何使用HuggingFace Transformers、Datasets和🤗 Trainer在CIFAR-10上微调Vision Transformer（ViT） | [Niels Rogge](https://github.com/nielsrogge) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb) |
| - [在Open Entity上评估LUKE，一个实体类型数据集](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb) | 如何在Open Entity数据集上评估*LukeForEntityClassification* | [Ikuya Yamada](https://github.com/ikuyamada) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb) |
| - [在TACRED上评估LUKE，一个关系抽取数据集](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb) | 如何在TACRED数据集上评估*LukeForEntityPairClassification* | [Ikuya Yamada](https://github.com/ikuyamada) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb) |
| [在CoNLL-2003上评估LUKE，一个重要的NER基准](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb) | 如何在CoNLL-2003数据集上评估*LukeForEntitySpanClassification* | [Ikuya Yamada](https://github.com/ikuyamada) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb) |
| [在PubMed数据集上评估BigBird-Pegasus](https://github.com/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb) | 如何在PubMed数据集上评估*BigBirdPegasusForConditionalGeneration* | [Vasudev Gupta](https://github.com/vasudevgupta7) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb) |
| [使用Wav2Vec2进行语音情感分类](https://github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb) | 如何利用预训练的Wav2Vec2模型对MEGA数据集进行情感分类 | [Mehrdad Farahani](https://github.com/m3hrdadfi) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb) |
| [使用DETR在图像中检测对象](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb) | 如何使用训练好的*DetrForObjectDetection*模型在图像中检测对象并可视化注意力 | [Niels Rogge](https://github.com/NielsRogge) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb) |
| [在自定义对象检测数据集上微调DETR](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb) | 如何在自定义对象检测数据集上微调*DetrForObjectDetection* | [Niels Rogge](https://github.com/NielsRogge) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb) |
| [为命名实体识别微调T5](https://github.com/ToluClassics/Notebooks/blob/main/T5_Ner_Finetuning.ipynb) | 如何在命名实体识别任务上微调*T5* | [Ogundepo Odunayo](https://github.com/ToluClassics) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing) |
