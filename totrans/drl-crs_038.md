# 术语表

> 原文：[`huggingface.co/learn/deep-rl-course/unit2/glossary`](https://huggingface.co/learn/deep-rl-course/unit2/glossary)

这是一个由社区创建的术语表。欢迎贡献！

### 寻找最优策略的策略

+   **基于策略的方法。**通常使用神经网络训练策略，以选择在给定状态下采取什么行动。在这种情况下，神经网络输出代理应该采取的行动，而不是使用值函数。根据环境接收到的经验，神经网络将被重新调整，并提供更好的行动。

+   **基于价值的方法。**在这种情况下，训练值函数以输出将代表我们策略的状态或状态-动作对的值。然而，这个值并不定义代理应该采取什么行动。相反，我们需要指定值函数的输出给定代理的行为。例如，我们可以决定采用一个策略，始终采取导致最大奖励的行动（贪婪策略）。总之，策略是一个贪婪策略（或用户采取的任何决定），它使用值函数的值来决定采取的行动。

### 在基于价值的方法中，我们可以找到两种主要策略

+   **状态值函数。**对于每个状态，状态值函数是代理在该状态开始并遵循策略直到结束时的期望回报。

+   **动作值函数。**与状态值函数相反，动作值函数计算了每个状态和动作对的期望回报，如果代理在该状态开始，采取该动作，然后按照策略永远执行。

### Epsilon-贪婪策略：

+   在强化学习中常用的一种策略，涉及平衡探索和开发。

+   选择具有最高期望奖励的动作，概率为 1-epsilon。

+   以 epsilon 的概率选择随机动作。

+   随着时间的推移，epsilon 通常会减少，以将焦点转向开发。

### 贪婪策略：

+   始终选择预期导致最高奖励的动作，基于对环境当前知识的了解。（仅开发）

+   始终选择具有最高期望奖励的动作。

+   不包括任何探索。

+   在存在不确定性或未知最优动作的环境中可能具有劣势。

### 离策略与在策略算法

+   **离策略算法：**在训练时间和推断时间使用不同的策略

+   **在策略算法：**训练和推断期间使用相同的策略

### 蒙特卡洛和时间差异学习策略

+   **蒙特卡洛（MC）：**在情节结束时学习。通过蒙特卡洛，我们等到情节结束，然后从完整的情节更新值函数（或策略函数）。

+   **时间差异（TD）：**每一步学习。通过时间差异学习，我们在每一步更新值函数（或策略函数），而无需完整的情节。

如果您想改进课程，可以[发起拉取请求。](https://github.com/huggingface/deep-rl-class/pulls)

这个术语表得以实现，感谢：

+   [Ramón Rueda](https://github.com/ramon-rd)

+   [Hasarindu Perera](https://github.com/hasarinduperera/)

+   [Arkady Arkhangorodsky](https://github.com/arkadyark/)
