["```py\n>>> # leverage checkpoints for Bert2Bert model...\n>>> # use BERT's cls token as BOS token and sep token as EOS token\n>>> encoder = BertGenerationEncoder.from_pretrained(\"bert-large-uncased\", bos_token_id=101, eos_token_id=102)\n>>> # add cross attention layers and use BERT's cls token as BOS token and sep token as EOS token\n>>> decoder = BertGenerationDecoder.from_pretrained(\n...     \"bert-large-uncased\", add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102\n... )\n>>> bert2bert = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n\n>>> # create tokenizer...\n>>> tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n\n>>> input_ids = tokenizer(\n...     \"This is a long article to summarize\", add_special_tokens=False, return_tensors=\"pt\"\n... ).input_ids\n>>> labels = tokenizer(\"This is a short summary\", return_tensors=\"pt\").input_ids\n\n>>> # train...\n>>> loss = bert2bert(input_ids=input_ids, decoder_input_ids=labels, labels=labels).loss\n>>> loss.backward()\n```", "```py\n>>> # instantiate sentence fusion model\n>>> sentence_fuser = EncoderDecoderModel.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n\n>>> input_ids = tokenizer(\n...     \"This is the first sentence. This is the second sentence.\", add_special_tokens=False, return_tensors=\"pt\"\n... ).input_ids\n\n>>> outputs = sentence_fuser.generate(input_ids)\n\n>>> print(tokenizer.decode(outputs[0]))\n```", "```py\n>>> from transformers import BertGenerationConfig, BertGenerationEncoder\n\n>>> # Initializing a BertGeneration config\n>>> configuration = BertGenerationConfig()\n\n>>> # Initializing a model (with random weights) from the config\n>>> model = BertGenerationEncoder(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoTokenizer, BertGenerationEncoder\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bert_for_seq_generation_L-24_bbc_encoder\")\n>>> model = BertGenerationEncoder.from_pretrained(\"google/bert_for_seq_generation_L-24_bbc_encoder\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, BertGenerationDecoder, BertGenerationConfig\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bert_for_seq_generation_L-24_bbc_encoder\")\n>>> config = BertGenerationConfig.from_pretrained(\"google/bert_for_seq_generation_L-24_bbc_encoder\")\n>>> config.is_decoder = True\n>>> model = BertGenerationDecoder.from_pretrained(\n...     \"google/bert_for_seq_generation_L-24_bbc_encoder\", config=config\n... )\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_token_type_ids=False, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> prediction_logits = outputs.logits\n```"]