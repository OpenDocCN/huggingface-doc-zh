["```py\n>>> # leverage checkpoints for Bert2Bert model...\n>>> # use BERT's cls token as BOS token and sep token as EOS token\n>>> encoder = BertGenerationEncoder.from_pretrained(\"bert-large-uncased\", bos_token_id=101, eos_token_id=102)\n>>> # add cross attention layers and use BERT's cls token as BOS token and sep token as EOS token\n>>> decoder = BertGenerationDecoder.from_pretrained(\n...     \"bert-large-uncased\", add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102\n... )\n>>> bert2bert = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n\n>>> # create tokenizer...\n>>> tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n\n>>> input_ids = tokenizer(\n...     \"This is a long article to summarize\", add_special_tokens=False, return_tensors=\"pt\"\n... ).input_ids\n>>> labels = tokenizer(\"This is a short summary\", return_tensors=\"pt\").input_ids\n\n>>> # train...\n>>> loss = bert2bert(input_ids=input_ids, decoder_input_ids=labels, labels=labels).loss\n>>> loss.backward()\n```", "```py\n>>> # instantiate sentence fusion model\n>>> sentence_fuser = EncoderDecoderModel.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n\n>>> input_ids = tokenizer(\n...     \"This is the first sentence. This is the second sentence.\", add_special_tokens=False, return_tensors=\"pt\"\n... ).input_ids\n\n>>> outputs = sentence_fuser.generate(input_ids)\n\n>>> print(tokenizer.decode(outputs[0]))\n```", "```py\n( vocab_size = 50358 hidden_size = 1024 num_hidden_layers = 24 num_attention_heads = 16 intermediate_size = 4096 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 bos_token_id = 2 eos_token_id = 1 position_embedding_type = 'absolute' use_cache = True **kwargs )\n```", "```py\n>>> from transformers import BertGenerationConfig, BertGenerationEncoder\n\n>>> # Initializing a BertGeneration config\n>>> configuration = BertGenerationConfig()\n\n>>> # Initializing a model (with random weights) from the config\n>>> model = BertGenerationEncoder(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file bos_token = '<s>' eos_token = '</s>' unk_token = '<unk>' pad_token = '<pad>' sep_token = '<::::>' sp_model_kwargs: Optional = None **kwargs )\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, BertGenerationEncoder\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bert_for_seq_generation_L-24_bbc_encoder\")\n>>> model = BertGenerationEncoder.from_pretrained(\"google/bert_for_seq_generation_L-24_bbc_encoder\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None labels: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, BertGenerationDecoder, BertGenerationConfig\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bert_for_seq_generation_L-24_bbc_encoder\")\n>>> config = BertGenerationConfig.from_pretrained(\"google/bert_for_seq_generation_L-24_bbc_encoder\")\n>>> config.is_decoder = True\n>>> model = BertGenerationDecoder.from_pretrained(\n...     \"google/bert_for_seq_generation_L-24_bbc_encoder\", config=config\n... )\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_token_type_ids=False, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> prediction_logits = outputs.logits\n```"]