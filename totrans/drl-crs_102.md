# 额外阅读

> 原文：[https://huggingface.co/learn/deep-rl-course/unit8/additional-readings](https://huggingface.co/learn/deep-rl-course/unit8/additional-readings)

如果您想深入了解，这些是**可选阅读**。

## PPO解释

+   [通过Daniel Bick提供的一致自包含解释向Proximal Policy Optimization迈进](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)

+   如何理解强化学习中的Proximal Policy Optimization算法的方法？

+   [深度强化学习基础系列，L4 TRPO和PPO，由Pieter Abbeel](https://youtu.be/KjWF8VIMGiY)

+   [OpenAI PPO博文](https://openai.com/blog/openai-baselines-ppo/)

+   [Spinning Up RL PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html)

+   [论文Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)

## PPO实现细节

+   [Proximal Policy Optimization的37个实现细节](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)

+   [第1部分- Proximal Policy Optimization实现：11个核心实现细节](https://www.youtube.com/watch?v=MEt6rrxH8W4)

## 重要性采样

+   重要性采样解释
