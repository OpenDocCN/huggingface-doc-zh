["```py\npip install bitsandbytes\n```", "```py\npip install git+https://github.com/huggingface/accelerate.git\n```", "```py\ngit clone https://github.com/karpathy/minGPT.git\npip install minGPT/\npip install huggingface_hub\n```", "```py\nfrom accelerate import init_empty_weights\nfrom mingpt.model import GPT\n\nmodel_config = GPT.get_default_config()\nmodel_config.model_type = 'gpt2-xl'\nmodel_config.vocab_size = 50257\nmodel_config.block_size = 1024\n\nwith init_empty_weights():\n    empty_model = GPT(model_config)\n```", "```py\nfrom huggingface_hub import snapshot_download\nweights_location = snapshot_download(repo_id=\"marcsun13/gpt2-xl-linear-sharded\")\n```", "```py\nfrom accelerate.utils import BnbQuantizationConfig\nbnb_quantization_config = BnbQuantizationConfig(load_in_8bit=True, llm_int8_threshold = 6)\n```", "```py\nfrom accelerate.utils import BnbQuantizationConfig\nbnb_quantization_config = BnbQuantizationConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n```", "```py\nfrom accelerate.utils import load_and_quantize_model\nquantized_model = load_and_quantize_model(empty_model, weights_location=weights_location, bnb_quantization_config=bnb_quantization_config, device_map = \"auto\")\n```", "```py\nfrom accelerate import Accelerator\naccelerate = Accelerator()\nnew_weights_location = \"path/to/save_directory\"\naccelerate.save_model(quantized_model, new_weights_location)\n\nquantized_model_from_saved = load_and_quantize_model(empty_model, weights_location=new_weights_location, bnb_quantization_config=bnb_quantization_config, device_map = \"auto\")\n```", "```py\ndevice_map = {\n    \"transformer.wte\": 0,\n    \"transformer.wpe\": 0,\n    \"transformer.drop\": 0,\n    \"transformer.h\": \"cpu\",\n    \"transformer.ln_f\": \"disk\",\n    \"lm_head\": \"disk\",\n}\n```"]