["```py\ndef normalize_bbox(bbox, width, height):\n    return [\n        int(1000 * (bbox[0] / width)),\n        int(1000 * (bbox[1] / height)),\n        int(1000 * (bbox[2] / width)),\n        int(1000 * (bbox[3] / height)),\n    ]\n```", "```py\nfrom PIL import Image\n\n# Document can be a png, jpg, etc. PDFs must be converted to images.\nimage = Image.open(name_of_your_document).convert(\"RGB\")\n\nwidth, height = image.size\n```", "```py\n( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 position_embedding_type = 'absolute' use_cache = True max_2d_position_embeddings = 1024 **kwargs )\n```", "```py\n>>> from transformers import LayoutLMConfig, LayoutLMModel\n\n>>> # Initializing a LayoutLM configuration\n>>> configuration = LayoutLMConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = LayoutLMModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file do_lower_case = True do_basic_tokenize = True never_split = None unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( tokens )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence |\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( vocab_file = None tokenizer_file = None do_lower_case = True unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs )\n```", "```py\n( token_ids_0 token_ids_1 = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence |\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LayoutLMModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n>>> model = LayoutLMModel.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n>>> words = [\"Hello\", \"world\"]\n>>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n\n>>> token_boxes = []\n>>> for word, box in zip(words, normalized_word_boxes):\n...     word_tokens = tokenizer.tokenize(word)\n...     token_boxes.extend([box] * len(word_tokens))\n>>> # add bounding boxes of cls + sep tokens\n>>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n\n>>> encoding = tokenizer(\" \".join(words), return_tensors=\"pt\")\n>>> input_ids = encoding[\"input_ids\"]\n>>> attention_mask = encoding[\"attention_mask\"]\n>>> token_type_ids = encoding[\"token_type_ids\"]\n>>> bbox = torch.tensor([token_boxes])\n\n>>> outputs = model(\n...     input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids\n... )\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.MaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LayoutLMForMaskedLM\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n>>> model = LayoutLMForMaskedLM.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n>>> words = [\"Hello\", \"[MASK]\"]\n>>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n\n>>> token_boxes = []\n>>> for word, box in zip(words, normalized_word_boxes):\n...     word_tokens = tokenizer.tokenize(word)\n...     token_boxes.extend([box] * len(word_tokens))\n>>> # add bounding boxes of cls + sep tokens\n>>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n\n>>> encoding = tokenizer(\" \".join(words), return_tensors=\"pt\")\n>>> input_ids = encoding[\"input_ids\"]\n>>> attention_mask = encoding[\"attention_mask\"]\n>>> token_type_ids = encoding[\"token_type_ids\"]\n>>> bbox = torch.tensor([token_boxes])\n\n>>> labels = tokenizer(\"Hello world\", return_tensors=\"pt\")[\"input_ids\"]\n\n>>> outputs = model(\n...     input_ids=input_ids,\n...     bbox=bbox,\n...     attention_mask=attention_mask,\n...     token_type_ids=token_type_ids,\n...     labels=labels,\n... )\n\n>>> loss = outputs.loss\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LayoutLMForSequenceClassification\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n>>> model = LayoutLMForSequenceClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n>>> words = [\"Hello\", \"world\"]\n>>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n\n>>> token_boxes = []\n>>> for word, box in zip(words, normalized_word_boxes):\n...     word_tokens = tokenizer.tokenize(word)\n...     token_boxes.extend([box] * len(word_tokens))\n>>> # add bounding boxes of cls + sep tokens\n>>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n\n>>> encoding = tokenizer(\" \".join(words), return_tensors=\"pt\")\n>>> input_ids = encoding[\"input_ids\"]\n>>> attention_mask = encoding[\"attention_mask\"]\n>>> token_type_ids = encoding[\"token_type_ids\"]\n>>> bbox = torch.tensor([token_boxes])\n>>> sequence_label = torch.tensor([1])\n\n>>> outputs = model(\n...     input_ids=input_ids,\n...     bbox=bbox,\n...     attention_mask=attention_mask,\n...     token_type_ids=token_type_ids,\n...     labels=sequence_label,\n... )\n\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LayoutLMForTokenClassification\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n>>> model = LayoutLMForTokenClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n>>> words = [\"Hello\", \"world\"]\n>>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n\n>>> token_boxes = []\n>>> for word, box in zip(words, normalized_word_boxes):\n...     word_tokens = tokenizer.tokenize(word)\n...     token_boxes.extend([box] * len(word_tokens))\n>>> # add bounding boxes of cls + sep tokens\n>>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n\n>>> encoding = tokenizer(\" \".join(words), return_tensors=\"pt\")\n>>> input_ids = encoding[\"input_ids\"]\n>>> attention_mask = encoding[\"attention_mask\"]\n>>> token_type_ids = encoding[\"token_type_ids\"]\n>>> bbox = torch.tensor([token_boxes])\n>>> token_labels = torch.tensor([1, 1, 0, 0]).unsqueeze(0)  # batch size of 1\n\n>>> outputs = model(\n...     input_ids=input_ids,\n...     bbox=bbox,\n...     attention_mask=attention_mask,\n...     token_type_ids=token_type_ids,\n...     labels=token_labels,\n... )\n\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n( config has_visual_segment_embedding = True )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.QuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LayoutLMForQuestionAnswering\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"impira/layoutlm-document-qa\", add_prefix_space=True)\n>>> model = LayoutLMForQuestionAnswering.from_pretrained(\"impira/layoutlm-document-qa\", revision=\"1e3ebac\")\n\n>>> dataset = load_dataset(\"nielsr/funsd\", split=\"train\")\n>>> example = dataset[0]\n>>> question = \"what's his name?\"\n>>> words = example[\"words\"]\n>>> boxes = example[\"bboxes\"]\n\n>>> encoding = tokenizer(\n...     question.split(), words, is_split_into_words=True, return_token_type_ids=True, return_tensors=\"pt\"\n... )\n>>> bbox = []\n>>> for i, s, w in zip(encoding.input_ids[0], encoding.sequence_ids(0), encoding.word_ids(0)):\n...     if s == 1:\n...         bbox.append(boxes[w])\n...     elif i == tokenizer.sep_token_id:\n...         bbox.append([1000] * 4)\n...     else:\n...         bbox.append([0] * 4)\n>>> encoding[\"bbox\"] = torch.tensor([bbox])\n\n>>> word_ids = encoding.word_ids(0)\n>>> outputs = model(**encoding)\n>>> loss = outputs.loss\n>>> start_scores = outputs.start_logits\n>>> end_scores = outputs.end_logits\n>>> start, end = word_ids[start_scores.argmax(-1)], word_ids[end_scores.argmax(-1)]\n>>> print(\" \".join(words[start : end + 1]))\nM. Hamann P. Harper, P. Martinez\n```", "```py\n( config: LayoutLMConfig *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None bbox: np.ndarray | tf.Tensor | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None encoder_hidden_states: np.ndarray | tf.Tensor | None = None encoder_attention_mask: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFLayoutLMModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n>>> model = TFLayoutLMModel.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n>>> words = [\"Hello\", \"world\"]\n>>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n\n>>> token_boxes = []\n>>> for word, box in zip(words, normalized_word_boxes):\n...     word_tokens = tokenizer.tokenize(word)\n...     token_boxes.extend([box] * len(word_tokens))\n>>> # add bounding boxes of cls + sep tokens\n>>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n\n>>> encoding = tokenizer(\" \".join(words), return_tensors=\"tf\")\n>>> input_ids = encoding[\"input_ids\"]\n>>> attention_mask = encoding[\"attention_mask\"]\n>>> token_type_ids = encoding[\"token_type_ids\"]\n>>> bbox = tf.convert_to_tensor([token_boxes])\n\n>>> outputs = model(\n...     input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids\n... )\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: LayoutLMConfig *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None bbox: np.ndarray | tf.Tensor | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFMaskedLMOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFLayoutLMForMaskedLM\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n>>> model = TFLayoutLMForMaskedLM.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n>>> words = [\"Hello\", \"[MASK]\"]\n>>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n\n>>> token_boxes = []\n>>> for word, box in zip(words, normalized_word_boxes):\n...     word_tokens = tokenizer.tokenize(word)\n...     token_boxes.extend([box] * len(word_tokens))\n>>> # add bounding boxes of cls + sep tokens\n>>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n\n>>> encoding = tokenizer(\" \".join(words), return_tensors=\"tf\")\n>>> input_ids = encoding[\"input_ids\"]\n>>> attention_mask = encoding[\"attention_mask\"]\n>>> token_type_ids = encoding[\"token_type_ids\"]\n>>> bbox = tf.convert_to_tensor([token_boxes])\n\n>>> labels = tokenizer(\"Hello world\", return_tensors=\"tf\")[\"input_ids\"]\n\n>>> outputs = model(\n...     input_ids=input_ids,\n...     bbox=bbox,\n...     attention_mask=attention_mask,\n...     token_type_ids=token_type_ids,\n...     labels=labels,\n... )\n\n>>> loss = outputs.loss\n```", "```py\n( config: LayoutLMConfig *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None bbox: np.ndarray | tf.Tensor | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSequenceClassifierOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFLayoutLMForSequenceClassification\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n>>> model = TFLayoutLMForSequenceClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n>>> words = [\"Hello\", \"world\"]\n>>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n\n>>> token_boxes = []\n>>> for word, box in zip(words, normalized_word_boxes):\n...     word_tokens = tokenizer.tokenize(word)\n...     token_boxes.extend([box] * len(word_tokens))\n>>> # add bounding boxes of cls + sep tokens\n>>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n\n>>> encoding = tokenizer(\" \".join(words), return_tensors=\"tf\")\n>>> input_ids = encoding[\"input_ids\"]\n>>> attention_mask = encoding[\"attention_mask\"]\n>>> token_type_ids = encoding[\"token_type_ids\"]\n>>> bbox = tf.convert_to_tensor([token_boxes])\n>>> sequence_label = tf.convert_to_tensor([1])\n\n>>> outputs = model(\n...     input_ids=input_ids,\n...     bbox=bbox,\n...     attention_mask=attention_mask,\n...     token_type_ids=token_type_ids,\n...     labels=sequence_label,\n... )\n\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n( config: LayoutLMConfig *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None bbox: np.ndarray | tf.Tensor | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFTokenClassifierOutput or tuple(tf.Tensor)\n```", "```py\n>>> import tensorflow as tf\n>>> from transformers import AutoTokenizer, TFLayoutLMForTokenClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n>>> model = TFLayoutLMForTokenClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n>>> words = [\"Hello\", \"world\"]\n>>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n\n>>> token_boxes = []\n>>> for word, box in zip(words, normalized_word_boxes):\n...     word_tokens = tokenizer.tokenize(word)\n...     token_boxes.extend([box] * len(word_tokens))\n>>> # add bounding boxes of cls + sep tokens\n>>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n\n>>> encoding = tokenizer(\" \".join(words), return_tensors=\"tf\")\n>>> input_ids = encoding[\"input_ids\"]\n>>> attention_mask = encoding[\"attention_mask\"]\n>>> token_type_ids = encoding[\"token_type_ids\"]\n>>> bbox = tf.convert_to_tensor([token_boxes])\n>>> token_labels = tf.convert_to_tensor([1, 1, 0, 0])\n\n>>> outputs = model(\n...     input_ids=input_ids,\n...     bbox=bbox,\n...     attention_mask=attention_mask,\n...     token_type_ids=token_type_ids,\n...     labels=token_labels,\n... )\n\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n( config: LayoutLMConfig *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None bbox: np.ndarray | tf.Tensor | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None start_positions: np.ndarray | tf.Tensor | None = None end_positions: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> import tensorflow as tf\n>>> from transformers import AutoTokenizer, TFLayoutLMForQuestionAnswering\n>>> from datasets import load_dataset\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"impira/layoutlm-document-qa\", add_prefix_space=True)\n>>> model = TFLayoutLMForQuestionAnswering.from_pretrained(\"impira/layoutlm-document-qa\", revision=\"1e3ebac\")\n\n>>> dataset = load_dataset(\"nielsr/funsd\", split=\"train\")\n>>> example = dataset[0]\n>>> question = \"what's his name?\"\n>>> words = example[\"words\"]\n>>> boxes = example[\"bboxes\"]\n\n>>> encoding = tokenizer(\n...     question.split(), words, is_split_into_words=True, return_token_type_ids=True, return_tensors=\"tf\"\n... )\n>>> bbox = []\n>>> for i, s, w in zip(encoding.input_ids[0], encoding.sequence_ids(0), encoding.word_ids(0)):\n...     if s == 1:\n...         bbox.append(boxes[w])\n...     elif i == tokenizer.sep_token_id:\n...         bbox.append([1000] * 4)\n...     else:\n...         bbox.append([0] * 4)\n>>> encoding[\"bbox\"] = tf.convert_to_tensor([bbox])\n\n>>> word_ids = encoding.word_ids(0)\n>>> outputs = model(**encoding)\n>>> loss = outputs.loss\n>>> start_scores = outputs.start_logits\n>>> end_scores = outputs.end_logits\n>>> start, end = word_ids[tf.math.argmax(start_scores, -1)[0]], word_ids[tf.math.argmax(end_scores, -1)[0]]\n>>> print(\" \".join(words[start : end + 1]))\nM. Hamann P. Harper, P. Martinez\n```"]