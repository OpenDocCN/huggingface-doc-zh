["```py\ndef normalize_bbox(bbox, width, height):\n    return [\n        int(1000 * (bbox[0] / width)),\n        int(1000 * (bbox[1] / height)),\n        int(1000 * (bbox[2] / width)),\n        int(1000 * (bbox[3] / height)),\n    ]\n```", "```py\nfrom PIL import Image\n\n# Document can be a png, jpg, etc. PDFs must be converted to images.\nimage = Image.open(name_of_your_document).convert(\"RGB\")\n\nwidth, height = image.size\n```", "```py\n>>> from transformers import LayoutLMConfig, LayoutLMModel\n\n>>> # Initializing a LayoutLM configuration\n>>> configuration = LayoutLMConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = LayoutLMModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence |\n```", "```py\n0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence |\n```", "```py\n>>> from transformers import AutoTokenizer, LayoutLMModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n>>> model = LayoutLMModel.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n>>> words = [\"Hello\", \"world\"]\n>>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n\n>>> token_boxes = []\n>>> for word, box in zip(words, normalized_word_boxes):\n...     word_tokens = tokenizer.tokenize(word)\n...     token_boxes.extend([box] * len(word_tokens))\n>>> # add bounding boxes of cls + sep tokens\n>>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n\n>>> encoding = tokenizer(\" \".join(words), return_tensors=\"pt\")\n>>> input_ids = encoding[\"input_ids\"]\n>>> attention_mask = encoding[\"attention_mask\"]\n>>> token_type_ids = encoding[\"token_type_ids\"]\n>>> bbox = torch.tensor([token_boxes])\n\n>>> outputs = model(\n...     input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids\n... )\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, LayoutLMForMaskedLM\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n>>> model = LayoutLMForMaskedLM.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n>>> words = [\"Hello\", \"[MASK]\"]\n>>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n\n>>> token_boxes = []\n>>> for word, box in zip(words, normalized_word_boxes):\n...     word_tokens = tokenizer.tokenize(word)\n...     token_boxes.extend([box] * len(word_tokens))\n>>> # add bounding boxes of cls + sep tokens\n>>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n\n>>> encoding = tokenizer(\" \".join(words), return_tensors=\"pt\")\n>>> input_ids = encoding[\"input_ids\"]\n>>> attention_mask = encoding[\"attention_mask\"]\n>>> token_type_ids = encoding[\"token_type_ids\"]\n>>> bbox = torch.tensor([token_boxes])\n\n>>> labels = tokenizer(\"Hello world\", return_tensors=\"pt\")[\"input_ids\"]\n\n>>> outputs = model(\n...     input_ids=input_ids,\n...     bbox=bbox,\n...     attention_mask=attention_mask,\n...     token_type_ids=token_type_ids,\n...     labels=labels,\n... )\n\n>>> loss = outputs.loss\n```", "```py\n>>> from transformers import AutoTokenizer, LayoutLMForSequenceClassification\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n>>> model = LayoutLMForSequenceClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n>>> words = [\"Hello\", \"world\"]\n>>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n\n>>> token_boxes = []\n>>> for word, box in zip(words, normalized_word_boxes):\n...     word_tokens = tokenizer.tokenize(word)\n...     token_boxes.extend([box] * len(word_tokens))\n>>> # add bounding boxes of cls + sep tokens\n>>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n\n>>> encoding = tokenizer(\" \".join(words), return_tensors=\"pt\")\n>>> input_ids = encoding[\"input_ids\"]\n>>> attention_mask = encoding[\"attention_mask\"]\n>>> token_type_ids = encoding[\"token_type_ids\"]\n>>> bbox = torch.tensor([token_boxes])\n>>> sequence_label = torch.tensor([1])\n\n>>> outputs = model(\n...     input_ids=input_ids,\n...     bbox=bbox,\n...     attention_mask=attention_mask,\n...     token_type_ids=token_type_ids,\n...     labels=sequence_label,\n... )\n\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, LayoutLMForTokenClassification\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n>>> model = LayoutLMForTokenClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n>>> words = [\"Hello\", \"world\"]\n>>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n\n>>> token_boxes = []\n>>> for word, box in zip(words, normalized_word_boxes):\n...     word_tokens = tokenizer.tokenize(word)\n...     token_boxes.extend([box] * len(word_tokens))\n>>> # add bounding boxes of cls + sep tokens\n>>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n\n>>> encoding = tokenizer(\" \".join(words), return_tensors=\"pt\")\n>>> input_ids = encoding[\"input_ids\"]\n>>> attention_mask = encoding[\"attention_mask\"]\n>>> token_type_ids = encoding[\"token_type_ids\"]\n>>> bbox = torch.tensor([token_boxes])\n>>> token_labels = torch.tensor([1, 1, 0, 0]).unsqueeze(0)  # batch size of 1\n\n>>> outputs = model(\n...     input_ids=input_ids,\n...     bbox=bbox,\n...     attention_mask=attention_mask,\n...     token_type_ids=token_type_ids,\n...     labels=token_labels,\n... )\n\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, LayoutLMForQuestionAnswering\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"impira/layoutlm-document-qa\", add_prefix_space=True)\n>>> model = LayoutLMForQuestionAnswering.from_pretrained(\"impira/layoutlm-document-qa\", revision=\"1e3ebac\")\n\n>>> dataset = load_dataset(\"nielsr/funsd\", split=\"train\")\n>>> example = dataset[0]\n>>> question = \"what's his name?\"\n>>> words = example[\"words\"]\n>>> boxes = example[\"bboxes\"]\n\n>>> encoding = tokenizer(\n...     question.split(), words, is_split_into_words=True, return_token_type_ids=True, return_tensors=\"pt\"\n... )\n>>> bbox = []\n>>> for i, s, w in zip(encoding.input_ids[0], encoding.sequence_ids(0), encoding.word_ids(0)):\n...     if s == 1:\n...         bbox.append(boxes[w])\n...     elif i == tokenizer.sep_token_id:\n...         bbox.append([1000] * 4)\n...     else:\n...         bbox.append([0] * 4)\n>>> encoding[\"bbox\"] = torch.tensor([bbox])\n\n>>> word_ids = encoding.word_ids(0)\n>>> outputs = model(**encoding)\n>>> loss = outputs.loss\n>>> start_scores = outputs.start_logits\n>>> end_scores = outputs.end_logits\n>>> start, end = word_ids[start_scores.argmax(-1)], word_ids[end_scores.argmax(-1)]\n>>> print(\" \".join(words[start : end + 1]))\nM. Hamann P. Harper, P. Martinez\n```", "```py\n>>> from transformers import AutoTokenizer, TFLayoutLMModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n>>> model = TFLayoutLMModel.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n>>> words = [\"Hello\", \"world\"]\n>>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n\n>>> token_boxes = []\n>>> for word, box in zip(words, normalized_word_boxes):\n...     word_tokens = tokenizer.tokenize(word)\n...     token_boxes.extend([box] * len(word_tokens))\n>>> # add bounding boxes of cls + sep tokens\n>>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n\n>>> encoding = tokenizer(\" \".join(words), return_tensors=\"tf\")\n>>> input_ids = encoding[\"input_ids\"]\n>>> attention_mask = encoding[\"attention_mask\"]\n>>> token_type_ids = encoding[\"token_type_ids\"]\n>>> bbox = tf.convert_to_tensor([token_boxes])\n\n>>> outputs = model(\n...     input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids\n... )\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, TFLayoutLMForMaskedLM\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n>>> model = TFLayoutLMForMaskedLM.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n>>> words = [\"Hello\", \"[MASK]\"]\n>>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n\n>>> token_boxes = []\n>>> for word, box in zip(words, normalized_word_boxes):\n...     word_tokens = tokenizer.tokenize(word)\n...     token_boxes.extend([box] * len(word_tokens))\n>>> # add bounding boxes of cls + sep tokens\n>>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n\n>>> encoding = tokenizer(\" \".join(words), return_tensors=\"tf\")\n>>> input_ids = encoding[\"input_ids\"]\n>>> attention_mask = encoding[\"attention_mask\"]\n>>> token_type_ids = encoding[\"token_type_ids\"]\n>>> bbox = tf.convert_to_tensor([token_boxes])\n\n>>> labels = tokenizer(\"Hello world\", return_tensors=\"tf\")[\"input_ids\"]\n\n>>> outputs = model(\n...     input_ids=input_ids,\n...     bbox=bbox,\n...     attention_mask=attention_mask,\n...     token_type_ids=token_type_ids,\n...     labels=labels,\n... )\n\n>>> loss = outputs.loss\n```", "```py\n>>> from transformers import AutoTokenizer, TFLayoutLMForSequenceClassification\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n>>> model = TFLayoutLMForSequenceClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n>>> words = [\"Hello\", \"world\"]\n>>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n\n>>> token_boxes = []\n>>> for word, box in zip(words, normalized_word_boxes):\n...     word_tokens = tokenizer.tokenize(word)\n...     token_boxes.extend([box] * len(word_tokens))\n>>> # add bounding boxes of cls + sep tokens\n>>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n\n>>> encoding = tokenizer(\" \".join(words), return_tensors=\"tf\")\n>>> input_ids = encoding[\"input_ids\"]\n>>> attention_mask = encoding[\"attention_mask\"]\n>>> token_type_ids = encoding[\"token_type_ids\"]\n>>> bbox = tf.convert_to_tensor([token_boxes])\n>>> sequence_label = tf.convert_to_tensor([1])\n\n>>> outputs = model(\n...     input_ids=input_ids,\n...     bbox=bbox,\n...     attention_mask=attention_mask,\n...     token_type_ids=token_type_ids,\n...     labels=sequence_label,\n... )\n\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n>>> import tensorflow as tf\n>>> from transformers import AutoTokenizer, TFLayoutLMForTokenClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n>>> model = TFLayoutLMForTokenClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n>>> words = [\"Hello\", \"world\"]\n>>> normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n\n>>> token_boxes = []\n>>> for word, box in zip(words, normalized_word_boxes):\n...     word_tokens = tokenizer.tokenize(word)\n...     token_boxes.extend([box] * len(word_tokens))\n>>> # add bounding boxes of cls + sep tokens\n>>> token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n\n>>> encoding = tokenizer(\" \".join(words), return_tensors=\"tf\")\n>>> input_ids = encoding[\"input_ids\"]\n>>> attention_mask = encoding[\"attention_mask\"]\n>>> token_type_ids = encoding[\"token_type_ids\"]\n>>> bbox = tf.convert_to_tensor([token_boxes])\n>>> token_labels = tf.convert_to_tensor([1, 1, 0, 0])\n\n>>> outputs = model(\n...     input_ids=input_ids,\n...     bbox=bbox,\n...     attention_mask=attention_mask,\n...     token_type_ids=token_type_ids,\n...     labels=token_labels,\n... )\n\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n>>> import tensorflow as tf\n>>> from transformers import AutoTokenizer, TFLayoutLMForQuestionAnswering\n>>> from datasets import load_dataset\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"impira/layoutlm-document-qa\", add_prefix_space=True)\n>>> model = TFLayoutLMForQuestionAnswering.from_pretrained(\"impira/layoutlm-document-qa\", revision=\"1e3ebac\")\n\n>>> dataset = load_dataset(\"nielsr/funsd\", split=\"train\")\n>>> example = dataset[0]\n>>> question = \"what's his name?\"\n>>> words = example[\"words\"]\n>>> boxes = example[\"bboxes\"]\n\n>>> encoding = tokenizer(\n...     question.split(), words, is_split_into_words=True, return_token_type_ids=True, return_tensors=\"tf\"\n... )\n>>> bbox = []\n>>> for i, s, w in zip(encoding.input_ids[0], encoding.sequence_ids(0), encoding.word_ids(0)):\n...     if s == 1:\n...         bbox.append(boxes[w])\n...     elif i == tokenizer.sep_token_id:\n...         bbox.append([1000] * 4)\n...     else:\n...         bbox.append([0] * 4)\n>>> encoding[\"bbox\"] = tf.convert_to_tensor([bbox])\n\n>>> word_ids = encoding.word_ids(0)\n>>> outputs = model(**encoding)\n>>> loss = outputs.loss\n>>> start_scores = outputs.start_logits\n>>> end_scores = outputs.end_logits\n>>> start, end = word_ids[tf.math.argmax(start_scores, -1)[0]], word_ids[tf.math.argmax(end_scores, -1)[0]]\n>>> print(\" \".join(words[start : end + 1]))\nM. Hamann P. Harper, P. Martinez\n```"]