- en: Inference pipelines with AWS Neuron (Inf2/Trn1)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/optimum-neuron/guides/pipelines](https://huggingface.co/docs/optimum-neuron/guides/pipelines)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: The `pipeline()` function makes it simple to use models from the [Model Hub](https://huggingface.co/models)
    for accelerated inference on a variety of tasks such as text classification, question
    answering and image classification.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#pipelines)
    function from Transformers and provide your NeurModel model class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently the supported tasks are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`feature-extraction`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fill-mask`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text-classification`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token-classification`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question-answering`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zero-shot-classification`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimum pipeline usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While each task has an associated pipeline class, it is simpler to use the general
    `pipeline()` function which wraps all the task-specific pipelines in one object.
    The `pipeline()` function automatically loads a default model and tokenizer/feature-extractor
    capable of performing inference for your task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by creating a pipeline by specifying an inference task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass your input text/image to the `pipeline()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Note: The default models used in the `pipeline()` function are not optimized
    for inference or quantized, so there won’t be a performance improvement compared
    to their PyTorch counterparts.*'
  prefs: []
  type: TYPE_NORMAL
- en: Using vanilla Transformers model and converting to AWS Neuron
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `pipeline()` function accepts any supported model from the [Hugging Face
    Hub](https://huggingface.co/models). There are tags on the Model Hub that allow
    you to filter for a model you’d like to use for your task.
  prefs: []
  type: TYPE_NORMAL
- en: To be able to load the model with the Neuron Runtime, the export to neuron needs
    to be supported for the considered architecture.
  prefs: []
  type: TYPE_NORMAL
- en: You can check the list of supported architectures [here](../package_reference/configuration#supported-architectures).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have picked an appropriate model, you can create the `pipeline()`
    by specifying the model repo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It is also possible to load it with the `from_pretrained(model_name_or_path,
    export=True)` method associated with the `NeuronModelForXXX` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here is how you can load the `~neuron.NeuronModelForQuestionAnswering`
    class for question answering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Defining Input Shapes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NeuronModels currently require static `input_shapes` to run inference. The default
    input shapes will be used if you are not providing input shapes when providing
    the `export=True` parameter. Below is an example of how to specify the input shapes
    for the sequence length and batch size.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
