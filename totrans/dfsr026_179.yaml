- en: unCLIP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/diffusers/api/pipelines/unclip](https://huggingface.co/docs/diffusers/api/pipelines/unclip)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/83.18cd9e19.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Tip.230e2334.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Docstring.93f6f462.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
  prefs: []
  type: TYPE_NORMAL
- en: '[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125)
    is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The
    unCLIP model in ðŸ¤— Diffusers comes from kakaobrainâ€™s [karlo](https://github.com/kakaobrain/karlo).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Contrastive models like CLIP have been shown to learn robust representations
    of images that capture both semantics and style. To leverage these representations
    for image generation, we propose a two-stage model: a prior that generates a CLIP
    image embedding given a text caption, and a decoder that generates an image conditioned
    on the image embedding. We show that explicitly generating image representations
    improves image diversity with minimal loss in photorealism and caption similarity.
    Our decoders conditioned on image representations can also produce variations
    of an image that preserve both its semantics and style, while varying the non-essential
    details absent from the image representation. Moreover, the joint embedding space
    of CLIP enables language-guided image manipulations in a zero-shot fashion. We
    use diffusion models for the decoder and experiment with both autoregressive and
    diffusion models for the prior, finding that the latter are computationally more
    efficient and produce higher-quality samples.*'
  prefs: []
  type: TYPE_NORMAL
- en: You can find lucidrainsâ€™ DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch).
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: UnCLIPPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.UnCLIPPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip.py#L34)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text_encoder` ([CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection))
    â€” Frozen text-encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    â€” A `CLIPTokenizer` to tokenize text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    â€” The canonical unCLIP prior to approximate the image embedding from the text
    embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_proj` (`UnCLIPTextProjModel`) â€” Utility class to prepare and combine
    the embeddings before they are passed to the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” The decoder to invert the image embedding into an image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`super_res_first` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    â€” Super resolution UNet. Used in all but the last step of the super resolution
    diffusion process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`super_res_last` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    â€” Super resolution UNet. Used in the last step of the super resolution diffusion
    process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_scheduler` (`UnCLIPScheduler`) â€” Scheduler used in the prior denoising
    process (a modified [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_scheduler` (`UnCLIPScheduler`) â€” Scheduler used in the decoder denoising
    process (a modified [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`super_res_scheduler` (`UnCLIPScheduler`) â€” Scheduler used in the super resolution
    denoising process (a modified [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for text-to-image generation using unCLIP.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip.py#L211)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide image generation.
    This can only be left undefined if `text_model_output` and `text_attention_mask`
    is passed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_num_inference_steps` (`int`, *optional*, defaults to 25) â€” The number
    of denoising steps for the prior. More denoising steps usually lead to a higher
    quality image at the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_num_inference_steps` (`int`, *optional*, defaults to 25) â€” The number
    of denoising steps for the decoder. More denoising steps usually lead to a higher
    quality image at the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`super_res_num_inference_steps` (`int`, *optional*, defaults to 7) â€” The number
    of denoising steps for super resolution. More denoising steps usually lead to
    a higher quality image at the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_latents` (`torch.FloatTensor` of shape (batch size, embeddings dimension),
    *optional*) â€” Pre-generated noisy latents to be used as inputs for the prior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_latents` (`torch.FloatTensor` of shape (batch size, channels, height,
    width), *optional*) â€” Pre-generated noisy latents to be used as inputs for the
    decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`super_res_latents` (`torch.FloatTensor` of shape (batch size, channels, super
    res height, super res width), *optional*) â€” Pre-generated noisy latents to be
    used as inputs for the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_guidance_scale` (`float`, *optional*, defaults to 4.0) â€” A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_guidance_scale` (`float`, *optional*, defaults to 4.0) â€” A higher
    guidance scale value encourages the model to generate images closely linked to
    the text `prompt` at the expense of lower image quality. Guidance scale is enabled
    when `guidance_scale > 1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_model_output` (`CLIPTextModelOutput`, *optional*) â€” Pre-defined `CLIPTextModel`
    outputs that can be derived from the text encoder. Pre-defined text outputs can
    be passed for tasks like text embedding interpolations. Make sure to also pass
    `text_attention_mask` in this case. `prompt` can the be left `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_attention_mask` (`torch.Tensor`, *optional*) â€” Pre-defined CLIP text
    attention mask that can be derived from the tokenizer. Pre-defined text attention
    masks are necessary when passing `text_model_output`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If `return_dict` is `True`, [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images.
  prefs: []
  type: TYPE_NORMAL
- en: The call function to the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: UnCLIPImageVariationPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.UnCLIPImageVariationPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py#L39)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text_encoder` ([CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection))
    â€” Frozen text-encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    â€” A `CLIPTokenizer` to tokenize text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor))
    â€” Model that extracts features from generated images to be used as inputs for
    the `image_encoder`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_encoder` ([CLIPVisionModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection))
    â€” Frozen CLIP image-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_proj` (`UnCLIPTextProjModel`) â€” Utility class to prepare and combine
    the embeddings before they are passed to the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” The decoder to invert the image embedding into an image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`super_res_first` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    â€” Super resolution UNet. Used in all but the last step of the super resolution
    diffusion process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`super_res_last` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    â€” Super resolution UNet. Used in the last step of the super resolution diffusion
    process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_scheduler` (`UnCLIPScheduler`) â€” Scheduler used in the decoder denoising
    process (a modified [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`super_res_scheduler` (`UnCLIPScheduler`) â€” Scheduler used in the super resolution
    denoising process (a modified [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline to generate image variations from an input image using UnCLIP.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py#L199)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`image` (`PIL.Image.Image` or `List[PIL.Image.Image]` or `torch.FloatTensor`)
    â€” `Image` or tensor representing an image batch to be used as the starting point.
    If you provide a tensor, it needs to be compatible with the `CLIPImageProcessor`
    [configuration](https://huggingface.co/fusing/karlo-image-variations-diffusers/blob/main/feature_extractor/preprocessor_config.json).
    Can be left as `None` only when `image_embeddings` are passed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_num_inference_steps` (`int`, *optional*, defaults to 25) â€” The number
    of denoising steps for the decoder. More denoising steps usually lead to a higher
    quality image at the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`super_res_num_inference_steps` (`int`, *optional*, defaults to 7) â€” The number
    of denoising steps for super resolution. More denoising steps usually lead to
    a higher quality image at the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator`, *optional*) â€” A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_latents` (`torch.FloatTensor` of shape (batch size, channels, height,
    width), *optional*) â€” Pre-generated noisy latents to be used as inputs for the
    decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`super_res_latents` (`torch.FloatTensor` of shape (batch size, channels, super
    res height, super res width), *optional*) â€” Pre-generated noisy latents to be
    used as inputs for the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_guidance_scale` (`float`, *optional*, defaults to 4.0) â€” A higher
    guidance scale value encourages the model to generate images closely linked to
    the text `prompt` at the expense of lower image quality. Guidance scale is enabled
    when `guidance_scale > 1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_embeddings` (`torch.Tensor`, *optional*) â€” Pre-defined image embeddings
    that can be derived from the image encoder. Pre-defined image embeddings can be
    passed for tasks like image interpolations. `image` can be left as `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If `return_dict` is `True`, [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images.
  prefs: []
  type: TYPE_NORMAL
- en: The call function to the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: ImagePipelineOutput
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.ImagePipelineOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L116)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`List[PIL.Image.Image]` or `np.ndarray`) â€” List of denoised PIL images
    of length `batch_size` or NumPy array of shape `(batch_size, height, width, num_channels)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output class for image pipelines.
  prefs: []
  type: TYPE_NORMAL
