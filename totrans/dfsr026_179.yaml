- en: unCLIP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: unCLIP
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/unclip](https://huggingface.co/docs/diffusers/api/pipelines/unclip)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/api/pipelines/unclip](https://huggingface.co/docs/diffusers/api/pipelines/unclip)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125)
    is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The
    unCLIP model in ğŸ¤— Diffusers comes from kakaobrainâ€™s [karlo](https://github.com/kakaobrain/karlo).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…·æœ‰CLIPæ½œåœ¨ç‰¹å¾çš„åˆ†å±‚æ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆ](https://huggingface.co/papers/2204.06125) æ˜¯ç”±Aditya
    Rameshï¼ŒPrafulla Dhariwalï¼ŒAlex Nicholï¼ŒCasey Chuï¼ŒMark Chenæ’°å†™çš„ã€‚ğŸ¤— Diffusersä¸­çš„unCLIPæ¨¡å‹æ¥è‡ªkakaobrainçš„[karlo](https://github.com/kakaobrain/karlo)ã€‚'
- en: 'The abstract from the paper is following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*Contrastive models like CLIP have been shown to learn robust representations
    of images that capture both semantics and style. To leverage these representations
    for image generation, we propose a two-stage model: a prior that generates a CLIP
    image embedding given a text caption, and a decoder that generates an image conditioned
    on the image embedding. We show that explicitly generating image representations
    improves image diversity with minimal loss in photorealism and caption similarity.
    Our decoders conditioned on image representations can also produce variations
    of an image that preserve both its semantics and style, while varying the non-essential
    details absent from the image representation. Moreover, the joint embedding space
    of CLIP enables language-guided image manipulations in a zero-shot fashion. We
    use diffusion models for the decoder and experiment with both autoregressive and
    diffusion models for the prior, finding that the latter are computationally more
    efficient and produce higher-quality samples.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*åƒCLIPè¿™æ ·çš„å¯¹æ¯”æ¨¡å‹å·²ç»è¢«è¯æ˜å¯ä»¥å­¦ä¹ åˆ°æ•æ‰è¯­ä¹‰å’Œé£æ ¼çš„å›¾åƒçš„ç¨³å¥è¡¨ç¤ºã€‚ä¸ºäº†åˆ©ç”¨è¿™äº›è¡¨ç¤ºæ¥è¿›è¡Œå›¾åƒç”Ÿæˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µæ¨¡å‹ï¼šä¸€ä¸ªå…ˆéªŒæ¨¡å‹æ ¹æ®æ–‡æœ¬æ ‡é¢˜ç”Ÿæˆä¸€ä¸ªCLIPå›¾åƒåµŒå…¥ï¼Œä¸€ä¸ªè§£ç å™¨æ ¹æ®å›¾åƒåµŒå…¥ç”Ÿæˆä¸€ä¸ªå›¾åƒã€‚æˆ‘ä»¬å±•ç¤ºäº†æ˜ç¡®ç”Ÿæˆå›¾åƒè¡¨ç¤ºå¯ä»¥æé«˜å›¾åƒå¤šæ ·æ€§ï¼ŒåŒæ—¶æœ€å°åŒ–é€¼çœŸåº¦å’Œæ ‡é¢˜ç›¸ä¼¼æ€§çš„æŸå¤±ã€‚æˆ‘ä»¬çš„è§£ç å™¨æ ¹æ®å›¾åƒè¡¨ç¤ºå¯ä»¥äº§ç”Ÿä¿ç•™å…¶è¯­ä¹‰å’Œé£æ ¼çš„å›¾åƒå˜ä½“ï¼ŒåŒæ—¶å˜åŒ–éå¿…è¦çš„ç»†èŠ‚ï¼Œè¿™äº›ç»†èŠ‚åœ¨å›¾åƒè¡¨ç¤ºä¸­ä¸å­˜åœ¨ã€‚æ­¤å¤–ï¼ŒCLIPçš„è”åˆåµŒå…¥ç©ºé—´ä½¿å¾—å¯ä»¥é€šè¿‡è¯­è¨€å¼•å¯¼çš„å›¾åƒæ“ä½œä»¥é›¶æ ·æœ¬çš„æ–¹å¼è¿›è¡Œã€‚æˆ‘ä»¬ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè§£ç å™¨ï¼Œå¹¶å°è¯•å…ˆéªŒæ¨¡å‹ä½¿ç”¨è‡ªå›å½’æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œå‘ç°åè€…åœ¨è®¡ç®—ä¸Šæ›´æœ‰æ•ˆï¼Œå¹¶äº§ç”Ÿæ›´é«˜è´¨é‡çš„æ ·æœ¬ã€‚*'
- en: You can find lucidrainsâ€™ DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨[lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch)æ‰¾åˆ°lucidrainsçš„DALL-E
    2é‡å»ºã€‚
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿æŸ¥çœ‹è°ƒåº¦ç¨‹åºæŒ‡å—ä»¥äº†è§£å¦‚ä½•åœ¨è°ƒåº¦ç¨‹åºé€Ÿåº¦å’Œè´¨é‡ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œå¹¶æŸ¥çœ‹é‡ç”¨ç»„ä»¶è·¨ç®¡é“éƒ¨åˆ†ä»¥äº†è§£å¦‚ä½•æœ‰æ•ˆåœ°å°†ç›¸åŒç»„ä»¶åŠ è½½åˆ°å¤šä¸ªç®¡é“ä¸­ã€‚
- en: UnCLIPPipeline
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UnCLIPPipeline
- en: '### `class diffusers.UnCLIPPipeline`'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.UnCLIPPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip.py#L34)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip.py#L34)'
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text_encoder` ([CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection))
    â€” Frozen text-encoder.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` ([CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection))
    â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚'
- en: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    â€” A `CLIPTokenizer` to tokenize text.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    â€” ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„ `CLIPTokenizer`ã€‚'
- en: '`prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    â€” The canonical unCLIP prior to approximate the image embedding from the text
    embedding.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    â€” ç”¨äºä»æ–‡æœ¬åµŒå…¥è¿‘ä¼¼å›¾åƒåµŒå…¥çš„è§„èŒƒunCLIPå…ˆéªŒã€‚'
- en: '`text_proj` (`UnCLIPTextProjModel`) â€” Utility class to prepare and combine
    the embeddings before they are passed to the decoder.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_proj` (`UnCLIPTextProjModel`) â€” åœ¨ä¼ é€’ç»™è§£ç å™¨ä¹‹å‰å‡†å¤‡å’Œç»„åˆåµŒå…¥çš„å®ç”¨ç±»ã€‚'
- en: '`decoder` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” The decoder to invert the image embedding into an image.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” å°†å›¾åƒåµŒå…¥åè½¬ä¸ºå›¾åƒçš„è§£ç å™¨ã€‚'
- en: '`super_res_first` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    â€” Super resolution UNet. Used in all but the last step of the super resolution
    diffusion process.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_first` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    â€” è¶…åˆ†è¾¨ç‡UNetã€‚ç”¨äºè¶…åˆ†è¾¨ç‡æ‰©æ•£è¿‡ç¨‹çš„é™¤æœ€åä¸€æ­¥ä¹‹å¤–çš„æ‰€æœ‰æ­¥éª¤ã€‚'
- en: '`super_res_last` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    â€” Super resolution UNet. Used in the last step of the super resolution diffusion
    process.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_last` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    â€” è¶…åˆ†è¾¨ç‡UNetã€‚ç”¨äºè¶…åˆ†è¾¨ç‡æ‰©æ•£è¿‡ç¨‹çš„æœ€åä¸€æ­¥ã€‚'
- en: '`prior_scheduler` (`UnCLIPScheduler`) â€” Scheduler used in the prior denoising
    process (a modified [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_scheduler` (`UnCLIPScheduler`) â€” ç”¨äºå…ˆéªŒå»å™ªè¿‡ç¨‹çš„è°ƒåº¦ç¨‹åºï¼ˆä¿®æ”¹çš„[DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)ï¼‰ã€‚'
- en: '`decoder_scheduler` (`UnCLIPScheduler`) â€” Scheduler used in the decoder denoising
    process (a modified [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_scheduler` (`UnCLIPScheduler`) â€” ç”¨äºè§£ç å™¨å»å™ªè¿‡ç¨‹çš„è°ƒåº¦ç¨‹åºï¼ˆä¿®æ”¹çš„[DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)ï¼‰ã€‚'
- en: '`super_res_scheduler` (`UnCLIPScheduler`) â€” Scheduler used in the super resolution
    denoising process (a modified [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_scheduler` (`UnCLIPScheduler`) â€” ç”¨äºè¶…åˆ†è¾¨ç‡å»å™ªè¿‡ç¨‹çš„è°ƒåº¦ç¨‹åºï¼ˆä¿®æ”¹çš„[DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)ï¼‰ã€‚'
- en: Pipeline for text-to-image generation using unCLIP.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ unCLIP è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç®¡é“ã€‚
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–æ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚
- en: '#### `__call__`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip.py#L211)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip.py#L211)'
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide image generation.
    This can only be left undefined if `text_model_output` and `text_attention_mask`
    is passed.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` æˆ– `List[str]`) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºã€‚åªæœ‰åœ¨ä¼ é€’ `text_model_output` å’Œ `text_attention_mask`
    æ—¶æ‰èƒ½å°†å…¶ç•™ç©ºã€‚'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *optional*, é»˜è®¤ä¸º1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚'
- en: '`prior_num_inference_steps` (`int`, *optional*, defaults to 25) â€” The number
    of denoising steps for the prior. More denoising steps usually lead to a higher
    quality image at the expense of slower inference.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_num_inference_steps` (`int`, *optional*, é»˜è®¤ä¸º25) â€” å…ˆéªŒçš„å»å™ªæ­¥éª¤æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚'
- en: '`decoder_num_inference_steps` (`int`, *optional*, defaults to 25) â€” The number
    of denoising steps for the decoder. More denoising steps usually lead to a higher
    quality image at the expense of slower inference.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_num_inference_steps` (`int`, *optional*, é»˜è®¤ä¸º25) â€” è§£ç å™¨çš„å»å™ªæ­¥éª¤æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚'
- en: '`super_res_num_inference_steps` (`int`, *optional*, defaults to 7) â€” The number
    of denoising steps for super resolution. More denoising steps usually lead to
    a higher quality image at the expense of slower inference.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_num_inference_steps` (`int`, *optional*, é»˜è®¤ä¸º7) â€” è¶…åˆ†è¾¨ç‡å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *optional*) â€” ç”¨äºä½¿ç”Ÿæˆç»“æœç¡®å®šæ€§çš„
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)ã€‚'
- en: '`prior_latents` (`torch.FloatTensor` of shape (batch size, embeddings dimension),
    *optional*) â€” Pre-generated noisy latents to be used as inputs for the prior.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_latents` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º (batch size, embeddings dimension)ï¼Œ*optional*)
    â€” é¢„å…ˆç”Ÿæˆçš„å˜ˆæ‚æ½œå˜é‡ï¼Œç”¨ä½œå…ˆéªŒçš„è¾“å…¥ã€‚'
- en: '`decoder_latents` (`torch.FloatTensor` of shape (batch size, channels, height,
    width), *optional*) â€” Pre-generated noisy latents to be used as inputs for the
    decoder.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_latents` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º (batch size, channels, height, width)ï¼Œ*optional*)
    â€” é¢„å…ˆç”Ÿæˆçš„å˜ˆæ‚æ½œå˜é‡ï¼Œç”¨ä½œè§£ç å™¨çš„è¾“å…¥ã€‚'
- en: '`super_res_latents` (`torch.FloatTensor` of shape (batch size, channels, super
    res height, super res width), *optional*) â€” Pre-generated noisy latents to be
    used as inputs for the decoder.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_latents` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º (batch size, channels, super res
    height, super res width)ï¼Œ*optional*) â€” é¢„å…ˆç”Ÿæˆçš„å˜ˆæ‚æ½œå˜é‡ï¼Œç”¨ä½œè§£ç å™¨çš„è¾“å…¥ã€‚'
- en: '`prior_guidance_scale` (`float`, *optional*, defaults to 4.0) â€” A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_guidance_scale` (`float`, *optional*, é»˜è®¤ä¸º4.0) â€” æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬ `prompt`
    å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å½“ `guidance_scale > 1` æ—¶å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚'
- en: '`decoder_guidance_scale` (`float`, *optional*, defaults to 4.0) â€” A higher
    guidance scale value encourages the model to generate images closely linked to
    the text `prompt` at the expense of lower image quality. Guidance scale is enabled
    when `guidance_scale > 1`.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_guidance_scale` (`float`, *optional*, é»˜è®¤ä¸º4.0) â€” æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬
    `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å½“ `guidance_scale > 1` æ—¶å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚'
- en: '`text_model_output` (`CLIPTextModelOutput`, *optional*) â€” Pre-defined `CLIPTextModel`
    outputs that can be derived from the text encoder. Pre-defined text outputs can
    be passed for tasks like text embedding interpolations. Make sure to also pass
    `text_attention_mask` in this case. `prompt` can the be left `None`.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_model_output` (`CLIPTextModelOutput`, *optional*) â€” å¯ä»æ–‡æœ¬ç¼–ç å™¨æ´¾ç”Ÿçš„é¢„å®šä¹‰ `CLIPTextModel`
    è¾“å‡ºã€‚é¢„å®šä¹‰çš„æ–‡æœ¬è¾“å‡ºå¯ä»¥ç”¨äºè¯¸å¦‚æ–‡æœ¬åµŒå…¥æ’å€¼ä¹‹ç±»çš„ä»»åŠ¡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç¡®ä¿è¿˜ä¼ é€’ `text_attention_mask`ã€‚`prompt` å¯ä»¥ç•™ç©ºã€‚'
- en: '`text_attention_mask` (`torch.Tensor`, *optional*) â€” Pre-defined CLIP text
    attention mask that can be derived from the tokenizer. Pre-defined text attention
    masks are necessary when passing `text_model_output`.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_attention_mask` (`torch.Tensor`, *optional*) â€” é¢„å®šä¹‰çš„ CLIP æ–‡æœ¬æ³¨æ„åŠ›æ©ç ï¼Œå¯ä»¥ä»åˆ†è¯å™¨ä¸­æ´¾ç”Ÿã€‚å½“ä¼ é€’
    `text_model_output` æ—¶ï¼Œé¢„å®šä¹‰çš„æ–‡æœ¬æ³¨æ„åŠ›æ©ç æ˜¯å¿…è¦çš„ã€‚'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, é»˜è®¤ä¸º`"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚é€‰æ‹© `PIL.Image` æˆ–
    `np.array` ä¹‹é—´çš„ä¸€ä¸ªã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å› [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„ tupleã€‚'
- en: Returns
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    æˆ– `tuple`'
- en: If `return_dict` is `True`, [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ `return_dict` ä¸º `True`ï¼Œåˆ™è¿”å› [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)ï¼Œå¦åˆ™è¿”å›ä¸€ä¸ª
    `tuple`ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯åŒ…å«ç”Ÿæˆå›¾åƒçš„åˆ—è¡¨ã€‚
- en: The call function to the pipeline for generation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºç”Ÿæˆçš„ç®¡é“çš„è°ƒç”¨å‡½æ•°ã€‚
- en: UnCLIPImageVariationPipeline
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UnCLIPImageVariationPipeline
- en: '### `class diffusers.UnCLIPImageVariationPipeline`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.UnCLIPImageVariationPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py#L39)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py#L39)'
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text_encoder` ([CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection))
    â€” Frozen text-encoder.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder`ï¼ˆ[CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection)ï¼‰â€”
    å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚'
- en: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    â€” A `CLIPTokenizer` to tokenize text.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`ï¼ˆ[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)ï¼‰â€”
    ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„ `CLIPTokenizer`ã€‚'
- en: '`feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor))
    â€” Model that extracts features from generated images to be used as inputs for
    the `image_encoder`.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor`ï¼ˆ[CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)ï¼‰â€”
    ä»ç”Ÿæˆçš„å›¾åƒä¸­æå–ç‰¹å¾çš„æ¨¡å‹ï¼Œç”¨ä½œ `image_encoder` çš„è¾“å…¥ã€‚'
- en: '`image_encoder` ([CLIPVisionModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection))
    â€” Frozen CLIP image-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_encoder`ï¼ˆ[CLIPVisionModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection)ï¼‰â€”
    å†»ç»“çš„ CLIP å›¾åƒç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚'
- en: '`text_proj` (`UnCLIPTextProjModel`) â€” Utility class to prepare and combine
    the embeddings before they are passed to the decoder.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_proj`ï¼ˆ`UnCLIPTextProjModel`ï¼‰â€” åœ¨ä¼ é€’ç»™è§£ç å™¨ä¹‹å‰å‡†å¤‡å’Œç»„åˆåµŒå…¥çš„å®ç”¨ç±»ã€‚'
- en: '`decoder` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” The decoder to invert the image embedding into an image.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder`ï¼ˆ[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)ï¼‰â€”
    å°†å›¾åƒåµŒå…¥åè½¬ä¸ºå›¾åƒçš„è§£ç å™¨ã€‚'
- en: '`super_res_first` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    â€” Super resolution UNet. Used in all but the last step of the super resolution
    diffusion process.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_first`ï¼ˆ[UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel)ï¼‰â€”
    è¶…åˆ†è¾¨ç‡ UNetã€‚ç”¨äºè¶…åˆ†è¾¨ç‡æ‰©æ•£è¿‡ç¨‹ä¸­çš„æ‰€æœ‰æ­¥éª¤ï¼Œé™¤äº†æœ€åä¸€æ­¥ã€‚'
- en: '`super_res_last` ([UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel))
    â€” Super resolution UNet. Used in the last step of the super resolution diffusion
    process.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_last`ï¼ˆ[UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel)ï¼‰â€”
    è¶…åˆ†è¾¨ç‡ UNetã€‚ç”¨äºè¶…åˆ†è¾¨ç‡æ‰©æ•£è¿‡ç¨‹çš„æœ€åä¸€æ­¥ã€‚'
- en: '`decoder_scheduler` (`UnCLIPScheduler`) â€” Scheduler used in the decoder denoising
    process (a modified [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)).'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_scheduler`ï¼ˆ`UnCLIPScheduler`ï¼‰â€” ç”¨äºè§£ç å™¨å»å™ªè¿‡ç¨‹çš„è°ƒåº¦å™¨ï¼ˆä¿®æ”¹è‡ª [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)ï¼‰ã€‚'
- en: '`super_res_scheduler` (`UnCLIPScheduler`) â€” Scheduler used in the super resolution
    denoising process (a modified [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_scheduler`ï¼ˆ`UnCLIPScheduler`ï¼‰â€” ç”¨äºè¶…åˆ†è¾¨ç‡å»å™ªè¿‡ç¨‹çš„è°ƒåº¦å™¨ï¼ˆä¿®æ”¹è‡ª [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)ï¼‰ã€‚'
- en: Pipeline to generate image variations from an input image using UnCLIP.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ UnCLIP ä»è¾“å…¥å›¾åƒç”Ÿæˆå›¾åƒå˜åŒ–çš„æµæ°´çº¿ã€‚
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–æ‰€æœ‰æµæ°´çº¿å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚
- en: '#### `__call__`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py#L199)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py#L199)'
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`image` (`PIL.Image.Image` or `List[PIL.Image.Image]` or `torch.FloatTensor`)
    â€” `Image` or tensor representing an image batch to be used as the starting point.
    If you provide a tensor, it needs to be compatible with the `CLIPImageProcessor`
    [configuration](https://huggingface.co/fusing/karlo-image-variations-diffusers/blob/main/feature_extractor/preprocessor_config.json).
    Can be left as `None` only when `image_embeddings` are passed.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`ï¼ˆ`PIL.Image.Image` æˆ– `List[PIL.Image.Image]` æˆ– `torch.FloatTensor`ï¼‰â€”
    ä»£è¡¨å›¾åƒæ‰¹æ¬¡çš„ `Image` æˆ–å¼ é‡ï¼Œç”¨ä½œèµ·å§‹ç‚¹ã€‚å¦‚æœæä¾›å¼ é‡ï¼Œåˆ™éœ€è¦ä¸ `CLIPImageProcessor` çš„ [é…ç½®](https://huggingface.co/fusing/karlo-image-variations-diffusers/blob/main/feature_extractor/preprocessor_config.json)
    å…¼å®¹ã€‚åªæœ‰åœ¨ä¼ é€’ `image_embeddings` æ—¶æ‰èƒ½ä¿æŒä¸º `None`ã€‚'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 1ï¼‰â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚'
- en: '`decoder_num_inference_steps` (`int`, *optional*, defaults to 25) â€” The number
    of denoising steps for the decoder. More denoising steps usually lead to a higher
    quality image at the expense of slower inference.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_num_inference_steps`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 25ï¼‰â€” è§£ç å™¨çš„å»å™ªæ­¥éª¤æ•°ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚'
- en: '`super_res_num_inference_steps` (`int`, *optional*, defaults to 7) â€” The number
    of denoising steps for super resolution. More denoising steps usually lead to
    a higher quality image at the expense of slower inference.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_num_inference_steps`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 7ï¼‰â€” è¶…åˆ†è¾¨ç‡çš„å»å™ªæ­¥éª¤æ•°ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚'
- en: '`generator` (`torch.Generator`, *optional*) â€” A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator`ï¼ˆ`torch.Generator`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§çš„ [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)ã€‚'
- en: '`decoder_latents` (`torch.FloatTensor` of shape (batch size, channels, height,
    width), *optional*) â€” Pre-generated noisy latents to be used as inputs for the
    decoder.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_latents`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸ºï¼ˆæ‰¹é‡å¤§å°ï¼Œé€šé“æ•°ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰ï¼Œ*å¯é€‰*ï¼‰â€” é¢„å…ˆç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œç”¨ä½œè§£ç å™¨çš„è¾“å…¥ã€‚'
- en: '`super_res_latents` (`torch.FloatTensor` of shape (batch size, channels, super
    res height, super res width), *optional*) â€” Pre-generated noisy latents to be
    used as inputs for the decoder.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super_res_latents` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º(batch size, channels, super res
    height, super res width)ï¼Œ*optional*) â€” é¢„ç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œç”¨ä½œè§£ç å™¨çš„è¾“å…¥ã€‚'
- en: '`decoder_guidance_scale` (`float`, *optional*, defaults to 4.0) â€” A higher
    guidance scale value encourages the model to generate images closely linked to
    the text `prompt` at the expense of lower image quality. Guidance scale is enabled
    when `guidance_scale > 1`.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_guidance_scale` (`float`, *optional*, é»˜è®¤å€¼ä¸º4.0) â€” è¾ƒé«˜çš„å¼•å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚
    å½“`guidance_scale > 1`æ—¶å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚'
- en: '`image_embeddings` (`torch.Tensor`, *optional*) â€” Pre-defined image embeddings
    that can be derived from the image encoder. Pre-defined image embeddings can be
    passed for tasks like image interpolations. `image` can be left as `None`.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeddings` (`torch.Tensor`, *optional*) â€” å¯ä»å›¾åƒç¼–ç å™¨æ´¾ç”Ÿçš„é¢„å®šä¹‰å›¾åƒåµŒå…¥ã€‚ å¯ä»¥ä¼ é€’é¢„å®šä¹‰çš„å›¾åƒåµŒå…¥ä»¥ç”¨äºè¯¸å¦‚å›¾åƒæ’å€¼ä¹‹ç±»çš„ä»»åŠ¡ã€‚
    `image` å¯ä»¥ä¿æŒä¸º `None`ã€‚'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, é»˜è®¤å€¼ä¸º`"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚ é€‰æ‹©`PIL.Image`æˆ–`np.array`ä¹‹é—´ã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, é»˜è®¤å€¼ä¸º`True`) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: Returns
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    æˆ– `tuple`'
- en: If `return_dict` is `True`, [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ`return_dict`ä¸º`True`ï¼Œåˆ™è¿”å›[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)ï¼Œå¦åˆ™è¿”å›ä¸€ä¸ªå…ƒç»„ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯åŒ…å«ç”Ÿæˆå›¾åƒçš„åˆ—è¡¨ã€‚
- en: The call function to the pipeline for generation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºç”Ÿæˆçš„ç®¡é“çš„è°ƒç”¨å‡½æ•°ã€‚
- en: ImagePipelineOutput
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImagePipelineOutput
- en: '### `class diffusers.ImagePipelineOutput`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.ImagePipelineOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L116)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L116)'
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`images` (`List[PIL.Image.Image]` or `np.ndarray`) â€” List of denoised PIL images
    of length `batch_size` or NumPy array of shape `(batch_size, height, width, num_channels)`.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`List[PIL.Image.Image]` æˆ– `np.ndarray`) â€” é•¿åº¦ä¸º`batch_size`çš„å»å™ªPILå›¾åƒåˆ—è¡¨æˆ–å½¢çŠ¶ä¸º`(batch_size,
    height, width, num_channels)`çš„NumPyæ•°ç»„ã€‚'
- en: Output class for image pipelines.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒç®¡é“çš„è¾“å‡ºç±»ã€‚
