- en: Tokenizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/tokenizers/index](https://huggingface.co/docs/tokenizers/index)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Fast State-of-the-art tokenizers, optimized for both research and production
  prefs: []
  type: TYPE_NORMAL
- en: '[ðŸ¤— Tokenizers](https://github.com/huggingface/tokenizers) provides an implementation
    of todayâ€™s most used tokenizers, with a focus on performance and versatility.
    These tokenizers are also used in [ðŸ¤— Transformers](https://github.com/huggingface/transformers).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Main features:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Train new vocabularies and tokenize, using todayâ€™s most used tokenizers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extremely fast (both training and tokenization), thanks to the Rust implementation.
    Takes less than 20 seconds to tokenize a GB of text on a serverâ€™s CPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to use, but also extremely versatile.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designed for both research and production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Full alignment tracking. Even with destructive normalization, itâ€™s always possible
    to get the part of the original sentence that corresponds to any token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Does all the pre-processing: Truncation, Padding, add the special tokens your
    model needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
