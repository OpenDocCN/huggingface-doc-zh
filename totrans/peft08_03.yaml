- en: Quicktour
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¿«é€Ÿå¯¼è§ˆ
- en: 'Original text: [https://huggingface.co/docs/peft/quicktour](https://huggingface.co/docs/peft/quicktour)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/peft/quicktour](https://huggingface.co/docs/peft/quicktour)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: PEFT offers parameter-efficient methods for finetuning large pretrained models.
    The traditional paradigm is to finetune all of a modelâ€™s parameters for each downstream
    task, but this is becoming exceedingly costly and impractical because of the enormous
    number of parameters in models today. Instead, it is more efficient to train a
    smaller number of prompt parameters or use a reparametrization method like low-rank
    adaptation (LoRA) to reduce the number of trainable parameters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: PEFTæä¾›äº†ç”¨äºå¾®è°ƒå¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°é«˜æ•ˆæ–¹æ³•ã€‚ä¼ ç»ŸèŒƒå¼æ˜¯ä¸ºæ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œä½†ç”±äºå½“ä»Šæ¨¡å‹ä¸­å‚æ•°æ•°é‡å·¨å¤§ï¼Œè¿™ç§æ–¹æ³•å˜å¾—æå…¶æ˜‚è´µå’Œä¸åˆ‡å®é™…ã€‚ç›¸åï¼Œæ›´æœ‰æ•ˆçš„æ–¹æ³•æ˜¯è®­ç»ƒè¾ƒå°‘æ•°é‡çš„æç¤ºå‚æ•°æˆ–ä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ç­‰é‡æ–°å‚æ•°åŒ–æ–¹æ³•æ¥å‡å°‘å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚
- en: This quicktour will show you PEFTâ€™s main features and how you can train or run
    inference on large models that would typically be inaccessible on consumer devices.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå¿«é€Ÿå¯¼è§ˆå°†å±•ç¤ºPEFTçš„ä¸»è¦ç‰¹ç‚¹ï¼Œä»¥åŠæ‚¨å¦‚ä½•è®­ç»ƒæˆ–åœ¨é€šå¸¸æ— æ³•åœ¨æ¶ˆè´¹è€…è®¾å¤‡ä¸Šè®¿é—®çš„å¤§å‹æ¨¡å‹ä¸Šè¿è¡Œæ¨ç†ã€‚
- en: Train
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒ
- en: 'Each PEFT method is defined by a [PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)
    class that stores all the important parameters for building a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel).
    For example, to train with LoRA, load and create a [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    class and specify the following parameters:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªPEFTæ–¹æ³•éƒ½ç”±ä¸€ä¸ª[PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)ç±»å®šä¹‰ï¼Œè¯¥ç±»å­˜å‚¨æ„å»º[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)æ‰€éœ€çš„æ‰€æœ‰é‡è¦å‚æ•°ã€‚ä¾‹å¦‚ï¼Œè¦ä½¿ç”¨LoRAè¿›è¡Œè®­ç»ƒï¼Œè¯·åŠ è½½å¹¶åˆ›å»ºä¸€ä¸ª[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)ç±»ï¼Œå¹¶æŒ‡å®šä»¥ä¸‹å‚æ•°ï¼š
- en: '`task_type`: the task to train for (sequence-to-sequence language modeling
    in this case)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_type`ï¼šè¦è¿›è¡Œè®­ç»ƒçš„ä»»åŠ¡ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸ºåºåˆ—åˆ°åºåˆ—è¯­è¨€å»ºæ¨¡ï¼‰'
- en: '`inference_mode`: whether youâ€™re using the model for inference or not'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inference_mode`ï¼šæ‚¨æ˜¯å¦åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†'
- en: '`r`: the dimension of the low-rank matrices'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r`ï¼šä½ç§©çŸ©é˜µçš„ç»´åº¦'
- en: '`lora_alpha`: the scaling factor for the low-rank matrices'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_alpha`ï¼šä½ç§©çŸ©é˜µçš„ç¼©æ”¾å› å­'
- en: '`lora_dropout`: the dropout probability of the LoRA layers'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_dropout`ï¼šLoRAå±‚çš„dropoutæ¦‚ç‡'
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: See the [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    reference for more details about other parameters you can adjust, such as the
    modules to target or the bias type.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³å…¶ä»–å¯è°ƒæ•´çš„å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)å‚è€ƒã€‚
- en: Once the [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    is setup, create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    with the [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    function. It takes a base model - which you can load from the Transformers library
    - and the [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)
    containing the parameters for how to configure a model for training with LoRA.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)è®¾ç½®å¥½ï¼Œä½¿ç”¨[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)å‡½æ•°åˆ›å»ºä¸€ä¸ª[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)ã€‚å®ƒéœ€è¦ä¸€ä¸ªåŸºç¡€æ¨¡å‹
    - æ‚¨å¯ä»¥ä»Transformersåº“ä¸­åŠ è½½ - å’ŒåŒ…å«å¦‚ä½•é…ç½®æ¨¡å‹ä»¥ä½¿ç”¨LoRAè¿›è¡Œè®­ç»ƒçš„[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)ã€‚
- en: Load the base model you want to finetune.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½æ‚¨æƒ³è¦å¾®è°ƒçš„åŸºç¡€æ¨¡å‹ã€‚
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Wrap the base model and `peft_config` with the [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    function to create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel).
    To get a sense of the number of trainable parameters in your model, use the `print_trainable_parameters`
    method.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)å‡½æ•°å°†åŸºç¡€æ¨¡å‹å’Œ`peft_config`å°è£…èµ·æ¥ï¼Œåˆ›å»ºä¸€ä¸ª[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)ã€‚ä½¿ç”¨`print_trainable_parameters`æ–¹æ³•æ¥äº†è§£æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Out of [bigscience/mt0-largeâ€™s](https://huggingface.co/bigscience/mt0-large)
    1.2B parameters, youâ€™re only training 0.19% of them!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[bigscience/mt0-large](https://huggingface.co/bigscience/mt0-large)çš„12äº¿å‚æ•°ä¸­ï¼Œæ‚¨åªè®­ç»ƒäº†å…¶ä¸­çš„0.19%ï¼
- en: That is it ğŸ‰! Now you can train the model with the Transformers [Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    Accelerate, or any custom PyTorch training loop.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯è¿™æ ·ğŸ‰ï¼ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨Transformersçš„[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ã€Accelerateæˆ–ä»»ä½•è‡ªå®šä¹‰çš„PyTorchè®­ç»ƒå¾ªç¯æ¥è®­ç»ƒæ¨¡å‹ã€‚
- en: For example, to train with the [Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class, setup a [TrainingArguments](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    class with some training hyperparameters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¦ä½¿ç”¨[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ç±»è¿›è¡Œè®­ç»ƒï¼Œè¯·è®¾ç½®ä¸€ä¸ªå¸¦æœ‰ä¸€äº›è®­ç»ƒè¶…å‚æ•°çš„[TrainingArguments](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ç±»ã€‚
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Pass the model, training arguments, dataset, tokenizer, and any other necessary
    component to the [Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    and call [train](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to start training.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹ã€è®­ç»ƒå‚æ•°ã€æ•°æ®é›†ã€åˆ†è¯å™¨å’Œå…¶ä»–å¿…è¦ç»„ä»¶ä¼ é€’ç»™[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼Œå¹¶è°ƒç”¨[train](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)å¼€å§‹è®­ç»ƒã€‚
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Save model
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¿å­˜æ¨¡å‹
- en: After your model is finished training, you can save your model to a directory
    using the [save_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)
    function.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[save_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)å‡½æ•°å°†æ¨¡å‹ä¿å­˜åˆ°ç›®å½•ä¸­ã€‚
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can also save your model to the Hub (make sure youâ€™re logged in to your
    Hugging Face account first) with the [push_to_hub](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)
    function.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ä½¿ç”¨[push_to_hub](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)å‡½æ•°å°†æ¨¡å‹ä¿å­˜åˆ°Hubï¼ˆè¯·ç¡®ä¿æ‚¨é¦–å…ˆç™»å½•åˆ°æ‚¨çš„Hugging
    Faceå¸æˆ·ï¼‰ã€‚
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Both methods only save the extra PEFT weights that were trained, meaning it
    is super efficient to store, transfer, and load. For example, this [facebook/opt-350m](https://huggingface.co/ybelkada/opt-350m-lora)
    model trained with LoRA only contains two files: `adapter_config.json` and `adapter_model.safetensors`.
    The `adapter_model.safetensors` file is just 6.3MB!'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ç§æ–¹æ³•åªä¿å­˜ç»è¿‡è®­ç»ƒçš„é¢å¤–PEFTæƒé‡ï¼Œè¿™æ„å‘³ç€å­˜å‚¨ã€ä¼ è¾“å’ŒåŠ è½½éå¸¸é«˜æ•ˆã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨LoRAè®­ç»ƒçš„è¿™ä¸ª[facebook/opt-350m](https://huggingface.co/ybelkada/opt-350m-lora)æ¨¡å‹åªåŒ…å«ä¸¤ä¸ªæ–‡ä»¶ï¼š`adapter_config.json`å’Œ`adapter_model.safetensors`ã€‚`adapter_model.safetensors`æ–‡ä»¶åªæœ‰6.3MBï¼
- en: '![](../Images/5e34ae8912ca7fcb5554d98cb511bc58.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e34ae8912ca7fcb5554d98cb511bc58.png)'
- en: The adapter weights for a opt-350m model stored on the Hub are only ~6MB compared
    to the full size of the model weights, which can be ~700MB.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ¨¡å‹æƒé‡çš„å®Œæ•´å¤§å°ç›¸æ¯”ï¼Œå­˜å‚¨åœ¨Hubä¸Šçš„opt-350mæ¨¡å‹çš„é€‚é…å™¨æƒé‡ä»…ä¸ºçº¦6MBï¼Œåè€…å¯èƒ½ä¸ºçº¦700MBã€‚
- en: Inference
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: Take a look at the [AutoPeftModel](package_reference/auto_class) API reference
    for a complete list of available `AutoPeftModel` classes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[AutoPeftModel](package_reference/auto_class) APIå‚è€ƒï¼Œäº†è§£å¯ç”¨çš„`AutoPeftModel`ç±»çš„å®Œæ•´åˆ—è¡¨ã€‚
- en: 'Easily load any PEFT-trained model for inference with the [AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)
    class and the [from_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)ç±»å’Œ[from_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•è½»æ¾åŠ è½½ä»»ä½•ç»è¿‡PEFTè®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼š
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For other tasks that arenâ€™t explicitly supported with an `AutoPeftModelFor`
    class - such as automatic speech recognition - you can still use the base [AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)
    class to load a model for the task.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ²¡æœ‰æ˜ç¡®æ”¯æŒçš„ä»»åŠ¡ï¼Œå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç­‰ï¼Œæ‚¨ä»ç„¶å¯ä»¥ä½¿ç”¨åŸºæœ¬çš„[AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)ç±»åŠ è½½ç”¨äºè¯¥ä»»åŠ¡çš„æ¨¡å‹ã€‚
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next steps
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥
- en: 'Now that youâ€™ve seen how to train a model with one of the PEFT methods, we
    encourage you to try out some of the other methods like prompt tuning. The steps
    are very similar to the ones shown in the quicktour:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å·²ç»çœ‹åˆ°å¦‚ä½•ä½¿ç”¨PEFTæ–¹æ³•ä¹‹ä¸€è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬é¼“åŠ±æ‚¨å°è¯•ä¸€äº›å…¶ä»–æ–¹æ³•ï¼Œå¦‚æç¤ºè°ƒæ•´ã€‚è¿™äº›æ­¥éª¤ä¸å¿«é€Ÿå…¥é—¨ä¸­æ˜¾ç¤ºçš„æ­¥éª¤éå¸¸ç›¸ä¼¼ï¼š
- en: prepare a [PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)
    for a PEFT method
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸ºPEFTæ–¹æ³•å‡†å¤‡ä¸€ä¸ª[PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)
- en: use the [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    method to create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    from the configuration and base model
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)æ–¹æ³•ä»é…ç½®å’ŒåŸºæœ¬æ¨¡å‹åˆ›å»º[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
- en: Then you can train it however you like! To load a PEFT model for inference,
    you can use the [AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)
    class.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ‚¨å¯ä»¥æŒ‰ç…§è‡ªå·±çš„å–œå¥½è¿›è¡Œè®­ç»ƒï¼è¦åŠ è½½ç”¨äºæ¨ç†çš„PEFTæ¨¡å‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)ç±»ã€‚
- en: Feel free to also take a look at the task guides if youâ€™re interested in training
    a model with another PEFT method for a specific task such as semantic segmentation,
    multilingual automatic speech recognition, DreamBooth, token classification, and
    more.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰å…´è¶£ä½¿ç”¨å…¶ä»–PEFTæ–¹æ³•è®­ç»ƒæ¨¡å‹ï¼Œä¾‹å¦‚è¯­ä¹‰åˆ†å‰²ã€å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€DreamBoothã€æ ‡è®°åˆ†ç±»ç­‰ï¼Œè¯·éšæ—¶æŸ¥çœ‹ä»»åŠ¡æŒ‡å—ã€‚
