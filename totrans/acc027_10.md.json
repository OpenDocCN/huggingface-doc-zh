["```py\n- import logging\n+ from accelerate.logging import get_logger\n- logger = logging.getLogger(__name__)\n+ logger = get_logger(__name__)\n```", "```py\nfrom accelerate.logging import get_logger\n\nlogger = get_logger(__name__, log_level=\"INFO\")\n```", "```py\naccelerate launch --debug {my_script.py} --arg1 --arg2\n```", "```py\nACCELERATE_DEBUG_MODE=\"1\" torchrun {my_script.py} --arg1 --arg2\n```", "```py\n compute_environment: LOCAL_MACHINE\n+debug: true\n```", "```py\nTraceback (most recent call last):\n  File \"/home/zach_mueller_huggingface_co/test.py\", line 18, in <module>\n    main()\n  File \"/home/zach_mueller_huggingface_co/test.py\", line 15, in main\n    broadcast_tensor = broadcast(tensor)\n  File \"/home/zach_mueller_huggingface_co/accelerate/src/accelerate/utils/operations.py\", line 303, in wrapper\naccelerate.utils.operations.DistributedOperationException:\n\nCannot apply desired operation due to shape mismatches. All shapes across devices must be valid.\n\nOperation: `accelerate.utils.operations.broadcast`\nInput shapes:\n  - Process 0: [1, 5]\n  - Process 1: [1, 2, 5]\n```", "```py\n# Assume `should_do_breakpoint` is a custom defined function that returns a conditional, \n# and that conditional might be true only on process 1\nif should_do_breakpoint(loss):\n    accelerator.set_breakpoint()\n\n# Later in the training script when we need to check for the breakpoint\nif accelerator.check_breakpoint():\n    break\n```", "```py\ndef training_function(args):\n    accelerator = Accelerator()\n\n+   @find_executable_batch_size(starting_batch_size=args.batch_size)\n+   def inner_training_loop(batch_size):\n+       nonlocal accelerator # Ensure they can be used in our context\n+       accelerator.free_memory() # Free all lingering references\n        model = get_model()\n        model.to(accelerator.device)\n        optimizer = get_optimizer()\n        train_dataloader, eval_dataloader = get_dataloaders(accelerator, batch_size)\n        lr_scheduler = get_scheduler(\n            optimizer, \n            num_training_steps=len(train_dataloader)*num_epochs\n        )\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n            model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n        )\n        train(model, optimizer, train_dataloader, lr_scheduler)\n        validate(model, eval_dataloader)\n+   inner_training_loop()\n```"]