# BLIP-Diffusion

> 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/blip_diffusion](https://huggingface.co/docs/diffusers/api/pipelines/blip_diffusion)

BLIP-Diffusion提出于[BLIP-Diffusion：用于可控文本到图像生成和编辑的预训练主题表示](https://arxiv.org/abs/2305.14720)。它实现了零样本主题驱动生成和控制引导的零样本生成。

论文摘要为：

*主题驱动的文本到图像生成模型根据文本提示创建输入主题的新版本。现有模型存在着冗长的微调和难以保持主题忠实度的困难。为了克服这些限制，我们引入了BLIP-Diffusion，这是一个新的主题驱动图像生成模型，支持多模态控制，消耗主题图像和文本提示的输入。与其他主题驱动生成模型不同，BLIP-Diffusion引入了一个新的多模态编码器，经过预训练以提供主题表示。我们首先按照BLIP-2预训练多模态编码器，以产生与文本对齐的视觉表示。然后我们设计一个主题表示学习任务，使扩散模型能够利用这种视觉表示并生成新的主题版本。与DreamBooth等先前方法相比，我们的模型实现了零样本主题驱动生成，并且对于自定义主题的高效微调可实现高达20倍的加速。我们还展示了BLIP-Diffusion可以灵活地与现有技术结合，如ControlNet和prompt-to-prompt，以实现新的主题驱动生成和编辑应用。项目页面位于[此https URL](https://dxli94.github.io/BLIP-Diffusion-website/)。*

原始代码库可以在[salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion)找到。您可以在[hf.co/SalesForce](https://hf.co/SalesForce)组织下找到官方BLIP-Diffusion检查点。

`BlipDiffusionPipeline`和`BlipDiffusionControlNetPipeline`由[`ayushtues`](https://github.com/ayushtues/)贡献。

请务必查看调度器指南以了解如何在调度器速度和质量之间进行权衡，并查看重用管道中的组件部分，以了解如何有效地将相同组件加载到多个管道中。

## BlipDiffusionPipeline

### `class diffusers.BlipDiffusionPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/blip_diffusion/pipeline_blip_diffusion.py#L75)

```py
( tokenizer: CLIPTokenizer text_encoder: ContextCLIPTextModel vae: AutoencoderKL unet: UNet2DConditionModel scheduler: PNDMScheduler qformer: Blip2QFormerModel image_processor: BlipImageProcessor ctx_begin_pos: int = 2 mean: List = None std: List = None )
```

参数

+   `tokenizer`（`CLIPTokenizer`）- 用于文本编码器的分词器

+   `text_encoder`（`ContextCLIPTextModel`）- 用于编码文本提示的文本编码器

+   `vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL））- VAE模型，将潜变量映射到图像

+   `unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel））- 用于去噪图像嵌入的条件U-Net架构。

+   `scheduler`（[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler））- 与`unet`结合使用的调度器，用于生成图像潜变量。

+   `qformer`（`Blip2QFormerModel`）- 用于从文本和图像获取多模态嵌入的QFormer模型。

+   `image_processor`（`BlipImageProcessor`）- 用于预处理和后处理图像的图像处理器。

+   `ctx_begin_pos`（整数，`可选`，默认为2）- 文本编码器中上下文标记的位置。

使用Blip Diffusion进行零样本主题驱动生成的管道。

此模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以了解库为所有管道实现的通用方法（如下载或保存，运行在特定设备上等）。

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/blip_diffusion/pipeline_blip_diffusion.py#L186)

```py
( prompt: List reference_image: Image source_subject_category: List target_subject_category: List latents: Optional = None guidance_scale: float = 7.5 height: int = 512 width: int = 512 num_inference_steps: int = 50 generator: Union = None neg_prompt: Optional = '' prompt_strength: float = 1.0 prompt_reps: int = 20 output_type: Optional = 'pil' return_dict: bool = True ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt` (`List[str]`) — 指导图像生成的提示或提示。

+   `reference_image` (`PIL.Image.Image`) — 用于条件生成的参考图像。

+   `source_subject_category` (`List[str]`) — 源主题类别。

+   `target_subject_category` (`List[str]`) — 目标主题类别。

+   `latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成的噪声潜变量，用作图像生成的输入。可用于使用不同提示微调相同的生成。如果未提供，则将通过随机抽样生成一个潜变量张量。

+   `guidance_scale` (`float`, *可选*, 默认为7.5) — 如[无分类器扩散指导](https://arxiv.org/abs/2207.12598)中定义的指导比例。`guidance_scale`被定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程式2的`w`。通过设置`guidance_scale > 1`启用指导比例。更高的指导比例鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `height` (`int`, *可选*, 默认为512) — 生成图像的高度。

+   `width` (`int`, *可选*, 默认为512) — 生成图像的宽度。

+   `num_inference_steps` (`int`, *可选*, 默认为50) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个[torch生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)用于使生成过程确定性。

+   `neg_prompt` (`str`, *可选*, 默认为"") — 不指导图像生成的提示或提示。当不使用指导时被忽略（即如果`guidance_scale`小于`1`，则被忽略）。

+   `prompt_strength` (`float`, *可选*, 默认为1.0) — 提示的强度。指定了提示重复的次数以及prompt_reps以放大提示。

+   `prompt_reps` (`int`, *可选*, 默认为20) — 提示重复的次数，以及prompt_strength以放大提示。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。可选择在`"pil"`（`PIL.Image.Image`）、`"np"`（`np.array`）或`"pt"`（`torch.Tensor`）之间选择。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)而不是普通元组。

返回

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) 或 `tuple`

在调用生成管道时调用的函数。

示例：

```py
>>> from diffusers.pipelines import BlipDiffusionPipeline
>>> from diffusers.utils import load_image
>>> import torch

>>> blip_diffusion_pipe = BlipDiffusionPipeline.from_pretrained(
...     "Salesforce/blipdiffusion", torch_dtype=torch.float16
... ).to("cuda")

>>> cond_subject = "dog"
>>> tgt_subject = "dog"
>>> text_prompt_input = "swimming underwater"

>>> cond_image = load_image(
...     "https://huggingface.co/datasets/ayushtues/blipdiffusion_images/resolve/main/dog.jpg"
... )
>>> guidance_scale = 7.5
>>> num_inference_steps = 25
>>> negative_prompt = "over-exposure, under-exposure, saturated, duplicate, out of frame, lowres, cropped, worst quality, low quality, jpeg artifacts, morbid, mutilated, out of frame, ugly, bad anatomy, bad proportions, deformed, blurry, duplicate"

>>> output = blip_diffusion_pipe(
...     text_prompt_input,
...     cond_image,
...     cond_subject,
...     tgt_subject,
...     guidance_scale=guidance_scale,
...     num_inference_steps=num_inference_steps,
...     neg_prompt=negative_prompt,
...     height=512,
...     width=512,
... ).images
>>> output[0].save("image.png")
```

## BlipDiffusionControlNetPipeline

### `class diffusers.BlipDiffusionControlNetPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_blip_diffusion.py#L82)

```py
( tokenizer: CLIPTokenizer text_encoder: ContextCLIPTextModel vae: AutoencoderKL unet: UNet2DConditionModel scheduler: PNDMScheduler qformer: Blip2QFormerModel controlnet: ControlNetModel image_processor: BlipImageProcessor ctx_begin_pos: int = 2 mean: List = None std: List = None )
```

参数

+   `tokenizer` (`CLIPTokenizer`) — 用于文本编码器的分词器

+   `text_encoder` (`ContextCLIPTextModel`) — 用于编码文本提示的文本编码器

+   `vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)) — 将潜变量映射到图像的VAE模型

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) — 用于去噪图像嵌入的条件U-Net架构。

+   `scheduler` ([PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)) — 用于与`unet`结合使用以生成图像潜变量的调度器。

+   `qformer` (`Blip2QFormerModel`) — 用于从文本和图像获取多模态嵌入的QFormer模型。

+   `controlnet` ([ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel)) — 用于获取条件图像嵌入的ControlNet模型。

+   `image_processor` (`BlipImageProcessor`) — 用于预处理和后处理图像的图像处理器。

+   `ctx_begin_pos` (int, `可选`, 默认为2) — 文本编码器中上下文标记的位置。

基于Canny边缘的受控主体驱动生成的Blip扩散管道。

该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档，了解库为所有管道实现的通用方法（如下载或保存，运行在特定设备上等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_blip_diffusion.py#L234)

```py
( prompt: List reference_image: Image condtioning_image: Image source_subject_category: List target_subject_category: List latents: Optional = None guidance_scale: float = 7.5 height: int = 512 width: int = 512 num_inference_steps: int = 50 generator: Union = None neg_prompt: Optional = '' prompt_strength: float = 1.0 prompt_reps: int = 20 output_type: Optional = 'pil' return_dict: bool = True ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt` (`List[str]`) — 用于指导图像生成的提示或提示。

+   `reference_image` (`PIL.Image.Image`) — 用于条件生成的参考图像。

+   `condtioning_image` (`PIL.Image.Image`) — 用于条件生成的条件Canny边缘图像。

+   `source_subject_category` (`List[str]`) — 源主题类别。

+   `target_subject_category` (`List[str]`) — 目标主题类别。

+   `latents` (`torch.FloatTensor`, *可选*) — 预先生成的嘈杂潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，将通过随机抽样生成潜变量张量。

+   `guidance_scale` (`float`, *可选*, 默认为7.5) — 指导比例，如[无分类器扩散指导](https://arxiv.org/abs/2207.12598)中定义。`guidance_scale`定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程2的`w`。通过设置`guidance_scale > 1`启用指导比例。更高的指导比例鼓励生成与文本`prompt`紧密相关的图像，通常以降低图像质量为代价。

+   `height` (`int`, *可选*, 默认为512) — 生成图像的高度。

+   `width` (`int`, *可选*, 默认为512) — 生成图像的宽度。

+   `seed` (`int`, *可选*, 默认为42) — 用于随机生成的种子。

+   `num_inference_steps` (`int`, *可选*, 默认为50) — 降噪步骤的数量。更多的降噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个[torch生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `neg_prompt` (`str`, *可选*, 默认为"") — 不用于指导图像生成的提示或提示。如果不使用指导（即如果`guidance_scale`小于`1`，则忽略）。

+   `prompt_strength` (`float`, *可选*, 默认为1.0) — 提示的强度。指定提示重复的次数，以及prompt_reps以放大提示。

+   `prompt_reps` (`int`, *可选*, 默认为20) — 提示重复的次数，以及prompt_strength以放大提示。

返回

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) 或 `tuple`

调用管道进行生成时调用的函数。

示例：

```py
>>> from diffusers.pipelines import BlipDiffusionControlNetPipeline
>>> from diffusers.utils import load_image
>>> from controlnet_aux import CannyDetector
>>> import torch

>>> blip_diffusion_pipe = BlipDiffusionControlNetPipeline.from_pretrained(
...     "Salesforce/blipdiffusion-controlnet", torch_dtype=torch.float16
... ).to("cuda")

>>> style_subject = "flower"
>>> tgt_subject = "teapot"
>>> text_prompt = "on a marble table"

>>> cldm_cond_image = load_image(
...     "https://huggingface.co/datasets/ayushtues/blipdiffusion_images/resolve/main/kettle.jpg"
... ).resize((512, 512))
>>> canny = CannyDetector()
>>> cldm_cond_image = canny(cldm_cond_image, 30, 70, output_type="pil")
>>> style_image = load_image(
...     "https://huggingface.co/datasets/ayushtues/blipdiffusion_images/resolve/main/flower.jpg"
... )
>>> guidance_scale = 7.5
>>> num_inference_steps = 50
>>> negative_prompt = "over-exposure, under-exposure, saturated, duplicate, out of frame, lowres, cropped, worst quality, low quality, jpeg artifacts, morbid, mutilated, out of frame, ugly, bad anatomy, bad proportions, deformed, blurry, duplicate"

>>> output = blip_diffusion_pipe(
...     text_prompt,
...     style_image,
...     cldm_cond_image,
...     style_subject,
...     tgt_subject,
...     guidance_scale=guidance_scale,
...     num_inference_steps=num_inference_steps,
...     neg_prompt=negative_prompt,
...     height=512,
...     width=512,
... ).images
>>> output[0].save("image.png")
```
