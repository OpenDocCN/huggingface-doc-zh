["```py\n>>> from transformers import BertConfig, Wav2Vec2Config, SpeechEncoderDecoderConfig, SpeechEncoderDecoderModel\n\n>>> config_encoder = Wav2Vec2Config()\n>>> config_decoder = BertConfig()\n\n>>> config = SpeechEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n>>> model = SpeechEncoderDecoderModel(config=config)\n```", "```py\n>>> from transformers import SpeechEncoderDecoderModel\n\n>>> model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(\n...     \"facebook/hubert-large-ll60k\", \"bert-base-uncased\"\n... )\n```", "```py\n>>> from transformers import Wav2Vec2Processor, SpeechEncoderDecoderModel\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> # load a fine-tuned speech translation model and corresponding processor\n>>> model = SpeechEncoderDecoderModel.from_pretrained(\"facebook/wav2vec2-xls-r-300m-en-to-15\")\n>>> processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-xls-r-300m-en-to-15\")\n\n>>> # let's perform inference on a piece of English speech (which we'll translate to German)\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n\n>>> # autoregressively generate transcription (uses greedy decoding by default)\n>>> generated_ids = model.generate(input_values)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> print(generated_text)\nMr. Quilter ist der Apostel der Mittelschicht und wir freuen uns, sein Evangelium willkommen hei\u00dfen zu k\u00f6nnen.\n```", "```py\n>>> from transformers import AutoTokenizer, AutoFeatureExtractor, SpeechEncoderDecoderModel\n>>> from datasets import load_dataset\n\n>>> encoder_id = \"facebook/wav2vec2-base-960h\"  # acoustic model encoder\n>>> decoder_id = \"bert-base-uncased\"  # text decoder\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)\n>>> tokenizer = AutoTokenizer.from_pretrained(decoder_id)\n>>> # Combine pre-trained encoder and pre-trained decoder to form a Seq2Seq model\n>>> model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id)\n\n>>> model.config.decoder_start_token_id = tokenizer.cls_token_id\n>>> model.config.pad_token_id = tokenizer.pad_token_id\n\n>>> # load an audio input and pre-process (normalise mean/std to 0/1)\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> input_values = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n\n>>> # load its corresponding transcription and tokenize to generate labels\n>>> labels = tokenizer(ds[0][\"text\"], return_tensors=\"pt\").input_ids\n\n>>> # the forward function automatically creates the correct decoder_input_ids\n>>> loss = model(input_values=input_values, labels=labels).loss\n>>> loss.backward()\n```", "```py\n>>> from transformers import BertConfig, Wav2Vec2Config, SpeechEncoderDecoderConfig, SpeechEncoderDecoderModel\n\n>>> # Initializing a Wav2Vec2 & BERT style configuration\n>>> config_encoder = Wav2Vec2Config()\n>>> config_decoder = BertConfig()\n\n>>> config = SpeechEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n\n>>> # Initializing a Wav2Vec2Bert model from a Wav2Vec2 & bert-base-uncased style configurations\n>>> model = SpeechEncoderDecoderModel(config=config)\n\n>>> # Accessing the model configuration\n>>> config_encoder = model.config.encoder\n>>> config_decoder = model.config.decoder\n>>> # set decoder config to causal lm\n>>> config_decoder.is_decoder = True\n>>> config_decoder.add_cross_attention = True\n\n>>> # Saving the model, including its configuration\n>>> model.save_pretrained(\"my-model\")\n\n>>> # loading model and config from pretrained folder\n>>> encoder_decoder_config = SpeechEncoderDecoderConfig.from_pretrained(\"my-model\")\n>>> model = SpeechEncoderDecoderModel.from_pretrained(\"my-model\", config=encoder_decoder_config)\n```", "```py\n>>> from transformers import SpeechEncoderDecoderModel, AutoProcessor\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-xls-r-300m-en-to-15\")\n>>> model = SpeechEncoderDecoderModel.from_pretrained(\"facebook/wav2vec2-xls-r-300m-en-to-15\")\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\n>>> input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n>>> # Inference: Translate English speech to German\n>>> generated = model.generate(input_values)\n>>> decoded = processor.batch_decode(generated, skip_special_tokens=True)[0]\n>>> decoded\n'Mr. Quilter ist der Apostel der Mittelschicht und wir freuen uns, sein Evangelium willkommen hei\u00dfen zu k\u00f6nnen.'\n\n>>> # Training: Train model on English transcription\n>>> labels = processor(text=ds[0][\"text\"], return_tensors=\"pt\").input_ids\n\n>>> loss = model(input_values, labels=labels).loss\n>>> loss.backward()\n```", "```py\n>>> from transformers import SpeechEncoderDecoderModel\n\n>>> # initialize a wav2vec2bert from a pretrained Wav2Vec2 and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized\n>>> model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(\n...     \"facebook/wav2vec2-base-960h\", \"bert-base-uncased\"\n... )\n>>> # saving model after fine-tuning\n>>> model.save_pretrained(\"./wav2vec2bert\")\n>>> # load fine-tuned model\n>>> model = SpeechEncoderDecoderModel.from_pretrained(\"./wav2vec2bert\")\n```", "```py\n>>> from transformers import FlaxSpeechEncoderDecoderModel, AutoTokenizer\n\n>>> # load a fine-tuned wav2vec2-2-bart model\n>>> model = FlaxSpeechEncoderDecoderModel.from_pretrained(\"patrickvonplaten/wav2vec2-2-bart-large\")\n>>> # load output tokenizer\n>>> tokenizer_output = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n\n>>> inputs = jnp.ones((2, 5000), dtype=jnp.float32)\n\n>>> # use bart's special bos, pad and eos tokens\n>>> model.config.decoder_start_token_id = model.decoder.config.bos_token_id\n>>> model.config.pad_token_id = model.decoder.config.pad_token_id\n>>> model.config.eos_token_id = model.decoder.config.eos_token_id\n\n>>> outputs = model.generate(inputs)\n# Assert something? More interesting input? dtype correct?\n```", "```py\n>>> from transformers import FlaxSpeechEncoderDecoderModel\n\n>>> # initialize a wav2vec2-2-bart from pretrained wav2vec2 and bart models. Note that the cross-attention layers will be randomly initialized\n>>> model = FlaxSpeechEncoderDecoderModel.from_encoder_decoder_pretrained(\n...     \"facebook/wav2vec2-large-lv60\", \"facebook/bart-large\"\n... )\n>>> # saving model after fine-tuning\n>>> model.save_pretrained(\"./wav2vec2-2-bart-large\")\n>>> # load fine-tuned model\n>>> model = FlaxSpeechEncoderDecoderModel.from_pretrained(\"./wav2vec2-2-bart-large\")\n```"]