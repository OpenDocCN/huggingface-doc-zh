- en: Quantization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é‡åŒ–
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/quantization](https://huggingface.co/docs/transformers/v4.37.2/en/quantization)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/quantization](https://huggingface.co/docs/transformers/v4.37.2/en/quantization)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Quantization techniques focus on representing data with less information while
    also trying to not lose too much accuracy. This often means converting a data
    type to represent the same information with fewer bits. For example, if your model
    weights are stored as 32-bit floating points and theyâ€™re quantized to 16-bit floating
    points, this halves the model size which makes it easier to store and reduces
    memory-usage. Lower precision can also speedup inference because it takes less
    time to perform calculations with fewer bits.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: é‡åŒ–æŠ€æœ¯ä¸“æ³¨äºç”¨æ›´å°‘çš„ä¿¡æ¯è¡¨ç¤ºæ•°æ®ï¼ŒåŒæ—¶ä¹Ÿè¯•å›¾ä¸ä¸¢å¤±å¤ªå¤šå‡†ç¡®æ€§ã€‚è¿™é€šå¸¸æ„å‘³ç€å°†æ•°æ®ç±»å‹è½¬æ¢ä¸ºç”¨æ›´å°‘çš„ä½è¡¨ç¤ºç›¸åŒä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨çš„æ¨¡å‹æƒé‡å­˜å‚¨ä¸º32ä½æµ®ç‚¹æ•°ï¼Œå¹¶ä¸”å®ƒä»¬è¢«é‡åŒ–ä¸º16ä½æµ®ç‚¹æ•°ï¼Œè¿™å°†ä½¿æ¨¡å‹å¤§å°å‡åŠï¼Œä½¿å…¶æ›´å®¹æ˜“å­˜å‚¨å¹¶å‡å°‘å†…å­˜ä½¿ç”¨ã€‚è¾ƒä½çš„ç²¾åº¦ä¹Ÿå¯ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦ï¼Œå› ä¸ºä½¿ç”¨æ›´å°‘çš„ä½è¿›è¡Œè®¡ç®—éœ€è¦æ›´å°‘çš„æ—¶é—´ã€‚
- en: Transformers supports several quantization schemes to help you run inference
    with large language models (LLMs) and finetune adapters on quantized models. This
    guide will show you how to use Activation-aware Weight Quantization (AWQ), AutoGPTQ,
    and bitsandbytes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Transformersæ”¯æŒå‡ ç§é‡åŒ–æ–¹æ¡ˆï¼Œå¸®åŠ©æ‚¨åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸Šè¿è¡Œæ¨ç†å’Œåœ¨é‡åŒ–æ¨¡å‹ä¸Šå¾®è°ƒé€‚é…å™¨ã€‚æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ¿€æ´»æ„ŸçŸ¥æƒé‡é‡åŒ–ï¼ˆAWQï¼‰ã€AutoGPTQå’Œbitsandbytesã€‚
- en: AWQ
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWQ
- en: Try AWQ quantization with this [notebook](https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY)!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å°è¯•ä½¿ç”¨è¿™ä¸ª[notebook](https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY)è¿›è¡ŒAWQé‡åŒ–ï¼
- en: '[Activation-aware Weight Quantization (AWQ)](https://hf.co/papers/2306.00978)
    doesnâ€™t quantize all the weights in a model, and instead, it preserves a small
    percentage of weights that are important for LLM performance. This significantly
    reduces quantization loss such that you can run models in 4-bit precision without
    experiencing any performance degradation.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ¿€æ´»æ„ŸçŸ¥æƒé‡é‡åŒ–ï¼ˆAWQï¼‰](https://hf.co/papers/2306.00978)ä¸ä¼šé‡åŒ–æ¨¡å‹ä¸­çš„æ‰€æœ‰æƒé‡ï¼Œè€Œæ˜¯ä¿ç•™å¯¹LLMæ€§èƒ½é‡è¦çš„ä¸€å°éƒ¨åˆ†æƒé‡ã€‚è¿™æ˜¾è‘—å‡å°‘äº†é‡åŒ–æŸå¤±ï¼Œä½¿æ‚¨å¯ä»¥åœ¨ä¸ç»å†ä»»ä½•æ€§èƒ½é™çº§çš„æƒ…å†µä¸‹ä»¥4ä½ç²¾åº¦è¿è¡Œæ¨¡å‹ã€‚'
- en: There are several libraries for quantizing models with the AWQ algorithm, such
    as [llm-awq](https://github.com/mit-han-lab/llm-awq), [autoawq](https://github.com/casper-hansen/AutoAWQ)
    or [optimum-intel](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc).
    Transformers supports loading models quantized with the llm-awq and autoawq libraries.
    This guide will show you how to load models quantized with autoawq, but the processs
    is similar for llm-awq quantized models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å‡ ä¸ªåº“å¯ç”¨äºä½¿ç”¨AWQç®—æ³•é‡åŒ–æ¨¡å‹ï¼Œä¾‹å¦‚[llm-awq](https://github.com/mit-han-lab/llm-awq)ã€[autoawq](https://github.com/casper-hansen/AutoAWQ)æˆ–[optimum-intel](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc)ã€‚
    Transformersæ”¯æŒåŠ è½½ä½¿ç”¨llm-awqå’Œautoawqåº“é‡åŒ–çš„æ¨¡å‹ã€‚æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•åŠ è½½ä½¿ç”¨autoawqé‡åŒ–çš„æ¨¡å‹ï¼Œä½†å¯¹äºä½¿ç”¨llm-awqé‡åŒ–çš„æ¨¡å‹ï¼Œè¿‡ç¨‹ç±»ä¼¼ã€‚
- en: 'Make sure you have autoawq installed:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿æ‚¨å·²å®‰è£…autoawqï¼š
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'AWQ-quantized models can be identified by checking the `quantization_config`
    attribute in the modelâ€™s [config.json](https://huggingface.co/TheBloke/zephyr-7B-alpha-AWQ/blob/main/config.json)
    file:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥é€šè¿‡æ£€æŸ¥æ¨¡å‹çš„[config.json](https://huggingface.co/TheBloke/zephyr-7B-alpha-AWQ/blob/main/config.json)æ–‡ä»¶ä¸­çš„`quantization_config`å±æ€§æ¥è¯†åˆ«AWQé‡åŒ–æ¨¡å‹ï¼š
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A quantized model is loaded with the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method. If you loaded your model on the CPU, make sure to move it to a GPU device
    first. Use the `device_map` parameter to specify where to place the model:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•åŠ è½½é‡åŒ–æ¨¡å‹ã€‚å¦‚æœæ‚¨åœ¨CPUä¸ŠåŠ è½½äº†æ¨¡å‹ï¼Œè¯·ç¡®ä¿é¦–å…ˆå°†å…¶ç§»åŠ¨åˆ°GPUè®¾å¤‡ä¸Šã€‚ä½¿ç”¨`device_map`å‚æ•°æŒ‡å®šæ¨¡å‹æ”¾ç½®çš„ä½ç½®ï¼š
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Loading an AWQ-quantized model automatically sets other weights to fp16 by
    default for performance reasons. If you want to load these other weights in a
    different format, use the `torch_dtype` parameter:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½AWQé‡åŒ–æ¨¡å‹ä¼šè‡ªåŠ¨å°†å…¶ä»–æƒé‡é»˜è®¤è®¾ç½®ä¸ºfp16ä»¥æé«˜æ€§èƒ½ã€‚å¦‚æœæ‚¨æƒ³ä»¥ä¸åŒæ ¼å¼åŠ è½½è¿™äº›å…¶ä»–æƒé‡ï¼Œè¯·ä½¿ç”¨`torch_dtype`å‚æ•°ï¼š
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'AWQ quantization can also be combined with [FlashAttention-2](perf_infer_gpu_one#flashattention-2)
    to further accelerate inference:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: AWQé‡åŒ–ä¹Ÿå¯ä»¥ä¸[FlashAttention-2](perf_infer_gpu_one#flashattention-2)ç»“åˆï¼Œä»¥è¿›ä¸€æ­¥åŠ é€Ÿæ¨ç†ï¼š
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Fused modules
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: èåˆæ¨¡å—
- en: Fused modules offers improved accuracy and performance and it is supported out-of-the-box
    for AWQ modules for [Llama](https://huggingface.co/meta-llama) and [Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1)
    architectures, but you can also fuse AWQ modules for unsupported architectures.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: èåˆæ¨¡å—æä¾›äº†æ”¹è¿›çš„å‡†ç¡®æ€§å’Œæ€§èƒ½ï¼Œå¹¶ä¸”å¯¹äº[Llama](https://huggingface.co/meta-llama)å’Œ[Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1)æ¶æ„çš„AWQæ¨¡å—æ”¯æŒå¼€ç®±å³ç”¨ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥ä¸ºä¸æ”¯æŒçš„æ¶æ„èåˆAWQæ¨¡å—ã€‚
- en: Fused modules cannot be combined with other optimization techniques such as
    FlashAttention-2.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: èåˆæ¨¡å—ä¸èƒ½ä¸FlashAttention-2ç­‰å…¶ä»–ä¼˜åŒ–æŠ€æœ¯ç»“åˆä½¿ç”¨ã€‚
- en: supported architecturesunsupported architectures
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒçš„æ¶æ„ä¸æ”¯æŒçš„æ¶æ„
- en: To enable fused modules for supported architectures, create an [AwqConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.AwqConfig)
    and set the parameters `fuse_max_seq_len` and `do_fuse=True`. The `fuse_max_seq_len`
    parameter is the total sequence length and it should include the context length
    and the expected generation length. You can set it to a larger value to be safe.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä¸ºæ”¯æŒçš„æ¶æ„å¯ç”¨èåˆæ¨¡å—ï¼Œè¯·åˆ›å»ºä¸€ä¸ª[AwqConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.AwqConfig)å¹¶è®¾ç½®å‚æ•°`fuse_max_seq_len`å’Œ`do_fuse=True`ã€‚`fuse_max_seq_len`å‚æ•°æ˜¯æ€»åºåˆ—é•¿åº¦ï¼Œåº”åŒ…æ‹¬ä¸Šä¸‹æ–‡é•¿åº¦å’Œé¢„æœŸç”Ÿæˆé•¿åº¦ã€‚æ‚¨å¯ä»¥å°†å…¶è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥ç¡®ä¿å®‰å…¨ã€‚
- en: For example, to fuse the AWQ modules of the [TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ)
    model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¦èåˆ[TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ)æ¨¡å‹çš„AWQæ¨¡å—ã€‚
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: AutoGPTQ
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoGPTQ
- en: Try GPTQ quantization with PEFT in this [notebook](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing)
    and learn more about itâ€™s details in this [blog post](https://huggingface.co/blog/gptq-integration)!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ª[notebook](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing)ä¸­å°è¯•ä½¿ç”¨PEFTè¿›è¡ŒGPTQé‡åŒ–ï¼Œå¹¶åœ¨è¿™ç¯‡[åšå®¢æ–‡ç« ](https://huggingface.co/blog/gptq-integration)ä¸­äº†è§£æ›´å¤šç»†èŠ‚ï¼
- en: The [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) library implements the
    GPTQ algorithm, a post-training quantization technique where each row of the weight
    matrix is quantized independently to find a version of the weights that minimizes
    the error. These weights are quantized to int4, but theyâ€™re restored to fp16 on
    the fly during inference. This can save your memory-usage by 4x because the int4
    weights are dequantized in a fused kernel rather than a GPUâ€™s global memory, and
    you can also expect a speedup in inference because using a lower bitwidth takes
    less time to communicate.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)åº“å®ç°äº†GPTQç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§åè®­ç»ƒé‡åŒ–æŠ€æœ¯ï¼Œå…¶ä¸­æƒé‡çŸ©é˜µçš„æ¯ä¸€è¡Œéƒ½ç‹¬ç«‹é‡åŒ–ï¼Œä»¥æ‰¾åˆ°æœ€å°åŒ–è¯¯å·®çš„æƒé‡ç‰ˆæœ¬ã€‚è¿™äº›æƒé‡è¢«é‡åŒ–ä¸ºint4ï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼šåŠ¨æ€æ¢å¤ä¸ºfp16ã€‚è¿™å¯ä»¥é€šè¿‡4å€å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œå› ä¸ºint4æƒé‡åœ¨èåˆå†…æ ¸ä¸­è€Œä¸æ˜¯GPUçš„å…¨å±€å†…å­˜ä¸­è¢«è§£é‡åŒ–ï¼Œæ‚¨è¿˜å¯ä»¥æœŸæœ›æ¨ç†é€Ÿåº¦æå‡ï¼Œå› ä¸ºä½¿ç”¨è¾ƒä½çš„ä½å®½éœ€è¦æ›´å°‘çš„é€šä¿¡æ—¶é—´ã€‚'
- en: 'Before you begin, make sure the following libraries are installed:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å®‰è£…äº†ä»¥ä¸‹åº“ï¼š
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To quantize a model (currently only supported for text models), you need to
    create a [GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig)
    class and set the number of bits to quantize to, a dataset to calibrate the weights
    for quantization, and a tokenizer to prepare the dataset.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¦é‡åŒ–ä¸€ä¸ªæ¨¡å‹ï¼ˆç›®å‰ä»…æ”¯æŒæ–‡æœ¬æ¨¡å‹ï¼‰ï¼Œæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ª[GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig)ç±»ï¼Œå¹¶è®¾ç½®è¦é‡åŒ–çš„ä½æ•°ã€ç”¨äºæ ¡å‡†æƒé‡çš„æ•°æ®é›†ä»¥åŠå‡†å¤‡æ•°æ®é›†çš„åˆ†è¯å™¨ã€‚
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You could also pass your own dataset as a list of strings, but it is highly
    recommended to use the same dataset from the GPTQ paper.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥å°†è‡ªå·±çš„æ•°æ®é›†ä½œä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ä¼ é€’ï¼Œä½†å¼ºçƒˆå»ºè®®ä½¿ç”¨GPTQè®ºæ–‡ä¸­ç›¸åŒçš„æ•°æ®é›†ã€‚
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Load a model to quantize and pass the `gptq_config` to the [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    method. Set `device_map="auto"` to automatically offload the model to a CPU to
    help fit the model in memory, and allow the model modules to be moved between
    the CPU and GPU for quantization.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½ä¸€ä¸ªè¦é‡åŒ–çš„æ¨¡å‹ï¼Œå¹¶å°†`gptq_config`ä¼ é€’ç»™[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)æ–¹æ³•ã€‚è®¾ç½®`device_map="auto"`ä»¥è‡ªåŠ¨å°†æ¨¡å‹å¸è½½åˆ°CPUï¼Œä»¥å¸®åŠ©å°†æ¨¡å‹é€‚é…åˆ°å†…å­˜ä¸­ï¼Œå¹¶å…è®¸æ¨¡å‹æ¨¡å—åœ¨CPUå’ŒGPUä¹‹é—´ç§»åŠ¨ä»¥è¿›è¡Œé‡åŒ–ã€‚
- en: '[PRE9]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If youâ€™re running out of memory because a dataset is too large, disk offloading
    is not supported. If this is the case, try passing the `max_memory` parameter
    to allocate the amount of memory to use on your device (GPU and CPU):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœç”±äºæ•°æ®é›†è¿‡å¤§è€Œå¯¼è‡´å†…å­˜ä¸è¶³ï¼Œä¸æ”¯æŒç£ç›˜å¸è½½ã€‚å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œè¯·å°è¯•ä¼ é€’`max_memory`å‚æ•°æ¥åˆ†é…è®¾å¤‡ï¼ˆGPUå’ŒCPUï¼‰ä¸Šè¦ä½¿ç”¨çš„å†…å­˜é‡ï¼š
- en: '[PRE10]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Depending on your hardware, it can take some time to quantize a model from scratch.
    It can take ~5 minutes to quantize the faceboook/opt-350m model on a free-tier
    Google Colab GPU, but itâ€™ll take ~4 hours to quantize a 175B parameter model on
    a NVIDIA A100\. Before you quantize a model, it is a good idea to check the Hub
    if a GPTQ-quantized version of the model already exists.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æ‚¨çš„ç¡¬ä»¶ï¼Œä»å¤´å¼€å§‹é‡åŒ–ä¸€ä¸ªæ¨¡å‹å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ã€‚åœ¨å…è´¹çš„Google Colab GPUä¸Šï¼Œé‡åŒ–faceboook/opt-350mæ¨¡å‹å¯èƒ½éœ€è¦çº¦5åˆ†é’Ÿï¼Œä½†åœ¨NVIDIA
    A100ä¸Šï¼Œé‡åŒ–ä¸€ä¸ª175Bå‚æ•°æ¨¡å‹å¯èƒ½éœ€è¦çº¦4å°æ—¶ã€‚åœ¨é‡åŒ–æ¨¡å‹ä¹‹å‰ï¼Œæœ€å¥½å…ˆæ£€æŸ¥Hubï¼Œçœ‹çœ‹æ¨¡å‹çš„GPTQé‡åŒ–ç‰ˆæœ¬æ˜¯å¦å·²ç»å­˜åœ¨ã€‚
- en: 'Once your model is quantized, you can push the model and tokenizer to the Hub
    where it can be easily shared and accessed. Use the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method to save the [GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨çš„æ¨¡å‹è¢«é‡åŒ–ï¼Œæ‚¨å¯ä»¥å°†æ¨¡å‹å’Œåˆ†è¯å™¨æ¨é€åˆ°Hubï¼Œè¿™æ ·å¯ä»¥è½»æ¾å…±äº«å’Œè®¿é—®ã€‚ä½¿ç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)æ–¹æ³•ä¿å­˜[GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig)ï¼š
- en: '[PRE11]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You could also save your quantized model locally with the [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)
    method. If the model was quantized with the `device_map` parameter, make sure
    to move the entire model to a GPU or CPU before saving it. For example, to save
    the model on a CPU:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ä½¿ç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)æ–¹æ³•å°†æ‚¨çš„é‡åŒ–æ¨¡å‹ä¿å­˜åœ¨æœ¬åœ°ã€‚å¦‚æœæ¨¡å‹æ˜¯ä½¿ç”¨`device_map`å‚æ•°é‡åŒ–çš„ï¼Œè¯·ç¡®ä¿åœ¨ä¿å­˜ä¹‹å‰å°†æ•´ä¸ªæ¨¡å‹ç§»åŠ¨åˆ°GPUæˆ–CPUã€‚ä¾‹å¦‚ï¼Œè¦åœ¨CPUä¸Šä¿å­˜æ¨¡å‹ï¼š
- en: '[PRE12]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Reload a quantized model with the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method, and set `device_map="auto"` to automatically distribute the model on all
    available GPUs to load the model faster without using more memory than needed.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•é‡æ–°åŠ è½½ä¸€ä¸ªé‡åŒ–æ¨¡å‹ï¼Œå¹¶è®¾ç½®`device_map="auto"`ä»¥è‡ªåŠ¨å°†æ¨¡å‹åˆ†å¸ƒåœ¨æ‰€æœ‰å¯ç”¨çš„GPUä¸Šï¼Œä»¥ä¾¿æ›´å¿«åœ°åŠ è½½æ¨¡å‹è€Œä¸ä½¿ç”¨æ¯”æ‰€éœ€æ›´å¤šçš„å†…å­˜ã€‚
- en: '[PRE13]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ExLlama
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ExLlama
- en: '[ExLlama](https://github.com/turboderp/exllama) is a Python/C++/CUDA implementation
    of the [Llama](model_doc/llama) model that is designed for faster inference with
    4-bit GPTQ weights (check out these [benchmarks](https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark)).
    The ExLlama kernel is activated by default when you create a [GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig)
    object. To boost inference speed even further, use the [ExLlamaV2](https://github.com/turboderp/exllamav2)
    kernels by configuring the `exllama_config` parameter:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[ExLlama](https://github.com/turboderp/exllama)æ˜¯[Llama](model_doc/llama)æ¨¡å‹çš„Python/C++/CUDAå®ç°ï¼Œæ—¨åœ¨é€šè¿‡4ä½GPTQæƒé‡å®ç°æ›´å¿«çš„æ¨ç†ï¼ˆæŸ¥çœ‹è¿™äº›[åŸºå‡†æµ‹è¯•](https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark)ï¼‰ã€‚å½“æ‚¨åˆ›å»ºä¸€ä¸ª[GPTQConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.GPTQConfig)å¯¹è±¡æ—¶ï¼Œé»˜è®¤æƒ…å†µä¸‹ä¼šæ¿€æ´»ExLlamaå†…æ ¸ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¨ç†é€Ÿåº¦ï¼Œè¯·é…ç½®`exllama_config`å‚æ•°ä½¿ç”¨[ExLlamaV2](https://github.com/turboderp/exllamav2)å†…æ ¸ï¼š'
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Only 4-bit models are supported, and we recommend deactivating the ExLlama kernels
    if youâ€™re finetuning a quantized model with PEFT.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…æ”¯æŒ4ä½æ¨¡å‹ï¼Œå¹¶ä¸”æˆ‘ä»¬å»ºè®®åœ¨å¯¹é‡åŒ–æ¨¡å‹è¿›è¡Œå¾®è°ƒæ—¶åœç”¨ExLlamaå†…æ ¸ã€‚
- en: The ExLlama kernels are only supported when the entire model is on the GPU.
    If youâ€™re doing inference on a CPU with AutoGPTQ (version > 0.4.2), then youâ€™ll
    need to disable the ExLlama kernel. This overwrites the attributes related to
    the ExLlama kernels in the quantization config of the config.json file.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰å½“æ•´ä¸ªæ¨¡å‹åœ¨GPUä¸Šæ—¶æ‰æ”¯æŒExLlamaå†…æ ¸ã€‚å¦‚æœæ‚¨åœ¨CPUä¸Šä½¿ç”¨AutoGPTQï¼ˆç‰ˆæœ¬>0.4.2ï¼‰è¿›è¡Œæ¨ç†ï¼Œåˆ™éœ€è¦ç¦ç”¨ExLlamaå†…æ ¸ã€‚è¿™å°†è¦†ç›–config.jsonæ–‡ä»¶ä¸­ä¸ExLlamaå†…æ ¸ç›¸å…³çš„å±æ€§ã€‚
- en: '[PRE15]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: bitsandbytes
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: bitsandbytes
- en: '[bitsandbytes](https://github.com/TimDettmers/bitsandbytes) is the easiest
    option for quantizing a model to 8 and 4-bit. 8-bit quantization multiplies outliers
    in fp16 with non-outliers in int8, converts the non-outlier values back to fp16,
    and then adds them together to return the weights in fp16\. This reduces the degradative
    effect outlier values have on a modelâ€™s performance. 4-bit quantization compresses
    a model even further, and it is commonly used with [QLoRA](https://hf.co/papers/2305.14314)
    to finetune quantized LLMs.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)æ˜¯å°†æ¨¡å‹é‡åŒ–ä¸º8ä½å’Œ4ä½çš„æœ€ç®€å•é€‰æ‹©ã€‚8ä½é‡åŒ–å°†fp16ä¸­çš„å¼‚å¸¸å€¼ä¸int8ä¸­çš„éå¼‚å¸¸å€¼ç›¸ä¹˜ï¼Œå°†éå¼‚å¸¸å€¼è½¬æ¢å›fp16ï¼Œç„¶åå°†å®ƒä»¬ç›¸åŠ ä»¥è¿”å›fp16ä¸­çš„æƒé‡ã€‚è¿™å‡å°‘äº†å¼‚å¸¸å€¼å¯¹æ¨¡å‹æ€§èƒ½çš„é™çº§å½±å“ã€‚4ä½é‡åŒ–è¿›ä¸€æ­¥å‹ç¼©æ¨¡å‹ï¼Œé€šå¸¸ä¸[QLoRA](https://hf.co/papers/2305.14314)ä¸€èµ·ç”¨äºå¾®è°ƒé‡åŒ–çš„LLMã€‚'
- en: 'To use bitsandbytes, make sure you have the following libraries installed:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨bitsandbytesï¼Œè¯·ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹åº“ï¼š
- en: 8-bit4-bit
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 8ä½4ä½
- en: '[PRE16]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now you can quantize a model with the `load_in_8bit` or `load_in_4bit` parameters
    in the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method. This works for any model in any modality, as long as it supports loading
    with Accelerate and contains `torch.nn.Linear` layers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä¸­çš„`load_in_8bit`æˆ–`load_in_4bit`å‚æ•°æ¥é‡åŒ–æ¨¡å‹ã€‚åªè¦æ¨¡å‹æ”¯æŒä½¿ç”¨AccelerateåŠ è½½å¹¶åŒ…å«`torch.nn.Linear`å±‚ï¼Œè¿™å¯¹ä»»ä½•æ¨¡æ€çš„æ¨¡å‹éƒ½é€‚ç”¨ã€‚
- en: 8-bit4-bit
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 8ä½4ä½
- en: 'Quantizing a model in 8-bit halves the memory-usage, and for large models,
    set `device_map="auto"` to efficiently use the GPUs available:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹é‡åŒ–ä¸º8ä½å¯ä»¥å‡åŠå†…å­˜ä½¿ç”¨é‡ï¼Œå¯¹äºå¤§å‹æ¨¡å‹ï¼Œè®¾ç½®`device_map="auto"`ä»¥æœ‰æ•ˆåœ°ä½¿ç”¨å¯ç”¨çš„GPUï¼š
- en: '[PRE17]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'By default, all the other modules such as `torch.nn.LayerNorm` are converted
    to `torch.float16`. You can change the data type of these modules with the `torch_dtype`
    parameter if you want:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å…¶ä»–æ¨¡å—ï¼ˆå¦‚`torch.nn.LayerNorm`ï¼‰éƒ½ä¼šè½¬æ¢ä¸º`torch.float16`ã€‚å¦‚æœéœ€è¦ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`torch_dtype`å‚æ•°æ›´æ”¹è¿™äº›æ¨¡å—çš„æ•°æ®ç±»å‹ï¼š
- en: '[PRE18]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Once a model is quantized to 8-bit, you canâ€™t push the quantized weights to
    the Hub unless youâ€™re using the latest version of Transformers and bitsandbytes.
    If you have the latest versions, then you can push the 8-bit model to the Hub
    with the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method. The quantization config.json file is pushed first, followed by the quantized
    model weights.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ¨¡å‹è¢«é‡åŒ–ä¸º8ä½ï¼Œé™¤éæ‚¨ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„Transformerså’Œbitsandbytesï¼Œå¦åˆ™æ— æ³•å°†é‡åŒ–çš„æƒé‡æ¨é€åˆ°Hubã€‚å¦‚æœæ‚¨æœ‰æœ€æ–°ç‰ˆæœ¬ï¼Œåˆ™å¯ä»¥ä½¿ç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)æ–¹æ³•å°†8ä½æ¨¡å‹æ¨é€åˆ°Hubã€‚é¦–å…ˆæ¨é€é‡åŒ–çš„config.jsonæ–‡ä»¶ï¼Œç„¶åæ˜¯é‡åŒ–çš„æ¨¡å‹æƒé‡ã€‚
- en: '[PRE19]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Training with 8-bit and 4-bit weights are only supported for training *extra*
    parameters.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…æ”¯æŒä½¿ç”¨8ä½å’Œ4ä½æƒé‡è¿›è¡Œè®­ç»ƒ*é¢å¤–*å‚æ•°ã€‚
- en: 'You can check your memory footprint with the `get_memory_footprint` method:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨`get_memory_footprint`æ–¹æ³•æ£€æŸ¥å†…å­˜å ç”¨æƒ…å†µï¼š
- en: '[PRE20]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Quantized models can be loaded from the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method without needing to specify the `load_in_8bit` or `load_in_4bit` parameters:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥ä»[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä¸­åŠ è½½é‡åŒ–æ¨¡å‹ï¼Œæ— éœ€æŒ‡å®š`load_in_8bit`æˆ–`load_in_4bit`å‚æ•°ï¼š
- en: '[PRE21]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 8-bit
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8ä½
- en: Learn more about the details of 8-bit quantization in this [blog post](https://huggingface.co/blog/hf-bitsandbytes-integration)!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡[åšå®¢æ–‡ç« ](https://huggingface.co/blog/hf-bitsandbytes-integration)ä¸­äº†è§£æ›´å¤šå…³äº8ä½é‡åŒ–çš„ç»†èŠ‚ï¼
- en: This section explores some of the specific features of 8-bit models, such as
    offloading, outlier thresholds, skipping module conversion, and finetuning.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚æ¢è®¨äº†8ä½æ¨¡å‹çš„ä¸€äº›ç‰¹å®šåŠŸèƒ½ï¼Œå¦‚è½¬ç§»ã€å¼‚å¸¸å€¼é˜ˆå€¼ã€è·³è¿‡æ¨¡å—è½¬æ¢å’Œå¾®è°ƒã€‚
- en: Offloading
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è½¬ç§»
- en: '8-bit models can offload weights between the CPU and GPU to support fitting
    very large models into memory. The weights dispatched to the CPU are actually
    stored in **float32**, and arenâ€™t converted to 8-bit. For example, to enable offloading
    for the [bigscience/bloom-1b7](https://huggingface.co/bigscience/bloom-1b7) model,
    start by creating a [BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 8ä½æ¨¡å‹å¯ä»¥åœ¨CPUå’ŒGPUä¹‹é—´è½¬ç§»æƒé‡ï¼Œä»¥æ”¯æŒå°†éå¸¸å¤§çš„æ¨¡å‹é€‚é…åˆ°å†…å­˜ä¸­ã€‚å‘é€åˆ°CPUçš„æƒé‡å®é™…ä¸Šæ˜¯ä»¥**float32**å­˜å‚¨çš„ï¼Œå¹¶ä¸”ä¸ä¼šè½¬æ¢ä¸º8ä½ã€‚ä¾‹å¦‚ï¼Œè¦ä¸º[bigscience/bloom-1b7](https://huggingface.co/bigscience/bloom-1b7)æ¨¡å‹å¯ç”¨è½¬ç§»ï¼Œè¯·é¦–å…ˆåˆ›å»ºä¸€ä¸ª[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)ï¼š
- en: '[PRE22]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Design a custom device map to fit everything on your GPU except for the `lm_head`,
    which youâ€™ll dispatch to the CPU:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾è®¡ä¸€ä¸ªè‡ªå®šä¹‰è®¾å¤‡æ˜ å°„ï¼Œå°†æ‰€æœ‰å†…å®¹éƒ½é€‚é…åˆ°GPUä¸Šï¼Œé™¤äº†`lm_head`ï¼Œè¿™éƒ¨åˆ†å°†å‘é€åˆ°CPUï¼š
- en: '[PRE23]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now load your model with the custom `device_map` and `quantization_config`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½¿ç”¨è‡ªå®šä¹‰çš„`device_map`å’Œ`quantization_config`åŠ è½½æ‚¨çš„æ¨¡å‹ï¼š
- en: '[PRE24]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Outlier threshold
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¼‚å¸¸å€¼é˜ˆå€¼
- en: An â€œoutlierâ€ is a hidden state value greater than a certain threshold, and these
    values are computed in fp16\. While the values are usually normally distributed
    ([-3.5, 3.5]), this distribution can be very different for large models ([-60,
    6] or [6, 60]). 8-bit quantization works well for values ~5, but beyond that,
    there is a significant performance penalty. A good default threshold value is
    6, but a lower threshold may be needed for more unstable models (small models
    or finetuning).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: â€œå¼‚å¸¸å€¼â€æ˜¯å¤§äºæŸä¸ªé˜ˆå€¼çš„éšè—çŠ¶æ€å€¼ï¼Œè¿™äº›å€¼æ˜¯åœ¨fp16ä¸­è®¡ç®—çš„ã€‚è™½ç„¶è¿™äº›å€¼é€šå¸¸æ˜¯æ­£æ€åˆ†å¸ƒçš„ï¼ˆ[-3.5, 3.5]ï¼‰ï¼Œä½†å¯¹äºå¤§å‹æ¨¡å‹ï¼ˆ[-60, 6]æˆ–[6,
    60]ï¼‰ï¼Œè¿™ç§åˆ†å¸ƒå¯èƒ½ä¼šæœ‰å¾ˆå¤§ä¸åŒã€‚8ä½é‡åŒ–é€‚ç”¨äºå€¼çº¦ä¸º5ï¼Œä½†è¶…è¿‡è¿™ä¸ªå€¼ï¼Œä¼šæœ‰æ˜¾è‘—çš„æ€§èƒ½æŸå¤±ã€‚ä¸€ä¸ªå¾ˆå¥½çš„é»˜è®¤é˜ˆå€¼æ˜¯6ï¼Œä½†å¯¹äºæ›´ä¸ç¨³å®šçš„æ¨¡å‹ï¼ˆå°æ¨¡å‹æˆ–å¾®è°ƒï¼‰ï¼Œå¯èƒ½éœ€è¦æ›´ä½çš„é˜ˆå€¼ã€‚
- en: 'To find the best threshold for your model, we recommend experimenting with
    the `llm_int8_threshold` parameter in [BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ‰¾åˆ°æ‚¨çš„æ¨¡å‹çš„æœ€ä½³é˜ˆå€¼ï¼Œæˆ‘ä»¬å»ºè®®å°è¯•åœ¨[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)ä¸­ä½¿ç”¨`llm_int8_threshold`å‚æ•°è¿›è¡Œå®éªŒï¼š
- en: '[PRE25]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Skip module conversion
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è·³è¿‡æ¨¡å—è½¬æ¢
- en: 'For some models, like [Jukebox](model_doc/jukebox), you donâ€™t need to quantize
    every module to 8-bit which can actually cause instability. With Jukebox, there
    are several `lm_head` modules that should be skipped using the `llm_int8_skip_modules`
    parameter in [BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸€äº›æ¨¡å‹ï¼Œå¦‚[Jukebox](model_doc/jukebox)ï¼Œæ‚¨ä¸éœ€è¦å°†æ¯ä¸ªæ¨¡å—é‡åŒ–ä¸º8ä½ï¼Œè¿™å®é™…ä¸Šå¯èƒ½ä¼šå¯¼è‡´ä¸ç¨³å®šæ€§ã€‚å¯¹äºJukeboxï¼Œæœ‰å‡ ä¸ª`lm_head`æ¨¡å—åº”è¯¥ä½¿ç”¨[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)ä¸­çš„`llm_int8_skip_modules`å‚æ•°è·³è¿‡ï¼š
- en: '[PRE26]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Finetuning
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¾®è°ƒ
- en: With the [PEFT](https://github.com/huggingface/peft) library, you can finetune
    large models like [flan-t5-large](https://huggingface.co/google/flan-t5-large)
    and [facebook/opt-6.7b](https://huggingface.co/facebook/opt-6.7b) with 8-bit quantization.
    You donâ€™t need to pass the `device_map` parameter for training because itâ€™ll automatically
    load your model on a GPU. However, you can still customize the device map with
    the `device_map` parameter if you want to (`device_map="auto"` should only be
    used for inference).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[PEFT](https://github.com/huggingface/peft)åº“ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨8ä½é‡åŒ–å¾®è°ƒå¤§å‹æ¨¡å‹ï¼Œå¦‚[flan-t5-large](https://huggingface.co/google/flan-t5-large)å’Œ[facebook/opt-6.7b](https://huggingface.co/facebook/opt-6.7b)ã€‚åœ¨è®­ç»ƒæ—¶ä¸éœ€è¦ä¼ é€’`device_map`å‚æ•°ï¼Œå› ä¸ºå®ƒä¼šè‡ªåŠ¨å°†æ‚¨çš„æ¨¡å‹åŠ è½½åˆ°GPUä¸Šã€‚ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³è¦ï¼Œä»ç„¶å¯ä»¥ä½¿ç”¨`device_map`å‚æ•°è‡ªå®šä¹‰è®¾å¤‡æ˜ å°„ï¼ˆ`device_map="auto"`ä»…åº”ç”¨äºæ¨æ–­ï¼‰ã€‚
- en: 4-bit
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4ä½
- en: Try 4-bit quantization in this [notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf)
    and learn more about itâ€™s details in this [blog post](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ª[notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf)ä¸­å°è¯•4ä½é‡åŒ–ï¼Œå¹¶åœ¨è¿™ç¯‡[åšå®¢æ–‡ç« ](https://huggingface.co/blog/4bit-transformers-bitsandbytes)ä¸­äº†è§£æ›´å¤šç»†èŠ‚ã€‚
- en: This section explores some of the specific features of 4-bit models, such as
    changing the compute data type, using the Normal Float 4 (NF4) data type, and
    using nested quantization.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚æ¢è®¨äº†4ä½æ¨¡å‹çš„ä¸€äº›ç‰¹å®šåŠŸèƒ½ï¼Œå¦‚æ›´æ”¹è®¡ç®—æ•°æ®ç±»å‹ã€ä½¿ç”¨Normal Float 4 (NF4)æ•°æ®ç±»å‹å’Œä½¿ç”¨åµŒå¥—é‡åŒ–ã€‚
- en: Compute data type
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®¡ç®—æ•°æ®ç±»å‹
- en: 'To speedup computation, you can change the data type from float32 (the default
    value) to bf16 using the `bnb_4bit_compute_dtype` parameter in [BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åŠ é€Ÿè®¡ç®—ï¼Œæ‚¨å¯ä»¥å°†æ•°æ®ç±»å‹ä»float32ï¼ˆé»˜è®¤å€¼ï¼‰æ›´æ”¹ä¸ºbf16ï¼Œä½¿ç”¨[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)ä¸­çš„`bnb_4bit_compute_dtype`å‚æ•°ï¼š
- en: '[PRE27]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Normal Float 4 (NF4)
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Normal Float 4 (NF4)
- en: 'NF4 is a 4-bit data type from the [QLoRA](https://hf.co/papers/2305.14314)
    paper, adapted for weights initialized from a normal distribution. You should
    use NF4 for training 4-bit base models. This can be configured with the `bnb_4bit_quant_type`
    parameter in the [BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: NF4æ˜¯æ¥è‡ª[QLoRA](https://hf.co/papers/2305.14314)è®ºæ–‡çš„4ä½æ•°æ®ç±»å‹ï¼Œé€‚ç”¨äºä»æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–çš„æƒé‡ã€‚æ‚¨åº”è¯¥ä½¿ç”¨NF4æ¥è®­ç»ƒ4ä½åŸºç¡€æ¨¡å‹ã€‚è¿™å¯ä»¥é€šè¿‡[BitsAndBytesConfig](/docs/transformers/v4.37.2/en/main_classes/quantization#transformers.BitsAndBytesConfig)ä¸­çš„`bnb_4bit_quant_type`å‚æ•°è¿›è¡Œé…ç½®ï¼š
- en: '[PRE28]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: For inference, the `bnb_4bit_quant_type` does not have a huge impact on performance.
    However, to remain consistent with the model weights, you should use the `bnb_4bit_compute_dtype`
    and `torch_dtype` values.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¨æ–­ï¼Œ`bnb_4bit_quant_type`å¯¹æ€§èƒ½æ²¡æœ‰å¤ªå¤§å½±å“ã€‚ä½†æ˜¯ï¼Œä¸ºäº†ä¿æŒä¸æ¨¡å‹æƒé‡ä¸€è‡´ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨`bnb_4bit_compute_dtype`å’Œ`torch_dtype`å€¼ã€‚
- en: Nested quantization
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: åµŒå¥—é‡åŒ–
- en: Nested quantization is a technique that can save additional memory at no additional
    performance cost. This feature performs a second quantization of the already quantized
    weights to save an addition 0.4 bits/parameter. For example, with nested quantization,
    you can finetune a [Llama-13b](https://huggingface.co/meta-llama/Llama-2-13b)
    model on a 16GB NVIDIA T4 GPU with a sequence length of 1024, a batch size of
    1, and enabling gradient accumulation with 4 steps.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå¥—é‡åŒ–æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œå¯ä»¥åœ¨ä¸å¢åŠ æ€§èƒ½æˆæœ¬çš„æƒ…å†µä¸‹èŠ‚çœé¢å¤–çš„å†…å­˜ã€‚æ­¤åŠŸèƒ½å¯¹å·²ç»é‡åŒ–çš„æƒé‡æ‰§è¡Œç¬¬äºŒæ¬¡é‡åŒ–ï¼Œä»¥èŠ‚çœé¢å¤–çš„0.4ä½/å‚æ•°ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨åµŒå¥—é‡åŒ–ï¼Œæ‚¨å¯ä»¥åœ¨16GBçš„NVIDIA
    T4 GPUä¸Šå¾®è°ƒ[Llama-13b](https://huggingface.co/meta-llama/Llama-2-13b)æ¨¡å‹ï¼Œåºåˆ—é•¿åº¦ä¸º1024ï¼Œæ‰¹é‡å¤§å°ä¸º1ï¼Œå¹¶å¯ç”¨æ¢¯åº¦ç´¯ç§¯4æ­¥ã€‚
- en: '[PRE29]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Optimum
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Optimum
- en: The [Optimum](https://huggingface.co/docs/optimum/index) library supports quantization
    for Intel, Furiosa, ONNX Runtime, GPTQ, and lower-level PyTorch quantization functions.
    Consider using Optimum for quantization if youâ€™re using specific and optimized
    hardware like Intel CPUs, Furiosa NPUs or a model accelerator like ONNX Runtime.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[Optimum](https://huggingface.co/docs/optimum/index)åº“æ”¯æŒIntelã€Furiosaã€ONNX Runtimeã€GPTQå’Œè¾ƒä½çº§åˆ«çš„PyTorché‡åŒ–åŠŸèƒ½ã€‚å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨åƒIntel
    CPUã€Furiosa NPUæˆ–åƒONNX Runtimeè¿™æ ·çš„æ¨¡å‹åŠ é€Ÿå™¨è¿™æ ·çš„ç‰¹å®šå’Œä¼˜åŒ–çš„ç¡¬ä»¶ï¼Œè¯·è€ƒè™‘ä½¿ç”¨Optimumè¿›è¡Œé‡åŒ–ã€‚'
- en: Benchmarks
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŸºå‡†æµ‹è¯•
- en: To compare the speed, throughput, and latency of each quantization scheme, check
    the following benchmarks obtained from the [optimum-benchmark](https://github.com/huggingface/optimum-benchmark)
    library. The benchmark was run on a NVIDIA A1000 for the [TheBloke/Mistral-7B-v0.1-AWQ](https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ)
    and [TheBloke/Mistral-7B-v0.1-GPTQ](https://huggingface.co/TheBloke/Mistral-7B-v0.1-GPTQ)
    models. These were also tested against the bitsandbytes quantization methods as
    well as a native fp16 model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ¯”è¾ƒæ¯ç§é‡åŒ–æ–¹æ¡ˆçš„é€Ÿåº¦ã€ååé‡å’Œå»¶è¿Ÿï¼Œè¯·æŸ¥çœ‹ä»[optimum-benchmark](https://github.com/huggingface/optimum-benchmark)åº“è·å¾—çš„ä»¥ä¸‹åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•åœ¨NVIDIA
    A1000ä¸Šè¿è¡Œï¼Œç”¨äº[TheBloke/Mistral-7B-v0.1-AWQ](https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ)å’Œ[TheBloke/Mistral-7B-v0.1-GPTQ](https://huggingface.co/TheBloke/Mistral-7B-v0.1-GPTQ)æ¨¡å‹ã€‚è¿™äº›è¿˜ä¸bitsandbytesé‡åŒ–æ–¹æ³•ä»¥åŠæœ¬æœºfp16æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ã€‚
- en: '![forward peak memory per batch size](../Images/7f7a45cc2b5704faa291fb7c3ca255d6.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![æ¯æ‰¹å‰å‘å³°å€¼å†…å­˜](../Images/7f7a45cc2b5704faa291fb7c3ca255d6.png)'
- en: forward peak memory/batch size
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å‰å‘å³°å€¼å†…å­˜/æ‰¹å¤„ç†å¤§å°
- en: '![generate peak memory per batch size](../Images/c53e9d1c99d4b8ce87793021dc7f7393.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![æ¯æ‰¹å‰å‘å³°å€¼å†…å­˜](../Images/c53e9d1c99d4b8ce87793021dc7f7393.png)'
- en: generate peak memory/batch size
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ‰¹ç”Ÿæˆå³°å€¼å†…å­˜/æ‰¹å¤„ç†å¤§å°
- en: '![generate throughput per batch size](../Images/40d57cb1a4c4282ec140a27de593156a.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![æ¯æ‰¹ç”Ÿæˆååé‡](../Images/40d57cb1a4c4282ec140a27de593156a.png)'
- en: generate throughput/batch size
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ‰¹ç”Ÿæˆååé‡/æ‰¹å¤„ç†å¤§å°
- en: '![forward latency per batch size](../Images/67c3f5ce2fa384e77579473e9efb77fa.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![æ¯æ‰¹å‰å‘å»¶è¿Ÿ](../Images/67c3f5ce2fa384e77579473e9efb77fa.png)'
- en: forward latency/batch size
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å‰å‘å»¶è¿Ÿ/æ‰¹å¤„ç†å¤§å°
- en: The benchmarks indicate AWQ quantization is the fastest for inference, text
    generation, and has the lowest peak memory for text generation. However, AWQ has
    the largest forward latency per batch size. For a more detailed discussion about
    the pros and cons of each quantization method, read the [Overview of natively
    supported quantization schemes in ğŸ¤— Transformers](https://huggingface.co/blog/overview-quantization-transformers)
    blog post.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒAWQé‡åŒ–åœ¨æ¨ç†ã€æ–‡æœ¬ç”Ÿæˆæ–¹é¢æ˜¯æœ€å¿«çš„ï¼Œå¹¶ä¸”åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢å…·æœ‰æœ€ä½çš„å³°å€¼å†…å­˜ã€‚ç„¶è€Œï¼ŒAWQåœ¨æ¯ä¸ªæ‰¹å¤„ç†å¤§å°ä¸Šå…·æœ‰æœ€å¤§çš„å‰å‘å»¶è¿Ÿã€‚è¦äº†è§£æ¯ç§é‡åŒ–æ–¹æ³•çš„ä¼˜ç¼ºç‚¹çš„æ›´è¯¦ç»†è®¨è®ºï¼Œè¯·é˜…è¯»[ğŸ¤—
    Transformersä¸­æœ¬åœ°æ”¯æŒçš„é‡åŒ–æ–¹æ¡ˆæ¦‚è¿°](https://huggingface.co/blog/overview-quantization-transformers)åšå®¢æ–‡ç« ã€‚
- en: Fused AWQ modules
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: èåˆAWQæ¨¡å—
- en: The [TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ)
    model was benchmarked with `batch_size=1` with and without fused modules.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ)æ¨¡å‹åœ¨`batch_size=1`ä¸‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæœ‰æ— èåˆæ¨¡å—ã€‚'
- en: Unfused module
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æœªèåˆæ¨¡å—
- en: '| Batch Size | Prefill Length | Decode Length | Prefill tokens/s | Decode tokens/s
    | Memory (VRAM) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: æ‰¹å¤„ç†å¤§å° | é¢„å¡«å……é•¿åº¦ | è§£ç é•¿åº¦ | é¢„å¡«å……æ ‡è®°/ç§’ | è§£ç æ ‡è®°/ç§’ | å†…å­˜ï¼ˆVRAMï¼‰ |
- en: '| --: | --: | --: | --: | --: | :-- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --: | --: | --: | --: | --: | :-- |'
- en: '| 1 | 32 | 32 | 60.0984 | 38.4537 | 4.50 GB (5.68%) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 32 | 32 | 60.0984 | 38.4537 | 4.50 GB (5.68%) |'
- en: '| 1 | 64 | 64 | 1333.67 | 31.6604 | 4.50 GB (5.68%) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 64 | 64 | 1333.67 | 31.6604 | 4.50 GB (5.68%) |'
- en: '| 1 | 128 | 128 | 2434.06 | 31.6272 | 4.50 GB (5.68%) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 128 | 128 | 2434.06 | 31.6272 | 4.50 GB (5.68%) |'
- en: '| 1 | 256 | 256 | 3072.26 | 38.1731 | 4.50 GB (5.68%) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 256 | 256 | 3072.26 | 38.1731 | 4.50 GB (5.68%) |'
- en: '| 1 | 512 | 512 | 3184.74 | 31.6819 | 4.59 GB (5.80%) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 512 | 512 | 3184.74 | 31.6819 | 4.59 GB (5.80%) |'
- en: '| 1 | 1024 | 1024 | 3148.18 | 36.8031 | 4.81 GB (6.07%) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1024 | 1024 | 3148.18 | 36.8031 | 4.81 GB (6.07%) |'
- en: '| 1 | 2048 | 2048 | 2927.33 | 35.2676 | 5.73 GB (7.23%) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2048 | 2048 | 2927.33 | 35.2676 | 5.73 GB (7.23%) |'
- en: Fused module
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: èåˆæ¨¡å—
- en: '| Batch Size | Prefill Length | Decode Length | Prefill tokens/s | Decode tokens/s
    | Memory (VRAM) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: æ‰¹å¤„ç†å¤§å° | é¢„å¡«å……é•¿åº¦ | è§£ç é•¿åº¦ | é¢„å¡«å……æ ‡è®°/ç§’ | è§£ç æ ‡è®°/ç§’ | å†…å­˜ï¼ˆVRAMï¼‰ |
- en: '| --: | --: | --: | --: | --: | :-- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| --: | --: | --: | --: | --: | :-- |'
- en: '| 1 | 32 | 32 | 81.4899 | 80.2569 | 4.00 GB (5.05%) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 32 | 32 | 81.4899 | 80.2569 | 4.00 GB (5.05%) |'
- en: '| 1 | 64 | 64 | 1756.1 | 106.26 | 4.00 GB (5.05%) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 64 | 64 | 1756.1 | 106.26 | 4.00 GB (5.05%) |'
- en: '| 1 | 128 | 128 | 2479.32 | 105.631 | 4.00 GB (5.06%) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 128 | 128 | 2479.32 | 105.631 | 4.00 GB (5.06%) |'
- en: '| 1 | 256 | 256 | 1813.6 | 85.7485 | 4.01 GB (5.06%) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 256 | 256 | 1813.6 | 85.7485 | 4.01 GB (5.06%) |'
- en: '| 1 | 512 | 512 | 2848.9 | 97.701 | 4.11 GB (5.19%) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 512 | 512 | 2848.9 | 97.701 | 4.11 GB (5.19%) |'
- en: '| 1 | 1024 | 1024 | 3044.35 | 87.7323 | 4.41 GB (5.57%) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1024 | 1024 | 3044.35 | 87.7323 | 4.41 GB (5.57%) |'
- en: '| 1 | 2048 | 2048 | 2715.11 | 89.4709 | 5.57 GB (7.04%) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2048 | 2048 | 2715.11 | 89.4709 | 5.57 GB (7.04%) |'
- en: The speed and throughput of fused and unfused modules were also tested with
    the [optimum-benchmark](https://github.com/huggingface/optimum-benchmark) library.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: èåˆå’Œæœªèåˆæ¨¡å—çš„é€Ÿåº¦å’Œååé‡ä¹Ÿç»è¿‡äº†[optimum-benchmark](https://github.com/huggingface/optimum-benchmark)åº“çš„æµ‹è¯•ã€‚
- en: '![generate throughput per batch size](../Images/c73fad074a31449cad262be148000a7b.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![æ¯æ‰¹ç”Ÿæˆååé‡](../Images/c73fad074a31449cad262be148000a7b.png)'
- en: foward peak memory/batch size
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å‰å‘å³°å€¼å†…å­˜/æ‰¹å¤„ç†å¤§å°
- en: '![forward latency per batch size](../Images/a6ca88db796d9aca9bc439edc51f861d.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![æ¯æ‰¹å‰å‘å»¶è¿Ÿ](../Images/a6ca88db796d9aca9bc439edc51f861d.png)'
- en: generate throughput/batch size
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ‰¹ç”Ÿæˆååé‡/æ‰¹å¤„ç†å¤§å°
