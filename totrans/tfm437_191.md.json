["```py\n>>> from transformers import LukeTokenizer, LukeModel, LukeForEntityPairClassification\n\n>>> model = LukeModel.from_pretrained(\"studio-ousia/luke-base\")\n>>> tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n# Example 1: Computing the contextualized entity representation corresponding to the entity mention \"Beyonc\u00e9\"\n\n>>> text = \"Beyonc\u00e9 lives in Los Angeles.\"\n>>> entity_spans = [(0, 7)]  # character-based entity span corresponding to \"Beyonc\u00e9\"\n>>> inputs = tokenizer(text, entity_spans=entity_spans, add_prefix_space=True, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> word_last_hidden_state = outputs.last_hidden_state\n>>> entity_last_hidden_state = outputs.entity_last_hidden_state\n# Example 2: Inputting Wikipedia entities to obtain enriched contextualized representations\n\n>>> entities = [\n...     \"Beyonc\u00e9\",\n...     \"Los Angeles\",\n... ]  # Wikipedia entity titles corresponding to the entity mentions \"Beyonc\u00e9\" and \"Los Angeles\"\n>>> entity_spans = [(0, 7), (17, 28)]  # character-based entity spans corresponding to \"Beyonc\u00e9\" and \"Los Angeles\"\n>>> inputs = tokenizer(text, entities=entities, entity_spans=entity_spans, add_prefix_space=True, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> word_last_hidden_state = outputs.last_hidden_state\n>>> entity_last_hidden_state = outputs.entity_last_hidden_state\n# Example 3: Classifying the relationship between two entities using LukeForEntityPairClassification head model\n\n>>> model = LukeForEntityPairClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n>>> tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n>>> entity_spans = [(0, 7), (17, 28)]  # character-based entity spans corresponding to \"Beyonc\u00e9\" and \"Los Angeles\"\n>>> inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n>>> predicted_class_idx = int(logits[0].argmax())\n>>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```", "```py\n( vocab_size = 50267 entity_vocab_size = 500000 hidden_size = 768 entity_emb_size = 256 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 use_entity_aware_attention = True classifier_dropout = None pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 **kwargs )\n```", "```py\n>>> from transformers import LukeConfig, LukeModel\n\n>>> # Initializing a LUKE configuration\n>>> configuration = LukeConfig()\n\n>>> # Initializing a model from the configuration\n>>> model = LukeModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file merges_file entity_vocab_file task = None max_entity_length = 32 max_mention_length = 30 entity_token_1 = '<ent>' entity_token_2 = '<ent2>' entity_unk_token = '[UNK]' entity_pad_token = '[PAD]' entity_mask_token = '[MASK]' entity_mask2_token = '[MASK2]' errors = 'replace' bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' add_prefix_space = False **kwargs )\n```", "```py\n>>> from transformers import LukeTokenizer\n\n>>> tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[0, 31414, 232, 2]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[0, 20920, 232, 2]\n```", "```py\n( text: Union text_pair: Union = None entity_spans: Union = None entity_spans_pair: Union = None entities: Union = None entities_pair: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None max_entity_length: Optional = None stride: int = 0 is_split_into_words: Optional = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( config: LukeConfig add_pooling_layer: bool = True )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None entity_ids: Optional = None entity_attention_mask: Optional = None entity_token_type_ids: Optional = None entity_position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.luke.modeling_luke.BaseLukeModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LukeModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n>>> model = LukeModel.from_pretrained(\"studio-ousia/luke-base\")\n# Compute the contextualized entity representation corresponding to the entity mention \"Beyonc\u00e9\"\n\n>>> text = \"Beyonc\u00e9 lives in Los Angeles.\"\n>>> entity_spans = [(0, 7)]  # character-based entity span corresponding to \"Beyonc\u00e9\"\n\n>>> encoding = tokenizer(text, entity_spans=entity_spans, add_prefix_space=True, return_tensors=\"pt\")\n>>> outputs = model(**encoding)\n>>> word_last_hidden_state = outputs.last_hidden_state\n>>> entity_last_hidden_state = outputs.entity_last_hidden_state\n# Input Wikipedia entities to obtain enriched contextualized representations of word tokens\n\n>>> text = \"Beyonc\u00e9 lives in Los Angeles.\"\n>>> entities = [\n...     \"Beyonc\u00e9\",\n...     \"Los Angeles\",\n... ]  # Wikipedia entity titles corresponding to the entity mentions \"Beyonc\u00e9\" and \"Los Angeles\"\n>>> entity_spans = [\n...     (0, 7),\n...     (17, 28),\n... ]  # character-based entity spans corresponding to \"Beyonc\u00e9\" and \"Los Angeles\"\n\n>>> encoding = tokenizer(\n...     text, entities=entities, entity_spans=entity_spans, add_prefix_space=True, return_tensors=\"pt\"\n... )\n>>> outputs = model(**encoding)\n>>> word_last_hidden_state = outputs.last_hidden_state\n>>> entity_last_hidden_state = outputs.entity_last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None entity_ids: Optional = None entity_attention_mask: Optional = None entity_token_type_ids: Optional = None entity_position_ids: Optional = None labels: Optional = None entity_labels: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.luke.modeling_luke.LukeMaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None entity_ids: Optional = None entity_attention_mask: Optional = None entity_token_type_ids: Optional = None entity_position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.luke.modeling_luke.EntityClassificationOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LukeForEntityClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-open-entity\")\n>>> model = LukeForEntityClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-open-entity\")\n\n>>> text = \"Beyonc\u00e9 lives in Los Angeles.\"\n>>> entity_spans = [(0, 7)]  # character-based entity span corresponding to \"Beyonc\u00e9\"\n>>> inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n>>> predicted_class_idx = logits.argmax(-1).item()\n>>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nPredicted class: person\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None entity_ids: Optional = None entity_attention_mask: Optional = None entity_token_type_ids: Optional = None entity_position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.luke.modeling_luke.EntityPairClassificationOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LukeForEntityPairClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n>>> model = LukeForEntityPairClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n\n>>> text = \"Beyonc\u00e9 lives in Los Angeles.\"\n>>> entity_spans = [\n...     (0, 7),\n...     (17, 28),\n... ]  # character-based entity spans corresponding to \"Beyonc\u00e9\" and \"Los Angeles\"\n>>> inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n>>> predicted_class_idx = logits.argmax(-1).item()\n>>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nPredicted class: per:cities_of_residence\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None entity_ids: Optional = None entity_attention_mask: Optional = None entity_token_type_ids: Optional = None entity_position_ids: Optional = None entity_start_positions: Optional = None entity_end_positions: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.luke.modeling_luke.EntitySpanClassificationOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LukeForEntitySpanClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")\n>>> model = LukeForEntitySpanClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")\n\n>>> text = \"Beyonc\u00e9 lives in Los Angeles\"\n# List all possible entity spans in the text\n\n>>> word_start_positions = [0, 8, 14, 17, 21]  # character-based start positions of word tokens\n>>> word_end_positions = [7, 13, 16, 20, 28]  # character-based end positions of word tokens\n>>> entity_spans = []\n>>> for i, start_pos in enumerate(word_start_positions):\n...     for end_pos in word_end_positions[i:]:\n...         entity_spans.append((start_pos, end_pos))\n\n>>> inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n>>> predicted_class_indices = logits.argmax(-1).squeeze().tolist()\n>>> for span, predicted_class_idx in zip(entity_spans, predicted_class_indices):\n...     if predicted_class_idx != 0:\n...         print(text[span[0] : span[1]], model.config.id2label[predicted_class_idx])\nBeyonc\u00e9 PER\nLos Angeles LOC\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None entity_ids: Optional = None entity_attention_mask: Optional = None entity_token_type_ids: Optional = None entity_position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.luke.modeling_luke.LukeSequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, LukeForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n>>> model = LukeForSequenceClassification.from_pretrained(\"studio-ousia/luke-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = LukeForSequenceClassification.from_pretrained(\"studio-ousia/luke-base\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, LukeForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n>>> model = LukeForSequenceClassification.from_pretrained(\"studio-ousia/luke-base\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = LukeForSequenceClassification.from_pretrained(\n...     \"studio-ousia/luke-base\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None entity_ids: Optional = None entity_attention_mask: Optional = None entity_token_type_ids: Optional = None entity_position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.luke.modeling_luke.LukeMultipleChoiceModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LukeForMultipleChoice\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n>>> model = LukeForMultipleChoice.from_pretrained(\"studio-ousia/luke-base\")\n\n>>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n>>> choice0 = \"It is eaten with a fork and a knife.\"\n>>> choice1 = \"It is eaten while held in the hand.\"\n>>> labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n\n>>> encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"pt\", padding=True)\n>>> outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1\n\n>>> # the linear classifier still needs to be trained\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None entity_ids: Optional = None entity_attention_mask: Optional = None entity_token_type_ids: Optional = None entity_position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.luke.modeling_luke.LukeTokenClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LukeForTokenClassification\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n>>> model = LukeForTokenClassification.from_pretrained(\"studio-ousia/luke-base\")\n\n>>> inputs = tokenizer(\n...     \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n... )\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_token_class_ids = logits.argmax(-1)\n\n>>> # Note that tokens are classified rather then input words which means that\n>>> # there might be more predicted token classes than words.\n>>> # Multiple token classes might account for the same word\n>>> predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n\n>>> labels = predicted_token_class_ids\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None entity_ids: Optional = None entity_attention_mask: Optional = None entity_token_type_ids: Optional = None entity_position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.luke.modeling_luke.LukeQuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LukeForQuestionAnswering\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n>>> model = LukeForQuestionAnswering.from_pretrained(\"studio-ousia/luke-base\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n\n>>> # target is \"nice puppet\"\n>>> target_start_index = torch.tensor([14])\n>>> target_end_index = torch.tensor([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = outputs.loss\n```"]