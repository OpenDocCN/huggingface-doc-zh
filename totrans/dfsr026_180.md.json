["```py\nimport torch\n\nfrom diffusers import UniDiffuserPipeline\n\ndevice = \"cuda\"\nmodel_id_or_path = \"thu-ml/unidiffuser-v1\"\npipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\npipe.to(device)\n\n# Unconditional image and text generation. The generation task is automatically inferred.\nsample = pipe(num_inference_steps=20, guidance_scale=8.0)\nimage = sample.images[0]\ntext = sample.text[0]\nimage.save(\"unidiffuser_joint_sample_image.png\")\nprint(text)\n```", "```py\n# Equivalent to the above.\npipe.set_joint_mode()\nsample = pipe(num_inference_steps=20, guidance_scale=8.0)\n```", "```py\n# Unlike other generation tasks, image-only and text-only generation don't use classifier-free guidance\n# Image-only generation\npipe.set_image_mode()\nsample_image = pipe(num_inference_steps=20).images[0]\n# Text-only generation\npipe.set_text_mode()\nsample_text = pipe(num_inference_steps=20).text[0]\n```", "```py\nimport torch\n\nfrom diffusers import UniDiffuserPipeline\n\ndevice = \"cuda\"\nmodel_id_or_path = \"thu-ml/unidiffuser-v1\"\npipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\npipe.to(device)\n\n# Text-to-image generation\nprompt = \"an elephant under the sea\"\n\nsample = pipe(prompt=prompt, num_inference_steps=20, guidance_scale=8.0)\nt2i_image = sample.images[0]\nt2i_image\n```", "```py\nimport torch\n\nfrom diffusers import UniDiffuserPipeline\nfrom diffusers.utils import load_image\n\ndevice = \"cuda\"\nmodel_id_or_path = \"thu-ml/unidiffuser-v1\"\npipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\npipe.to(device)\n\n# Image-to-text generation\nimage_url = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/unidiffuser/unidiffuser_example_image.jpg\"\ninit_image = load_image(image_url).resize((512, 512))\n\nsample = pipe(image=init_image, num_inference_steps=20, guidance_scale=8.0)\ni2t_text = sample.text[0]\nprint(i2t_text)\n```", "```py\nimport torch\n\nfrom diffusers import UniDiffuserPipeline\nfrom diffusers.utils import load_image\n\ndevice = \"cuda\"\nmodel_id_or_path = \"thu-ml/unidiffuser-v1\"\npipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\npipe.to(device)\n\n# Image variation can be performed with an image-to-text generation followed by a text-to-image generation:\n# 1\\. Image-to-text generation\nimage_url = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/unidiffuser/unidiffuser_example_image.jpg\"\ninit_image = load_image(image_url).resize((512, 512))\n\nsample = pipe(image=init_image, num_inference_steps=20, guidance_scale=8.0)\ni2t_text = sample.text[0]\nprint(i2t_text)\n\n# 2\\. Text-to-image generation\nsample = pipe(prompt=i2t_text, num_inference_steps=20, guidance_scale=8.0)\nfinal_image = sample.images[0]\nfinal_image.save(\"unidiffuser_image_variation_sample.png\")\n```", "```py\nimport torch\n\nfrom diffusers import UniDiffuserPipeline\n\ndevice = \"cuda\"\nmodel_id_or_path = \"thu-ml/unidiffuser-v1\"\npipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\npipe.to(device)\n\n# Text variation can be performed with a text-to-image generation followed by a image-to-text generation:\n# 1\\. Text-to-image generation\nprompt = \"an elephant under the sea\"\n\nsample = pipe(prompt=prompt, num_inference_steps=20, guidance_scale=8.0)\nt2i_image = sample.images[0]\nt2i_image.save(\"unidiffuser_text2img_sample_image.png\")\n\n# 2\\. Image-to-text generation\nsample = pipe(image=t2i_image, num_inference_steps=20, guidance_scale=8.0)\nfinal_prompt = sample.text[0]\nprint(final_prompt)\n```", "```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel image_encoder: CLIPVisionModelWithProjection clip_image_processor: CLIPImageProcessor clip_tokenizer: CLIPTokenizer text_decoder: UniDiffuserTextDecoder text_tokenizer: GPT2Tokenizer unet: UniDiffuserModel scheduler: KarrasDiffusionSchedulers )\n```", "```py\n( prompt: Union = None image: Union = None height: Optional = None width: Optional = None data_type: Optional = 1 num_inference_steps: int = 50 guidance_scale: float = 8.0 negative_prompt: Union = None num_images_per_prompt: Optional = 1 num_prompts_per_image: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_latents: Optional = None vae_latents: Optional = None clip_latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 ) \u2192 export const metadata = 'undefined';ImageTextPipelineOutput or tuple\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( images: Union text: Union )\n```"]