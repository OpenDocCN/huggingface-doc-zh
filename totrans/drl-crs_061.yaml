- en: Diving deeper into policy-gradient methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入研究政策梯度方法
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit4/policy-gradient](https://huggingface.co/learn/deep-rl-course/unit4/policy-gradient)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/learn/deep-rl-course/unit4/policy-gradient](https://huggingface.co/learn/deep-rl-course/unit4/policy-gradient)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Getting the big picture
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获得大局
- en: We just learned that policy-gradient methods aim to find parameters<math><semantics><mrow><mi>θ</mi></mrow>
    <annotation encoding="application/x-tex">\theta</annotation></semantics></math>
    θ that **maximize the expected return**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学到，政策梯度方法的目标是找到**最大化预期回报的参数**。
- en: The idea is that we have a *parameterized stochastic policy*. In our case, a
    neural network outputs a probability distribution over actions. The probability
    of taking each action is also called the *action preference*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是我们有一个*参数化的随机策略*。在我们的情况下，一个神经网络输出动作的概率分布。采取每个动作的概率也被称为*动作偏好*。
- en: 'If we take the example of CartPole-v1:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以CartPole-v1为例：
- en: As input, we have a state.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为输入，我们有一个状态。
- en: As output, we have a probability distribution over actions at that state.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为输出，我们有在该状态下的动作概率分布。
- en: '![Policy based](../Images/7b4b24746a62f4244cc0e64f74bdaef3.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![基于政策的](../Images/7b4b24746a62f4244cc0e64f74bdaef3.png)'
- en: Our goal with policy-gradient is to **control the probability distribution of
    actions** by tuning the policy such that **good actions (that maximize the return) are
    sampled more frequently in the future.** Each time the agent interacts with the
    environment, we tweak the parameters such that good actions will be sampled more
    likely in the future.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的政策梯度的目标是通过调整策略来**控制动作的概率分布**，使得**好的动作（最大化回报）在未来更频繁地被采样**。每次代理与环境交互时，我们调整参数，以便未来更有可能采样到好的动作。
- en: But **how are we going to optimize the weights using the expected return**?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但是**我们如何使用预期回报来优化权重**？
- en: The idea is that we’re going to **let the agent interact during an episode**.
    And if we win the episode, we consider that each action taken was good and must
    be more sampled in the future since they lead to win.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是**让代理在一个episode期间与环境交互**。如果我们赢得了这一episode，我们认为每个动作都是好的，并且在未来必须更频繁地被采样，因为它们导致了胜利。
- en: 'So for each state-action pair, we want to increase the<math><semantics><mrow><mi>P</mi><mo
    stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(a|s)</annotation></semantics></math>P(a∣s):
    the probability of taking that action at that state. Or decrease if we lost.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于每个状态-动作对，我们希望增加<math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>a</mi><mi
    mathvariant="normal">∣</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">P(a|s)</annotation></semantics></math>P(a∣s)：在该状态下采取该动作的概率。或者如果我们失败了就减少。
- en: 'The Policy-gradient algorithm (simplified) looks like this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 政策梯度算法（简化版）如下所示：
- en: '![Policy Gradient Big Picture](../Images/78f8ee2b54067a5bb57aa1023ac8af61.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![政策梯度大局](../Images/78f8ee2b54067a5bb57aa1023ac8af61.png)'
- en: Now that we got the big picture, let’s dive deeper into policy-gradient methods.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了大局，让我们深入研究政策梯度方法。
- en: Diving deeper into policy-gradient methods
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入研究政策梯度方法
- en: We have our stochastic policy<math><semantics><mrow><mi>π</mi></mrow><annotation
    encoding="application/x-tex">\pi</annotation></semantics></math>π which has a
    parameter<math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math>θ.
    This<math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math>π,
    given a state, **outputs a probability distribution of actions**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有我们的随机策略<math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math>π，它有一个参数<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ。这个<math><semantics><mrow><mi>π</mi></mrow><annotation
    encoding="application/x-tex">\pi</annotation></semantics></math>π，在给定一个状态时，**输出动作的概率分布**。
- en: '![Policy](../Images/9123df7ebedfa0c5bc669c2d2531968f.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![政策](../Images/9123df7ebedfa0c5bc669c2d2531968f.png)'
- en: Where<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi
    mathvariant="normal">∣</mi><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_\theta(a_t|s_t)</annotation></semantics></math>πθ​(at​∣st​)
    is the probability of the agent selecting action<math><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">a_t</annotation></semantics></math>at​ from state<math><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">s_t</annotation></semantics></math>st​ given our
    policy.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi
    mathvariant="normal">∣</mi><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">\pi_\theta(a_t|s_t)</annotation></semantics></math>πθ​(at​∣st​)是代理根据我们的策略从状态<math><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">s_t</annotation></semantics></math>st​选择动作<math><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">a_t</annotation></semantics></math>at​的概率。
- en: '**But how do we know if our policy is good?** We need to have a way to measure
    it. To know that, we define a score/objective function called<math><semantics><mrow><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**但是我们如何知道我们的策略是否好？**我们需要有一种方法来衡量它。为了知道这一点，我们定义了一个叫做<math><semantics><mrow><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ)的得分/目标函数。'
- en: The objective function
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标函数
- en: The *objective function* gives us the **performance of the agent** given a trajectory
    (state action sequence without considering reward (contrary to an episode)), and
    it outputs the *expected cumulative reward*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标函数*给出了在给定轨迹（不考虑奖励的状态动作序列（与一个episode相反））的情况下代理的**表现**，并输出*预期累积奖励*。'
- en: '![Return](../Images/d6c60dbb37bf65407a91cf7b220d932f.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![返回](../Images/d6c60dbb37bf65407a91cf7b220d932f.png)'
- en: 'Let’s give some more details on this formula:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对这个公式进行更详细的说明：
- en: The *expected return* (also called expected cumulative reward), is the weighted
    average (where the weights are given by<math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>τ</mi><mo
    separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">P(\tau;\theta)</annotation></semantics></math>P(τ;θ)
    of all possible values that the return<math><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>τ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R(\tau)</annotation></semantics></math>R(τ)
    can take).
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*期望回报*（也称为期望累积奖励）是所有可能的回报<math><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>τ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R(\tau)</annotation></semantics></math>R(τ)可能取值的加权平均值（其中权重由<math><semantics><mrow><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">P(\tau;\theta)</annotation></semantics></math>P(τ;θ)给出）。'
- en: '![Return](../Images/91e5d228c6d4b4dfdbd4a60c7ae8fa63.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![回报](../Images/91e5d228c6d4b4dfdbd4a60c7ae8fa63.png)'
- en: '-<math><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>τ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R(\tau)</annotation></semantics></math>R(τ)
    : Return from an arbitrary trajectory. To take this quantity and use it to calculate
    the expected return, we need to multiply it by the probability of each possible
    trajectory. -<math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>τ</mi><mo
    separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">P(\tau;\theta)</annotation></semantics></math>P(τ;θ)
    : Probability of each possible trajectory<math><semantics><mrow><mi>τ</mi></mrow><annotation
    encoding="application/x-tex">\tau</annotation></semantics></math>τ (that probability
    depends on<math><semantics><mrow><mi>θ</mi></mrow> <annotation encoding="application/x-tex">\theta</annotation></semantics></math>θ
    since it defines the policy that it uses to select the actions of the trajectory
    which has an impact of the states visited).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: -<math><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>τ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">R(\tau)</annotation></semantics></math>R(τ)：任意轨迹的回报。要使用此数量并用其计算期望回报，我们需要将其乘以每个可能轨迹的概率。-<math><semantics><mrow><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">P(\tau;\theta)</annotation></semantics></math>P(τ;θ)：每个可能轨迹<math><semantics><mrow><mi>τ</mi></mrow><annotation
    encoding="application/x-tex">\tau</annotation></semantics></math>τ的概率（该概率取决于<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ，因为它定义了用于选择轨迹动作的策略，这对访问的状态产生影响）。
- en: '![Probability](../Images/4b4baebaf1b080cb2852e25cd8686dec.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![概率](../Images/4b4baebaf1b080cb2852e25cd8686dec.png)'
- en: '-<math><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ)
    : Expected return, we calculate it by summing for all trajectories, the probability
    of taking that trajectory given<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math> θ multiplied
    by the return of this trajectory.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: -<math><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ)：期望回报，我们通过对所有轨迹求和来计算它，即给定<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ的情况下，取该轨迹的概率乘以该轨迹的回报。
- en: 'Our objective then is to maximize the expected cumulative reward by finding
    the<math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
    θ that will output the best action probability distributions:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是通过找到将输出最佳动作概率分布的<math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math>θ来最大化期望累积奖励：
- en: '![Max objective](../Images/31ba87d76bb37e63c67651a76498624b.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: 最大目标
- en: Gradient Ascent and the Policy-gradient Theorem
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度上升和策略梯度定理
- en: 'Policy-gradient is an optimization problem: we want to find the values of<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ that maximize
    our objective function<math><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ),
    so we need to use **gradient-ascent**. It’s the inverse of *gradient-descent*
    since it gives the direction of the steepest increase of<math><semantics><mrow><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度是一个优化问题：我们希望找到最大化我们目标函数<math><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ)的值，因此我们需要使用**梯度上升**。这是*梯度下降*的反向，因为它给出了<math><semantics><mrow><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ)最陡增加的方向。
- en: (If you need a refresher on the difference between gradient descent and gradient
    ascent [check this](https://www.baeldung.com/cs/gradient-descent-vs-ascent) and
    [this](https://stats.stackexchange.com/questions/258721/gradient-ascent-vs-gradient-descent-in-logistic-regression)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: （如果您需要了解梯度下降和梯度上升之间的区别，请查看[这里](https://www.baeldung.com/cs/gradient-descent-vs-ascent)和[这里](https://stats.stackexchange.com/questions/258721/gradient-ascent-vs-gradient-descent-in-logistic-regression)）。
- en: 'Our update step for gradient-ascent is: <math><semantics><mrow><mi>θ</mi><mo>←</mo><mi>θ</mi><mo>+</mo><mi>α</mi><mo>∗</mo><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">\theta
    \leftarrow \theta + \alpha * \nabla_\theta J(\theta)</annotation></semantics></math>
    θ←θ+α∗∇θ​J(θ)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们梯度上升的更新步骤是：<math><semantics><mrow><mi>θ</mi><mo>←</mo><mi>θ</mi><mo>+</mo><mi>α</mi><mo>∗</mo><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">\theta
    \leftarrow \theta + \alpha * \nabla_\theta J(\theta)</annotation></semantics></math>
    θ←θ+α∗∇θ​J(θ)
- en: We can repeatedly apply this update in the hopes that<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math> θ converges
    to the value that maximizes<math><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以反复应用这个更新，希望<math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
    θ收敛到最大化<math><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ)的值。
- en: 'However, there are two problems with computing the derivative of<math><semantics><mrow><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，计算<math><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ)的导数存在两个问题：
- en: We can’t calculate the true gradient of the objective function since it requires
    calculating the probability of each possible trajectory, which is computationally
    super expensive. So we want to **calculate a gradient estimation with a sample-based
    estimate (collect some trajectories)**.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们无法计算目标函数的真实梯度，因为这需要计算每个可能轨迹的概率，这在计算上非常昂贵。因此，我们希望**使用基于样本的估计（收集一些轨迹）来计算梯度估计**。
- en: We have another problem that I explain in the next optional section. To differentiate
    this objective function, we need to differentiate the state distribution, called
    the Markov Decision Process dynamics. This is attached to the environment. It
    gives us the probability of the environment going into the next state, given the
    current state and the action taken by the agent. The problem is that we can’t
    differentiate it because we might not know about it.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还有另一个问题，我会在下一节（可选）中解释。为了区分这个目标函数，我们需要区分状态分布，称为马尔可夫决策过程动态。这与环境相关联。它给出了环境进入下一个状态的概率，考虑到当前状态和代理采取的动作。问题在于我们无法区分它，因为我们可能不了解它。
- en: '![Probability](../Images/4b4baebaf1b080cb2852e25cd8686dec.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![概率](../Images/4b4baebaf1b080cb2852e25cd8686dec.png)'
- en: Fortunately we’re going to use a solution called the Policy Gradient Theorem
    that will help us to reformulate the objective function into a differentiable
    function that does not involve the differentiation of the state distribution.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们将使用一个称为策略梯度定理的解决方案，它将帮助我们将目标函数重新构造为一个可微分的函数，不涉及状态分布的区分。
- en: '![Policy Gradient](../Images/5a9b6c1a3ee9cf5b0e888fb819446af5.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![策略梯度](../Images/5a9b6c1a3ee9cf5b0e888fb819446af5.png)'
- en: If you want to understand how we derive this formula for approximating the gradient,
    check out the next (optional) section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解我们如何推导出这个近似梯度的公式，请查看下一节（可选）。
- en: The Reinforce algorithm (Monte Carlo Reinforce)
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Reinforce算法（蒙特卡洛Reinforce）
- en: 'The Reinforce algorithm, also called Monte-Carlo policy-gradient, is a policy-gradient
    algorithm that **uses an estimated return from an entire episode to update the
    policy parameter** <math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math>θ:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Reinforce算法，也称为蒙特卡洛策略梯度，是一种策略梯度算法，**使用整个episode的估计回报来更新策略参数** <math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ：
- en: 'In a loop:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个循环中：
- en: Use the policy<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics></math>πθ​ to
    collect an episode<math><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math>τ
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用策略<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics></math>πθ​来收集一个episode<math><semantics><mrow><mi>τ</mi></mrow><annotation
    encoding="application/x-tex">\tau</annotation></semantics></math>τ
- en: Use the episode to estimate the gradient<math><semantics><mrow><mover accent="true"><mi>g</mi><mo>^</mo></mover><mo>=</mo><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{g}
    = \nabla_\theta J(\theta)</annotation></semantics></math>g^​=∇θ​J(θ)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用episode来估计梯度<math><semantics><mrow><mover accent="true"><mi>g</mi><mo>^</mo></mover><mo>=</mo><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{g}
    = \nabla_\theta J(\theta)</annotation></semantics></math>g^​=∇θ​J(θ)
- en: '![Policy Gradient](../Images/e23396a1ff30adf8f68d0df9a6cbd339.png)'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![策略梯度](../Images/e23396a1ff30adf8f68d0df9a6cbd339.png)'
- en: Update the weights of the policy:<math><semantics><mrow><mi>θ</mi><mo>←</mo><mi>θ</mi><mo>+</mo><mi>α</mi><mover
    accent="true"><mi>g</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\theta
    \leftarrow \theta + \alpha \hat{g}</annotation></semantics></math>θ←θ+αg^​
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新策略的权重：<math><semantics><mrow><mi>θ</mi><mo>←</mo><mi>θ</mi><mo>+</mo><mi>α</mi><mover
    accent="true"><mi>g</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\theta
    \leftarrow \theta + \alpha \hat{g}</annotation></semantics></math>θ←θ+αg^​
- en: 'We can interpret this update as follows: -<math><semantics><mrow><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><msub><mi>a</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>s</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\nabla_\theta
    log \pi_\theta(a_t|s_t)</annotation></semantics></math>∇θ​logπθ​(at​∣st​) is the
    direction of **steepest increase of the (log) probability** of selecting action
    at from state st. This tells us **how we should change the weights of policy**
    if we want to increase/decrease the log probability of selecting action<math><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">a_t</annotation></semantics></math>at​ at state<math><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">s_t</annotation></semantics></math>st​. -<math><semantics><mrow><mi>R</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">R(\tau)</annotation></semantics></math>R(τ): is the
    scoring function:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个更新解释如下：-∇θ​logπθ​(at​∣st​)是选择动作at​从状态st​的（对数）概率的**最陡增加方向**。这告诉我们如果我们想增加/减少选择动作at​的对数概率，我们应该如何改变策略的权重。
- en: If the return is high, it will **push up the probabilities** of the (state,
    action) combinations.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，如果回报很低，它将**降低**（状态，动作）组合的概率。
- en: Otherwise, if the return is low, it will **push down the probabilities** of
    the (state, action) combinations.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R(τ)：是评分函数：
- en: 'We can also **collect multiple episodes (trajectories)** to estimate the gradient:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以**收集多个轨迹（路径）**来估计梯度：
- en: '![Policy Gradient](../Images/327a4c57d46079088dd2af66d56e830d.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: 如果回报很高，它将**提高**（状态，动作）组合的概率。
