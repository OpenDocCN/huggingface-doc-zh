- en: Accelerator
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ é€Ÿå™¨
- en: 'Original text: [https://huggingface.co/docs/accelerate/package_reference/accelerator](https://huggingface.co/docs/accelerate/package_reference/accelerator)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/accelerate/package_reference/accelerator](https://huggingface.co/docs/accelerate/package_reference/accelerator)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    is the main class provided by ğŸ¤— Accelerate. It serves at the main entry point
    for the API.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)æ˜¯ğŸ¤—
    Accelerateæä¾›çš„ä¸»è¦ç±»ã€‚å®ƒä½œä¸ºAPIçš„ä¸»è¦å…¥å£ç‚¹ã€‚'
- en: Quick adaptation of your code
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»£ç çš„å¿«é€Ÿé€‚åº”
- en: 'To quickly adapt your script to work on any kind of setup with ğŸ¤— Accelerate
    just:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è„šæœ¬å¿«é€Ÿé€‚åº”ä»»ä½•ç±»å‹çš„è®¾ç½®ä¸ğŸ¤— Accelerateä¸€èµ·å·¥ä½œåªéœ€ï¼š
- en: Initialize an [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    object (that we will call `accelerator` throughout this page) as early as possible
    in your script.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°½æ—©åœ¨è„šæœ¬ä¸­åˆå§‹åŒ–ä¸€ä¸ª[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)å¯¹è±¡ï¼ˆæˆ‘ä»¬å°†åœ¨æœ¬é¡µä¸­ç§°ä¹‹ä¸º`accelerator`ï¼‰ã€‚
- en: Pass your dataloader(s), model(s), optimizer(s), and scheduler(s) to the [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    method.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ‚¨çš„æ•°æ®åŠ è½½å™¨ã€æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ä¼ é€’ç»™[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)æ–¹æ³•ã€‚
- en: Remove all the `.cuda()` or `.to(device)` from your code and let the `accelerator`
    handle the device placement for you.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»æ‚¨çš„ä»£ç ä¸­åˆ é™¤æ‰€æœ‰`.cuda()`æˆ–`.to(device)`ï¼Œè®©`accelerator`ä¸ºæ‚¨å¤„ç†è®¾å¤‡æ”¾ç½®ã€‚
- en: Step three is optional, but considered a best practice.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰æ­¥æ˜¯å¯é€‰çš„ï¼Œä½†è¢«è®¤ä¸ºæ˜¯æœ€ä½³å®è·µã€‚
- en: Replace `loss.backward()` in your code with `accelerator.backward(loss)`
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ‚¨çš„ä»£ç ä¸­ç”¨`accelerator.backward(loss)`æ›¿æ¢`loss.backward()`
- en: Gather your predictions and labels before storing them or using them for metric
    computation using [gather()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather)
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨å­˜å‚¨æˆ–ä½¿ç”¨é¢„æµ‹å’Œæ ‡ç­¾ä¹‹å‰ï¼Œä½¿ç”¨[gather()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather)æ¥æ”¶é›†å®ƒä»¬ï¼Œä»¥è¿›è¡Œåº¦é‡è®¡ç®—ã€‚
- en: Step five is mandatory when using distributed evaluation
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿›è¡Œåˆ†å¸ƒå¼è¯„ä¼°æ—¶ï¼Œç¬¬äº”æ­¥æ˜¯å¼ºåˆ¶çš„
- en: 'In most cases this is all that is needed. The next section lists a few more
    advanced use cases and nice features you should search for and replace by the
    corresponding methods of your `accelerator`:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè¿™å°±æ˜¯æ‰€éœ€çš„ã€‚ä¸‹ä¸€èŠ‚åˆ—å‡ºäº†ä¸€äº›æ›´é«˜çº§çš„ç”¨ä¾‹å’Œæ‚¨åº”è¯¥æœç´¢å¹¶æ›¿æ¢ä¸º`accelerator`ç›¸åº”æ–¹æ³•çš„ä¸€äº›ä¸é”™çš„åŠŸèƒ½ï¼š
- en: Advanced recommendations
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é«˜çº§å»ºè®®
- en: Printing
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‰“å°
- en: '`print` statements should be replaced by [print()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.print)
    to be printed once per process:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å°†`print`è¯­å¥æ›¿æ¢ä¸º[print()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.print)ä»¥æ¯ä¸ªè¿›ç¨‹æ‰“å°ä¸€æ¬¡ï¼š
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Executing processes
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‰§è¡Œè¿›ç¨‹
- en: Once on a single server
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä¸€æ¬¡åœ¨å•ä¸ªæœåŠ¡å™¨ä¸Š
- en: 'For statements that should be executed once per server, use `is_local_main_process`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåº”åœ¨æ¯å°æœåŠ¡å™¨ä¸Šæ‰§è¡Œä¸€æ¬¡çš„è¯­å¥ï¼Œä½¿ç”¨`is_local_main_process`ï¼š
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A function can be wrapped using the [on_local_main_process()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.on_local_main_process)
    function to achieve the same behavior on a functionâ€™s execution:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[on_local_main_process()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.on_local_main_process)å‡½æ•°æ¥åŒ…è£…å‡½æ•°ï¼Œä»¥åœ¨å‡½æ•°æ‰§è¡Œæ—¶å®ç°ç›¸åŒçš„è¡Œä¸ºï¼š
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Only ever once across all servers
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: åœ¨æ‰€æœ‰æœåŠ¡å™¨ä¸Šä»…ä¸€æ¬¡
- en: 'For statements that should only ever be executed once, use `is_main_process`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåº”ä»…æ‰§è¡Œä¸€æ¬¡çš„è¯­å¥ï¼Œä½¿ç”¨`is_main_process`ï¼š
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A function can be wrapped using the [on_main_process()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.on_main_process)
    function to achieve the same behavior on a functionâ€™s execution:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[on_main_process()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.on_main_process)å‡½æ•°æ¥åŒ…è£…å‡½æ•°ï¼Œä»¥åœ¨å‡½æ•°æ‰§è¡Œæ—¶å®ç°ç›¸åŒçš„è¡Œä¸ºï¼š
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: On specific processes
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: åœ¨ç‰¹å®šè¿›ç¨‹ä¸Š
- en: 'If a function should be ran on a specific overall or local process index, there
    are similar decorators to achieve this:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå‡½æ•°åº”åœ¨ç‰¹å®šçš„æ•´ä½“æˆ–æœ¬åœ°è¿›ç¨‹ç´¢å¼•ä¸Šè¿è¡Œï¼Œæœ‰ç±»ä¼¼çš„è£…é¥°å™¨å¯å®ç°æ­¤ç›®çš„ï¼š
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Synchronicity control
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŒæ­¥æ§åˆ¶
- en: Use [wait_for_everyone()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.wait_for_everyone)
    to make sure all processes join that point before continuing. (Useful before a
    model save for instance).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[wait_for_everyone()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.wait_for_everyone)ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åœ¨ç»§ç»­ä¹‹å‰åŠ å…¥è¯¥ç‚¹ã€‚ï¼ˆä¾‹å¦‚ï¼Œåœ¨ä¿å­˜æ¨¡å‹ä¹‹å‰å¾ˆæœ‰ç”¨ï¼‰ã€‚
- en: Saving and loading
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¿å­˜å’ŒåŠ è½½
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Use [save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)
    instead of `torch.save` to save a model. It will remove all model wrappers added
    during the distributed process, get the state_dict of the model and save it. The
    state_dict will be in the same precision as the model being trained.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)ä»£æ›¿`torch.save`æ¥ä¿å­˜æ¨¡å‹ã€‚å®ƒå°†åˆ é™¤åœ¨åˆ†å¸ƒå¼è¿‡ç¨‹ä¸­æ·»åŠ çš„æ‰€æœ‰æ¨¡å‹åŒ…è£…å™¨ï¼Œè·å–æ¨¡å‹çš„state_dictå¹¶ä¿å­˜å®ƒã€‚state_dictå°†ä¸æ­£åœ¨è®­ç»ƒçš„æ¨¡å‹å…·æœ‰ç›¸åŒçš„ç²¾åº¦ã€‚
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)
    can also save a model into sharded checkpoints or with safetensors format. Here
    is an example:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)è¿˜å¯ä»¥å°†æ¨¡å‹ä¿å­˜ä¸ºåˆ†ç‰‡æ£€æŸ¥ç‚¹æˆ–ä½¿ç”¨safetensorsæ ¼å¼ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š'
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ğŸ¤— Transformers models
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ğŸ¤— Transformersæ¨¡å‹
- en: If you are using models from the [ğŸ¤— Transformers](https://huggingface.co/docs/transformers/)
    library, you can use the `.save_pretrained()` method.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨[ğŸ¤— Transformers](https://huggingface.co/docs/transformers/)åº“ä¸­çš„æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨`.save_pretrained()`æ–¹æ³•ã€‚
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This will ensure your model stays compatible with other ğŸ¤— Transformers functionality
    like the `.from_pretrained()` method.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†ç¡®ä¿æ‚¨çš„æ¨¡å‹ä¸å…¶ä»–ğŸ¤— TransformersåŠŸèƒ½ä¿æŒå…¼å®¹ï¼Œå¦‚`.from_pretrained()`æ–¹æ³•ã€‚
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Operations
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ“ä½œ
- en: Use [clip*grad_norm*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_norm_)
    instead of `torch.nn.utils.clip_grad_norm_` and [clip*grad_value*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_value_)
    instead of `torch.nn.utils.clip_grad_value`
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[clip*grad_norm*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_norm_)ä»£æ›¿`torch.nn.utils.clip_grad_norm_`ï¼Œå¹¶ä½¿ç”¨[clip*grad_value*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_value_)ä»£æ›¿`torch.nn.utils.clip_grad_value`
- en: Gradient Accumulation
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯
- en: 'To perform gradient accumulation use [accumulate()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.accumulate)
    and specify a gradient_accumulation_steps. This will also automatically ensure
    the gradients are synced or unsynced when on multi-device training, check if the
    step should actually be performed, and auto-scale the loss:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ‰§è¡Œæ¢¯åº¦ç´¯ç§¯ï¼Œè¯·ä½¿ç”¨[accumulate()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.accumulate)å¹¶æŒ‡å®šgradient_accumulation_stepsã€‚è¿™ä¹Ÿå°†è‡ªåŠ¨ç¡®ä¿åœ¨å¤šè®¾å¤‡è®­ç»ƒæ—¶åŒæ­¥æˆ–å¼‚æ­¥æ¢¯åº¦ï¼Œæ£€æŸ¥æ˜¯å¦å®é™…æ‰§è¡Œæ­¥éª¤ï¼Œå¹¶è‡ªåŠ¨ç¼©æ”¾æŸå¤±ï¼š
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: GradientAccumulationPlugin
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GradientAccumulationPlugin
- en: '### `class accelerate.utils.GradientAccumulationPlugin`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.utils.GradientAccumulationPlugin`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L505)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L505)'
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: A plugin to configure gradient accumulation behavior.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç”¨äºé…ç½®æ¢¯åº¦ç´¯ç§¯è¡Œä¸ºçš„æ’ä»¶ã€‚
- en: Instead of passing `gradient_accumulation_steps` you can instantiate a GradientAccumulationPlugin
    and pass it to the [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)â€™s
    `__init__` as `gradient_accumulation_plugin`. You can only pass either one of
    `gradient_accumulation_plugin` or `gradient_accumulation_steps` passing both will
    raise an error.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¦ä¼ é€’`gradient_accumulation_steps`ï¼Œæ‚¨å¯ä»¥å®ä¾‹åŒ–ä¸€ä¸ªGradientAccumulationPluginå¹¶å°†å…¶ä¼ é€’ç»™[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)çš„`__init__`ä½œä¸º`gradient_accumulation_plugin`ã€‚æ‚¨åªèƒ½ä¼ é€’`gradient_accumulation_plugin`æˆ–`gradient_accumulation_steps`ä¸­çš„ä¸€ä¸ªï¼Œä¼ é€’ä¸¤è€…å°†å¼•å‘é”™è¯¯ã€‚
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In addition to the number of steps, this also lets you configure whether or
    not you adjust your learning rate scheduler to account for the change in steps
    due to accumulation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æ­¥æ•°ä¹‹å¤–ï¼Œè¿™è¿˜è®©æ‚¨é…ç½®æ˜¯å¦è°ƒæ•´å­¦ä¹ ç‡è°ƒåº¦ç¨‹åºä»¥è€ƒè™‘ç”±äºç´¯ç§¯è€Œå¯¼è‡´çš„æ­¥éª¤å˜åŒ–ã€‚
- en: 'Overall API documentation:'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ€»ä½“APIæ–‡æ¡£ï¼š
- en: '### `class accelerate.Accelerator`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.Accelerator`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L153)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L153)'
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`device_placement` (`bool`, *optional*, defaults to `True`) â€” Whether or not
    the accelerator should put objects on device (tensors yielded by the dataloader,
    model, etcâ€¦).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_placement` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” åŠ é€Ÿå™¨æ˜¯å¦åº”è¯¥å°†å¯¹è±¡æ”¾åœ¨è®¾å¤‡ä¸Šï¼ˆæ•°æ®åŠ è½½å™¨äº§ç”Ÿçš„å¼ é‡ï¼Œæ¨¡å‹ç­‰ï¼‰ã€‚'
- en: '`split_batches` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    the accelerator should split the batches yielded by the dataloaders across the
    devices. If `True` the actual batch size used will be the same on any kind of
    distributed processes, but it must be a round multiple of the `num_processes`
    you are using. If `False`, actual batch size used will be the one set in your
    script multiplied by the number of processes.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_batches` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” åŠ é€Ÿå™¨æ˜¯å¦åº”è¯¥å°†æ•°æ®åŠ è½½å™¨äº§ç”Ÿçš„æ‰¹æ¬¡åˆ†é…åˆ°ä¸åŒçš„è®¾å¤‡ä¸Šã€‚å¦‚æœä¸º`True`ï¼Œå®é™…ä½¿ç”¨çš„æ‰¹æ¬¡å¤§å°å°†åœ¨ä»»ä½•ç±»å‹çš„åˆ†å¸ƒå¼è¿›ç¨‹ä¸Šç›¸åŒï¼Œä½†å¿…é¡»æ˜¯æ‚¨ä½¿ç”¨çš„`num_processes`çš„åœ†å€æ•°ã€‚å¦‚æœä¸º`False`ï¼Œå®é™…ä½¿ç”¨çš„æ‰¹æ¬¡å¤§å°å°†æ˜¯è„šæœ¬ä¸­è®¾ç½®çš„æ‰¹æ¬¡å¤§å°ä¹˜ä»¥è¿›ç¨‹æ•°ã€‚'
- en: '`mixed_precision` (`str`, *optional*) â€” Whether or not to use mixed precision
    training. Choose from â€˜noâ€™,â€˜fp16â€™,â€˜bf16 or â€˜fp8â€™. Will default to the value in
    the environment variable `ACCELERATE_MIXED_PRECISION`, which will use the default
    value in the accelerate config of the current system or the flag passed with the
    `accelerate.launch` command. â€˜fp8â€™ requires the installation of transformers-engine.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mixed_precision` (`str`, *optional*) â€” æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒã€‚é€‰æ‹©â€˜noâ€™ã€â€˜fp16â€™ã€â€˜bf16â€™æˆ–â€˜fp8â€™ã€‚å°†é»˜è®¤ä¸ºç¯å¢ƒå˜é‡`ACCELERATE_MIXED_PRECISION`ä¸­çš„å€¼ï¼Œè¯¥å€¼å°†ä½¿ç”¨å½“å‰ç³»ç»Ÿçš„åŠ é€Ÿé…ç½®ä¸­çš„é»˜è®¤å€¼æˆ–ä½¿ç”¨`accelerate.launch`å‘½ä»¤ä¼ é€’çš„æ ‡å¿—ã€‚â€˜fp8â€™éœ€è¦å®‰è£…transformers-engineã€‚'
- en: '`gradient_accumulation_steps` (`int`, *optional*, default to 1) â€” The number
    of steps that should pass before gradients are accumulated. A number > 1 should
    be combined with `Accelerator.accumulate`. If not passed, will default to the
    value in the environment variable `ACCELERATE_GRADIENT_ACCUMULATION_STEPS`. Can
    also be configured through a `GradientAccumulationPlugin`.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_accumulation_steps` (`int`, *optional*, default to 1) â€” åœ¨æ¢¯åº¦ç´¯ç§¯ä¹‹å‰åº”è¯¥ç»è¿‡çš„æ­¥æ•°ã€‚æ•°å­—å¤§äº1åº”è¯¥ä¸`Accelerator.accumulate`ç»“åˆä½¿ç”¨ã€‚å¦‚æœæœªä¼ é€’ï¼Œåˆ™é»˜è®¤ä¸ºç¯å¢ƒå˜é‡`ACCELERATE_GRADIENT_ACCUMULATION_STEPS`ä¸­çš„å€¼ã€‚ä¹Ÿå¯ä»¥é€šè¿‡`GradientAccumulationPlugin`è¿›è¡Œé…ç½®ã€‚'
- en: '`cpu` (`bool`, *optional*) â€” Whether or not to force the script to execute
    on CPU. Will ignore GPU available if set to `True` and force the execution on
    one process only.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cpu` (`bool`, *optional*) â€” æ˜¯å¦å¼ºåˆ¶è„šæœ¬åœ¨CPUä¸Šæ‰§è¡Œã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†å¿½ç•¥GPUçš„å¯ç”¨æ€§ï¼Œå¹¶å¼ºåˆ¶åœ¨ä¸€ä¸ªè¿›ç¨‹ä¸Šæ‰§è¡Œã€‚'
- en: '`deepspeed_plugin` (`DeepSpeedPlugin`, *optional*) â€” Tweak your DeepSpeed related
    args using this argument. This argument is optional and can be configured directly
    using *accelerate config*'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deepspeed_plugin` (`DeepSpeedPlugin`, *optional*) â€” ä½¿ç”¨æ­¤å‚æ•°è°ƒæ•´æ‚¨çš„DeepSpeedç›¸å…³å‚æ•°ã€‚æ­¤å‚æ•°æ˜¯å¯é€‰çš„ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨*accelerate
    config*è¿›è¡Œé…ç½®ã€‚'
- en: '`fsdp_plugin` (`FullyShardedDataParallelPlugin`, *optional*) â€” Tweak your FSDP
    related args using this argument. This argument is optional and can be configured
    directly using *accelerate config*'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsdp_plugin` (`FullyShardedDataParallelPlugin`, *optional*) â€” ä½¿ç”¨æ­¤å‚æ•°è°ƒæ•´æ‚¨çš„FSDPç›¸å…³å‚æ•°ã€‚æ­¤å‚æ•°æ˜¯å¯é€‰çš„ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨*accelerate
    config*è¿›è¡Œé…ç½®ã€‚'
- en: '`megatron_lm_plugin` (`MegatronLMPlugin`, *optional*) â€” Tweak your MegatronLM
    related args using this argument. This argument is optional and can be configured
    directly using *accelerate config*'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`megatron_lm_plugin` (`MegatronLMPlugin`, *optional*) â€” ä½¿ç”¨æ­¤å‚æ•°è°ƒæ•´æ‚¨çš„MegatronLMç›¸å…³å‚æ•°ã€‚æ­¤å‚æ•°æ˜¯å¯é€‰çš„ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨*accelerate
    config*è¿›è¡Œé…ç½®ã€‚'
- en: '`rng_types` (list of `str` or [RNGType](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.RNGType))
    â€” The list of random number generators to synchronize at the beginning of each
    iteration in your prepared dataloaders. Should be one or several of:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rng_types`ï¼ˆ`str`åˆ—è¡¨æˆ–[RNGType](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.RNGType)ï¼‰â€”
    åœ¨å‡†å¤‡å¥½çš„æ•°æ®åŠ è½½å™¨çš„æ¯æ¬¡è¿­ä»£å¼€å§‹æ—¶è¦åŒæ­¥çš„éšæœºæ•°ç”Ÿæˆå™¨åˆ—è¡¨ã€‚åº”è¯¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€æˆ–å‡ ä¸ªï¼š'
- en: '`"torch"`: the base torch random number generator'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"torch"`ï¼šåŸºæœ¬torchéšæœºæ•°ç”Ÿæˆå™¨'
- en: '`"cuda"`: the CUDA random number generator (GPU only)'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"cuda"`ï¼šCUDAéšæœºæ•°ç”Ÿæˆå™¨ï¼ˆä»…é™GPUï¼‰'
- en: '`"xla"`: the XLA random number generator (TPU only)'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"xla"`ï¼šXLAéšæœºæ•°ç”Ÿæˆå™¨ï¼ˆä»…é™TPUï¼‰'
- en: '`"generator"`: the `torch.Generator` of the sampler (or batch sampler if there
    is no sampler in your dataloader) or of the iterable dataset (if it exists) if
    the underlying dataset is of that type.'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"generator"`ï¼šé‡‡æ ·å™¨çš„`torch.Generator`ï¼ˆå¦‚æœæ•°æ®åŠ è½½å™¨ä¸­æ²¡æœ‰é‡‡æ ·å™¨ï¼Œåˆ™ä¸ºæ‰¹æ¬¡é‡‡æ ·å™¨ï¼‰æˆ–å¯è¿­ä»£æ•°æ®é›†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰çš„ç”Ÿæˆå™¨ï¼ˆå¦‚æœåŸºç¡€æ•°æ®é›†æ˜¯è¯¥ç±»å‹ï¼‰ã€‚'
- en: Will default to `["torch"]` for PyTorch versions <=1.5.1 and `["generator"]`
    for PyTorch versions >= 1.6.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äºPyTorchç‰ˆæœ¬<=1.5.1ï¼Œé»˜è®¤ä¸º`["torch"]`ï¼Œå¯¹äºPyTorchç‰ˆæœ¬>= 1.6ï¼Œé»˜è®¤ä¸º`["generator"]`ã€‚
- en: '`log_with` (list of `str`, [LoggerType](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.LoggerType)
    or [GeneralTracker](/docs/accelerate/v0.27.2/en/package_reference/tracking#accelerate.tracking.GeneralTracker),
    *optional*) â€” A list of loggers to be setup for experiment tracking. Should be
    one or several of:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_with`ï¼ˆ`str`åˆ—è¡¨ï¼Œ[LoggerType](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.LoggerType)æˆ–[GeneralTracker](/docs/accelerate/v0.27.2/en/package_reference/tracking#accelerate.tracking.GeneralTracker)ï¼Œ*å¯é€‰*ï¼‰â€”
    è¦ä¸ºå®éªŒè·Ÿè¸ªè®¾ç½®çš„è®°å½•å™¨åˆ—è¡¨ã€‚åº”è¯¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€æˆ–å‡ ä¸ªï¼š'
- en: '`"all"`'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"all"`'
- en: '`"tensorboard"`'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"tensorboard"`'
- en: '`"wandb"`'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"wandb"`'
- en: '`"comet_ml"` If `"all"` is selected, will pick up all available trackers in
    the environment and initialize them. Can also accept implementations of `GeneralTracker`
    for custom trackers, and can be combined with `"all"`.'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"comet_ml"` å¦‚æœé€‰æ‹©`"all"`ï¼Œå°†æ¡èµ·ç¯å¢ƒä¸­æ‰€æœ‰å¯ç”¨çš„è·Ÿè¸ªå™¨å¹¶åˆå§‹åŒ–å®ƒä»¬ã€‚è¿˜å¯ä»¥æ¥å—`GeneralTracker`çš„è‡ªå®šä¹‰è·Ÿè¸ªå™¨çš„å®ç°ï¼Œå¹¶ä¸”å¯ä»¥ä¸`"all"`ç»„åˆä½¿ç”¨ã€‚'
- en: '`project_config` (`ProjectConfiguration`, *optional*) â€” A configuration for
    how saving the state can be handled.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`project_config`ï¼ˆ`ProjectConfiguration`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºå¤„ç†çŠ¶æ€ä¿å­˜çš„é…ç½®ã€‚'
- en: '`project_dir` (`str`, `os.PathLike`, *optional*) â€” A path to a directory for
    storing data such as logs of locally-compatible loggers and potentially saved
    checkpoints.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`project_dir`ï¼ˆ`str`ï¼Œ`os.PathLike`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºå­˜å‚¨æ•°æ®çš„ç›®å½•è·¯å¾„ï¼Œä¾‹å¦‚æœ¬åœ°å…¼å®¹æ—¥å¿—è®°å½•å™¨çš„æ—¥å¿—å’Œå¯èƒ½çš„ä¿å­˜æ£€æŸ¥ç‚¹ã€‚'
- en: '`dispatch_batches` (`bool`, *optional*) â€” If set to `True`, the dataloader
    prepared by the Accelerator is only iterated through on the main process and then
    the batches are split and broadcast to each process. Will default to `True` for
    `DataLoader` whose underlying dataset is an `IterableDataset`, `False` otherwise.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dispatch_batches`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™ç”±åŠ é€Ÿå™¨å‡†å¤‡çš„æ•°æ®åŠ è½½å™¨ä»…åœ¨ä¸»è¿›ç¨‹ä¸Šè¿›è¡Œè¿­ä»£ï¼Œç„¶åå°†æ‰¹æ¬¡åˆ†å‰²å¹¶å¹¿æ’­åˆ°æ¯ä¸ªè¿›ç¨‹ã€‚å¯¹äºå…¶åŸºç¡€æ•°æ®é›†ä¸º`IterableDataset`çš„`DataLoader`ï¼Œé»˜è®¤ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`ã€‚'
- en: '`even_batches` (`bool`, *optional*, defaults to `True`) â€” If set to `True`,
    in cases where the total batch size across all processes does not exactly divide
    the dataset, samples at the start of the dataset will be duplicated so the batch
    can be divided equally among all workers.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`even_batches`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåœ¨æ‰€æœ‰è¿›ç¨‹çš„æ€»æ‰¹æ¬¡å¤§å°ä¸èƒ½å®Œå…¨æ•´é™¤æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œæ•°æ®é›†å¼€å¤´çš„æ ·æœ¬å°†è¢«å¤åˆ¶ï¼Œä»¥ä¾¿æ‰¹æ¬¡å¯ä»¥åœ¨æ‰€æœ‰å·¥ä½œè¿›ç¨‹ä¹‹é—´å‡åŒ€åˆ†é…ã€‚'
- en: '`use_seedable_sampler` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not use a fully seedable random sampler (`SeedableRandomSampler`). Ensures
    training results are fully reproducable using a different sampling technique.
    While seed-to-seed results may differ, on average the differences are neglible
    when using multiple different seeds to compare. Should also be ran with [set_seed()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.set_seed)
    each time for the best results.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_seedable_sampler`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦ä½¿ç”¨å®Œå…¨å¯ç§å­çš„éšæœºé‡‡æ ·å™¨ï¼ˆ`SeedableRandomSampler`ï¼‰ã€‚ç¡®ä¿ä½¿ç”¨ä¸åŒçš„é‡‡æ ·æŠ€æœ¯æ—¶è®­ç»ƒç»“æœæ˜¯å®Œå…¨å¯é‡ç°çš„ã€‚è™½ç„¶ç§å­åˆ°ç§å­çš„ç»“æœå¯èƒ½ä¸åŒï¼Œä½†å¹³å‡è€Œè¨€ï¼Œä½¿ç”¨å¤šä¸ªä¸åŒçš„ç§å­è¿›è¡Œæ¯”è¾ƒæ—¶å·®å¼‚å¾®ä¸è¶³é“ã€‚æ¯æ¬¡éƒ½åº”è¯¥ä¸[set_seed()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.set_seed)ä¸€èµ·è¿è¡Œä»¥è·å¾—æœ€ä½³ç»“æœã€‚'
- en: '`step_scheduler_with_optimizer` (`bool`, *optional`, defaults to` True`) --
    Set` True`if the learning rate scheduler is stepped at the same time as the optimizer,`False`
    if only done under certain circumstances (at the end of each epoch, for instance).'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`step_scheduler_with_optimizer`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰-- å¦‚æœå­¦ä¹ ç‡è°ƒåº¦ç¨‹åºä¸ä¼˜åŒ–å™¨åŒæ—¶è¿›è¡Œæ­¥è¿›ï¼Œåˆ™è®¾ç½®ä¸º`True`ï¼Œå¦‚æœä»…åœ¨æŸäº›æƒ…å†µä¸‹è¿›è¡Œï¼ˆä¾‹å¦‚åœ¨æ¯ä¸ªæ—¶æœŸç»“æŸæ—¶ï¼‰ï¼Œåˆ™è®¾ç½®ä¸º`False`ã€‚'
- en: '`kwargs_handlers` (`list[KwargHandler]`, *optional*) â€” A list of `KwargHandler`
    to customize how the objects related to distributed training or mixed precision
    are created. See [kwargs](kwargs) for more information.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs_handlers`ï¼ˆ`list[KwargHandler]`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè‡ªå®šä¹‰ä¸åˆ†å¸ƒå¼è®­ç»ƒæˆ–æ··åˆç²¾åº¦ç›¸å…³çš„å¯¹è±¡å¦‚ä½•åˆ›å»ºçš„`KwargHandler`åˆ—è¡¨ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[kwargs](kwargs)ã€‚'
- en: '`dynamo_backend` (`str` or `DynamoBackend`, *optional*, defaults to `"no"`)
    â€” Set to one of the possible dynamo backends to optimize your training with torch
    dynamo.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dynamo_backend`ï¼ˆ`str`æˆ–`DynamoBackend`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"no"`ï¼‰â€” è®¾ç½®ä¸ºå¯èƒ½çš„dynamoåç«¯ä¹‹ä¸€ï¼Œä»¥ä¼˜åŒ–ä½¿ç”¨torch
    dynamoè¿›è¡Œè®­ç»ƒã€‚'
- en: '`gradient_accumulation_plugin` (`GradientAccumulationPlugin`, *optional*) â€”
    A configuration for how gradient accumulation should be handled, if more tweaking
    than just the `gradient_accumulation_steps` is needed.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_accumulation_plugin`ï¼ˆ`GradientAccumulationPlugin`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºå¤„ç†æ¢¯åº¦ç´¯ç§¯çš„é…ç½®ï¼Œå¦‚æœéœ€è¦æ¯”ä»…ä½¿ç”¨`gradient_accumulation_steps`æ›´å¤šçš„è°ƒæ•´ã€‚'
- en: Creates an instance of an accelerator for distributed training (on multi-GPU,
    TPU) or mixed precision training.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªç”¨äºåˆ†å¸ƒå¼è®­ç»ƒï¼ˆåœ¨å¤šGPUã€TPUä¸Šï¼‰æˆ–æ··åˆç²¾åº¦è®­ç»ƒçš„åŠ é€Ÿå™¨å®ä¾‹ã€‚
- en: '**Available attributes:**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¯ç”¨å±æ€§ï¼š**'
- en: '`device` (`torch.device`) â€” The device to use.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`ï¼ˆ`torch.device`ï¼‰â€” è¦ä½¿ç”¨çš„è®¾å¤‡ã€‚'
- en: '`distributed_type` ([DistributedType](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.DistributedType))
    â€” The distributed training configuration.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distributed_type`ï¼ˆ[DistributedType](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.DistributedType)ï¼‰â€”
    åˆ†å¸ƒå¼è®­ç»ƒé…ç½®ã€‚'
- en: '`local_process_index` (`int`) â€” The process index on the current machine.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_process_index`ï¼ˆ`int`ï¼‰â€” å½“å‰æœºå™¨ä¸Šçš„è¿›ç¨‹ç´¢å¼•ã€‚'
- en: '`mixed_precision` (`str`) â€” The configured mixed precision mode.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mixed_precision`ï¼ˆ`str`ï¼‰-é…ç½®çš„æ··åˆç²¾åº¦æ¨¡å¼ã€‚'
- en: '`num_processes` (`int`) â€” The total number of processes used for training.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_processes`ï¼ˆ`int`ï¼‰-ç”¨äºè®­ç»ƒçš„è¿›ç¨‹æ€»æ•°ã€‚'
- en: '`optimizer_step_was_skipped` (`bool`) â€” Whether or not the optimizer update
    was skipped (because of gradient overflow in mixed precision), in which case the
    learning rate should not be changed.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer_step_was_skipped`ï¼ˆ`bool`ï¼‰-æ˜¯å¦è·³è¿‡äº†ä¼˜åŒ–å™¨æ›´æ–°ï¼ˆå› ä¸ºåœ¨æ··åˆç²¾åº¦ä¸­æ¢¯åº¦æº¢å‡ºï¼‰ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹å­¦ä¹ ç‡ä¸åº”è¯¥æ”¹å˜ã€‚'
- en: '`process_index` (`int`) â€” The overall index of the current process among all
    processes.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`process_index`ï¼ˆ`int`ï¼‰-å½“å‰è¿›ç¨‹åœ¨æ‰€æœ‰è¿›ç¨‹ä¸­çš„æ€»ä½“ç´¢å¼•ã€‚'
- en: '`state` ([AcceleratorState](/docs/accelerate/v0.27.2/en/package_reference/state#accelerate.state.AcceleratorState))
    â€” The distributed setup state.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state`ï¼ˆ[AcceleratorState](/docs/accelerate/v0.27.2/en/package_reference/state#accelerate.state.AcceleratorState)ï¼‰-åˆ†å¸ƒå¼è®¾ç½®çŠ¶æ€ã€‚'
- en: '`sync_gradients` (`bool`) â€” Whether the gradients are currently being synced
    across all processes.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sync_gradients`ï¼ˆ`bool`ï¼‰-å½“å‰æ˜¯å¦æ­£åœ¨è·¨æ‰€æœ‰è¿›ç¨‹åŒæ­¥æ¢¯åº¦ã€‚'
- en: '`use_distributed` (`bool`) â€” Whether the current configuration is for distributed
    training.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_distributed`ï¼ˆ`bool`ï¼‰-å½“å‰é…ç½®æ˜¯å¦ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒã€‚'
- en: '#### `accumulate`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accumulate`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L967)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L967)'
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`*models` (list of `torch.nn.Module`) â€” PyTorch Modules that were prepared
    with `Accelerator.prepare`. Models passed to `accumulate()` will skip gradient
    syncing during backward pass in distributed training'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*models`ï¼ˆ`torch.nn.Module`åˆ—è¡¨ï¼‰-ä½¿ç”¨`Accelerator.prepare`å‡†å¤‡çš„PyTorchæ¨¡å—ã€‚ä¼ é€’ç»™`accumulate()`çš„æ¨¡å‹å°†åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„åå‘ä¼ é€’æœŸé—´è·³è¿‡æ¢¯åº¦åŒæ­¥'
- en: A context manager that will lightly wrap around and perform gradient accumulation
    automatically
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œå°†è½»æ¾åŒ…è£…å¹¶è‡ªåŠ¨æ‰§è¡Œæ¢¯åº¦ç´¯ç§¯
- en: 'Example:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#### `autocast`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `autocast`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3109)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3109)'
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Will apply automatic mixed-precision inside the block inside this context manager,
    if it is enabled. Nothing different will happen otherwise.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå¯ç”¨ï¼Œå°†åœ¨æ­¤ä¸Šä¸‹æ–‡ç®¡ç†å™¨å†…éƒ¨çš„å—ä¸­åº”ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ã€‚å¦åˆ™ä¸ä¼šå‘ç”Ÿä»»ä½•ä¸åŒã€‚
- en: A different `autocast_handler` can be passed in to override the one set in the
    `Accelerator` object. This is useful in blocks under `autocast` where you want
    to revert to fp32.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥ä¼ å…¥ä¸åŒçš„`autocast_handler`æ¥è¦†ç›–`Accelerator`å¯¹è±¡ä¸­è®¾ç½®çš„å¤„ç†ç¨‹åºã€‚è¿™åœ¨`autocast`ä¸‹çš„å—ä¸­å¾ˆæœ‰ç”¨ï¼Œæ‚¨æƒ³è¦æ¢å¤åˆ°fp32ã€‚
- en: 'Example:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#### `backward`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `backward`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1938)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1938)'
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Scales the gradients in accordance to the `GradientAccumulationPlugin` and calls
    the correct `backward()` based on the configuration.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®`GradientAccumulationPlugin`ç¼©æ”¾æ¢¯åº¦ï¼Œå¹¶æ ¹æ®é…ç½®è°ƒç”¨æ­£ç¡®çš„`backward()`ã€‚
- en: Should be used in lieu of `loss.backward()`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: åº”è¯¥ç”¨æ¥ä»£æ›¿`loss.backward()`ã€‚
- en: 'Example:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#### `check_trigger`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `check_trigger`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1994)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1994)'
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Checks if the internal trigger tensor has been set to 1 in any of the processes.
    If so, will return `True` and reset the trigger tensor to 0.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥å†…éƒ¨è§¦å‘å¼ é‡æ˜¯å¦åœ¨ä»»ä½•è¿›ç¨‹ä¸­è®¾ç½®ä¸º1ã€‚å¦‚æœæ˜¯ï¼Œåˆ™å°†è¿”å›`True`å¹¶å°†è§¦å‘å¼ é‡é‡ç½®ä¸º0ã€‚
- en: 'Note: Does not require `wait_for_everyone()`'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šä¸éœ€è¦`wait_for_everyone()`
- en: 'Example:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '#### `clear`'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `clear`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2973)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2973)'
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Alias for `Accelerate.free_memory`, releases all references to the internal
    objects stored and call the garbage collector. You should call this method between
    two trainings with different models/optimizers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ«åä¸º`Accelerate.free_memory`ï¼Œé‡Šæ”¾æ‰€æœ‰å¯¹å­˜å‚¨çš„å†…éƒ¨å¯¹è±¡çš„å¼•ç”¨å¹¶è°ƒç”¨åƒåœ¾å›æ”¶å™¨ã€‚æ‚¨åº”è¯¥åœ¨ä¸¤ä¸ªå…·æœ‰ä¸åŒæ¨¡å‹/ä¼˜åŒ–å™¨çš„è®­ç»ƒä¹‹é—´è°ƒç”¨æ­¤æ–¹æ³•ã€‚
- en: 'Example:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE25]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#### `clip_grad_norm_`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `clip_grad_norm_`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2066)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2066)'
- en: '[PRE26]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Returns
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`torch.Tensor`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.Tensor`'
- en: Total norm of the parameter gradients (viewed as a single vector).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°æ¢¯åº¦çš„æ€»èŒƒæ•°ï¼ˆè§†ä¸ºå•ä¸ªå‘é‡ï¼‰ã€‚
- en: Should be used in place of `torch.nn.utils.clip_grad_norm_`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: åº”è¯¥ç”¨æ¥ä»£æ›¿`torch.nn.utils.clip_grad_norm_`ã€‚
- en: 'Example:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE27]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '#### `clip_grad_value_`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `clip_grad_value_`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2104)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2104)'
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Should be used in place of `torch.nn.utils.clip_grad_value_`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: åº”è¯¥ç”¨æ¥ä»£æ›¿`torch.nn.utils.clip_grad_value_`ã€‚
- en: 'Example:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '#### `free_memory`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `free_memory`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2948)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2948)'
- en: '[PRE30]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Will release all references to the internal objects stored and call the garbage
    collector. You should call this method between two trainings with different models/optimizers.
    Also will reset `Accelerator.step` to 0.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: å°†é‡Šæ”¾æ‰€æœ‰å¯¹å­˜å‚¨çš„å†…éƒ¨å¯¹è±¡çš„å¼•ç”¨å¹¶è°ƒç”¨åƒåœ¾å›æ”¶å™¨ã€‚æ‚¨åº”è¯¥åœ¨ä¸¤ä¸ªå…·æœ‰ä¸åŒæ¨¡å‹/ä¼˜åŒ–å™¨çš„è®­ç»ƒä¹‹é—´è°ƒç”¨æ­¤æ–¹æ³•ã€‚è¿˜å°†`Accelerator.step`é‡ç½®ä¸º0ã€‚
- en: 'Example:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '#### `gather`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `gather`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2131)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2131)'
- en: '[PRE32]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tensor` (`torch.Tensor`, or a nested tuple/list/dictionary of `torch.Tensor`)
    â€” The tensors to gather across all processes.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor`ï¼ˆ`torch.Tensor`ï¼Œæˆ–`torch.Tensor`çš„åµŒå¥—å…ƒç»„/åˆ—è¡¨/å­—å…¸ï¼‰-è¦åœ¨æ‰€æœ‰è¿›ç¨‹ä¸­æ”¶é›†çš„å¼ é‡ã€‚'
- en: Returns
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`torch.Tensor`, or a nested tuple/list/dictionary of `torch.Tensor`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.Tensor`ï¼Œæˆ–`torch.Tensor`çš„åµŒå¥—å…ƒç»„/åˆ—è¡¨/å­—å…¸'
- en: The gathered tensor(s). Note that the first dimension of the result is *num_processes*
    multiplied by the first dimension of the input tensors.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¶é›†çš„å¼ é‡ã€‚è¯·æ³¨æ„ï¼Œç»“æœçš„ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯* num_processes *ä¹˜ä»¥è¾“å…¥å¼ é‡çš„ç¬¬ä¸€ä¸ªç»´åº¦ã€‚
- en: Gather the values in *tensor* across all processes and concatenate them on the
    first dimension. Useful to regroup the predictions from all processes when doing
    evaluation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¶é›†*å¼ é‡*åœ¨æ‰€æœ‰è¿›ç¨‹ä¸­çš„å€¼ï¼Œå¹¶åœ¨ç¬¬ä¸€ç»´ä¸Šå°†å®ƒä»¬è¿æ¥èµ·æ¥ã€‚åœ¨è¿›è¡Œè¯„ä¼°æ—¶ï¼Œæœ‰åŠ©äºé‡æ–°ç»„åˆæ‰€æœ‰è¿›ç¨‹çš„é¢„æµ‹ã€‚
- en: 'Note: This gather happens in all processes.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šæ­¤æ”¶é›†åœ¨æ‰€æœ‰è¿›ç¨‹ä¸­å‘ç”Ÿã€‚
- en: 'Example:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '#### `gather_for_metrics`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `gather_for_metrics`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2163)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2163)'
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Parameters
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input` (`torch.Tensor`, `object`, a nested tuple/list/dictionary of `torch.Tensor`,
    or a nested tuple/list/dictionary of `object`) â€” The tensors or objects for calculating
    metrics across all processes'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input` (`torch.Tensor`, `object`, `torch.Tensor`çš„åµŒå¥—å…ƒç»„/åˆ—è¡¨/å­—å…¸ï¼Œæˆ–`object`çš„åµŒå¥—å…ƒç»„/åˆ—è¡¨/å­—å…¸)
    â€” ç”¨äºè®¡ç®—æ‰€æœ‰è¿›ç¨‹é—´æŒ‡æ ‡çš„å¼ é‡æˆ–å¯¹è±¡'
- en: Gathers `input_data` and potentially drops duplicates in the last batch if on
    a distributed system. Should be used for gathering the inputs and targets for
    metric calculation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¶é›†`input_data`å¹¶åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸Šå¯èƒ½åˆ é™¤æœ€åä¸€æ‰¹ä¸­çš„é‡å¤é¡¹ã€‚åº”ç”¨äºæ”¶é›†ç”¨äºæŒ‡æ ‡è®¡ç®—çš„è¾“å…¥å’Œç›®æ ‡ã€‚
- en: 'Example:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE35]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '#### `get_state_dict`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_state_dict`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3017)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3017)'
- en: '[PRE36]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Parameters
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`torch.nn.Module`) â€” A PyTorch model sent through [Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) â€” é€šè¿‡[Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)å‘é€çš„PyTorchæ¨¡å‹'
- en: '`unwrap` (`bool`, *optional*, defaults to `True`) â€” Whether to return the original
    underlying state_dict of `model` or to return the wrapped state_dict'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unwrap` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦è¿”å›`model`çš„åŸå§‹åº•å±‚`state_dict`æˆ–è¿”å›åŒ…è£…çš„`state_dict`'
- en: Returns
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`dict`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`dict`'
- en: The state dictionary of the model potentially without full precision.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„çŠ¶æ€å­—å…¸ï¼Œå¯èƒ½ä¸æ˜¯å®Œå…¨ç²¾ç¡®ã€‚
- en: Returns the state dictionary of a model sent through [Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    potentially without full precision.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›é€šè¿‡[Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)å‘é€çš„æ¨¡å‹çš„çŠ¶æ€å­—å…¸ï¼Œå¯èƒ½ä¸æ˜¯å®Œå…¨ç²¾ç¡®ã€‚
- en: 'Example:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE37]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '#### `get_tracker`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_tracker`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2400)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2400)'
- en: '[PRE38]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`name` (`str`) â€” The name of a tracker, corresponding to the `.name` property.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name` (`str`) â€” ä¸€ä¸ªè·Ÿè¸ªå™¨çš„åç§°ï¼Œå¯¹åº”äº`.name`å±æ€§ã€‚'
- en: '`unwrap` (`bool`) â€” Whether to return the internal tracking mechanism or to
    return the wrapped tracker instead (recommended).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unwrap` (`bool`) â€” æ˜¯å¦è¿”å›å†…éƒ¨è·Ÿè¸ªæœºåˆ¶æˆ–è¿”å›åŒ…è£…çš„è·Ÿè¸ªå™¨ï¼ˆæ¨èï¼‰ã€‚'
- en: Returns
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`GeneralTracker`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`GeneralTracker`'
- en: The tracker corresponding to `name` if it exists.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå­˜åœ¨ï¼Œåˆ™è¿”å›ä¸`name`å¯¹åº”çš„è·Ÿè¸ªå™¨ã€‚
- en: Returns a `tracker` from `self.trackers` based on `name` on the main process
    only.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…åœ¨ä¸»è¿›ç¨‹ä¸ŠåŸºäº`name`ä»`self.trackers`è¿”å›ä¸€ä¸ª`tracker`ã€‚
- en: 'Example:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE39]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '#### `join_uneven_inputs`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `join_uneven_inputs`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1001)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1001)'
- en: '[PRE40]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`joinables` (`list[torch.distributed.algorithms.Joinable]`) â€” A list of models
    or optimizers that subclass `torch.distributed.algorithms.Joinable`. Most commonly,
    a PyTorch Module that was prepared with `Accelerator.prepare` for DistributedDataParallel
    training.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`joinables` (`list[torch.distributed.algorithms.Joinable]`) â€” ä¸€ä¸ªå­ç±»ä¸º`torch.distributed.algorithms.Joinable`çš„æ¨¡å‹æˆ–ä¼˜åŒ–å™¨åˆ—è¡¨ã€‚æœ€å¸¸è§çš„æ˜¯ï¼Œä½¿ç”¨`Accelerator.prepare`ä¸ºåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œè®­ç»ƒå‡†å¤‡çš„PyTorchæ¨¡å—ã€‚'
- en: '`even_batches` (`bool`, *optional*) â€” If set, this will override the value
    of `even_batches` set in the `Accelerator`. If it is not provided, the default
    `Accelerator` value wil be used.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`even_batches` (`bool`, *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œè¿™å°†è¦†ç›–åœ¨`Accelerator`ä¸­è®¾ç½®çš„`even_batches`çš„å€¼ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†ä½¿ç”¨é»˜è®¤çš„`Accelerator`å€¼ã€‚'
- en: A context manager that facilitates distributed training or evaluation on uneven
    inputs, which acts as a wrapper around `torch.distributed.algorithms.join`. This
    is useful when the total batch size does not evenly divide the length of the dataset.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºåœ¨ä¸å‡åŒ€è¾“å…¥ä¸Šè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒæˆ–è¯„ä¼°ï¼Œä½œä¸º`torch.distributed.algorithms.join`å‘¨å›´çš„åŒ…è£…å™¨ã€‚å½“æ€»æ‰¹é‡å¤§å°ä¸èƒ½å®Œå…¨æ•´é™¤æ•°æ®é›†çš„é•¿åº¦æ—¶ï¼Œè¿™æ˜¯æœ‰ç”¨çš„ã€‚
- en: '`join_uneven_inputs` is only supported for Distributed Data Parallel training
    on multiple GPUs. For any other configuration, this method will have no effect.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`join_uneven_inputs`ä»…æ”¯æŒåœ¨å¤šä¸ªGPUä¸Šè¿›è¡Œåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œè®­ç»ƒã€‚å¯¹äºä»»ä½•å…¶ä»–é…ç½®ï¼Œæ­¤æ–¹æ³•å°†ä¸èµ·ä½œç”¨ã€‚'
- en: Overidding `even_batches` will not affect iterable-style data loaders.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: è¦†ç›–`even_batches`ä¸ä¼šå½±å“å¯è¿­ä»£æ ·å¼çš„æ•°æ®åŠ è½½å™¨ã€‚
- en: 'Example:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE41]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '#### `load_state`'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `load_state`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2816)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2816)'
- en: '[PRE42]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Parameters
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_dir` (`str` or `os.PathLike`) â€” The name of the folder all relevant
    weights and states were saved in. Can be `None` if `automatic_checkpoint_naming`
    is used, and will pick up from the latest checkpoint.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_dir` (`str`æˆ–`os.PathLike`) â€” æ‰€æœ‰ç›¸å…³æƒé‡å’ŒçŠ¶æ€ä¿å­˜åœ¨çš„æ–‡ä»¶å¤¹çš„åç§°ã€‚å¦‚æœä½¿ç”¨`automatic_checkpoint_naming`ï¼Œåˆ™å¯ä»¥ä¸º`None`ï¼Œå¹¶å°†ä»æœ€æ–°çš„æ£€æŸ¥ç‚¹ä¸­è·å–ã€‚'
- en: '`load_model_func_kwargs` (`dict`, *optional*) â€” Additional keyword arguments
    for loading model which can be passed to the underlying load function, such as
    optional arguments for DeepSpeedâ€™s `load_checkpoint` function or a `map_location`
    to load the model and optimizer on.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_model_func_kwargs` (`dict`, *å¯é€‰*) â€” ç”¨äºåŠ è½½æ¨¡å‹çš„é™„åŠ å…³é”®å­—å‚æ•°ï¼Œå¯ä»¥ä¼ é€’ç»™åº•å±‚åŠ è½½å‡½æ•°ï¼Œä¾‹å¦‚DeepSpeedçš„`load_checkpoint`å‡½æ•°çš„å¯é€‰å‚æ•°æˆ–`map_location`ä»¥åŠ è½½æ¨¡å‹å’Œä¼˜åŒ–å™¨ã€‚'
- en: Loads the current states of the model, optimizer, scaler, RNG generators, and
    registered objects.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€ç¼©æ”¾å™¨ã€RNGç”Ÿæˆå™¨å’Œå·²æ³¨å†Œå¯¹è±¡çš„å½“å‰çŠ¶æ€ã€‚
- en: Should only be used in conjunction with [Accelerator.save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state).
    If a file is not registered for checkpointing, it will not be loaded if stored
    in the directory.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: åº”è¯¥ä¸ [Accelerator.save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)
    ç»“åˆä½¿ç”¨ã€‚å¦‚æœæœªæ³¨å†Œç”¨äºæ£€æŸ¥ç‚¹çš„æ–‡ä»¶ï¼Œåˆ™å¦‚æœå­˜å‚¨åœ¨ç›®å½•ä¸­ï¼Œåˆ™ä¸ä¼šåŠ è½½ã€‚
- en: 'Example:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE43]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '#### `local_main_process_first`'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `local_main_process_first`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L830)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L830)'
- en: '[PRE44]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Lets the local main process go inside a with block.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æœ¬åœ°ä¸»è¿›ç¨‹åœ¨ with å—å†…æ‰§è¡Œã€‚
- en: The other processes will enter the with block after the main process exits.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–è¿›ç¨‹å°†åœ¨ä¸»è¿›ç¨‹é€€å‡ºåè¿›å…¥ with å—ã€‚
- en: 'Example:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE45]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '#### `main_process_first`'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `main_process_first`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L808)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L808)'
- en: '[PRE46]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Lets the main process go first inside a with block.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: è®©ä¸»è¿›ç¨‹åœ¨ with å—å†…å…ˆæ‰§è¡Œã€‚
- en: The other processes will enter the with block after the main process exits.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–è¿›ç¨‹å°†åœ¨ä¸»è¿›ç¨‹é€€å‡ºåè¿›å…¥ with å—ã€‚
- en: 'Example:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE47]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '#### `no_sync`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `no_sync`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L852)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L852)'
- en: '[PRE48]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Parameters
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`torch.nn.Module`) â€” PyTorch Module that was prepared with `Accelerator.prepare`'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) â€” ä½¿ç”¨ `Accelerator.prepare` å‡†å¤‡çš„ PyTorch æ¨¡å—ã€‚'
- en: A context manager to disable gradient synchronizations across DDP processes
    by calling `torch.nn.parallel.DistributedDataParallel.no_sync`.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œé€šè¿‡è°ƒç”¨ `torch.nn.parallel.DistributedDataParallel.no_sync` æ¥ç¦ç”¨ DDP è¿›ç¨‹ä¹‹é—´çš„æ¢¯åº¦åŒæ­¥ã€‚
- en: If `model` is not in DDP, this context manager does nothing
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ `model` ä¸åœ¨ DDP ä¸­ï¼Œåˆ™æ­¤ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸èµ·ä½œç”¨
- en: 'Example:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE49]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '#### `on_last_process`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_last_process`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L676)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L676)'
- en: '[PRE50]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Parameters
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`function` (`Callable`) â€” The function to decorate.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function` (`Callable`) â€” è¦è£…é¥°çš„å‡½æ•°ã€‚'
- en: A decorator that will run the decorated function on the last process only. Can
    also be called using the `PartialState` class.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªè£…é¥°å™¨ï¼Œåªä¼šåœ¨æœ€åä¸€ä¸ªè¿›ç¨‹ä¸Šè¿è¡Œè£…é¥°çš„å‡½æ•°ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨ `PartialState` ç±»è°ƒç”¨ã€‚
- en: 'Example:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE51]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '#### `on_local_main_process`'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_local_main_process`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L634)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L634)'
- en: '[PRE52]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Parameters
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`function` (`Callable`) â€” The function to decorate.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function` (`Callable`) â€” è¦è£…é¥°çš„å‡½æ•°ã€‚'
- en: A decorator that will run the decorated function on the local main process only.
    Can also be called using the `PartialState` class.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªè£…é¥°å™¨ï¼Œåªä¼šåœ¨æœ¬åœ°ä¸»è¿›ç¨‹ä¸Šè¿è¡Œè£…é¥°çš„å‡½æ•°ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨ `PartialState` ç±»è°ƒç”¨ã€‚
- en: 'Example:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE53]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '#### `on_local_process`'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_local_process`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L760)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L760)'
- en: '[PRE54]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Parameters
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`function` (`Callable`, *optional*) â€” The function to decorate.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function` (`Callable`, *å¯é€‰*) â€” è¦è£…é¥°çš„å‡½æ•°ã€‚'
- en: '`local_process_index` (`int`, *optional*) â€” The index of the local process
    on which to run the function.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`local_process_index` (`int`, *å¯é€‰*) â€” è¦è¿è¡Œå‡½æ•°çš„æœ¬åœ°è¿›ç¨‹çš„ç´¢å¼•ã€‚'
- en: A decorator that will run the decorated function on a given local process index
    only. Can also be called using the `PartialState` class.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªè£…é¥°å™¨ï¼Œåªä¼šåœ¨ç»™å®šçš„æœ¬åœ°è¿›ç¨‹ç´¢å¼•ä¸Šè¿è¡Œè£…é¥°çš„å‡½æ•°ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨ `PartialState` ç±»è°ƒç”¨ã€‚
- en: 'Example:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE55]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '#### `on_main_process`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_main_process`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L595)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L595)'
- en: '[PRE56]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Parameters
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`function` (`Callable`) â€” The function to decorate.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function` (`Callable`) â€” è¦è£…é¥°çš„å‡½æ•°ã€‚'
- en: A decorator that will run the decorated function on the main process only. Can
    also be called using the `PartialState` class.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªè£…é¥°å™¨ï¼Œåªä¼šåœ¨ä¸»è¿›ç¨‹ä¸Šè¿è¡Œè£…é¥°çš„å‡½æ•°ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨ `PartialState` ç±»è°ƒç”¨ã€‚
- en: 'Example:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE57]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '#### `on_process`'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `on_process`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L715)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L715)'
- en: '[PRE58]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Parameters
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`function` (`Callable`, `optional`) â€” The function to decorate.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function` (`Callable`, `å¯é€‰`) â€” è¦è£…é¥°çš„å‡½æ•°ã€‚'
- en: '`process_index` (`int`, `optional`) â€” The index of the process on which to
    run the function.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`process_index` (`int`, `å¯é€‰`) â€” è¦è¿è¡Œå‡½æ•°çš„è¿›ç¨‹çš„ç´¢å¼•ã€‚'
- en: A decorator that will run the decorated function on a given process index only.
    Can also be called using the `PartialState` class.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªè£…é¥°å™¨ï¼Œåªä¼šåœ¨ç»™å®šçš„è¿›ç¨‹ç´¢å¼•ä¸Šè¿è¡Œè£…é¥°çš„å‡½æ•°ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨ `PartialState` ç±»è°ƒç”¨ã€‚
- en: 'Example:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE59]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '#### `pad_across_processes`'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `pad_across_processes`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2261)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2261)'
- en: '[PRE60]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Parameters
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tensor` (nested list/tuple/dictionary of `torch.Tensor`) â€” The data to gather.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor`ï¼ˆ`torch.Tensor`çš„åµŒå¥—åˆ—è¡¨/å…ƒç»„/å­—å…¸ï¼‰ â€” è¦æ”¶é›†çš„æ•°æ®ã€‚'
- en: '`dim` (`int`, *optional*, defaults to 0) â€” The dimension on which to pad.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0) â€” è¦å¡«å……çš„ç»´åº¦ã€‚'
- en: '`pad_index` (`int`, *optional*, defaults to 0) â€” The value with which to pad.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_index` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0) â€” ç”¨äºå¡«å……çš„å€¼ã€‚'
- en: '`pad_first` (`bool`, *optional*, defaults to `False`) â€” Whether to pad at the
    beginning or the end.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_first` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨å¼€å¤´æˆ–ç»“å°¾å¡«å……ã€‚'
- en: Returns
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`torch.Tensor`, or a nested tuple/list/dictionary of `torch.Tensor`'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.Tensor`ï¼Œæˆ– `torch.Tensor` çš„åµŒå¥—å…ƒç»„/åˆ—è¡¨/å­—å…¸'
- en: The padded tensor(s).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: å¡«å……åçš„å¼ é‡ã€‚
- en: Recursively pad the tensors in a nested list/tuple/dictionary of tensors from
    all devices to the same size so they can safely be gathered.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: é€’å½’å¡«å……åµŒå¥—åˆ—è¡¨/å…ƒç»„/å¼ é‡å­—å…¸ä¸­çš„å¼ é‡ï¼Œä½¿å®ƒä»¬å¯ä»¥å®‰å…¨åœ°è¢«æ”¶é›†åˆ°ç›¸åŒçš„å¤§å°ã€‚
- en: 'Example:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE61]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '#### `prepare`'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prepare`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1116)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1116)'
- en: '[PRE62]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Parameters
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`*args` (list of objects) â€” Any of the following type of objects:'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*args`ï¼ˆå¯¹è±¡åˆ—è¡¨ï¼‰ â€” ä»¥ä¸‹ç±»å‹çš„ä»»ä½•å¯¹è±¡ï¼š'
- en: '`torch.utils.data.DataLoader`: PyTorch Dataloader'
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.utils.data.DataLoader`: PyTorchæ•°æ®åŠ è½½å™¨'
- en: '`torch.nn.Module`: PyTorch Module'
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.nn.Module`: PyTorchæ¨¡å—'
- en: '`torch.optim.Optimizer`: PyTorch Optimizer'
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.optim.Optimizer`: PyTorchä¼˜åŒ–å™¨'
- en: '`torch.optim.lr_scheduler.LRScheduler`: PyTorch LR Scheduler'
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.optim.lr_scheduler.LRScheduler`: PyTorch LRè°ƒåº¦ç¨‹åº'
- en: '`device_placement` (`list[bool]`, *optional*) â€” Used to customize whether automatic
    device placement should be performed for each object passed. Needs to be a list
    of the same length as `args`. Not compatible with DeepSpeed or FSDP.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_placement` (`list[bool]`, *å¯é€‰*) â€” ç”¨äºè‡ªå®šä¹‰æ˜¯å¦åº”ä¸ºä¼ é€’çš„æ¯ä¸ªå¯¹è±¡æ‰§è¡Œè‡ªåŠ¨è®¾å¤‡æ”¾ç½®ã€‚éœ€è¦ä¸ `args`
    é•¿åº¦ç›¸åŒçš„åˆ—è¡¨ã€‚ä¸DeepSpeedæˆ–FSDPä¸å…¼å®¹ã€‚'
- en: Prepare all objects passed in `args` for distributed training and mixed precision,
    then return them in the same order.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†å¤‡ä¼ é€’çš„æ‰€æœ‰å¯¹è±¡è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒå’Œæ··åˆç²¾åº¦ï¼Œç„¶åä»¥ç›¸åŒé¡ºåºè¿”å›å®ƒä»¬ã€‚
- en: You donâ€™t need to prepare a model if you only use it for inference without any
    kind of mixed precision
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»…ç”¨äºæ¨æ–­è€Œä¸æ¶‰åŠä»»ä½•æ··åˆç²¾åº¦ï¼Œåˆ™æ— éœ€å‡†å¤‡æ¨¡å‹
- en: 'Examples:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE63]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '#### `prepare_data_loader`'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prepare_data_loader`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1812)'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1812)'
- en: '[PRE65]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Parameters
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`data_loader` (`torch.utils.data.DataLoader`) â€” A vanilla PyTorch DataLoader
    to prepare'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_loader` (`torch.utils.data.DataLoader`) â€” ä¸€ä¸ªæ™®é€šçš„PyTorchæ•°æ®åŠ è½½å™¨è¦å‡†å¤‡'
- en: '`device_placement` (`bool`, *optional*) â€” Whether or not to place the batches
    on the proper device in the prepared dataloader. Will default to `self.device_placement`.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_placement` (`bool`, *å¯é€‰*) â€” æ˜¯å¦å°†æ‰¹æ¬¡æ”¾ç½®åœ¨å‡†å¤‡å¥½çš„æ•°æ®åŠ è½½å™¨ä¸­çš„æ­£ç¡®è®¾å¤‡ä¸Šã€‚é»˜è®¤ä¸º `self.device_placement`ã€‚'
- en: '`slice_fn_for_dispatch` (`Callable`, *optional*`) -- If passed, this function
    will be used to slice tensors across` num_processes`. Will default to [slice_tensors()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.slice_tensors).
    This argument is used only when` dispatch_batches`is set to`True` and will be
    ignored otherwise.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`slice_fn_for_dispatch` (`Callable`, *å¯é€‰*`) -- å¦‚æœä¼ é€’ï¼Œå°†ä½¿ç”¨æ­¤å‡½æ•°åœ¨` num_processes`ä¸Šåˆ‡ç‰‡å¼ é‡ã€‚é»˜è®¤ä¸º[slice_tensors()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.slice_tensors)ã€‚æ­¤å‚æ•°ä»…åœ¨`
    dispatch_batches`è®¾ç½®ä¸º`True`æ—¶ä½¿ç”¨ï¼Œå¦åˆ™å°†è¢«å¿½ç•¥ã€‚'
- en: Prepares a PyTorch DataLoader for training in any distributed setup. It is recommended
    to use [Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    instead.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºåœ¨ä»»ä½•åˆ†å¸ƒå¼è®¾ç½®ä¸­è®­ç»ƒå‡†å¤‡ä¸€ä¸ªPyTorchæ•°æ®åŠ è½½å™¨ã€‚å»ºè®®ä½¿ç”¨[Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)ä»£æ›¿ã€‚
- en: 'Example:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE66]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '#### `prepare_model`'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prepare_model`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1252)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1252)'
- en: '[PRE67]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Parameters
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`torch.nn.Module`) â€” A PyTorch model to prepare. You donâ€™t need to
    prepare a model if it is used only for inference without any kind of mixed precision'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) â€” ä¸€ä¸ªè¦å‡†å¤‡çš„PyTorchæ¨¡å‹ã€‚å¦‚æœä»…ç”¨äºæ¨æ–­è€Œä¸æ¶‰åŠä»»ä½•æ··åˆç²¾åº¦ï¼Œåˆ™æ— éœ€å‡†å¤‡æ¨¡å‹'
- en: '`device_placement` (`bool`, *optional*) â€” Whether or not to place the model
    on the proper device. Will default to `self.device_placement`.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_placement` (`bool`, *å¯é€‰*) â€” æ˜¯å¦å°†æ¨¡å‹æ”¾ç½®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šã€‚é»˜è®¤ä¸º `self.device_placement`ã€‚'
- en: '`evaluation_mode` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to set the model for evaluation only, by just applying mixed precision and `torch.compile`
    (if configured in the `Accelerator` object).'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`evaluation_mode` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä»…å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œåªéœ€åº”ç”¨æ··åˆç²¾åº¦å’Œ `torch.compile`ï¼ˆå¦‚æœåœ¨
    `Accelerator` å¯¹è±¡ä¸­é…ç½®ï¼‰ã€‚'
- en: Prepares a PyTorch model for training in any distributed setup. It is recommended
    to use [Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    instead.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºåœ¨ä»»ä½•åˆ†å¸ƒå¼è®¾ç½®ä¸­è®­ç»ƒå‡†å¤‡ä¸€ä¸ªPyTorchæ¨¡å‹ã€‚å»ºè®®ä½¿ç”¨[Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)ä»£æ›¿ã€‚
- en: 'Example:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE68]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '#### `prepare_optimizer`'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prepare_optimizer`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1864)'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1864)'
- en: '[PRE69]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Parameters
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`optimizer` (`torch.optim.Optimizer`) â€” A vanilla PyTorch optimizer to prepare'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer` (`torch.optim.Optimizer`) â€” ä¸€ä¸ªæ™®é€šçš„PyTorchä¼˜åŒ–å™¨è¦å‡†å¤‡'
- en: '`device_placement` (`bool`, *optional*) â€” Whether or not to place the optimizer
    on the proper device. Will default to `self.device_placement`.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_placement` (`bool`, *å¯é€‰*) â€” æ˜¯å¦å°†ä¼˜åŒ–å™¨æ”¾ç½®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šã€‚é»˜è®¤ä¸º `self.device_placement`ã€‚'
- en: Prepares a PyTorch Optimizer for training in any distributed setup. It is recommended
    to use [Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    instead.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºåœ¨ä»»ä½•åˆ†å¸ƒå¼è®¾ç½®ä¸­è®­ç»ƒå‡†å¤‡ä¸€ä¸ªPyTorchä¼˜åŒ–å™¨ã€‚å»ºè®®ä½¿ç”¨[Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)ä»£æ›¿ã€‚
- en: 'Example:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE70]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '#### `prepare_scheduler`'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prepare_scheduler`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1897)'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1897)'
- en: '[PRE71]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Parameters
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`scheduler` (`torch.optim.lr_scheduler.LRScheduler`) â€” A vanilla PyTorch scheduler
    to prepare'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` (`torch.optim.lr_scheduler.LRScheduler`) â€” è¦å‡†å¤‡çš„ä¸€ä¸ªæ™®é€šçš„PyTorchè°ƒåº¦ç¨‹åº'
- en: Prepares a PyTorch Scheduler for training in any distributed setup. It is recommended
    to use [Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    instead.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºåœ¨ä»»ä½•åˆ†å¸ƒå¼è®¾ç½®ä¸­è®­ç»ƒå‡†å¤‡ä¸€ä¸ªPyTorchè°ƒåº¦ç¨‹åºã€‚å»ºè®®ä½¿ç”¨[Accelerator.prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)ä»£æ›¿ã€‚
- en: 'Example:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE72]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '#### `print`'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `print`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1084)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1084)'
- en: '[PRE73]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Drop in replacement of `print()` to only print once per server.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '`print()`çš„æ›¿ä»£æ–¹æ³•ï¼Œæ¯ä¸ªæœåŠ¡å™¨åªæ‰“å°ä¸€æ¬¡ã€‚'
- en: 'Example:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE74]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '#### `reduce`'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `reduce`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2225)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2225)'
- en: '[PRE75]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Parameters
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tensor` (`torch.Tensor`, or a nested tuple/list/dictionary of `torch.Tensor`)
    â€” The tensors to reduce across all processes.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor`ï¼ˆ`torch.Tensor`ï¼Œæˆ–`torch.Tensor`çš„åµŒå¥—å…ƒç»„/åˆ—è¡¨/å­—å…¸ï¼‰â€” è¦åœ¨æ‰€æœ‰è¿›ç¨‹ä¸­å‡å°‘çš„å¼ é‡ã€‚'
- en: '`reduction` (`str`, *optional*, defaults to â€œsumâ€) â€” A reduction type, can
    be one of â€˜sumâ€™, â€˜meanâ€™, or â€˜noneâ€™. If â€˜noneâ€™, will not perform any operation.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduction`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸ºâ€œsumâ€ï¼‰â€” å‡å°‘ç±»å‹ï¼Œå¯ä»¥æ˜¯â€œsumâ€ã€â€œmeanâ€æˆ–â€œnoneâ€ä¸­çš„ä¸€ä¸ªã€‚å¦‚æœæ˜¯â€œnoneâ€ï¼Œåˆ™ä¸æ‰§è¡Œä»»ä½•æ“ä½œã€‚'
- en: '`scale` (`float`, *optional*, defaults to 1.0) â€” A default scaling value to
    be applied after the reduce, only valied on XLA.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1.0ï¼‰â€” åœ¨å‡å°‘ååº”ç”¨çš„é»˜è®¤ç¼©æ”¾å€¼ï¼Œä»…åœ¨XLAä¸Šæœ‰æ•ˆã€‚'
- en: Returns
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`torch.Tensor`, or a nested tuple/list/dictionary of `torch.Tensor`'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.Tensor`ï¼Œæˆ–`torch.Tensor`çš„åµŒå¥—å…ƒç»„/åˆ—è¡¨/å­—å…¸'
- en: The reduced tensor(s).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å°‘çš„å¼ é‡ã€‚
- en: Reduce the values in *tensor* across all processes based on *reduction*.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®*reduction*åœ¨*å¼ é‡*ä¸­å‡å°‘æ‰€æœ‰è¿›ç¨‹ä¸­çš„å€¼ã€‚
- en: 'Note: All processes get the reduced value.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šæ‰€æœ‰è¿›ç¨‹éƒ½ä¼šå¾—åˆ°å‡å°‘çš„å€¼ã€‚
- en: 'Example:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE76]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '#### `register_for_checkpointing`'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `register_for_checkpointing`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3073)'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3073)'
- en: '[PRE77]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Makes note of `objects` and will save or load them in during `save_state` or
    `load_state`.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„`objects`å¹¶å°†åœ¨`save_state`æˆ–`load_state`æœŸé—´ä¿å­˜æˆ–åŠ è½½å®ƒä»¬ã€‚
- en: These should be utilized when the state is being loaded or saved in the same
    script. It is not designed to be used in different scripts.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åº”è¯¥åœ¨ç›¸åŒè„šæœ¬ä¸­åŠ è½½æˆ–ä¿å­˜çŠ¶æ€æ—¶ä½¿ç”¨ã€‚ä¸åº”è¯¥åœ¨ä¸åŒè„šæœ¬ä¸­ä½¿ç”¨ã€‚
- en: Every `object` must have a `load_state_dict` and `state_dict` function to be
    stored.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ª`object`å¿…é¡»æœ‰ä¸€ä¸ª`load_state_dict`å’Œ`state_dict`å‡½æ•°ä»¥è¿›è¡Œå­˜å‚¨ã€‚
- en: 'Example:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE78]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '#### `register_load_state_pre_hook`'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `register_load_state_pre_hook`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2785)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2785)'
- en: '[PRE79]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Parameters
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`hook` (`Callable`) â€” A function to be called in [Accelerator.load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)
    before `load_checkpoint`.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hook`ï¼ˆ`Callable`ï¼‰â€” åœ¨[Accelerator.load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)ä¹‹å‰è°ƒç”¨çš„å‡½æ•°ï¼Œç”¨äº`load_checkpoint`ã€‚'
- en: Returns
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`torch.utils.hooks.RemovableHandle`'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.utils.hooks.RemovableHandle`'
- en: a handle that can be used to remove the added hook by calling `handle.remove()`
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¥æŸ„ï¼Œå¯ä»¥é€šè¿‡è°ƒç”¨`handle.remove()`æ¥ç§»é™¤æ·»åŠ çš„é’©å­
- en: Registers a pre hook to be run before `load_checkpoint` is called in [Accelerator.load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨å†Œä¸€ä¸ªé¢„é’©å­ï¼Œåœ¨[Accelerator.load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)ä¸­è°ƒç”¨`load_checkpoint`ä¹‹å‰è¿è¡Œã€‚
- en: 'The hook should have the following signature:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 'é’©å­åº”è¯¥å…·æœ‰ä»¥ä¸‹ç­¾å:'
- en: '`hook(models: list[torch.nn.Module], input_dir: str) -> None`'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '`hook(models: list[torch.nn.Module], input_dir: str) -> None`'
- en: The `models` argument are the models as saved in the accelerator state under
    `accelerator._models`, and the `input_dir` argument is the `input_dir` argument
    passed to [Accelerator.load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state).
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '`models`å‚æ•°æ˜¯ä¿å­˜åœ¨åŠ é€Ÿå™¨çŠ¶æ€ä¸‹çš„æ¨¡å‹`accelerator._models`ï¼Œ`input_dir`å‚æ•°æ˜¯ä¼ é€’ç»™[Accelerator.load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)çš„`input_dir`å‚æ•°ã€‚'
- en: Should only be used in conjunction with [Accelerator.register_save_state_pre_hook()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_save_state_pre_hook).
    Can be useful to load configurations in addition to model weights. Can also be
    used to overwrite model loading with a customized method. In this case, make sure
    to remove already loaded models from the models list.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: åº”è¯¥åªä¸[Accelerator.register_save_state_pre_hook()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_save_state_pre_hook)ä¸€èµ·ä½¿ç”¨ã€‚å¯ä»¥ç”¨äºåŠ è½½é…ç½®ä»¥åŠæ¨¡å‹æƒé‡ã€‚ä¹Ÿå¯ä»¥ç”¨äºä½¿ç”¨è‡ªå®šä¹‰æ–¹æ³•è¦†ç›–æ¨¡å‹åŠ è½½ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯·ç¡®ä¿ä»æ¨¡å‹åˆ—è¡¨ä¸­åˆ é™¤å·²åŠ è½½çš„æ¨¡å‹ã€‚
- en: '#### `register_save_state_pre_hook`'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `register_save_state_pre_hook`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2619)'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2619)'
- en: '[PRE80]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Parameters
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`hook` (`Callable`) â€” A function to be called in [Accelerator.save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)
    before `save_checkpoint`.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hook`ï¼ˆ`Callable`ï¼‰â€” åœ¨[Accelerator.save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)ä¹‹å‰è°ƒç”¨çš„å‡½æ•°ï¼Œç”¨äº`save_checkpoint`ã€‚'
- en: Returns
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`torch.utils.hooks.RemovableHandle`'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.utils.hooks.RemovableHandle`'
- en: a handle that can be used to remove the added hook by calling `handle.remove()`
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¥æŸ„ï¼Œå¯ä»¥é€šè¿‡è°ƒç”¨`handle.remove()`æ¥ç§»é™¤æ·»åŠ çš„é’©å­
- en: Registers a pre hook to be run before `save_checkpoint` is called in [Accelerator.save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨å†Œä¸€ä¸ªé¢„é’©å­ï¼Œåœ¨[Accelerator.save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)ä¸­è°ƒç”¨`save_checkpoint`ä¹‹å‰è¿è¡Œã€‚
- en: 'The hook should have the following signature:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: é’©å­åº”è¯¥å…·æœ‰ä»¥ä¸‹ç­¾åï¼š
- en: '`hook(models: list[torch.nn.Module], weights: list[dict[str, torch.Tensor]],
    input_dir: str) -> None`'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '`hook(models: list[torch.nn.Module], weights: list[dict[str, torch.Tensor]],
    input_dir: str) -> None`'
- en: The `models` argument are the models as saved in the accelerator state under
    `accelerator._models`, `weigths` argument are the state dicts of the `models`,
    and the `input_dir` argument is the `input_dir` argument passed to [Accelerator.load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '`models`å‚æ•°æ˜¯ä¿å­˜åœ¨åŠ é€Ÿå™¨çŠ¶æ€ä¸‹çš„æ¨¡å‹`accelerator._models`ï¼Œ`weights`å‚æ•°æ˜¯`models`çš„çŠ¶æ€å­—å…¸ï¼Œ`input_dir`å‚æ•°æ˜¯ä¼ é€’ç»™[Accelerator.load_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.load_state)çš„`input_dir`å‚æ•°ã€‚'
- en: Should only be used in conjunction with [Accelerator.register_load_state_pre_hook()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_load_state_pre_hook).
    Can be useful to save configurations in addition to model weights. Can also be
    used to overwrite model saving with a customized method. In this case, make sure
    to remove already loaded weights from the weights list.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ä»…ä¸[Accelerator.register_load_state_pre_hook()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.register_load_state_pre_hook)ä¸€èµ·ä½¿ç”¨ã€‚é™¤äº†æ¨¡å‹æƒé‡å¤–ï¼Œä¿å­˜é…ç½®ä¹Ÿå¾ˆæœ‰ç”¨ã€‚ä¹Ÿå¯ä»¥ç”¨äºä½¿ç”¨è‡ªå®šä¹‰æ–¹æ³•è¦†ç›–æ¨¡å‹ä¿å­˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯·ç¡®ä¿ä»æƒé‡åˆ—è¡¨ä¸­åˆ é™¤å·²åŠ è½½çš„æƒé‡ã€‚
- en: '#### `save`'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2482)'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2482)'
- en: '[PRE81]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Parameters
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`obj` (`object`) â€” The object to save.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`obj`ï¼ˆ`object`ï¼‰â€” è¦ä¿å­˜çš„å¯¹è±¡ã€‚'
- en: '`f` (`str` or `os.PathLike`) â€” Where to save the content of `obj`.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`f`ï¼ˆ`str`æˆ–`os.PathLike`ï¼‰â€” è¦ä¿å­˜`obj`å†…å®¹çš„ä½ç½®ã€‚'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `False`) â€” Whether to
    save `obj` using `safetensors`'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦ä½¿ç”¨`safetensors`ä¿å­˜`obj`'
- en: Save the object passed to disk once per machine. Use in place of `torch.save`.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ä¼ é€’ç»™ç£ç›˜çš„å¯¹è±¡æ¯å°æœºå™¨ä¿å­˜ä¸€æ¬¡ã€‚ç”¨äºæ›¿ä»£`torch.save`ã€‚
- en: 'Note: If `save_on_each_node` was passed in as a `ProjectConfiguration`, will
    save the object once per node, rather than only once on the main node.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå¦‚æœ`save_on_each_node`ä½œä¸º`ProjectConfiguration`ä¼ å…¥ï¼Œåˆ™ä¼šåœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šä¿å­˜å¯¹è±¡ä¸€æ¬¡ï¼Œè€Œä¸ä»…åœ¨ä¸»èŠ‚ç‚¹ä¸Šä¿å­˜ä¸€æ¬¡ã€‚
- en: 'Example:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE82]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '#### `save_model`'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_model`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2512)'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2512)'
- en: '[PRE83]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Parameters
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`save_directory` (`str` or `os.PathLike`) â€” Directory to which to save. Will
    be created if it doesnâ€™t exist.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory`ï¼ˆ`str`æˆ–`os.PathLike`ï¼‰â€” è¦ä¿å­˜åˆ°çš„ç›®å½•ã€‚å¦‚æœä¸å­˜åœ¨ï¼Œå°†ä¼šåˆ›å»ºã€‚'
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"10GB"`) â€” The maximum
    size for a checkpoint before being sharded. Checkpoints shard will then be each
    of size lower than this size. If expressed as a string, needs to be digits followed
    by a unit (like `"5MB"`).'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_shard_size`ï¼ˆ`int`æˆ–`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"10GB"`ï¼‰â€” åœ¨åˆ†ç‰‡ä¹‹å‰çš„æ£€æŸ¥ç‚¹çš„æœ€å¤§å¤§å°ã€‚ç„¶åï¼Œæ£€æŸ¥ç‚¹åˆ†ç‰‡å°†æ¯ä¸ªå¤§å°éƒ½å°äºæ­¤å¤§å°ã€‚å¦‚æœè¡¨ç¤ºä¸ºå­—ç¬¦ä¸²ï¼Œåˆ™éœ€è¦æ˜¯æ•°å­—åè·Ÿä¸€ä¸ªå•ä½ï¼ˆå¦‚`"5MB"`ï¼‰ã€‚'
- en: If a single weight of the model is bigger than `max_shard_size`, it will be
    in its own checkpoint shard which will be bigger than `max_shard_size`.
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹çš„å•ä¸ªæƒé‡å¤§äº`max_shard_size`ï¼Œåˆ™å®ƒå°†åœ¨è‡ªå·±çš„æ£€æŸ¥ç‚¹åˆ†ç‰‡ä¸­ï¼Œè¯¥åˆ†ç‰‡å°†å¤§äº`max_shard_size`ã€‚
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) â€” Whether to
    save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦ä½¿ç”¨`safetensors`æˆ–ä¼ ç»Ÿçš„ PyTorch
    æ–¹å¼ï¼ˆä½¿ç”¨`pickle`ï¼‰ä¿å­˜æ¨¡å‹ã€‚'
- en: Save a model so that it can be re-loaded using load_checkpoint_in_model
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿å­˜æ¨¡å‹ï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨`load_checkpoint_in_model`é‡æ–°åŠ è½½
- en: 'Example:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE84]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '#### `save_state`'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_state`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2651)'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2651)'
- en: '[PRE85]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Parameters
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`output_dir` (`str` or `os.PathLike`) â€” The name of the folder to save all
    relevant weights and states.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_dir`ï¼ˆ`str`æˆ–`os.PathLike`ï¼‰â€” ä¿å­˜æ‰€æœ‰ç›¸å…³æƒé‡å’ŒçŠ¶æ€çš„æ–‡ä»¶å¤¹çš„åç§°ã€‚'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) â€” Whether to
    save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦ä½¿ç”¨`safetensors`æˆ–ä¼ ç»Ÿçš„ PyTorch
    æ–¹å¼ï¼ˆä½¿ç”¨`pickle`ï¼‰ä¿å­˜æ¨¡å‹ã€‚'
- en: '`save_model_func_kwargs` (`dict`, *optional*) â€” Additional keyword arguments
    for saving model which can be passed to the underlying save function, such as
    optional arguments for DeepSpeedâ€™s `save_checkpoint` function.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_model_func_kwargs`ï¼ˆ`dict`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä¿å­˜æ¨¡å‹çš„é¢å¤–å…³é”®å­—å‚æ•°ï¼Œå¯ä»¥ä¼ é€’ç»™åº•å±‚ä¿å­˜å‡½æ•°ï¼Œä¾‹å¦‚ DeepSpeed
    çš„`save_checkpoint`å‡½æ•°çš„å¯é€‰å‚æ•°ã€‚'
- en: Saves the current states of the model, optimizer, scaler, RNG generators, and
    registered objects to a folder.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€ç¼©æ”¾å™¨ã€RNG ç”Ÿæˆå™¨å’Œå·²æ³¨å†Œå¯¹è±¡çš„å½“å‰çŠ¶æ€ä¿å­˜åˆ°ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ã€‚
- en: If a `ProjectConfiguration` was passed to the `Accelerator` object with `automatic_checkpoint_naming`
    enabled then checkpoints will be saved to `self.project_dir/checkpoints`. If the
    number of current saves is greater than `total_limit` then the oldest save is
    deleted. Each checkpoint is saved in seperate folders named `checkpoint_<iteration>`.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå°†`ProjectConfiguration`ä¼ é€’ç»™å¯ç”¨äº†`automatic_checkpoint_naming`çš„`Accelerator`å¯¹è±¡ï¼Œåˆ™æ£€æŸ¥ç‚¹å°†ä¿å­˜åœ¨`self.project_dir/checkpoints`ä¸­ã€‚å¦‚æœå½“å‰ä¿å­˜çš„æ•°é‡å¤§äº`total_limit`ï¼Œåˆ™å°†åˆ é™¤æœ€æ—§çš„ä¿å­˜ã€‚æ¯ä¸ªæ£€æŸ¥ç‚¹éƒ½ä¿å­˜åœ¨åä¸º`checkpoint_<iteration>`çš„å•ç‹¬æ–‡ä»¶å¤¹ä¸­ã€‚
- en: Otherwise they are just saved to `output_dir`.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: å¦åˆ™å®ƒä»¬åªä¿å­˜åˆ°`output_dir`ã€‚
- en: Should only be used when wanting to save a checkpoint during training and restoring
    the state in the same environment.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…åœ¨å¸Œæœ›åœ¨è®­ç»ƒæœŸé—´ä¿å­˜æ£€æŸ¥ç‚¹å¹¶åœ¨ç›¸åŒç¯å¢ƒä¸­æ¢å¤çŠ¶æ€æ—¶ä½¿ç”¨ã€‚
- en: 'Example:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE86]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '#### `set_trigger`'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_trigger`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1968)'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L1968)'
- en: '[PRE87]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Sets the internal trigger tensor to 1 on the current process. A latter check
    should follow using this which will check across all processes.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å½“å‰è¿›ç¨‹çš„å†…éƒ¨è§¦å‘å¼ é‡è®¾ç½®ä¸º1ã€‚éšååº”è¯¥ä½¿ç”¨æ­¤æ£€æŸ¥ï¼Œè¯¥æ£€æŸ¥å°†è·¨æ‰€æœ‰è¿›ç¨‹è¿›è¡Œæ£€æŸ¥ã€‚
- en: 'Note: Does not require `wait_for_everyone()`'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šä¸éœ€è¦`wait_for_everyone()`
- en: 'Example:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE88]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '#### `skip_first_batches`'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `skip_first_batches`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3156)'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3156)'
- en: '[PRE89]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Parameters
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`dataloader` (`torch.utils.data.DataLoader`) â€” The data loader in which to
    skip batches.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataloader`ï¼ˆ`torch.utils.data.DataLoader`ï¼‰â€” è¦è·³è¿‡æ‰¹æ¬¡çš„æ•°æ®åŠ è½½å™¨ã€‚'
- en: '`num_batches` (`int`, *optional*, defaults to 0) â€” The number of batches to
    skip'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_batches`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰â€” è¦è·³è¿‡çš„æ‰¹æ¬¡æ•°'
- en: Creates a new `torch.utils.data.DataLoader` that will efficiently skip the first
    `num_batches`.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªæ–°çš„`torch.utils.data.DataLoader`ï¼Œå®ƒå°†é«˜æ•ˆåœ°è·³è¿‡å‰`num_batches`ã€‚
- en: 'Example:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE90]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '#### `split_between_processes`'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `split_between_processes`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L553)'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L553)'
- en: '[PRE91]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Parameters
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`inputs` (`list`, `tuple`, `torch.Tensor`, or `dict` of `list`/`tuple`/`torch.Tensor`)
    â€” The input to split between processes.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs`ï¼ˆ`list`ï¼Œ`tuple`ï¼Œ`torch.Tensor`æˆ–`dict`çš„`list`/`tuple`/`torch.Tensor`ï¼‰â€”
    åœ¨è¿›ç¨‹ä¹‹é—´æ‹†åˆ†çš„è¾“å…¥ã€‚'
- en: '`apply_padding` (`bool`, `optional`, defaults to `False`) â€” Whether to apply
    padding by repeating the last element of the input so that all processes have
    the same number of elements. Useful when trying to perform actions such as `Accelerator.gather()`
    on the outputs or passing in less inputs than there are processes. If so, just
    remember to drop the padded elements afterwards.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apply_padding`ï¼ˆ`bool`ï¼Œ`å¯é€‰`ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦é€šè¿‡é‡å¤è¾“å…¥çš„æœ€åä¸€ä¸ªå…ƒç´ æ¥åº”ç”¨å¡«å……ï¼Œä»¥ä¾¿æ‰€æœ‰è¿›ç¨‹å…·æœ‰ç›¸åŒæ•°é‡çš„å…ƒç´ ã€‚åœ¨å°è¯•å¯¹è¾“å‡ºæ‰§è¡Œ`Accelerator.gather()`ç­‰æ“ä½œæˆ–ä¼ å…¥å°‘äºè¿›ç¨‹æ•°çš„è¾“å…¥æ—¶å¾ˆæœ‰ç”¨ã€‚å¦‚æœæ˜¯è¿™æ ·ï¼Œè¯·è®°å¾—ä¹‹ååˆ é™¤å¡«å……çš„å…ƒç´ ã€‚'
- en: Splits `input` between `self.num_processes` quickly and can be then used on
    that process. Useful when doing distributed inference, such as with different
    prompts.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: å¿«é€Ÿå°†`input`åœ¨`self.num_processes`ä¹‹é—´æ‹†åˆ†ï¼Œç„¶åå¯ä»¥åœ¨è¯¥è¿›ç¨‹ä¸Šä½¿ç”¨ã€‚åœ¨è¿›è¡Œåˆ†å¸ƒå¼æ¨ç†æ—¶å¾ˆæœ‰ç”¨ï¼Œä¾‹å¦‚ä½¿ç”¨ä¸åŒçš„æç¤ºã€‚
- en: Note that when using a `dict`, all keys need to have the same number of elements.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä½¿ç”¨`dict`æ—¶ï¼Œæ‰€æœ‰é”®éƒ½éœ€è¦å…·æœ‰ç›¸åŒæ•°é‡çš„å…ƒç´ ã€‚
- en: 'Example:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE92]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '#### `trigger_sync_in_backward`'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `trigger_sync_in_backward`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L893)'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L893)'
- en: '[PRE93]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Parameters
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`torch.nn.Module`) â€” The model for which to trigger the gradient synchronization.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`ï¼ˆ`torch.nn.Module`ï¼‰â€” è¦è§¦å‘æ¢¯åº¦åŒæ­¥çš„æ¨¡å‹ã€‚'
- en: Trigger the sync of the gradients in the next backward pass of the model after
    multiple forward passes under `Accelerator.no_sync` (only applicable in multi-GPU
    scenarios).
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨`Accelerator.no_sync`ä¸‹çš„å¤šæ¬¡å‰å‘ä¼ é€’åï¼Œè§¦å‘æ¨¡å‹åœ¨ä¸‹ä¸€ä¸ªåå‘ä¼ é€’ä¸­åŒæ­¥æ¢¯åº¦ï¼ˆä»…é€‚ç”¨äºå¤šGPUåœºæ™¯ï¼‰ã€‚
- en: If the script is not launched in distributed mode, this context manager does
    nothing.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè„šæœ¬æœªåœ¨åˆ†å¸ƒå¼æ¨¡å¼ä¸‹å¯åŠ¨ï¼Œåˆ™æ­¤ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸èµ·ä½œç”¨ã€‚
- en: 'Example:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE94]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '#### `unscale_gradients`'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `unscale_gradients`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2027)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2027)'
- en: '[PRE95]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Parameters
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`optimizer` (`torch.optim.Optimizer` or `list[torch.optim.Optimizer]`, *optional*)
    â€” The optimizer(s) for which to unscale gradients. If not set, will unscale gradients
    on all optimizers that were passed to [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare).'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer`ï¼ˆ`torch.optim.Optimizer`æˆ–`list[torch.optim.Optimizer]`ï¼Œ*å¯é€‰*ï¼‰â€” è¦å¯¹æ¢¯åº¦è¿›è¡Œä¸ç¼©æ”¾çš„ä¼˜åŒ–å™¨ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†å¯¹ä¼ é€’ç»™[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)çš„æ‰€æœ‰ä¼˜åŒ–å™¨è¿›è¡Œæ¢¯åº¦ä¸ç¼©æ”¾ã€‚'
- en: Unscale the gradients in mixed precision training with AMP. This is a noop in
    all other settings.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨AMPæ··åˆç²¾åº¦è®­ç»ƒä¸­ä¸ç¼©æ”¾æ¢¯åº¦ã€‚åœ¨æ‰€æœ‰å…¶ä»–è®¾ç½®ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ªç©ºæ“ä½œã€‚
- en: Likely should be called through [Accelerator.clip*grad_norm*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_norm_)
    or [Accelerator.clip*grad_value*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_value_)
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: å¯èƒ½åº”è¯¥é€šè¿‡[Accelerator.clip*grad_norm*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_norm_)æˆ–[Accelerator.clip*grad_value*()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.clip_grad_value_)æ¥è°ƒç”¨
- en: 'Example:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE96]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '#### `unwrap_model`'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `unwrap_model`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2296)'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2296)'
- en: '[PRE97]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Parameters
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`torch.nn.Module`) â€” The model to unwrap.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`ï¼ˆ`torch.nn.Module`ï¼‰â€” è¦å–æ¶ˆåŒ…è£…çš„æ¨¡å‹ã€‚'
- en: '`keep_fp32_wrapper` (`bool`, *optional*, defaults to `True`) â€” Whether to not
    remove the mixed precision hook if it was added.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_fp32_wrapper`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦åœ¨æ·»åŠ æ—¶ä¸åˆ é™¤æ··åˆç²¾åº¦é’©å­ã€‚'
- en: Returns
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`torch.nn.Module`'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Module`'
- en: The unwrapped model.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: æœªåŒ…è£…çš„æ¨¡å‹ã€‚
- en: Unwraps the `model` from the additional layer possible added by [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare).
    Useful before saving the model.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: ä»[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)å¯èƒ½æ·»åŠ çš„é¢å¤–å±‚ä¸­å–æ¶ˆåŒ…è£…`model`ã€‚åœ¨ä¿å­˜æ¨¡å‹ä¹‹å‰å¾ˆæœ‰ç”¨ã€‚
- en: 'Example:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE98]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '#### `verify_device_map`'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `verify_device_map`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3192)'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L3192)'
- en: '[PRE99]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Verifies that `model` has not been prepared with big model inference with a
    device-map resembling `auto`.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: éªŒè¯`model`æ˜¯å¦æœªå‡†å¤‡å¥½ä½¿ç”¨ç±»ä¼¼`auto`çš„è®¾å¤‡æ˜ å°„è¿›è¡Œå¤§å‹æ¨¡å‹æ¨ç†ã€‚
- en: '#### `wait_for_everyone`'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `wait_for_everyone`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2329)'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/accelerator.py#L2329)'
- en: '[PRE100]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: Will stop the execution of the current process until every other process has
    reached that point (so this does nothing when the script is only run in one process).
    Useful to do before saving a model.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: å°†åœæ­¢å½“å‰è¿›ç¨‹çš„æ‰§è¡Œï¼Œç›´åˆ°æ¯ä¸ªå…¶ä»–è¿›ç¨‹éƒ½è¾¾åˆ°è¯¥ç‚¹ï¼ˆå› æ­¤å½“è„šæœ¬ä»…åœ¨ä¸€ä¸ªè¿›ç¨‹ä¸­è¿è¡Œæ—¶ï¼Œæ­¤æ“ä½œæ— æ•ˆï¼‰ã€‚åœ¨ä¿å­˜æ¨¡å‹ä¹‹å‰æ‰§è¡Œæ­¤æ“ä½œå¾ˆæœ‰ç”¨ã€‚
- en: 'Example:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE101]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
