# æ•…éšœæ’é™¤

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/peft/developer_guides/troubleshooting](https://huggingface.co/docs/peft/developer_guides/troubleshooting)

å¦‚æœåœ¨ä½¿ç”¨PEFTæ—¶é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹å¸¸è§é—®é¢˜åŠå…¶è§£å†³æ–¹æ¡ˆåˆ—è¡¨ã€‚

## ç¤ºä¾‹ä¸èµ·ä½œç”¨

ç¤ºä¾‹é€šå¸¸ä¾èµ–äºæœ€æ–°çš„è½¯ä»¶åŒ…ç‰ˆæœ¬ï¼Œå› æ­¤è¯·ç¡®ä¿å®ƒä»¬æ˜¯æœ€æ–°çš„ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹è½¯ä»¶åŒ…ç‰ˆæœ¬ï¼š

+   `peft`

+   `transformers`

+   `accelerate`

+   `torch`

é€šå¸¸æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥é€šè¿‡åœ¨Pythonç¯å¢ƒä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥æ›´æ–°è½¯ä»¶åŒ…ç‰ˆæœ¬ï¼š

```py
python -m pip install -U <package_name>
```

ä»æºä»£ç å®‰è£…PEFTå¯¹äºè·Ÿä¸Šæœ€æ–°å‘å±•æ˜¯æœ‰ç”¨çš„ï¼š

```py
python -m pip install git+https://github.com/huggingface/peft
```

## ValueError: å°è¯•å¯¹FP16æ¢¯åº¦è¿›è¡Œåç¼©æ”¾

è¿™ä¸ªé”™è¯¯å¯èƒ½æ˜¯å› ä¸ºæ¨¡å‹åŠ è½½æ—¶ä½¿ç”¨äº†`torch_dtype=torch.float16`ï¼Œç„¶ååœ¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰ç¯å¢ƒä¸­ä½¿ç”¨ï¼Œä¾‹å¦‚é€šè¿‡åœ¨ğŸ¤— Transformersçš„[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ç±»ä¸­è®¾ç½®`fp16=True`ã€‚åŸå› æ˜¯åœ¨ä½¿ç”¨AMPæ—¶ï¼Œå¯è®­ç»ƒæƒé‡ä¸åº”ä½¿ç”¨fp16ã€‚ä¸ºäº†ä½¿å…¶åœ¨ä¸åŠ è½½æ•´ä¸ªæ¨¡å‹åˆ°fp32çš„æƒ…å†µä¸‹å·¥ä½œï¼Œè¯·å°†ä»¥ä¸‹å†…å®¹æ·»åŠ åˆ°æ‚¨çš„ä»£ç ä¸­ï¼š

```py
peft_model = get_peft_model(...)

# add this:
for param in model.parameters():
    if param.requires_grad:
        param.data = param.data.float()

# proceed as usual
trainer = Trainer(model=peft_model, fp16=True, ...)
trainer.train()
```

æˆ–è€…ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[cast_mixed_precision_params()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.cast_mixed_precision_params)å‡½æ•°æ­£ç¡®è½¬æ¢æƒé‡ï¼š

```py
from peft import cast_mixed_precision_params

peft_model = get_peft_model(...)
cast_mixed_precision_params(peft_model, dtype=torch.float16)

# proceed as usual
trainer = Trainer(model=peft_model, fp16=True, ...)
trainer.train()
```

## ä»åŠ è½½çš„PEFTæ¨¡å‹ä¸­è·å¾—ç³Ÿç³•çš„ç»“æœ

ä»åŠ è½½çš„PEFTæ¨¡å‹ä¸­è·å¾—ç³Ÿç³•ç»“æœå¯èƒ½æœ‰å‡ ä¸ªåŸå› ï¼Œåˆ—ä¸¾å¦‚ä¸‹ã€‚å¦‚æœæ‚¨ä»ç„¶æ— æ³•è§£å†³é—®é¢˜ï¼Œè¯·æŸ¥çœ‹GitHubä¸Šæ˜¯å¦æœ‰å…¶ä»–äººé‡åˆ°ç±»ä¼¼çš„[é—®é¢˜](https://github.com/huggingface/peft/issues)ï¼Œå¦‚æœæ‰¾ä¸åˆ°ï¼Œè¯·æå‡ºæ–°é—®é¢˜ã€‚

åœ¨æå‡ºé—®é¢˜æ—¶ï¼Œå¦‚æœæ‚¨æä¾›ä¸€ä¸ªèƒ½å¤Ÿé‡ç°é—®é¢˜çš„æœ€å°ä»£ç ç¤ºä¾‹ï¼Œå°†ä¼šå¾ˆæœ‰å¸®åŠ©ã€‚å¦å¤–ï¼Œè¯·æŠ¥å‘ŠåŠ è½½çš„æ¨¡å‹æ˜¯å¦ä¸å¾®è°ƒå‰çš„æ¨¡å‹è¡¨ç°ç›¸åŒï¼Œæ˜¯å¦è¡¨ç°åœ¨éšæœºæ°´å¹³ä¸Šï¼Œæˆ–è€…æ˜¯å¦åªæ¯”é¢„æœŸç¨å·®ã€‚è¿™äº›ä¿¡æ¯æœ‰åŠ©äºæˆ‘ä»¬æ›´å¿«åœ°è¯†åˆ«é—®é¢˜ã€‚

### éšæœºåå·®

å¦‚æœæ‚¨çš„æ¨¡å‹è¾“å‡ºä¸ä¹‹å‰çš„è¿è¡Œç»“æœä¸å®Œå…¨ç›¸åŒï¼Œå¯èƒ½å­˜åœ¨éšæœºå…ƒç´ çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼š

1.  è¯·ç¡®ä¿å¤„äº`.eval()`æ¨¡å¼ï¼Œè¿™å¾ˆé‡è¦ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæ¨¡å‹ä½¿ç”¨äº†dropout

1.  å¦‚æœåœ¨è¯­è¨€æ¨¡å‹ä¸Šä½¿ç”¨[generate](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)ï¼Œå¯èƒ½ä¼šè¿›è¡ŒéšæœºæŠ½æ ·ï¼Œå› æ­¤è¦è·å¾—ç›¸åŒçš„ç»“æœï¼Œéœ€è¦è®¾ç½®ä¸€ä¸ªéšæœºç§å­

1.  å¦‚æœæ‚¨ä½¿ç”¨äº†é‡åŒ–å¹¶åˆå¹¶äº†æƒé‡ï¼Œç”±äºå››èˆäº”å…¥è¯¯å·®ï¼Œå¯èƒ½ä¼šå‡ºç°å°çš„åå·®ã€‚

### é”™è¯¯åŠ è½½çš„æ¨¡å‹

è¯·ç¡®ä¿æ­£ç¡®åŠ è½½æ¨¡å‹ã€‚ä¸€ä¸ªå¸¸è§é”™è¯¯æ˜¯å°è¯•ä½¿ç”¨[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)åŠ è½½*è®­ç»ƒè¿‡çš„*æ¨¡å‹ï¼Œè¿™æ˜¯ä¸æ­£ç¡®çš„ã€‚ç›¸åï¼ŒåŠ è½½ä»£ç åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
from peft import PeftModel, PeftConfig

base_model = ...  # to load the base model, use the same code as when you trained it
config = PeftConfig.from_pretrained(peft_model_id)
peft_model = PeftModel.from_pretrained(base_model, peft_model_id)
```

### éšæœºåˆå§‹åŒ–çš„å±‚

å¯¹äºæŸäº›ä»»åŠ¡ï¼Œæ­£ç¡®é…ç½®`config`ä¸­çš„`modules_to_save`éå¸¸é‡è¦ï¼Œä»¥è€ƒè™‘éšæœºåˆå§‹åŒ–çš„å±‚ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä½¿ç”¨LoRAå¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œåºåˆ—åˆ†ç±»ï¼Œå› ä¸ºğŸ¤— Transformersåœ¨æ¨¡å‹é¡¶éƒ¨æ·»åŠ äº†ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„åˆ†ç±»å¤´ã€‚å¦‚æœæ‚¨æ²¡æœ‰å°†æ­¤å±‚æ·»åŠ åˆ°`modules_to_save`ä¸­ï¼Œåˆ†ç±»å¤´å°†ä¸ä¼šè¢«ä¿å­˜ã€‚ä¸‹æ¬¡åŠ è½½æ¨¡å‹æ—¶ï¼Œæ‚¨å°†è·å¾—ä¸€ä¸ª*ä¸åŒçš„*éšæœºåˆå§‹åŒ–çš„åˆ†ç±»å¤´ï¼Œå¯¼è‡´å®Œå…¨ä¸åŒçš„ç»“æœã€‚

å¦‚æœåœ¨é…ç½®ä¸­æä¾›äº†`task_type`å‚æ•°ï¼ŒPEFTä¼šå°è¯•æ­£ç¡®çŒœæµ‹`modules_to_save`ã€‚è¿™å¯¹äºéµå¾ªæ ‡å‡†å‘½åæ–¹æ¡ˆçš„transformersæ¨¡å‹åº”è¯¥æœ‰æ•ˆã€‚ä¸è¿‡æœ€å¥½è¿˜æ˜¯ä»”ç»†æ£€æŸ¥ä¸€ä¸‹ï¼Œå› ä¸ºæˆ‘ä»¬æ— æ³•ä¿è¯æ‰€æœ‰æ¨¡å‹éƒ½éµå¾ªå‘½åæ–¹æ¡ˆã€‚

å½“æ‚¨åŠ è½½ä¸€ä¸ªå…·æœ‰éšæœºåˆå§‹åŒ–å±‚çš„transformersæ¨¡å‹æ—¶ï¼Œæ‚¨åº”è¯¥çœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹çš„è­¦å‘Šï¼š

```py
Some weights of <MODEL> were not initialized from the model checkpoint at <ID> and are newly initialized: [<LAYER_NAMES>].
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

åº”å°†æåˆ°çš„å±‚æ·»åŠ åˆ°é…ç½®ä¸­çš„`modules_to_save`ä¸­ï¼Œä»¥é¿å…æ‰€æè¿°çš„é—®é¢˜ã€‚

### æ‰©å±•è¯æ±‡è¡¨

å¯¹äºè®¸å¤šè¯­è¨€å¾®è°ƒä»»åŠ¡ï¼Œéœ€è¦æ‰©å±•æ¨¡å‹çš„è¯æ±‡è¡¨ï¼Œå› ä¸ºå¼•å…¥äº†æ–°çš„æ ‡è®°ã€‚è¿™éœ€è¦æ‰©å±•åµŒå…¥å±‚ä»¥è€ƒè™‘æ–°çš„æ ‡è®°ï¼Œå¹¶åœ¨ä¿å­˜é€‚é…å™¨æ—¶å°†åµŒå…¥å±‚å­˜å‚¨åœ¨é€‚é…å™¨æƒé‡ä¹‹å¤–ã€‚

é€šè¿‡å°†åµŒå…¥å±‚æ·»åŠ åˆ°é…ç½®çš„`target_modules`ä¸­ä¿å­˜åµŒå…¥å±‚ã€‚åµŒå…¥å±‚çš„åç§°å¿…é¡»éµå¾ªTransformersçš„æ ‡å‡†å‘½åæ–¹æ¡ˆã€‚ä¾‹å¦‚ï¼ŒMistralé…ç½®å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
config = LoraConfig(..., target_modules=["embed_tokens", "lm_head", "q_proj", "v_proj"])
```

ä¸€æ—¦æ·»åŠ åˆ°`target_modules`ï¼ŒPEFTåœ¨ä¿å­˜é€‚é…å™¨æ—¶ä¼šè‡ªåŠ¨å­˜å‚¨åµŒå…¥å±‚ï¼Œå¦‚æœæ¨¡å‹å…·æœ‰[get_input_embeddings](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.get_input_embeddings)å’Œ[get_output_embeddings](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.get_output_embeddings)ã€‚è¿™é€šå¸¸é€‚ç”¨äºTransformersæ¨¡å‹ã€‚

å¦‚æœæ¨¡å‹çš„åµŒå…¥å±‚ä¸éµå¾ªTransformerçš„å‘½åæ–¹æ¡ˆï¼Œä»å¯ä»¥é€šè¿‡åœ¨ä¿å­˜é€‚é…å™¨æ—¶æ‰‹åŠ¨ä¼ é€’`save_embedding_layers=True`æ¥ä¿å­˜å®ƒã€‚

```py
model = get_peft_model(...)
# train the model
model.save_adapter("my_adapter", save_embedding_layers=True)
```

å¯¹äºæ¨ç†ï¼Œé¦–å…ˆåŠ è½½åŸºç¡€æ¨¡å‹å¹¶ä»¥ä¸è®­ç»ƒæ¨¡å‹ä¹‹å‰ç›¸åŒçš„æ–¹å¼è°ƒæ•´å¤§å°ã€‚è°ƒæ•´å¤§å°åï¼Œå¯ä»¥åŠ è½½PEFTæ£€æŸ¥ç‚¹ã€‚

æœ‰å…³å®Œæ•´ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹[æ­¤ç¬”è®°æœ¬](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_with_additional_tokens.ipynb)ã€‚
