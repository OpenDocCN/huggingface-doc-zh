# 故障排除

> 原始文本：[https://huggingface.co/docs/peft/developer_guides/troubleshooting](https://huggingface.co/docs/peft/developer_guides/troubleshooting)

如果在使用PEFT时遇到任何问题，请检查以下常见问题及其解决方案列表。

## 示例不起作用

示例通常依赖于最新的软件包版本，因此请确保它们是最新的。特别是，请检查以下软件包版本：

+   `peft`

+   `transformers`

+   `accelerate`

+   `torch`

通常情况下，您可以通过在Python环境中运行以下命令来更新软件包版本：

```py
python -m pip install -U <package_name>
```

从源代码安装PEFT对于跟上最新发展是有用的：

```py
python -m pip install git+https://github.com/huggingface/peft
```

## ValueError: 尝试对FP16梯度进行反缩放

这个错误可能是因为模型加载时使用了`torch_dtype=torch.float16`，然后在自动混合精度（AMP）环境中使用，例如通过在🤗 Transformers的[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类中设置`fp16=True`。原因是在使用AMP时，可训练权重不应使用fp16。为了使其在不加载整个模型到fp32的情况下工作，请将以下内容添加到您的代码中：

```py
peft_model = get_peft_model(...)

# add this:
for param in model.parameters():
    if param.requires_grad:
        param.data = param.data.float()

# proceed as usual
trainer = Trainer(model=peft_model, fp16=True, ...)
trainer.train()
```

或者，您可以使用[cast_mixed_precision_params()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.cast_mixed_precision_params)函数正确转换权重：

```py
from peft import cast_mixed_precision_params

peft_model = get_peft_model(...)
cast_mixed_precision_params(peft_model, dtype=torch.float16)

# proceed as usual
trainer = Trainer(model=peft_model, fp16=True, ...)
trainer.train()
```

## 从加载的PEFT模型中获得糟糕的结果

从加载的PEFT模型中获得糟糕结果可能有几个原因，列举如下。如果您仍然无法解决问题，请查看GitHub上是否有其他人遇到类似的[问题](https://github.com/huggingface/peft/issues)，如果找不到，请提出新问题。

在提出问题时，如果您提供一个能够重现问题的最小代码示例，将会很有帮助。另外，请报告加载的模型是否与微调前的模型表现相同，是否表现在随机水平上，或者是否只比预期稍差。这些信息有助于我们更快地识别问题。

### 随机偏差

如果您的模型输出与之前的运行结果不完全相同，可能存在随机元素的问题。例如：

1.  请确保处于`.eval()`模式，这很重要，例如，如果模型使用了dropout

1.  如果在语言模型上使用[generate](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)，可能会进行随机抽样，因此要获得相同的结果，需要设置一个随机种子

1.  如果您使用了量化并合并了权重，由于四舍五入误差，可能会出现小的偏差。

### 错误加载的模型

请确保正确加载模型。一个常见错误是尝试使用[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)加载*训练过的*模型，这是不正确的。相反，加载代码应该如下所示：

```py
from peft import PeftModel, PeftConfig

base_model = ...  # to load the base model, use the same code as when you trained it
config = PeftConfig.from_pretrained(peft_model_id)
peft_model = PeftModel.from_pretrained(base_model, peft_model_id)
```

### 随机初始化的层

对于某些任务，正确配置`config`中的`modules_to_save`非常重要，以考虑随机初始化的层。

例如，如果您使用LoRA对语言模型进行微调以进行序列分类，因为🤗 Transformers在模型顶部添加了一个随机初始化的分类头。如果您没有将此层添加到`modules_to_save`中，分类头将不会被保存。下次加载模型时，您将获得一个*不同的*随机初始化的分类头，导致完全不同的结果。

如果在配置中提供了`task_type`参数，PEFT会尝试正确猜测`modules_to_save`。这对于遵循标准命名方案的transformers模型应该有效。不过最好还是仔细检查一下，因为我们无法保证所有模型都遵循命名方案。

当您加载一个具有随机初始化层的transformers模型时，您应该看到类似以下的警告：

```py
Some weights of <MODEL> were not initialized from the model checkpoint at <ID> and are newly initialized: [<LAYER_NAMES>].
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

应将提到的层添加到配置中的`modules_to_save`中，以避免所描述的问题。

### 扩展词汇表

对于许多语言微调任务，需要扩展模型的词汇表，因为引入了新的标记。这需要扩展嵌入层以考虑新的标记，并在保存适配器时将嵌入层存储在适配器权重之外。

通过将嵌入层添加到配置的`target_modules`中保存嵌入层。嵌入层的名称必须遵循Transformers的标准命名方案。例如，Mistral配置可能如下所示：

```py
config = LoraConfig(..., target_modules=["embed_tokens", "lm_head", "q_proj", "v_proj"])
```

一旦添加到`target_modules`，PEFT在保存适配器时会自动存储嵌入层，如果模型具有[get_input_embeddings](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.get_input_embeddings)和[get_output_embeddings](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.get_output_embeddings)。这通常适用于Transformers模型。

如果模型的嵌入层不遵循Transformer的命名方案，仍可以通过在保存适配器时手动传递`save_embedding_layers=True`来保存它。

```py
model = get_peft_model(...)
# train the model
model.save_adapter("my_adapter", save_embedding_layers=True)
```

对于推理，首先加载基础模型并以与训练模型之前相同的方式调整大小。调整大小后，可以加载PEFT检查点。

有关完整示例，请查看[此笔记本](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_with_additional_tokens.ipynb)。
