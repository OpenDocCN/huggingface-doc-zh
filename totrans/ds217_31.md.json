["```py\n>>> from datasets import load_dataset, Audio\n\n>>> dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\")\n>>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n```", "```py\n>>> dataset[0][\"audio\"]\n{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,\n         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n 'sampling_rate': 16000}\n```", "```py\n    >>> from transformers import AutoTokenizer, AutoFeatureExtractor, AutoProcessor\n\n    >>> model_checkpoint = \"facebook/wav2vec2-large-xlsr-53\"\n    # after defining a vocab.json file you can instantiate a tokenizer object:\n    >>> tokenizer = AutoTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n    >>> feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n    >>> processor = AutoProcessor.from_pretrained(feature_extractor=feature_extractor, tokenizer=tokenizer)\n    ```", "```py\n    >>> from transformers import AutoProcessor\n\n    >>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n    ```", "```py\n>>> def prepare_dataset(batch):\n...     audio = batch[\"audio\"]\n...     batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n...     batch[\"input_length\"] = len(batch[\"input_values\"])\n...     with processor.as_target_processor():\n...         batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n...     return batch\n>>> dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)\n```"]