- en: ðŸ¤— Accelerateâ€™s internal mechanisms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/accelerate/concept_guides/internal_mechanism](https://huggingface.co/docs/accelerate/concept_guides/internal_mechanism)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/accelerate/v0.27.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/start.6e0fb178.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/scheduler.69131cc3.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/singletons.ac467c20.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/paths.b2f3aeca.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/app.67e11fc0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/index.e1f30d73.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/0.bfeed9f0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/11.d14486dc.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Tip.22e79575.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Heading.0aab6758.js">
  prefs: []
  type: TYPE_NORMAL
- en: Internally, ðŸ¤— Accelerate works by first analyzing the environment in which the
    script is launched to determine which kind of distributed setup is used, how many
    different processes there are and which one the current script is in. All that
    information is stored in the `~AcceleratorState`.
  prefs: []
  type: TYPE_NORMAL
- en: This class is initialized the first time you instantiate an [~Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    as well as performing any specific initialization your distributed setup needs.
    Its state is then uniquely shared through all instances of [AcceleratorState](/docs/accelerate/v0.27.2/en/package_reference/state#accelerate.state.AcceleratorState).
    (The same can also be done with the [PartialState](/docs/accelerate/v0.27.2/en/package_reference/state#accelerate.PartialState),
    a more barebones version it inherits)
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, when calling [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare),
    the library:'
  prefs: []
  type: TYPE_NORMAL
- en: wraps your model(s) in the container adapted for the distributed setup,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: wraps your optimizer(s) in an [AcceleratedOptimizer](/docs/accelerate/v0.27.2/en/package_reference/torch_wrappers#accelerate.optimizer.AcceleratedOptimizer),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: wraps your scheduler(s) in an [AcceleratedScheduler](/docs/accelerate/v0.27.2/en/package_reference/torch_wrappers#accelerate.scheduler.AcceleratedScheduler)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: creates a new version of your dataloader(s) in a [DataLoaderShard](/docs/accelerate/v0.27.2/en/package_reference/torch_wrappers#accelerate.data_loader.DataLoaderShard)
    or [DataLoaderDispatcher](/docs/accelerate/v0.27.2/en/package_reference/torch_wrappers#accelerate.data_loader.DataLoaderDispatcher)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the model(s), optimizer(s), and scheduler(s) are just put in simple wrappers,
    the dataloader(s) are re-created. This is mostly because PyTorch does not let
    the user change the `batch_sampler` of a dataloader once itâ€™s been created and
    the library handles the sharding of your data between processes by changing that
    `batch_sampler` to yield every other `num_processes` batches (if enabled).
  prefs: []
  type: TYPE_NORMAL
- en: 'The [DataLoaderShard](/docs/accelerate/v0.27.2/en/package_reference/torch_wrappers#accelerate.data_loader.DataLoaderShard)
    subclasses `DataLoader` to add the following functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: it synchronizes the appropriate random number generator of all processes at
    each new iteration, to ensure any randomization (like shuffling) is done the exact
    same way across processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it puts the batches on the proper device before yielding them (unless you have
    opted out of `device_placement=True`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [DataLoaderDispatcher](/docs/accelerate/v0.27.2/en/package_reference/torch_wrappers#accelerate.data_loader.DataLoaderDispatcher)
    subclasses differs from the [DataLoaderShard](/docs/accelerate/v0.27.2/en/package_reference/torch_wrappers#accelerate.data_loader.DataLoaderShard)
    in that when iterating through the `DataLoader`, the data is all starting from
    process 0 and *then* split and sent off to each process rather than it happening
    at the dataset level.
  prefs: []
  type: TYPE_NORMAL
- en: 'The random number generator synchronization will by default synchronize:'
  prefs: []
  type: TYPE_NORMAL
- en: the `generator` attribute of a given sampler (like the PyTorch `RandomSampler`)
    for PyTorch >= 1.6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the main random number generator in PyTorch <=1.5.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can choose which random number generator(s) to synchronize with the `rng_types`
    argument of the main [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator).
    In PyTorch >= 1.6, it is recommended to rely on a local `generator` to avoid setting
    the same seed in the main random number generator in all processes.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization of the main torch (or CUDA or XLA) random number generator will
    affect any other potential random artifacts you could have in your dataset (like
    random data augmentation) in the sense that all processes will get the same random
    numbers from the torch random modules (so will apply the same random data augmentation
    if itâ€™s controlled by torch).
  prefs: []
  type: TYPE_NORMAL
- en: The randomization part of your custom sampler, batch sampler or iterable dataset
    should be done using a local `torch.Generator` object (in PyTorch >= 1.6), see
    the traditional `RandomSampler`, as an example.
  prefs: []
  type: TYPE_NORMAL
- en: For more details about the internals, see the [Internals page](package_reference/torch_wrappers).
  prefs: []
  type: TYPE_NORMAL
