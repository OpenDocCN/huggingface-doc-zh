- en: FAQs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见问题解答
- en: 'Original text: [https://huggingface.co/docs/inference-endpoints/faq](https://huggingface.co/docs/inference-endpoints/faq)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/inference-endpoints/faq](https://huggingface.co/docs/inference-endpoints/faq)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: In which regions are Inference Endpoints available?'
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问：推理端点在哪些地区可用？
- en: 'A: Inference Endpoints are currently available on AWS in us-east-1 (N. Virginia)
    & eu-west-1 (Ireland) and on Azure in eastus (Virginia). If you need to deploy
    in a different region, please let us know.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 答：推理端点目前在AWS的us-east-1（北弗吉尼亚）和eu-west-1（爱尔兰）以及Azure的eastus（弗吉尼亚）上可用。如果您需要在其他地区部署，请告诉我们。
- en: 'Q: Can I access the instance my Endpoint is running on?'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问：我可以访问我的端点正在运行的实例吗？
- en: 'A: No, you cannot access the instance hosting your Endpoint. But if you are
    missing information or need more insights on the machine where the Endpoint is
    running, please contact us.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 答：不，您无法访问托管您的端点的实例。但是，如果您缺少信息或需要有关运行端点的机器的更多见解，请联系我们。
- en: 'Q: Can I see my Private Endpoint running on my VPC account?'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问：我能看到我的私有端点在我的VPC帐户上运行吗？
- en: 'A: No, when creating a Private Endpoint (a Hugging Face Inference Endpoint
    linked to your VPC via AWS/Azure PrivateLink), you can only see the ENI in your
    VPC where the Endpoint is available.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 答：不，当创建私有端点（一个通过AWS/Azure PrivateLink与您的VPC连接的Hugging Face推理端点）时，您只能看到端点可用的VPC中的ENI。
- en: 'Q: Can I run inference in batches?'
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问：我可以批量运行推理吗？
- en: 'A: It depends on the Task. The [supported Tasks](/docs/inference-endpoints/supported_tasks)
    are using the transformers or sentence-transformers pipelines under the hood.
    If your Task pipeline supports batching, e.g. Zero-Shot Classification then batch
    inference is supported. In any case, you can always create your own [inference
    handler](/docs/inference-endpoints/guides/custom_handler) and implement batching.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 答：这取决于任务。[支持的任务](/docs/inference-endpoints/supported_tasks)在幕后使用transformers或sentence-transformers管道。如果您的任务管道支持批处理，例如零样本分类，则支持批量推理。无论如何，您始终可以创建自己的[推理处理程序](/docs/inference-endpoints/guides/custom_handler)并实现批处理。
- en: 'Q: How can I scale my deployment?'
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问：如何扩展我的部署？
- en: 'A: The Endpoints are scaled automatically for you, the only information you
    need to provide is a min replica target and a max replica target. Then the system
    will scale your Endpoint based on the load. Scaling to zero is supported.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 答：端点会自动为您扩展，您需要提供的唯一信息是最小副本目标和最大副本目标。然后系统将根据负载来扩展您的端点。支持缩放到零。
- en: 'Q: Will my endpoint still be running if no more requests are processed?'
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问：如果不再处理请求，我的端点是否仍在运行？
- en: 'A: Yes, your Endpoint will always stay available/up with the number of min
    replicas defined in the Advanced configuration.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 答：是的，您的端点将始终保持可用/上线，与高级配置中定义的最小副本数量相匹配。
- en: 'Q: I would like to deploy a model which is not in the supported tasks, is this
    possible?'
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问：我想部署一个不在支持任务中的模型，这可能吗？
- en: 'A: Yes, you can deploy any repository from the [Hugging Face Hub](https://huggingface.co/models)
    and if your task/model/framework is not supported out of the box, you can [create
    your own inference handler](/docs/inference-endpoints/guides/custom_handler) and
    then deploy your model to an Endpoint.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 答：是的，您可以部署来自[Hugging Face Hub](https://huggingface.co/models)的任何存储库，如果您的任务/模型/框架不受支持，您可以[创建自己的推理处理程序](/docs/inference-endpoints/guides/custom_handler)，然后将您的模型部署到端点。
- en: 'Q: How much does it cost to run my Endpoint?'
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问：运行我的端点需要多少费用？
- en: 'A: The Endpoints are billed based on the compute hours of your Running Endpoints,
    and the associated instance types. We may add usage costs for load balancers and
    Private Links in the future.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 答：端点的计费基于运行端点的计算小时数和相关的实例类型。我们可能会在将来为负载均衡器和私有链接增加使用成本。
- en: 'Q: Is the data transiting to the Endpoint encrypted?'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问：传输到端点的数据是否加密？
- en: 'A: Yes, data is encrypted during transit with TLS/SSL.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 答：是的，数据在传输过程中使用TLS/SSL加密。
- en: 'Q: How can I reduce the latency of my Endpoint?'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问：如何减少我的端点的延迟？
- en: 'A: There are several ways to reduce the latency of your Endpoint. One is to
    deploy your Endpoint in a region close to your application to reduce the network
    overhead. Another is to optimize your model using [Hugging Face Optimum](https://huggingface.co/docs/optimum/index)
    before creating your Endpoint. If you need help or have more questions about reducing
    latency, please contact us.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 答：有几种方法可以减少您的端点的延迟。一种方法是将您的端点部署在靠近您的应用程序的地区，以减少网络开销。另一种方法是在创建端点之前使用[Hugging
    Face Optimum](https://huggingface.co/docs/optimum/index)优化您的模型。如果您需要帮助或有关减少延迟的更多问题，请联系我们。
- en: 'Q: How do I monitor my deployed Endpoint?'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问：如何监视我部署的端点？
- en: 'A: You can currently monitor your Endpoint through the [🤗 Inference Endpoints
    web application](https://ui.endpoints.huggingface.co/endpoints), where you have
    access to the [Logs of your Endpoints](/docs/inference-endpoints/guides/logs)
    as well as a [metrics dashboard](/docs/inference-endpoints/guides/metrics). If
    you need programmatic access or more information, please contact us.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 答：您目前可以通过[🤗推理端点Web应用程序](https://ui.endpoints.huggingface.co/endpoints)监视您的端点，在那里您可以访问您的端点的[日志](/docs/inference-endpoints/guides/logs)以及[指标仪表板](/docs/inference-endpoints/guides/metrics)。如果您需要编程访问或更多信息，请联系我们。
- en: 'Q: What if I would like to deploy to a different instance type that is not
    listed?'
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问：如果我想部署到未列出的不同实例类型怎么办？
- en: 'A: Please contact us if you feel your model would do better on a different
    instance type than what is listed.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 答：如果您觉得您的模型在不同于所列出的实例类型上表现更好，请联系我们。
- en: 'Q: I accidentally leaked my token. Do I need to delete my endpoint?'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问：我不小心泄露了我的令牌。我需要删除我的端点吗？
- en: 'A: You can invalidate existing personal tokens and create new ones in your
    settings here: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens).
    For organization tokens, go to the organization settings.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 答：您可以在此处使现有的个人令牌无效并创建新的令牌：[https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)。对于组织令牌，请转到组织设置。
