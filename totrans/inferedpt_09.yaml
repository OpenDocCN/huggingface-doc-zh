- en: FAQs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¸¸è§é—®é¢˜è§£ç­”
- en: 'Original text: [https://huggingface.co/docs/inference-endpoints/faq](https://huggingface.co/docs/inference-endpoints/faq)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/inference-endpoints/faq](https://huggingface.co/docs/inference-endpoints/faq)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: In which regions are Inference Endpoints available?'
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é—®ï¼šæ¨ç†ç«¯ç‚¹åœ¨å“ªäº›åœ°åŒºå¯ç”¨ï¼Ÿ
- en: 'A: Inference Endpoints are currently available on AWS in us-east-1 (N. Virginia)
    & eu-west-1 (Ireland) and on Azure in eastus (Virginia). If you need to deploy
    in a different region, please let us know.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”ï¼šæ¨ç†ç«¯ç‚¹ç›®å‰åœ¨AWSçš„us-east-1ï¼ˆåŒ—å¼—å‰å°¼äºšï¼‰å’Œeu-west-1ï¼ˆçˆ±å°”å…°ï¼‰ä»¥åŠAzureçš„eastusï¼ˆå¼—å‰å°¼äºšï¼‰ä¸Šå¯ç”¨ã€‚å¦‚æœæ‚¨éœ€è¦åœ¨å…¶ä»–åœ°åŒºéƒ¨ç½²ï¼Œè¯·å‘Šè¯‰æˆ‘ä»¬ã€‚
- en: 'Q: Can I access the instance my Endpoint is running on?'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é—®ï¼šæˆ‘å¯ä»¥è®¿é—®æˆ‘çš„ç«¯ç‚¹æ­£åœ¨è¿è¡Œçš„å®ä¾‹å—ï¼Ÿ
- en: 'A: No, you cannot access the instance hosting your Endpoint. But if you are
    missing information or need more insights on the machine where the Endpoint is
    running, please contact us.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”ï¼šä¸ï¼Œæ‚¨æ— æ³•è®¿é—®æ‰˜ç®¡æ‚¨çš„ç«¯ç‚¹çš„å®ä¾‹ã€‚ä½†æ˜¯ï¼Œå¦‚æœæ‚¨ç¼ºå°‘ä¿¡æ¯æˆ–éœ€è¦æœ‰å…³è¿è¡Œç«¯ç‚¹çš„æœºå™¨çš„æ›´å¤šè§è§£ï¼Œè¯·è”ç³»æˆ‘ä»¬ã€‚
- en: 'Q: Can I see my Private Endpoint running on my VPC account?'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é—®ï¼šæˆ‘èƒ½çœ‹åˆ°æˆ‘çš„ç§æœ‰ç«¯ç‚¹åœ¨æˆ‘çš„VPCå¸æˆ·ä¸Šè¿è¡Œå—ï¼Ÿ
- en: 'A: No, when creating a Private Endpoint (a Hugging Face Inference Endpoint
    linked to your VPC via AWS/Azure PrivateLink), you can only see the ENI in your
    VPC where the Endpoint is available.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”ï¼šä¸ï¼Œå½“åˆ›å»ºç§æœ‰ç«¯ç‚¹ï¼ˆä¸€ä¸ªé€šè¿‡AWS/Azure PrivateLinkä¸æ‚¨çš„VPCè¿æ¥çš„Hugging Faceæ¨ç†ç«¯ç‚¹ï¼‰æ—¶ï¼Œæ‚¨åªèƒ½çœ‹åˆ°ç«¯ç‚¹å¯ç”¨çš„VPCä¸­çš„ENIã€‚
- en: 'Q: Can I run inference in batches?'
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é—®ï¼šæˆ‘å¯ä»¥æ‰¹é‡è¿è¡Œæ¨ç†å—ï¼Ÿ
- en: 'A: It depends on the Task. The [supported Tasks](/docs/inference-endpoints/supported_tasks)
    are using the transformers or sentence-transformers pipelines under the hood.
    If your Task pipeline supports batching, e.g. Zero-Shot Classification then batch
    inference is supported. In any case, you can always create your own [inference
    handler](/docs/inference-endpoints/guides/custom_handler) and implement batching.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”ï¼šè¿™å–å†³äºä»»åŠ¡ã€‚[æ”¯æŒçš„ä»»åŠ¡](/docs/inference-endpoints/supported_tasks)åœ¨å¹•åä½¿ç”¨transformersæˆ–sentence-transformersç®¡é“ã€‚å¦‚æœæ‚¨çš„ä»»åŠ¡ç®¡é“æ”¯æŒæ‰¹å¤„ç†ï¼Œä¾‹å¦‚é›¶æ ·æœ¬åˆ†ç±»ï¼Œåˆ™æ”¯æŒæ‰¹é‡æ¨ç†ã€‚æ— è®ºå¦‚ä½•ï¼Œæ‚¨å§‹ç»ˆå¯ä»¥åˆ›å»ºè‡ªå·±çš„[æ¨ç†å¤„ç†ç¨‹åº](/docs/inference-endpoints/guides/custom_handler)å¹¶å®ç°æ‰¹å¤„ç†ã€‚
- en: 'Q: How can I scale my deployment?'
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é—®ï¼šå¦‚ä½•æ‰©å±•æˆ‘çš„éƒ¨ç½²ï¼Ÿ
- en: 'A: The Endpoints are scaled automatically for you, the only information you
    need to provide is a min replica target and a max replica target. Then the system
    will scale your Endpoint based on the load. Scaling to zero is supported.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”ï¼šç«¯ç‚¹ä¼šè‡ªåŠ¨ä¸ºæ‚¨æ‰©å±•ï¼Œæ‚¨éœ€è¦æä¾›çš„å”¯ä¸€ä¿¡æ¯æ˜¯æœ€å°å‰¯æœ¬ç›®æ ‡å’Œæœ€å¤§å‰¯æœ¬ç›®æ ‡ã€‚ç„¶åç³»ç»Ÿå°†æ ¹æ®è´Ÿè½½æ¥æ‰©å±•æ‚¨çš„ç«¯ç‚¹ã€‚æ”¯æŒç¼©æ”¾åˆ°é›¶ã€‚
- en: 'Q: Will my endpoint still be running if no more requests are processed?'
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é—®ï¼šå¦‚æœä¸å†å¤„ç†è¯·æ±‚ï¼Œæˆ‘çš„ç«¯ç‚¹æ˜¯å¦ä»åœ¨è¿è¡Œï¼Ÿ
- en: 'A: Yes, your Endpoint will always stay available/up with the number of min
    replicas defined in the Advanced configuration.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”ï¼šæ˜¯çš„ï¼Œæ‚¨çš„ç«¯ç‚¹å°†å§‹ç»ˆä¿æŒå¯ç”¨/ä¸Šçº¿ï¼Œä¸é«˜çº§é…ç½®ä¸­å®šä¹‰çš„æœ€å°å‰¯æœ¬æ•°é‡ç›¸åŒ¹é…ã€‚
- en: 'Q: I would like to deploy a model which is not in the supported tasks, is this
    possible?'
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é—®ï¼šæˆ‘æƒ³éƒ¨ç½²ä¸€ä¸ªä¸åœ¨æ”¯æŒä»»åŠ¡ä¸­çš„æ¨¡å‹ï¼Œè¿™å¯èƒ½å—ï¼Ÿ
- en: 'A: Yes, you can deploy any repository from the [Hugging Face Hub](https://huggingface.co/models)
    and if your task/model/framework is not supported out of the box, you can [create
    your own inference handler](/docs/inference-endpoints/guides/custom_handler) and
    then deploy your model to an Endpoint.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”ï¼šæ˜¯çš„ï¼Œæ‚¨å¯ä»¥éƒ¨ç½²æ¥è‡ª[Hugging Face Hub](https://huggingface.co/models)çš„ä»»ä½•å­˜å‚¨åº“ï¼Œå¦‚æœæ‚¨çš„ä»»åŠ¡/æ¨¡å‹/æ¡†æ¶ä¸å—æ”¯æŒï¼Œæ‚¨å¯ä»¥[åˆ›å»ºè‡ªå·±çš„æ¨ç†å¤„ç†ç¨‹åº](/docs/inference-endpoints/guides/custom_handler)ï¼Œç„¶åå°†æ‚¨çš„æ¨¡å‹éƒ¨ç½²åˆ°ç«¯ç‚¹ã€‚
- en: 'Q: How much does it cost to run my Endpoint?'
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é—®ï¼šè¿è¡Œæˆ‘çš„ç«¯ç‚¹éœ€è¦å¤šå°‘è´¹ç”¨ï¼Ÿ
- en: 'A: The Endpoints are billed based on the compute hours of your Running Endpoints,
    and the associated instance types. We may add usage costs for load balancers and
    Private Links in the future.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”ï¼šç«¯ç‚¹çš„è®¡è´¹åŸºäºè¿è¡Œç«¯ç‚¹çš„è®¡ç®—å°æ—¶æ•°å’Œç›¸å…³çš„å®ä¾‹ç±»å‹ã€‚æˆ‘ä»¬å¯èƒ½ä¼šåœ¨å°†æ¥ä¸ºè´Ÿè½½å‡è¡¡å™¨å’Œç§æœ‰é“¾æ¥å¢åŠ ä½¿ç”¨æˆæœ¬ã€‚
- en: 'Q: Is the data transiting to the Endpoint encrypted?'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é—®ï¼šä¼ è¾“åˆ°ç«¯ç‚¹çš„æ•°æ®æ˜¯å¦åŠ å¯†ï¼Ÿ
- en: 'A: Yes, data is encrypted during transit with TLS/SSL.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”ï¼šæ˜¯çš„ï¼Œæ•°æ®åœ¨ä¼ è¾“è¿‡ç¨‹ä¸­ä½¿ç”¨TLS/SSLåŠ å¯†ã€‚
- en: 'Q: How can I reduce the latency of my Endpoint?'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é—®ï¼šå¦‚ä½•å‡å°‘æˆ‘çš„ç«¯ç‚¹çš„å»¶è¿Ÿï¼Ÿ
- en: 'A: There are several ways to reduce the latency of your Endpoint. One is to
    deploy your Endpoint in a region close to your application to reduce the network
    overhead. Another is to optimize your model using [Hugging Face Optimum](https://huggingface.co/docs/optimum/index)
    before creating your Endpoint. If you need help or have more questions about reducing
    latency, please contact us.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”ï¼šæœ‰å‡ ç§æ–¹æ³•å¯ä»¥å‡å°‘æ‚¨çš„ç«¯ç‚¹çš„å»¶è¿Ÿã€‚ä¸€ç§æ–¹æ³•æ˜¯å°†æ‚¨çš„ç«¯ç‚¹éƒ¨ç½²åœ¨é è¿‘æ‚¨çš„åº”ç”¨ç¨‹åºçš„åœ°åŒºï¼Œä»¥å‡å°‘ç½‘ç»œå¼€é”€ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯åœ¨åˆ›å»ºç«¯ç‚¹ä¹‹å‰ä½¿ç”¨[Hugging
    Face Optimum](https://huggingface.co/docs/optimum/index)ä¼˜åŒ–æ‚¨çš„æ¨¡å‹ã€‚å¦‚æœæ‚¨éœ€è¦å¸®åŠ©æˆ–æœ‰å…³å‡å°‘å»¶è¿Ÿçš„æ›´å¤šé—®é¢˜ï¼Œè¯·è”ç³»æˆ‘ä»¬ã€‚
- en: 'Q: How do I monitor my deployed Endpoint?'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é—®ï¼šå¦‚ä½•ç›‘è§†æˆ‘éƒ¨ç½²çš„ç«¯ç‚¹ï¼Ÿ
- en: 'A: You can currently monitor your Endpoint through the [ğŸ¤— Inference Endpoints
    web application](https://ui.endpoints.huggingface.co/endpoints), where you have
    access to the [Logs of your Endpoints](/docs/inference-endpoints/guides/logs)
    as well as a [metrics dashboard](/docs/inference-endpoints/guides/metrics). If
    you need programmatic access or more information, please contact us.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”ï¼šæ‚¨ç›®å‰å¯ä»¥é€šè¿‡[ğŸ¤—æ¨ç†ç«¯ç‚¹Webåº”ç”¨ç¨‹åº](https://ui.endpoints.huggingface.co/endpoints)ç›‘è§†æ‚¨çš„ç«¯ç‚¹ï¼Œåœ¨é‚£é‡Œæ‚¨å¯ä»¥è®¿é—®æ‚¨çš„ç«¯ç‚¹çš„[æ—¥å¿—](/docs/inference-endpoints/guides/logs)ä»¥åŠ[æŒ‡æ ‡ä»ªè¡¨æ¿](/docs/inference-endpoints/guides/metrics)ã€‚å¦‚æœæ‚¨éœ€è¦ç¼–ç¨‹è®¿é—®æˆ–æ›´å¤šä¿¡æ¯ï¼Œè¯·è”ç³»æˆ‘ä»¬ã€‚
- en: 'Q: What if I would like to deploy to a different instance type that is not
    listed?'
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é—®ï¼šå¦‚æœæˆ‘æƒ³éƒ¨ç½²åˆ°æœªåˆ—å‡ºçš„ä¸åŒå®ä¾‹ç±»å‹æ€ä¹ˆåŠï¼Ÿ
- en: 'A: Please contact us if you feel your model would do better on a different
    instance type than what is listed.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”ï¼šå¦‚æœæ‚¨è§‰å¾—æ‚¨çš„æ¨¡å‹åœ¨ä¸åŒäºæ‰€åˆ—å‡ºçš„å®ä¾‹ç±»å‹ä¸Šè¡¨ç°æ›´å¥½ï¼Œè¯·è”ç³»æˆ‘ä»¬ã€‚
- en: 'Q: I accidentally leaked my token. Do I need to delete my endpoint?'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é—®ï¼šæˆ‘ä¸å°å¿ƒæ³„éœ²äº†æˆ‘çš„ä»¤ç‰Œã€‚æˆ‘éœ€è¦åˆ é™¤æˆ‘çš„ç«¯ç‚¹å—ï¼Ÿ
- en: 'A: You can invalidate existing personal tokens and create new ones in your
    settings here: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens).
    For organization tokens, go to the organization settings.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”ï¼šæ‚¨å¯ä»¥åœ¨æ­¤å¤„ä½¿ç°æœ‰çš„ä¸ªäººä»¤ç‰Œæ— æ•ˆå¹¶åˆ›å»ºæ–°çš„ä»¤ç‰Œï¼š[https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)ã€‚å¯¹äºç»„ç»‡ä»¤ç‰Œï¼Œè¯·è½¬åˆ°ç»„ç»‡è®¾ç½®ã€‚
