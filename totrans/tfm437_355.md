# LiLT

> 原文：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/lilt`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/lilt)

## 概述

LiLT 模型在[Jiapeng Wang, Lianwen Jin, Kai Ding 撰写的《LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding》](https://arxiv.org/abs/2202.13669)中提出。LiLT 允许将任何预训练的 RoBERTa 文本编码器与轻量级的 Layout Transformer 结合起来，以实现多种语言的 LayoutLM 类似文档理解。

从论文摘要中得出的结论是：

*结构化文档理解近年来引起了广泛关注并取得了显著进展，这归功于其在智能文档处理中的关键作用。然而，大多数现有的相关模型只能处理特定语言（通常是英语）的文档数据，这些文档数据包含在预训练集合中，这是极其有限的。为了解决这个问题，我们提出了一个简单而有效的 Language-independent Layout Transformer（LiLT）用于结构化文档理解。LiLT 可以在单一语言的结构化文档上进行预训练，然后直接在其他语言上进行对应的现成的单语/多语预训练文本模型的微调。在八种语言上的实验结果表明，LiLT 可以在各种广泛使用的下游基准测试中取得竞争性甚至优越的性能，这使得可以从文档布局结构的预训练中获益而不受语言限制。*

![drawing](img/b7a4714ff512347c68b1404d0d624c5c.png) LiLT 架构。摘自[原始论文](https://arxiv.org/abs/2202.13669)。

此模型由[nielsr](https://huggingface.co/nielsr)贡献。原始代码可以在[此处](https://github.com/jpwang/lilt)找到。

## 使用提示

+   将 Language-Independent Layout Transformer 与来自[hub](https://huggingface.co/models?search=roberta)的新 RoBERTa 检查点结合起来，请参考[此指南](https://github.com/jpWang/LiLT#or-generate-your-own-checkpoint-optional)。脚本将导致`config.json`和`pytorch_model.bin`文件被存储在本地。完成此操作后，可以执行以下操作（假设您已登录您的 HuggingFace 帐户）：

```py
from transformers import LiltModel

model = LiltModel.from_pretrained("path_to_your_files")
model.push_to_hub("name_of_repo_on_the_hub")
```

+   在为模型准备数据时，请确保使用与您与 Layout Transformer 结合的 RoBERTa 检查点相对应的标记词汇表。

+   由于[lilt-roberta-en-base](https://huggingface.co/SCUT-DLVCLab/lilt-roberta-en-base)使用与 LayoutLMv3 相同的词汇表，因此可以使用 LayoutLMv3TokenizerFast 来为模型准备数据。对于[lilt-roberta-en-base](https://huggingface.co/SCUT-DLVCLab/lilt-infoxlm-base)也是如此：可以使用 LayoutXLMTokenizerFast 来为该模型准备数据。

## 资源

官方 Hugging Face 和社区（由🌎表示）资源列表，可帮助您开始使用 LiLT。

+   可以在[此处](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/LiLT)找到 LiLT 的演示笔记本。

**文档资源**

+   文本分类任务指南

+   标记分类任务指南

+   问答任务指南

如果您有兴趣提交资源以包含在此处，请随时打开 Pull Request，我们将对其进行审查！资源应该展示一些新内容，而不是重复现有资源。

## LiltConfig

### `class transformers.LiltConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lilt/configuration_lilt.py#L30)

```py
( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 position_embedding_type = 'absolute' classifier_dropout = None channel_shrink_ratio = 4 max_2d_position_embeddings = 1024 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, 默认为 30522) — LiLT 模型的词汇量。定义了在调用 LiltModel 时可以表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, 默认为 768) — 编码器层和池化器层的维度。应为 24 的倍数。

+   `num_hidden_layers` (`int`, *optional*, 默认为 12) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, 默认为 12) — Transformer 编码器中每个注意力层的注意力头数量。

+   `intermediate_size` (`int`, *optional*, 默认为 3072) — Transformer 编码器中“中间”（通常称为前馈）层的维度。

+   `hidden_act` (`str` 或 `Callable`, *optional*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`, `"relu"`, `"silu"` 和 `"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *optional*, 默认为 0.1) — 嵌入层、编码器和池化器中所有全连接层的 dropout 概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, 默认为 0.1) — 注意力概率的 dropout 比率。

+   `max_position_embeddings` (`int`, *optional*, 默认为 512) — 该模型可能使用的最大序列长度。通常设置为较大的值以防万一（例如 512、1024 或 2048）。

+   `type_vocab_size` (`int`, *optional*, 默认为 2) — 在调用 LiltModel 时传递的`token_type_ids`的词汇量。

+   `initializer_range` (`float`, *optional*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, 默认为 1e-12) — 层归一化层使用的 epsilon。

+   `position_embedding_type` (`str`, *optional*, 默认为`"absolute"`) — 位置嵌入的类型。选择`"absolute"`, `"relative_key"`, `"relative_key_query"`之一。对于位置嵌入，请使用`"absolute"`。有关`"relative_key"`的更多信息，请参阅[Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)。有关`"relative_key_query"`的更多信息，请参阅[Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658)中的*Method 4*。

+   `classifier_dropout` (`float`, *optional*) — 分类头的 dropout 比率。

+   `channel_shrink_ratio` (`int`, *optional*, 默认为 4) — 与布局嵌入的`hidden_size`相比，通道维度的收缩比率。

+   `max_2d_position_embeddings` (`int`, *optional*, 默认为 1024) — 2D 位置嵌入可能使用的最大值。通常设置为较大的值以防万一（例如 1024）。

这是一个配置类，用于存储 LiltModel 的配置。根据指定的参数实例化一个 LiLT 模型，定义模型架构。使用默认值实例化配置将产生与 LiLT [SCUT-DLVCLab/lilt-roberta-en-base](https://huggingface.co/SCUT-DLVCLab/lilt-roberta-en-base)架构类似的配置。配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import LiltConfig, LiltModel

>>> # Initializing a LiLT SCUT-DLVCLab/lilt-roberta-en-base style configuration
>>> configuration = LiltConfig()
>>> # Randomly initializing a model from the SCUT-DLVCLab/lilt-roberta-en-base style configuration
>>> model = LiltModel(configuration)
>>> # Accessing the model configuration
>>> configuration = model.config
```

## LiltModel

### `class transformers.LiltModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lilt/modeling_lilt.py#L677)

```py
( config add_pooling_layer = True )
```

参数

+   `config` (LiltConfig) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

裸 LiLT 模型变压器输出原始隐藏状态，没有特定的头部。这个模型继承自 PreTrainedModel。检查超类文档，了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lilt/modeling_lilt.py#L709)

```py
( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参见 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是 input IDs？

+   `bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*) — 每个输入序列标记的边界框。选择范围为`[0, config.max_2d_position_embeddings-1]`。每个边界框应该是一个规范化版本，格式为(x0, y0, x1, y1)，其中(x0, y0)对应于边界框左上角的位置，(x1, y1)表示右下角的位置。有关规范化，请参见概述。

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示未被`masked`的标记，

    +   0 表示被`masked`的标记。

    什么是 attention masks？

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`之间：

    +   0 对应于*句子 A*标记，

    +   1 对应于*句子 B*标记。

    什么是 token type IDs？

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    什么是 position IDs？

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中的选定头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示头部未被`masked`，

    +   0 表示头部被`masked`。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回 ModelOutput 而不是普通元组。

返回

transformers.modeling_outputs.BaseModelOutputWithPooling 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.BaseModelOutputWithPooling 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`，或者`return_dict=False`时）包含根据配置(LiltConfig)和输入的各种元素。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`) — 序列第一个标记（分类标记）的最后一层隐藏状态，在通过用于辅助预训练任务的层进一步处理后。例如，对于 BERT 系列模型，这返回经过线性层和 tanh 激活函数处理后的分类标记。线性层的权重在预训练期间从下一个句子预测（分类）目标中训练。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有嵌入层，+ 一个用于每个层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

LiltModel 的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModel
>>> from datasets import load_dataset

>>> tokenizer = AutoTokenizer.from_pretrained("SCUT-DLVCLab/lilt-roberta-en-base")
>>> model = AutoModel.from_pretrained("SCUT-DLVCLab/lilt-roberta-en-base")

>>> dataset = load_dataset("nielsr/funsd-layoutlmv3", split="train")
>>> example = dataset[0]
>>> words = example["tokens"]
>>> boxes = example["bboxes"]

>>> encoding = tokenizer(words, boxes=boxes, return_tensors="pt")

>>> outputs = model(**encoding)
>>> last_hidden_states = outputs.last_hidden_state
```

## LiltForSequenceClassification

### `class transformers.LiltForSequenceClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lilt/modeling_lilt.py#L823)

```py
( config )
```

参数

+   `config` (LiltConfig) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

LiLT 模型变压器，顶部带有一个序列分类/回归头（池化输出之上的线性层），例如用于 GLUE 任务。

此模型继承自 PreTrainedModel。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型还是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lilt/modeling_lilt.py#L843)

```py
( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*) — 每个输入序列标记的边界框。选择范围为`[0, config.max_2d_position_embeddings-1]`。每个边界框应该是(x0, y0, x1, y1)格式的归一化版本，其中(x0, y0)对应于边界框左上角的位置，(x1, y1)表示右下角的位置。有关归一化，请参阅概述。

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   对于未被`masked`的标记为 1，

    +   对于被`masked`的标记为 0。

    什么是注意力掩码？

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`之间：

    +   0 对应于*句子 A*标记，

    +   1 对应于*句子 B*标记。

    什么是标记类型 ID？

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    什么是位置 ID？

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示头部未被`masked`，

    +   0 表示头部被`masked`。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size,)`, *可选的*) — 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

transformers.modeling_outputs.SequenceClassifierOutput 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.SequenceClassifierOutput 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，具体取决于配置（LiltConfig）和输入。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`, *可选的*, 当提供`labels`时返回) — 分类（如果 config.num_labels==1 则为回归）损失。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果 config.num_labels==1 则为回归）得分（SoftMax 之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有一个嵌入层，+ 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在自注意力头中用于计算加权平均值的注意力 softmax 之后的注意力权重。

LiltForSequenceClassification 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者负责运行前后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification
>>> from datasets import load_dataset

>>> tokenizer = AutoTokenizer.from_pretrained("SCUT-DLVCLab/lilt-roberta-en-base")
>>> model = AutoModelForSequenceClassification.from_pretrained("SCUT-DLVCLab/lilt-roberta-en-base")

>>> dataset = load_dataset("nielsr/funsd-layoutlmv3", split="train")
>>> example = dataset[0]
>>> words = example["tokens"]
>>> boxes = example["bboxes"]

>>> encoding = tokenizer(words, boxes=boxes, return_tensors="pt")

>>> outputs = model(**encoding)
>>> predicted_class_idx = outputs.logits.argmax(-1).item()
>>> predicted_class = model.config.id2label[predicted_class_idx]
```

## LiltForTokenClassification

### `class transformers.LiltForTokenClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lilt/modeling_lilt.py#L941)

```py
( config )
```

参数

+   `config` (LiltConfig) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

在模型的每一层输出之上有一个标记分类头的 Lilt 模型（在隐藏状态输出之上有一个线性层），例如用于命名实体识别（NER）任务。

这个模型继承自 PreTrainedModel。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lilt/modeling_lilt.py#L964)

```py
( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `bbox` (`torch.LongTensor`，形状为`(batch_size, sequence_length, 4)`，*可选*) — 每个输入序列标记的边界框。在范围`[0, config.max_2d_position_embeddings-1]`中选择。每个边界框应该是(x0, y0, x1, y1)格式的归一化版本，其中(x0, y0)对应于边界框左上角的位置，(x1, y1)表示右下角的位置。有关归一化，请参阅概览。

+   `attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：

    +   对于`未屏蔽`的标记，

    +   对于`被屏蔽`的标记，为 0。

    什么是注意力掩码？

+   `token_type_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 指示输入的第一部分和第二部分的段标记索引。索引在`[0, 1]`中选择：

    +   0 对应于 *句子 A* 标记，

    +   1 对应于 *句子 B* 标记。

    什么是标记类型 ID？

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置 ID？

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*) — 用于使自注意力模块的选定头部失效的掩码。掩码值在`[0, 1]`中选择：

    +   1 表示头部`未屏蔽`，

    +   0 表示头部`被屏蔽`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*) — 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回 ModelOutput 而不是普通元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于计算标记分类损失的标签。索引应在`[0, ..., config.num_labels - 1]`中。

返回

transformers.modeling_outputs.TokenClassifierOutput 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.TokenClassifierOutput 或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包括根据配置（LiltConfig）和输入的不同元素。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 分类损失。

+   `logits`（形状为`(batch_size, sequence_length, config.num_labels)`的`torch.FloatTensor`）— 分类分数（SoftMax 之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出和每一层的输出）。

    模型在每一层输出处的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力权重在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

LiltForTokenClassification 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForTokenClassification
>>> from datasets import load_dataset

>>> tokenizer = AutoTokenizer.from_pretrained("SCUT-DLVCLab/lilt-roberta-en-base")
>>> model = AutoModelForTokenClassification.from_pretrained("SCUT-DLVCLab/lilt-roberta-en-base")

>>> dataset = load_dataset("nielsr/funsd-layoutlmv3", split="train")
>>> example = dataset[0]
>>> words = example["tokens"]
>>> boxes = example["bboxes"]

>>> encoding = tokenizer(words, boxes=boxes, return_tensors="pt")

>>> outputs = model(**encoding)
>>> predicted_class_indices = outputs.logits.argmax(-1)
```

## LiltForQuestionAnswering

### `class transformers.LiltForQuestionAnswering`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lilt/modeling_lilt.py#L1067)

```py
( config )
```

参数

+   `config`（LiltConfig）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

Lilt 模型在顶部具有一个跨度分类头，用于提取式问答任务，如 SQuAD（在隐藏状态输出的顶部有线性层，用于计算`span start logits`和`span end logits`）。

这个模型继承自 PreTrainedModel。检查超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lilt/modeling_lilt.py#L1086)

```py
( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.QuestionAnsweringModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。 

    什么是输入 ID？

+   `bbox` (`torch.LongTensor`，形状为`(batch_size, sequence_length, 4)`，*optional*) — 每个输入序列标记的边界框。选择范围为`[0, config.max_2d_position_embeddings-1]`。每个边界框应该是(x0, y0, x1, y1)格式的归一化版本，其中(x0, y0)对应于边界框左上角的位置，(x1, y1)表示右下角的位置。有关归一化，请参阅概述。

+   `attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示未被`masked`的标记，

    +   0 表示被`masked`的标记。

    什么是注意力掩码？

+   `token_type_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`之间：

    +   0 对应于*句子 A*标记，

    +   1 对应于*句子 B*标记。

    什么是标记类型 ID？

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    什么是位置 ID？

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示头部未被`masked`，

    +   0 表示头部被`masked`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`，*optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`，*optional*) — 是否返回 ModelOutput 而不是普通元组。

+   `start_positions` (`torch.LongTensor`，形状为`(batch_size,)`，*optional*) — 用于计算标记分类损失的标记跨度开始位置（索引）的标签。位置被夹紧到序列的长度(`sequence_length`)。序列外的位置不会用于计算损失。

+   `end_positions` (`torch.LongTensor`，形状为`(batch_size,)`，*optional*) — 用于计算标记分类损失的标记跨度结束位置（索引）的标签。位置被夹紧到序列的长度(`sequence_length`)。序列外的位置不会用于计算损失。

返回

transformers.modeling_outputs.QuestionAnsweringModelOutput 或 `tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.QuestionAnsweringModelOutput 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包括根据配置（LiltConfig）和输入的不同元素。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）- 总跨度提取损失是起始和结束位置的交叉熵之和。

+   `start_logits`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）- 跨度起始得分（SoftMax 之前）。

+   `end_logits`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）- 跨度结束得分（SoftMax 之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+ 一个用于每个层的输出）。

    模型在每一层输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

LiltForQuestionAnswering 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForQuestionAnswering
>>> from datasets import load_dataset

>>> tokenizer = AutoTokenizer.from_pretrained("SCUT-DLVCLab/lilt-roberta-en-base")
>>> model = AutoModelForQuestionAnswering.from_pretrained("SCUT-DLVCLab/lilt-roberta-en-base")

>>> dataset = load_dataset("nielsr/funsd-layoutlmv3", split="train")
>>> example = dataset[0]
>>> words = example["tokens"]
>>> boxes = example["bboxes"]

>>> encoding = tokenizer(words, boxes=boxes, return_tensors="pt")

>>> outputs = model(**encoding)

>>> answer_start_index = outputs.start_logits.argmax()
>>> answer_end_index = outputs.end_logits.argmax()

>>> predict_answer_tokens = encoding.input_ids[0, answer_start_index : answer_end_index + 1]
>>> predicted_answer = tokenizer.decode(predict_answer_tokens)
```
