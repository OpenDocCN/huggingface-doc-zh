# 图像分割

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/semantic_segmentation](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/semantic_segmentation)

[https://www.youtube-nocookie.com/embed/dKE8SIt9C-w](https://www.youtube-nocookie.com/embed/dKE8SIt9C-w)

图像分割模型将图像中对应不同感兴趣区域的区域分开。这些模型通过为每个像素分配一个标签来工作。有几种类型的分割：语义分割、实例分割和全景分割。

在本指南中，我们将：

1.  [查看不同类型的分割](#types-of-segmentation)。

1.  [有一个用于语义分割的端到端微调示例](#fine-tuning-a-model-for-segmentation)。

在开始之前，请确保已安装所有必要的库：

```py
pip install -q datasets transformers evaluate
```

我们鼓励您登录您的Hugging Face帐户，这样您就可以上传和与社区分享您的模型。在提示时，输入您的令牌以登录：

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## 分割类型

语义分割为图像中的每个像素分配一个标签或类。让我们看一下语义分割模型的输出。它将为图像中遇到的每个对象实例分配相同的类，例如，所有猫都将被标记为“cat”而不是“cat-1”、“cat-2”。我们可以使用transformers的图像分割管道快速推断一个语义分割模型。让我们看一下示例图像。

```py
from transformers import pipeline
from PIL import Image
import requests

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation_input.jpg"
image = Image.open(requests.get(url, stream=True).raw)
image
```

![分割输入](../Images/64569586219c10ac0e1d0e7dcb95de24.png)

我们将使用[nvidia/segformer-b1-finetuned-cityscapes-1024-1024](https://huggingface.co/nvidia/segformer-b1-finetuned-cityscapes-1024-1024)。

```py
semantic_segmentation = pipeline("image-segmentation", "nvidia/segformer-b1-finetuned-cityscapes-1024-1024")
results = semantic_segmentation(image)
results
```

分割管道输出包括每个预测类的掩码。

```py
[{'score': None,
  'label': 'road',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'sidewalk',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'building',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'wall',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'pole',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'traffic sign',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'vegetation',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'terrain',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'sky',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>}]
```

查看汽车类的掩码，我们可以看到每辆汽车都被分类为相同的掩码。

```py
results[-1]["mask"]
```

![语义分割输出](../Images/b6bfa966a8f6ae7291a1b6e1dd4d6e5a.png)

在实例分割中，目标不是对每个像素进行分类，而是为给定图像中的**每个对象实例**预测一个掩码。它的工作方式与目标检测非常相似，其中每个实例都有一个边界框，而这里有一个分割掩码。我们将使用[facebook/mask2former-swin-large-cityscapes-instance](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-instance)。

```py
instance_segmentation = pipeline("image-segmentation", "facebook/mask2former-swin-large-cityscapes-instance")
results = instance_segmentation(Image.open(image))
results
```

如下所示，有多辆汽车被分类，除了属于汽车和人实例的像素之外，没有对其他像素进行分类。

```py
[{'score': 0.999944,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999945,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999652,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.903529,
  'label': 'person',
  'mask': <PIL.Image.Image image mode=L size=612x415>}]
```

查看下面的一辆汽车掩码。

```py
results[2]["mask"]
```

![语义分割输出](../Images/b99b68749dae50fc56726918431cadc3.png)

全景分割结合了语义分割和实例分割，其中每个像素被分类为一个类和该类的一个实例，并且每个类的每个实例有多个掩码。我们可以使用[facebook/mask2former-swin-large-cityscapes-panoptic](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-panoptic)。

```py
panoptic_segmentation = pipeline("image-segmentation", "facebook/mask2former-swin-large-cityscapes-panoptic")
results = panoptic_segmentation(Image.open(image))
results
```

如下所示，我们有更多的类。稍后我们将说明，每个像素都被分类为其中的一个类。

```py
[{'score': 0.999981,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999958,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.99997,
  'label': 'vegetation',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999575,
  'label': 'pole',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999958,
  'label': 'building',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999634,
  'label': 'road',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.996092,
  'label': 'sidewalk',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999221,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.99987,
  'label': 'sky',
  'mask': <PIL.Image.Image image mode=L size=612x415>}]
```

让我们对所有类型的分割进行一次并排比较。

![分割地图比较](../Images/3c09fa36de09454c16c19c1b23a22865.png)

看到所有类型的分割，让我们深入研究为语义分割微调模型。

语义分割的常见实际应用包括训练自动驾驶汽车识别行人和重要的交通信息，识别医学图像中的细胞和异常，以及监测卫星图像中的环境变化。

## 为分割微调模型

我们现在将：

1.  在[SceneParse150](https://huggingface.co/datasets/scene_parse_150)数据集上对[SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer)进行微调。

1.  使用您微调的模型进行推断。

本教程中演示的任务由以下模型架构支持：

[BEiT](../model_doc/beit), [Data2VecVision](../model_doc/data2vec-vision), [DPT](../model_doc/dpt), [MobileNetV2](../model_doc/mobilenet_v2), [MobileViT](../model_doc/mobilevit), [MobileViTV2](../model_doc/mobilevitv2), [SegFormer](../model_doc/segformer), [UPerNet](../model_doc/upernet)

### 加载 SceneParse150 数据集

首先从 🤗 数据集库中加载 SceneParse150 数据集的一个较小子集。这将让您有机会进行实验，并确保一切正常，然后再花更多时间在完整数据集上进行训练。

```py
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
```

使用 [train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split) 方法将数据集的 `train` 分割为训练集和测试集：

```py
>>> ds = ds.train_test_split(test_size=0.2)
>>> train_ds = ds["train"]
>>> test_ds = ds["test"]
```

然后看一个例子：

```py
>>> train_ds[0]
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,
 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,
 'scene_category': 368}
```

+   `image`：场景的PIL图像。

+   `annotation`：分割地图的 PIL 图像，也是模型的目标。

+   `scene_category`：描述图像场景的类别 id，如“厨房”或“办公室”。在本指南中，您只需要 `image` 和 `annotation`，两者都是 PIL 图像。

您还需要创建一个将标签 id 映射到标签类的字典，这在稍后设置模型时会很有用。从 Hub 下载映射并创建 `id2label` 和 `label2id` 字典：

```py
>>> import json
>>> from huggingface_hub import cached_download, hf_hub_url

>>> repo_id = "huggingface/label-files"
>>> filename = "ade20k-id2label.json"
>>> id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename, repo_type="dataset")), "r"))
>>> id2label = {int(k): v for k, v in id2label.items()}
>>> label2id = {v: k for k, v in id2label.items()}
>>> num_labels = len(id2label)
```

#### 自定义数据集

如果您更喜欢使用 [run_semantic_segmentation.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/run_semantic_segmentation.py) 脚本而不是笔记本实例进行训练，您也可以创建并使用自己的数据集。该脚本需要：

1.  一个包含两个 [Image](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Image) 列“image”和“label”的 [DatasetDict](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.DatasetDict)。

    ```py
    from datasets import Dataset, DatasetDict, Image

    image_paths_train = ["path/to/image_1.jpg/jpg", "path/to/image_2.jpg/jpg", ..., "path/to/image_n.jpg/jpg"]
    label_paths_train = ["path/to/annotation_1.png", "path/to/annotation_2.png", ..., "path/to/annotation_n.png"]

    image_paths_validation = [...]
    label_paths_validation = [...]

    def create_dataset(image_paths, label_paths):
        dataset = Dataset.from_dict({"image": sorted(image_paths),
                                    "label": sorted(label_paths)})
        dataset = dataset.cast_column("image", Image())
        dataset = dataset.cast_column("label", Image())
        return dataset

    # step 1: create Dataset objects
    train_dataset = create_dataset(image_paths_train, label_paths_train)
    validation_dataset = create_dataset(image_paths_validation, label_paths_validation)

    # step 2: create DatasetDict
    dataset = DatasetDict({
         "train": train_dataset,
         "validation": validation_dataset,
         }
    )

    # step 3: push to Hub (assumes you have ran the huggingface-cli login command in a terminal/notebook)
    dataset.push_to_hub("your-name/dataset-repo")

    # optionally, you can push to a private repo on the Hub
    # dataset.push_to_hub("name of repo on the hub", private=True)
    ```

1.  一个 id2label 字典，将类整数映射到它们的类名

    ```py
    import json
    # simple example
    id2label = {0: 'cat', 1: 'dog'}
    with open('id2label.json', 'w') as fp:
    json.dump(id2label, fp)
    ```

例如，查看这个[示例数据集](https://huggingface.co/datasets/nielsr/ade20k-demo)，该数据集是使用上述步骤创建的。

### 预处理

下一步是加载一个 SegFormer 图像处理器，准备图像和注释以供模型使用。某些数据集，如此类数据集，使用零索引作为背景类。但是，背景类实际上不包括在 150 个类中，因此您需要设置 `reduce_labels=True`，从所有标签中减去一个。零索引被替换为 `255`，因此 SegFormer 的损失函数会忽略它： 

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = "nvidia/mit-b0"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, reduce_labels=True)
```

Pytorch隐藏 Pytorch 内容

通常会对图像数据集应用一些数据增强，以使模型更具抗过拟合能力。在本指南中，您将使用 [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html) 函数从 [torchvision](https://pytorch.org/vision/stable/index.html) 随机更改图像的颜色属性，但您也可以使用任何您喜欢的图像库。

```py
>>> from torchvision.transforms import ColorJitter

>>> jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)
```

现在创建两个预处理函数，准备图像和注释以供模型使用。这些函数将图像转换为 `pixel_values`，将注释转换为 `labels`。对于训练集，在将图像提供给图像处理器之前应用 `jitter`。对于测试集，图像处理器裁剪和规范化 `images`，仅裁剪 `labels`，因为在测试期间不应用数据增强。

```py
>>> def train_transforms(example_batch):
...     images = [jitter(x) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs

>>> def val_transforms(example_batch):
...     images = [x for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

要在整个数据集上应用 `jitter`，请使用 🤗 数据集 [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform) 函数。变换是实时应用的，速度更快，占用的磁盘空间更少：

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```

TensorFlow隐藏 TensorFlow 内容

对图像数据集应用一些数据增强是常见的，可以使模型更具抗过拟合能力。在本指南中，您将使用[`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image)来随机更改图像的颜色属性，但您也可以使用任何您喜欢的图像库。定义两个单独的转换函数：

+   包括图像增强的训练数据转换

+   验证数据转换仅转置图像，因为🤗 Transformers中的计算机视觉模型期望通道优先布局

```py
>>> import tensorflow as tf

>>> def aug_transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.image.random_brightness(image, 0.25)
...     image = tf.image.random_contrast(image, 0.5, 2.0)
...     image = tf.image.random_saturation(image, 0.75, 1.25)
...     image = tf.image.random_hue(image, 0.1)
...     image = tf.transpose(image, (2, 0, 1))
...     return image

>>> def transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.transpose(image, (2, 0, 1))
...     return image
```

接下来，创建两个预处理函数，用于为模型准备图像和注释的批处理。这些函数应用图像转换，并使用之前加载的`image_processor`将图像转换为`pixel_values`，将注释转换为`labels`。`ImageProcessor`还负责调整大小和规范化图像。

```py
>>> def train_transforms(example_batch):
...     images = [aug_transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs

>>> def val_transforms(example_batch):
...     images = [transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

要在整个数据集上应用预处理转换，使用🤗 Datasets [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)函数。转换是实时应用的，速度更快，占用的磁盘空间更少：

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```

### 评估

在训练过程中包含一个度量标准通常有助于评估模型的性能。您可以使用🤗 [Evaluate](https://huggingface.co/docs/evaluate/index)库快速加载一个评估方法。对于这个任务，加载[mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy) (IoU)度量标准（查看🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)以了解如何加载和计算度量标准）：

```py
>>> import evaluate

>>> metric = evaluate.load("mean_iou")
```

然后创建一个函数来`compute`度量标准。您的预测需要首先转换为logits，然后重新调整形状以匹配标签的大小，然后才能调用`compute`：

PytorchHide Pytorch内容

```py
>>> import numpy as np
>>> import torch
>>> from torch import nn

>>> def compute_metrics(eval_pred):
...     with torch.no_grad():
...         logits, labels = eval_pred
...         logits_tensor = torch.from_numpy(logits)
...         logits_tensor = nn.functional.interpolate(
...             logits_tensor,
...             size=labels.shape[-2:],
...             mode="bilinear",
...             align_corners=False,
...         ).argmax(dim=1)

...         pred_labels = logits_tensor.detach().cpu().numpy()
...         metrics = metric.compute(
...             predictions=pred_labels,
...             references=labels,
...             num_labels=num_labels,
...             ignore_index=255,
...             reduce_labels=False,
...         )
...         for key, value in metrics.items():
...             if isinstance(value, np.ndarray):
...                 metrics[key] = value.tolist()
...         return metrics
```

TensorFlowHide TensorFlow内容

```py
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])
...     logits_resized = tf.image.resize(
...         logits,
...         size=tf.shape(labels)[1:],
...         method="bilinear",
...     )

...     pred_labels = tf.argmax(logits_resized, axis=-1)
...     metrics = metric.compute(
...         predictions=pred_labels,
...         references=labels,
...         num_labels=num_labels,
...         ignore_index=-1,
...         reduce_labels=image_processor.do_reduce_labels,
...     )

...     per_category_accuracy = metrics.pop("per_category_accuracy").tolist()
...     per_category_iou = metrics.pop("per_category_iou").tolist()

...     metrics.update({f"accuracy_{id2label[i]}": v for i, v in enumerate(per_category_accuracy)})
...     metrics.update({f"iou_{id2label[i]}": v for i, v in enumerate(per_category_iou)})
...     return {"val_" + k: v for k, v in metrics.items()}
```

您的`compute_metrics`函数现在已经准备就绪，当您设置训练时会再次用到它。

### 训练

PytorchHide Pytorch内容

如果您不熟悉如何使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)对模型进行微调，请查看这里的基本教程[../training#finetune-with-trainer]！

您现在已经准备好开始训练您的模型了！使用[AutoModelForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSemanticSegmentation)加载SegFormer，并将模型传递给标签id和标签类之间的映射：

```py
>>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer

>>> model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)
```

目前只剩下三个步骤：

1.  在[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)中定义您的训练超参数。重要的是不要删除未使用的列，因为这会删除`image`列。没有`image`列，您就无法创建`pixel_values`。设置`remove_unused_columns=False`以防止这种行为！另一个必需的参数是`output_dir`，指定保存模型的位置。通过设置`push_to_hub=True`将此模型推送到Hub（您需要登录Hugging Face才能上传您的模型）。在每个epoch结束时，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)将评估IoU度量标准并保存训练检查点。

1.  将训练参数传递给[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)，同时还需要传递模型、数据集、分词器、数据整理器和`compute_metrics`函数。

1.  调用[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)来微调您的模型。

```py
>>> training_args = TrainingArguments(
...     output_dir="segformer-b0-scene-parse-150",
...     learning_rate=6e-5,
...     num_train_epochs=50,
...     per_device_train_batch_size=2,
...     per_device_eval_batch_size=2,
...     save_total_limit=3,
...     evaluation_strategy="steps",
...     save_strategy="steps",
...     save_steps=20,
...     eval_steps=20,
...     logging_steps=1,
...     eval_accumulation_steps=5,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=train_ds,
...     eval_dataset=test_ds,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

训练完成后，使用[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)方法将您的模型共享到Hub，这样每个人都可以使用您的模型：

```py
>>> trainer.push_to_hub()
```

TensorFlowHide TensorFlow内容

如果您不熟悉使用Keras进行模型微调，请先查看[基本教程](./training#train-a-tensorflow-model-with-keras)！

要在TensorFlow中微调模型，请按照以下步骤进行：

1.  定义训练超参数，并设置优化器和学习率调度。

1.  实例化一个预训练模型。

1.  将一个🤗数据集转换为`tf.data.Dataset`。

1.  编译您的模型。

1.  添加回调以计算指标并将您的模型上传到🤗 Hub

1.  使用`fit()`方法运行训练。

首先定义超参数、优化器和学习率调度：

```py
>>> from transformers import create_optimizer

>>> batch_size = 2
>>> num_epochs = 50
>>> num_train_steps = len(train_ds) * num_epochs
>>> learning_rate = 6e-5
>>> weight_decay_rate = 0.01

>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=learning_rate,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=weight_decay_rate,
...     num_warmup_steps=0,
... )
```

然后，使用[TFAutoModelForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForSemanticSegmentation)加载SegFormer以及标签映射，并使用优化器对其进行编译。请注意，Transformers模型都有一个默认的与任务相关的损失函数，因此除非您想要指定一个，否则不需要指定：

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
... )
>>> model.compile(optimizer=optimizer)  # No loss argument!
```

使用[to_tf_dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)和[DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator)将您的数据集转换为`tf.data.Dataset`格式：

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")

>>> tf_train_dataset = train_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )

>>> tf_eval_dataset = test_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )
```

要从预测中计算准确率并将您的模型推送到🤗 Hub，请使用[Keras回调](../main_classes/keras_callbacks)。将您的`compute_metrics`函数传递给[KerasMetricCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.KerasMetricCallback)，并使用[PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback)来上传模型：

```py
>>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback

>>> metric_callback = KerasMetricCallback(
...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=["labels"]
... )

>>> push_to_hub_callback = PushToHubCallback(output_dir="scene_segmentation", tokenizer=image_processor)

>>> callbacks = [metric_callback, push_to_hub_callback]
```

最后，您已经准备好训练您的模型了！使用您的训练和验证数据集、时代数量和回调来调用`fit()`来微调模型：

```py
>>> model.fit(
...     tf_train_dataset,
...     validation_data=tf_eval_dataset,
...     callbacks=callbacks,
...     epochs=num_epochs,
... )
```

恭喜！您已经对模型进行了微调并在🤗 Hub上分享了它。现在您可以用它进行推理！

### 推理

很好，现在您已经对模型进行了微调，可以用它进行推理！

加载一张图片进行推理：

```py
>>> image = ds[0]["image"]
>>> image
```

![卧室图像](../Images/1f1abe71d12160da7bd59d35ef05323c.png)Pytorch隐藏 Pytorch内容

现在我们将看到如何在没有管道的情况下进行推理。使用图像处理器处理图像，并将`pixel_values`放在GPU上：

```py
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # use GPU if available, otherwise use a CPU
>>> encoding = image_processor(image, return_tensors="pt")
>>> pixel_values = encoding.pixel_values.to(device)
```

将输入传递给模型并返回`logits`：

```py
>>> outputs = model(pixel_values=pixel_values)
>>> logits = outputs.logits.cpu()
```

接下来，将logits重新缩放到原始图像大小：

```py
>>> upsampled_logits = nn.functional.interpolate(
...     logits,
...     size=image.size[::-1],
...     mode="bilinear",
...     align_corners=False,
... )

>>> pred_seg = upsampled_logits.argmax(dim=1)[0]
```

TensorFlow隐藏 TensorFlow内容

加载一个图像处理器来预处理图像并将输入返回为TensorFlow张量：

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("MariaK/scene_segmentation")
>>> inputs = image_processor(image, return_tensors="tf")
```

将输入传递给模型并返回`logits`：

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained("MariaK/scene_segmentation")
>>> logits = model(**inputs).logits
```

接下来，将logits重新缩放到原始图像大小，并在类维度上应用argmax：

```py
>>> logits = tf.transpose(logits, [0, 2, 3, 1])

>>> upsampled_logits = tf.image.resize(
...     logits,
...     # We reverse the shape of `image` because `image.size` returns width and height.
...     image.size[::-1],
... )

>>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]
```

要可视化结果，加载[数据集颜色调色板](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51)作为`ade_palette()`，将每个类映射到它们的RGB值。然后您可以组合并绘制您的图像和预测的分割地图：

```py
>>> import matplotlib.pyplot as plt
>>> import numpy as np

>>> color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)
>>> palette = np.array(ade_palette())
>>> for label, color in enumerate(palette):
...     color_seg[pred_seg == label, :] = color
>>> color_seg = color_seg[..., ::-1]  # convert to BGR

>>> img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map
>>> img = img.astype(np.uint8)

>>> plt.figure(figsize=(15, 10))
>>> plt.imshow(img)
>>> plt.show()
```

![覆盖有分割地图的卧室图像](../Images/2bc12ae66cec2ea434945aaaa1e5c6bf.png)
