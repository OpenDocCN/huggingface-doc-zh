# å›¾åƒåˆ†å‰²

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/semantic_segmentation](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/semantic_segmentation)

[https://www.youtube-nocookie.com/embed/dKE8SIt9C-w](https://www.youtube-nocookie.com/embed/dKE8SIt9C-w)

å›¾åƒåˆ†å‰²æ¨¡å‹å°†å›¾åƒä¸­å¯¹åº”ä¸åŒæ„Ÿå…´è¶£åŒºåŸŸçš„åŒºåŸŸåˆ†å¼€ã€‚è¿™äº›æ¨¡å‹é€šè¿‡ä¸ºæ¯ä¸ªåƒç´ åˆ†é…ä¸€ä¸ªæ ‡ç­¾æ¥å·¥ä½œã€‚æœ‰å‡ ç§ç±»å‹çš„åˆ†å‰²ï¼šè¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²ã€‚

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š

1.  [æŸ¥çœ‹ä¸åŒç±»å‹çš„åˆ†å‰²](#types-of-segmentation)ã€‚

1.  [æœ‰ä¸€ä¸ªç”¨äºè¯­ä¹‰åˆ†å‰²çš„ç«¯åˆ°ç«¯å¾®è°ƒç¤ºä¾‹](#fine-tuning-a-model-for-segmentation)ã€‚

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š

```py
pip install -q datasets transformers evaluate
```

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„Hugging Faceå¸æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å’Œä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åˆ†å‰²ç±»å‹

è¯­ä¹‰åˆ†å‰²ä¸ºå›¾åƒä¸­çš„æ¯ä¸ªåƒç´ åˆ†é…ä¸€ä¸ªæ ‡ç­¾æˆ–ç±»ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è¯­ä¹‰åˆ†å‰²æ¨¡å‹çš„è¾“å‡ºã€‚å®ƒå°†ä¸ºå›¾åƒä¸­é‡åˆ°çš„æ¯ä¸ªå¯¹è±¡å®ä¾‹åˆ†é…ç›¸åŒçš„ç±»ï¼Œä¾‹å¦‚ï¼Œæ‰€æœ‰çŒ«éƒ½å°†è¢«æ ‡è®°ä¸ºâ€œcatâ€è€Œä¸æ˜¯â€œcat-1â€ã€â€œcat-2â€ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨transformersçš„å›¾åƒåˆ†å‰²ç®¡é“å¿«é€Ÿæ¨æ–­ä¸€ä¸ªè¯­ä¹‰åˆ†å‰²æ¨¡å‹ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ç¤ºä¾‹å›¾åƒã€‚

```py
from transformers import pipeline
from PIL import Image
import requests

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation_input.jpg"
image = Image.open(requests.get(url, stream=True).raw)
image
```

![åˆ†å‰²è¾“å…¥](../Images/64569586219c10ac0e1d0e7dcb95de24.png)

æˆ‘ä»¬å°†ä½¿ç”¨[nvidia/segformer-b1-finetuned-cityscapes-1024-1024](https://huggingface.co/nvidia/segformer-b1-finetuned-cityscapes-1024-1024)ã€‚

```py
semantic_segmentation = pipeline("image-segmentation", "nvidia/segformer-b1-finetuned-cityscapes-1024-1024")
results = semantic_segmentation(image)
results
```

åˆ†å‰²ç®¡é“è¾“å‡ºåŒ…æ‹¬æ¯ä¸ªé¢„æµ‹ç±»çš„æ©ç ã€‚

```py
[{'score': None,
  'label': 'road',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'sidewalk',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'building',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'wall',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'pole',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'traffic sign',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'vegetation',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'terrain',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'sky',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': None,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>}]
```

æŸ¥çœ‹æ±½è½¦ç±»çš„æ©ç ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯è¾†æ±½è½¦éƒ½è¢«åˆ†ç±»ä¸ºç›¸åŒçš„æ©ç ã€‚

```py
results[-1]["mask"]
```

![è¯­ä¹‰åˆ†å‰²è¾“å‡º](../Images/b6bfa966a8f6ae7291a1b6e1dd4d6e5a.png)

åœ¨å®ä¾‹åˆ†å‰²ä¸­ï¼Œç›®æ ‡ä¸æ˜¯å¯¹æ¯ä¸ªåƒç´ è¿›è¡Œåˆ†ç±»ï¼Œè€Œæ˜¯ä¸ºç»™å®šå›¾åƒä¸­çš„**æ¯ä¸ªå¯¹è±¡å®ä¾‹**é¢„æµ‹ä¸€ä¸ªæ©ç ã€‚å®ƒçš„å·¥ä½œæ–¹å¼ä¸ç›®æ ‡æ£€æµ‹éå¸¸ç›¸ä¼¼ï¼Œå…¶ä¸­æ¯ä¸ªå®ä¾‹éƒ½æœ‰ä¸€ä¸ªè¾¹ç•Œæ¡†ï¼Œè€Œè¿™é‡Œæœ‰ä¸€ä¸ªåˆ†å‰²æ©ç ã€‚æˆ‘ä»¬å°†ä½¿ç”¨[facebook/mask2former-swin-large-cityscapes-instance](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-instance)ã€‚

```py
instance_segmentation = pipeline("image-segmentation", "facebook/mask2former-swin-large-cityscapes-instance")
results = instance_segmentation(Image.open(image))
results
```

å¦‚ä¸‹æ‰€ç¤ºï¼Œæœ‰å¤šè¾†æ±½è½¦è¢«åˆ†ç±»ï¼Œé™¤äº†å±äºæ±½è½¦å’Œäººå®ä¾‹çš„åƒç´ ä¹‹å¤–ï¼Œæ²¡æœ‰å¯¹å…¶ä»–åƒç´ è¿›è¡Œåˆ†ç±»ã€‚

```py
[{'score': 0.999944,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999945,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999652,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.903529,
  'label': 'person',
  'mask': <PIL.Image.Image image mode=L size=612x415>}]
```

æŸ¥çœ‹ä¸‹é¢çš„ä¸€è¾†æ±½è½¦æ©ç ã€‚

```py
results[2]["mask"]
```

![è¯­ä¹‰åˆ†å‰²è¾“å‡º](../Images/b99b68749dae50fc56726918431cadc3.png)

å…¨æ™¯åˆ†å‰²ç»“åˆäº†è¯­ä¹‰åˆ†å‰²å’Œå®ä¾‹åˆ†å‰²ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ è¢«åˆ†ç±»ä¸ºä¸€ä¸ªç±»å’Œè¯¥ç±»çš„ä¸€ä¸ªå®ä¾‹ï¼Œå¹¶ä¸”æ¯ä¸ªç±»çš„æ¯ä¸ªå®ä¾‹æœ‰å¤šä¸ªæ©ç ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨[facebook/mask2former-swin-large-cityscapes-panoptic](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-panoptic)ã€‚

```py
panoptic_segmentation = pipeline("image-segmentation", "facebook/mask2former-swin-large-cityscapes-panoptic")
results = panoptic_segmentation(Image.open(image))
results
```

å¦‚ä¸‹æ‰€ç¤ºï¼Œæˆ‘ä»¬æœ‰æ›´å¤šçš„ç±»ã€‚ç¨åæˆ‘ä»¬å°†è¯´æ˜ï¼Œæ¯ä¸ªåƒç´ éƒ½è¢«åˆ†ç±»ä¸ºå…¶ä¸­çš„ä¸€ä¸ªç±»ã€‚

```py
[{'score': 0.999981,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999958,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.99997,
  'label': 'vegetation',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999575,
  'label': 'pole',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999958,
  'label': 'building',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999634,
  'label': 'road',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.996092,
  'label': 'sidewalk',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.999221,
  'label': 'car',
  'mask': <PIL.Image.Image image mode=L size=612x415>},
 {'score': 0.99987,
  'label': 'sky',
  'mask': <PIL.Image.Image image mode=L size=612x415>}]
```

è®©æˆ‘ä»¬å¯¹æ‰€æœ‰ç±»å‹çš„åˆ†å‰²è¿›è¡Œä¸€æ¬¡å¹¶æ’æ¯”è¾ƒã€‚

![åˆ†å‰²åœ°å›¾æ¯”è¾ƒ](../Images/3c09fa36de09454c16c19c1b23a22865.png)

çœ‹åˆ°æ‰€æœ‰ç±»å‹çš„åˆ†å‰²ï¼Œè®©æˆ‘ä»¬æ·±å…¥ç ”ç©¶ä¸ºè¯­ä¹‰åˆ†å‰²å¾®è°ƒæ¨¡å‹ã€‚

è¯­ä¹‰åˆ†å‰²çš„å¸¸è§å®é™…åº”ç”¨åŒ…æ‹¬è®­ç»ƒè‡ªåŠ¨é©¾é©¶æ±½è½¦è¯†åˆ«è¡Œäººå’Œé‡è¦çš„äº¤é€šä¿¡æ¯ï¼Œè¯†åˆ«åŒ»å­¦å›¾åƒä¸­çš„ç»†èƒå’Œå¼‚å¸¸ï¼Œä»¥åŠç›‘æµ‹å«æ˜Ÿå›¾åƒä¸­çš„ç¯å¢ƒå˜åŒ–ã€‚

## ä¸ºåˆ†å‰²å¾®è°ƒæ¨¡å‹

æˆ‘ä»¬ç°åœ¨å°†ï¼š

1.  åœ¨[SceneParse150](https://huggingface.co/datasets/scene_parse_150)æ•°æ®é›†ä¸Šå¯¹[SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer)è¿›è¡Œå¾®è°ƒã€‚

1.  ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚

æœ¬æ•™ç¨‹ä¸­æ¼”ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡å‹æ¶æ„æ”¯æŒï¼š

[BEiT](../model_doc/beit), [Data2VecVision](../model_doc/data2vec-vision), [DPT](../model_doc/dpt), [MobileNetV2](../model_doc/mobilenet_v2), [MobileViT](../model_doc/mobilevit), [MobileViTV2](../model_doc/mobilevitv2), [SegFormer](../model_doc/segformer), [UPerNet](../model_doc/upernet)

### åŠ è½½ SceneParse150 æ•°æ®é›†

é¦–å…ˆä» ğŸ¤— æ•°æ®é›†åº“ä¸­åŠ è½½ SceneParse150 æ•°æ®é›†çš„ä¸€ä¸ªè¾ƒå°å­é›†ã€‚è¿™å°†è®©æ‚¨æœ‰æœºä¼šè¿›è¡Œå®éªŒï¼Œå¹¶ç¡®ä¿ä¸€åˆ‡æ­£å¸¸ï¼Œç„¶åå†èŠ±æ›´å¤šæ—¶é—´åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚

```py
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
```

ä½¿ç”¨ [train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split) æ–¹æ³•å°†æ•°æ®é›†çš„ `train` åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```py
>>> ds = ds.train_test_split(test_size=0.2)
>>> train_ds = ds["train"]
>>> test_ds = ds["test"]
```

ç„¶åçœ‹ä¸€ä¸ªä¾‹å­ï¼š

```py
>>> train_ds[0]
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,
 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,
 'scene_category': 368}
```

+   `image`ï¼šåœºæ™¯çš„PILå›¾åƒã€‚

+   `annotation`ï¼šåˆ†å‰²åœ°å›¾çš„ PIL å›¾åƒï¼Œä¹Ÿæ˜¯æ¨¡å‹çš„ç›®æ ‡ã€‚

+   `scene_category`ï¼šæè¿°å›¾åƒåœºæ™¯çš„ç±»åˆ« idï¼Œå¦‚â€œå¨æˆ¿â€æˆ–â€œåŠå…¬å®¤â€ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨åªéœ€è¦ `image` å’Œ `annotation`ï¼Œä¸¤è€…éƒ½æ˜¯ PIL å›¾åƒã€‚

æ‚¨è¿˜éœ€è¦åˆ›å»ºä¸€ä¸ªå°†æ ‡ç­¾ id æ˜ å°„åˆ°æ ‡ç­¾ç±»çš„å­—å…¸ï¼Œè¿™åœ¨ç¨åè®¾ç½®æ¨¡å‹æ—¶ä¼šå¾ˆæœ‰ç”¨ã€‚ä» Hub ä¸‹è½½æ˜ å°„å¹¶åˆ›å»º `id2label` å’Œ `label2id` å­—å…¸ï¼š

```py
>>> import json
>>> from huggingface_hub import cached_download, hf_hub_url

>>> repo_id = "huggingface/label-files"
>>> filename = "ade20k-id2label.json"
>>> id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename, repo_type="dataset")), "r"))
>>> id2label = {int(k): v for k, v in id2label.items()}
>>> label2id = {v: k for k, v in id2label.items()}
>>> num_labels = len(id2label)
```

#### è‡ªå®šä¹‰æ•°æ®é›†

å¦‚æœæ‚¨æ›´å–œæ¬¢ä½¿ç”¨ [run_semantic_segmentation.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/run_semantic_segmentation.py) è„šæœ¬è€Œä¸æ˜¯ç¬”è®°æœ¬å®ä¾‹è¿›è¡Œè®­ç»ƒï¼Œæ‚¨ä¹Ÿå¯ä»¥åˆ›å»ºå¹¶ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†ã€‚è¯¥è„šæœ¬éœ€è¦ï¼š

1.  ä¸€ä¸ªåŒ…å«ä¸¤ä¸ª [Image](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Image) åˆ—â€œimageâ€å’Œâ€œlabelâ€çš„ [DatasetDict](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.DatasetDict)ã€‚

    ```py
    from datasets import Dataset, DatasetDict, Image

    image_paths_train = ["path/to/image_1.jpg/jpg", "path/to/image_2.jpg/jpg", ..., "path/to/image_n.jpg/jpg"]
    label_paths_train = ["path/to/annotation_1.png", "path/to/annotation_2.png", ..., "path/to/annotation_n.png"]

    image_paths_validation = [...]
    label_paths_validation = [...]

    def create_dataset(image_paths, label_paths):
        dataset = Dataset.from_dict({"image": sorted(image_paths),
                                    "label": sorted(label_paths)})
        dataset = dataset.cast_column("image", Image())
        dataset = dataset.cast_column("label", Image())
        return dataset

    # step 1: create Dataset objects
    train_dataset = create_dataset(image_paths_train, label_paths_train)
    validation_dataset = create_dataset(image_paths_validation, label_paths_validation)

    # step 2: create DatasetDict
    dataset = DatasetDict({
         "train": train_dataset,
         "validation": validation_dataset,
         }
    )

    # step 3: push to Hub (assumes you have ran the huggingface-cli login command in a terminal/notebook)
    dataset.push_to_hub("your-name/dataset-repo")

    # optionally, you can push to a private repo on the Hub
    # dataset.push_to_hub("name of repo on the hub", private=True)
    ```

1.  ä¸€ä¸ª id2label å­—å…¸ï¼Œå°†ç±»æ•´æ•°æ˜ å°„åˆ°å®ƒä»¬çš„ç±»å

    ```py
    import json
    # simple example
    id2label = {0: 'cat', 1: 'dog'}
    with open('id2label.json', 'w') as fp:
    json.dump(id2label, fp)
    ```

ä¾‹å¦‚ï¼ŒæŸ¥çœ‹è¿™ä¸ª[ç¤ºä¾‹æ•°æ®é›†](https://huggingface.co/datasets/nielsr/ade20k-demo)ï¼Œè¯¥æ•°æ®é›†æ˜¯ä½¿ç”¨ä¸Šè¿°æ­¥éª¤åˆ›å»ºçš„ã€‚

### é¢„å¤„ç†

ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ä¸€ä¸ª SegFormer å›¾åƒå¤„ç†å™¨ï¼Œå‡†å¤‡å›¾åƒå’Œæ³¨é‡Šä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚æŸäº›æ•°æ®é›†ï¼Œå¦‚æ­¤ç±»æ•°æ®é›†ï¼Œä½¿ç”¨é›¶ç´¢å¼•ä½œä¸ºèƒŒæ™¯ç±»ã€‚ä½†æ˜¯ï¼ŒèƒŒæ™¯ç±»å®é™…ä¸Šä¸åŒ…æ‹¬åœ¨ 150 ä¸ªç±»ä¸­ï¼Œå› æ­¤æ‚¨éœ€è¦è®¾ç½® `reduce_labels=True`ï¼Œä»æ‰€æœ‰æ ‡ç­¾ä¸­å‡å»ä¸€ä¸ªã€‚é›¶ç´¢å¼•è¢«æ›¿æ¢ä¸º `255`ï¼Œå› æ­¤ SegFormer çš„æŸå¤±å‡½æ•°ä¼šå¿½ç•¥å®ƒï¼š 

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = "nvidia/mit-b0"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, reduce_labels=True)
```

Pytorchéšè— Pytorch å†…å®¹

é€šå¸¸ä¼šå¯¹å›¾åƒæ•°æ®é›†åº”ç”¨ä¸€äº›æ•°æ®å¢å¼ºï¼Œä»¥ä½¿æ¨¡å‹æ›´å…·æŠ—è¿‡æ‹Ÿåˆèƒ½åŠ›ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨ [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html) å‡½æ•°ä» [torchvision](https://pytorch.org/vision/stable/index.html) éšæœºæ›´æ”¹å›¾åƒçš„é¢œè‰²å±æ€§ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ä»»ä½•æ‚¨å–œæ¬¢çš„å›¾åƒåº“ã€‚

```py
>>> from torchvision.transforms import ColorJitter

>>> jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)
```

ç°åœ¨åˆ›å»ºä¸¤ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œå‡†å¤‡å›¾åƒå’Œæ³¨é‡Šä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚è¿™äº›å‡½æ•°å°†å›¾åƒè½¬æ¢ä¸º `pixel_values`ï¼Œå°†æ³¨é‡Šè½¬æ¢ä¸º `labels`ã€‚å¯¹äºè®­ç»ƒé›†ï¼Œåœ¨å°†å›¾åƒæä¾›ç»™å›¾åƒå¤„ç†å™¨ä¹‹å‰åº”ç”¨ `jitter`ã€‚å¯¹äºæµ‹è¯•é›†ï¼Œå›¾åƒå¤„ç†å™¨è£å‰ªå’Œè§„èŒƒåŒ– `images`ï¼Œä»…è£å‰ª `labels`ï¼Œå› ä¸ºåœ¨æµ‹è¯•æœŸé—´ä¸åº”ç”¨æ•°æ®å¢å¼ºã€‚

```py
>>> def train_transforms(example_batch):
...     images = [jitter(x) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs

>>> def val_transforms(example_batch):
...     images = [x for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨ `jitter`ï¼Œè¯·ä½¿ç”¨ ğŸ¤— æ•°æ®é›† [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform) å‡½æ•°ã€‚å˜æ¢æ˜¯å®æ—¶åº”ç”¨çš„ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œå ç”¨çš„ç£ç›˜ç©ºé—´æ›´å°‘ï¼š

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```

TensorFlowéšè— TensorFlow å†…å®¹

å¯¹å›¾åƒæ•°æ®é›†åº”ç”¨ä¸€äº›æ•°æ®å¢å¼ºæ˜¯å¸¸è§çš„ï¼Œå¯ä»¥ä½¿æ¨¡å‹æ›´å…·æŠ—è¿‡æ‹Ÿåˆèƒ½åŠ›ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨[`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image)æ¥éšæœºæ›´æ”¹å›¾åƒçš„é¢œè‰²å±æ€§ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ä»»ä½•æ‚¨å–œæ¬¢çš„å›¾åƒåº“ã€‚å®šä¹‰ä¸¤ä¸ªå•ç‹¬çš„è½¬æ¢å‡½æ•°ï¼š

+   åŒ…æ‹¬å›¾åƒå¢å¼ºçš„è®­ç»ƒæ•°æ®è½¬æ¢

+   éªŒè¯æ•°æ®è½¬æ¢ä»…è½¬ç½®å›¾åƒï¼Œå› ä¸ºğŸ¤— Transformersä¸­çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹æœŸæœ›é€šé“ä¼˜å…ˆå¸ƒå±€

```py
>>> import tensorflow as tf

>>> def aug_transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.image.random_brightness(image, 0.25)
...     image = tf.image.random_contrast(image, 0.5, 2.0)
...     image = tf.image.random_saturation(image, 0.75, 1.25)
...     image = tf.image.random_hue(image, 0.1)
...     image = tf.transpose(image, (2, 0, 1))
...     return image

>>> def transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.transpose(image, (2, 0, 1))
...     return image
```

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸¤ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œç”¨äºä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒå’Œæ³¨é‡Šçš„æ‰¹å¤„ç†ã€‚è¿™äº›å‡½æ•°åº”ç”¨å›¾åƒè½¬æ¢ï¼Œå¹¶ä½¿ç”¨ä¹‹å‰åŠ è½½çš„`image_processor`å°†å›¾åƒè½¬æ¢ä¸º`pixel_values`ï¼Œå°†æ³¨é‡Šè½¬æ¢ä¸º`labels`ã€‚`ImageProcessor`è¿˜è´Ÿè´£è°ƒæ•´å¤§å°å’Œè§„èŒƒåŒ–å›¾åƒã€‚

```py
>>> def train_transforms(example_batch):
...     images = [aug_transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs

>>> def val_transforms(example_batch):
...     images = [transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†è½¬æ¢ï¼Œä½¿ç”¨ğŸ¤— Datasets [set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)å‡½æ•°ã€‚è½¬æ¢æ˜¯å®æ—¶åº”ç”¨çš„ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œå ç”¨çš„ç£ç›˜ç©ºé—´æ›´å°‘ï¼š

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```

### è¯„ä¼°

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«ä¸€ä¸ªåº¦é‡æ ‡å‡†é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)åº“å¿«é€ŸåŠ è½½ä¸€ä¸ªè¯„ä¼°æ–¹æ³•ã€‚å¯¹äºè¿™ä¸ªä»»åŠ¡ï¼ŒåŠ è½½[mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy) (IoU)åº¦é‡æ ‡å‡†ï¼ˆæŸ¥çœ‹ğŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)ä»¥äº†è§£å¦‚ä½•åŠ è½½å’Œè®¡ç®—åº¦é‡æ ‡å‡†ï¼‰ï¼š

```py
>>> import evaluate

>>> metric = evaluate.load("mean_iou")
```

ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥`compute`åº¦é‡æ ‡å‡†ã€‚æ‚¨çš„é¢„æµ‹éœ€è¦é¦–å…ˆè½¬æ¢ä¸ºlogitsï¼Œç„¶åé‡æ–°è°ƒæ•´å½¢çŠ¶ä»¥åŒ¹é…æ ‡ç­¾çš„å¤§å°ï¼Œç„¶åæ‰èƒ½è°ƒç”¨`compute`ï¼š

PytorchHide Pytorchå†…å®¹

```py
>>> import numpy as np
>>> import torch
>>> from torch import nn

>>> def compute_metrics(eval_pred):
...     with torch.no_grad():
...         logits, labels = eval_pred
...         logits_tensor = torch.from_numpy(logits)
...         logits_tensor = nn.functional.interpolate(
...             logits_tensor,
...             size=labels.shape[-2:],
...             mode="bilinear",
...             align_corners=False,
...         ).argmax(dim=1)

...         pred_labels = logits_tensor.detach().cpu().numpy()
...         metrics = metric.compute(
...             predictions=pred_labels,
...             references=labels,
...             num_labels=num_labels,
...             ignore_index=255,
...             reduce_labels=False,
...         )
...         for key, value in metrics.items():
...             if isinstance(value, np.ndarray):
...                 metrics[key] = value.tolist()
...         return metrics
```

TensorFlowHide TensorFlowå†…å®¹

```py
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])
...     logits_resized = tf.image.resize(
...         logits,
...         size=tf.shape(labels)[1:],
...         method="bilinear",
...     )

...     pred_labels = tf.argmax(logits_resized, axis=-1)
...     metrics = metric.compute(
...         predictions=pred_labels,
...         references=labels,
...         num_labels=num_labels,
...         ignore_index=-1,
...         reduce_labels=image_processor.do_reduce_labels,
...     )

...     per_category_accuracy = metrics.pop("per_category_accuracy").tolist()
...     per_category_iou = metrics.pop("per_category_iou").tolist()

...     metrics.update({f"accuracy_{id2label[i]}": v for i, v in enumerate(per_category_accuracy)})
...     metrics.update({f"iou_{id2label[i]}": v for i, v in enumerate(per_category_iou)})
...     return {"val_" + k: v for k, v in metrics.items()}
```

æ‚¨çš„`compute_metrics`å‡½æ•°ç°åœ¨å·²ç»å‡†å¤‡å°±ç»ªï¼Œå½“æ‚¨è®¾ç½®è®­ç»ƒæ—¶ä¼šå†æ¬¡ç”¨åˆ°å®ƒã€‚

### è®­ç»ƒ

PytorchHide Pytorchå†…å®¹

å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰å¦‚ä½•ä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¯·æŸ¥çœ‹è¿™é‡Œçš„åŸºæœ¬æ•™ç¨‹[../training#finetune-with-trainer]ï¼

æ‚¨ç°åœ¨å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨[AutoModelForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSemanticSegmentation)åŠ è½½SegFormerï¼Œå¹¶å°†æ¨¡å‹ä¼ é€’ç»™æ ‡ç­¾idå’Œæ ‡ç­¾ç±»ä¹‹é—´çš„æ˜ å°„ï¼š

```py
>>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer

>>> model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)
```

ç›®å‰åªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1.  åœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚é‡è¦çš„æ˜¯ä¸è¦åˆ é™¤æœªä½¿ç”¨çš„åˆ—ï¼Œå› ä¸ºè¿™ä¼šåˆ é™¤`image`åˆ—ã€‚æ²¡æœ‰`image`åˆ—ï¼Œæ‚¨å°±æ— æ³•åˆ›å»º`pixel_values`ã€‚è®¾ç½®`remove_unused_columns=False`ä»¥é˜²æ­¢è¿™ç§è¡Œä¸ºï¼å¦ä¸€ä¸ªå¿…éœ€çš„å‚æ•°æ˜¯`output_dir`ï¼ŒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½®`push_to_hub=True`å°†æ­¤æ¨¡å‹æ¨é€åˆ°Hubï¼ˆæ‚¨éœ€è¦ç™»å½•Hugging Faceæ‰èƒ½ä¸Šä¼ æ‚¨çš„æ¨¡å‹ï¼‰ã€‚åœ¨æ¯ä¸ªepochç»“æŸæ—¶ï¼Œ[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å°†è¯„ä¼°IoUåº¦é‡æ ‡å‡†å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚

1.  å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼ŒåŒæ—¶è¿˜éœ€è¦ä¼ é€’æ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ`compute_metrics`å‡½æ•°ã€‚

1.  è°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="segformer-b0-scene-parse-150",
...     learning_rate=6e-5,
...     num_train_epochs=50,
...     per_device_train_batch_size=2,
...     per_device_eval_batch_size=2,
...     save_total_limit=3,
...     evaluation_strategy="steps",
...     save_strategy="steps",
...     save_steps=20,
...     eval_steps=20,
...     logging_steps=1,
...     eval_accumulation_steps=5,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=train_ds,
...     eval_dataset=test_ds,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ°Hubï¼Œè¿™æ ·æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š

```py
>>> trainer.push_to_hub()
```

TensorFlowHide TensorFlowå†…å®¹

å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨Kerasè¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œè¯·å…ˆæŸ¥çœ‹[åŸºæœ¬æ•™ç¨‹](./training#train-a-tensorflow-model-with-keras)ï¼

è¦åœ¨TensorFlowä¸­å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š

1.  å®šä¹‰è®­ç»ƒè¶…å‚æ•°ï¼Œå¹¶è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ã€‚

1.  å®ä¾‹åŒ–ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ã€‚

1.  å°†ä¸€ä¸ªğŸ¤—æ•°æ®é›†è½¬æ¢ä¸º`tf.data.Dataset`ã€‚

1.  ç¼–è¯‘æ‚¨çš„æ¨¡å‹ã€‚

1.  æ·»åŠ å›è°ƒä»¥è®¡ç®—æŒ‡æ ‡å¹¶å°†æ‚¨çš„æ¨¡å‹ä¸Šä¼ åˆ°ğŸ¤— Hub

1.  ä½¿ç”¨`fit()`æ–¹æ³•è¿è¡Œè®­ç»ƒã€‚

é¦–å…ˆå®šä¹‰è¶…å‚æ•°ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ï¼š

```py
>>> from transformers import create_optimizer

>>> batch_size = 2
>>> num_epochs = 50
>>> num_train_steps = len(train_ds) * num_epochs
>>> learning_rate = 6e-5
>>> weight_decay_rate = 0.01

>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=learning_rate,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=weight_decay_rate,
...     num_warmup_steps=0,
... )
```

ç„¶åï¼Œä½¿ç”¨[TFAutoModelForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForSemanticSegmentation)åŠ è½½SegFormerä»¥åŠæ ‡ç­¾æ˜ å°„ï¼Œå¹¶ä½¿ç”¨ä¼˜åŒ–å™¨å¯¹å…¶è¿›è¡Œç¼–è¯‘ã€‚è¯·æ³¨æ„ï¼ŒTransformersæ¨¡å‹éƒ½æœ‰ä¸€ä¸ªé»˜è®¤çš„ä¸ä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°ï¼Œå› æ­¤é™¤éæ‚¨æƒ³è¦æŒ‡å®šä¸€ä¸ªï¼Œå¦åˆ™ä¸éœ€è¦æŒ‡å®šï¼š

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
... )
>>> model.compile(optimizer=optimizer)  # No loss argument!
```

ä½¿ç”¨[to_tf_dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset)å’Œ[DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator)å°†æ‚¨çš„æ•°æ®é›†è½¬æ¢ä¸º`tf.data.Dataset`æ ¼å¼ï¼š

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")

>>> tf_train_dataset = train_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )

>>> tf_eval_dataset = test_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )
```

è¦ä»é¢„æµ‹ä¸­è®¡ç®—å‡†ç¡®ç‡å¹¶å°†æ‚¨çš„æ¨¡å‹æ¨é€åˆ°ğŸ¤— Hubï¼Œè¯·ä½¿ç”¨[Keraså›è°ƒ](../main_classes/keras_callbacks)ã€‚å°†æ‚¨çš„`compute_metrics`å‡½æ•°ä¼ é€’ç»™[KerasMetricCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.KerasMetricCallback)ï¼Œå¹¶ä½¿ç”¨[PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback)æ¥ä¸Šä¼ æ¨¡å‹ï¼š

```py
>>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback

>>> metric_callback = KerasMetricCallback(
...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=["labels"]
... )

>>> push_to_hub_callback = PushToHubCallback(output_dir="scene_segmentation", tokenizer=image_processor)

>>> callbacks = [metric_callback, push_to_hub_callback]
```

æœ€åï¼Œæ‚¨å·²ç»å‡†å¤‡å¥½è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨æ‚¨çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€æ—¶ä»£æ•°é‡å’Œå›è°ƒæ¥è°ƒç”¨`fit()`æ¥å¾®è°ƒæ¨¡å‹ï¼š

```py
>>> model.fit(
...     tf_train_dataset,
...     validation_data=tf_eval_dataset,
...     callbacks=callbacks,
...     epochs=num_epochs,
... )
```

æ­å–œï¼æ‚¨å·²ç»å¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒå¹¶åœ¨ğŸ¤— Hubä¸Šåˆ†äº«äº†å®ƒã€‚ç°åœ¨æ‚¨å¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†ï¼

### æ¨ç†

å¾ˆå¥½ï¼Œç°åœ¨æ‚¨å·²ç»å¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†ï¼

åŠ è½½ä¸€å¼ å›¾ç‰‡è¿›è¡Œæ¨ç†ï¼š

```py
>>> image = ds[0]["image"]
>>> image
```

![å§å®¤å›¾åƒ](../Images/1f1abe71d12160da7bd59d35ef05323c.png)Pytorchéšè— Pytorchå†…å®¹

ç°åœ¨æˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•åœ¨æ²¡æœ‰ç®¡é“çš„æƒ…å†µä¸‹è¿›è¡Œæ¨ç†ã€‚ä½¿ç”¨å›¾åƒå¤„ç†å™¨å¤„ç†å›¾åƒï¼Œå¹¶å°†`pixel_values`æ”¾åœ¨GPUä¸Šï¼š

```py
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # use GPU if available, otherwise use a CPU
>>> encoding = image_processor(image, return_tensors="pt")
>>> pixel_values = encoding.pixel_values.to(device)
```

å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›`logits`ï¼š

```py
>>> outputs = model(pixel_values=pixel_values)
>>> logits = outputs.logits.cpu()
```

æ¥ä¸‹æ¥ï¼Œå°†logitsé‡æ–°ç¼©æ”¾åˆ°åŸå§‹å›¾åƒå¤§å°ï¼š

```py
>>> upsampled_logits = nn.functional.interpolate(
...     logits,
...     size=image.size[::-1],
...     mode="bilinear",
...     align_corners=False,
... )

>>> pred_seg = upsampled_logits.argmax(dim=1)[0]
```

TensorFlowéšè— TensorFlowå†…å®¹

åŠ è½½ä¸€ä¸ªå›¾åƒå¤„ç†å™¨æ¥é¢„å¤„ç†å›¾åƒå¹¶å°†è¾“å…¥è¿”å›ä¸ºTensorFlowå¼ é‡ï¼š

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("MariaK/scene_segmentation")
>>> inputs = image_processor(image, return_tensors="tf")
```

å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›`logits`ï¼š

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained("MariaK/scene_segmentation")
>>> logits = model(**inputs).logits
```

æ¥ä¸‹æ¥ï¼Œå°†logitsé‡æ–°ç¼©æ”¾åˆ°åŸå§‹å›¾åƒå¤§å°ï¼Œå¹¶åœ¨ç±»ç»´åº¦ä¸Šåº”ç”¨argmaxï¼š

```py
>>> logits = tf.transpose(logits, [0, 2, 3, 1])

>>> upsampled_logits = tf.image.resize(
...     logits,
...     # We reverse the shape of `image` because `image.size` returns width and height.
...     image.size[::-1],
... )

>>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]
```

è¦å¯è§†åŒ–ç»“æœï¼ŒåŠ è½½[æ•°æ®é›†é¢œè‰²è°ƒè‰²æ¿](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51)ä½œä¸º`ade_palette()`ï¼Œå°†æ¯ä¸ªç±»æ˜ å°„åˆ°å®ƒä»¬çš„RGBå€¼ã€‚ç„¶åæ‚¨å¯ä»¥ç»„åˆå¹¶ç»˜åˆ¶æ‚¨çš„å›¾åƒå’Œé¢„æµ‹çš„åˆ†å‰²åœ°å›¾ï¼š

```py
>>> import matplotlib.pyplot as plt
>>> import numpy as np

>>> color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)
>>> palette = np.array(ade_palette())
>>> for label, color in enumerate(palette):
...     color_seg[pred_seg == label, :] = color
>>> color_seg = color_seg[..., ::-1]  # convert to BGR

>>> img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map
>>> img = img.astype(np.uint8)

>>> plt.figure(figsize=(15, 10))
>>> plt.imshow(img)
>>> plt.show()
```

![è¦†ç›–æœ‰åˆ†å‰²åœ°å›¾çš„å§å®¤å›¾åƒ](../Images/2bc12ae66cec2ea434945aaaa1e5c6bf.png)
