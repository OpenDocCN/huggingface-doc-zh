- en: Efficient Training on Multiple GPUs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/perf_train_gpu_many](https://huggingface.co/docs/transformers/v4.37.2/en/perf_train_gpu_many)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: If training a model on a single GPU is too slow or if the modelâ€™s weights do
    not fit in a single GPUâ€™s memory, transitioning to a multi-GPU setup may be a
    viable option. Prior to making this transition, thoroughly explore all the strategies
    covered in the [Methods and tools for efficient training on a single GPU](perf_train_gpu_one)
    as they are universally applicable to model training on any number of GPUs. Once
    you have employed those strategies and found them insufficient for your case on
    a single GPU, consider moving to multiple GPUs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Transitioning from a single GPU to multiple GPUs requires the introduction of
    some form of parallelism, as the workload must be distributed across the resources.
    Multiple techniques can be employed to achieve parallelism, such as data parallelism,
    tensor parallelism, and pipeline parallelism. Itâ€™s important to note that there
    isnâ€™t a one-size-fits-all solution, and the optimal settings depend on the specific
    hardware configuration you are using.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: This guide offers an in-depth overview of individual types of parallelism, as
    well as guidance on ways to combine
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: techniques and choosing an appropriate approach. For step-by-step tutorials
    on distributed training, please refer to the [ğŸ¤— Accelerate documentation](https://huggingface.co/docs/accelerate/index).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: While the main concepts discussed in this guide are likely applicable across
    frameworks, here we focus on PyTorch-based implementations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Before diving deeper into the specifics of each technique, letâ€™s go over the
    rough decision process when training large models on a large infrastructure.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Scalability strategy
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Begin by estimating how much vRAM is required to train your model. For models
    hosted on the ğŸ¤— Hub, use our [Model Memory Calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage),
    which gives you accurate calculations within a few percent margin.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallelization strategy for a single Node / multi-GPU setup**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'When training a model on a single node with multiple GPUs, your choice of parallelization
    strategy can significantly impact performance. Hereâ€™s a breakdown of your options:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 1: Your model fits onto a single GPU**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'If your model can comfortably fit onto a single GPU, you have two primary options:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: DDP - Distributed DataParallel
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ZeRO - depending on the situation and configuration used, this method may or
    may not be faster, however, itâ€™s worth experimenting with it.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Case 2: Your model doesnâ€™t fit onto a single GPU:**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'If your model is too large for a single GPU, you have several alternatives
    to consider:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: PipelineParallel (PP)
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ZeRO
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TensorParallel (TP)
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With very fast inter-node connectivity (e.g., NVLINK or NVSwitch) all three
    strategies (PP, ZeRO, TP) should result in similar performance. However, without
    these, PP will be faster than TP or ZeRO. The degree of TP may also make a difference.
    Itâ€™s best to experiment with your specific setup to determine the most suitable
    strategy.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: TP is almost always used within a single node. That is TP size <= GPUs per node.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 3: Largest layer of your model does not fit onto a single GPU**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: If you are not using ZeRO, you have to use TensorParallel (TP), because PipelineParallel
    (PP) alone wonâ€™t be sufficient to accommodate the large layer.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you are using ZeRO, additionally adopt techniques from the [Methods and tools
    for efficient training on a single GPU](perf_train_gpu_one).
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Parallelization strategy for a multi-Node / multi-GPU setup**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'When you have fast inter-node connectivity (e.g., NVLINK or NVSwitch) consider
    using one of these options:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZeRO - as it requires close to no modifications to the model
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A combination of PipelineParallel(PP) with TensorParallel(TP) and DataParallel(DP)
    - this approach will result in fewer communications, but requires significant
    changes to the model
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you have slow inter-node connectivity and still low on GPU memory:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Employ a combination of DataParallel(DP) with PipelineParallel(PP), TensorParallel(TP),
    and ZeRO.
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following sections of this guide we dig deeper into how these different
    parallelism methods work.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Data Parallelism
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even with only 2 GPUs, you can readily leverage the accelerated training capabilities
    offered by PyTorchâ€™s built-in features, such as `DataParallel` (DP) and `DistributedDataParallel`
    (DDP). Note that [PyTorch documentation](https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html)
    recommends to prefer `DistributedDataParallel` (DDP) over `DataParallel` (DP)
    for multi-GPU training as it works for all models. Letâ€™s take a look at how these
    two methods work and what makes them different.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: DataParallel vs DistributedDataParallel
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To understand the key differences in inter-GPU communication overhead between
    the two methods, letâ€™s review the processes per batch:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[DDP](https://pytorch.org/docs/master/notes/ddp.html):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: At the start time the main process replicates the model once from GPU 0 to the
    rest of GPUs
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then for each batch:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each GPU directly consumes its mini-batch of data.
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: During `backward`, once the local gradients are ready, they are averaged across
    all processes.
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[DP](https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'For each batch:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: GPU 0 reads the batch of data and then sends a mini-batch to each GPU.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The up-to-date model is replicated from GPU 0 to each GPU.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`forward` is executed, and output from each GPU is sent to GPU 0 to compute
    the loss.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loss is distributed from GPU 0 to all GPUs, and `backward` is run.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradients from each GPU are sent to GPU 0 and averaged.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Key differences include:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: DDP performs only a single communication per batch - sending gradients, while
    DP performs five different data exchanges per batch. DDP copies data using [torch.distributed](https://pytorch.org/docs/master/distributed.html),
    while DP copies data within the process via Python threads (which introduces limitations
    associated with GIL). As a result, **`DistributedDataParallel` (DDP) is generally
    faster than `DataParallel` (DP)** unless you have slow GPU card inter-connectivity.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under DP, GPU 0 performs significantly more work than other GPUs, resulting
    in GPU under-utilization.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DDP supports distributed training across multiple machines, whereas DP does
    not.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is not an exhaustive list of differences between DP and DDP, however, other
    nuances are out of scope of this guide. You can get a deeper understanding of
    these methods by reading this [article](https://www.telesens.co/2019/04/04/distributed-data-parallel-training-using-pytorch-on-aws/).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s illustrate the differences between DP and DDP with an experiment. Weâ€™ll
    benchmark the differences between DP and DDP with an added context of NVLink presence:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Hardware: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks (`NV2` in `nvidia-smi
    topo -m`).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Software: `pytorch-1.8-to-be` + `cuda-11.0` / `transformers==4.3.0.dev0`.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To disable the NVLink feature on one of the benchmarks, we use `NCCL_P2P_DISABLE=1`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the benchmarking code and outputs:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '**DP**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**DDP w/ NVlink**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**DDP w/o NVlink**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here are the same benchmarking results gathered in a table for convenience:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | NVlink | Time |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '| :-- | --- | --: |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| 2:DP | Y | 110s |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| 2:DDP | Y | 101s |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| 2:DDP | N | 131s |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: As you can see, in this case DP is ~10% slower than DDP with NVlink, but ~15%
    faster than DDP without NVlink. The real difference will depend on how much data
    each GPU needs to sync with the others - the more there is to sync, the more a
    slow link will impede the overall runtime.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: ZeRO Data Parallelism
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ZeRO-powered data parallelism (ZeRO-DP) is illustrated in the following diagram
    from this [blog post](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![DeepSpeed-Image-1](../Images/61da03a3a1a0e2c5e704642f87f2f216.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: While it may appear complex, it is a very similar concept to `DataParallel`
    (DP). The difference is that instead of replicating the full model parameters,
    gradients and optimizer states, each GPU stores only a slice of it. Then, at run-time
    when the full layer parameters are needed just for the given layer, all GPUs synchronize
    to give each other parts that they miss.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶çœ‹èµ·æ¥å¤æ‚ï¼Œä½†è¿™ä¸`DataParallel`ï¼ˆDPï¼‰éå¸¸ç›¸ä¼¼ã€‚ä¸åŒä¹‹å¤„åœ¨äºï¼Œæ¯ä¸ªGPUåªå­˜å‚¨å…¶ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯å¤åˆ¶å®Œæ•´çš„æ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ã€‚ç„¶åï¼Œåœ¨è¿è¡Œæ—¶ï¼Œå½“éœ€è¦å®Œæ•´çš„å±‚å‚æ•°æ—¶ï¼Œæ‰€æœ‰GPUä¼šåŒæ­¥ä»¥äº’ç›¸æä¾›å®ƒä»¬ç¼ºå°‘çš„éƒ¨åˆ†ã€‚
- en: 'To illustrate this idea, consider a simple model with 3 layers (La, Lb, and
    Lc), where each layer has 3 parameters. Layer La, for example, has weights a0,
    a1 and a2:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜è¿™ä¸ªæƒ³æ³•ï¼Œè€ƒè™‘ä¸€ä¸ªå…·æœ‰3å±‚ï¼ˆLaï¼ŒLbå’ŒLcï¼‰çš„ç®€å•æ¨¡å‹ï¼Œå…¶ä¸­æ¯å±‚æœ‰3ä¸ªå‚æ•°ã€‚ä¾‹å¦‚ï¼Œå±‚Laæœ‰æƒé‡a0ï¼Œa1å’Œa2ï¼š
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we have 3 GPUs, ZeRO-DP splits the model onto 3 GPUs like so:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æœ‰3ä¸ªGPUï¼ŒZeRO-DPå°†æ¨¡å‹åˆ†å‰²åˆ°3ä¸ªGPUä¸Šï¼š
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In a way, this is the same horizontal slicing as tensor parallelism, as opposed
    to Vertical slicing, where one puts whole layer-groups on different GPUs. Now
    letâ€™s see how this works:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œè¿™ä¸å¼ é‡å¹¶è¡Œæ€§ç›¸åŒçš„æ°´å¹³åˆ‡ç‰‡ï¼Œä¸å‚ç›´åˆ‡ç‰‡ç›¸å¯¹ï¼Œå…¶ä¸­å°†æ•´ä¸ªå±‚ç»„æ”¾åœ¨ä¸åŒçš„GPUä¸Šã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹è¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼š
- en: 'Each of these GPUs will get the usual mini-batch as it works in DP:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›GPUä¸­çš„æ¯ä¸€ä¸ªéƒ½å°†åƒDPä¸­é‚£æ ·è·å¾—é€šå¸¸çš„å°æ‰¹é‡ï¼š
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The inputs are passed without modifications as if they would be processed by
    the original model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥è¢«ä¼ é€’è€Œä¸è¿›è¡Œä¿®æ”¹ï¼Œå°±å¥½åƒåŸå§‹æ¨¡å‹ä¼šå¤„ç†å®ƒä»¬ä¸€æ ·ã€‚
- en: First, the inputs get to the layer `La`. What happens at this point?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè¾“å…¥åˆ°è¾¾å±‚`La`ã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
- en: 'On GPU0: the x0 mini-batch requires the a0, a1, a2 parameters to do its forward
    path through the layer, but the GPU0 has only a0. It will get a1 from GPU1 and
    a2 from GPU2, bringing all the pieces of the model together.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨GPU0ä¸Šï¼šx0å°æ‰¹é‡éœ€è¦a0ï¼Œa1ï¼Œa2å‚æ•°é€šè¿‡å±‚è¿›è¡Œå‰å‘è·¯å¾„ï¼Œä½†GPU0åªæœ‰a0ã€‚å®ƒå°†ä»GPU1è·å–a1ï¼Œä»GPU2è·å–a2ï¼Œå°†æ¨¡å‹çš„æ‰€æœ‰éƒ¨åˆ†æ±‡é›†åœ¨ä¸€èµ·ã€‚
- en: In parallel, GPU1 gets another mini-batch - x1\. GPU1 has the a1 parameter,
    but needs a0 and a2, so it gets those from GPU0 and GPU2. Same happens to GPU2
    that gets the mini-batch x2\. It gets a0 and a1 from GPU0 and GPU1.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ—¶ï¼ŒGPU1è·å¾—å¦ä¸€ä¸ªå°æ‰¹é‡-x1ã€‚GPU1å…·æœ‰a1å‚æ•°ï¼Œä½†éœ€è¦a0å’Œa2ï¼Œå› æ­¤å®ƒä»GPU0å’ŒGPU2è·å–è¿™äº›ã€‚GPU2ä¹Ÿå‘ç”ŸåŒæ ·çš„æƒ…å†µï¼Œå®ƒè·å¾—å°æ‰¹é‡x2ã€‚å®ƒä»GPU0å’ŒGPU1è·å–a0å’Œa1ã€‚
- en: This way each of the 3 GPUs gets the full tensors reconstructed and makes a
    forward pass with its own mini-batch. As soon as the calculation is done, the
    data that is no longer needed gets dropped - itâ€™s only used during the calculation.
    The reconstruction is done efficiently via a pre-fetch.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œæ¯ä¸ª3ä¸ªGPUéƒ½ä¼šå¾—åˆ°å®Œæ•´çš„å¼ é‡é‡å»ºï¼Œå¹¶ä½¿ç”¨è‡ªå·±çš„å°æ‰¹é‡è¿›è¡Œå‰å‘ä¼ é€’ã€‚ä¸€æ—¦è®¡ç®—å®Œæˆï¼Œä¸å†éœ€è¦çš„æ•°æ®å°†è¢«ä¸¢å¼ƒ-å®ƒåªåœ¨è®¡ç®—è¿‡ç¨‹ä¸­ä½¿ç”¨ã€‚é‡å»ºé€šè¿‡é¢„å–é«˜æ•ˆåœ°å®Œæˆã€‚
- en: Then the whole process is repeated for layer Lb, then Lc forward-wise, and then
    backward Lc -> Lb -> La.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ•´ä¸ªè¿‡ç¨‹ä¼šä¸ºå±‚Lbé‡å¤ï¼Œç„¶åæ˜¯Lcçš„å‰å‘ï¼Œç„¶åæ˜¯Lc -> Lb -> Laçš„åå‘ã€‚
- en: 'This mechanism is similar to an efficient group backpacking strategy: person
    A carries the tent, person B carries the stove, and person C carries the axe.
    Each night they all share what they have with others and get from others what
    they donâ€™t have, and in the morning they pack up their allocated type of gear
    and continue on their way. This is what ZeRO DP/Sharded DDP is. Compare this strategy
    to the simple one where each person has to carry their own tent, stove and axe
    (similar to DataParallel (DP and DDP) in PyTorch), which would be far more inefficient.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æœºåˆ¶ç±»ä¼¼äºä¸€ç§é«˜æ•ˆçš„å›¢ä½“èƒŒåŒ…ç­–ç•¥ï¼šAäººæºå¸¦å¸ç¯·ï¼ŒBäººæºå¸¦ç‚‰ç¶ï¼ŒCäººæºå¸¦æ–§å¤´ã€‚æ¯æ™šä»–ä»¬éƒ½åˆ†äº«è‡ªå·±æ‹¥æœ‰çš„ä¸œè¥¿ï¼Œå¹¶ä»ä»–äººé‚£é‡Œå¾—åˆ°ä»–ä»¬æ²¡æœ‰çš„ä¸œè¥¿ï¼Œæ—©ä¸Šä»–ä»¬æ”¶æ‹¾å¥½è‡ªå·±åˆ†é…çš„è£…å¤‡ç±»å‹ï¼Œç»§ç»­å‰è¿›ã€‚è¿™å°±æ˜¯ZeRO
    DP/Sharded DDPã€‚å°†è¿™ç§ç­–ç•¥ä¸æ¯ä¸ªäººéƒ½å¿…é¡»æºå¸¦è‡ªå·±çš„å¸ç¯·ã€ç‚‰ç¶å’Œæ–§å¤´çš„ç®€å•ç­–ç•¥è¿›è¡Œæ¯”è¾ƒï¼ˆç±»ä¼¼äºPyTorchä¸­çš„DataParallelï¼ˆDPå’ŒDDPï¼‰ï¼‰ï¼Œåè€…å°†æ›´åŠ ä½æ•ˆã€‚
- en: 'While reading the literature on this topic you may encounter the following
    synonyms: Sharded, Partitioned. If you pay close attention the way ZeRO partitions
    the modelâ€™s weights - it looks very similar to tensor parallelism which will be
    discussed later. This is because it partitions/shards each layerâ€™s weights, unlike
    vertical model parallelism which is discussed next.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é˜…è¯»è¿™ä¸ªä¸»é¢˜çš„æ–‡çŒ®æ—¶ï¼Œæ‚¨å¯èƒ½ä¼šé‡åˆ°ä»¥ä¸‹åŒä¹‰è¯ï¼šåˆ†ç‰‡ï¼Œåˆ†åŒºã€‚å¦‚æœæ‚¨ä»”ç»†æ³¨æ„ZeROå¦‚ä½•åˆ†å‰²æ¨¡å‹çš„æƒé‡-å®ƒçœ‹èµ·æ¥éå¸¸ç±»ä¼¼äºå¼ é‡å¹¶è¡Œæ€§ï¼Œç¨åå°†å¯¹æ­¤è¿›è¡Œè®¨è®ºã€‚è¿™æ˜¯å› ä¸ºå®ƒåˆ†å‰²/åˆ†ç‰‡æ¯ä¸ªå±‚çš„æƒé‡ï¼Œä¸æ¥ä¸‹æ¥å°†è®¨è®ºçš„å‚ç›´æ¨¡å‹å¹¶è¡Œæ€§ä¸åŒã€‚
- en: 'Implementations:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å®æ–½ï¼š
- en: '[DeepSpeed](https://www.deepspeed.ai/tutorials/zero/) ZeRO-DP stages 1+2+3'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DeepSpeed](https://www.deepspeed.ai/tutorials/zero/) ZeRO-DP é˜¶æ®µ1+2+3'
- en: '[`Accelerate` integration](https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`åŠ é€Ÿ`é›†æˆ](https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed)'
- en: '[`transformers` integration](main_classes/trainer#trainer-integrations)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`transformers` é›†æˆ](main_classes/trainer#trainer-integrations)'
- en: From Naive Model Parallelism to Pipeline Parallelism
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»å¤©çœŸçš„æ¨¡å‹å¹¶è¡Œæ€§åˆ°ç®¡é“å¹¶è¡Œæ€§
- en: To explain Pipeline parallelism, weâ€™ll first look into Naive Model Parallelism
    (MP), also known as Vertical MP. This approach involves distributing groups of
    model layers across multiple GPUs by assigning specific layers to specific GPUs
    with `.to()`. As data flows through these layers, it is moved to the same GPU
    as the layer, while the other layers remain untouched.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è§£é‡Šç®¡é“å¹¶è¡Œæ€§ï¼Œæˆ‘ä»¬é¦–å…ˆå°†ç ”ç©¶å¤©çœŸçš„æ¨¡å‹å¹¶è¡Œæ€§ï¼ˆMPï¼‰ï¼Œä¹Ÿç§°ä¸ºå‚ç›´MPã€‚è¿™ç§æ–¹æ³•æ¶‰åŠé€šè¿‡ä½¿ç”¨`.to()`å°†æ¨¡å‹å±‚ç»„åˆ†é…åˆ°å¤šä¸ªGPUä¸Šï¼Œå°†ç‰¹å®šå±‚åˆ†é…ç»™ç‰¹å®šGPUã€‚å½“æ•°æ®æµç»è¿™äº›å±‚æ—¶ï¼Œå®ƒè¢«ç§»åŠ¨åˆ°ä¸è¯¥å±‚ç›¸åŒçš„GPUä¸Šï¼Œè€Œå…¶ä»–å±‚ä¿æŒä¸å˜ã€‚
- en: 'We refer to this Model parallelism as â€œVerticalâ€ because of how models are
    typically visualized. For example, the following diagram shows an 8-layer model
    split vertically into two slices, placing layers 0-3 onto GPU0 and 4-7 to GPU1:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¿™ç§æ¨¡å‹å¹¶è¡Œæ€§ç§°ä¸ºâ€œå‚ç›´â€ï¼Œå› ä¸ºæ¨¡å‹é€šå¸¸æ˜¯å¦‚ä½•å¯è§†åŒ–çš„ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹å›¾è¡¨æ˜¾ç¤ºå°†8å±‚æ¨¡å‹å‚ç›´åˆ†å‰²æˆä¸¤ä¸ªåˆ‡ç‰‡ï¼Œå°†å±‚0-3æ”¾åœ¨GPU0ä¸Šï¼Œå°†å±‚4-7æ”¾åœ¨GPU1ä¸Šï¼š
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this example, when data moves from layer 0 to 3, itâ€™s no different from regular
    forward pass. However, passing data from layer 3 to 4 requires moving it from
    GPU0 to GPU1, introducing a communication overhead. If the participating GPUs
    are on the same compute node (e.g. same physical machine) this copying is fast,
    but if the GPUs are distributed across different compute nodes (e.g. multiple
    machines), the communication overhead could be substantially greater.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Following that, layers 4 to 7 work as they would in the original model. Upon
    completion of the 7th layer, there is often a need to send the data back to layer
    0 where the labels are (or alternatively send the labels to the last layer). Now
    the loss can be computed and the optimizer can do its work.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'Naive Model Parallelism comes several shortcomings:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '**All but one GPU are idle at any given moment**: if 4 GPUs are used, itâ€™s
    nearly identical to quadrupling the amount of memory of a single GPU, and ignoring
    the rest of the hardware.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overhead in data transfer between devices**: E.g. 4x 6GB cards will be able
    to accommodate the same size as 1x 24GB card using naive MP, but a single 24GB
    card will complete the training faster, because it doesnâ€™t have the data copying
    overhead. But, say, if you have 40GB cards and need to fit a 45GB model you can
    with 4x 40GB cards (but barely because of the gradient and optimizer states)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Copying shared embeddings**: Shared embeddings may need to get copied back
    and forth between GPUs.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you are familiar with how the naive approach to model parallelism works
    and its shortcomings, letâ€™s look at Pipeline Parallelism (PP). PP is almost identical
    to a naive MP, but it solves the GPU idling problem by chunking the incoming batch
    into micro-batches and artificially creating a pipeline, which allows different
    GPUs to concurrently participate in the computation process.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'The following illustration from the [GPipe paper](https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html)
    shows the naive MP on the top, and PP on the bottom:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![MP vs PP](../Images/ba7b60c4785caa5f69f0731aec905196.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: At the bottom of the diagram, you can observe that the Pipeline Parallelism
    (PP) approach minimizes the number of idle GPU zones, referred to as â€˜bubblesâ€™.
    Both parts of the diagram show a parallelism level of degree 4, meaning that 4
    GPUs are involved in the pipeline. You can see that thereâ€™s a forward path of
    4 pipe stages (F0, F1, F2 and F3) followed by a backward path in reverse order
    (B3, B2, B1, and B0).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: PP introduces a new hyperparameter to tune - `chunks`, which determines how
    many data chunks are sent in a sequence through the same pipe stage. For example,
    in the bottom diagram you can see `chunks=4`. GPU0 performs the same forward path
    on chunk 0, 1, 2 and 3 (F0,0, F0,1, F0,2, F0,3) and then it waits for other GPUs
    to do complete their work. Only when the other GPUs begin to complete their work,
    GPU0 starts to work again doing the backward path for chunks 3, 2, 1 and 0 (B0,3,
    B0,2, B0,1, B0,0).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is the same concept as gradient accumulation steps. PyTorch uses
    `chunks`, while DeepSpeed refers to the same hyperparameter as gradient accumulation
    steps.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of the chunks, PP introduces the notion of micro-batches (MBS). DP
    splits the global data batch size into mini-batches, so if you have a DP degree
    of 4, a global batch size of 1024 gets split up into 4 mini-batches of 256 each
    (1024/4). And if the number of `chunks` (or GAS) is 32 we end up with a micro-batch
    size of 8 (256/32). Each Pipeline stage works with a single micro-batch at a time.
    To calculate the global batch size of the DP + PP setup, use the formula: `mbs
    * chunks * dp_degree` (`8 * 32 * 4 = 1024`). With `chunks=1` you end up with the
    naive MP, which is inefficient. With a large `chunks` value you end up with tiny
    micro-batch sizes which is also inefficient. For this reason, we encourage to
    experiment with the `chunks` value to find the one that leads to the most efficient
    GPUs utilization.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºchunksï¼ŒPPå¼•å…¥äº†å¾®æ‰¹æ¬¡ï¼ˆMBSï¼‰çš„æ¦‚å¿µã€‚DPå°†å…¨å±€æ•°æ®æ‰¹æ¬¡å¤§å°åˆ†æˆå°æ‰¹æ¬¡ï¼Œå› æ­¤å¦‚æœDPåº¦ä¸º4ï¼Œåˆ™å…¨å±€æ‰¹æ¬¡å¤§å°ä¸º1024å°†åˆ†æˆ4ä¸ªæ¯ä¸ª256çš„å°æ‰¹æ¬¡ï¼ˆ1024/4ï¼‰ã€‚å¦‚æœ`chunks`ï¼ˆæˆ–GASï¼‰çš„æ•°é‡ä¸º32ï¼Œåˆ™æˆ‘ä»¬æœ€ç»ˆå¾—åˆ°ä¸€ä¸ªå¾®æ‰¹æ¬¡å¤§å°ä¸º8ï¼ˆ256/32ï¼‰ã€‚æ¯ä¸ªç®¡é“é˜¶æ®µä¸€æ¬¡å¤„ç†ä¸€ä¸ªå¾®æ‰¹æ¬¡ã€‚è¦è®¡ç®—DP
    + PPè®¾ç½®çš„å…¨å±€æ‰¹æ¬¡å¤§å°ï¼Œè¯·ä½¿ç”¨å…¬å¼ï¼š`mbs * chunks * dp_degree`ï¼ˆ`8 * 32 * 4 = 1024`ï¼‰ã€‚å½“`chunks=1`æ—¶ï¼Œæ‚¨å°†å¾—åˆ°å¤©çœŸçš„MPï¼Œè¿™æ˜¯ä½æ•ˆçš„ã€‚å½“`chunks`å€¼å¾ˆå¤§æ—¶ï¼Œæ‚¨å°†å¾—åˆ°å¾®å°çš„å¾®æ‰¹æ¬¡å¤§å°ï¼Œè¿™ä¹Ÿæ˜¯ä½æ•ˆçš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é¼“åŠ±å°è¯•ä¸åŒçš„`chunks`å€¼ï¼Œä»¥æ‰¾åˆ°å¯¼è‡´æœ€æœ‰æ•ˆçš„GPUåˆ©ç”¨ç‡çš„å€¼ã€‚
- en: You may notice a bubble of â€œdeadâ€ time on the diagram that canâ€™t be parallelized
    because the last `forward` stage has to wait for `backward` to complete the pipeline.
    The purpose of finding the best value for `chunks` is to enable a high concurrent
    GPU utilization across all participating GPUs which translates to minimizing the
    size of the bubble.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°å›¾è¡¨ä¸Šçš„â€œæ­»â€æ—¶é—´æ³¡æ³¡ï¼Œå› ä¸ºæœ€åçš„`forward`é˜¶æ®µå¿…é¡»ç­‰å¾…`backward`å®Œæˆç®¡é“ã€‚æ‰¾åˆ°`chunks`çš„æœ€ä½³å€¼çš„ç›®çš„æ˜¯å®ç°æ‰€æœ‰å‚ä¸GPUä¹‹é—´çš„é«˜å¹¶å‘GPUåˆ©ç”¨ç‡ï¼Œä»è€Œæœ€å°åŒ–æ³¡æ³¡çš„å¤§å°ã€‚
- en: 'Pipeline API solutions have been implemented in:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Pipeline APIè§£å†³æ–¹æ¡ˆå·²åœ¨ä»¥ä¸‹å¹³å°ä¸­å®æ–½ï¼š
- en: PyTorch
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: DeepSpeed
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepSpeed
- en: Megatron-LM
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Megatron-LM
- en: 'These come with some shortcomings:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å­˜åœ¨ä¸€äº›ç¼ºç‚¹ï¼š
- en: They have to modify the model quite heavily, because Pipeline requires one to
    rewrite the normal flow of modules into a `nn.Sequential` sequence of the same,
    which may require changes to the design of the model.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»–ä»¬å¿…é¡»å¯¹æ¨¡å‹è¿›è¡Œç›¸å½“å¤§çš„ä¿®æ”¹ï¼Œå› ä¸ºPipelineè¦æ±‚å°†æ¨¡å—çš„æ­£å¸¸æµé‡å†™ä¸ºç›¸åŒçš„`nn.Sequential`åºåˆ—ï¼Œè¿™å¯èƒ½éœ€è¦å¯¹æ¨¡å‹çš„è®¾è®¡è¿›è¡Œæ›´æ”¹ã€‚
- en: Currently the Pipeline API is very restricted. If you had a bunch of Python
    variables being passed in the very first stage of the Pipeline, you will have
    to find a way around it. Currently, the pipeline interface requires either a single
    Tensor or a tuple of Tensors as the only input and output. These tensors must
    have a batch size as the very first dimension, since pipeline is going to chunk
    the mini batch into micro-batches. Possible improvements are being discussed here
    [https://github.com/pytorch/pytorch/pull/50693](https://github.com/pytorch/pytorch/pull/50693)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼ŒPipeline APIéå¸¸å—é™åˆ¶ã€‚å¦‚æœåœ¨ç®¡é“çš„ç¬¬ä¸€ä¸ªé˜¶æ®µä¼ é€’äº†ä¸€å †Pythonå˜é‡ï¼Œæ‚¨å°†ä¸å¾—ä¸æ‰¾åˆ°è§£å†³æ–¹æ³•ã€‚ç›®å‰ï¼Œç®¡é“æ¥å£è¦æ±‚ä½œä¸ºå”¯ä¸€è¾“å…¥å’Œè¾“å‡ºçš„æ˜¯å•ä¸ªå¼ é‡æˆ–å¼ é‡å…ƒç»„ã€‚è¿™äº›å¼ é‡çš„æ‰¹æ¬¡å¤§å°å¿…é¡»ä½œä¸ºç¬¬ä¸€ä¸ªç»´åº¦ï¼Œå› ä¸ºç®¡é“å°†å°æ‰¹æ¬¡åˆ†æˆå¾®æ‰¹æ¬¡ã€‚è¿™é‡Œæ­£åœ¨è®¨è®ºå¯èƒ½çš„æ”¹è¿›[https://github.com/pytorch/pytorch/pull/50693](https://github.com/pytorch/pytorch/pull/50693)
- en: Conditional control flow at the level of pipe stages is not possible - e.g.,
    Encoder-Decoder models like T5 require special workarounds to handle a conditional
    encoder stage.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç®¡é“é˜¶æ®µçš„æ¡ä»¶æ§åˆ¶æµä¸å¯èƒ½-ä¾‹å¦‚ï¼Œç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼ˆå¦‚T5ï¼‰éœ€è¦ç‰¹æ®Šçš„è§£å†³æ–¹æ¡ˆæ¥å¤„ç†æ¡ä»¶ç¼–ç å™¨é˜¶æ®µã€‚
- en: They have to arrange each layer so that the output of one layer becomes an input
    to the other layer.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»–ä»¬å¿…é¡»å®‰æ’æ¯ä¸€å±‚ï¼Œä»¥ä¾¿ä¸€å±‚çš„è¾“å‡ºæˆä¸ºå¦ä¸€å±‚çš„è¾“å…¥ã€‚
- en: 'More recent solutions include:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´è¿‘æœŸçš„è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ï¼š
- en: Varuna
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Varuna
- en: Sagemaker
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sagemaker
- en: We have not experimented with Varuna and SageMaker but their papers report that
    they have overcome the list of problems mentioned above and that they require
    smaller changes to the userâ€™s model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°šæœªå°è¯•Varunaå’ŒSageMakerï¼Œä½†ä»–ä»¬çš„è®ºæ–‡æŠ¥å‘Šç§°ï¼Œä»–ä»¬å·²ç»å…‹æœäº†ä¸Šè¿°é—®é¢˜åˆ—è¡¨ï¼Œå¹¶ä¸”éœ€è¦å¯¹ç”¨æˆ·æ¨¡å‹è¿›è¡Œè¾ƒå°çš„æ›´æ”¹ã€‚
- en: 'Implementations:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å®æ–½ï¼š
- en: '[PyTorch](https://pytorch.org/docs/stable/pipeline.html) (initial support in
    pytorch-1.8, and progressively getting improved in 1.9 and more so in 1.10). Some
    [examples](https://github.com/pytorch/pytorch/blob/master/benchmarks/distributed/pipeline/pipe.py)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch](https://pytorch.org/docs/stable/pipeline.html)ï¼ˆåœ¨pytorch-1.8ä¸­æä¾›åˆå§‹æ”¯æŒï¼Œå¹¶åœ¨1.9ä¸­é€æ¸æ”¹è¿›ï¼Œåœ¨1.10ä¸­æ›´æ˜¯å¦‚æ­¤ï¼‰ã€‚ä¸€äº›[ç¤ºä¾‹](https://github.com/pytorch/pytorch/blob/master/benchmarks/distributed/pipeline/pipe.py)'
- en: '[DeepSpeed](https://www.deepspeed.ai/tutorials/pipeline/)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DeepSpeed](https://www.deepspeed.ai/tutorials/pipeline/)'
- en: '[Megatron-LM](https://github.com/NVIDIA/Megatron-LM) has an internal implementation
    - no API.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Megatron-LM](https://github.com/NVIDIA/Megatron-LM)å…·æœ‰å†…éƒ¨å®ç°-æ²¡æœ‰APIã€‚'
- en: '[Varuna](https://github.com/microsoft/varuna)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Varuna](https://github.com/microsoft/varuna)'
- en: '[SageMaker](https://arxiv.org/abs/2111.05972) - this is a proprietary solution
    that can only be used on AWS.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SageMaker](https://arxiv.org/abs/2111.05972) - è¿™æ˜¯ä¸€ç§ä¸“æœ‰è§£å†³æ–¹æ¡ˆï¼Œåªèƒ½åœ¨AWSä¸Šä½¿ç”¨ã€‚'
- en: '[OSLO](https://github.com/tunib-ai/oslo) - this is implemented based on the
    Hugging Face Transformers.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OSLO](https://github.com/tunib-ai/oslo) - è¿™æ˜¯åŸºäºHugging Face Transformerså®ç°çš„ã€‚'
- en: 'ğŸ¤— Transformers status: as of this writing none of the models supports full-PP.
    GPT2 and T5 models have naive MP support. The main obstacle is being unable to
    convert the models to `nn.Sequential` and have all the inputs to be Tensors. This
    is because currently the models include many features that make the conversion
    very complicated, and will need to be removed to accomplish that.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— TransformersçŠ¶æ€ï¼šæˆªè‡³ç›®å‰ï¼Œæ²¡æœ‰æ¨¡å‹æ”¯æŒå®Œæ•´çš„PPã€‚GPT2å’ŒT5æ¨¡å‹å…·æœ‰å¤©çœŸçš„MPæ”¯æŒã€‚ä¸»è¦éšœç¢æ˜¯æ— æ³•å°†æ¨¡å‹è½¬æ¢ä¸º`nn.Sequential`å¹¶ä½¿æ‰€æœ‰è¾“å…¥ä¸ºå¼ é‡ã€‚è¿™æ˜¯å› ä¸ºå½“å‰æ¨¡å‹åŒ…å«è®¸å¤šä½¿è½¬æ¢éå¸¸å¤æ‚çš„ç‰¹æ€§ï¼Œå¹¶ä¸”éœ€è¦å°†å…¶åˆ é™¤æ‰èƒ½å®ç°è½¬æ¢ã€‚
- en: DeepSpeed and Megatron-LM integrations are available in [ğŸ¤— Accelerate](https://huggingface.co/docs/accelerate/main/en/usage_guides/deepspeed)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeedå’ŒMegatron-LMé›†æˆå¯åœ¨[ğŸ¤— Accelerate](https://huggingface.co/docs/accelerate/main/en/usage_guides/deepspeed)ä¸­æ‰¾åˆ°
- en: 'Other approaches:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–æ–¹æ³•ï¼š
- en: DeepSpeed, Varuna and SageMaker use the concept of an [Interleaved Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeedã€Varunaå’ŒSageMakerä½¿ç”¨[äº¤é”™ç®¡é“](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html)çš„æ¦‚å¿µ
- en: '![Interleaved pipeline execution](../Images/fe4c5248602104a9d60765bc134bfd01.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![äº¤é”™ç®¡é“æ‰§è¡Œ](../Images/fe4c5248602104a9d60765bc134bfd01.png)'
- en: Here the bubble (idle time) is further minimized by prioritizing backward passes.
    Varuna further attempts to improve the schedule by using simulations to discover
    the most efficient scheduling.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œé€šè¿‡ä¼˜å…ˆè€ƒè™‘åå‘ä¼ é€’æ¥è¿›ä¸€æ­¥å‡å°‘æ°”æ³¡ï¼ˆç©ºé—²æ—¶é—´ï¼‰ã€‚Varunaé€šè¿‡ä½¿ç”¨æ¨¡æ‹Ÿæ¥å‘ç°æœ€æœ‰æ•ˆçš„è°ƒåº¦æ¥å°è¯•æ”¹è¿›æ—¶é—´è¡¨ã€‚
- en: OSLO has pipeline parallelism implementation based on the Transformers without
    `nn.Sequential` conversion.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: OSLOå…·æœ‰åŸºäºTransformersçš„ç®¡é“å¹¶è¡Œå®ç°ï¼Œæ— éœ€`nn.Sequential`è½¬æ¢ã€‚
- en: Tensor Parallelism
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼ é‡å¹¶è¡Œ
- en: 'In Tensor Parallelism, each GPU processes a slice of a tensor and only aggregates
    the full tensor for operations requiring it. To describe this method, this section
    of the guide relies on the concepts and diagrams from the [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
    paper: [Efficient Large-Scale Language Model Training on GPU Clusters](https://arxiv.org/abs/2104.04473).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼ é‡å¹¶è¡Œä¸­ï¼Œæ¯ä¸ªGPUå¤„ç†å¼ é‡çš„ä¸€ä¸ªåˆ‡ç‰‡ï¼Œå¹¶ä¸”ä»…åœ¨éœ€è¦æ—¶èšåˆå®Œæ•´çš„å¼ é‡è¿›è¡Œæ“ä½œã€‚ä¸ºäº†æè¿°è¿™ç§æ–¹æ³•ï¼Œæœ¬æŒ‡å—çš„è¿™ä¸€éƒ¨åˆ†ä¾èµ–äº[Megatron-LM](https://github.com/NVIDIA/Megatron-LM)è®ºæ–‡ä¸­çš„æ¦‚å¿µå’Œå›¾è¡¨ï¼š[åœ¨GPUé›†ç¾¤ä¸Šè¿›è¡Œé«˜æ•ˆçš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒ](https://arxiv.org/abs/2104.04473)ã€‚
- en: The main building block of any transformer is a fully connected `nn.Linear`
    followed by a nonlinear activation `GeLU`. The dot dot-product part of it, following
    the Megatronâ€™s paper notation, can be written as `Y = GeLU(XA)`, where `X` is
    an input vector, `Y` is the output vector, and `A` is the weight matrix.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ä»»ä½•transformerçš„ä¸»è¦æ„å»ºå—æ˜¯ä¸€ä¸ªå®Œå…¨è¿æ¥çš„`nn.Linear`ï¼Œåé¢è·Ÿç€ä¸€ä¸ªéçº¿æ€§æ¿€æ´»`GeLU`ã€‚å®ƒçš„ç‚¹ç§¯éƒ¨åˆ†ï¼Œæ ¹æ®Megatronçš„è®ºæ–‡ç¬¦å·è¡¨ç¤ºæ³•ï¼Œå¯ä»¥å†™æˆ`Y
    = GeLU(XA)`ï¼Œå…¶ä¸­`X`æ˜¯è¾“å…¥å‘é‡ï¼Œ`Y`æ˜¯è¾“å‡ºå‘é‡ï¼Œ`A`æ˜¯æƒé‡çŸ©é˜µã€‚
- en: 'If we look at the computation in matrix form, you can see how the matrix multiplication
    can be split between multiple GPUs:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ä»¥çŸ©é˜µå½¢å¼çœ‹è®¡ç®—ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°çŸ©é˜µä¹˜æ³•å¦‚ä½•åœ¨å¤šä¸ªGPUä¹‹é—´åˆ†å‰²ï¼š
- en: '![Parallel GEMM](../Images/24e76c89c6e86afa3bc79043047b8082.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![å¹¶è¡ŒGEMM](../Images/24e76c89c6e86afa3bc79043047b8082.png)'
- en: 'If we split the weight matrix `A` column-wise across `N` GPUs and perform matrix
    multiplications `XA_1` through `XA_n` in parallel, then we will end up with `N`
    output vectors `Y_1, Y_2, ..., Y_n` which can be fed into `GeLU` independently:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°†æƒé‡çŸ©é˜µ`A`åœ¨`N`ä¸ªGPUä¸ŠæŒ‰åˆ—åˆ’åˆ†ï¼Œå¹¶åœ¨å¹¶è¡Œæ‰§è¡ŒçŸ©é˜µä¹˜æ³•`XA_1`åˆ°`XA_n`ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†å¾—åˆ°`N`ä¸ªè¾“å‡ºå‘é‡`Y_1, Y_2,
    ..., Y_n`ï¼Œå¯ä»¥ç‹¬ç«‹åœ°è¾“å…¥åˆ°`GeLU`ä¸­ï¼š
- en: '![Independent GeLU](../Images/03ee256eff323b38a197978ce627170d.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![ç‹¬ç«‹GeLU](../Images/03ee256eff323b38a197978ce627170d.png)'
- en: 'Using this principle, we can update a multi-layer perceptron of arbitrary depth,
    without the need for any synchronization between GPUs until the very end, where
    we need to reconstruct the output vector from shards. The Megatron-LM paper authors
    provide a helpful illustration for that:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ä¸ªåŸåˆ™ï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ–°ä»»æ„æ·±åº¦çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œè€Œæ— éœ€åœ¨GPUä¹‹é—´è¿›è¡Œä»»ä½•åŒæ­¥ï¼Œç›´åˆ°æœ€åï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬éœ€è¦ä»ç¢ç‰‡ä¸­é‡å»ºè¾“å‡ºå‘é‡ã€‚Megatron-LMè®ºæ–‡çš„ä½œè€…ä¸ºæ­¤æä¾›äº†ä¸€ä¸ªæœ‰ç”¨çš„æ’å›¾ï¼š
- en: '![Parallel shard processing](../Images/31d292bdb9b6fa0edbf59e631b8b21c8.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![å¹¶è¡Œç¢ç‰‡å¤„ç†](../Images/31d292bdb9b6fa0edbf59e631b8b21c8.png)'
- en: Parallelizing the multi-headed attention layers is even simpler, since they
    are already inherently parallel, due to having multiple independent heads!
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶è¡ŒåŒ–å¤šå¤´æ³¨æ„åŠ›å±‚ç”šè‡³æ›´ç®€å•ï¼Œå› ä¸ºå®ƒä»¬å·²ç»å¤©ç”Ÿæ˜¯å¹¶è¡Œçš„ï¼Œç”±äºå…·æœ‰å¤šä¸ªç‹¬ç«‹çš„å¤´ï¼
- en: '![Parallel self-attention](../Images/6fc3dbac64ede57965d6026e9a2fe8c2.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![å¹¶è¡Œè‡ªæ³¨æ„åŠ›](../Images/6fc3dbac64ede57965d6026e9a2fe8c2.png)'
- en: 'Special considerations: TP requires very fast network, and therefore itâ€™s not
    advisable to do TP across more than one node. Practically, if a node has 4 GPUs,
    the highest TP degree is therefore 4\. If you need a TP degree of 8, you need
    to use nodes that have at least 8 GPUs.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹æ®Šè€ƒè™‘ï¼šTPéœ€è¦éå¸¸å¿«çš„ç½‘ç»œï¼Œå› æ­¤ä¸å»ºè®®åœ¨å¤šä¸ªèŠ‚ç‚¹ä¹‹é—´è¿›è¡ŒTPã€‚å®é™…ä¸Šï¼Œå¦‚æœä¸€ä¸ªèŠ‚ç‚¹æœ‰4ä¸ªGPUï¼Œåˆ™æœ€é«˜çš„TPåº¦æ•°ä¸º4ã€‚å¦‚æœæ‚¨éœ€è¦8çš„TPåº¦æ•°ï¼Œåˆ™éœ€è¦ä½¿ç”¨è‡³å°‘æœ‰8ä¸ªGPUçš„èŠ‚ç‚¹ã€‚
- en: This section is based on the original much more [detailed TP overview](https://github.com/huggingface/transformers/issues/10321#issuecomment-783543530).
    by [@anton-l](https://github.com/anton-l).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚åŸºäºåŸå§‹çš„æ›´è¯¦ç»†çš„[TPæ¦‚è¿°](https://github.com/huggingface/transformers/issues/10321#issuecomment-783543530)ã€‚by
    [@anton-l](https://github.com/anton-l)ã€‚
- en: 'Alternative names:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: æ›¿ä»£åç§°ï¼š
- en: DeepSpeed calls it [tensor slicing](https://www.deepspeed.ai/training/#model-parallelism)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepSpeedç§°ä¹‹ä¸º[å¼ é‡åˆ‡ç‰‡](https://www.deepspeed.ai/training/#model-parallelism)
- en: 'Implementations:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: å®æ–½ï¼š
- en: '[Megatron-LM](https://github.com/NVIDIA/Megatron-LM) has an internal implementation,
    as itâ€™s very model-specific'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Megatron-LM](https://github.com/NVIDIA/Megatron-LM)å…·æœ‰å†…éƒ¨å®ç°ï¼Œå› ä¸ºå®ƒéå¸¸ç‰¹å®šäºæ¨¡å‹ã€‚'
- en: '[parallelformers](https://github.com/tunib-ai/parallelformers) (only inference
    at the moment)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[parallelformers](https://github.com/tunib-ai/parallelformers)ï¼ˆç›®å‰ä»…æ¨ç†ï¼‰'
- en: '[SageMaker](https://arxiv.org/abs/2111.05972) - this is a proprietary solution
    that can only be used on AWS.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SageMaker](https://arxiv.org/abs/2111.05972) - è¿™æ˜¯ä¸€ä¸ªä¸“æœ‰è§£å†³æ–¹æ¡ˆï¼Œåªèƒ½åœ¨AWSä¸Šä½¿ç”¨ã€‚'
- en: '[OSLO](https://github.com/tunib-ai/oslo) has the tensor parallelism implementation
    based on the Transformers.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OSLO](https://github.com/tunib-ai/oslo)å…·æœ‰åŸºäºTransformersçš„å¼ é‡å¹¶è¡Œå®ç°ã€‚'
- en: SageMaker combines TP with DP for a more efficient processing.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: SageMakerå°†TPä¸DPç»“åˆèµ·æ¥ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„å¤„ç†ã€‚
- en: 'ğŸ¤— Transformers status:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— TransformersçŠ¶æ€ï¼š
- en: 'core: not yet implemented in the core'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ¸å¿ƒï¼šæ ¸å¿ƒå°šæœªå®ç°
- en: but if you want inference [parallelformers](https://github.com/tunib-ai/parallelformers)
    provides this support for most of our models. So until this is implemented in
    the core you can use theirs. And hopefully training mode will be supported too.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¦‚æœæ‚¨éœ€è¦æ¨ç†[parallelformers](https://github.com/tunib-ai/parallelformers)ä¸ºæˆ‘ä»¬å¤§å¤šæ•°æ¨¡å‹æä¾›äº†æ”¯æŒã€‚å› æ­¤ï¼Œåœ¨æ ¸å¿ƒå®ç°ä¹‹å‰ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒä»¬çš„æ”¯æŒã€‚å¸Œæœ›è®­ç»ƒæ¨¡å¼ä¹Ÿä¼šå¾—åˆ°æ”¯æŒã€‚
- en: Deepspeed-Inference also supports our BERT, GPT-2, and GPT-Neo models in their
    super-fast CUDA-kernel-based inference mode, see more [here](https://www.deepspeed.ai/tutorials/inference-tutorial/)
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deepspeed-Inferenceè¿˜æ”¯æŒæˆ‘ä»¬çš„BERTã€GPT-2å’ŒGPT-Neoæ¨¡å‹ï¼Œé‡‡ç”¨è¶…å¿«çš„åŸºäºCUDAå†…æ ¸çš„æ¨ç†æ¨¡å¼ï¼Œæ›´å¤šä¿¡æ¯è¯·å‚è§[è¿™é‡Œ](https://www.deepspeed.ai/tutorials/inference-tutorial/)
- en: ğŸ¤— Accelerate integrates with [TP from Megatron-LM](https://huggingface.co/docs/accelerate/v0.23.0/en/usage_guides/megatron_lm).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Accelerateé›†æˆäº†[Megatron-LMçš„TP](https://huggingface.co/docs/accelerate/v0.23.0/en/usage_guides/megatron_lm)ã€‚
- en: Data Parallelism + Pipeline Parallelism
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®å¹¶è¡Œ + ç®¡é“å¹¶è¡Œ
- en: The following diagram from the DeepSpeed [pipeline tutorial](https://www.deepspeed.ai/tutorials/pipeline/)
    demonstrates how one can combine DP with PP.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªDeepSpeed [pipelineæ•™ç¨‹](https://www.deepspeed.ai/tutorials/pipeline/)çš„ä»¥ä¸‹å›¾è¡¨æ¼”ç¤ºäº†å¦‚ä½•å°†DPä¸PPç»“åˆä½¿ç”¨ã€‚
- en: '![DP + PP-2d](../Images/2aea4e58b8f741571e02a891090ecb48.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![DP + PP-2d](../Images/2aea4e58b8f741571e02a891090ecb48.png)'
- en: Here itâ€™s important to see how DP rank 0 doesnâ€™t see GPU2 and DP rank 1 doesnâ€™t
    see GPU3\. To DP there is just GPUs 0 and 1 where it feeds data as if there were
    just 2 GPUs. GPU0 â€œsecretlyâ€ offloads some of its load to GPU2 using PP. And GPU1
    does the same by enlisting GPU3 to its aid.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œé‡è¦çš„æ˜¯çœ‹åˆ°DPç­‰çº§0çœ‹ä¸åˆ°GPU2ï¼ŒDPç­‰çº§1çœ‹ä¸åˆ°GPU3ã€‚å¯¹äºDPæ¥è¯´ï¼Œåªæœ‰GPU 0å’Œ1ï¼Œå®ƒä»¬åƒåªæœ‰2ä¸ªGPUä¸€æ ·æä¾›æ•°æ®ã€‚GPU0â€œç§˜å¯†â€åœ°å°†ä¸€äº›è´Ÿè½½è½¬ç§»åˆ°GPU2ä¸Šï¼Œä½¿ç”¨PPã€‚GPU1ä¹Ÿé€šè¿‡å°†GPU3åˆ—å…¥å…¶æ´åŠ©æ¥åšåŒæ ·çš„äº‹æƒ…ã€‚
- en: Since each dimension requires at least 2 GPUs, here youâ€™d need at least 4 GPUs.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ¯ä¸ªç»´åº¦è‡³å°‘éœ€è¦2ä¸ªGPUï¼Œæ‰€ä»¥è¿™é‡Œè‡³å°‘éœ€è¦4ä¸ªGPUã€‚
- en: 'Implementations:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°ï¼š
- en: '[DeepSpeed](https://github.com/microsoft/DeepSpeed)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DeepSpeed](https://github.com/microsoft/DeepSpeed)'
- en: '[Megatron-LM](https://github.com/NVIDIA/Megatron-LM)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Megatron-LM](https://github.com/NVIDIA/Megatron-LM)'
- en: '[Varuna](https://github.com/microsoft/varuna)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Varuna](https://github.com/microsoft/varuna)'
- en: '[SageMaker](https://arxiv.org/abs/2111.05972)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SageMaker](https://arxiv.org/abs/2111.05972)'
- en: '[OSLO](https://github.com/tunib-ai/oslo)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OSLO](https://github.com/tunib-ai/oslo)'
- en: 'ğŸ¤— Transformers status: not yet implemented'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— TransformersçŠ¶æ€ï¼šå°šæœªå®ç°
- en: Data Parallelism + Pipeline Parallelism + Tensor Parallelism
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®å¹¶è¡Œ + æµæ°´çº¿å¹¶è¡Œ + å¼ é‡å¹¶è¡Œ
- en: To get an even more efficient training a 3D parallelism is used where PP is
    combined with TP and DP. This can be seen in the following diagram.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è·å¾—æ›´é«˜æ•ˆçš„è®­ç»ƒï¼Œä½¿ç”¨äº†3Då¹¶è¡Œï¼Œå…¶ä¸­PPä¸TPå’ŒDPç»“åˆä½¿ç”¨ã€‚å¯ä»¥åœ¨ä»¥ä¸‹å›¾è¡¨ä¸­çœ‹åˆ°ã€‚
- en: '![dp-pp-tp-3d](../Images/576dd96e11eaad105f11f0b16e8458b1.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![dp-pp-tp-3d](../Images/576dd96e11eaad105f11f0b16e8458b1.png)'
- en: 'This diagram is from a blog post [3D parallelism: Scaling to trillion-parameter
    models](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/),
    which is a good read as well.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå›¾è¡¨æ¥è‡ªä¸€ç¯‡åšæ–‡[3Då¹¶è¡Œï¼šæ‰©å±•åˆ°ä¸‡äº¿å‚æ•°æ¨¡å‹](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)ï¼Œä¹Ÿæ˜¯ä¸€ç¯‡å¾ˆå¥½çš„é˜…è¯»ã€‚
- en: Since each dimension requires at least 2 GPUs, here youâ€™d need at least 8 GPUs.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ¯ä¸ªç»´åº¦è‡³å°‘éœ€è¦2ä¸ªGPUï¼Œæ‰€ä»¥è¿™é‡Œè‡³å°‘éœ€è¦8ä¸ªGPUã€‚
- en: 'Implementations:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°ï¼š
- en: '[DeepSpeed](https://github.com/microsoft/DeepSpeed) - DeepSpeed also includes
    an even more efficient DP, which they call ZeRO-DP.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DeepSpeed](https://github.com/microsoft/DeepSpeed) - DeepSpeedè¿˜åŒ…æ‹¬ä¸€ä¸ªæ›´é«˜æ•ˆçš„DPï¼Œä»–ä»¬ç§°ä¹‹ä¸ºZeRO-DPã€‚'
- en: '[Megatron-LM](https://github.com/NVIDIA/Megatron-LM)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Megatron-LM](https://github.com/NVIDIA/Megatron-LM)'
- en: '[Varuna](https://github.com/microsoft/varuna)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Varuna](https://github.com/microsoft/varuna)'
- en: '[SageMaker](https://arxiv.org/abs/2111.05972)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SageMaker](https://arxiv.org/abs/2111.05972)'
- en: '[OSLO](https://github.com/tunib-ai/oslo)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OSLO](https://github.com/tunib-ai/oslo)'
- en: 'ğŸ¤— Transformers status: not yet implemented, since we have no PP and TP.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— TransformersçŠ¶æ€ï¼šå°šæœªå®ç°ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰PPå’ŒTPã€‚
- en: ZeRO Data Parallelism + Pipeline Parallelism + Tensor Parallelism
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ZeROæ•°æ®å¹¶è¡Œ + æµæ°´çº¿å¹¶è¡Œ + å¼ é‡å¹¶è¡Œ
- en: One of the main features of DeepSpeed is ZeRO, which is a super-scalable extension
    of DP. It has already been discussed in [ZeRO Data Parallelism](#zero-data-parallelism).
    Normally itâ€™s a standalone feature that doesnâ€™t require PP or TP. But it can be
    combined with PP and TP.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeedçš„ä¸»è¦ç‰¹ç‚¹ä¹‹ä¸€æ˜¯ZeROï¼Œå®ƒæ˜¯DPçš„ä¸€ä¸ªè¶…å¯æ‰©å±•æ‰©å±•ã€‚å·²ç»åœ¨[ZeROæ•°æ®å¹¶è¡Œ](#zero-data-parallelism)ä¸­è®¨è®ºè¿‡ã€‚é€šå¸¸å®ƒæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„åŠŸèƒ½ï¼Œä¸éœ€è¦PPæˆ–TPã€‚ä½†å®ƒå¯ä»¥ä¸PPå’ŒTPç»“åˆä½¿ç”¨ã€‚
- en: When ZeRO-DP is combined with PP (and optionally TP) it typically enables only
    ZeRO stage 1 (optimizer sharding).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ZeRO-DPä¸PPï¼ˆå’Œå¯é€‰çš„TPï¼‰ç»“åˆæ—¶ï¼Œé€šå¸¸åªå¯ç”¨ZeROé˜¶æ®µ1ï¼ˆä¼˜åŒ–å™¨åˆ†ç‰‡ï¼‰ã€‚
- en: While itâ€™s theoretically possible to use ZeRO stage 2 (gradient sharding) with
    Pipeline Parallelism, it will have negative performance impacts. There would need
    to be an additional reduce-scatter collective for every micro-batch to aggregate
    the gradients before sharding, which adds a potentially significant communication
    overhead. By nature of Pipeline Parallelism, small micro-batches are used and
    instead the focus is on trying to balance arithmetic intensity (micro-batch size)
    with minimizing the Pipeline bubble (number of micro-batches). Therefore those
    communication costs are going to impact the performance.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ç†è®ºä¸Šå¯ä»¥ä½¿ç”¨ZeROé˜¶æ®µ2ï¼ˆæ¢¯åº¦åˆ†ç‰‡ï¼‰ä¸æµæ°´çº¿å¹¶è¡Œï¼Œä½†ä¼šå¯¹æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚æ¯ä¸ªå¾®æ‰¹æ¬¡éƒ½éœ€è¦é¢å¤–çš„reduce-scatteré›†åˆæ¥èšåˆæ¢¯åº¦ï¼Œç„¶åå†è¿›è¡Œåˆ†ç‰‡ï¼Œè¿™ä¼šå¢åŠ æ½œåœ¨çš„æ˜¾è‘—é€šä¿¡å¼€é”€ã€‚ç”±äºæµæ°´çº¿å¹¶è¡Œçš„ç‰¹æ€§ï¼Œä½¿ç”¨å°å¾®æ‰¹æ¬¡ï¼Œè€Œé‡ç‚¹æ˜¯å°è¯•å¹³è¡¡ç®—æœ¯å¼ºåº¦ï¼ˆå¾®æ‰¹æ¬¡å¤§å°ï¼‰å’Œæœ€å°åŒ–æµæ°´çº¿æ°”æ³¡ï¼ˆå¾®æ‰¹æ¬¡æ•°é‡ï¼‰ã€‚å› æ­¤ï¼Œè¿™äº›é€šä¿¡æˆæœ¬å°†å½±å“æ€§èƒ½ã€‚
- en: In addition, there are already fewer layers than normal due to PP and so the
    memory savings wonâ€™t be huge. PP already reduces gradient size by `1/PP`, and
    so gradient sharding savings on top of that are less significant than pure DP.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œç”±äºPPå·²ç»å‡å°‘äº†æ¢¯åº¦å¤§å°`1/PP`ï¼Œæ‰€ä»¥æ¢¯åº¦åˆ†ç‰‡çš„èŠ‚çœå¹¶ä¸åƒçº¯DPé‚£æ ·æ˜¾è‘—ã€‚
- en: ZeRO stage 3 is not a good choice either for the same reason - more inter-node
    communications required.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ZeROé˜¶æ®µ3ä¹Ÿä¸æ˜¯ä¸€ä¸ªå¥½é€‰æ‹©ï¼ŒåŸå› æ˜¯éœ€è¦æ›´å¤šçš„èŠ‚ç‚¹é—´é€šä¿¡ã€‚
- en: And since we have ZeRO, the other benefit is ZeRO-Offload. Since this is stage
    1 optimizer states can be offloaded to CPU.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬æœ‰ZeROï¼Œå¦ä¸€ä¸ªå¥½å¤„æ˜¯ZeRO-Offloadã€‚ç”±äºè¿™æ˜¯é˜¶æ®µ1ä¼˜åŒ–å™¨çŠ¶æ€å¯ä»¥è½¬ç§»åˆ°CPUã€‚
- en: 'Implementations:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°ï¼š
- en: '[Megatron-DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed) and [Megatron-Deepspeed
    from BigScience](https://github.com/bigscience-workshop/Megatron-DeepSpeed), which
    is the fork of the former repo.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Megatron-DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed)å’Œ[BigScienceçš„Megatron-Deepspeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed)ï¼Œè¿™æ˜¯å‰ä¸€ä¸ªå­˜å‚¨åº“çš„åˆ†æ”¯ã€‚'
- en: '[OSLO](https://github.com/tunib-ai/oslo)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OSLO](https://github.com/tunib-ai/oslo)'
- en: 'Important papers:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦è®ºæ–‡ï¼š
- en: '[Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale
    Generative Language Model](https://arxiv.org/abs/2201.11990)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨DeepSpeedå’ŒMegatronè®­ç»ƒMegatron-Turing NLG 530Bï¼Œä¸€ä¸ªå¤§è§„æ¨¡ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹
- en: 'ğŸ¤— Transformers status: not yet implemented, since we have no PP and TP.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— TransformersçŠ¶æ€ï¼šå°šæœªå®ç°ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰PPå’ŒTPã€‚
- en: FlexFlow
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlexFlow
- en: '[FlexFlow](https://github.com/flexflow/FlexFlow) also solves the parallelization
    problem in a slightly different approach.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlexFlow](https://github.com/flexflow/FlexFlow) ä¹Ÿä»¥ç•¥æœ‰ä¸åŒçš„æ–¹å¼è§£å†³äº†å¹¶è¡ŒåŒ–é—®é¢˜ã€‚'
- en: 'Paper: [â€œBeyond Data and Model Parallelism for Deep Neural Networksâ€ by Zhihao
    Jia, Matei Zaharia, Alex Aiken](https://arxiv.org/abs/1807.05358)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ï¼š[â€œBeyond Data and Model Parallelism for Deep Neural Networksâ€ by Zhihao Jia,
    Matei Zaharia, Alex Aiken](https://arxiv.org/abs/1807.05358)
- en: It performs a sort of 4D Parallelism over Sample-Operator-Attribute-Parameter.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ‰§è¡Œä¸€ç§4Då¹¶è¡ŒåŒ–ï¼Œæ¶µç›–æ ·æœ¬-æ“ä½œ-å±æ€§-å‚æ•°ã€‚
- en: Sample = Data Parallelism (sample-wise parallel)
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ ·æœ¬ = æ•°æ®å¹¶è¡ŒåŒ–ï¼ˆæ ·æœ¬æ–¹å‘å¹¶è¡Œï¼‰
- en: Operator = Parallelize a single operation into several sub-operations
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ“ä½œ = å°†å•ä¸ªæ“ä½œå¹¶è¡ŒåŒ–ä¸ºå¤šä¸ªå­æ“ä½œ
- en: Attribute = Data Parallelism (length-wise parallel)
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å±æ€§ = æ•°æ®å¹¶è¡ŒåŒ–ï¼ˆé•¿åº¦æ–¹å‘å¹¶è¡Œï¼‰
- en: Parameter = Model Parallelism (regardless of dimension - horizontal or vertical)
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‚æ•° = æ¨¡å‹å¹¶è¡ŒåŒ–ï¼ˆæ— è®ºç»´åº¦æ˜¯æ°´å¹³è¿˜æ˜¯å‚ç›´ï¼‰
- en: 'Examples:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: Sample
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ·æœ¬
- en: Letâ€™s take 10 batches of sequence length 512\. If we parallelize them by sample
    dimension into 2 devices, we get 10 x 512 which becomes be 5 x 2 x 512.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ‹¿10æ‰¹æ¬¡çš„åºåˆ—é•¿åº¦ä¸º512ã€‚å¦‚æœæˆ‘ä»¬æŒ‰æ ·æœ¬ç»´åº¦å°†å®ƒä»¬å¹¶è¡ŒåŒ–ä¸º2ä¸ªè®¾å¤‡ï¼Œæˆ‘ä»¬å¾—åˆ°10 x 512ï¼Œè¿™å°†å˜ä¸º 5 x 2 x 512ã€‚
- en: Operator
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ“ä½œ
- en: If we perform layer normalization, we compute std first and mean second, and
    then we can normalize data. Operator parallelism allows computing std and mean
    in parallel. So if we parallelize them by operator dimension into 2 devices (cuda:0,
    cuda:1), first we copy input data into both devices, and cuda:0 computes std,
    cuda:1 computes mean at the same time.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æ‰§è¡Œå±‚å½’ä¸€åŒ–ï¼Œé¦–å…ˆè®¡ç®—æ ‡å‡†å·®ï¼Œç„¶åè®¡ç®—å‡å€¼ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ã€‚æ“ä½œå¹¶è¡Œæ€§å…è®¸å¹¶è¡Œè®¡ç®—æ ‡å‡†å·®å’Œå‡å€¼ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬æŒ‰æ“ä½œç»´åº¦å°†å®ƒä»¬å¹¶è¡ŒåŒ–ä¸º2ä¸ªè®¾å¤‡ï¼ˆcuda:0ï¼Œcuda:1ï¼‰ï¼Œé¦–å…ˆå°†è¾“å…¥æ•°æ®å¤åˆ¶åˆ°ä¸¤ä¸ªè®¾å¤‡ä¸­ï¼Œcuda:0åŒæ—¶è®¡ç®—æ ‡å‡†å·®ï¼Œcuda:1è®¡ç®—å‡å€¼ã€‚
- en: Attribute
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å±æ€§
- en: We have 10 batches of 512 length. If we parallelize them by attribute dimension
    into 2 devices, 10 x 512 will be 10 x 2 x 256.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰10æ‰¹æ¬¡ï¼Œæ¯ä¸ªé•¿åº¦ä¸º512ã€‚å¦‚æœæˆ‘ä»¬æŒ‰å±æ€§ç»´åº¦å°†å®ƒä»¬å¹¶è¡ŒåŒ–ä¸º2ä¸ªè®¾å¤‡ï¼Œ10 x 512 å°†å˜ä¸º 10 x 2 x 256ã€‚
- en: Parameter
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: It is similar with tensor model parallelism or naive layer-wise model parallelism.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸å¼ é‡æ¨¡å‹å¹¶è¡ŒåŒ–æˆ–å¤©çœŸçš„é€å±‚æ¨¡å‹å¹¶è¡ŒåŒ–ç±»ä¼¼ã€‚
- en: '![flex-flow-soap](../Images/afd0fc78628855e58b9faf6fa5778e42.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![flex-flow-soap](../Images/afd0fc78628855e58b9faf6fa5778e42.png)'
- en: The significance of this framework is that it takes resources like (1) GPU/TPU/CPU
    vs. (2) RAM/DRAM vs. (3) fast-intra-connect/slow-inter-connect and it automatically
    optimizes all these algorithmically deciding which parallelisation to use where.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¡†æ¶çš„é‡è¦æ€§åœ¨äºï¼Œå®ƒä»¥ç®—æ³•æ–¹å¼å ç”¨èµ„æºï¼Œå¦‚ï¼ˆ1ï¼‰GPU/TPU/CPU vs.ï¼ˆ2ï¼‰RAM/DRAM vs.ï¼ˆ3ï¼‰å¿«é€Ÿå†…éƒ¨è¿æ¥/æ…¢é€Ÿå¤–éƒ¨è¿æ¥ï¼Œå¹¶è‡ªåŠ¨ä¼˜åŒ–æ‰€æœ‰è¿™äº›ï¼Œå†³å®šåœ¨å“ªé‡Œä½¿ç”¨å“ªç§å¹¶è¡ŒåŒ–ã€‚
- en: One very important aspect is that FlexFlow is designed for optimizing DNN parallelizations
    for models with static and fixed workloads, since models with dynamic behavior
    may prefer different parallelization strategies across iterations.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªéå¸¸é‡è¦çš„æ–¹é¢æ˜¯ï¼ŒFlexFlowä¸“ä¸ºä¼˜åŒ–å…·æœ‰é™æ€å’Œå›ºå®šå·¥ä½œè´Ÿè½½çš„DNNå¹¶è¡ŒåŒ–è€Œè®¾è®¡ï¼Œå› ä¸ºå…·æœ‰åŠ¨æ€è¡Œä¸ºçš„æ¨¡å‹å¯èƒ½ä¼šåœ¨è¿­ä»£ä¸­æ›´å–œæ¬¢ä¸åŒçš„å¹¶è¡ŒåŒ–ç­–ç•¥ã€‚
- en: So the promise is very attractive - it runs a 30min simulation on the cluster
    of choice and it comes up with the best strategy to utilise this specific environment.
    If you add/remove/replace any parts itâ€™ll run and re-optimize the plan for that.
    And then you can train. A different setup will have its own custom optimization.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™ä¸ªæ‰¿è¯ºéå¸¸å¸å¼•äºº - å®ƒåœ¨æ‰€é€‰é›†ç¾¤ä¸Šè¿è¡Œ30åˆ†é’Ÿçš„æ¨¡æ‹Ÿï¼Œå¹¶æå‡ºäº†æœ€ä½³ç­–ç•¥æ¥åˆ©ç”¨è¿™ä¸ªç‰¹å®šç¯å¢ƒã€‚å¦‚æœæ·»åŠ /åˆ é™¤/æ›¿æ¢ä»»ä½•éƒ¨åˆ†ï¼Œå®ƒå°†è¿è¡Œå¹¶é‡æ–°ä¼˜åŒ–è¯¥è®¡åˆ’ã€‚ç„¶åæ‚¨å¯ä»¥è¿›è¡Œè®­ç»ƒã€‚ä¸åŒçš„è®¾ç½®å°†æœ‰è‡ªå·±çš„å®šåˆ¶ä¼˜åŒ–ã€‚
- en: 'ğŸ¤— Transformers status: Transformers models are FX-trace-able via [transformers.utils.fx](https://github.com/huggingface/transformers/blob/master/src/transformers/utils/fx.py),
    which is a prerequisite for FlexFlow, however, changes are required on the FlexFlow
    side to make it work with Transformers models.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Transformers çŠ¶æ€ï¼šTransformers æ¨¡å‹å¯ä»¥é€šè¿‡[transformers.utils.fx](https://github.com/huggingface/transformers/blob/master/src/transformers/utils/fx.py)è¿›è¡Œ
    FX-trace-ableï¼Œè¿™æ˜¯FlexFlowçš„å…ˆå†³æ¡ä»¶ï¼Œä½†éœ€è¦åœ¨FlexFlowæ–¹é¢è¿›è¡Œæ›´æ”¹ä»¥ä½¿å…¶ä¸Transformersæ¨¡å‹é…åˆä½¿ç”¨ã€‚
- en: GPU selection
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPUé€‰æ‹©
- en: When training on multiple GPUs, you can specify the number of GPUs to use and
    in what order. This can be useful for instance when you have GPUs with different
    computing power and want to use the faster GPU first. The selection process works
    for both [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
    and [DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)
    to use only a subset of the available GPUs, and you donâ€™t need Accelerate or the
    [DeepSpeed integration](./main_classes/deepspeed).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤šä¸ªGPUä¸Šè®­ç»ƒæ—¶ï¼Œæ‚¨å¯ä»¥æŒ‡å®šè¦ä½¿ç”¨çš„GPUæ•°é‡å’Œé¡ºåºã€‚ä¾‹å¦‚ï¼Œå½“æ‚¨æœ‰è®¡ç®—èƒ½åŠ›ä¸åŒçš„GPUå¹¶å¸Œæœ›é¦–å…ˆä½¿ç”¨é€Ÿåº¦æ›´å¿«çš„GPUæ—¶ï¼Œè¿™å¯èƒ½å¾ˆæœ‰ç”¨ã€‚é€‰æ‹©è¿‡ç¨‹é€‚ç”¨äº[DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)å’Œ[DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)ï¼Œä»¥ä»…ä½¿ç”¨å¯ç”¨GPUçš„å­é›†ï¼Œæ‚¨ä¸éœ€è¦Accelerateæˆ–[DeepSpeed
    integration](./main_classes/deepspeed)ã€‚
- en: Number of GPUs
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPUçš„æ•°é‡
- en: 'For example, if you have 4 GPUs and you only want to use the first 2:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰4ä¸ªGPUï¼Œä½†åªæƒ³ä½¿ç”¨å‰2ä¸ªï¼š
- en: torchrunAccelerateDeepSpeed
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: torchrunAccelerateDeepSpeed
- en: Use the `--nproc_per_node` to select how many GPUs to use.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`--nproc_per_node`æ¥é€‰æ‹©ä½¿ç”¨å¤šå°‘ä¸ªGPUã€‚
- en: '[PRE7]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Order of GPUs
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPUçš„é¡ºåº
- en: 'Now, to select which GPUs to use and their order, youâ€™ll use the `CUDA_VISIBLE_DEVICES`
    environment variable. It is easiest to set the environment variable in a `~/bashrc`
    or another startup config file. `CUDA_VISIBLE_DEVICES` is used to map which GPUs
    are used. For example, if you have 4 GPUs (0, 1, 2, 3) and you only want to run
    GPUs 0 and 2:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè¦é€‰æ‹©è¦ä½¿ç”¨çš„GPUåŠå…¶é¡ºåºï¼Œæ‚¨å°†ä½¿ç”¨`CUDA_VISIBLE_DEVICES`ç¯å¢ƒå˜é‡ã€‚æœ€ç®€å•çš„æ–¹æ³•æ˜¯åœ¨`~/bashrc`æˆ–å…¶ä»–å¯åŠ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®ç¯å¢ƒå˜é‡ã€‚`CUDA_VISIBLE_DEVICES`ç”¨äºæ˜ å°„è¦ä½¿ç”¨çš„GPUã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰4ä¸ªGPUï¼ˆ0ã€1ã€2ã€3ï¼‰ï¼Œä½†åªæƒ³è¿è¡ŒGPU
    0å’Œ 2ï¼š
- en: '[PRE8]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Only the 2 physical GPUs (0 and 2) are â€œvisibleâ€ to PyTorch and these are mapped
    to `cuda:0` and `cuda:1` respectively. You can also reverse the order of the GPUs
    to use 2 first. Now, the mapping is `cuda:1` for GPU 0 and `cuda:0` for GPU 2.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰2ä¸ªç‰©ç†GPUï¼ˆ0å’Œ2ï¼‰å¯¹PyTorchæ˜¯â€œå¯è§çš„â€ï¼Œå®ƒä»¬åˆ†åˆ«æ˜ å°„åˆ°`cuda:0`å’Œ`cuda:1`ã€‚æ‚¨è¿˜å¯ä»¥é¢ å€’GPUçš„é¡ºåºä»¥å…ˆä½¿ç”¨2ä¸ªã€‚ç°åœ¨ï¼Œæ˜ å°„æ˜¯GPU
    0ä¸º`cuda:1`ï¼ŒGPU 2ä¸º`cuda:0`ã€‚
- en: '[PRE9]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can also set the `CUDA_VISIBLE_DEVICES` environment variable to an empty
    value to create an environment without GPUs.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥å°† `CUDA_VISIBLE_DEVICES` ç¯å¢ƒå˜é‡è®¾ç½®ä¸ºç©ºå€¼ï¼Œä»¥åˆ›å»ºä¸€ä¸ªæ²¡æœ‰ GPU çš„ç¯å¢ƒã€‚
- en: '[PRE10]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As with any environment variable, they can be exported instead of being added
    to the command line. However, this is not recommended because it can be confusing
    if you forget how the environment variable was setup and you end up using the
    wrong GPUs. Instead, it is common practice to set the environment variable for
    a specific training run on the same command line.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä»»ä½•ç¯å¢ƒå˜é‡ä¸€æ ·ï¼Œå®ƒä»¬å¯ä»¥è¢«å¯¼å‡ºï¼Œè€Œä¸æ˜¯æ·»åŠ åˆ°å‘½ä»¤è¡Œä¸­ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ¨èï¼Œå› ä¸ºå¦‚æœæ‚¨å¿˜è®°äº†ç¯å¢ƒå˜é‡çš„è®¾ç½®æ–¹å¼ï¼Œæœ€ç»ˆä½¿ç”¨äº†é”™è¯¯çš„ GPUï¼Œä¼šè®©äººæ„Ÿåˆ°å›°æƒ‘ã€‚ç›¸åï¼Œé€šå¸¸çš„åšæ³•æ˜¯åœ¨åŒä¸€å‘½ä»¤è¡Œä¸Šä¸ºç‰¹å®šçš„è®­ç»ƒè¿è¡Œè®¾ç½®ç¯å¢ƒå˜é‡ã€‚
- en: '`CUDA_DEVICE_ORDER` is an alternative environment variable you can use to control
    how the GPUs are ordered. You can either order them by:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`CUDA_DEVICE_ORDER` æ˜¯ä¸€ä¸ªæ›¿ä»£ç¯å¢ƒå˜é‡ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒæ¥æ§åˆ¶ GPU çš„é¡ºåºã€‚æ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼š'
- en: PCIe bus IDâ€™s that matches the order of [`nvidia-smi`](https://developer.nvidia.com/nvidia-system-management-interface)
    and [`rocm-smi`](https://rocm.docs.amd.com/projects/rocm_smi_lib/en/latest/.doxygen/docBin/html/index.html)
    for NVIDIA and AMD GPUs respectively
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸ [`nvidia-smi`](https://developer.nvidia.com/nvidia-system-management-interface)
    å’Œ [`rocm-smi`](https://rocm.docs.amd.com/projects/rocm_smi_lib/en/latest/.doxygen/docBin/html/index.html)
    åˆ†åˆ«åŒ¹é…çš„ PCIe æ€»çº¿ IDï¼Œç”¨äº NVIDIA å’Œ AMD GPU
- en: '[PRE11]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: GPU compute ability
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPU è®¡ç®—èƒ½åŠ›
- en: '[PRE12]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `CUDA_DEVICE_ORDER` is especially useful if your training setup consists
    of an older and newer GPU, where the older GPU appears first, but you cannot physically
    swap the cards to make the newer GPU appear first. In this case, set `CUDA_DEVICE_ORDER=FASTEST_FIRST`
    to always use the newer and faster GPU first (`nvidia-smi` or `rocm-smi` still
    reports the GPUs in their PCIe order). Or you could also set `export CUDA_VISIBLE_DEVICES=1,0`.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„è®­ç»ƒè®¾ç½®åŒ…æ‹¬ä¸€å°è¾ƒæ—§å’Œä¸€å°è¾ƒæ–°çš„ GPUï¼Œå…¶ä¸­è¾ƒæ—§çš„ GPU æ˜¾ç¤ºåœ¨å‰ï¼Œä½†æ‚¨æ— æ³•ç‰©ç†äº¤æ¢å¡ç‰‡ä½¿è¾ƒæ–°çš„ GPU æ˜¾ç¤ºåœ¨å‰ï¼Œé‚£ä¹ˆ `CUDA_DEVICE_ORDER`
    å°±ç‰¹åˆ«æœ‰ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®¾ç½® `CUDA_DEVICE_ORDER=FASTEST_FIRST`ï¼Œå§‹ç»ˆä½¿ç”¨è¾ƒæ–°å’Œæ›´å¿«çš„ GPUï¼ˆ`nvidia-smi`
    æˆ– `rocm-smi` ä»ç„¶æŒ‰ç…§ PCIe é¡ºåºæŠ¥å‘Š GPUï¼‰ã€‚æˆ–è€…æ‚¨ä¹Ÿå¯ä»¥è®¾ç½® `export CUDA_VISIBLE_DEVICES=1,0`ã€‚
