- en: Video classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è§†é¢‘åˆ†ç±»
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/video_classification](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/video_classification)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/video_classification](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/video_classification)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Video classification is the task of assigning a label or class to an entire
    video. Videos are expected to have only one class for each video. Video classification
    models take a video as input and return a prediction about which class the video
    belongs to. These models can be used to categorize what a video is all about.
    A real-world application of video classification is action / activity recognition,
    which is useful for fitness applications. It is also helpful for vision-impaired
    individuals, especially when they are commuting.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è§†é¢‘åˆ†ç±»æ˜¯å°†æ ‡ç­¾æˆ–ç±»åˆ«åˆ†é…ç»™æ•´ä¸ªè§†é¢‘çš„ä»»åŠ¡ã€‚é¢„æœŸæ¯ä¸ªè§†é¢‘åªæœ‰ä¸€ä¸ªç±»åˆ«ã€‚è§†é¢‘åˆ†ç±»æ¨¡å‹å°†è§†é¢‘ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›å…³äºè§†é¢‘å±äºå“ªä¸ªç±»åˆ«çš„é¢„æµ‹ã€‚è¿™äº›æ¨¡å‹å¯ç”¨äºå¯¹è§†é¢‘å†…å®¹è¿›è¡Œåˆ†ç±»ã€‚è§†é¢‘åˆ†ç±»çš„ç°å®åº”ç”¨æ˜¯åŠ¨ä½œ/æ´»åŠ¨è¯†åˆ«ï¼Œå¯¹äºå¥èº«åº”ç”¨éå¸¸æœ‰ç”¨ã€‚å¯¹äºè§†åŠ›å—æŸçš„ä¸ªä½“ï¼Œå°¤å…¶æ˜¯åœ¨é€šå‹¤æ—¶ï¼Œè¿™ä¹Ÿæ˜¯æœ‰å¸®åŠ©çš„ã€‚
- en: 'This guide will show you how to:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š
- en: Fine-tune [VideoMAE](https://huggingface.co/docs/transformers/main/en/model_doc/videomae)
    on a subset of the [UCF101](https://www.crcv.ucf.edu/data/UCF101.php) dataset.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨[UCF101](https://www.crcv.ucf.edu/data/UCF101.php)æ•°æ®é›†çš„å­é›†ä¸Šå¯¹[VideoMAE](https://huggingface.co/docs/transformers/main/en/model_doc/videomae)è¿›è¡Œå¾®è°ƒã€‚
- en: Use your fine-tuned model for inference.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚
- en: 'The task illustrated in this tutorial is supported by the following model architectures:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹ä¸­æ‰€ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡å‹æ¶æ„æ”¯æŒï¼š
- en: '[TimeSformer](../model_doc/timesformer), [VideoMAE](../model_doc/videomae),
    [ViViT](../model_doc/vivit)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[TimeSformer](../model_doc/timesformer), [VideoMAE](../model_doc/videomae),
    [ViViT](../model_doc/vivit)'
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You will use [PyTorchVideo](https://pytorchvideo.org/) (dubbed `pytorchvideo`)
    to process and prepare the videos.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å°†ä½¿ç”¨[PyTorchVideo](https://pytorchvideo.org/)ï¼ˆç§°ä¸º`pytorchvideo`ï¼‰æ¥å¤„ç†å’Œå‡†å¤‡è§†é¢‘ã€‚
- en: 'We encourage you to log in to your Hugging Face account so you can upload and
    share your model with the community. When prompted, enter your token to log in:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„Hugging Faceå¸æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å’Œä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚æç¤ºæ—¶ï¼Œè¯·è¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Load UCF101 dataset
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½UCF101æ•°æ®é›†
- en: Start by loading a subset of the [UCF-101 dataset](https://www.crcv.ucf.edu/data/UCF101.php).
    This will give you a chance to experiment and make sure everything works before
    spending more time training on the full dataset.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆåŠ è½½[UCF-101æ•°æ®é›†](https://www.crcv.ucf.edu/data/UCF101.php)çš„å­é›†ã€‚è¿™å°†è®©æ‚¨æœ‰æœºä¼šè¿›è¡Œå®éªŒï¼Œå¹¶ç¡®ä¿ä¸€åˆ‡æ­£å¸¸ï¼Œç„¶åå†èŠ±æ›´å¤šæ—¶é—´åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After the subset has been downloaded, you need to extract the compressed archive:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹è½½å­é›†åï¼Œæ‚¨éœ€è¦æå–å‹ç¼©å­˜æ¡£ï¼š
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'At a high level, the dataset is organized like so:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é«˜å±‚æ¬¡ä¸Šï¼Œæ•°æ®é›†çš„ç»„ç»‡æ–¹å¼å¦‚ä¸‹ï¼š
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The (`sorted`) video paths appear like so:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆæ’åºåçš„ï¼‰è§†é¢‘è·¯å¾„çœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You will notice that there are video clips belonging to the same group / scene
    where group is denoted by `g` in the video file paths. `v_ApplyEyeMakeup_g07_c04.avi`
    and `v_ApplyEyeMakeup_g07_c06.avi`, for example.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ä¼šæ³¨æ„åˆ°æœ‰å±äºåŒä¸€ç»„/åœºæ™¯çš„è§†é¢‘ç‰‡æ®µï¼Œå…¶ä¸­ç»„åœ¨è§†é¢‘æ–‡ä»¶è·¯å¾„ä¸­ç”¨`g`è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œ`v_ApplyEyeMakeup_g07_c04.avi`å’Œ`v_ApplyEyeMakeup_g07_c06.avi`ã€‚
- en: For the validation and evaluation splits, you wouldnâ€™t want to have video clips
    from the same group / scene to prevent [data leakage](https://www.kaggle.com/code/alexisbcook/data-leakage).
    The subset that you are using in this tutorial takes this information into account.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºéªŒè¯å’Œè¯„ä¼°æ‹†åˆ†ï¼Œæ‚¨ä¸å¸Œæœ›ä»åŒä¸€ç»„/åœºæ™¯ä¸­è·å–è§†é¢‘ç‰‡æ®µï¼Œä»¥é˜²æ­¢[æ•°æ®æ³„æ¼](https://www.kaggle.com/code/alexisbcook/data-leakage)ã€‚æœ¬æ•™ç¨‹ä¸­ä½¿ç”¨çš„å­é›†è€ƒè™‘äº†è¿™äº›ä¿¡æ¯ã€‚
- en: 'Next up, you will derive the set of labels present in the dataset. Also, create
    two dictionaries thatâ€™ll be helpful when initializing the model:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæ‚¨å°†æ¨å¯¼æ•°æ®é›†ä¸­å­˜åœ¨çš„æ ‡ç­¾é›†ã€‚è¿˜è¦åˆ›å»ºä¸¤ä¸ªåœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶æœ‰ç”¨çš„å­—å…¸ï¼š
- en: '`label2id`: maps the class names to integers.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label2id`ï¼šå°†ç±»åæ˜ å°„åˆ°æ•´æ•°ã€‚'
- en: '`id2label`: maps the integers to class names.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id2label`ï¼šå°†æ•´æ•°æ˜ å°„åˆ°ç±»åã€‚'
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: There are 10 unique classes. For each class, there are 30 videos in the training
    set.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰10ä¸ªç‹¬ç‰¹çš„ç±»åˆ«ã€‚æ¯ä¸ªç±»åˆ«åœ¨è®­ç»ƒé›†ä¸­æœ‰30ä¸ªè§†é¢‘ã€‚
- en: Load a model to fine-tune
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½ä¸€ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒ
- en: Instantiate a video classification model from a pretrained checkpoint and its
    associated image processor. The modelâ€™s encoder comes with pre-trained parameters,
    and the classification head is randomly initialized. The image processor will
    come in handy when writing the preprocessing pipeline for our dataset.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é¢„è®­ç»ƒçš„æ£€æŸ¥ç‚¹å’Œå…¶å…³è”çš„å›¾åƒå¤„ç†å™¨å®ä¾‹åŒ–ä¸€ä¸ªè§†é¢‘åˆ†ç±»æ¨¡å‹ã€‚æ¨¡å‹çš„ç¼–ç å™¨å¸¦æœ‰é¢„è®­ç»ƒå‚æ•°ï¼Œåˆ†ç±»å¤´æ˜¯éšæœºåˆå§‹åŒ–çš„ã€‚å½“ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†ç¼–å†™é¢„å¤„ç†æµæ°´çº¿æ—¶ï¼Œå›¾åƒå¤„ç†å™¨ä¼šæ´¾ä¸Šç”¨åœºã€‚
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'While the model is loading, you might notice the following warning:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ¨¡å‹åŠ è½½æ—¶ï¼Œæ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°ä»¥ä¸‹è­¦å‘Šï¼š
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The warning is telling us we are throwing away some weights (e.g. the weights
    and bias of the `classifier` layer) and randomly initializing some others (the
    weights and bias of a new `classifier` layer). This is expected in this case,
    because we are adding a new head for which we donâ€™t have pretrained weights, so
    the library warns us we should fine-tune this model before using it for inference,
    which is exactly what we are going to do.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è­¦å‘Šå‘Šè¯‰æˆ‘ä»¬ï¼Œæˆ‘ä»¬æ­£åœ¨ä¸¢å¼ƒä¸€äº›æƒé‡ï¼ˆä¾‹å¦‚`classifier`å±‚çš„æƒé‡å’Œåå·®ï¼‰ï¼Œå¹¶éšæœºåˆå§‹åŒ–å…¶ä»–ä¸€äº›æƒé‡å’Œåå·®ï¼ˆæ–°`classifier`å±‚çš„æƒé‡å’Œåå·®ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™æ˜¯é¢„æœŸçš„ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨æ·»åŠ ä¸€ä¸ªæ–°çš„å¤´éƒ¨ï¼Œæˆ‘ä»¬æ²¡æœ‰é¢„è®­ç»ƒçš„æƒé‡ï¼Œæ‰€ä»¥åº“è­¦å‘Šæˆ‘ä»¬åœ¨ä½¿ç”¨å®ƒè¿›è¡Œæ¨æ–­ä¹‹å‰åº”è¯¥å¾®è°ƒè¿™ä¸ªæ¨¡å‹ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬è¦åšçš„ã€‚
- en: '**Note** that [this checkpoint](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics)
    leads to better performance on this task as the checkpoint was obtained fine-tuning
    on a similar downstream task having considerable domain overlap. You can check
    out [this checkpoint](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset)
    which was obtained by fine-tuning `MCG-NJU/videomae-base-finetuned-kinetics`.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¯·æ³¨æ„**ï¼Œ[æ­¤æ£€æŸ¥ç‚¹](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics)åœ¨æ­¤ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼Œå› ä¸ºè¯¥æ£€æŸ¥ç‚¹æ˜¯åœ¨ä¸€ä¸ªå…·æœ‰ç›¸å½“å¤§é¢†åŸŸé‡å çš„ç±»ä¼¼ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¾®è°ƒå¾—åˆ°çš„ã€‚æ‚¨å¯ä»¥æŸ¥çœ‹[æ­¤æ£€æŸ¥ç‚¹](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset)ï¼Œè¯¥æ£€æŸ¥ç‚¹æ˜¯é€šè¿‡å¾®è°ƒ`MCG-NJU/videomae-base-finetuned-kinetics`è·å¾—çš„ã€‚'
- en: Prepare the datasets for training
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºè®­ç»ƒå‡†å¤‡æ•°æ®é›†
- en: For preprocessing the videos, you will leverage the [PyTorchVideo library](https://pytorchvideo.org/).
    Start by importing the dependencies we need.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¯¹è§†é¢‘è¿›è¡Œé¢„å¤„ç†ï¼Œæ‚¨å°†åˆ©ç”¨[PyTorchVideoåº“](https://pytorchvideo.org/)ã€‚é¦–å…ˆå¯¼å…¥æˆ‘ä»¬éœ€è¦çš„ä¾èµ–é¡¹ã€‚
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For the training dataset transformations, use a combination of uniform temporal
    subsampling, pixel normalization, random cropping, and random horizontal flipping.
    For the validation and evaluation dataset transformations, keep the same transformation
    chain except for random cropping and horizontal flipping. To learn more about
    the details of these transformations check out the [official documentation of
    PyTorchVideo](https://pytorchvideo.org).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè®­ç»ƒæ•°æ®é›†çš„è½¬æ¢ï¼Œä½¿ç”¨ç»Ÿä¸€çš„æ—¶é—´å­é‡‡æ ·ã€åƒç´ å½’ä¸€åŒ–ã€éšæœºè£å‰ªå’Œéšæœºæ°´å¹³ç¿»è½¬çš„ç»„åˆã€‚å¯¹äºéªŒè¯å’Œè¯„ä¼°æ•°æ®é›†çš„è½¬æ¢ï¼Œä¿æŒç›¸åŒçš„è½¬æ¢é“¾ï¼Œé™¤äº†éšæœºè£å‰ªå’Œæ°´å¹³ç¿»è½¬ã€‚è¦äº†è§£è¿™äº›è½¬æ¢çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[PyTorchVideoçš„å®˜æ–¹æ–‡æ¡£](https://pytorchvideo.org)ã€‚
- en: 'Use the `image_processor` associated with the pre-trained model to obtain the
    following information:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸é¢„è®­ç»ƒæ¨¡å‹ç›¸å…³è”çš„`image_processor`æ¥è·å–ä»¥ä¸‹ä¿¡æ¯ï¼š
- en: Image mean and standard deviation with which the video frame pixels will be
    normalized.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨äºå½’ä¸€åŒ–è§†é¢‘å¸§åƒç´ çš„å›¾åƒå‡å€¼å’Œæ ‡å‡†å·®ã€‚
- en: Spatial resolution to which the video frames will be resized.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†è§†é¢‘å¸§è°ƒæ•´ä¸ºçš„ç©ºé—´åˆ†è¾¨ç‡ã€‚
- en: Start by defining some constants.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆå®šä¹‰ä¸€äº›å¸¸é‡ã€‚
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, define the dataset-specific transformations and the datasets respectively.
    Starting with the training set:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œåˆ†åˆ«å®šä¹‰æ•°æ®é›†ç‰¹å®šçš„è½¬æ¢å’Œæ•°æ®é›†ã€‚ä»è®­ç»ƒé›†å¼€å§‹ï¼š
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The same sequence of workflow can be applied to the validation and evaluation
    sets:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åŒçš„å·¥ä½œæµç¨‹é¡ºåºå¯ä»¥åº”ç”¨äºéªŒè¯é›†å’Œè¯„ä¼°é›†ï¼š
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Note**: The above dataset pipelines are taken from the [official PyTorchVideo
    example](https://pytorchvideo.org/docs/tutorial_classification#dataset). Weâ€™re
    using the [`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101)
    function because itâ€™s tailored for the UCF-101 dataset. Under the hood, it returns
    a [`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset)
    object. `LabeledVideoDataset` class is the base class for all things video in
    the PyTorchVideo dataset. So, if you want to use a custom dataset not supported
    off-the-shelf by PyTorchVideo, you can extend the `LabeledVideoDataset` class
    accordingly. Refer to the `data` API [documentation to](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html)
    learn more. Also, if your dataset follows a similar structure (as shown above),
    then using the `pytorchvideo.data.Ucf101()` should work just fine.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„**ï¼šä¸Šè¿°æ•°æ®é›†ç®¡é“å–è‡ª[å®˜æ–¹PyTorchVideoç¤ºä¾‹](https://pytorchvideo.org/docs/tutorial_classification#dataset)ã€‚æˆ‘ä»¬ä½¿ç”¨[`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101)å‡½æ•°ï¼Œå› ä¸ºå®ƒä¸“ä¸ºUCF-101æ•°æ®é›†å®šåˆ¶ã€‚åœ¨å†…éƒ¨ï¼Œå®ƒè¿”å›ä¸€ä¸ª[`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset)å¯¹è±¡ã€‚`LabeledVideoDataset`ç±»æ˜¯PyTorchVideoæ•°æ®é›†ä¸­æ‰€æœ‰è§†é¢‘ç›¸å…³å†…å®¹çš„åŸºç±»ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨æƒ³ä½¿ç”¨PyTorchVideoä¸æ”¯æŒçš„è‡ªå®šä¹‰æ•°æ®é›†ï¼Œå¯ä»¥ç›¸åº”åœ°æ‰©å±•`LabeledVideoDataset`ç±»ã€‚è¯·å‚è€ƒ`data`
    API [æ–‡æ¡£](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html)ä»¥äº†è§£æ›´å¤šã€‚æ­¤å¤–ï¼Œå¦‚æœæ‚¨çš„æ•°æ®é›†éµå¾ªç±»ä¼¼çš„ç»“æ„ï¼ˆå¦‚ä¸Šæ‰€ç¤ºï¼‰ï¼Œé‚£ä¹ˆä½¿ç”¨`pytorchvideo.data.Ucf101()`åº”è¯¥å¯ä»¥æ­£å¸¸å·¥ä½œã€‚'
- en: You can access the `num_videos` argument to know the number of videos in the
    dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥è®¿é—®`num_videos`å‚æ•°ä»¥äº†è§£æ•°æ®é›†ä¸­çš„è§†é¢‘æ•°é‡ã€‚
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Visualize the preprocessed video for better debugging
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–é¢„å¤„ç†åçš„è§†é¢‘ä»¥è¿›è¡Œæ›´å¥½çš„è°ƒè¯•
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Person playing basketball](../Images/094cb43675960282973ed2c77587e204.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![æ‰“ç¯®çƒçš„äºº](../Images/094cb43675960282973ed2c77587e204.png)'
- en: Train the model
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹
- en: Leverage [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer)
    from ğŸ¤— Transformers for training the model. To instantiate a `Trainer`, you need
    to define the training configuration and an evaluation metric. The most important
    is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments),
    which is a class that contains all the attributes to configure the training. It
    requires an output folder name, which will be used to save the checkpoints of
    the model. It also helps sync all the information in the model repository on ğŸ¤—
    Hub.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ©ç”¨ğŸ¤— Transformersä¸­çš„[`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer)æ¥è®­ç»ƒæ¨¡å‹ã€‚è¦å®ä¾‹åŒ–ä¸€ä¸ª`Trainer`ï¼Œæ‚¨éœ€è¦å®šä¹‰è®­ç»ƒé…ç½®å’Œä¸€ä¸ªè¯„ä¼°æŒ‡æ ‡ã€‚æœ€é‡è¦çš„æ˜¯[`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰å±æ€§ä»¥é…ç½®è®­ç»ƒçš„ç±»ã€‚å®ƒéœ€è¦ä¸€ä¸ªè¾“å‡ºæ–‡ä»¶å¤¹åç§°ï¼Œç”¨äºä¿å­˜æ¨¡å‹çš„æ£€æŸ¥ç‚¹ã€‚å®ƒè¿˜æœ‰åŠ©äºå°†æ¨¡å‹å­˜å‚¨åº“ä¸­çš„æ‰€æœ‰ä¿¡æ¯åŒæ­¥åˆ°ğŸ¤—
    Hubä¸­ã€‚
- en: Most of the training arguments are self-explanatory, but one that is quite important
    here is `remove_unused_columns=False`. This one will drop any features not used
    by the modelâ€™s call function. By default itâ€™s `True` because usually itâ€™s ideal
    to drop unused feature columns, making it easier to unpack inputs into the modelâ€™s
    call function. But, in this case, you need the unused features (â€˜videoâ€™ in particular)
    in order to create `pixel_values` (which is a mandatory key our model expects
    in its inputs).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°è®­ç»ƒå‚æ•°éƒ½æ˜¯ä¸è¨€è‡ªæ˜çš„ï¼Œä½†è¿™é‡Œæœ‰ä¸€ä¸ªéå¸¸é‡è¦çš„å‚æ•°æ˜¯`remove_unused_columns=False`ã€‚è¿™ä¸ªå‚æ•°å°†åˆ é™¤æ¨¡å‹è°ƒç”¨å‡½æ•°æœªä½¿ç”¨çš„ä»»ä½•ç‰¹å¾ã€‚é»˜è®¤æƒ…å†µä¸‹æ˜¯`True`ï¼Œå› ä¸ºé€šå¸¸æœ€å¥½åˆ é™¤æœªä½¿ç”¨çš„ç‰¹å¾åˆ—ï¼Œè¿™æ ·æ›´å®¹æ˜“å°†è¾“å…¥è§£å‹ç¼©åˆ°æ¨¡å‹çš„è°ƒç”¨å‡½æ•°ä¸­ã€‚ä½†æ˜¯ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦æœªä½¿ç”¨çš„ç‰¹å¾ï¼ˆç‰¹åˆ«æ˜¯â€˜videoâ€™ï¼‰ä»¥ä¾¿åˆ›å»º`pixel_values`ï¼ˆè¿™æ˜¯æˆ‘ä»¬çš„æ¨¡å‹åœ¨è¾“å…¥ä¸­æœŸæœ›çš„ä¸€ä¸ªå¿…éœ€é”®ï¼‰ã€‚
- en: '[PRE15]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The dataset returned by `pytorchvideo.data.Ucf101()` doesnâ€™t implement the `__len__`
    method. As such, we must define `max_steps` when instantiating `TrainingArguments`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`pytorchvideo.data.Ucf101()`è¿”å›çš„æ•°æ®é›†æ²¡æœ‰å®ç°`__len__`æ–¹æ³•ã€‚å› æ­¤ï¼Œåœ¨å®ä¾‹åŒ–`TrainingArguments`æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»å®šä¹‰`max_steps`ã€‚'
- en: 'Next, you need to define a function to compute the metrics from the predictions,
    which will use the `metric` youâ€™ll load now. The only preprocessing you have to
    do is to take the argmax of our predicted logits:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—ä»é¢„æµ‹ä¸­å¾—å‡ºçš„æŒ‡æ ‡ï¼Œè¯¥å‡½æ•°å°†ä½¿ç”¨æ‚¨ç°åœ¨å°†åŠ è½½çš„`metric`ã€‚æ‚¨å”¯ä¸€éœ€è¦åšçš„é¢„å¤„ç†æ˜¯å–å‡ºæˆ‘ä»¬é¢„æµ‹çš„logitsçš„argmaxï¼š
- en: '[PRE16]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**A note on evaluation**:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…³äºè¯„ä¼°çš„è¯´æ˜**ï¼š'
- en: In the [VideoMAE paper](https://arxiv.org/abs/2203.12602), the authors use the
    following evaluation strategy. They evaluate the model on several clips from test
    videos and apply different crops to those clips and report the aggregate score.
    However, in the interest of simplicity and brevity, we donâ€™t consider that in
    this tutorial.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[VideoMAEè®ºæ–‡](https://arxiv.org/abs/2203.12602)ä¸­ï¼Œä½œè€…ä½¿ç”¨ä»¥ä¸‹è¯„ä¼°ç­–ç•¥ã€‚ä»–ä»¬åœ¨æµ‹è¯•è§†é¢‘çš„å‡ ä¸ªå‰ªè¾‘ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œå¹¶å¯¹è¿™äº›å‰ªè¾‘åº”ç”¨ä¸åŒçš„è£å‰ªï¼Œå¹¶æŠ¥å‘Šèšåˆå¾—åˆ†ã€‚ç„¶è€Œï¼Œå‡ºäºç®€å•å’Œç®€æ´çš„è€ƒè™‘ï¼Œæˆ‘ä»¬åœ¨æœ¬æ•™ç¨‹ä¸­ä¸è€ƒè™‘è¿™ä¸€ç‚¹ã€‚
- en: Also, define a `collate_fn`, which will be used to batch examples together.
    Each batch consists of 2 keys, namely `pixel_values` and `labels`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå®šä¹‰ä¸€ä¸ª`collate_fn`ï¼Œç”¨äºå°†ç¤ºä¾‹æ‰¹å¤„ç†åœ¨ä¸€èµ·ã€‚æ¯ä¸ªæ‰¹æ¬¡åŒ…æ‹¬2ä¸ªé”®ï¼Œå³`pixel_values`å’Œ`labels`ã€‚
- en: '[PRE17]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then you just pass all of this along with the datasets to `Trainer`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œå°†æ‰€æœ‰è¿™äº›ä¸æ•°æ®é›†ä¸€èµ·ä¼ é€’ç»™`Trainer`ï¼š
- en: '[PRE18]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You might wonder why you passed along the `image_processor` as a tokenizer when
    you preprocessed the data already. This is only to make sure the image processor
    configuration file (stored as JSON) will also be uploaded to the repo on the Hub.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯èƒ½æƒ³çŸ¥é“ä¸ºä»€ä¹ˆåœ¨é¢„å¤„ç†æ•°æ®æ—¶å°†`image_processor`ä½œä¸ºæ ‡è®°å™¨ä¼ é€’ã€‚è¿™åªæ˜¯ä¸ºäº†ç¡®ä¿å›¾åƒå¤„ç†å™¨é…ç½®æ–‡ä»¶ï¼ˆå­˜å‚¨ä¸ºJSONï¼‰ä¹Ÿå°†ä¸Šä¼ åˆ°Hubä¸Šçš„å­˜å‚¨åº“ä¸­ã€‚
- en: 'Now fine-tune our model by calling the `train` method:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨é€šè¿‡è°ƒç”¨`train`æ–¹æ³•å¯¹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼š
- en: '[PRE19]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Once training is completed, share your model to the Hub with the [push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)
    method so everyone can use your model:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ°Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š
- en: '[PRE20]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Inference
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨æ–­
- en: Great, now that you have fine-tuned a model, you can use it for inference!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œç°åœ¨æ‚¨å·²ç»å¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¯ä»¥å°†å…¶ç”¨äºæ¨æ–­ï¼
- en: 'Load a video for inference:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½è§†é¢‘è¿›è¡Œæ¨æ–­ï¼š
- en: '[PRE21]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Teams playing basketball](../Images/27c05b85bfaaba1e373898da43772d52.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![ç¯®çƒæ¯”èµ›çš„é˜Ÿä¼](../Images/27c05b85bfaaba1e373898da43772d52.png)'
- en: 'The simplest way to try out your fine-tuned model for inference is to use it
    in a [`pipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.VideoClassificationPipeline).
    Instantiate a `pipeline` for video classification with your model, and pass your
    video to it:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å°è¯•ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨æ–­çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨[`pipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.VideoClassificationPipeline)ä¸­ä½¿ç”¨å®ƒã€‚ä½¿ç”¨æ‚¨çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªè§†é¢‘åˆ†ç±»çš„`pipeline`ï¼Œå¹¶å°†è§†é¢‘ä¼ é€’ç»™å®ƒï¼š
- en: '[PRE22]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You can also manually replicate the results of the `pipeline` if youâ€™d like.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ„¿æ„ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶`pipeline`çš„ç»“æœã€‚
- en: '[PRE23]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, pass your input to the model and return the `logits`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›`logits`ï¼š
- en: '[PRE24]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Decoding the `logits`, we get:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç `logits`ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
- en: '[PRE25]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
