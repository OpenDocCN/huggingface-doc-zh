- en: Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/peft/package_reference/peft_model](https://huggingface.co/docs/peft/package_reference/peft_model)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/peft/v0.8.2/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/start.c9bed6ec.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/scheduler.d627b047.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/singletons.95cf6adf.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.a57a1c33.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/paths.5d07c46f.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/app.72c78cae.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.d48c4817.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/0.aa346fde.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/28.0dbe16f8.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Tip.9bd3babf.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Docstring.270658d8.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Heading.47e562a9.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/CodeBlock.5da89496.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/ExampleCodeBlock.a22db1c3.js">
  prefs: []
  type: TYPE_NORMAL
- en: '[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    is the base model class for specifying the base Transformer model and configuration
    to apply a PEFT method to. The base `PeftModel` contains methods for loading and
    saving models from the Hub.'
  prefs: []
  type: TYPE_NORMAL
- en: PeftModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.PeftModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L88)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — The base transformer model used for Peft.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` ([PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig))
    — The configuration of the Peft model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`, *optional*) — The name of the adapter, defaults to `"default"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base model encompassing various Peft methods.
  prefs: []
  type: TYPE_NORMAL
- en: '**Attributes**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`base_model` (`torch.nn.Module`) — The base transformer model used for Peft.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` ([PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig))
    — The configuration of the Peft model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modules_to_save` (`list` of `str`) — The list of sub-module names to save
    when saving the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_encoder` ([PromptEncoder](/docs/peft/v0.8.2/en/package_reference/p_tuning#peft.PromptEncoder))
    — The prompt encoder used for Peft if using [PromptLearningConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PromptLearningConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_tokens` (`torch.Tensor`) — The virtual prompt tokens used for Peft
    if using [PromptLearningConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PromptLearningConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformer_backbone_name` (`str`) — The name of the transformer backbone
    in the base model if using [PromptLearningConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PromptLearningConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`word_embeddings` (`torch.nn.Embedding`) — The word embeddings of the transformer
    backbone in the base model if using [PromptLearningConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PromptLearningConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `add_adapter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L587)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`) — The name of the adapter to be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` ([PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig))
    — The configuration of the adapter to be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add an adapter to the model based on the passed configuration.
  prefs: []
  type: TYPE_NORMAL
- en: The name for the new adapter should be unique.
  prefs: []
  type: TYPE_NORMAL
- en: The new adapter is not automatically set as the active adapter. Use [PeftModel.set_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.set_adapter)
    to set the active adapter.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `create_or_update_model_card`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L777)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Updates or create model card to include information about peft:'
  prefs: []
  type: TYPE_NORMAL
- en: Adds `peft` library tag
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adds peft version
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adds base model info
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adds quantization information if it was used
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '#### `disable_adapter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L547)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Context manager that disables the adapter module. Use this to run inference
    on the base model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L533)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Forward pass of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L283)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model to be adapted. For 🤗 Transformers models,
    the model should be initialized with the [from_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_id` (`str` or `os.PathLike`) — The name of the PEFT configuration to
    use. Can be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the `model id` of a PEFT configuration hosted inside a model repo
    on the Hugging Face Hub.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a directory containing a PEFT configuration file saved using the `save_pretrained`
    method (`./my_peft_config_directory/`).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`, *optional*, defaults to `"default"`) — The name of the
    adapter to be loaded. This is useful for loading multiple adapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_trainable` (`bool`, *optional*, defaults to `False`) — Whether the adapter
    should be trainable or not. If `False`, the adapter will be frozen and can only
    be used for inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config` ([PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig),
    *optional*) — The configuration object to use instead of an automatically loaded
    configuration. This configuration object is mutually exclusive with `model_id`
    and `kwargs`. This is useful when configuration is already loaded before calling
    `from_pretrained`. kwargs — (`optional`): Additional keyword arguments passed
    along to the specific PEFT configuration class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate a PEFT model from a pretrained model and loaded PEFT weights.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the passed `model` may be modified inplace.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_base_model`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L577)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Returns the base model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_nb_trainable_parameters`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L492)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Returns the number of trainable parameters and the number of all parameters
    in the model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_prompt`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L446)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Returns the virtual prompts to use for Peft. Only applicable when using a prompt
    learning method.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_prompt_embedding_to_save`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L427)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Returns the prompt embedding to save when saving the model. Only applicable
    when using a prompt learning method.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `load_adapter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L652)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`) — The name of the adapter to be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` ([PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig))
    — The configuration of the adapter to be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_trainable` (`bool`, *optional*, defaults to `False`) — Whether the adapter
    should be trainable or not. If `False`, the adapter will be frozen and can only
    be used for inference. kwargs — (`optional`): Additional arguments to modify the
    way the adapter is loaded, e.g. the token for Hugging Face Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load a trained adapter into the model.
  prefs: []
  type: TYPE_NORMAL
- en: The name for the new adapter should be unique.
  prefs: []
  type: TYPE_NORMAL
- en: The new adapter is not automatically set as the active adapter. Use [PeftModel.set_adapter()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.set_adapter)
    to set the active adapter.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `print_trainable_parameters`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L516)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Prints the number of trainable parameters in the model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L161)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`save_directory` (`str`) — Directory where the adapter model and configuration
    files will be saved (will be created if it does not exist).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safe_serialization` (`bool`, *optional*) — Whether to save the adapter files
    in safetensors format, defaults to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`selected_adapters` (`List[str]`, *optional*) — A list of adapters to be saved.
    If `None`, will default to all adapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save_embedding_layers` (`Union[bool, str]`, *optional*, defaults to `"auto"`)
    — If `True`, save the embedding layers in addition to adapter weights. If `auto`,
    checks the common embedding layers `peft.utils.other.EMBEDDING_LAYER_NAMES` in
    config’s `target_modules` when available. and automatically sets the boolean flag.
    This only works for 🤗 transformers models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_main_process` (`bool`, *optional*) — Whether the process calling this is
    the main process or not. Will default to `True`. Will not save the checkpoint
    if not on the main process, which is important for multi device setups (e.g. DDP).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (additional keyword arguments, *optional*) — Additional keyword arguments
    passed along to the `push_to_hub` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This function saves the adapter model and the adapter configuration files to
    a directory, so that it can be reloaded using the [PeftModel.from_pretrained()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.from_pretrained)
    class method, and also used by the `PeftModel.push_to_hub()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_adapter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L743)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`) — The name of the adapter to be set as active. The adapter
    must be loaded first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sets the active adapter.
  prefs: []
  type: TYPE_NORMAL
- en: Only one adapter can be active at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, this function will set the specified adapter to trainable (i.e.,
    requires_grad=True). If this is not desired, use the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: PeftModelForSequenceClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `PeftModel` for sequence classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class peft.PeftModelForSequenceClassification`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L830)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — Base transformer model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` ([PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig))
    — Peft config.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peft model for sequence classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Attributes**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PretrainedConfig](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The configuration object of the base model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls_layer_name` (`str`) — The name of the classification layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: PeftModelForTokenClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `PeftModel` for token classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class peft.PeftModelForTokenClassification`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L1460)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — Base transformer model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` ([PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig))
    — Peft config.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peft model for token classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Attributes**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PretrainedConfig](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The configuration object of the base model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls_layer_name` (`str`) — The name of the classification layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: PeftModelForCausalLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `PeftModel` for causal language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class peft.PeftModelForCausalLM`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L1021)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — Base transformer model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` ([PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig))
    — Peft config.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peft model for causal language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: PeftModelForSeq2SeqLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `PeftModel` for sequence-to-sequence language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class peft.PeftModelForSeq2SeqLM`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L1211)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — Base transformer model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` ([PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig))
    — Peft config.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peft model for sequence-to-sequence language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: PeftModelForQuestionAnswering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `PeftModel` for question answering.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class peft.PeftModelForQuestionAnswering`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L1635)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — Base transformer model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` ([PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig))
    — Peft config.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peft model for extractive question answering.
  prefs: []
  type: TYPE_NORMAL
- en: '**Attributes**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PretrainedConfig](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The configuration object of the base model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls_layer_name` (`str`) — The name of the classification layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: PeftModelForFeatureExtraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `PeftModel` for getting extracting features/embeddings from transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class peft.PeftModelForFeatureExtraction`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L1830)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — Base transformer model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` ([PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig))
    — Peft config.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peft model for extracting features/embeddings from transformer models
  prefs: []
  type: TYPE_NORMAL
- en: '**Attributes**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PretrainedConfig](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The configuration object of the base model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: PeftMixedModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `PeftModel` for mixing different adapter types (e.g. LoRA and LoHa).
  prefs: []
  type: TYPE_NORMAL
- en: '### `class peft.PeftMixedModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L83)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model to be tuned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config` (`PeftConfig`) — The config of the model to be tuned. The adapter
    type must be compatible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`, `optional`, defaults to `"default"`) — The name of the
    first adapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PeftMixedModel for loading mixing different types of adapters for inference.
  prefs: []
  type: TYPE_NORMAL
- en: This class does not support loading/saving, and it shouldn’t usually be initialized
    directly. Instead, use `get_peft_model` with the argument `mixed=True`.
  prefs: []
  type: TYPE_NORMAL
- en: Read the [Mixed adapter types](https://huggingface.co/docs/peft/en/developer_guides/mixed_models)
    guide to learn more about using different adapter types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '#### `disable_adapter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L203)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Disables the adapter module.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L191)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Forward pass of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L329)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`nn.Module`) — The model to be adapted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_id` (`str` or `os.PathLike`) — The name of the PEFT configuration to
    use. Can be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the `model id` of a PEFT configuration hosted inside a model repo
    on the Hugging Face Hub.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a directory containing a PEFT configuration file saved using the `save_pretrained`
    method (`./my_peft_config_directory/`).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`, *optional*, defaults to `"default"`) — The name of the
    adapter to be loaded. This is useful for loading multiple adapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_trainable` (`bool`, *optional*, defaults to `False`) — Whether the adapter
    should be trainable or not. If `False`, the adapter will be frozen and use for
    inference'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config` ([PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig),
    *optional*) — The configuration object to use instead of an automatically loaded
    configuration. This configuration object is mutually exclusive with `model_id`
    and `kwargs`. This is useful when configuration is already loaded before calling
    `from_pretrained`. kwargs — (`optional`): Additional keyword arguments passed
    along to the specific PEFT configuration class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate a PEFT mixed model from a pretrained model and loaded PEFT weights.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the passed `model` may be modified inplace.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L197)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Generate output.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_nb_trainable_parameters`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L146)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Returns the number of trainable parameters and number of all parameters in the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `merge_and_unload`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L283)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`progressbar` (`bool`) — whether to show a progressbar indicating the unload
    and merge process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safe_merge` (`bool`) — whether to activate the safe merging check to check
    if there is any potential Nan in the adapter weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_names` (`List[str]`, *optional*) — The list of adapter names that
    should be merged. If None, all active adapters will be merged. Defaults to `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method merges the adapter layers into the base model. This is needed if
    someone wants to use the base model as a standalone model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `print_trainable_parameters`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L171)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Prints the number of trainable parameters in the model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_adapter`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L237)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`adapter_name` (`str` or `List[str]`) — The name of the adapter(s) to be activated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sets the active adapter(s) for the model.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the order in which the adapters are applied during the forward pass
    may not be the same as the order in which they are passed to this function. Instead,
    the order during the forward pass is determined by the order in which the adapters
    were loaded into the model. The active adapters only determine which adapters
    are active during the forward pass, but not the order in which they are applied.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, this function will set the specified adapters to trainable (i.e.,
    requires_grad=True). If this is not desired, use the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '#### `unload`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L300)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Gets back the base model by removing all the adapter modules without merging.
    This gives back the original base model.
  prefs: []
  type: TYPE_NORMAL
- en: Utilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '#### `peft.cast_mixed_precision_params`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/utils/other.py#L523)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model to cast the non-trainable parameters
    of.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`torch.dtype`) — The dtype to cast the non-trainable parameters to.
    The `dtype` can be `torch.float16` or'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cast all non-trainable parameters of the model to the given `dtype`. The `dtype`
    can be `torch.float16` or `torch.bfloat16` as per the mixed-precision training
    you are performing. The trainable parameters are cast to full precision. This
    is meant to reduce the GPU memory usage when using PEFT methods by using half-precision
    dtype for non-trainable parameters. Having the trainable parameters in full-precision
    preserves training stability when using automatic mixed-precision training.
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.bfloat16` as per the mixed-precision training you are performing.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `peft.get_peft_model`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mapping.py#L106)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([transformers.PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — Model to be wrapped.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` ([PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig))
    — Configuration object containing the parameters of the Peft model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`, `optional`, defaults to `"default"`) — The name of the
    adapter to be injected, if not provided, the default adapter name is used (“default”).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mixed` (`bool`, `optional`, defaults to `False`) — Whether to allow mixing
    different (compatible) adapter types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns a Peft model object from a model and a config.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `peft.inject_adapter_in_model`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mapping.py#L139)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`peft_config` (`PeftConfig`) — Configuration object containing the parameters
    of the Peft model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The input model where the adapter will be injected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`, `optional`, defaults to `"default"`) — The name of the
    adapter to be injected, if not provided, the default adapter name is used (“default”).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A simple API to create and inject adapter in-place into a model. Currently the
    API does not support prompt learning methods and adaption prompt. Make sure to
    have the correct `target_names` set in the `peft_config` object. The API calls
    `get_peft_model` under the hood but would be restricted only to non-prompt learning
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `peft.get_peft_model_state_dict`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/utils/save_and_load.py#L46)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel))
    — The Peft model. When using torch.nn.DistributedDataParallel, DeepSpeed or FSDP,
    the model should be the underlying model/unwrapped model (i.e. model.module).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`state_dict` (`dict`, *optional*, defaults to `None`) — The state dict of the
    model. If not provided, the state dict of the passed model will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`, *optional*, defaults to `"default"`) — The name of the
    adapter whose state dict should be returned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unwrap_compiled` (`bool`, *optional*, defaults to `False`) — Whether to unwrap
    the model if torch.compile was used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save_embedding_layers` (`Union[bool, str]`, , *optional*, defaults to `auto`)
    — If `True`, save the embedding layers in addition to adapter weights. If `auto`,
    checks the common embedding layers `peft.utils.other.EMBEDDING_LAYER_NAMES` in
    config’s `target_modules` when available. Based on it sets the boolean flag. This
    only works for 🤗 transformers models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the state dict of the Peft model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `peft.prepare_model_for_kbit_training`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/utils/other.py#L77)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`transformers.PreTrainedModel`) — The loaded model from `transformers`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_gradient_checkpointing` (`bool`, *optional*, defaults to `True`) — If
    True, use gradient checkpointing to save memory at the expense of slower backward
    pass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gradient_checkpointing_kwargs` (`dict`, *optional*, defaults to `None`) —
    Keyword arguments to pass to the gradient checkpointing function, please refer
    to the documentation of `torch.utils.checkpoint.checkpoint` for more details about
    the arguments that you can pass to that method. Note this is only available in
    the latest transformers versions (> 4.34.1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note this method only works for `transformers` models.
  prefs: []
  type: TYPE_NORMAL
- en: 'This method wraps the entire protocol for preparing a model before running
    a training. This includes: 1- Cast the layernorm in fp32 2- making output embedding
    layer require grads 3- Add the upcasting of the lm head to fp32'
  prefs: []
  type: TYPE_NORMAL
