- en: Introducing Q-Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/q-learning](https://huggingface.co/learn/deep-rl-course/unit2/q-learning)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: What is Q-Learning?
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Q-Learning is an **off-policy value-based method that uses a TD approach to
    train its action-value function:**
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '*Off-policy*: we’ll talk about that at the end of this unit.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Value-based method*: finds the optimal policy indirectly by training a value
    or action-value function that will tell us **the value of each state or each state-action
    pair.**'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TD approach:* **updates its action-value function at each step instead of
    at the end of the episode.**'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Q-Learning is the algorithm we use to train our Q-function**, an **action-value
    function** that determines the value of being at a particular state and taking
    a specific action at that state.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-function](../Images/d98598351e812f60049067f862f79c69.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: Given a state and action, our Q Function outputs a state-action value (also
    called Q-value)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The **Q comes from “the Quality” (the value) of that action at that state.**
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s recap the difference between value and reward:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: The *value of a state*, or a *state-action pair* is the expected cumulative
    reward our agent gets if it starts at this state (or state-action pair) and then
    acts accordingly to its policy.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *reward* is the **feedback I get from the environment** after performing
    an action at a state.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Internally, our Q-function is encoded by **a Q-table, a table where each cell
    corresponds to a state-action pair value.** Think of this Q-table as **the memory
    or cheat sheet of our Q-function.**
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through an example of a maze.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Maze example](../Images/444a72a9a003265b612877410c530a95.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: The Q-table is initialized. That’s why all values are = 0\. This table **contains,
    for each state and action, the corresponding state-action values.** For this simple
    example, the state is only defined by the position of the mouse. Therefore, we
    have 2*3 rows in our Q-table, one row for each possible position of the mouse.
    In more complex scenarios, the state could contain more information than the position
    of the actor.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![Maze example](../Images/cd71daf8ad310d839422ce684bcb5c10.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Here we see that the **state-action value of the initial state and going up
    is 0:**
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![Maze example](../Images/6b2018a29d1825bf6cbb14397a596eae.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: 'So: the Q-function uses a Q-table **that has the value of each state-action
    pair.** Given a state and action, **our Q-function will search inside its Q-table
    to output the value.**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-function](../Images/c6f51357ba01781edc9f3041b33e5be4.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: If we recap, *Q-Learning* **is the RL algorithm that:**
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Trains a *Q-function* (an **action-value function**), which internally is a **Q-table
    that contains all the state-action pair values.**
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a state and action, our Q-function **will search its Q-table for the corresponding
    value.**
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the training is done, **we have an optimal Q-function, which means we have
    optimal Q-table.**
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And if we **have an optimal Q-function**, we **have an optimal policy** since
    we **know the best action to take at each state.**
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Link value policy](../Images/06e7785cc764e6109bfc6c89005a4d92.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: In the beginning, **our Q-table is useless since it gives arbitrary values for
    each state-action pair** (most of the time, we initialize the Q-table to 0). As
    the agent **explores the environment and we update the Q-table, it will give us
    a better and better approximation** to the optimal policy.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-learning](../Images/5eeae1d543b1ae0da4b6f9afe9fe07c9.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: We see here that with the training, our Q-table is better since, thanks to it,
    we can know the value of each state-action pair.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand what Q-Learning, Q-functions, and Q-tables are, **let’s
    dive deeper into the Q-Learning algorithm**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: The Q-Learning algorithm
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the Q-Learning pseudocode; let’s study each part and **see how it works
    with a simple example before implementing it.** Don’t be intimidated by it, it’s
    simpler than it looks! We’ll go over each step.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Q-Learning伪代码；让我们在实现之前研究每个部分，**看看在实现之前如何使用一个简单的例子。**不要被吓到，它比看起来简单！我们将逐步讲解每个步骤。
- en: '![Q-learning](../Images/e98aadd735672374a66857c170d3b2ce.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![Q-learning](../Images/e98aadd735672374a66857c170d3b2ce.png)'
- en: 'Step 1: We initialize the Q-table'
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤1：我们初始化Q表
- en: '![Q-learning](../Images/01250a85fb5041af0c4b2aaf8c987543.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![Q-learning](../Images/01250a85fb5041af0c4b2aaf8c987543.png)'
- en: We need to initialize the Q-table for each state-action pair. **Most of the
    time, we initialize with values of 0.**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为每个状态-动作对初始化Q表。**大多数情况下，我们使用值为0进行初始化。**
- en: 'Step 2: Choose an action using the epsilon-greedy strategy'
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤2：使用ε-贪心策略选择一个动作
- en: '![Q-learning](../Images/30b0aba4490af7f85f0594dc198e9c03.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![Q-learning](../Images/30b0aba4490af7f85f0594dc198e9c03.png)'
- en: The epsilon-greedy strategy is a policy that handles the exploration/exploitation
    trade-off.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ε-贪心策略是一种处理探索/利用权衡的策略。
- en: 'The idea is that, with an initial value of ɛ = 1.0:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是，初始值为ɛ=1.0时：
- en: '*With probability 1 — ɛ* : we do **exploitation** (aka our agent selects the
    action with the highest state-action pair value).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*概率为1-ε*：我们进行**利用**（即我们的代理选择具有最高状态-动作对值的动作）。'
- en: With probability ɛ: **we do exploration** (trying random action).
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以概率ɛ：**我们进行探索**（尝试随机动作）。
- en: At the beginning of the training, **the probability of doing exploration will
    be huge since ɛ is very high, so most of the time, we’ll explore.** But as the
    training goes on, and consequently our **Q-table gets better and better in its
    estimations, we progressively reduce the epsilon value** since we will need less
    and less exploration and more exploitation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练开始时，**由于ɛ非常高，进行探索的概率将非常大，因此大部分时间我们将进行探索。**但随着训练的进行，因此我们的**Q表在估计方面变得越来越好，我们逐渐减少ε值**，因为我们将需要越来越少的探索和更多的利用。
- en: '![Q-learning](../Images/4d0d8f523643ebe543960e9ea3a2a4b7.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![Q-learning](../Images/4d0d8f523643ebe543960e9ea3a2a4b7.png)'
- en: 'Step 3: Perform action At, get reward Rt+1 and next state St+1'
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤3：执行动作At，获得奖励Rt+1和下一个状态St+1
- en: '![Q-learning](../Images/f834496430b9ed9ec65c64061d432454.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![Q-learning](../Images/f834496430b9ed9ec65c64061d432454.png)'
- en: 'Step 4: Update Q(St, At)'
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤4：更新Q(St, At)
- en: Remember that in TD Learning, we update our policy or value function (depending
    on the RL method we choose) **after one step of the interaction.**
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '请记住，在TD学习中，我们在一步交互之后更新我们的策略或值函数（取决于我们选择的RL方法）。 '
- en: To produce our TD target, **we used the immediate reward<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​ plus
    the discounted value of the next state**, computed by finding the action that
    maximizes the current Q-function at the next state. (We call that bootstrap).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成我们的TD目标，**我们使用了即时奖励<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​加上下一个状态的折现值**，通过找到最大化下一个状态当前Q函数的动作来计算。（我们称之为自举）。
- en: '![Q-learning](../Images/bdb95512c529cc741a6379b8a07de2b0.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![Q-learning](../Images/bdb95512c529cc741a6379b8a07de2b0.png)'
- en: Therefore, our<math><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    separator="true">,</mo><msub><mi>A</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">Q(S_t, A_t)</annotation></semantics></math>Q(St​,At​) **update
    formula goes like this:**
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的<math><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    separator="true">,</mo><msub><mi>A</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">Q(S_t, A_t)</annotation></semantics></math>Q(St​,At​)
    **更新公式如下：**
- en: '![Q-learning](../Images/0bfe74186cd45c67a2935f61d93e0937.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![Q-learning](../Images/0bfe74186cd45c67a2935f61d93e0937.png)'
- en: 'This means that to update our<math><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    separator="true">,</mo><msub><mi>A</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">Q(S_t, A_t)</annotation></semantics></math>Q(St​,At​):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着为了更新我们的<math><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    separator="true">,</mo><msub><mi>A</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">Q(S_t, A_t)</annotation></semantics></math>Q(St​,At​)：
- en: We need<math><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>A</mi><mi>t</mi></msub><mo
    separator="true">,</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    separator="true">,</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">S_t, A_t, R_{t+1}, S_{t+1}</annotation></semantics></math>St​,At​,Rt+1​,St+1​.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要<math><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>A</mi><mi>t</mi></msub><mo
    separator="true">,</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    separator="true">,</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">S_t, A_t, R_{t+1}, S_{t+1}</annotation></semantics></math>St​,At​,Rt+1​,St+1​。
- en: To update our Q-value at a given state-action pair, we use the TD target.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了更新给定状态-动作对的Q值，我们使用TD目标。
- en: How do we form the TD target?
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何形成TD目标？
- en: We obtain the reward<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​ after
    taking the action<math><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">A_t</annotation></semantics></math>At​.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在执行动作<math><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">A_t</annotation></semantics></math>At后获得奖励<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​。
- en: To get the **best state-action pair value** for the next state, we use a greedy
    policy to select the next best action. Note that this is not an epsilon-greedy
    policy, this will always take the action with the highest state-action value.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了获得下一个状态的**最佳状态-动作对值**，我们使用贪心策略选择下一个最佳动作。请注意，这不是ε-贪心策略，这将始终选择具有最高状态-动作值的动作。
- en: Then when the update of this Q-value is done, we start in a new state and select
    our action **using a epsilon-greedy policy again.**
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后当这个Q值更新完成后，我们从一个新状态开始，并再次使用ε-贪心策略选择我们的动作。
- en: '**This is why we say that Q Learning is an off-policy algorithm.**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**这就是为什么我们说Q学习是一种离线策略算法。**'
- en: Off-policy vs On-policy
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离线策略 vs 在线策略
- en: 'The difference is subtle:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种差异微妙：
- en: '*Off-policy*: using **a different policy for acting (inference) and updating
    (training).**'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*离线策略*：在行动（推理）和更新（训练）中使用**不同的策略。**'
- en: For instance, with Q-Learning, the epsilon-greedy policy (acting policy), is
    different from the greedy policy that is **used to select the best next-state
    action value to update our Q-value (updating policy).**
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于Q学习，ε-贪心策略（行动策略）与贪心策略不同，后者**用于选择最佳的下一个状态动作值来更新我们的Q值（更新策略）。**
- en: '![Off-on policy](../Images/962ba1bb9bae9ace4837824a79c98010.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![离线策略](../Images/962ba1bb9bae9ace4837824a79c98010.png)'
- en: Acting Policy
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 行动策略
- en: 'Is different from the policy we use during the training part:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在训练过程中使用的策略不同：
- en: '![Off-on policy](../Images/e14d304b020044952e6f8de20e71e925.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![离线策略](../Images/e14d304b020044952e6f8de20e71e925.png)'
- en: Updating policy
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 更新策略
- en: '*On-policy:* using the **same policy for acting and updating.**'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在线策略*：在行动和更新中使用**相同的策略。**'
- en: For instance, with Sarsa, another value-based algorithm, **the epsilon-greedy
    policy selects the next state-action pair, not a greedy policy.**
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于Sarsa，另一个基于值的算法，**ε-贪心策略选择下一个状态-动作对，而不是贪心策略。**
- en: '![Off-on policy](../Images/b57fef603ca195b7a4707f530c6082f7.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![离线策略](../Images/b57fef603ca195b7a4707f530c6082f7.png)'
- en: Sarsa
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Sarsa
- en: '![Off-on policy](../Images/ce691ce98ae89b58669eb975be3f446c.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![离线策略](../Images/ce691ce98ae89b58669eb975be3f446c.png)'
