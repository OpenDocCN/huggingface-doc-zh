# BertJapanese

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bert-japanese](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bert-japanese)

## 概述

在日文文本上训练的BERT模型。

有两种不同的分词方法的模型：

+   使用MeCab和WordPiece进行标记化。这需要一些额外的依赖项，[fugashi](https://github.com/polm/fugashi)是[MeCab](https://taku910.github.io/mecab/)的包装器。

+   将标记化为字符。

要使用*MecabTokenizer*，您应该`pip install transformers["ja"]`（或者如果您从源代码安装，则应该`pip install -e .["ja"]`）来安装依赖项。

请参阅[cl-tohoku存储库上的详细信息](https://github.com/cl-tohoku/bert-japanese)。

使用MeCab和WordPiece分词的模型的示例：

```py
>>> import torch
>>> from transformers import AutoModel, AutoTokenizer

>>> bertjapanese = AutoModel.from_pretrained("cl-tohoku/bert-base-japanese")
>>> tokenizer = AutoTokenizer.from_pretrained("cl-tohoku/bert-base-japanese")

>>> ## Input Japanese Text
>>> line = "吾輩は猫である。"

>>> inputs = tokenizer(line, return_tensors="pt")

>>> print(tokenizer.decode(inputs["input_ids"][0]))
[CLS] 吾輩 は 猫 で ある 。 [SEP]

>>> outputs = bertjapanese(**inputs)
```

使用字符分词的模型示例：

```py
>>> bertjapanese = AutoModel.from_pretrained("cl-tohoku/bert-base-japanese-char")
>>> tokenizer = AutoTokenizer.from_pretrained("cl-tohoku/bert-base-japanese-char")

>>> ## Input Japanese Text
>>> line = "吾輩は猫である。"

>>> inputs = tokenizer(line, return_tensors="pt")

>>> print(tokenizer.decode(inputs["input_ids"][0]))
[CLS] 吾 輩 は 猫 で あ る 。 [SEP]

>>> outputs = bertjapanese(**inputs)
```

这个模型是由[cl-tohoku](https://huggingface.co/cl-tohoku)贡献的。

这个实现与BERT相同，只是分词方法不同。有关API参考信息，请参考[BERT文档](bert)。

## BertJapaneseTokenizer

### `class transformers.BertJapaneseTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L107)

```py
( vocab_file spm_file = None do_lower_case = False do_word_tokenize = True do_subword_tokenize = True word_tokenizer_type = 'basic' subword_tokenizer_type = 'wordpiece' never_split = None unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' mecab_kwargs = None sudachi_kwargs = None jumanpp_kwargs = None **kwargs )
```

参数

+   `vocab_file` (`str`) — 指向一个每行一个wordpiece的词汇文件的路径。

+   `spm_file` (`str`, *可选*) — 指向包含词汇表的[SentencePiece](https://github.com/google/sentencepiece)文件的路径（通常具有.spm或.model扩展名）。

+   `do_lower_case` (`bool`, *可选*, 默认为`True`) — 是否将输入转换为小写。仅在do_basic_tokenize=True时有效。

+   `do_word_tokenize` (`bool`, *可选*, 默认为`True`) — 是否进行词分词。

+   `do_subword_tokenize` (`bool`, *可选*, 默认为`True`) — 是否进行子词分词。

+   `word_tokenizer_type` (`str`, *可选*, 默认为`"basic"`) — 词分词器的类型。可选择从[`"basic"`, “mecab”, “sudachi”, “jumanpp”]中选择。

+   `subword_tokenizer_type` (`str`, *可选*, 默认为`"wordpiece"`) — 子词分词器的类型。可选择从[`"wordpiece"`, “character”, “sentencepiece”]中选择。

+   `mecab_kwargs` (`dict`, *可选*) — 传递给`MecabTokenizer`构造函数的字典。

+   `sudachi_kwargs` (`dict`, *可选*) — 传递给`SudachiTokenizer`构造函数的字典。

+   `jumanpp_kwargs` (`dict`, *可选*) — 传递给`JumanppTokenizer`构造函数的字典。

为日文文本构建一个BERT分词器。

这个分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考：这个超类以获取有关这些方法的更多信息。

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L307)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID列表，将添加特殊标记。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。

返回

`List[int]`

包含适当特殊标记的[输入ID](../glossary#input-ids)列表。

通过连接和添加特殊标记，从序列或序列对构建用于序列分类任务的模型输入。BERT序列的格式如下：

+   单个序列：`[CLS] X [SEP]`

+   序列对：`[CLS] A [SEP] B [SEP]`

#### `convert_tokens_to_string`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L299)

```py
( tokens )
```

将一系列标记（字符串）转换为单个字符串。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L362)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。

返回

`List[int]`

根据给定序列的[标记类型ID](../glossary#token-type-ids)列表。

从传递的两个序列中创建一个用于序列对分类任务的掩码。一个BERT序列

序列掩码的格式如下：

```py
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |
```

如果`token_ids_1`为`None`，则此方法仅返回掩码的第一部分（0）。

#### `get_special_tokens_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L333)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。

+   `already_has_special_tokens` (`bool`, *可选*，默认为`False`) — 标记列表是否已经使用特殊标记格式化。

返回

`List[int]`

一个整数列表，范围在[0, 1]之间：1表示特殊标记，0表示序列标记。

从没有添加特殊标记的标记列表中检索序列ID。当使用分词器`prepare_for_model`方法添加特殊标记时调用此方法。
