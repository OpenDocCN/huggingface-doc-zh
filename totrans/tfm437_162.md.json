["```py\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\nmodel = AutoModel.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n```", "```py\n( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 task_type_vocab_size = 3 use_task_id = False initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 position_embedding_type = 'absolute' use_cache = True classifier_dropout = None **kwargs )\n```", "```py\n>>> from transformers import ErnieConfig, ErnieModel\n\n>>> # Initializing a ERNIE nghuyong/ernie-3.0-base-zh style configuration\n>>> configuration = ErnieConfig()\n\n>>> # Initializing a model (with random weights) from the nghuyong/ernie-3.0-base-zh style configuration\n>>> model = ErnieModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( loss: Optional = None prediction_logits: FloatTensor = None seq_relationship_logits: FloatTensor = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( config add_pooling_layer = True )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None task_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, ErnieModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n>>> model = ErnieModel.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None task_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None next_sentence_label: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.ernie.modeling_ernie.ErnieForPreTrainingOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, ErnieForPreTraining\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n>>> model = ErnieForPreTraining.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> prediction_logits = outputs.prediction_logits\n>>> seq_relationship_logits = outputs.seq_relationship_logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None task_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None labels: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, ErnieForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n>>> model = ErnieForCausalLM.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None task_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.MaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, ErnieForMaskedLM\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n>>> model = ErnieForMaskedLM.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n\n>>> inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> # retrieve index of [MASK]\n>>> mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n\n>>> predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n>>> tokenizer.decode(predicted_token_id)\n'paris'\n\n>>> labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n>>> # mask labels of non-[MASK] tokens\n>>> labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n\n>>> outputs = model(**inputs, labels=labels)\n>>> round(outputs.loss.item(), 2)\n0.88\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None task_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.NextSentencePredictorOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, ErnieForNextSentencePrediction\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n>>> model = ErnieForNextSentencePrediction.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n\n>>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n>>> next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n>>> encoding = tokenizer(prompt, next_sentence, return_tensors=\"pt\")\n\n>>> outputs = model(**encoding, labels=torch.LongTensor([1]))\n>>> logits = outputs.logits\n>>> assert logits[0, 0] < logits[0, 1]  # next sentence was random\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None task_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None task_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.MultipleChoiceModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, ErnieForMultipleChoice\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n>>> model = ErnieForMultipleChoice.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n\n>>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n>>> choice0 = \"It is eaten with a fork and a knife.\"\n>>> choice1 = \"It is eaten while held in the hand.\"\n>>> labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n\n>>> encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"pt\", padding=True)\n>>> outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1\n\n>>> # the linear classifier still needs to be trained\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None task_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None task_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```"]