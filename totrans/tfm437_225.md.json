["```py\n( vocab_size = 30522 hidden_size = 768 retriever_proj_size = 128 num_hidden_layers = 12 num_attention_heads = 12 num_candidates = 8 intermediate_size = 3072 hidden_act = 'gelu_new' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 span_hidden_size = 256 max_span_width = 10 reader_layer_norm_eps = 0.001 reader_beam_size = 5 reader_seq_len = 320 num_block_records = 13353718 searcher_beam_size = 5000 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 **kwargs )\n```", "```py\n>>> from transformers import RealmConfig, RealmEmbedder\n\n>>> # Initializing a REALM realm-cc-news-pretrained-* style configuration\n>>> configuration = RealmConfig()\n\n>>> # Initializing a model (with random weights) from the google/realm-cc-news-pretrained-embedder style configuration\n>>> model = RealmEmbedder(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file do_lower_case = True do_basic_tokenize = True never_split = None unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence |\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( text **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n>>> from transformers import RealmTokenizer\n\n>>> # batch_size = 2, num_candidates = 2\n>>> text = [[\"Hello world!\", \"Nice to meet you!\"], [\"The cute cat.\", \"The adorable dog.\"]]\n\n>>> tokenizer = RealmTokenizer.from_pretrained(\"google/realm-cc-news-pretrained-encoder\")\n>>> tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors=\"pt\")\n```", "```py\n( vocab_file = None tokenizer_file = None do_lower_case = True unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs )\n```", "```py\n( text **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n>>> from transformers import RealmTokenizerFast\n\n>>> # batch_size = 2, num_candidates = 2\n>>> text = [[\"Hello world!\", \"Nice to meet you!\"], [\"The cute cat.\", \"The adorable dog.\"]]\n\n>>> tokenizer = RealmTokenizerFast.from_pretrained(\"google/realm-cc-news-pretrained-encoder\")\n>>> tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors=\"pt\")\n```", "```py\n( block_records tokenizer )\n```", "```py\n( concat_inputs answer_ids )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.realm.modeling_realm.RealmEmbedderOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, RealmEmbedder\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/realm-cc-news-pretrained-embedder\")\n>>> model = RealmEmbedder.from_pretrained(\"google/realm-cc-news-pretrained-embedder\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> projected_score = outputs.projected_score\n```", "```py\n( config query_embedder = None )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None candidate_input_ids: Optional = None candidate_attention_mask: Optional = None candidate_token_type_ids: Optional = None candidate_inputs_embeds: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.realm.modeling_realm.RealmScorerOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, RealmScorer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/realm-cc-news-pretrained-scorer\")\n>>> model = RealmScorer.from_pretrained(\"google/realm-cc-news-pretrained-scorer\", num_candidates=2)\n\n>>> # batch_size = 2, num_candidates = 2\n>>> input_texts = [\"How are you?\", \"What is the item in the picture?\"]\n>>> candidates_texts = [[\"Hello world!\", \"Nice to meet you!\"], [\"A cute cat.\", \"An adorable dog.\"]]\n\n>>> inputs = tokenizer(input_texts, return_tensors=\"pt\")\n>>> candidates_inputs = tokenizer.batch_encode_candidates(candidates_texts, max_length=10, return_tensors=\"pt\")\n\n>>> outputs = model(\n...     **inputs,\n...     candidate_input_ids=candidates_inputs.input_ids,\n...     candidate_attention_mask=candidates_inputs.attention_mask,\n...     candidate_token_type_ids=candidates_inputs.token_type_ids,\n... )\n>>> relevance_score = outputs.relevance_score\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None relevance_score: Optional = None labels: Optional = None mlm_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.MaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, RealmKnowledgeAugEncoder\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/realm-cc-news-pretrained-encoder\")\n>>> model = RealmKnowledgeAugEncoder.from_pretrained(\n...     \"google/realm-cc-news-pretrained-encoder\", num_candidates=2\n... )\n\n>>> # batch_size = 2, num_candidates = 2\n>>> text = [[\"Hello world!\", \"Nice to meet you!\"], [\"The cute cat.\", \"The adorable dog.\"]]\n\n>>> inputs = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None relevance_score: Optional = None block_mask: Optional = None start_positions: Optional = None end_positions: Optional = None has_answers: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.realm.modeling_realm.RealmReaderOutput or tuple(torch.FloatTensor)\n```", "```py\n( config retriever = None )\n```", "```py\n( device )\n```", "```py\n( input_ids: Optional attention_mask: Optional = None token_type_ids: Optional = None answer_ids: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.realm.modeling_realm.RealmForOpenQAOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import RealmForOpenQA, RealmRetriever, AutoTokenizer\n\n>>> retriever = RealmRetriever.from_pretrained(\"google/realm-orqa-nq-openqa\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/realm-orqa-nq-openqa\")\n>>> model = RealmForOpenQA.from_pretrained(\"google/realm-orqa-nq-openqa\", retriever=retriever)\n\n>>> question = \"Who is the pioneer in modern computer science?\"\n>>> question_ids = tokenizer([question], return_tensors=\"pt\")\n>>> answer_ids = tokenizer(\n...     [\"alan mathison turing\"],\n...     add_special_tokens=False,\n...     return_token_type_ids=False,\n...     return_attention_mask=False,\n... ).input_ids\n\n>>> reader_output, predicted_answer_ids = model(**question_ids, answer_ids=answer_ids, return_dict=False)\n>>> predicted_answer = tokenizer.decode(predicted_answer_ids)\n>>> loss = reader_output.loss\n```"]