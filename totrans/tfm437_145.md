# BORT

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bort](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bort)

该模型目前处于维护模式，我们不接受任何修改其代码的新PR。

如果您在运行此模型时遇到任何问题，请重新安装支持此模型的最后一个版本：v4.30.0。您可以通过运行以下命令来执行：`pip install -U transformers==4.30.0`。

## 概述

BORT模型是由Adrian de Wynter和Daniel J. Perry在[为BERT提取最佳子架构](https://arxiv.org/abs/2010.10499)中提出的。这是BERT的一组最佳架构参数，作者称之为“Bort”。

论文摘要如下：

*我们从Devlin等人（2018）的BERT架构中提取了一组最佳的架构参数，通过应用神经架构搜索算法的最新突破。这个最佳子集，我们称之为“Bort”，明显更小，有效（即不计算嵌入层）大小为原始BERT-large架构的5.5%，以及净尺寸的16%。Bort还能在288个GPU小时内进行预训练，这相当于预训练最高性能的BERT参数化架构变体RoBERTa-large（Liu等人，2019）所需时间的1.2%，以及在相同硬件上训练BERT-large所需的GPU小时的世界纪录的约33%。它在CPU上也快了7.9倍，比架构的其他压缩变体以及一些非压缩变体表现更好：在多个公共自然语言理解（NLU）基准测试中，相对于BERT-large，它获得了0.3%至31%的性能改进，绝对值。*

该模型由[stefan-it](https://huggingface.co/stefan-it)贡献。原始代码可以在[这里](https://github.com/alexa/bort/)找到。

## 使用提示

+   BORT的模型架构基于BERT，参考[BERT的文档页面](bert)获取模型的API参考以及使用示例。

+   BORT使用RoBERTa分词器而不是BERT分词器，参考[RoBERTa的文档页面](roberta)获取分词器的API参考以及使用示例。

+   BORT需要一种特定的微调算法，称为[Agora](https://adewynter.github.io/notes/bort_algorithms_and_applications.html#fine-tuning-with-algebraic-topology)，遗憾的是目前尚未开源。如果有人尝试实现该算法以使BORT微调工作，对社区将非常有用。
