# BORT

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/bort`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bort)

该模型目前处于维护模式，我们不接受任何修改其代码的新 PR。

如果您在运行此模型时遇到任何问题，请重新安装支持此模型的最后一个版本：v4.30.0。您可以通过运行以下命令来执行：`pip install -U transformers==4.30.0`。

## 概述

BORT 模型是由 Adrian de Wynter 和 Daniel J. Perry 在[为 BERT 提取最佳子架构](https://arxiv.org/abs/2010.10499)中提出的。这是 BERT 的一组最佳架构参数，作者称之为“Bort”。

论文摘要如下：

*我们从 Devlin 等人（2018）的 BERT 架构中提取了一组最佳的架构参数，通过应用神经架构搜索算法的最新突破。这个最佳子集，我们称之为“Bort”，明显更小，有效（即不计算嵌入层）大小为原始 BERT-large 架构的 5.5%，以及净尺寸的 16%。Bort 还能在 288 个 GPU 小时内进行预训练，这相当于预训练最高性能的 BERT 参数化架构变体 RoBERTa-large（Liu 等人，2019）所需时间的 1.2%，以及在相同硬件上训练 BERT-large 所需的 GPU 小时的世界纪录的约 33%。它在 CPU 上也快了 7.9 倍，比架构的其他压缩变体以及一些非压缩变体表现更好：在多个公共自然语言理解（NLU）基准测试中，相对于 BERT-large，它获得了 0.3%至 31%的性能改进，绝对值。*

该模型由[stefan-it](https://huggingface.co/stefan-it)贡献。原始代码可以在[这里](https://github.com/alexa/bort/)找到。

## 使用提示

+   BORT 的模型架构基于 BERT，参考 BERT 的文档页面获取模型的 API 参考以及使用示例。

+   BORT 使用 RoBERTa 分词器而不是 BERT 分词器，参考 RoBERTa 的文档页面获取分词器的 API 参考以及使用示例。

+   BORT 需要一种特定的微调算法，称为[Agora](https://adewynter.github.io/notes/bort_algorithms_and_applications.html#fine-tuning-with-algebraic-topology)，遗憾的是目前尚未开源。如果有人尝试实现该算法以使 BORT 微调工作，对社区将非常有用。
