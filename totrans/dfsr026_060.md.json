["```py\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```", "```py\ncd examples/kandinsky2_2/text_to_image\npip install -r requirements.txt\n```", "```py\naccelerate config\n```", "```py\naccelerate config default\n```", "```py\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```", "```py\naccelerate launch train_text_to_image_prior.py \\\n  --mixed_precision=\"fp16\"\n```", "```py\naccelerate launch train_text_to_image_prior.py \\\n  --snr_gamma=5.0\n```", "```py\nnoise_scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\", prediction_type=\"sample\")\nimage_processor = CLIPImageProcessor.from_pretrained(\n    args.pretrained_prior_model_name_or_path, subfolder=\"image_processor\"\n)\ntokenizer = CLIPTokenizer.from_pretrained(args.pretrained_prior_model_name_or_path, subfolder=\"tokenizer\")\n\nwith ContextManagers(deepspeed_zero_init_disabled_context_manager()):\n    image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n        args.pretrained_prior_model_name_or_path, subfolder=\"image_encoder\", torch_dtype=weight_dtype\n    ).eval()\n    text_encoder = CLIPTextModelWithProjection.from_pretrained(\n        args.pretrained_prior_model_name_or_path, subfolder=\"text_encoder\", torch_dtype=weight_dtype\n    ).eval()\n```", "```py\nprior = PriorTransformer.from_pretrained(args.pretrained_prior_model_name_or_path, subfolder=\"prior\")\nprior.train()\noptimizer = optimizer_cls(\n    prior.parameters(),\n    lr=args.learning_rate,\n    betas=(args.adam_beta1, args.adam_beta2),\n    weight_decay=args.adam_weight_decay,\n    eps=args.adam_epsilon,\n)\n```", "```py\ndef preprocess_train(examples):\n    images = [image.convert(\"RGB\") for image in examples[image_column]]\n    examples[\"clip_pixel_values\"] = image_processor(images, return_tensors=\"pt\").pixel_values\n    examples[\"text_input_ids\"], examples[\"text_mask\"] = tokenize_captions(examples)\n    return examples\n```", "```py\nmodel_pred = prior(\n    noisy_latents,\n    timestep=timesteps,\n    proj_embedding=prompt_embeds,\n    encoder_hidden_states=text_encoder_hidden_states,\n    attention_mask=text_mask,\n).predicted_image_embedding\n```", "```py\nexport DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"\n\naccelerate launch --mixed_precision=\"fp16\"  train_text_to_image_prior.py \\\n  --dataset_name=$DATASET_NAME \\\n  --resolution=768 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --checkpoints_total_limit=3 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --validation_prompts=\"A robot pokemon, 4k photo\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub \\\n  --output_dir=\"kandi2-prior-pokemon-model\" \n```", "```py\nfrom diffusers import AutoPipelineForText2Image, DiffusionPipeline\nimport torch\n\nprior_pipeline = DiffusionPipeline.from_pretrained(output_dir, torch_dtype=torch.float16)\nprior_components = {\"prior_\" + k: v for k,v in prior_pipeline.components.items()}\npipeline = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", **prior_components, torch_dtype=torch.float16)\n\npipe.enable_model_cpu_offload()\nprompt=\"A robot pokemon, 4k photo\"\nimage = pipeline(prompt=prompt, negative_prompt=negative_prompt).images[0]\n```"]