- en: Community
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 社区
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/community](https://huggingface.co/docs/transformers/v4.37.2/en/community)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/community](https://huggingface.co/docs/transformers/v4.37.2/en/community)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This page regroups resources around 🤗 Transformers developed by the community.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此页面汇集了由社区开发的🤗 Transformers周围的资源。
- en: 'Community resources:'
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 社区资源：
- en: '| Resource | Description | Author |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | 描述 | 作者 |'
- en: '| :-- | :-- | --: |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | --: |'
- en: '| [Hugging Face Transformers Glossary Flashcards](https://www.darigovresearch.com/huggingface-transformers-glossary-flashcards)
    | A set of flashcards based on the [Transformers Docs Glossary](glossary) that
    has been put into a form which can be easily learned/revised using [Anki](https://apps.ankiweb.net/)
    an open source, cross platform app specifically designed for long term knowledge
    retention. See this [Introductory video on how to use the flashcards](https://www.youtube.com/watch?v=Dji_h7PILrw).
    | [Darigov Research](https://www.darigovresearch.com/) |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| [Hugging Face Transformers词汇表闪卡](https://www.darigovresearch.com/huggingface-transformers-glossary-flashcards)
    | 基于[Transformers Docs词汇表](glossary)的一套闪卡，已经制作成易于使用[Anki](https://apps.ankiweb.net/)学习/复习的形式，Anki是一款专门设计用于长期知识保留的开源、跨平台应用程序。查看这个[介绍性视频，了解如何使用闪卡](https://www.youtube.com/watch?v=Dji_h7PILrw)。
    | [Darigov Research](https://www.darigovresearch.com/) |'
- en: 'Community notebooks:'
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 社区笔记本：
- en: '| Notebook | Description | Author |  |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| 笔记本 | 描述 | 作者 |  |'
- en: '| :-- | :-- | :-- | --: |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- | --: |'
- en: '| [Fine-tune a pre-trained Transformer to generate lyrics](https://github.com/AlekseyKorshuk/huggingartists)
    | How to generate lyrics in the style of your favorite artist by fine-tuning a
    GPT-2 model | [Aleksey Korshuk](https://github.com/AlekseyKorshuk) | [![Open In
    Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb)
    |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| [微调预训练的Transformer以生成歌词](https://github.com/AlekseyKorshuk/huggingartists)
    | 如何通过微调GPT-2模型生成您最喜爱艺术家风格的歌词 | [Aleksey Korshuk](https://github.com/AlekseyKorshuk)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb)
    |'
- en: '| [Train T5 in Tensorflow 2](https://github.com/snapthat/TF-T5-text-to-text)
    | How to train T5 for any task using Tensorflow 2\. This notebook demonstrates
    a Question & Answer task implemented in Tensorflow 2 using SQUAD | [Muhammad Harris](https://github.com/HarrisDePerceptron)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb)
    |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| [在Tensorflow 2中训练T5](https://github.com/snapthat/TF-T5-text-to-text) | 如何使用Tensorflow
    2为任何任务训练T5。这个笔记本演示了在Tensorflow 2中实现的一个问答任务，使用SQUAD | [Muhammad Harris](https://github.com/HarrisDePerceptron)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb)
    |'
- en: '| [Train T5 on TPU](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb)
    | How to train T5 on SQUAD with Transformers and Nlp | [Suraj Patil](https://github.com/patil-suraj)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil)
    |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| [在TPU上训练T5](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb)
    | 如何使用Transformers和Nlp在SQUAD上训练T5 | [Suraj Patil](https://github.com/patil-suraj)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil)
    |'
- en: '| [Fine-tune T5 for Classification and Multiple Choice](https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb)
    | How to fine-tune T5 for classification and multiple choice tasks using a text-to-text
    format with PyTorch Lightning | [Suraj Patil](https://github.com/patil-suraj)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb)
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| [为分类和多项选择微调T5](https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb)
    | 如何使用PyTorch Lightning以文本-文本格式微调T5以进行分类和多项选择任务 | [Suraj Patil](https://github.com/patil-suraj)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb)
    |'
- en: '| [Fine-tune DialoGPT on New Datasets and Languages](https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb)
    | How to fine-tune the DialoGPT model on a new dataset for open-dialog conversational
    chatbots | [Nathan Cooper](https://github.com/ncoop57) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb)
    |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| [在新数据集和语言上微调DialoGPT](https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb)
    | 如何在新数据集上微调DialoGPT模型，用于开放对话聊天机器人 | [Nathan Cooper](https://github.com/ncoop57)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb)
    |'
- en: '| [Long Sequence Modeling with Reformer](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)
    | How to train on sequences as long as 500,000 tokens with Reformer | [Patrick
    von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)
    |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| [使用Reformer进行长序列建模](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)
    | 如何使用Reformer在长度为500,000个标记的序列上进行训练 | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)
    |'
- en: '| [Fine-tune BART for Summarization](https://github.com/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb)
    | How to fine-tune BART for summarization with fastai using blurr | [Wayde Gilliam](https://ohmeow.com/)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb)
    |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| [为摘要微调BART](https://github.com/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb)
    | 如何使用blurr使用fastai微调BART进行摘要 | [Wayde Gilliam](https://ohmeow.com/) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb)
    |'
- en: '| [Fine-tune a pre-trained Transformer on anyone’s tweets](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb)
    | How to generate tweets in the style of your favorite Twitter account by fine-tuning
    a GPT-2 model | [Boris Dayma](https://github.com/borisdayma) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb)
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| [为任何人的推文微调预训练Transformer](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb)
    | 如何通过微调GPT-2模型生成您喜爱的Twitter账户风格的推文 | [Boris Dayma](https://github.com/borisdayma)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb)
    |'
- en: '| [Optimize 🤗 Hugging Face models with Weights & Biases](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb)
    | A complete tutorial showcasing W&B integration with Hugging Face | [Boris Dayma](https://github.com/borisdayma)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb)
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| [使用Weights＆Biases优化🤗Hugging Face模型](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb)
    | 一个完整的教程，展示了W＆B与Hugging Face的集成 | [Boris Dayma](https://github.com/borisdayma)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb)
    |'
- en: '| [Pretrain Longformer](https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb)
    | How to build a “long” version of existing pretrained models | [Iz Beltagy](https://beltagy.net)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb)
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| [预训练Longformer](https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb)
    | 如何构建现有预训练模型的“长”版本 | [Iz Beltagy](https://beltagy.net) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb)
    |'
- en: '| [Fine-tune Longformer for QA](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb)
    | How to fine-tune longformer model for QA task | [Suraj Patil](https://github.com/patil-suraj)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb)
    |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| [为QA微调Longformer](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb)
    | 如何为QA任务微调Longformer模型 | [Suraj Patil](https://github.com/patil-suraj) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb)
    |'
- en: '| [Evaluate Model with 🤗nlp](https://github.com/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb)
    | How to evaluate longformer on TriviaQA with `nlp` | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1m7eTGlPmLRgoPkkA7rkhQdZ9ydpmsdLE?usp=sharing)
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| [使用🤗nlp评估模型](https://github.com/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb)
    | 如何使用`nlp`在TriviaQA上评估Longformer | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1m7eTGlPmLRgoPkkA7rkhQdZ9ydpmsdLE?usp=sharing)
    |'
- en: '| [Fine-tune T5 for Sentiment Span Extraction](https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb)
    | How to fine-tune T5 for sentiment span extraction using a text-to-text format
    with PyTorch Lightning | [Lorenzo Ampil](https://github.com/enzoampil) | [![Open
    In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb)
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| [为情感跨度提取微调T5](https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb)
    | 如何使用PyTorch Lightning以文本到文本格式微调T5进行情感跨度提取 | [Lorenzo Ampil](https://github.com/enzoampil)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb)
    |'
- en: '| [Fine-tune DistilBert for Multiclass Classification](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb)
    | How to fine-tune DistilBert for multiclass classification with PyTorch | [Abhishek
    Kumar Mishra](https://github.com/abhimishra91) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb)
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| [为多类分类微调DistilBert](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb)
    | 如何使用PyTorch微调DistilBert进行多类分类 | [Abhishek Kumar Mishra](https://github.com/abhimishra91)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb)
    |'
- en: '| [Fine-tune BERT for Multi-label Classification](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)
    | How to fine-tune BERT for multi-label classification using PyTorch | [Abhishek
    Kumar Mishra](https://github.com/abhimishra91) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| [微调BERT进行多标签分类](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)
    | 如何使用PyTorch微调BERT进行多标签分类 | [Abhishek Kumar Mishra](https://github.com/abhimishra91)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)
    |'
- en: '| [Fine-tune T5 for Summarization](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)
    | How to fine-tune T5 for summarization in PyTorch and track experiments with
    WandB | [Abhishek Kumar Mishra](https://github.com/abhimishra91) | [![Open In
    Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| [微调T5进行摘要](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)
    | 如何在PyTorch中微调T5进行摘要，并使用WandB跟踪实验 | [Abhishek Kumar Mishra](https://github.com/abhimishra91)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)
    |'
- en: '| [Speed up Fine-Tuning in Transformers with Dynamic Padding / Bucketing](https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb)
    | How to speed up fine-tuning by a factor of 2 using dynamic padding / bucketing
    | [Michael Benesty](https://github.com/pommedeterresautee) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1CBfRU1zbfu7-ijiOqAAQUA-RJaxfcJoO?usp=sharing)
    |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| [使用动态填充/分桶加速Transformer中的微调](https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb)
    | 如何通过动态填充/分桶将微调加速2倍 | [Michael Benesty](https://github.com/pommedeterresautee)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1CBfRU1zbfu7-ijiOqAAQUA-RJaxfcJoO?usp=sharing)
    |'
- en: '| [Pretrain Reformer for Masked Language Modeling](https://github.com/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb)
    | How to train a Reformer model with bi-directional self-attention layers | [Patrick
    von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1tzzh0i8PgDQGV3SMFUGxM7_gGae3K-uW?usp=sharing)
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| [为遮蔽语言建模预训练Reformer](https://github.com/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb)
    | 如何训练具有双向自注意力层的Reformer模型 | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1tzzh0i8PgDQGV3SMFUGxM7_gGae3K-uW?usp=sharing)
    |'
- en: '| [Expand and Fine Tune Sci-BERT](https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb)
    | How to increase vocabulary of a pretrained SciBERT model from AllenAI on the
    CORD dataset and pipeline it. | [Tanmay Thakur](https://github.com/lordtt13) |
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1rqAR40goxbAfez1xvF3hBJphSCsvXmh8)
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| [扩展和微调Sci-BERT](https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb)
    | 如何在CORD数据集上增加预训练的SciBERT模型的词汇量并进行流水线处理。 | [Tanmay Thakur](https://github.com/lordtt13)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1rqAR40goxbAfez1xvF3hBJphSCsvXmh8)
    |'
- en: '| [Fine Tune BlenderBotSmall for Summarization using the Trainer API](https://github.com/lordtt13/transformers-experiments/blob/master/Custom%20Tasks/fine-tune-blenderbot_small-for-summarization.ipynb)
    | How to fine-tune BlenderBotSmall for summarization on a custom dataset, using
    the Trainer API. | [Tanmay Thakur](https://github.com/lordtt13) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/19Wmupuls7mykSGyRN_Qo6lPQhgp56ymq?usp=sharing)
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| [使用Trainer API微调BlenderBotSmall进行摘要](https://github.com/lordtt13/transformers-experiments/blob/master/Custom%20Tasks/fine-tune-blenderbot_small-for-summarization.ipynb)
    | 如何在自定义数据集上使用Trainer API微调BlenderBotSmall进行摘要 | [Tanmay Thakur](https://github.com/lordtt13)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/19Wmupuls7mykSGyRN_Qo6lPQhgp56ymq?usp=sharing)
    |'
- en: '| [Fine-tune Electra and interpret with Integrated Gradients](https://github.com/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb)
    | How to fine-tune Electra for sentiment analysis and interpret predictions with
    Captum Integrated Gradients | [Eliza Szczechla](https://elsanns.github.io) | [![Open
    In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb)
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| [微调Electra并使用Integrated Gradients进行解释](https://github.com/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb)
    | 如何微调Electra进行情感分析，并使用Captum Integrated Gradients解释预测 | [Eliza Szczechla](https://elsanns.github.io)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb)
    |'
- en: '| [fine-tune a non-English GPT-2 Model with Trainer class](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)
    | How to fine-tune a non-English GPT-2 Model with Trainer class | [Philipp Schmid](https://www.philschmid.de)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| [使用Trainer类微调非英语GPT-2模型](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)
    | 如何使用Trainer类微调非英语GPT-2模型 | [Philipp Schmid](https://www.philschmid.de) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)
    |'
- en: '| [Fine-tune a DistilBERT Model for Multi Label Classification task](https://github.com/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb)
    | How to fine-tune a DistilBERT Model for Multi Label Classification task | [Dhaval
    Taunk](https://github.com/DhavalTaunk08) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb)
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| [为多标签分类任务微调DistilBERT模型](https://github.com/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb)
    | 如何为多标签分类任务微调DistilBERT模型 | [Dhaval Taunk](https://github.com/DhavalTaunk08)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb)
    |'
- en: '| [Fine-tune ALBERT for sentence-pair classification](https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb)
    | How to fine-tune an ALBERT model or another BERT-based model for the sentence-pair
    classification task | [Nadir El Manouzi](https://github.com/NadirEM) | [![Open
    In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb)
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| [为句对分类微调ALBERT](https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb)
    | 如何为句对分类任务微调ALBERT模型或其他基于BERT的模型 | [Nadir El Manouzi](https://github.com/NadirEM)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb)
    |'
- en: '| [Fine-tune Roberta for sentiment analysis](https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb)
    | How to fine-tune a Roberta model for sentiment analysis | [Dhaval Taunk](https://github.com/DhavalTaunk08)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb)
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| [Fine-tune Roberta for sentiment analysis](https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb)
    | 如何为情感分析微调一个Roberta模型 | [Dhaval Taunk](https://github.com/DhavalTaunk08) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb)
    |'
- en: '| [Evaluating Question Generation Models](https://github.com/flexudy-pipe/qugeev)
    | How accurate are the answers to questions generated by your seq2seq transformer
    model? | [Pascal Zoleko](https://github.com/zolekode) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1bpsSqCQU-iw_5nNoRm_crPq6FRuJthq_?usp=sharing)
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| [评估问题生成模型](https://github.com/flexudy-pipe/qugeev) | 您的seq2seq变压器模型生成的问题的答案有多准确？
    | [Pascal Zoleko](https://github.com/zolekode) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1bpsSqCQU-iw_5nNoRm_crPq6FRuJthq_?usp=sharing)
    |'
- en: '| [Classify text with DistilBERT and Tensorflow](https://github.com/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb)
    | How to fine-tune DistilBERT for text classification in TensorFlow | [Peter Bayerle](https://github.com/peterbayerle)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb)
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| [使用DistilBERT和Tensorflow对文本进行分类](https://github.com/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb)
    | 如何在TensorFlow中为文本分类微调DistilBERT | [Peter Bayerle](https://github.com/peterbayerle)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb)
    |'
- en: '| [Leverage BERT for Encoder-Decoder Summarization on CNN/Dailymail](https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)
    | How to warm-start a *EncoderDecoderModel* with a *bert-base-uncased* checkpoint
    for summarization on CNN/Dailymail | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| [利用BERT进行CNN/Dailymail的编码器-解码器摘要](https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)
    | 如何使用*bert-base-uncased*检查点对CNN/Dailymail的摘要进行热启动*EncoderDecoderModel* | [Patrick
    von Platen](https://github.com/patrickvonplaten) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)
    |'
- en: '| [Leverage RoBERTa for Encoder-Decoder Summarization on BBC XSum](https://github.com/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb)
    | How to warm-start a shared *EncoderDecoderModel* with a *roberta-base* checkpoint
    for summarization on BBC/XSum | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb)
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| [利用RoBERTa进行BBC XSum的编码器-解码器摘要](https://github.com/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb)
    | 如何使用*roberta-base*检查点对BBC/XSum的摘要进行热启动共享*EncoderDecoderModel* | [Patrick von
    Platen](https://github.com/patrickvonplaten) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb)
    |'
- en: '| [Fine-tune TAPAS on Sequential Question Answering (SQA)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)
    | How to fine-tune *TapasForQuestionAnswering* with a *tapas-base* checkpoint
    on the Sequential Question Answering (SQA) dataset | [Niels Rogge](https://github.com/nielsrogge)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| [在顺序问答（SQA）上微调TAPAS](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)
    | 如何在顺序问答（SQA）数据集上使用*tapas-base*检查点微调*TapasForQuestionAnswering* | [Niels Rogge](https://github.com/nielsrogge)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)
    |'
- en: '| [Evaluate TAPAS on Table Fact Checking (TabFact)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb)
    | How to evaluate a fine-tuned *TapasForSequenceClassification* with a *tapas-base-finetuned-tabfact*
    checkpoint using a combination of the 🤗 datasets and 🤗 transformers libraries
    | [Niels Rogge](https://github.com/nielsrogge) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb)
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| [在Table Fact Checking（TabFact）上评估TAPAS](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb)
    | 如何使用🤗数据集和🤗transformers库的组合评估经过微调的*TapasForSequenceClassification*，使用*tapas-base-finetuned-tabfact*检查点
    | [Niels Rogge](https://github.com/nielsrogge) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb)
    |'
- en: '| [Fine-tuning mBART for translation](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb)
    | How to fine-tune mBART using Seq2SeqTrainer for Hindi to English translation
    | [Vasudev Gupta](https://github.com/vasudevgupta7) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb)
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| [为翻译微调mBART](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb)
    | 如何使用Seq2SeqTrainer为印地语到英语翻译微调mBART | [Vasudev Gupta](https://github.com/vasudevgupta7)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb)
    |'
- en: '| [Fine-tune LayoutLM on FUNSD (a form understanding dataset)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb)
    | How to fine-tune *LayoutLMForTokenClassification* on the FUNSD dataset for information
    extraction from scanned documents | [Niels Rogge](https://github.com/nielsrogge)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb)
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| [在FUNSD上微调LayoutLM（一种表单理解数据集）](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb)
    | 如何在FUNSD数据集上微调*LayoutLMForTokenClassification*，从扫描文档中提取信息 | [Niels Rogge](https://github.com/nielsrogge)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb)
    |'
- en: '| [Fine-Tune DistilGPT2 and Generate Text](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb)
    | How to fine-tune DistilGPT2 and generate text | [Aakash Tripathi](https://github.com/tripathiaakash)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb)
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| [微调DistilGPT2并生成文本](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb)
    | 如何微调DistilGPT2并生成文本 | [Aakash Tripathi](https://github.com/tripathiaakash) |
    [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb)
    |'
- en: '| [Fine-Tune LED on up to 8K tokens](https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb)
    | How to fine-tune LED on pubmed for long-range summarization | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb)
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| [在最多8K标记上微调LED](https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb)
    | 如何在pubmed上微调LED进行长距离摘要 | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb)
    |'
- en: '| [Evaluate LED on Arxiv](https://github.com/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb)
    | How to effectively evaluate LED on long-range summarization | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb)
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| [在Arxiv上评估LED](https://github.com/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb)
    | 如何有效评估LED进行长距离摘要 | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb)
    |'
- en: '| [Fine-tune LayoutLM on RVL-CDIP (a document image classification dataset)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb)
    | How to fine-tune *LayoutLMForSequenceClassification* on the RVL-CDIP dataset
    for scanned document classification | [Niels Rogge](https://github.com/nielsrogge)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb)
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| [在RVL-CDIP上微调LayoutLM（一种文档图像分类数据集）](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb)
    | 如何在RVL-CDIP数据集上微调*LayoutLMForSequenceClassification*，用于扫描文档分类 | [Niels Rogge](https://github.com/nielsrogge)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb)
    |'
- en: '| [Wav2Vec2 CTC decoding with GPT2 adjustment](https://github.com/voidful/huggingface_notebook/blob/main/xlsr_gpt.ipynb)
    | How to decode CTC sequence with language model adjustment | [Eric Lam](https://github.com/voidful)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1e_z5jQHYbO2YKEaUgzb1ww1WwiAyydAj?usp=sharing)
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '- [使用GPT2调整进行Wav2Vec2 CTC解码](https://github.com/voidful/huggingface_notebook/blob/main/xlsr_gpt.ipynb)
    | 如何使用语言模型调整解码CTC序列 | [Eric Lam](https://github.com/voidful) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1e_z5jQHYbO2YKEaUgzb1ww1WwiAyydAj?usp=sharing)
    |'
- en: '| [Fine-tune BART for summarization in two languages with Trainer class](https://github.com/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb)
    | How to fine-tune BART for summarization in two languages with Trainer class
    | [Eliza Szczechla](https://github.com/elsanns) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb)
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '- [使用Trainer类在两种语言中对BART进行摘要微调](https://github.com/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb)
    | 如何使用Trainer类在两种语言中对BART进行摘要微调 | [Eliza Szczechla](https://github.com/elsanns)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb)
    |'
- en: '| [Evaluate Big Bird on Trivia QA](https://github.com/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb)
    | How to evaluate BigBird on long document question answering on Trivia QA | [Patrick
    von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb)
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '- [在Trivia QA上评估Big Bird](https://github.com/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb)
    | 如何在Trivia QA上评估BigBird在长文档问答上的表现 | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb)
    |'
- en: '| [Create video captions using Wav2Vec2](https://github.com/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb)
    | How to create YouTube captions from any video by transcribing the audio with
    Wav2Vec | [Niklas Muennighoff](https://github.com/Muennighoff) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb)
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '- [使用Wav2Vec2创建视频字幕](https://github.com/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb)
    | 如何通过使用Wav2Vec转录音频来从任何视频创建YouTube字幕 | [Niklas Muennighoff](https://github.com/Muennighoff)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb)
    |'
- en: '| [Fine-tune the Vision Transformer on CIFAR-10 using PyTorch Lightning](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb)
    | How to fine-tune the Vision Transformer (ViT) on CIFAR-10 using HuggingFace
    Transformers, Datasets and PyTorch Lightning | [Niels Rogge](https://github.com/nielsrogge)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb)
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '- [使用PyTorch Lightning在CIFAR-10上微调Vision Transformer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb)
    | 如何使用HuggingFace Transformers、Datasets和PyTorch Lightning在CIFAR-10上微调Vision Transformer（ViT）
    | [Niels Rogge](https://github.com/nielsrogge) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb)
    |'
- en: '| [Fine-tune the Vision Transformer on CIFAR-10 using the 🤗 Trainer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb)
    | How to fine-tune the Vision Transformer (ViT) on CIFAR-10 using HuggingFace
    Transformers, Datasets and the 🤗 Trainer | [Niels Rogge](https://github.com/nielsrogge)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb)
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '- [使用🤗 Trainer在CIFAR-10上微调Vision Transformer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb)
    | 如何使用HuggingFace Transformers、Datasets和🤗 Trainer在CIFAR-10上微调Vision Transformer（ViT）
    | [Niels Rogge](https://github.com/nielsrogge) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb)
    |'
- en: '| [Evaluate LUKE on Open Entity, an entity typing dataset](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb)
    | How to evaluate *LukeForEntityClassification* on the Open Entity dataset | [Ikuya
    Yamada](https://github.com/ikuyamada) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb)
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '- [在Open Entity上评估LUKE，一个实体类型数据集](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb)
    | 如何在Open Entity数据集上评估*LukeForEntityClassification* | [Ikuya Yamada](https://github.com/ikuyamada)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb)
    |'
- en: '| [Evaluate LUKE on TACRED, a relation extraction dataset](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb)
    | How to evaluate *LukeForEntityPairClassification* on the TACRED dataset | [Ikuya
    Yamada](https://github.com/ikuyamada) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb)
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '- [在TACRED上评估LUKE，一个关系抽取数据集](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb)
    | 如何在TACRED数据集上评估*LukeForEntityPairClassification* | [Ikuya Yamada](https://github.com/ikuyamada)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb)
    |'
- en: '| [Evaluate LUKE on CoNLL-2003, an important NER benchmark](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb)
    | How to evaluate *LukeForEntitySpanClassification* on the CoNLL-2003 dataset
    | [Ikuya Yamada](https://github.com/ikuyamada) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb)
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| [在CoNLL-2003上评估LUKE，一个重要的NER基准](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb)
    | 如何在CoNLL-2003数据集上评估*LukeForEntitySpanClassification* | [Ikuya Yamada](https://github.com/ikuyamada)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb)
    |'
- en: '| [Evaluate BigBird-Pegasus on PubMed dataset](https://github.com/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb)
    | How to evaluate *BigBirdPegasusForConditionalGeneration* on PubMed dataset |
    [Vasudev Gupta](https://github.com/vasudevgupta7) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb)
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| [在PubMed数据集上评估BigBird-Pegasus](https://github.com/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb)
    | 如何在PubMed数据集上评估*BigBirdPegasusForConditionalGeneration* | [Vasudev Gupta](https://github.com/vasudevgupta7)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb)
    |'
- en: '| [Speech Emotion Classification with Wav2Vec2](https://github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb)
    | How to leverage a pretrained Wav2Vec2 model for Emotion Classification on the
    MEGA dataset | [Mehrdad Farahani](https://github.com/m3hrdadfi) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb)
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| [使用Wav2Vec2进行语音情感分类](https://github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb)
    | 如何利用预训练的Wav2Vec2模型对MEGA数据集进行情感分类 | [Mehrdad Farahani](https://github.com/m3hrdadfi)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb)
    |'
- en: '| [Detect objects in an image with DETR](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb)
    | How to use a trained *DetrForObjectDetection* model to detect objects in an
    image and visualize attention | [Niels Rogge](https://github.com/NielsRogge) |
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb)
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| [使用DETR在图像中检测对象](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb)
    | 如何使用训练好的*DetrForObjectDetection*模型在图像中检测对象并可视化注意力 | [Niels Rogge](https://github.com/NielsRogge)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb)
    |'
- en: '| [Fine-tune DETR on a custom object detection dataset](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb)
    | How to fine-tune *DetrForObjectDetection* on a custom object detection dataset
    | [Niels Rogge](https://github.com/NielsRogge) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb)
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| [在自定义对象检测数据集上微调DETR](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb)
    | 如何在自定义对象检测数据集上微调*DetrForObjectDetection* | [Niels Rogge](https://github.com/NielsRogge)
    | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb)
    |'
- en: '| [Finetune T5 for Named Entity Recognition](https://github.com/ToluClassics/Notebooks/blob/main/T5_Ner_Finetuning.ipynb)
    | How to fine-tune *T5* on a Named Entity Recognition Task | [Ogundepo Odunayo](https://github.com/ToluClassics)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing)
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| [为命名实体识别微调T5](https://github.com/ToluClassics/Notebooks/blob/main/T5_Ner_Finetuning.ipynb)
    | 如何在命名实体识别任务上微调*T5* | [Ogundepo Odunayo](https://github.com/ToluClassics) | [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing)
    |'
