- en: Community
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¤¾åŒº
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/community](https://huggingface.co/docs/transformers/v4.37.2/en/community)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/community](https://huggingface.co/docs/transformers/v4.37.2/en/community)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This page regroups resources around ğŸ¤— Transformers developed by the community.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤é¡µé¢æ±‡é›†äº†ç”±ç¤¾åŒºå¼€å‘çš„ğŸ¤— Transformerså‘¨å›´çš„èµ„æºã€‚
- en: 'Community resources:'
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¤¾åŒºèµ„æºï¼š
- en: '| Resource | Description | Author |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| èµ„æº | æè¿° | ä½œè€… |'
- en: '| :-- | :-- | --: |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | --: |'
- en: '| [Hugging Face Transformers Glossary Flashcards](https://www.darigovresearch.com/huggingface-transformers-glossary-flashcards)
    | A set of flashcards based on the [Transformers Docs Glossary](glossary) that
    has been put into a form which can be easily learned/revised using [Anki](https://apps.ankiweb.net/)
    an open source, cross platform app specifically designed for long term knowledge
    retention. See this [Introductory video on how to use the flashcards](https://www.youtube.com/watch?v=Dji_h7PILrw).
    | [Darigov Research](https://www.darigovresearch.com/) |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| [Hugging Face Transformersè¯æ±‡è¡¨é—ªå¡](https://www.darigovresearch.com/huggingface-transformers-glossary-flashcards)
    | åŸºäº[Transformers Docsè¯æ±‡è¡¨](glossary)çš„ä¸€å¥—é—ªå¡ï¼Œå·²ç»åˆ¶ä½œæˆæ˜“äºä½¿ç”¨[Anki](https://apps.ankiweb.net/)å­¦ä¹ /å¤ä¹ çš„å½¢å¼ï¼ŒAnkiæ˜¯ä¸€æ¬¾ä¸“é—¨è®¾è®¡ç”¨äºé•¿æœŸçŸ¥è¯†ä¿ç•™çš„å¼€æºã€è·¨å¹³å°åº”ç”¨ç¨‹åºã€‚æŸ¥çœ‹è¿™ä¸ª[ä»‹ç»æ€§è§†é¢‘ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨é—ªå¡](https://www.youtube.com/watch?v=Dji_h7PILrw)ã€‚
    | [Darigov Research](https://www.darigovresearch.com/) |'
- en: 'Community notebooks:'
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¤¾åŒºç¬”è®°æœ¬ï¼š
- en: '| Notebook | Description | Author |  |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| ç¬”è®°æœ¬ | æè¿° | ä½œè€… |  |'
- en: '| :-- | :-- | :-- | --: |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- | --: |'
- en: '| [Fine-tune a pre-trained Transformer to generate lyrics](https://github.com/AlekseyKorshuk/huggingartists)
    | How to generate lyrics in the style of your favorite artist by fine-tuning a
    GPT-2 model | [Aleksey Korshuk](https://github.com/AlekseyKorshuk) | [![Open In
    Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb)
    |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| [å¾®è°ƒé¢„è®­ç»ƒçš„Transformerä»¥ç”Ÿæˆæ­Œè¯](https://github.com/AlekseyKorshuk/huggingartists)
    | å¦‚ä½•é€šè¿‡å¾®è°ƒGPT-2æ¨¡å‹ç”Ÿæˆæ‚¨æœ€å–œçˆ±è‰ºæœ¯å®¶é£æ ¼çš„æ­Œè¯ | [Aleksey Korshuk](https://github.com/AlekseyKorshuk)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb)
    |'
- en: '| [Train T5 in Tensorflow 2](https://github.com/snapthat/TF-T5-text-to-text)
    | How to train T5 for any task using Tensorflow 2\. This notebook demonstrates
    a Question & Answer task implemented in Tensorflow 2 using SQUAD | [Muhammad Harris](https://github.com/HarrisDePerceptron)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb)
    |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| [åœ¨Tensorflow 2ä¸­è®­ç»ƒT5](https://github.com/snapthat/TF-T5-text-to-text) | å¦‚ä½•ä½¿ç”¨Tensorflow
    2ä¸ºä»»ä½•ä»»åŠ¡è®­ç»ƒT5ã€‚è¿™ä¸ªç¬”è®°æœ¬æ¼”ç¤ºäº†åœ¨Tensorflow 2ä¸­å®ç°çš„ä¸€ä¸ªé—®ç­”ä»»åŠ¡ï¼Œä½¿ç”¨SQUAD | [Muhammad Harris](https://github.com/HarrisDePerceptron)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb)
    |'
- en: '| [Train T5 on TPU](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb)
    | How to train T5 on SQUAD with Transformers and Nlp | [Suraj Patil](https://github.com/patil-suraj)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil)
    |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| [åœ¨TPUä¸Šè®­ç»ƒT5](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb)
    | å¦‚ä½•ä½¿ç”¨Transformerså’ŒNlpåœ¨SQUADä¸Šè®­ç»ƒT5 | [Suraj Patil](https://github.com/patil-suraj)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil)
    |'
- en: '| [Fine-tune T5 for Classification and Multiple Choice](https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb)
    | How to fine-tune T5 for classification and multiple choice tasks using a text-to-text
    format with PyTorch Lightning | [Suraj Patil](https://github.com/patil-suraj)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb)
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| [ä¸ºåˆ†ç±»å’Œå¤šé¡¹é€‰æ‹©å¾®è°ƒT5](https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb)
    | å¦‚ä½•ä½¿ç”¨PyTorch Lightningä»¥æ–‡æœ¬-æ–‡æœ¬æ ¼å¼å¾®è°ƒT5ä»¥è¿›è¡Œåˆ†ç±»å’Œå¤šé¡¹é€‰æ‹©ä»»åŠ¡ | [Suraj Patil](https://github.com/patil-suraj)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb)
    |'
- en: '| [Fine-tune DialoGPT on New Datasets and Languages](https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb)
    | How to fine-tune the DialoGPT model on a new dataset for open-dialog conversational
    chatbots | [Nathan Cooper](https://github.com/ncoop57) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb)
    |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| [åœ¨æ–°æ•°æ®é›†å’Œè¯­è¨€ä¸Šå¾®è°ƒDialoGPT](https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb)
    | å¦‚ä½•åœ¨æ–°æ•°æ®é›†ä¸Šå¾®è°ƒDialoGPTæ¨¡å‹ï¼Œç”¨äºå¼€æ”¾å¯¹è¯èŠå¤©æœºå™¨äºº | [Nathan Cooper](https://github.com/ncoop57)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb)
    |'
- en: '| [Long Sequence Modeling with Reformer](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)
    | How to train on sequences as long as 500,000 tokens with Reformer | [Patrick
    von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)
    |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| [ä½¿ç”¨Reformerè¿›è¡Œé•¿åºåˆ—å»ºæ¨¡](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)
    | å¦‚ä½•ä½¿ç”¨Reformeråœ¨é•¿åº¦ä¸º500,000ä¸ªæ ‡è®°çš„åºåˆ—ä¸Šè¿›è¡Œè®­ç»ƒ | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)
    |'
- en: '| [Fine-tune BART for Summarization](https://github.com/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb)
    | How to fine-tune BART for summarization with fastai using blurr | [Wayde Gilliam](https://ohmeow.com/)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb)
    |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| [ä¸ºæ‘˜è¦å¾®è°ƒBART](https://github.com/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb)
    | å¦‚ä½•ä½¿ç”¨blurrä½¿ç”¨fastaiå¾®è°ƒBARTè¿›è¡Œæ‘˜è¦ | [Wayde Gilliam](https://ohmeow.com/) | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb)
    |'
- en: '| [Fine-tune a pre-trained Transformer on anyoneâ€™s tweets](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb)
    | How to generate tweets in the style of your favorite Twitter account by fine-tuning
    a GPT-2 model | [Boris Dayma](https://github.com/borisdayma) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb)
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| [ä¸ºä»»ä½•äººçš„æ¨æ–‡å¾®è°ƒé¢„è®­ç»ƒTransformer](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb)
    | å¦‚ä½•é€šè¿‡å¾®è°ƒGPT-2æ¨¡å‹ç”Ÿæˆæ‚¨å–œçˆ±çš„Twitterè´¦æˆ·é£æ ¼çš„æ¨æ–‡ | [Boris Dayma](https://github.com/borisdayma)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb)
    |'
- en: '| [Optimize ğŸ¤— Hugging Face models with Weights & Biases](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb)
    | A complete tutorial showcasing W&B integration with Hugging Face | [Boris Dayma](https://github.com/borisdayma)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb)
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| [ä½¿ç”¨Weightsï¼†Biasesä¼˜åŒ–ğŸ¤—Hugging Faceæ¨¡å‹](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb)
    | ä¸€ä¸ªå®Œæ•´çš„æ•™ç¨‹ï¼Œå±•ç¤ºäº†Wï¼†Bä¸Hugging Faceçš„é›†æˆ | [Boris Dayma](https://github.com/borisdayma)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb)
    |'
- en: '| [Pretrain Longformer](https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb)
    | How to build a â€œlongâ€ version of existing pretrained models | [Iz Beltagy](https://beltagy.net)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb)
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| [é¢„è®­ç»ƒLongformer](https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb)
    | å¦‚ä½•æ„å»ºç°æœ‰é¢„è®­ç»ƒæ¨¡å‹çš„â€œé•¿â€ç‰ˆæœ¬ | [Iz Beltagy](https://beltagy.net) | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb)
    |'
- en: '| [Fine-tune Longformer for QA](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb)
    | How to fine-tune longformer model for QA task | [Suraj Patil](https://github.com/patil-suraj)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb)
    |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| [ä¸ºQAå¾®è°ƒLongformer](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb)
    | å¦‚ä½•ä¸ºQAä»»åŠ¡å¾®è°ƒLongformeræ¨¡å‹ | [Suraj Patil](https://github.com/patil-suraj) | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb)
    |'
- en: '| [Evaluate Model with ğŸ¤—nlp](https://github.com/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb)
    | How to evaluate longformer on TriviaQA with `nlp` | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1m7eTGlPmLRgoPkkA7rkhQdZ9ydpmsdLE?usp=sharing)
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| [ä½¿ç”¨ğŸ¤—nlpè¯„ä¼°æ¨¡å‹](https://github.com/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb)
    | å¦‚ä½•ä½¿ç”¨`nlp`åœ¨TriviaQAä¸Šè¯„ä¼°Longformer | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1m7eTGlPmLRgoPkkA7rkhQdZ9ydpmsdLE?usp=sharing)
    |'
- en: '| [Fine-tune T5 for Sentiment Span Extraction](https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb)
    | How to fine-tune T5 for sentiment span extraction using a text-to-text format
    with PyTorch Lightning | [Lorenzo Ampil](https://github.com/enzoampil) | [![Open
    In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb)
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| [ä¸ºæƒ…æ„Ÿè·¨åº¦æå–å¾®è°ƒT5](https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb)
    | å¦‚ä½•ä½¿ç”¨PyTorch Lightningä»¥æ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼å¾®è°ƒT5è¿›è¡Œæƒ…æ„Ÿè·¨åº¦æå– | [Lorenzo Ampil](https://github.com/enzoampil)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb)
    |'
- en: '| [Fine-tune DistilBert for Multiclass Classification](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb)
    | How to fine-tune DistilBert for multiclass classification with PyTorch | [Abhishek
    Kumar Mishra](https://github.com/abhimishra91) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb)
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| [ä¸ºå¤šç±»åˆ†ç±»å¾®è°ƒDistilBert](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb)
    | å¦‚ä½•ä½¿ç”¨PyTorchå¾®è°ƒDistilBertè¿›è¡Œå¤šç±»åˆ†ç±» | [Abhishek Kumar Mishra](https://github.com/abhimishra91)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb)
    |'
- en: '| [Fine-tune BERT for Multi-label Classification](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)
    | How to fine-tune BERT for multi-label classification using PyTorch | [Abhishek
    Kumar Mishra](https://github.com/abhimishra91) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| [å¾®è°ƒBERTè¿›è¡Œå¤šæ ‡ç­¾åˆ†ç±»](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)
    | å¦‚ä½•ä½¿ç”¨PyTorchå¾®è°ƒBERTè¿›è¡Œå¤šæ ‡ç­¾åˆ†ç±» | [Abhishek Kumar Mishra](https://github.com/abhimishra91)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)
    |'
- en: '| [Fine-tune T5 for Summarization](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)
    | How to fine-tune T5 for summarization in PyTorch and track experiments with
    WandB | [Abhishek Kumar Mishra](https://github.com/abhimishra91) | [![Open In
    Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| [å¾®è°ƒT5è¿›è¡Œæ‘˜è¦](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)
    | å¦‚ä½•åœ¨PyTorchä¸­å¾®è°ƒT5è¿›è¡Œæ‘˜è¦ï¼Œå¹¶ä½¿ç”¨WandBè·Ÿè¸ªå®éªŒ | [Abhishek Kumar Mishra](https://github.com/abhimishra91)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)
    |'
- en: '| [Speed up Fine-Tuning in Transformers with Dynamic Padding / Bucketing](https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb)
    | How to speed up fine-tuning by a factor of 2 using dynamic padding / bucketing
    | [Michael Benesty](https://github.com/pommedeterresautee) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1CBfRU1zbfu7-ijiOqAAQUA-RJaxfcJoO?usp=sharing)
    |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| [ä½¿ç”¨åŠ¨æ€å¡«å……/åˆ†æ¡¶åŠ é€ŸTransformerä¸­çš„å¾®è°ƒ](https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb)
    | å¦‚ä½•é€šè¿‡åŠ¨æ€å¡«å……/åˆ†æ¡¶å°†å¾®è°ƒåŠ é€Ÿ2å€ | [Michael Benesty](https://github.com/pommedeterresautee)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1CBfRU1zbfu7-ijiOqAAQUA-RJaxfcJoO?usp=sharing)
    |'
- en: '| [Pretrain Reformer for Masked Language Modeling](https://github.com/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb)
    | How to train a Reformer model with bi-directional self-attention layers | [Patrick
    von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1tzzh0i8PgDQGV3SMFUGxM7_gGae3K-uW?usp=sharing)
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| [ä¸ºé®è”½è¯­è¨€å»ºæ¨¡é¢„è®­ç»ƒReformer](https://github.com/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb)
    | å¦‚ä½•è®­ç»ƒå…·æœ‰åŒå‘è‡ªæ³¨æ„åŠ›å±‚çš„Reformeræ¨¡å‹ | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1tzzh0i8PgDQGV3SMFUGxM7_gGae3K-uW?usp=sharing)
    |'
- en: '| [Expand and Fine Tune Sci-BERT](https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb)
    | How to increase vocabulary of a pretrained SciBERT model from AllenAI on the
    CORD dataset and pipeline it. | [Tanmay Thakur](https://github.com/lordtt13) |
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1rqAR40goxbAfez1xvF3hBJphSCsvXmh8)
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| [æ‰©å±•å’Œå¾®è°ƒSci-BERT](https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb)
    | å¦‚ä½•åœ¨CORDæ•°æ®é›†ä¸Šå¢åŠ é¢„è®­ç»ƒçš„SciBERTæ¨¡å‹çš„è¯æ±‡é‡å¹¶è¿›è¡Œæµæ°´çº¿å¤„ç†ã€‚ | [Tanmay Thakur](https://github.com/lordtt13)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1rqAR40goxbAfez1xvF3hBJphSCsvXmh8)
    |'
- en: '| [Fine Tune BlenderBotSmall for Summarization using the Trainer API](https://github.com/lordtt13/transformers-experiments/blob/master/Custom%20Tasks/fine-tune-blenderbot_small-for-summarization.ipynb)
    | How to fine-tune BlenderBotSmall for summarization on a custom dataset, using
    the Trainer API. | [Tanmay Thakur](https://github.com/lordtt13) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/19Wmupuls7mykSGyRN_Qo6lPQhgp56ymq?usp=sharing)
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| [ä½¿ç”¨Trainer APIå¾®è°ƒBlenderBotSmallè¿›è¡Œæ‘˜è¦](https://github.com/lordtt13/transformers-experiments/blob/master/Custom%20Tasks/fine-tune-blenderbot_small-for-summarization.ipynb)
    | å¦‚ä½•åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šä½¿ç”¨Trainer APIå¾®è°ƒBlenderBotSmallè¿›è¡Œæ‘˜è¦ | [Tanmay Thakur](https://github.com/lordtt13)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/19Wmupuls7mykSGyRN_Qo6lPQhgp56ymq?usp=sharing)
    |'
- en: '| [Fine-tune Electra and interpret with Integrated Gradients](https://github.com/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb)
    | How to fine-tune Electra for sentiment analysis and interpret predictions with
    Captum Integrated Gradients | [Eliza Szczechla](https://elsanns.github.io) | [![Open
    In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb)
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| [å¾®è°ƒElectraå¹¶ä½¿ç”¨Integrated Gradientsè¿›è¡Œè§£é‡Š](https://github.com/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb)
    | å¦‚ä½•å¾®è°ƒElectraè¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œå¹¶ä½¿ç”¨Captum Integrated Gradientsè§£é‡Šé¢„æµ‹ | [Eliza Szczechla](https://elsanns.github.io)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb)
    |'
- en: '| [fine-tune a non-English GPT-2 Model with Trainer class](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)
    | How to fine-tune a non-English GPT-2 Model with Trainer class | [Philipp Schmid](https://www.philschmid.de)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| [ä½¿ç”¨Trainerç±»å¾®è°ƒéè‹±è¯­GPT-2æ¨¡å‹](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)
    | å¦‚ä½•ä½¿ç”¨Trainerç±»å¾®è°ƒéè‹±è¯­GPT-2æ¨¡å‹ | [Philipp Schmid](https://www.philschmid.de) | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)
    |'
- en: '| [Fine-tune a DistilBERT Model for Multi Label Classification task](https://github.com/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb)
    | How to fine-tune a DistilBERT Model for Multi Label Classification task | [Dhaval
    Taunk](https://github.com/DhavalTaunk08) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb)
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| [ä¸ºå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡å¾®è°ƒDistilBERTæ¨¡å‹](https://github.com/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb)
    | å¦‚ä½•ä¸ºå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡å¾®è°ƒDistilBERTæ¨¡å‹ | [Dhaval Taunk](https://github.com/DhavalTaunk08)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb)
    |'
- en: '| [Fine-tune ALBERT for sentence-pair classification](https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb)
    | How to fine-tune an ALBERT model or another BERT-based model for the sentence-pair
    classification task | [Nadir El Manouzi](https://github.com/NadirEM) | [![Open
    In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb)
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| [ä¸ºå¥å¯¹åˆ†ç±»å¾®è°ƒALBERT](https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb)
    | å¦‚ä½•ä¸ºå¥å¯¹åˆ†ç±»ä»»åŠ¡å¾®è°ƒALBERTæ¨¡å‹æˆ–å…¶ä»–åŸºäºBERTçš„æ¨¡å‹ | [Nadir El Manouzi](https://github.com/NadirEM)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb)
    |'
- en: '| [Fine-tune Roberta for sentiment analysis](https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb)
    | How to fine-tune a Roberta model for sentiment analysis | [Dhaval Taunk](https://github.com/DhavalTaunk08)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb)
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| [Fine-tune Roberta for sentiment analysis](https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb)
    | å¦‚ä½•ä¸ºæƒ…æ„Ÿåˆ†æå¾®è°ƒä¸€ä¸ªRobertaæ¨¡å‹ | [Dhaval Taunk](https://github.com/DhavalTaunk08) | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb)
    |'
- en: '| [Evaluating Question Generation Models](https://github.com/flexudy-pipe/qugeev)
    | How accurate are the answers to questions generated by your seq2seq transformer
    model? | [Pascal Zoleko](https://github.com/zolekode) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1bpsSqCQU-iw_5nNoRm_crPq6FRuJthq_?usp=sharing)
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| [è¯„ä¼°é—®é¢˜ç”Ÿæˆæ¨¡å‹](https://github.com/flexudy-pipe/qugeev) | æ‚¨çš„seq2seqå˜å‹å™¨æ¨¡å‹ç”Ÿæˆçš„é—®é¢˜çš„ç­”æ¡ˆæœ‰å¤šå‡†ç¡®ï¼Ÿ
    | [Pascal Zoleko](https://github.com/zolekode) | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1bpsSqCQU-iw_5nNoRm_crPq6FRuJthq_?usp=sharing)
    |'
- en: '| [Classify text with DistilBERT and Tensorflow](https://github.com/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb)
    | How to fine-tune DistilBERT for text classification in TensorFlow | [Peter Bayerle](https://github.com/peterbayerle)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb)
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| [ä½¿ç”¨DistilBERTå’ŒTensorflowå¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»](https://github.com/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb)
    | å¦‚ä½•åœ¨TensorFlowä¸­ä¸ºæ–‡æœ¬åˆ†ç±»å¾®è°ƒDistilBERT | [Peter Bayerle](https://github.com/peterbayerle)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb)
    |'
- en: '| [Leverage BERT for Encoder-Decoder Summarization on CNN/Dailymail](https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)
    | How to warm-start a *EncoderDecoderModel* with a *bert-base-uncased* checkpoint
    for summarization on CNN/Dailymail | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| [åˆ©ç”¨BERTè¿›è¡ŒCNN/Dailymailçš„ç¼–ç å™¨-è§£ç å™¨æ‘˜è¦](https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)
    | å¦‚ä½•ä½¿ç”¨*bert-base-uncased*æ£€æŸ¥ç‚¹å¯¹CNN/Dailymailçš„æ‘˜è¦è¿›è¡Œçƒ­å¯åŠ¨*EncoderDecoderModel* | [Patrick
    von Platen](https://github.com/patrickvonplaten) | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)
    |'
- en: '| [Leverage RoBERTa for Encoder-Decoder Summarization on BBC XSum](https://github.com/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb)
    | How to warm-start a shared *EncoderDecoderModel* with a *roberta-base* checkpoint
    for summarization on BBC/XSum | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb)
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| [åˆ©ç”¨RoBERTaè¿›è¡ŒBBC XSumçš„ç¼–ç å™¨-è§£ç å™¨æ‘˜è¦](https://github.com/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb)
    | å¦‚ä½•ä½¿ç”¨*roberta-base*æ£€æŸ¥ç‚¹å¯¹BBC/XSumçš„æ‘˜è¦è¿›è¡Œçƒ­å¯åŠ¨å…±äº«*EncoderDecoderModel* | [Patrick von
    Platen](https://github.com/patrickvonplaten) | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb)
    |'
- en: '| [Fine-tune TAPAS on Sequential Question Answering (SQA)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)
    | How to fine-tune *TapasForQuestionAnswering* with a *tapas-base* checkpoint
    on the Sequential Question Answering (SQA) dataset | [Niels Rogge](https://github.com/nielsrogge)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| [åœ¨é¡ºåºé—®ç­”ï¼ˆSQAï¼‰ä¸Šå¾®è°ƒTAPAS](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)
    | å¦‚ä½•åœ¨é¡ºåºé—®ç­”ï¼ˆSQAï¼‰æ•°æ®é›†ä¸Šä½¿ç”¨*tapas-base*æ£€æŸ¥ç‚¹å¾®è°ƒ*TapasForQuestionAnswering* | [Niels Rogge](https://github.com/nielsrogge)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)
    |'
- en: '| [Evaluate TAPAS on Table Fact Checking (TabFact)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb)
    | How to evaluate a fine-tuned *TapasForSequenceClassification* with a *tapas-base-finetuned-tabfact*
    checkpoint using a combination of the ğŸ¤— datasets and ğŸ¤— transformers libraries
    | [Niels Rogge](https://github.com/nielsrogge) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb)
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| [åœ¨Table Fact Checkingï¼ˆTabFactï¼‰ä¸Šè¯„ä¼°TAPAS](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb)
    | å¦‚ä½•ä½¿ç”¨ğŸ¤—æ•°æ®é›†å’ŒğŸ¤—transformersåº“çš„ç»„åˆè¯„ä¼°ç»è¿‡å¾®è°ƒçš„*TapasForSequenceClassification*ï¼Œä½¿ç”¨*tapas-base-finetuned-tabfact*æ£€æŸ¥ç‚¹
    | [Niels Rogge](https://github.com/nielsrogge) | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb)
    |'
- en: '| [Fine-tuning mBART for translation](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb)
    | How to fine-tune mBART using Seq2SeqTrainer for Hindi to English translation
    | [Vasudev Gupta](https://github.com/vasudevgupta7) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb)
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| [ä¸ºç¿»è¯‘å¾®è°ƒmBART](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb)
    | å¦‚ä½•ä½¿ç”¨Seq2SeqTrainerä¸ºå°åœ°è¯­åˆ°è‹±è¯­ç¿»è¯‘å¾®è°ƒmBART | [Vasudev Gupta](https://github.com/vasudevgupta7)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb)
    |'
- en: '| [Fine-tune LayoutLM on FUNSD (a form understanding dataset)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb)
    | How to fine-tune *LayoutLMForTokenClassification* on the FUNSD dataset for information
    extraction from scanned documents | [Niels Rogge](https://github.com/nielsrogge)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb)
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| [åœ¨FUNSDä¸Šå¾®è°ƒLayoutLMï¼ˆä¸€ç§è¡¨å•ç†è§£æ•°æ®é›†ï¼‰](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb)
    | å¦‚ä½•åœ¨FUNSDæ•°æ®é›†ä¸Šå¾®è°ƒ*LayoutLMForTokenClassification*ï¼Œä»æ‰«ææ–‡æ¡£ä¸­æå–ä¿¡æ¯ | [Niels Rogge](https://github.com/nielsrogge)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb)
    |'
- en: '| [Fine-Tune DistilGPT2 and Generate Text](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb)
    | How to fine-tune DistilGPT2 and generate text | [Aakash Tripathi](https://github.com/tripathiaakash)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb)
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| [å¾®è°ƒDistilGPT2å¹¶ç”Ÿæˆæ–‡æœ¬](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb)
    | å¦‚ä½•å¾®è°ƒDistilGPT2å¹¶ç”Ÿæˆæ–‡æœ¬ | [Aakash Tripathi](https://github.com/tripathiaakash) |
    [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb)
    |'
- en: '| [Fine-Tune LED on up to 8K tokens](https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb)
    | How to fine-tune LED on pubmed for long-range summarization | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb)
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| [åœ¨æœ€å¤š8Kæ ‡è®°ä¸Šå¾®è°ƒLED](https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb)
    | å¦‚ä½•åœ¨pubmedä¸Šå¾®è°ƒLEDè¿›è¡Œé•¿è·ç¦»æ‘˜è¦ | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb)
    |'
- en: '| [Evaluate LED on Arxiv](https://github.com/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb)
    | How to effectively evaluate LED on long-range summarization | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb)
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| [åœ¨Arxivä¸Šè¯„ä¼°LED](https://github.com/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb)
    | å¦‚ä½•æœ‰æ•ˆè¯„ä¼°LEDè¿›è¡Œé•¿è·ç¦»æ‘˜è¦ | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb)
    |'
- en: '| [Fine-tune LayoutLM on RVL-CDIP (a document image classification dataset)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb)
    | How to fine-tune *LayoutLMForSequenceClassification* on the RVL-CDIP dataset
    for scanned document classification | [Niels Rogge](https://github.com/nielsrogge)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb)
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| [åœ¨RVL-CDIPä¸Šå¾®è°ƒLayoutLMï¼ˆä¸€ç§æ–‡æ¡£å›¾åƒåˆ†ç±»æ•°æ®é›†ï¼‰](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb)
    | å¦‚ä½•åœ¨RVL-CDIPæ•°æ®é›†ä¸Šå¾®è°ƒ*LayoutLMForSequenceClassification*ï¼Œç”¨äºæ‰«ææ–‡æ¡£åˆ†ç±» | [Niels Rogge](https://github.com/nielsrogge)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb)
    |'
- en: '| [Wav2Vec2 CTC decoding with GPT2 adjustment](https://github.com/voidful/huggingface_notebook/blob/main/xlsr_gpt.ipynb)
    | How to decode CTC sequence with language model adjustment | [Eric Lam](https://github.com/voidful)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1e_z5jQHYbO2YKEaUgzb1ww1WwiAyydAj?usp=sharing)
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '- [ä½¿ç”¨GPT2è°ƒæ•´è¿›è¡ŒWav2Vec2 CTCè§£ç ](https://github.com/voidful/huggingface_notebook/blob/main/xlsr_gpt.ipynb)
    | å¦‚ä½•ä½¿ç”¨è¯­è¨€æ¨¡å‹è°ƒæ•´è§£ç CTCåºåˆ— | [Eric Lam](https://github.com/voidful) | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1e_z5jQHYbO2YKEaUgzb1ww1WwiAyydAj?usp=sharing)
    |'
- en: '| [Fine-tune BART for summarization in two languages with Trainer class](https://github.com/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb)
    | How to fine-tune BART for summarization in two languages with Trainer class
    | [Eliza Szczechla](https://github.com/elsanns) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb)
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '- [ä½¿ç”¨Trainerç±»åœ¨ä¸¤ç§è¯­è¨€ä¸­å¯¹BARTè¿›è¡Œæ‘˜è¦å¾®è°ƒ](https://github.com/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb)
    | å¦‚ä½•ä½¿ç”¨Trainerç±»åœ¨ä¸¤ç§è¯­è¨€ä¸­å¯¹BARTè¿›è¡Œæ‘˜è¦å¾®è°ƒ | [Eliza Szczechla](https://github.com/elsanns)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb)
    |'
- en: '| [Evaluate Big Bird on Trivia QA](https://github.com/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb)
    | How to evaluate BigBird on long document question answering on Trivia QA | [Patrick
    von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb)
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '- [åœ¨Trivia QAä¸Šè¯„ä¼°Big Bird](https://github.com/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb)
    | å¦‚ä½•åœ¨Trivia QAä¸Šè¯„ä¼°BigBirdåœ¨é•¿æ–‡æ¡£é—®ç­”ä¸Šçš„è¡¨ç° | [Patrick von Platen](https://github.com/patrickvonplaten)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb)
    |'
- en: '| [Create video captions using Wav2Vec2](https://github.com/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb)
    | How to create YouTube captions from any video by transcribing the audio with
    Wav2Vec | [Niklas Muennighoff](https://github.com/Muennighoff) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb)
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '- [ä½¿ç”¨Wav2Vec2åˆ›å»ºè§†é¢‘å­—å¹•](https://github.com/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb)
    | å¦‚ä½•é€šè¿‡ä½¿ç”¨Wav2Vecè½¬å½•éŸ³é¢‘æ¥ä»ä»»ä½•è§†é¢‘åˆ›å»ºYouTubeå­—å¹• | [Niklas Muennighoff](https://github.com/Muennighoff)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb)
    |'
- en: '| [Fine-tune the Vision Transformer on CIFAR-10 using PyTorch Lightning](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb)
    | How to fine-tune the Vision Transformer (ViT) on CIFAR-10 using HuggingFace
    Transformers, Datasets and PyTorch Lightning | [Niels Rogge](https://github.com/nielsrogge)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb)
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '- [ä½¿ç”¨PyTorch Lightningåœ¨CIFAR-10ä¸Šå¾®è°ƒVision Transformer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb)
    | å¦‚ä½•ä½¿ç”¨HuggingFace Transformersã€Datasetså’ŒPyTorch Lightningåœ¨CIFAR-10ä¸Šå¾®è°ƒVision Transformerï¼ˆViTï¼‰
    | [Niels Rogge](https://github.com/nielsrogge) | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb)
    |'
- en: '| [Fine-tune the Vision Transformer on CIFAR-10 using the ğŸ¤— Trainer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb)
    | How to fine-tune the Vision Transformer (ViT) on CIFAR-10 using HuggingFace
    Transformers, Datasets and the ğŸ¤— Trainer | [Niels Rogge](https://github.com/nielsrogge)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb)
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '- [ä½¿ç”¨ğŸ¤— Traineråœ¨CIFAR-10ä¸Šå¾®è°ƒVision Transformer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb)
    | å¦‚ä½•ä½¿ç”¨HuggingFace Transformersã€Datasetså’ŒğŸ¤— Traineråœ¨CIFAR-10ä¸Šå¾®è°ƒVision Transformerï¼ˆViTï¼‰
    | [Niels Rogge](https://github.com/nielsrogge) | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb)
    |'
- en: '| [Evaluate LUKE on Open Entity, an entity typing dataset](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb)
    | How to evaluate *LukeForEntityClassification* on the Open Entity dataset | [Ikuya
    Yamada](https://github.com/ikuyamada) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb)
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '- [åœ¨Open Entityä¸Šè¯„ä¼°LUKEï¼Œä¸€ä¸ªå®ä½“ç±»å‹æ•°æ®é›†](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb)
    | å¦‚ä½•åœ¨Open Entityæ•°æ®é›†ä¸Šè¯„ä¼°*LukeForEntityClassification* | [Ikuya Yamada](https://github.com/ikuyamada)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb)
    |'
- en: '| [Evaluate LUKE on TACRED, a relation extraction dataset](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb)
    | How to evaluate *LukeForEntityPairClassification* on the TACRED dataset | [Ikuya
    Yamada](https://github.com/ikuyamada) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb)
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '- [åœ¨TACREDä¸Šè¯„ä¼°LUKEï¼Œä¸€ä¸ªå…³ç³»æŠ½å–æ•°æ®é›†](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb)
    | å¦‚ä½•åœ¨TACREDæ•°æ®é›†ä¸Šè¯„ä¼°*LukeForEntityPairClassification* | [Ikuya Yamada](https://github.com/ikuyamada)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb)
    |'
- en: '| [Evaluate LUKE on CoNLL-2003, an important NER benchmark](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb)
    | How to evaluate *LukeForEntitySpanClassification* on the CoNLL-2003 dataset
    | [Ikuya Yamada](https://github.com/ikuyamada) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb)
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| [åœ¨CoNLL-2003ä¸Šè¯„ä¼°LUKEï¼Œä¸€ä¸ªé‡è¦çš„NERåŸºå‡†](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb)
    | å¦‚ä½•åœ¨CoNLL-2003æ•°æ®é›†ä¸Šè¯„ä¼°*LukeForEntitySpanClassification* | [Ikuya Yamada](https://github.com/ikuyamada)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb)
    |'
- en: '| [Evaluate BigBird-Pegasus on PubMed dataset](https://github.com/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb)
    | How to evaluate *BigBirdPegasusForConditionalGeneration* on PubMed dataset |
    [Vasudev Gupta](https://github.com/vasudevgupta7) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb)
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| [åœ¨PubMedæ•°æ®é›†ä¸Šè¯„ä¼°BigBird-Pegasus](https://github.com/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb)
    | å¦‚ä½•åœ¨PubMedæ•°æ®é›†ä¸Šè¯„ä¼°*BigBirdPegasusForConditionalGeneration* | [Vasudev Gupta](https://github.com/vasudevgupta7)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb)
    |'
- en: '| [Speech Emotion Classification with Wav2Vec2](https://github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb)
    | How to leverage a pretrained Wav2Vec2 model for Emotion Classification on the
    MEGA dataset | [Mehrdad Farahani](https://github.com/m3hrdadfi) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb)
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| [ä½¿ç”¨Wav2Vec2è¿›è¡Œè¯­éŸ³æƒ…æ„Ÿåˆ†ç±»](https://github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb)
    | å¦‚ä½•åˆ©ç”¨é¢„è®­ç»ƒçš„Wav2Vec2æ¨¡å‹å¯¹MEGAæ•°æ®é›†è¿›è¡Œæƒ…æ„Ÿåˆ†ç±» | [Mehrdad Farahani](https://github.com/m3hrdadfi)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb)
    |'
- en: '| [Detect objects in an image with DETR](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb)
    | How to use a trained *DetrForObjectDetection* model to detect objects in an
    image and visualize attention | [Niels Rogge](https://github.com/NielsRogge) |
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb)
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| [ä½¿ç”¨DETRåœ¨å›¾åƒä¸­æ£€æµ‹å¯¹è±¡](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb)
    | å¦‚ä½•ä½¿ç”¨è®­ç»ƒå¥½çš„*DetrForObjectDetection*æ¨¡å‹åœ¨å›¾åƒä¸­æ£€æµ‹å¯¹è±¡å¹¶å¯è§†åŒ–æ³¨æ„åŠ› | [Niels Rogge](https://github.com/NielsRogge)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb)
    |'
- en: '| [Fine-tune DETR on a custom object detection dataset](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb)
    | How to fine-tune *DetrForObjectDetection* on a custom object detection dataset
    | [Niels Rogge](https://github.com/NielsRogge) | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb)
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| [åœ¨è‡ªå®šä¹‰å¯¹è±¡æ£€æµ‹æ•°æ®é›†ä¸Šå¾®è°ƒDETR](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb)
    | å¦‚ä½•åœ¨è‡ªå®šä¹‰å¯¹è±¡æ£€æµ‹æ•°æ®é›†ä¸Šå¾®è°ƒ*DetrForObjectDetection* | [Niels Rogge](https://github.com/NielsRogge)
    | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb)
    |'
- en: '| [Finetune T5 for Named Entity Recognition](https://github.com/ToluClassics/Notebooks/blob/main/T5_Ner_Finetuning.ipynb)
    | How to fine-tune *T5* on a Named Entity Recognition Task | [Ogundepo Odunayo](https://github.com/ToluClassics)
    | [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing)
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| [ä¸ºå‘½åå®ä½“è¯†åˆ«å¾®è°ƒT5](https://github.com/ToluClassics/Notebooks/blob/main/T5_Ner_Finetuning.ipynb)
    | å¦‚ä½•åœ¨å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸Šå¾®è°ƒ*T5* | [Ogundepo Odunayo](https://github.com/ToluClassics) | [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing)
    |'
