["```py\n>>> from transformers import XGLMModel, XGLMConfig\n\n>>> # Initializing a XGLM facebook/xglm-564M style configuration\n>>> configuration = XGLMConfig()\n\n>>> # Initializing a model from the facebook/xglm-564M style configuration\n>>> model = XGLMModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoTokenizer, XGLMModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-564M\")\n>>> model = XGLMModel.from_pretrained(\"facebook/xglm-564M\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, XGLMForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-564M\")\n>>> model = XGLMForCausalLM.from_pretrained(\"facebook/xglm-564M\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, TFXGLMModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-564M\")\n>>> model = TFXGLMModel.from_pretrained(\"facebook/xglm-564M\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, TFXGLMForCausalLM\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-564M\")\n>>> model = TFXGLMForCausalLM.from_pretrained(\"facebook/xglm-564M\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxXGLMModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-564M\")\n>>> model = FlaxXGLMModel.from_pretrained(\"facebook/xglm-564M\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxXGLMForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-564M\")\n>>> model = FlaxXGLMForCausalLM.from_pretrained(\"facebook/xglm-564M\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"np\")\n>>> outputs = model(**inputs)\n\n>>> # retrieve logts for next token\n>>> next_token_logits = outputs.logits[:, -1]\n```"]