- en: BertJapanese
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bert-japanese](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bert-japanese)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/69.4f9d8c9f.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The BERT models trained on Japanese text.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are models with two different tokenization methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenize with MeCab and WordPiece. This requires some extra dependencies, [fugashi](https://github.com/polm/fugashi)
    which is a wrapper around [MeCab](https://taku910.github.io/mecab/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenize into characters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To use *MecabTokenizer*, you should `pip install transformers["ja"]` (or `pip
    install -e .["ja"]` if you install from source) to install dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: See [details on cl-tohoku repository](https://github.com/cl-tohoku/bert-japanese).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example of using a model with MeCab and WordPiece tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Example of using a model with Character tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This model was contributed by [cl-tohoku](https://huggingface.co/cl-tohoku).
  prefs: []
  type: TYPE_NORMAL
- en: This implementation is the same as BERT, except for tokenization method. Refer
    to [BERT documentation](bert) for API reference information.
  prefs: []
  type: TYPE_NORMAL
- en: BertJapaneseTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.BertJapaneseTokenizer'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L107)'
  prefs: []
  type: TYPE_NORMAL
- en: ( vocab_file spm_file = None do_lower_case = False do_word_tokenize = True do_subword_tokenize
    = True word_tokenizer_type = 'basic' subword_tokenizer_type = 'wordpiece' never_split
    = None unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token =
    '[CLS]' mask_token = '[MASK]' mecab_kwargs = None sudachi_kwargs = None jumanpp_kwargs
    = None **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab_file** (`str`) — Path to a one-wordpiece-per-line vocabulary file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**spm_file** (`str`, *optional*) — Path to [SentencePiece](https://github.com/google/sentencepiece)
    file (generally has a .spm or .model extension) that contains the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_lower_case** (`bool`, *optional*, defaults to `True`) — Whether to lower
    case the input. Only has an effect when do_basic_tokenize=True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_word_tokenize** (`bool`, *optional*, defaults to `True`) — Whether to
    do word tokenization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_subword_tokenize** (`bool`, *optional*, defaults to `True`) — Whether
    to do subword tokenization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**word_tokenizer_type** (`str`, *optional*, defaults to `"basic"`) — Type of
    word tokenizer. Choose from [“basic”, “mecab”, “sudachi”, “jumanpp”].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**subword_tokenizer_type** (`str`, *optional*, defaults to `"wordpiece"`) —
    Type of subword tokenizer. Choose from [“wordpiece”, “character”, “sentencepiece”,].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mecab_kwargs** (`dict`, *optional*) — Dictionary passed to the `MecabTokenizer`
    constructor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sudachi_kwargs** (`dict`, *optional*) — Dictionary passed to the `SudachiTokenizer`
    constructor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**jumanpp_kwargs** (`dict`, *optional*) — Dictionary passed to the `JumanppTokenizer`
    constructor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a BERT tokenizer for Japanese text.
  prefs: []
  type: TYPE_NORMAL
- en: 'This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to: this superclass
    for more information regarding those methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### build_inputs_with_special_tokens'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L307)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. A BERT sequence has the following
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'single sequence: `[CLS] X [SEP]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'pair of sequences: `[CLS] A [SEP] B [SEP]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### convert_tokens_to_string'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L299)'
  prefs: []
  type: TYPE_NORMAL
- en: ( tokens )
  prefs: []
  type: TYPE_NORMAL
- en: Converts a sequence of tokens (string) in a single string.
  prefs: []
  type: TYPE_NORMAL
- en: '#### create_token_type_ids_from_sequences'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L362)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of [token type IDs](../glossary#token-type-ids) according to the given
    sequence(s).
  prefs: []
  type: TYPE_NORMAL
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. A BERT sequence
  prefs: []
  type: TYPE_NORMAL
- en: 'pair mask has the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If `token_ids_1` is `None`, this method only returns the first portion of the
    mask (0s).
  prefs: []
  type: TYPE_NORMAL
- en: '#### get_special_tokens_mask'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L333)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens:
    bool = False ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**already_has_special_tokens** (`bool`, *optional*, defaults to `False`) —
    Whether or not the token list is already formatted with special tokens for the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  prefs: []
  type: TYPE_NORMAL
