- en: The Command Line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/accelerate/package_reference/cli](https://huggingface.co/docs/accelerate/package_reference/cli)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/accelerate/v0.27.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/start.6e0fb178.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/scheduler.69131cc3.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/singletons.ac467c20.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/paths.b2f3aeca.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/app.67e11fc0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/index.e1f30d73.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/0.bfeed9f0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/18.04dad9a3.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Tip.22e79575.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/CodeBlock.30cef355.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Heading.0aab6758.js">
  prefs: []
  type: TYPE_NORMAL
- en: Below is a list of all the available commands 🤗 Accelerate with their parameters
  prefs: []
  type: TYPE_NORMAL
- en: accelerate config
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Command**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`accelerate config` or `accelerate-config`'
  prefs: []
  type: TYPE_NORMAL
- en: Launches a series of prompts to create and save a `default_config.yml` configuration
    file for your training system. Should always be ran first on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: '**Usage**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Optional Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--config_file CONFIG_FILE` (`str`) — The path to use to store the config file.
    Will default to a file named default_config.yaml in the cache location, which
    is the content of the environment `HF_HOME` suffixed with ‘accelerate’, or if
    you don’t have such an environment variable, your cache directory (`~/.cache`
    or the content of `XDG_CACHE_HOME`) suffixed with `huggingface`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-h`, `--help` (`bool`) — Show a help message and exit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: accelerate config default
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Command**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`accelerate config default` or `accelerate-config default`'
  prefs: []
  type: TYPE_NORMAL
- en: Create a default config file for Accelerate with only a few flags set.
  prefs: []
  type: TYPE_NORMAL
- en: '**Usage**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Optional Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--config_file CONFIG_FILE` (`str`) — The path to use to store the config file.
    Will default to a file named default_config.yaml in the cache location, which
    is the content of the environment `HF_HOME` suffixed with ‘accelerate’, or if
    you don’t have such an environment variable, your cache directory (`~/.cache`
    or the content of `XDG_CACHE_HOME`) suffixed with `huggingface`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-h`, `--help` (`bool`) — Show a help message and exit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--mixed_precision {no,fp16,bf16}` (`str`) — Whether or not to use mixed precision
    training. Choose between FP16 and BF16 (bfloat16) training. BF16 training is only
    supported on Nvidia Ampere GPUs and PyTorch 1.10 or later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: accelerate config update
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Command**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`accelerate config update` or `accelerate-config update`'
  prefs: []
  type: TYPE_NORMAL
- en: Update an existing config file with the latest defaults while maintaining the
    old configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '**Usage**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Optional Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--config_file CONFIG_FILE` (`str`) — The path to the config file to update.
    Will default to a file named default_config.yaml in the cache location, which
    is the content of the environment `HF_HOME` suffixed with ‘accelerate’, or if
    you don’t have such an environment variable, your cache directory (`~/.cache`
    or the content of `XDG_CACHE_HOME`) suffixed with `huggingface`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-h`, `--help` (`bool`) — Show a help message and exit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: accelerate env
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Command**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`accelerate env` or `accelerate-env` or `python -m accelerate.commands.env`'
  prefs: []
  type: TYPE_NORMAL
- en: Lists the contents of the passed 🤗 Accelerate configuration file. Should always
    be used when opening an issue on the [GitHub repository](https://github.com/huggingface/accelerate).
  prefs: []
  type: TYPE_NORMAL
- en: '**Usage**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Optional Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--config_file CONFIG_FILE` (`str`) — The path to use to store the config file.
    Will default to a file named default_config.yaml in the cache location, which
    is the content of the environment `HF_HOME` suffixed with ‘accelerate’, or if
    you don’t have such an environment variable, your cache directory (`~/.cache`
    or the content of `XDG_CACHE_HOME`) suffixed with `huggingface`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-h`, `--help` (`bool`) — Show a help message and exit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: accelerate launch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Command**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`accelerate launch` or `accelerate-launch` or `python -m accelerate.commands.launch`'
  prefs: []
  type: TYPE_NORMAL
- en: Launches a specified script on a distributed system with the right parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Usage**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Positional Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`{training_script}` — The full path to the script to be launched in parallel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--{training_script-argument-1}` — Arguments of the training script'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optional Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-h`, `--help` (`bool`) — Show a help message and exit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--config_file CONFIG_FILE` (`str`)— The config file to use for the default
    values in the launching script.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-m`, `--module` (`bool`) — Change each process to interpret the launch script
    as a Python module, executing with the same behavior as ‘python -m’.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--no_python` (`bool`) — Skip prepending the training script with ‘python’
    - just execute it directly. Useful when the script is not a Python script.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--debug` (`bool`) — Whether to print out the torch.distributed stack trace
    when something fails.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-q`, `--quiet` (`bool`) — Silence subprocess errors from the launch stack
    trace to only show the relevant tracebacks. (Only applicable to DeepSpeed and
    single-process configurations).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rest of these arguments are configured through `accelerate config` and are
    read in from the specified `--config_file` (or default configuration) for their
    values. They can also be passed in manually.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware Selection Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--cpu` (`bool`) — Whether or not to force the training on the CPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--multi_gpu` (`bool`) — Whether or not this should launch a distributed GPU
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--tpu` (`bool`) — Whether or not this should launch a TPU training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--ipex` (`bool`) — Whether or not this should launch an Intel Pytorch Extension
    (IPEX) training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource Selection Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following arguments are useful for fine-tuning how available hardware should
    be used
  prefs: []
  type: TYPE_NORMAL
- en: '`--mixed_precision {no,fp16,bf16}` (`str`) — Whether or not to use mixed precision
    training. Choose between FP16 and BF16 (bfloat16) training. BF16 training is only
    supported on Nvidia Ampere GPUs and PyTorch 1.10 or later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--num_processes NUM_PROCESSES` (`int`) — The total number of processes to
    be launched in parallel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--num_machines NUM_MACHINES` (`int`) — The total number of machines used in
    this training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--num_cpu_threads_per_process NUM_CPU_THREADS_PER_PROCESS` (`int`) — The number
    of CPU threads per process. Can be tuned for optimal performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training Paradigm Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following arguments are useful for selecting which training paradigm to
    use.
  prefs: []
  type: TYPE_NORMAL
- en: '`--use_deepspeed` (`bool`) — Whether or not to use DeepSpeed for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--use_fsdp` (`bool`) — Whether or not to use FullyShardedDataParallel for
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--use_megatron_lm` (`bool`) — Whether or not to use Megatron-LM for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--use_xpu` (`bool`) — Whether to use IPEX plugin to speed up training on XPU
    specifically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed GPU Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following arguments are only useful when `multi_gpu` is passed or multi-gpu
    training is configured through `accelerate config`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--gpu_ids` (`str`) — What GPUs (by id) should be used for training on this
    machine as a comma-seperated list'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--same_network` (`bool`) — Whether all machines used for multinode training
    exist on the same local network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--machine_rank MACHINE_RANK` (`int`) — The rank of the machine on which this
    script is launched.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--main_process_ip MAIN_PROCESS_IP` (`str`) — The IP address of the machine
    of rank 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--main_process_port MAIN_PROCESS_PORT` (`int`) — The port to use to communicate
    with the machine of rank 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--rdzv_backend` (`str`) — The rendezvous method to use, such as “static” or
    “c10d”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--rdzv_conf` (`str`) — Additional rendezvous configuration (<key1>=<value1>,<key2>=<value2>,…).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--max_restarts` (`int`) — Maximum number of worker group restarts before failing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--monitor_interval` (`float`) — Interval, in seconds, to monitor the state
    of workers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TPU Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following arguments are only useful when `tpu` is passed or TPU training
    is configured through `accelerate config`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--main_training_function MAIN_TRAINING_FUNCTION` (`str`) — The name of the
    main function to be executed in your script.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--downcast_bf16` (`bool`) — Whether when using bf16 precision on TPUs if both
    float and double tensors are cast to bfloat16 or if double tensors remain as float32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeepSpeed Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following arguments are only useful when `use_deepspeed` is passed or `deepspeed`
    is configured through `accelerate config`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--deepspeed_config_file` (`str`) — DeepSpeed config file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--zero_stage` (`int`) — DeepSpeed’s ZeRO optimization stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--offload_optimizer_device` (`str`) — Decides where (none|cpu|nvme) to offload
    optimizer states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--offload_param_device` (`str`) — Decides where (none|cpu|nvme) to offload
    parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--gradient_accumulation_steps` (`int`) — No of gradient_accumulation_steps
    used in your training script.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--gradient_clipping` (`float`) — Gradient clipping value used in your training
    script.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--zero3_init_flag` (`str`) — Decides Whether (true|false) to enable `deepspeed.zero.Init`
    for constructing massive models. Only applicable with DeepSpeed ZeRO Stage-3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--zero3_save_16bit_model` (`str`) — Decides Whether (true|false) to save 16-bit
    model weights when using ZeRO Stage-3\. Only applicable with DeepSpeed ZeRO Stage-3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--deepspeed_hostfile` (`str`) — DeepSpeed hostfile for configuring multi-node
    compute resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--deepspeed_exclusion_filter` (`str`) — DeepSpeed exclusion filter string
    when using mutli-node setup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--deepspeed_inclusion_filter` (`str`) — DeepSpeed inclusion filter string
    when using mutli-node setup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--deepspeed_multinode_launcher` (`str`) — DeepSpeed multi-node launcher to
    use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fully Sharded Data Parallelism Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following arguments are only useful when `use_fsdp` is passed or Fully
    Sharded Data Parallelism is configured through `accelerate config`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--fsdp_offload_params` (`str`) — Decides Whether (true|false) to offload parameters
    and gradients to CPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--fsdp_min_num_params` (`int`) — FSDP’s minimum number of parameters for Default
    Auto Wrapping.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--fsdp_sharding_strategy` (`int`) — FSDP’s Sharding Strategy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--fsdp_auto_wrap_policy` (`str`) — FSDP’s auto wrap policy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--fsdp_transformer_layer_cls_to_wrap` (`str`) — Transformer layer class name
    (case-sensitive) to wrap, e.g, `BertLayer`, `GPTJBlock`, `T5Block` …'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--fsdp_backward_prefetch_policy` (`str`) — FSDP’s backward prefetch policy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--fsdp_state_dict_type` (`str`) — FSDP’s state dict type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Megatron-LM Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following arguments are only useful when `use_megatron_lm` is passed or
    Megatron-LM is configured through `accelerate config`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--megatron_lm_tp_degree` (“) — Megatron-LM’s Tensor Parallelism (TP) degree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--megatron_lm_pp_degree` (“) — Megatron-LM’s Pipeline Parallelism (PP) degree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--megatron_lm_num_micro_batches` (“) — Megatron-LM’s number of micro batches
    when PP degree > 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--megatron_lm_sequence_parallelism` (“) — Decides Whether (true|false) to
    enable Sequence Parallelism when TP degree > 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--megatron_lm_recompute_activations` (“) — Decides Whether (true|false) to
    enable Selective Activation Recomputation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--megatron_lm_use_distributed_optimizer` (“) — Decides Whether (true|false)
    to use distributed optimizer which shards optimizer state and gradients across
    Data Parallel (DP) ranks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--megatron_lm_gradient_clipping` (“) — Megatron-LM’s gradient clipping value
    based on global L2 Norm (0 to disable).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS SageMaker Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following arguments are only useful when training in SageMaker
  prefs: []
  type: TYPE_NORMAL
- en: '`--aws_access_key_id AWS_ACCESS_KEY_ID` (`str`) — The AWS_ACCESS_KEY_ID used
    to launch the Amazon SageMaker training job'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--aws_secret_access_key AWS_SECRET_ACCESS_KEY` (`str`) — The AWS_SECRET_ACCESS_KEY
    used to launch the Amazon SageMaker training job'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: accelerate estimate-memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Command**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`accelerate estimate-memory` or `accelerate-estimate-memory` or `python -m
    accelerate.commands.estimate`'
  prefs: []
  type: TYPE_NORMAL
- en: Estimates the total vRAM a particular model hosted on the Hub needs to be loaded
    in with an estimate for training. Requires that `huggingface_hub` be installed.
  prefs: []
  type: TYPE_NORMAL
- en: When performing inference, typically add ≤20% to the result as overall allocation
    [as referenced here](https://blog.eleuther.ai/transformer-math/). We will have
    more extensive estimations in the future that will automatically be included in
    the calculation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Usage**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Required Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MODEL_NAME` (`str`)— The model name on the Hugging Face Hub'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optional Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--library_name {timm,transformers}` (`str`) — The library the model has an
    integration with, such as `transformers`, needed only if this information is not
    stored on the Hub'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--dtypes {float32,float16,int8,int4}` (`[{float32,float16,int8,int4} ...]`)
    — The dtypes to use for the model, must be one (or many) of `float32`, `float16`,
    `int8`, and `int4`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--trust_remote_code` (`bool`) — Whether or not to allow for custom models
    defined on the Hub in their own modeling files. This option should only be passed
    for repositories you trust and in which you have read the code, as it will execute
    code present on the Hub on your local machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: accelerate tpu-config
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`accelerate tpu-config`'
  prefs: []
  type: TYPE_NORMAL
- en: '**Usage**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Optional Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-h`, `--help` (`bool`) — Show a help message and exit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Config Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: Arguments that can be configured through `accelerate config`.
  prefs: []
  type: TYPE_NORMAL
- en: '`--config_file` (`str`) — Path to the config file to use for accelerate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--tpu_name` (`str`) — The name of the TPU to use. If not specified, will use
    the TPU specified in the config file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--tpu_zone` (`str`) — The zone of the TPU to use. If not specified, will use
    the zone specified in the config file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TPU Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: Arguments for options ran inside the TPU.
  prefs: []
  type: TYPE_NORMAL
- en: '`--command_file` (`str`) — The path to the file containing the commands to
    run on the pod on startup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--command` (`str`) — A command to run on the pod. Can be passed multiple times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--install_accelerate` (`bool`) — Whether to install accelerate on the pod.
    Defaults to False.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--accelerate_version` (`str`) — The version of accelerate to install on the
    pod. If not specified, will use the latest pypi version. Specify ‘dev’ to install
    from GitHub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--debug` (`bool`) — If set, will print the command that would be run instead
    of running it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: accelerate test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`accelerate test` or `accelerate-test`'
  prefs: []
  type: TYPE_NORMAL
- en: Runs `accelerate/test_utils/test_script.py` to verify that 🤗 Accelerate has
    been properly configured on your system and runs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Usage**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Optional Arguments**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--config_file CONFIG_FILE` (`str`) — The path to use to store the config file.
    Will default to a file named default_config.yaml in the cache location, which
    is the content of the environment `HF_HOME` suffixed with ‘accelerate’, or if
    you don’t have such an environment variable, your cache directory (`~/.cache`
    or the content of `XDG_CACHE_HOME`) suffixed with `huggingface`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-h`, `--help` (`bool`) — Show a help message and exit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
