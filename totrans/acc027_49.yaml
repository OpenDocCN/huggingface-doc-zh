- en: Helpful Utilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/accelerate/package_reference/utilities](https://huggingface.co/docs/accelerate/package_reference/utilities)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/accelerate/v0.27.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/start.6e0fb178.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/scheduler.69131cc3.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/singletons.ac467c20.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/paths.b2f3aeca.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/app.67e11fc0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/index.e1f30d73.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/0.bfeed9f0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/29.e1941334.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Tip.22e79575.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Docstring.ae1a1e2d.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Heading.0aab6758.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/CodeBlock.30cef355.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/ExampleCodeBlock.e7a3d5fe.js">
  prefs: []
  type: TYPE_NORMAL
- en: Below are a variety of utility functions that 🤗 Accelerate provides, broken
    down by use-case.
  prefs: []
  type: TYPE_NORMAL
- en: Constants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Constants used throughout 🤗 Accelerate for reference
  prefs: []
  type: TYPE_NORMAL
- en: The following are constants used when utilizing [Accelerator.save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)
  prefs: []
  type: TYPE_NORMAL
- en: '`utils.MODEL_NAME`: `"pytorch_model"` `utils.OPTIMIZER_NAME`: `"optimizer"`
    `utils.RNG_STATE_NAME`: `"random_states"` `utils.SCALER_NAME`: `"scaler.pt` `utils.SCHEDULER_NAME`:
    `"scheduler`'
  prefs: []
  type: TYPE_NORMAL
- en: The following are constants used when utilizing [Accelerator.save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)
  prefs: []
  type: TYPE_NORMAL
- en: '`utils.WEIGHTS_NAME`: `"pytorch_model.bin"` `utils.SAFE_WEIGHTS_NAME`: `"model.safetensors"`
    `utils.WEIGHTS_INDEX_NAME`: `"pytorch_model.bin.index.json"` `utils.SAFE_WEIGHTS_INDEX_NAME`:
    `"model.safetensors.index.json"`'
  prefs: []
  type: TYPE_NORMAL
- en: Data Classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These are basic dataclasses used throughout 🤗 Accelerate and they can be passed
    in as parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Standalone
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These are standalone dataclasses used for checks, such as the type of distributed
    system being used
  prefs: []
  type: TYPE_NORMAL
- en: '### `class accelerate.utils.ComputeEnvironment`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L329)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Represents a type of the compute environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LOCAL_MACHINE` — private/custom cluster hardware.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AMAZON_SAGEMAKER` — Amazon SageMaker as compute environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### `class accelerate.DistributedType`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L285)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Represents a type of distributed environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NO` — Not a distributed environment, just a single process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MULTI_CPU` — Distributed on multiple CPU nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MULTI_GPU` — Distributed on multiple GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MULTI_NPU` — Distributed on multiple NPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MULTI_XPU` — Distributed on multiple XPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DEEPSPEED` — Using DeepSpeed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TPU` — Distributed on TPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### `class accelerate.utils.DynamoBackend`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L344)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Represents a dynamo backend (see [https://github.com/pytorch/torchdynamo](https://github.com/pytorch/torchdynamo)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NO` — Do not use torch dynamo.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EAGER` — Uses PyTorch to run the extracted GraphModule. This is quite useful
    in debugging TorchDynamo issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AOT_EAGER` — Uses AotAutograd with no compiler, i.e, just using PyTorch eager
    for the AotAutograd’s extracted forward and backward graphs. This is useful for
    debugging, and unlikely to give speedups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`INDUCTOR` — Uses TorchInductor backend with AotAutograd and cudagraphs by
    leveraging codegened Triton kernels. [Read more](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AOT_TS_NVFUSER` — nvFuser with AotAutograd/TorchScript. [Read more](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NVPRIMS_NVFUSER` — nvFuser with PrimTorch. [Read more](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CUDAGRAPHS` — cudagraphs with AotAutograd. [Read more](https://github.com/pytorch/torchdynamo/pull/757)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OFI` — Uses Torchscript optimize_for_inference. Inference only. [Read more](https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FX2TRT` — Uses Nvidia TensorRT for inference optimizations. Inference only.
    [Read more](https://github.com/pytorch/TensorRT/blob/master/docsrc/tutorials/getting_started_with_fx_path.rst)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ONNXRT` — Uses ONNXRT for inference on CPU/GPU. Inference only. [Read more](https://onnxruntime.ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TENSORRT` — Uses ONNXRT to run TensorRT for inference optimizations. [Read
    more](https://github.com/onnx/onnx-tensorrt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IPEX` — Uses IPEX for inference on CPU. Inference only. [Read more](https://github.com/intel/intel-extension-for-pytorch).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TVM` — Uses Apach TVM for inference optimizations. [Read more](https://tvm.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### `class accelerate.utils.LoggerType`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L392)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Represents a type of supported experiment tracker
  prefs: []
  type: TYPE_NORMAL
- en: 'Values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ALL` — all available trackers in the environment that are supported'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TENSORBOARD` — TensorBoard as an experiment tracker'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WANDB` — wandb as an experiment tracker'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`COMETML` — comet_ml as an experiment tracker'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DVCLIVE` — dvclive as an experiment tracker'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### `class accelerate.utils.PrecisionType`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L414)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Represents a type of precision used on floating point values
  prefs: []
  type: TYPE_NORMAL
- en: 'Values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NO` — using full precision (FP32)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FP16` — using half precision'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BF16` — using brain floating point precision'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### `class accelerate.utils.RNGType`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L430)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: An enumeration.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class accelerate.utils.SageMakerDistributedType`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L312)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Represents a type of distributed environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NO` — Not a distributed environment, just a single process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DATA_PARALLEL` — using sagemaker distributed data parallelism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MODEL_PARALLEL` — using sagemaker distributed model parallelism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwargs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These are configurable arguments for specific interactions throughout the PyTorch
    ecosystem that Accelerate handles under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class accelerate.AutocastKwargs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L60)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize how `torch.autocast` behaves. Please refer to the documentation of
    this [context manager](https://pytorch.org/docs/stable/amp.html#torch.autocast)
    for more information on each argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '### `class accelerate.DistributedDataParallelKwargs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L82)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize how your model is wrapped in a `torch.nn.parallel.DistributedDataParallel`.
    Please refer to the documentation of this [wrapper](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
    for more information on each argument.
  prefs: []
  type: TYPE_NORMAL
- en: '`gradient_as_bucket_view` is only available in PyTorch 1.7.0 and later versions.'
  prefs: []
  type: TYPE_NORMAL
- en: '`static_graph` is only available in PyTorch 1.11.0 and later versions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '### `class accelerate.utils.FP8RecipeKwargs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L179)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`backend` (`str`, *optional*, defaults to “msamp”) — Which FP8 engine to use.
    Must be one of `"msamp"` (MS-AMP) or `"te"` (TransformerEngine).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`margin` (`int`, *optional*, default to 0) — The margin to use for the gradient
    scaling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`interval` (`int`, *optional*, default to 1) — The interval to use for how
    often the scaling factor is recomputed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fp8_format` (`str`, *optional*, default to “E4M3”) — The format to use for
    the FP8 recipe. Must be one of `E4M3` or `HYBRID`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`amax_history_len` (`int`, *optional*, default to 1024) — The length of the
    history to use for the scaling factor computation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`amax_compute_algo` (`str`, *optional*, default to “most_recent”) — The algorithm
    to use for the scaling factor computation. Must be one of `max` or `most_recent`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`override_linear_precision` (`tuple` of three `bool`, *optional*, default to
    `(False, False, False)`) — Whether or not to execute `fprop`, `dgrad`, and `wgrad`
    GEMMS in higher precision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimization_level` (`str`), one of `O1`, `O2`. (default is `O2`) — What level
    of 8-bit collective communication should be used with MS-AMP. In general:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'O1: Weight gradients and `all_reduce` communications are done in fp8, reducing
    GPU memory usage and communication bandwidth'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'O2: First-order optimizer states are in 8-bit, and second order states are
    in FP16. Only available when using Adam or AdamW. This maintains accuracy and
    can potentially save the highest memory.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '03: Specifically for DeepSpeed, implements capabilities so weights and master
    weights of models are stored in FP8\. If `fp8` is selected and deepspeed is enabled,
    will be used by default. (Not available currently).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize the initialization of the recipe for FP8 mixed precision training
    with `transformer-engine` or `ms-amp`.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on `transformer-engine` args, please refer to the API [documentation](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html).
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the `ms-amp` args, please refer to the Optimization
    Level [documentation](https://azure.github.io/MS-AMP/docs/user-tutorial/optimization-level).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To use MS-AMP as an engine, pass `backend="msamp"` and the `optimization_level`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '### `class accelerate.GradScalerKwargs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L118)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize the behavior of mixed precision, specifically how the `torch.cuda.amp.GradScaler`
    used is created. Please refer to the documentation of this [scaler](https://pytorch.org/docs/stable/amp.html?highlight=gradscaler)
    for more information on each argument.
  prefs: []
  type: TYPE_NORMAL
- en: '`GradScaler` is only available in PyTorch 1.5.0 and later versions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '### `class accelerate.InitProcessGroupKwargs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L149)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize the initialization of the distributed processes. Please refer to
    the documentation of this [method](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)
    for more information on each argument.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Plugins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are plugins that can be passed to the [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    object. While they are defined elsewhere in the documentation, for convenience
    all of them are available to see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '### `class accelerate.DeepSpeedPlugin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L562)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This plugin is used to integrate DeepSpeed.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `deepspeed_config_process`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L756)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Process the DeepSpeed config with the values from the kwargs.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class accelerate.FullyShardedDataParallelPlugin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L872)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This plugin is used to enable fully sharded data parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_module_class_from_name`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L1025)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`module` (`torch.nn.Module`) — The module to get the class from.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name` (`str`) — The name of the class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gets a class from a module by its name.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class accelerate.utils.GradientAccumulationPlugin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L505)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: A plugin to configure gradient accumulation behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class accelerate.utils.MegatronLMPlugin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L1105)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Plugin for Megatron-LM to enable tensor, pipeline, sequence and data parallelism.
    Also to enable selective activation recomputation and optimized fused kernels.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class accelerate.utils.TorchDynamoPlugin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L526)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This plugin is used to compile a model with PyTorch 2.0
  prefs: []
  type: TYPE_NORMAL
- en: Configurations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These are classes which can be configured and passed through to the appropriate
    integration
  prefs: []
  type: TYPE_NORMAL
- en: '### `class accelerate.utils.BnbQuantizationConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L1480)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: A plugin to enable BitsAndBytes 4bit and 8bit quantization
  prefs: []
  type: TYPE_NORMAL
- en: '### `class accelerate.utils.ProjectConfiguration`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L457)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Configuration for the Accelerator object based on inner-project needs.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_directories`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L495)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Sets `self.project_dir` and `self.logging_dir` to the appropriate values.
  prefs: []
  type: TYPE_NORMAL
- en: Environmental Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These are environmental variables that can be enabled for different use cases
  prefs: []
  type: TYPE_NORMAL
- en: '`ACCELERATE_DEBUG_MODE` (`str`): Whether to run accelerate in debug mode. More
    info available [here](../usage_guides/debug.md).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Manipulation and Operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These include data operations that mimic the same `torch` ops but can be used
    on distributed processes.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.broadcast`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L542)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`tensor` (nested list/tuple/dictionary of `torch.Tensor`) — The data to gather.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`from_process` (`int`, *optional*, defaults to 0) — The process from which
    to send the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursively broadcast tensor in a nested list/tuple/dictionary of tensors to
    all devices.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.broadcast_object_list`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L564)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`object_list` (list of picklable objects) — The list of objects to broadcast.
    This list will be modified inplace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`from_process` (`int`, *optional*, defaults to 0) — The process from which
    to send the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Broadcast a list of picklable objects form one process to the others.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.concatenate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L605)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`data` (nested list/tuple/dictionary of lists of tensors `torch.Tensor`) —
    The data to concatenate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dim` (`int`, *optional*, defaults to 0) — The dimension on which to concatenate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursively concatenate the tensors in a nested list/tuple/dictionary of lists
    of tensors with the same shape.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.convert_outputs_to_fp32`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L813)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '#### `accelerate.utils.convert_to_fp32`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L766)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`tensor` (nested list/tuple/dictionary of `torch.Tensor`) — The data to convert
    from FP16/BF16 to FP32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursively converts the elements nested list/tuple/dictionary of tensors in
    FP16/BF16 precision to FP32.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.gather`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L422)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`tensor` (nested list/tuple/dictionary of `torch.Tensor`) — The data to gather.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursively gather tensor in a nested list/tuple/dictionary of tensors from
    all devices.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.gather_object`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L449)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`object` (nested list/tuple/dictionary of picklable object) — The data to gather.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursively gather object in a nested list/tuple/dictionary of objects from
    all devices.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.listify`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L283)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`data` (nested list/tuple/dictionary of `torch.Tensor`) — The data from which
    to convert to regular numbers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursively finds tensors in a nested list/tuple/dictionary and converts them
    to a list of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.pad_across_processes`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L631)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`tensor` (nested list/tuple/dictionary of `torch.Tensor`) — The data to gather.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dim` (`int`, *optional*, defaults to 0) — The dimension on which to pad.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_index` (`int`, *optional*, defaults to 0) — The value with which to pad.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_first` (`bool`, *optional*, defaults to `False`) — Whether to pad at the
    beginning or the end.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursively pad the tensors in a nested list/tuple/dictionary of tensors from
    all devices to the same size so they can safely be gathered.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.recursively_apply`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L93)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`func` (`callable`) — The function to recursively apply.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data` (nested list/tuple/dictionary of `main_type`) — The data on which to
    apply `func` *args — Positional arguments that will be passed to `func` when applied
    on the unpacked data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`main_type` (`type`, *optional*, defaults to `torch.Tensor`) — The base type
    of the objects to which apply `func`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`error_on_other_type` (`bool`, *optional*, defaults to `False`) — Whether to
    return an error or not if after unpacking `data`, we get on an object that is
    not of type `main_type`. If `False`, the function will leave objects of types
    different than `main_type` unchanged. **kwargs — Keyword arguments that will be
    passed to `func` when applied on the unpacked data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursively apply a function on a data structure that is a nested list/tuple/dictionary
    of a given base type.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.reduce`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L724)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`tensor` (nested list/tuple/dictionary of `torch.Tensor`) — The data to reduce.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduction` (`str`, *optional*, defaults to `"mean"`) — A reduction method.
    Can be of “mean”, “sum”, or “none”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scale` (`float`, *optional*) — A default scaling value to be applied after
    the reduce, only valied on XLA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursively reduce the tensors in a nested list/tuple/dictionary of lists of
    tensors across all processes by the mean of a given operation.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.send_to_device`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L144)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`tensor` (nested list/tuple/dictionary of `torch.Tensor`) — The data to send
    to a given device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`torch.device`) — The device to send the data to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursively sends the elements in a nested list/tuple/dictionary of tensors
    to a given device.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.slice_tensors`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L585)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`data` (nested list/tuple/dictionary of `torch.Tensor`) — The data to slice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tensor_slice` (`slice`) — The slice to take.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursively takes a slice in a nested list/tuple/dictionary of tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Environment Checks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These functionalities check the state of the current working environment including
    information about the operating system itself, what it can support, and if particular
    dependencies are installed.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.is_bf16_available`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/imports.py#L130)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Checks if bf16 is supported, optionally ignoring the TPU
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.is_ipex_available`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/imports.py#L255)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '#### `accelerate.utils.is_mps_available`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/imports.py#L251)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '#### `accelerate.utils.is_npu_available`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Checks if `torch_npu` is installed and potentially if a NPU is in the environment
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.is_torch_version`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/versions.py#L46)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`operation` (`str`) — A string representation of an operator, such as `">"`
    or `"<="`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`version` (`str`) — A string version of PyTorch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compares the current PyTorch version to a given reference with an operation.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.is_tpu_available`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Checks if `torch_xla` is installed and potentially if a TPU is in the environment
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.is_xpu_available`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: check if user disables it explicitly
  prefs: []
  type: TYPE_NORMAL
- en: Environment Manipulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.patch_environment`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L219)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: A context manager that will add each keyword argument passed to `os.environ`
    and remove them when exiting.
  prefs: []
  type: TYPE_NORMAL
- en: Will convert the values in `kwargs` to strings and upper-case all the keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '#### `accelerate.utils.clear_environment`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L186)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: A context manager that will cache origin `os.environ` and replace it with a
    empty dictionary in this context.
  prefs: []
  type: TYPE_NORMAL
- en: When this context exits, the cached `os.environ` will be back.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '#### `accelerate.commands.config.default.write_basic_config`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/commands/config/default.py#L29)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`mixed_precision` (`str`, *optional*, defaults to “no”) — Mixed Precision to
    use. Should be one of “no”, “fp16”, or “bf16”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save_location` (`str`, *optional*, defaults to `default_json_config_file`)
    — Optional custom save location. Should be passed to `--config_file` when using
    `accelerate launch`. Default location is inside the huggingface cache folder (`~/.cache/huggingface`)
    but can be overriden by setting the `HF_HOME` environmental variable, followed
    by `accelerate/default_config.yaml`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_xpu` (`bool`, *optional*, defaults to `False`) — Whether to use XPU if
    available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates and saves a basic cluster config to be used on a local machine with
    potentially multiple GPUs. Will also set CPU if it is a CPU-only machine.
  prefs: []
  type: TYPE_NORMAL
- en: When setting up 🤗 Accelerate for the first time, rather than running `accelerate
    config` [~utils.write_basic_config] can be used as an alternative for quick configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '#### `accelerate.find_executable_batch_size`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/memory.py#L83)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`function` (`callable`, *optional*) — A function to wrap'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`starting_batch_size` (`int`, *optional*) — The batch size to try and fit into
    memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A basic decorator that will try to execute `function`. If it fails from exceptions
    related to out-of-memory or CUDNN, the batch size is cut in half and passed to
    `function`
  prefs: []
  type: TYPE_NORMAL
- en: '`function` must take in a `batch_size` parameter as its first argument.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These utilities relate to interacting with PyTorch models
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.calculate_maximum_sizes`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1004)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Computes the total size of the model and its largest layer
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.compute_module_sizes`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L686)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Compute the size of each submodule of a given model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.extract_model_from_parallel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L56)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model to extract.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_fp32_wrapper` (`bool`, *optional*) — Whether to remove mixed precision
    hooks from the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.nn.Module`'
  prefs: []
  type: TYPE_NORMAL
- en: The extracted model.
  prefs: []
  type: TYPE_NORMAL
- en: Extract a model from its distributed containers.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.get_balanced_memory`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L872)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model to analyze.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_memory` (`Dict`, *optional*) — A dictionary device identifier to maximum
    memory. Will default to the maximum memory available if unset. Example: `max_memory={0:
    "1GB"}`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`no_split_module_classes` (`List[str]`, *optional*) — A list of layer class
    names that should never be split across device (for instance any layer that has
    a residual connection).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`str` or `torch.dtype`, *optional*) — If provided, the weights will
    be converted to that type when loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`special_dtypes` (`Dict[str, Union[str, torch.device]]`, *optional*) — If provided,
    special dtypes to consider for some specific weights (will override dtype used
    as default for all weights).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`low_zero` (`bool`, *optional*) — Minimizes the number of weights on GPU 0,
    which is convenient when it’s used for other operations (like the Transformers
    generate function).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute a `max_memory` dictionary for [infer_auto_device_map()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.infer_auto_device_map)
    that will balance the use of each available GPU.
  prefs: []
  type: TYPE_NORMAL
- en: All computation is done analyzing sizes and dtypes of the model parameters.
    As a result, the model can be on the meta device (as it would if initialized within
    the `init_empty_weights` context manager).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.get_max_layer_size`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L719)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`modules` (`List[Tuple[str, torch.nn.Module]]`) — The list of named modules
    where we want to determine the maximum layer size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`module_sizes` (`Dict[str, int]`) — A dictionary mapping each layer name to
    its size (as generated by `compute_module_sizes`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`no_split_module_classes` (`List[str]`) — A list of class names for layers
    we don’t want to be split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Tuple[int, List[str]]`'
  prefs: []
  type: TYPE_NORMAL
- en: The maximum size of a layer with the list of layer names realizing that maximum
    size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Utility function that will scan a list of named modules and return the maximum
    size used by one full layer. The definition of a layer being:'
  prefs: []
  type: TYPE_NORMAL
- en: a module with no direct children (just parameters and buffers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a module whose class name is in the list `no_split_module_classes`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `accelerate.infer_auto_device_map`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1022)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model to analyze.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_memory` (`Dict`, *optional*) — A dictionary device identifier to maximum
    memory. Will default to the maximum memory available if unset. Example: `max_memory={0:
    "1GB"}`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`no_split_module_classes` (`List[str]`, *optional*) — A list of layer class
    names that should never be split across device (for instance any layer that has
    a residual connection).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`str` or `torch.dtype`, *optional*) — If provided, the weights will
    be converted to that type when loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`special_dtypes` (`Dict[str, Union[str, torch.device]]`, *optional*) — If provided,
    special dtypes to consider for some specific weights (will override dtype used
    as default for all weights).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose` (`bool`, *optional*, defaults to `False`) — Whether or not to provide
    debugging statements as the function builds the device_map.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clean_result` (`bool`, *optional*, defaults to `True`) — Clean the resulting
    device_map by grouping all submodules that go on the same device together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compute a device map for a given model giving priority to GPUs, then offload
    on CPU and finally offload to disk, such that:'
  prefs: []
  type: TYPE_NORMAL
- en: we don’t exceed the memory available of any of the GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if offload to the CPU is needed, there is always room left on GPU 0 to put back
    the layer offloaded on CPU that has the largest size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if offload to the CPU is needed,we don’t exceed the RAM available on the CPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if offload to the disk is needed, there is always room left on the CPU to put
    back the layer offloaded on disk that has the largest size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All computation is done analyzing sizes and dtypes of the model parameters.
    As a result, the model can be on the meta device (as it would if initialized within
    the `init_empty_weights` context manager).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.load_checkpoint_in_model`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1442)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model in which we want to load a checkpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`checkpoint` (`str` or `os.PathLike`) — The folder checkpoint to load. It can
    be:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a path to a file containing a whole model state dict
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a path to a `.json` file containing the index to a sharded checkpoint
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a path to a folder containing a unique `.index.json` file and the shards of
    a checkpoint.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a path to a folder containing a unique pytorch_model.bin or a model.safetensors
    file.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_map` (`Dict[str, Union[int, str, torch.device]]`, *optional*) — A map
    that specifies where each submodule should go. It doesn’t need to be refined to
    each parameter/buffer name, once a given module name is inside, every submodule
    of it will be sent to the same device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offload_folder` (`str` or `os.PathLike`, *optional*) — If the `device_map`
    contains any value `"disk"`, the folder where we will offload weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`str` or `torch.dtype`, *optional*) — If provided, the weights will
    be converted to that type when loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offload_state_dict` (`bool`, *optional*, defaults to `False`) — If `True`,
    will temporarily offload the CPU state dict on the hard drive to avoid getting
    out of CPU RAM if the weight of the CPU state dict + the biggest shard does not
    fit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offload_buffers` (`bool`, *optional*, defaults to `False`) — Whether or not
    to include the buffers in the weights offloaded to disk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_fp32_modules(List[str],` *optional*) — A list of the modules that
    we keep in `torch.float32` dtype.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offload_8bit_bnb` (`bool`, *optional*) — Whether or not to enable offload
    of 8-bit modules on cpu/disk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loads a (potentially sharded) checkpoint inside a model, potentially sending
    weights to a given device as they are loaded.
  prefs: []
  type: TYPE_NORMAL
- en: Once loaded across devices, you still need to call [dispatch_model()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.dispatch_model)
    on your model to make it able to run. To group the checkpoint loading and dispatch
    in one single call, use [load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.load_offloaded_weights`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L842)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model to load the weights into.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index` (`dict`) — A dictionary containing the parameter name and its metadata
    for each parameter that was offloaded from the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offload_folder` (`str`) — The folder where the offloaded weights are stored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loads the weights from the offload folder into the model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.load_state_dict`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1301)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`checkpoint_file` (`str`) — The path to the checkpoint to load.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_map` (`Dict[str, Union[int, str, torch.device]]`, *optional*) — A map
    that specifies where each submodule should go. It doesn’t need to be refined to
    each parameter/buffer name, once a given module name is inside, every submodule
    of it will be sent to the same device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load a checkpoint from a given file. If the checkpoint is in the safetensors
    format and a device map is passed, the weights can be fast-loaded directly on
    the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.offload_state_dict`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/offload.py#L85)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`save_dir` (`str` or `os.PathLike`) — The directory in which to offload the
    state dict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`state_dict` (`Dict[str, torch.Tensor]`) — The dictionary of tensors to offload.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offload a state dict in a given folder.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.retie_parameters`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L644)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model in which to retie parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tied_params` (`List[List[str]]`) — A mapping parameter name to tied parameter
    name as obtained by `find_tied_parameters`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reties tied parameters in a given model if the link was broken (for instance
    when adding hooks).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.set_module_tensor_to_device`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L275)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`module` (`torch.nn.Module`) — The module in which the tensor we want to move
    lives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tensor_name` (`str`) — The full name of the parameter/buffer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, `str` or `torch.device`) — The device on which to set the
    tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`value` (`torch.Tensor`, *optional*) — The value of the tensor (useful when
    going from the meta device to any other device).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`torch.dtype`, *optional*) — If passed along the value of the parameter
    will be cast to this `dtype`. Otherwise, `value` will be cast to the dtype of
    the existing parameter in the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fp16_statistics` (`torch.HalfTensor`, *optional*) — The list of fp16 statistics
    to set on the module, used for 8 bit model serialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tied_params_map` (Dict[int, Dict[torch.device, torch.Tensor]], *optional*,
    defaults to `None`) — A map of current data pointers to dictionaries of devices
    to already dispatched tied weights. For a given execution device, this parameter
    is useful to reuse the first available pointer of a shared weight on the device
    for all others, instead of duplicating memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A helper function to set a given tensor (parameter of buffer) of a module on
    a specific device (note that doing `param.to(device)` creates a new tensor not
    linked to the parameter, which is why we need this function).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.shard_checkpoint`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L193)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`state_dict` (`Dict[str, torch.Tensor]`) — The state dictionary of a model
    to save.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"10GB"`) — The maximum
    size of each sub-checkpoint. If expressed as a string, needs to be digits followed
    by a unit (like `"5MB"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weights_name` (`str`, *optional*, defaults to `"pytorch_model.bin"`) — The
    name of the model save file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splits a model state dictionary in sub-checkpoints so that the final size of
    each sub-checkpoint does not exceed a given size.
  prefs: []
  type: TYPE_NORMAL
- en: The sub-checkpoints are determined by iterating through the `state_dict` in
    the order of its keys, so there is no optimization made to make each sub-checkpoint
    as close as possible to the maximum size passed. For example, if the limit is
    10GB and we have weights of sizes [6GB, 6GB, 2GB, 6GB, 2GB, 2GB] they will get
    sharded as [6GB], [6+2GB], [6+2+2GB] and not [6+2+2GB], [6+2GB], [6GB].
  prefs: []
  type: TYPE_NORMAL
- en: If one of the model’s weight is bigger that `max_sahrd_size`, it will end up
    in its own sub-checkpoint which will have a size greater than `max_shard_size`.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These include general utilities that should be used when working in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.extract_model_from_parallel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L56)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model to extract.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_fp32_wrapper` (`bool`, *optional*) — Whether to remove mixed precision
    hooks from the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.nn.Module`'
  prefs: []
  type: TYPE_NORMAL
- en: The extracted model.
  prefs: []
  type: TYPE_NORMAL
- en: Extract a model from its distributed containers.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.save`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L156)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`save_on_each_node` (`bool`, *optional*, defaults to `False`) — Whether to
    only save on the global main process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safe_serialization` (`bool`, *optional*, defaults to `False`) — Whether to
    save `obj` using `safetensors` or the traditional PyTorch way (that uses `pickle`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the data to disk. Use in place of `torch.save()`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.wait_for_everyone`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L108)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Introduces a blocking point in the script, making sure all processes have reached
    this point before continuing.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure all processes will reach this instruction otherwise one of your processes
    will hang forever.
  prefs: []
  type: TYPE_NORMAL
- en: Random
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These utilities relate to setting and synchronizing of all the random states.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.set_seed`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/random.py#L31)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`seed` (`int`) — The seed to set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_specific` (`bool`, *optional*, defaults to `False`) — Whether to differ
    the seed on each device slightly with `self.process_index`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helper function for reproducible behavior to set the seed in `random`, `numpy`,
    `torch`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.synchronize_rng_state`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/random.py#L57)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '#### `accelerate.synchronize_rng_states`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/random.py#L109)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: PyTorch XLA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These include utilities that are useful while using PyTorch with XLA.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.install_xla`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/torch_xla.py#L20)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`upgrade` (`bool`, *optional*, defaults to `False`) — Whether to upgrade `torch`
    and install the latest `torch_xla` wheels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helper function to install appropriate xla wheels based on the `torch` version
    in Google Colaboratory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Loading model weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These include utilities that are useful to load checkpoints.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.load_checkpoint_in_model`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1442)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model in which we want to load a checkpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`checkpoint` (`str` or `os.PathLike`) — The folder checkpoint to load. It can
    be:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a path to a file containing a whole model state dict
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a path to a `.json` file containing the index to a sharded checkpoint
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a path to a folder containing a unique `.index.json` file and the shards of
    a checkpoint.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a path to a folder containing a unique pytorch_model.bin or a model.safetensors
    file.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_map` (`Dict[str, Union[int, str, torch.device]]`, *optional*) — A map
    that specifies where each submodule should go. It doesn’t need to be refined to
    each parameter/buffer name, once a given module name is inside, every submodule
    of it will be sent to the same device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offload_folder` (`str` or `os.PathLike`, *optional*) — If the `device_map`
    contains any value `"disk"`, the folder where we will offload weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`str` or `torch.dtype`, *optional*) — If provided, the weights will
    be converted to that type when loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offload_state_dict` (`bool`, *optional*, defaults to `False`) — If `True`,
    will temporarily offload the CPU state dict on the hard drive to avoid getting
    out of CPU RAM if the weight of the CPU state dict + the biggest shard does not
    fit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offload_buffers` (`bool`, *optional*, defaults to `False`) — Whether or not
    to include the buffers in the weights offloaded to disk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_fp32_modules(List[str],` *optional*) — A list of the modules that
    we keep in `torch.float32` dtype.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offload_8bit_bnb` (`bool`, *optional*) — Whether or not to enable offload
    of 8-bit modules on cpu/disk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loads a (potentially sharded) checkpoint inside a model, potentially sending
    weights to a given device as they are loaded.
  prefs: []
  type: TYPE_NORMAL
- en: Once loaded across devices, you still need to call [dispatch_model()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.dispatch_model)
    on your model to make it able to run. To group the checkpoint loading and dispatch
    in one single call, use [load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch).
  prefs: []
  type: TYPE_NORMAL
- en: Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These include utilities that are useful to quantize model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.utils.load_and_quantize_model`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/bnb.py#L44)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — Input model. The model can be already loaded
    or on the meta device'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bnb_quantization_config` (`BnbQuantizationConfig`) — The bitsandbytes quantization
    parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weights_location` (`str` or `os.PathLike`) — The folder weights_location to
    load. It can be:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a path to a file containing a whole model state dict
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a path to a `.json` file containing the index to a sharded checkpoint
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a path to a folder containing a unique `.index.json` file and the shards of
    a checkpoint.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a path to a folder containing a unique pytorch_model.bin file.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_map` (`Dict[str, Union[int, str, torch.device]]`, *optional*) — A map
    that specifies where each submodule should go. It doesn’t need to be refined to
    each parameter/buffer name, once a given module name is inside, every submodule
    of it will be sent to the same device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`no_split_module_classes` (`List[str]`, *optional*) — A list of layer class
    names that should never be split across device (for instance any layer that has
    a residual connection).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_memory` (`Dict`, *optional*) — A dictionary device identifier to maximum
    memory. Will default to the maximum memory available if unset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offload_folder` (`str` or `os.PathLike`, *optional*) — If the `device_map`
    contains any value `"disk"`, the folder where we will offload weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offload_state_dict` (`bool`, *optional*, defaults to `False`) — If `True`,
    will temporarily offload the CPU state dict on the hard drive to avoid getting
    out of CPU RAM if the weight of the CPU state dict + the biggest shard does not
    fit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.nn.Module`'
  prefs: []
  type: TYPE_NORMAL
- en: The quantized model
  prefs: []
  type: TYPE_NORMAL
- en: This function will quantize the input model with the associated config passed
    in `bnb_quantization_config`. If the model is in the meta device, we will load
    and dispatch the weights according to the `device_map` passed. If the model is
    already loaded, we will quantize the model and put the model on the GPU,
  prefs: []
  type: TYPE_NORMAL
