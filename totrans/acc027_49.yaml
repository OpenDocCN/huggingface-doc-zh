- en: Helpful Utilities
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ‰ç”¨çš„å·¥å…·
- en: 'Original text: [https://huggingface.co/docs/accelerate/package_reference/utilities](https://huggingface.co/docs/accelerate/package_reference/utilities)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://huggingface.co/docs/accelerate/package_reference/utilities](https://huggingface.co/docs/accelerate/package_reference/utilities)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Below are a variety of utility functions that ğŸ¤— Accelerate provides, broken
    down by use-case.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ğŸ¤— Accelerateæä¾›çš„å„ç§å®ç”¨å‡½æ•°ï¼ŒæŒ‰ç”¨ä¾‹åˆ†ç±»ã€‚
- en: Constants
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¸¸é‡
- en: Constants used throughout ğŸ¤— Accelerate for reference
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºå‚è€ƒçš„ğŸ¤— Accelerateä¸­ä½¿ç”¨çš„å¸¸é‡
- en: The following are constants used when utilizing [Accelerator.save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯åœ¨ä½¿ç”¨[Accelerator.save_state()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_state)æ—¶ä½¿ç”¨çš„å¸¸é‡
- en: '`utils.MODEL_NAME`: `"pytorch_model"` `utils.OPTIMIZER_NAME`: `"optimizer"`
    `utils.RNG_STATE_NAME`: `"random_states"` `utils.SCALER_NAME`: `"scaler.pt` `utils.SCHEDULER_NAME`:
    `"scheduler`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '`utils.MODEL_NAME`: `"pytorch_model"` `utils.OPTIMIZER_NAME`: `"optimizer"`
    `utils.RNG_STATE_NAME`: `"random_states"` `utils.SCALER_NAME`: `"scaler.pt` `utils.SCHEDULER_NAME`:
    `"scheduler`'
- en: The following are constants used when utilizing [Accelerator.save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯åœ¨ä½¿ç”¨[Accelerator.save_model()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.save_model)æ—¶ä½¿ç”¨çš„å¸¸é‡
- en: '`utils.WEIGHTS_NAME`: `"pytorch_model.bin"` `utils.SAFE_WEIGHTS_NAME`: `"model.safetensors"`
    `utils.WEIGHTS_INDEX_NAME`: `"pytorch_model.bin.index.json"` `utils.SAFE_WEIGHTS_INDEX_NAME`:
    `"model.safetensors.index.json"`'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`utils.WEIGHTS_NAME`: `"pytorch_model.bin"` `utils.SAFE_WEIGHTS_NAME`: `"model.safetensors"`
    `utils.WEIGHTS_INDEX_NAME`: `"pytorch_model.bin.index.json"` `utils.SAFE_WEIGHTS_INDEX_NAME`:
    `"model.safetensors.index.json"`'
- en: Data Classes
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®ç±»
- en: These are basic dataclasses used throughout ğŸ¤— Accelerate and they can be passed
    in as parameters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯åœ¨ğŸ¤— Accelerateä¸­ä½¿ç”¨çš„åŸºæœ¬æ•°æ®ç±»ï¼Œå¯ä»¥ä½œä¸ºå‚æ•°ä¼ é€’ã€‚
- en: Standalone
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç‹¬ç«‹çš„
- en: These are standalone dataclasses used for checks, such as the type of distributed
    system being used
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯ç”¨äºæ£€æŸ¥çš„ç‹¬ç«‹æ•°æ®ç±»ï¼Œä¾‹å¦‚æ­£åœ¨ä½¿ç”¨çš„åˆ†å¸ƒå¼ç³»ç»Ÿç±»å‹
- en: '### `class accelerate.utils.ComputeEnvironment`'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.utils.ComputeEnvironment`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L329)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L329)'
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Represents a type of the compute environment.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£è¡¨è®¡ç®—ç¯å¢ƒçš„ä¸€ç§ç±»å‹ã€‚
- en: 'Values:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼ï¼š
- en: '`LOCAL_MACHINE` â€” private/custom cluster hardware.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LOCAL_MACHINE` â€” ç§æœ‰/è‡ªå®šä¹‰é›†ç¾¤ç¡¬ä»¶ã€‚'
- en: '`AMAZON_SAGEMAKER` â€” Amazon SageMaker as compute environment.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AMAZON_SAGEMAKER` â€” Amazon SageMakerä½œä¸ºè®¡ç®—ç¯å¢ƒã€‚'
- en: '### `class accelerate.DistributedType`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.DistributedType`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L285)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L285)'
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Represents a type of distributed environment.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£è¡¨ä¸€ç§åˆ†å¸ƒå¼ç¯å¢ƒç±»å‹ã€‚
- en: 'Values:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼ï¼š
- en: '`NO` â€” Not a distributed environment, just a single process.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NO` â€” ä¸æ˜¯åˆ†å¸ƒå¼ç¯å¢ƒï¼Œåªæ˜¯å•ä¸ªè¿›ç¨‹ã€‚'
- en: '`MULTI_CPU` â€” Distributed on multiple CPU nodes.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MULTI_CPU` â€” åœ¨å¤šä¸ªCPUèŠ‚ç‚¹ä¸Šåˆ†å¸ƒã€‚'
- en: '`MULTI_GPU` â€” Distributed on multiple GPUs.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MULTI_GPU` â€” åœ¨å¤šä¸ªGPUä¸Šåˆ†å¸ƒã€‚'
- en: '`MULTI_NPU` â€” Distributed on multiple NPUs.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MULTI_NPU` â€” åœ¨å¤šä¸ªNPUä¸Šåˆ†å¸ƒã€‚'
- en: '`MULTI_XPU` â€” Distributed on multiple XPUs.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MULTI_XPU` â€” åœ¨å¤šä¸ªXPUä¸Šåˆ†å¸ƒã€‚'
- en: '`DEEPSPEED` â€” Using DeepSpeed.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DEEPSPEED` â€” ä½¿ç”¨DeepSpeedã€‚'
- en: '`TPU` â€” Distributed on TPUs.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TPU` â€” åœ¨TPUä¸Šåˆ†å¸ƒã€‚'
- en: '### `class accelerate.utils.DynamoBackend`'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.utils.DynamoBackend`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L344)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L344)'
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Represents a dynamo backend (see [https://github.com/pytorch/torchdynamo](https://github.com/pytorch/torchdynamo)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£è¡¨ä¸€ä¸ªdynamoåç«¯ï¼ˆå‚è§[https://github.com/pytorch/torchdynamo](https://github.com/pytorch/torchdynamo)ï¼‰ã€‚
- en: 'Values:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼ï¼š
- en: '`NO` â€” Do not use torch dynamo.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NO` â€” ä¸ä½¿ç”¨torch dynamoã€‚'
- en: '`EAGER` â€” Uses PyTorch to run the extracted GraphModule. This is quite useful
    in debugging TorchDynamo issues.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EAGER` â€” ä½¿ç”¨PyTorchæ¥è¿è¡Œæå–çš„GraphModuleã€‚åœ¨è°ƒè¯•TorchDynamoé—®é¢˜æ—¶éå¸¸æœ‰ç”¨ã€‚'
- en: '`AOT_EAGER` â€” Uses AotAutograd with no compiler, i.e, just using PyTorch eager
    for the AotAutogradâ€™s extracted forward and backward graphs. This is useful for
    debugging, and unlikely to give speedups.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AOT_EAGER` â€” ä½¿ç”¨AotAutogradè€Œä¸ä½¿ç”¨ç¼–è¯‘å™¨ï¼Œå³ä»…ä½¿ç”¨PyTorch eagerè¿›è¡ŒAotAutogradçš„æå–çš„å‰å‘å’Œåå‘å›¾ã€‚è¿™å¯¹è°ƒè¯•å¾ˆæœ‰ç”¨ï¼Œä¸å¤ªå¯èƒ½æä¾›åŠ é€Ÿã€‚'
- en: '`INDUCTOR` â€” Uses TorchInductor backend with AotAutograd and cudagraphs by
    leveraging codegened Triton kernels. [Read more](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`INDUCTOR` â€” ä½¿ç”¨TorchInductoråç«¯ä¸AotAutogradå’Œcudagraphsï¼Œé€šè¿‡åˆ©ç”¨codegened Tritonå†…æ ¸ã€‚[é˜…è¯»æ›´å¤š](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)'
- en: '`AOT_TS_NVFUSER` â€” nvFuser with AotAutograd/TorchScript. [Read more](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AOT_TS_NVFUSER` â€” ä½¿ç”¨AotAutograd/TorchScriptçš„nvFuserã€‚[é˜…è¯»æ›´å¤š](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)'
- en: '`NVPRIMS_NVFUSER` â€” nvFuser with PrimTorch. [Read more](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NVPRIMS_NVFUSER` â€” ä½¿ç”¨PrimTorchçš„nvFuserã€‚[é˜…è¯»æ›´å¤š](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)'
- en: '`CUDAGRAPHS` â€” cudagraphs with AotAutograd. [Read more](https://github.com/pytorch/torchdynamo/pull/757)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CUDAGRAPHS` â€” ä½¿ç”¨AotAutogradçš„cudagraphsã€‚[é˜…è¯»æ›´å¤š](https://github.com/pytorch/torchdynamo/pull/757)'
- en: '`OFI` â€” Uses Torchscript optimize_for_inference. Inference only. [Read more](https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OFI` â€” ä½¿ç”¨Torchscript optimize_for_inferenceã€‚ä»…æ¨ç†ã€‚[é˜…è¯»æ›´å¤š](https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html)'
- en: '`FX2TRT` â€” Uses Nvidia TensorRT for inference optimizations. Inference only.
    [Read more](https://github.com/pytorch/TensorRT/blob/master/docsrc/tutorials/getting_started_with_fx_path.rst)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FX2TRT` â€” ä½¿ç”¨Nvidia TensorRTè¿›è¡Œæ¨ç†ä¼˜åŒ–ã€‚ä»…æ¨ç†ã€‚[é˜…è¯»æ›´å¤š](https://github.com/pytorch/TensorRT/blob/master/docsrc/tutorials/getting_started_with_fx_path.rst)'
- en: '`ONNXRT` â€” Uses ONNXRT for inference on CPU/GPU. Inference only. [Read more](https://onnxruntime.ai/)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ONNXRT` â€” ä½¿ç”¨ONNXRTåœ¨CPU/GPUä¸Šè¿›è¡Œæ¨ç†ã€‚ä»…æ¨ç†ã€‚[é˜…è¯»æ›´å¤š](https://onnxruntime.ai/)'
- en: '`TENSORRT` â€” Uses ONNXRT to run TensorRT for inference optimizations. [Read
    more](https://github.com/onnx/onnx-tensorrt)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TENSORRT` â€” ä½¿ç”¨ONNXRTæ¥è¿è¡ŒTensorRTè¿›è¡Œæ¨ç†ä¼˜åŒ–ã€‚[é˜…è¯»æ›´å¤š](https://github.com/onnx/onnx-tensorrt)'
- en: '`IPEX` â€” Uses IPEX for inference on CPU. Inference only. [Read more](https://github.com/intel/intel-extension-for-pytorch).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IPEX` â€” åœ¨CPUä¸Šä½¿ç”¨IPEXè¿›è¡Œæ¨æ–­ã€‚ä»…æ¨æ–­ã€‚[é˜…è¯»æ›´å¤š](https://github.com/intel/intel-extension-for-pytorch)ã€‚'
- en: '`TVM` â€” Uses Apach TVM for inference optimizations. [Read more](https://tvm.apache.org/)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TVM` â€” ä½¿ç”¨Apach TVMè¿›è¡Œæ¨æ–­ä¼˜åŒ–ã€‚[é˜…è¯»æ›´å¤š](https://tvm.apache.org/)'
- en: '### `class accelerate.utils.LoggerType`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.utils.LoggerType`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L392)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L392)'
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Represents a type of supported experiment tracker
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºæ”¯æŒçš„å®éªŒè·Ÿè¸ªå™¨ç±»å‹
- en: 'Values:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼ï¼š
- en: '`ALL` â€” all available trackers in the environment that are supported'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ALL` â€” ç¯å¢ƒä¸­æ‰€æœ‰å¯ç”¨çš„å—æ”¯æŒè·Ÿè¸ªå™¨'
- en: '`TENSORBOARD` â€” TensorBoard as an experiment tracker'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TENSORBOARD` â€” TensorBoardä½œä¸ºå®éªŒè·Ÿè¸ªå™¨'
- en: '`WANDB` â€” wandb as an experiment tracker'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WANDB` â€” wandbä½œä¸ºå®éªŒè·Ÿè¸ªå™¨'
- en: '`COMETML` â€” comet_ml as an experiment tracker'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`COMETML` â€” comet_mlä½œä¸ºå®éªŒè·Ÿè¸ªå™¨'
- en: '`DVCLIVE` â€” dvclive as an experiment tracker'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DVCLIVE` â€” dvcliveä½œä¸ºå®éªŒè·Ÿè¸ªå™¨'
- en: '### `class accelerate.utils.PrecisionType`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.utils.PrecisionType`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L414)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L414)'
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Represents a type of precision used on floating point values
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºæµ®ç‚¹å€¼ä¸Šä½¿ç”¨çš„ç²¾åº¦ç±»å‹
- en: 'Values:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼ï¼š
- en: '`NO` â€” using full precision (FP32)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NO` â€” ä½¿ç”¨å…¨ç²¾åº¦ï¼ˆFP32ï¼‰'
- en: '`FP16` â€” using half precision'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FP16` â€” ä½¿ç”¨åŠç²¾åº¦'
- en: '`BF16` â€” using brain floating point precision'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BF16` â€” ä½¿ç”¨è„‘æµ®ç‚¹ç²¾åº¦'
- en: '### `class accelerate.utils.RNGType`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.utils.RNGType`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L430)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L430)'
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: An enumeration.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæšä¸¾ã€‚
- en: '### `class accelerate.utils.SageMakerDistributedType`'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.utils.SageMakerDistributedType`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L312)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L312)'
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Represents a type of distributed environment.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºåˆ†å¸ƒå¼ç¯å¢ƒç±»å‹ã€‚
- en: 'Values:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼ï¼š
- en: '`NO` â€” Not a distributed environment, just a single process.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NO` â€” ä¸æ˜¯åˆ†å¸ƒå¼ç¯å¢ƒï¼Œåªæ˜¯ä¸€ä¸ªå•ä¸€è¿›ç¨‹ã€‚'
- en: '`DATA_PARALLEL` â€” using sagemaker distributed data parallelism.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DATA_PARALLEL` â€” ä½¿ç”¨sagemakeråˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œã€‚'
- en: '`MODEL_PARALLEL` â€” using sagemaker distributed model parallelism.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MODEL_PARALLEL` â€” ä½¿ç”¨sagemakeråˆ†å¸ƒå¼æ¨¡å‹å¹¶è¡Œã€‚'
- en: Kwargs
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kwargs
- en: These are configurable arguments for specific interactions throughout the PyTorch
    ecosystem that Accelerate handles under the hood.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯åŠ é€Ÿåœ¨å¹•åå¤„ç†çš„PyTorchç”Ÿæ€ç³»ç»Ÿä¸­ç‰¹å®šäº¤äº’çš„å¯é…ç½®å‚æ•°ã€‚
- en: '### `class accelerate.AutocastKwargs`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.AutocastKwargs`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L60)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L60)'
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize how `torch.autocast` behaves. Please refer to the documentation of
    this [context manager](https://pytorch.org/docs/stable/amp.html#torch.autocast)
    for more information on each argument.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‚¨çš„[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)ä¸­ä½¿ç”¨æ­¤å¯¹è±¡ï¼Œä»¥è‡ªå®šä¹‰`torch.autocast`çš„è¡Œä¸ºã€‚è¯·å‚è€ƒæ­¤[ä¸Šä¸‹æ–‡ç®¡ç†å™¨](https://pytorch.org/docs/stable/amp.html#torch.autocast)çš„æ–‡æ¡£ï¼Œä»¥è·å–æœ‰å…³æ¯ä¸ªå‚æ•°çš„æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '### `class accelerate.DistributedDataParallelKwargs`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.DistributedDataParallelKwargs`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L82)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L82)'
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize how your model is wrapped in a `torch.nn.parallel.DistributedDataParallel`.
    Please refer to the documentation of this [wrapper](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
    for more information on each argument.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‚¨çš„[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)ä¸­ä½¿ç”¨æ­¤å¯¹è±¡ï¼Œä»¥è‡ªå®šä¹‰å¦‚ä½•å°†æ‚¨çš„æ¨¡å‹åŒ…è£…åœ¨`torch.nn.parallel.DistributedDataParallel`ä¸­ã€‚è¯·å‚è€ƒæ­¤[åŒ…è£…å™¨](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)çš„æ–‡æ¡£ï¼Œä»¥è·å–æœ‰å…³æ¯ä¸ªå‚æ•°çš„æ›´å¤šä¿¡æ¯ã€‚
- en: '`gradient_as_bucket_view` is only available in PyTorch 1.7.0 and later versions.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`gradient_as_bucket_view` ä»…åœ¨PyTorch 1.7.0åŠæ›´é«˜ç‰ˆæœ¬ä¸­å¯ç”¨ã€‚'
- en: '`static_graph` is only available in PyTorch 1.11.0 and later versions.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`static_graph` ä»…åœ¨PyTorch 1.11.0åŠæ›´é«˜ç‰ˆæœ¬ä¸­å¯ç”¨ã€‚'
- en: 'Example:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '### `class accelerate.utils.FP8RecipeKwargs`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.utils.FP8RecipeKwargs`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L179)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L179)'
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`backend` (`str`, *optional*, defaults to â€œmsampâ€) â€” Which FP8 engine to use.
    Must be one of `"msamp"` (MS-AMP) or `"te"` (TransformerEngine).'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backend` (`str`, *å¯é€‰*, é»˜è®¤ä¸ºâ€œmsampâ€) â€” ä½¿ç”¨çš„FP8å¼•æ“ã€‚å¿…é¡»æ˜¯`"msamp"`ï¼ˆMS-AMPï¼‰æˆ–`"te"`ï¼ˆTransformerEngineï¼‰ä¹‹ä¸€ã€‚'
- en: '`margin` (`int`, *optional*, default to 0) â€” The margin to use for the gradient
    scaling.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`margin` (`int`, *å¯é€‰*, é»˜è®¤ä¸º0) â€” ç”¨äºæ¢¯åº¦ç¼©æ”¾çš„è¾¹è·ã€‚'
- en: '`interval` (`int`, *optional*, default to 1) â€” The interval to use for how
    often the scaling factor is recomputed.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`interval` (`int`, *å¯é€‰*, é»˜è®¤ä¸º1) â€” ç”¨äºé‡æ–°è®¡ç®—ç¼©æ”¾å› å­çš„é—´éš”ã€‚'
- en: '`fp8_format` (`str`, *optional*, default to â€œE4M3â€) â€” The format to use for
    the FP8 recipe. Must be one of `E4M3` or `HYBRID`.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp8_format` (`str`, *å¯é€‰*, é»˜è®¤ä¸ºâ€œE4M3â€) â€” ç”¨äºFP8é…æ–¹çš„æ ¼å¼ã€‚å¿…é¡»æ˜¯`E4M3`æˆ–`HYBRID`ä¹‹ä¸€ã€‚'
- en: '`amax_history_len` (`int`, *optional*, default to 1024) â€” The length of the
    history to use for the scaling factor computation'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`amax_history_len` (`int`, *å¯é€‰*, é»˜è®¤ä¸º1024) â€” ç”¨äºç¼©æ”¾å› å­è®¡ç®—çš„å†å²é•¿åº¦'
- en: '`amax_compute_algo` (`str`, *optional*, default to â€œmost_recentâ€) â€” The algorithm
    to use for the scaling factor computation. Must be one of `max` or `most_recent`.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`amax_compute_algo` (`str`, *å¯é€‰*, é»˜è®¤ä¸ºâ€œmost_recentâ€) â€” ç”¨äºç¼©æ”¾å› å­è®¡ç®—çš„ç®—æ³•ã€‚å¿…é¡»æ˜¯`max`æˆ–`most_recent`ä¹‹ä¸€ã€‚'
- en: '`override_linear_precision` (`tuple` of three `bool`, *optional*, default to
    `(False, False, False)`) â€” Whether or not to execute `fprop`, `dgrad`, and `wgrad`
    GEMMS in higher precision.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`override_linear_precision`ï¼ˆä¸‰ä¸ª`bool`çš„`tuple`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`(False, False, False)`ï¼‰
    â€” æ˜¯å¦åœ¨æ›´é«˜ç²¾åº¦ä¸­æ‰§è¡Œ`fprop`ã€`dgrad`å’Œ`wgrad` GEMMSã€‚'
- en: '`optimization_level` (`str`), one of `O1`, `O2`. (default is `O2`) â€” What level
    of 8-bit collective communication should be used with MS-AMP. In general:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimization_level`ï¼ˆ`str`ï¼‰ï¼Œå…¶ä¸­ä¹‹ä¸€ä¸º`O1`ã€`O2`ã€‚ ï¼ˆé»˜è®¤ä¸º`O2`ï¼‰ â€” ä½¿ç”¨MS-AMPæ—¶åº”ä½¿ç”¨çš„8ä½é›†ä½“é€šä¿¡çº§åˆ«ã€‚ä¸€èˆ¬æ¥è¯´ï¼š'
- en: 'O1: Weight gradients and `all_reduce` communications are done in fp8, reducing
    GPU memory usage and communication bandwidth'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'O1: æƒé‡æ¢¯åº¦å’Œ`all_reduce`é€šä¿¡åœ¨fp8ä¸­å®Œæˆï¼Œå‡å°‘äº†GPUå†…å­˜ä½¿ç”¨å’Œé€šä¿¡å¸¦å®½'
- en: 'O2: First-order optimizer states are in 8-bit, and second order states are
    in FP16. Only available when using Adam or AdamW. This maintains accuracy and
    can potentially save the highest memory.'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'O2: ä¸€é˜¶ä¼˜åŒ–å™¨çŠ¶æ€ä¸º8ä½ï¼ŒäºŒé˜¶çŠ¶æ€ä¸ºFP16ã€‚ ä»…åœ¨ä½¿ç”¨Adamæˆ–AdamWæ—¶å¯ç”¨ã€‚è¿™å¯ä»¥ä¿æŒå‡†ç¡®æ€§ï¼Œå¹¶å¯èƒ½èŠ‚çœæœ€é«˜çš„å†…å­˜ã€‚'
- en: '03: Specifically for DeepSpeed, implements capabilities so weights and master
    weights of models are stored in FP8\. If `fp8` is selected and deepspeed is enabled,
    will be used by default. (Not available currently).'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '03: ä¸“é—¨ä¸ºDeepSpeedå®ç°åŠŸèƒ½ï¼Œä½¿æ¨¡å‹çš„æƒé‡å’Œä¸»æƒé‡å­˜å‚¨åœ¨FP8ä¸­ã€‚å¦‚æœé€‰æ‹©äº†`fp8`å¹¶å¯ç”¨äº†deepspeedï¼Œåˆ™å°†é»˜è®¤ä½¿ç”¨ï¼ˆç›®å‰ä¸å¯ç”¨ï¼‰ã€‚'
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize the initialization of the recipe for FP8 mixed precision training
    with `transformer-engine` or `ms-amp`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‚¨çš„[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)ä¸­ä½¿ç”¨æ­¤å¯¹è±¡ï¼Œä»¥è‡ªå®šä¹‰ä½¿ç”¨`transformer-engine`æˆ–`ms-amp`è¿›è¡ŒFP8æ··åˆç²¾åº¦è®­ç»ƒçš„é…æ–¹çš„åˆå§‹åŒ–ã€‚
- en: For more information on `transformer-engine` args, please refer to the API [documentation](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³`transformer-engine`å‚æ•°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒAPI[æ–‡æ¡£](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html)ã€‚
- en: For more information on the `ms-amp` args, please refer to the Optimization
    Level [documentation](https://azure.github.io/MS-AMP/docs/user-tutorial/optimization-level).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³`ms-amp`å‚æ•°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒä¼˜åŒ–çº§åˆ«[æ–‡æ¡£](https://azure.github.io/MS-AMP/docs/user-tutorial/optimization-level)ã€‚
- en: '[PRE12]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To use MS-AMP as an engine, pass `backend="msamp"` and the `optimization_level`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å°†MS-AMPä½œä¸ºå¼•æ“ä½¿ç”¨ï¼Œè¯·ä¼ é€’`backend="msamp"`å’Œ`optimization_level`ï¼š
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '### `class accelerate.GradScalerKwargs`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.GradScalerKwargs`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L118)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L118)'
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize the behavior of mixed precision, specifically how the `torch.cuda.amp.GradScaler`
    used is created. Please refer to the documentation of this [scaler](https://pytorch.org/docs/stable/amp.html?highlight=gradscaler)
    for more information on each argument.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‚¨çš„[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)ä¸­ä½¿ç”¨æ­¤å¯¹è±¡ï¼Œä»¥è‡ªå®šä¹‰æ··åˆç²¾åº¦çš„è¡Œä¸ºï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•åˆ›å»ºæ‰€ä½¿ç”¨çš„`torch.cuda.amp.GradScaler`ã€‚è¯·å‚è€ƒæ­¤[scaler](https://pytorch.org/docs/stable/amp.html?highlight=gradscaler)çš„æ–‡æ¡£ï¼Œä»¥è·å–æœ‰å…³æ¯ä¸ªå‚æ•°çš„æ›´å¤šä¿¡æ¯ã€‚
- en: '`GradScaler` is only available in PyTorch 1.5.0 and later versions.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`GradScaler`ä»…åœ¨PyTorch 1.5.0åŠæ›´é«˜ç‰ˆæœ¬ä¸­å¯ç”¨ã€‚'
- en: 'Example:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '### `class accelerate.InitProcessGroupKwargs`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.InitProcessGroupKwargs`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L149)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L149)'
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Use this object in your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to customize the initialization of the distributed processes. Please refer to
    the documentation of this [method](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)
    for more information on each argument.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‚¨çš„[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)ä¸­ä½¿ç”¨æ­¤å¯¹è±¡ï¼Œä»¥è‡ªå®šä¹‰åˆ†å¸ƒå¼è¿›ç¨‹çš„åˆå§‹åŒ–ã€‚è¯·å‚è€ƒæ­¤[method](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)çš„æ–‡æ¡£ï¼Œä»¥è·å–æœ‰å…³æ¯ä¸ªå‚æ•°çš„æ›´å¤šä¿¡æ¯ã€‚
- en: '[PRE17]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Plugins
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ’ä»¶
- en: 'These are plugins that can be passed to the [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    object. While they are defined elsewhere in the documentation, for convenience
    all of them are available to see here:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯å¯ä»¥ä¼ é€’ç»™[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)å¯¹è±¡çš„æ’ä»¶ã€‚è™½ç„¶å®ƒä»¬åœ¨æ–‡æ¡£çš„å…¶ä»–åœ°æ–¹å®šä¹‰ï¼Œä½†ä¸ºäº†æ–¹ä¾¿ï¼Œæ‰€æœ‰æ’ä»¶éƒ½å¯ä»¥åœ¨æ­¤å¤„æŸ¥çœ‹ï¼š
- en: '### `class accelerate.DeepSpeedPlugin`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.DeepSpeedPlugin`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L562)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L562)'
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This plugin is used to integrate DeepSpeed.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ’ä»¶ç”¨äºé›†æˆDeepSpeedã€‚
- en: '#### `deepspeed_config_process`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `deepspeed_config_process`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L756)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L756)'
- en: '[PRE19]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Process the DeepSpeed config with the values from the kwargs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨kwargsä¸­çš„å€¼å¤„ç†DeepSpeedé…ç½®ã€‚
- en: '### `class accelerate.FullyShardedDataParallelPlugin`'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.FullyShardedDataParallelPlugin`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L872)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L872)'
- en: '[PRE20]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This plugin is used to enable fully sharded data parallelism.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ’ä»¶ç”¨äºå¯ç”¨å®Œå…¨åˆ†ç‰‡çš„æ•°æ®å¹¶è¡Œã€‚
- en: '#### `get_module_class_from_name`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_module_class_from_name`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L1025)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L1025)'
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`module` (`torch.nn.Module`) â€” The module to get the class from.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`module`ï¼ˆ`torch.nn.Module`ï¼‰ â€” è¦ä»ä¸­è·å–ç±»çš„æ¨¡å—ã€‚'
- en: '`name` (`str`) â€” The name of the class.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`ï¼ˆ`str`ï¼‰ â€” ç±»çš„åç§°ã€‚'
- en: Gets a class from a module by its name.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åç§°ä»æ¨¡å—ä¸­è·å–ä¸€ä¸ªç±»ã€‚
- en: '### `class accelerate.utils.GradientAccumulationPlugin`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.utils.GradientAccumulationPlugin`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L505)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L505)'
- en: '[PRE22]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: A plugin to configure gradient accumulation behavior.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ’ä»¶ï¼Œç”¨äºé…ç½®æ¢¯åº¦ç´¯ç§¯è¡Œä¸ºã€‚
- en: '### `class accelerate.utils.MegatronLMPlugin`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.utils.MegatronLMPlugin`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L1105)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L1105)'
- en: '[PRE23]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Plugin for Megatron-LM to enable tensor, pipeline, sequence and data parallelism.
    Also to enable selective activation recomputation and optimized fused kernels.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºMegatron-LMçš„æ’ä»¶ï¼Œä»¥å¯ç”¨å¼ é‡ã€ç®¡é“ã€åºåˆ—å’Œæ•°æ®å¹¶è¡Œã€‚è¿˜å¯å¯ç”¨é€‰æ‹©æ€§æ¿€æ´»é‡è®¡ç®—å’Œä¼˜åŒ–èåˆå†…æ ¸ã€‚
- en: '### `class accelerate.utils.TorchDynamoPlugin`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.utils.TorchDynamoPlugin`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L526)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L526)'
- en: '[PRE24]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This plugin is used to compile a model with PyTorch 2.0
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ’ä»¶ç”¨äºä½¿ç”¨PyTorch 2.0ç¼–è¯‘æ¨¡å‹
- en: Configurations
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é…ç½®
- en: These are classes which can be configured and passed through to the appropriate
    integration
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯å¯ä»¥é…ç½®å¹¶ä¼ é€’ç»™é€‚å½“é›†æˆçš„ç±»
- en: '### `class accelerate.utils.BnbQuantizationConfig`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.utils.BnbQuantizationConfig`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L1480)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L1480)'
- en: '[PRE25]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: A plugin to enable BitsAndBytes 4bit and 8bit quantization
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ’ä»¶ï¼Œç”¨äºå¯ç”¨BitsAndBytes 4ä½å’Œ8ä½é‡åŒ–
- en: '### `class accelerate.utils.ProjectConfiguration`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class accelerate.utils.ProjectConfiguration`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L457)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L457)'
- en: '[PRE26]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Configuration for the Accelerator object based on inner-project needs.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå†…éƒ¨é¡¹ç›®éœ€æ±‚çš„åŠ é€Ÿå™¨å¯¹è±¡çš„é…ç½®ã€‚
- en: '#### `set_directories`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_directories`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L495)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L495)'
- en: '[PRE27]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Sets `self.project_dir` and `self.logging_dir` to the appropriate values.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: å°†`self.project_dir`å’Œ`self.logging_dir`è®¾ç½®ä¸ºé€‚å½“çš„å€¼ã€‚
- en: Environmental Variables
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¯å¢ƒå˜é‡
- en: These are environmental variables that can be enabled for different use cases
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯å¯ä»¥ä¸ºä¸åŒç”¨ä¾‹å¯ç”¨çš„ç¯å¢ƒå˜é‡
- en: '`ACCELERATE_DEBUG_MODE` (`str`): Whether to run accelerate in debug mode. More
    info available [here](../usage_guides/debug.md).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ACCELERATE_DEBUG_MODE`ï¼ˆ`str`ï¼‰ï¼šæ˜¯å¦åœ¨è°ƒè¯•æ¨¡å¼ä¸‹è¿è¡ŒåŠ é€Ÿã€‚æ›´å¤šä¿¡æ¯è¯·å‚é˜…[æ­¤å¤„](../usage_guides/debug.md)ã€‚'
- en: Data Manipulation and Operations
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®æ“ä½œå’Œæ“ä½œ
- en: These include data operations that mimic the same `torch` ops but can be used
    on distributed processes.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åŒ…æ‹¬æ¨¡æ‹Ÿç›¸åŒ`torch`æ“ä½œçš„æ•°æ®æ“ä½œï¼Œä½†å¯ä»¥åœ¨åˆ†å¸ƒå¼è¿›ç¨‹ä¸Šä½¿ç”¨ã€‚
- en: '#### `accelerate.utils.broadcast`'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.broadcast`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L542)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L542)'
- en: '[PRE28]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tensor` (nested list/tuple/dictionary of `torch.Tensor`) â€” The data to gather.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor`ï¼ˆåµŒå¥—åˆ—è¡¨/å…ƒç»„/å¼ é‡å­—å…¸çš„`torch.Tensor`ï¼‰-è¦æ”¶é›†çš„æ•°æ®ã€‚'
- en: '`from_process` (`int`, *optional*, defaults to 0) â€” The process from which
    to send the data'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`from_process`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰-è¦å‘é€æ•°æ®çš„è¿›ç¨‹'
- en: Recursively broadcast tensor in a nested list/tuple/dictionary of tensors to
    all devices.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: é€’å½’å¹¿æ’­åµŒå¥—åˆ—è¡¨/å…ƒç»„/å¼ é‡å­—å…¸ä¸­çš„å¼ é‡åˆ°æ‰€æœ‰è®¾å¤‡ã€‚
- en: '#### `accelerate.utils.broadcast_object_list`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.broadcast_object_list`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L564)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L564)'
- en: '[PRE29]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`object_list` (list of picklable objects) â€” The list of objects to broadcast.
    This list will be modified inplace.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`object_list`ï¼ˆå¯æ‹¾å–å¯¹è±¡åˆ—è¡¨ï¼‰-è¦å¹¿æ’­çš„å¯¹è±¡åˆ—è¡¨ã€‚æ­¤åˆ—è¡¨å°†è¢«å°±åœ°ä¿®æ”¹ã€‚'
- en: '`from_process` (`int`, *optional*, defaults to 0) â€” The process from which
    to send the data.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`from_process`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰-è¦å‘é€æ•°æ®çš„è¿›ç¨‹ã€‚'
- en: Broadcast a list of picklable objects form one process to the others.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸€ä¸ªè¿›ç¨‹å‘å…¶ä»–è¿›ç¨‹å¹¿æ’­ä¸€ä¸ªå¯æ‹¾å–å¯¹è±¡åˆ—è¡¨ã€‚
- en: '#### `accelerate.utils.concatenate`'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.concatenate`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L605)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L605)'
- en: '[PRE30]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`data` (nested list/tuple/dictionary of lists of tensors `torch.Tensor`) â€”
    The data to concatenate.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data`ï¼ˆåµŒå¥—åˆ—è¡¨/å…ƒç»„/å¼ é‡å­—å…¸çš„å¼ é‡åˆ—è¡¨`torch.Tensor`ï¼‰-è¦è¿æ¥çš„æ•°æ®ã€‚'
- en: '`dim` (`int`, *optional*, defaults to 0) â€” The dimension on which to concatenate.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰-è¦è¿æ¥çš„ç»´åº¦ã€‚'
- en: Recursively concatenate the tensors in a nested list/tuple/dictionary of lists
    of tensors with the same shape.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: é€’å½’è¿æ¥å…·æœ‰ç›¸åŒå½¢çŠ¶çš„å¼ é‡åˆ—è¡¨/å…ƒç»„/å­—å…¸ä¸­çš„å¼ é‡ã€‚
- en: '#### `accelerate.utils.convert_outputs_to_fp32`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.convert_outputs_to_fp32`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L813)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L813)'
- en: '[PRE31]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '#### `accelerate.utils.convert_to_fp32`'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.convert_to_fp32`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L766)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L766)'
- en: '[PRE32]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tensor` (nested list/tuple/dictionary of `torch.Tensor`) â€” The data to convert
    from FP16/BF16 to FP32.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor`ï¼ˆåµŒå¥—åˆ—è¡¨/å…ƒç»„/å¼ é‡å­—å…¸çš„`torch.Tensor`ï¼‰-è¦ä»FP16/BF16è½¬æ¢ä¸ºFP32çš„æ•°æ®ã€‚'
- en: Recursively converts the elements nested list/tuple/dictionary of tensors in
    FP16/BF16 precision to FP32.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: é€’å½’å°†FP16/BF16ç²¾åº¦ä¸­çš„åµŒå¥—åˆ—è¡¨/å…ƒç»„/å¼ é‡å­—å…¸ä¸­çš„å…ƒç´ è½¬æ¢ä¸ºFP32ã€‚
- en: '#### `accelerate.utils.gather`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.gather`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L422)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L422)'
- en: '[PRE33]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Parameters
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tensor` (nested list/tuple/dictionary of `torch.Tensor`) â€” The data to gather.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor`ï¼ˆåµŒå¥—åˆ—è¡¨/å…ƒç»„/å¼ é‡å­—å…¸çš„`torch.Tensor`ï¼‰-è¦æ”¶é›†çš„æ•°æ®ã€‚'
- en: Recursively gather tensor in a nested list/tuple/dictionary of tensors from
    all devices.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ‰€æœ‰è®¾å¤‡ä¸­é€’å½’æ”¶é›†åµŒå¥—åˆ—è¡¨/å…ƒç»„/å¼ é‡å­—å…¸ä¸­çš„å¼ é‡ã€‚
- en: '#### `accelerate.utils.gather_object`'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.gather_object`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L449)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L449)'
- en: '[PRE34]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Parameters
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`object` (nested list/tuple/dictionary of picklable object) â€” The data to gather.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`å¯¹è±¡`ï¼ˆåµŒå¥—çš„åˆ—è¡¨/å…ƒç»„/å­—å…¸ï¼ŒåŒ…å«å¯pickleå¯¹è±¡ï¼‰- è¦æ”¶é›†çš„æ•°æ®ã€‚'
- en: Recursively gather object in a nested list/tuple/dictionary of objects from
    all devices.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: é€’å½’åœ°ä»æ‰€æœ‰è®¾å¤‡ä¸­çš„åµŒå¥—åˆ—è¡¨/å…ƒç»„/å­—å…¸å¯¹è±¡ä¸­æ”¶é›†å¯¹è±¡ã€‚
- en: '#### `accelerate.utils.listify`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.listify`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L283)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L283)'
- en: '[PRE35]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Parameters
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`data` (nested list/tuple/dictionary of `torch.Tensor`) â€” The data from which
    to convert to regular numbers.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`æ•°æ®`ï¼ˆåµŒå¥—çš„åˆ—è¡¨/å…ƒç»„/å­—å…¸ï¼ŒåŒ…å«`torch.Tensor`ï¼‰- è¦è½¬æ¢ä¸ºå¸¸è§„æ•°å­—çš„æ•°æ®ã€‚'
- en: Recursively finds tensors in a nested list/tuple/dictionary and converts them
    to a list of numbers.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: é€’å½’åœ°æŸ¥æ‰¾åµŒå¥—åˆ—è¡¨/å…ƒç»„/å­—å…¸ä¸­çš„å¼ é‡ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºæ•°å­—åˆ—è¡¨ã€‚
- en: '#### `accelerate.utils.pad_across_processes`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.pad_across_processes`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L631)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L631)'
- en: '[PRE36]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Parameters
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tensor` (nested list/tuple/dictionary of `torch.Tensor`) â€” The data to gather.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`å¼ é‡`ï¼ˆåµŒå¥—çš„åˆ—è¡¨/å…ƒç»„/å­—å…¸ï¼ŒåŒ…å«`torch.Tensor`ï¼‰- è¦æ”¶é›†çš„æ•°æ®ã€‚'
- en: '`dim` (`int`, *optional*, defaults to 0) â€” The dimension on which to pad.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰- è¦å¡«å……çš„ç»´åº¦ã€‚'
- en: '`pad_index` (`int`, *optional*, defaults to 0) â€” The value with which to pad.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰- ç”¨äºå¡«å……çš„å€¼ã€‚'
- en: '`pad_first` (`bool`, *optional*, defaults to `False`) â€” Whether to pad at the
    beginning or the end.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_first`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰- æ˜¯å¦åœ¨å¼€å¤´æˆ–ç»“å°¾å¡«å……ã€‚'
- en: Recursively pad the tensors in a nested list/tuple/dictionary of tensors from
    all devices to the same size so they can safely be gathered.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: é€’å½’åœ°å¡«å……æ¥è‡ªæ‰€æœ‰è®¾å¤‡çš„åµŒå¥—åˆ—è¡¨/å…ƒç»„/å­—å…¸ä¸­çš„å¼ é‡ï¼Œä½¿å®ƒä»¬è¾¾åˆ°ç›¸åŒçš„å¤§å°ï¼Œä»¥ä¾¿å®‰å…¨åœ°æ”¶é›†ã€‚
- en: '#### `accelerate.utils.recursively_apply`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.recursively_apply`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L93)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L93)'
- en: '[PRE37]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Parameters
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`func` (`callable`) â€” The function to recursively apply.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`func`ï¼ˆ`callable`ï¼‰- è¦é€’å½’åº”ç”¨çš„å‡½æ•°ã€‚'
- en: '`data` (nested list/tuple/dictionary of `main_type`) â€” The data on which to
    apply `func` *args â€” Positional arguments that will be passed to `func` when applied
    on the unpacked data.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`æ•°æ®`ï¼ˆåµŒå¥—çš„åˆ—è¡¨/å…ƒç»„/å­—å…¸ï¼ŒåŒ…å«`main_type`ï¼‰- è¦åº”ç”¨`func`çš„æ•°æ®*args- å½“åº”ç”¨äºè§£åŒ…æ•°æ®æ—¶å°†ä¼ é€’ç»™`func`çš„ä½ç½®å‚æ•°ã€‚'
- en: '`main_type` (`type`, *optional*, defaults to `torch.Tensor`) â€” The base type
    of the objects to which apply `func`.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`main_type`ï¼ˆ`type`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`torch.Tensor`ï¼‰- è¦åº”ç”¨`func`çš„å¯¹è±¡çš„åŸºæœ¬ç±»å‹ã€‚'
- en: '`error_on_other_type` (`bool`, *optional*, defaults to `False`) â€” Whether to
    return an error or not if after unpacking `data`, we get on an object that is
    not of type `main_type`. If `False`, the function will leave objects of types
    different than `main_type` unchanged. **kwargs â€” Keyword arguments that will be
    passed to `func` when applied on the unpacked data.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`error_on_other_type`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰- å¦‚æœåœ¨è§£åŒ…`æ•°æ®`åå¾—åˆ°ä¸€ä¸ªä¸æ˜¯`main_type`ç±»å‹çš„å¯¹è±¡ï¼Œæ˜¯å¦è¿”å›é”™è¯¯ã€‚å¦‚æœä¸º`False`ï¼Œå‡½æ•°å°†ä¿æŒä¸`main_type`ä¸åŒç±»å‹çš„å¯¹è±¡ä¸å˜ã€‚**kwargs-
    å½“åº”ç”¨äºè§£åŒ…æ•°æ®æ—¶å°†ä¼ é€’ç»™`func`çš„å…³é”®å­—å‚æ•°ã€‚'
- en: Recursively apply a function on a data structure that is a nested list/tuple/dictionary
    of a given base type.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: é€’å½’åœ°åœ¨ç»™å®šåŸºæœ¬ç±»å‹çš„åµŒå¥—åˆ—è¡¨/å…ƒç»„/å­—å…¸ä¸Šåº”ç”¨å‡½æ•°ã€‚
- en: '#### `accelerate.utils.reduce`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.reduce`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L724)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L724)'
- en: '[PRE38]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tensor` (nested list/tuple/dictionary of `torch.Tensor`) â€” The data to reduce.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`å¼ é‡`ï¼ˆåµŒå¥—çš„åˆ—è¡¨/å…ƒç»„/å­—å…¸ï¼ŒåŒ…å«`torch.Tensor`ï¼‰- è¦å‡å°‘çš„æ•°æ®ã€‚'
- en: '`reduction` (`str`, *optional*, defaults to `"mean"`) â€” A reduction method.
    Can be of â€œmeanâ€, â€œsumâ€, or â€œnoneâ€'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduction`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"mean"`ï¼‰- ä¸€ç§å‡å°‘æ–¹æ³•ã€‚å¯ä»¥æ˜¯â€œmeanâ€ï¼Œâ€œsumâ€æˆ–â€œnoneâ€'
- en: '`scale` (`float`, *optional*) â€” A default scaling value to be applied after
    the reduce, only valied on XLA.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼‰- åœ¨å‡å°‘ååº”ç”¨çš„é»˜è®¤ç¼©æ”¾å€¼ï¼Œä»…åœ¨XLAä¸Šæœ‰æ•ˆã€‚'
- en: Recursively reduce the tensors in a nested list/tuple/dictionary of lists of
    tensors across all processes by the mean of a given operation.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ç»™å®šæ“ä½œçš„å¹³å‡å€¼ï¼Œé€’å½’åœ°å‡å°‘æ‰€æœ‰è¿›ç¨‹ä¸­çš„å¼ é‡åˆ—è¡¨/å…ƒç»„/å­—å…¸ä¸­çš„å¼ é‡ã€‚
- en: '#### `accelerate.utils.send_to_device`'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.send_to_device`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L144)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L144)'
- en: '[PRE39]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Parameters
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tensor` (nested list/tuple/dictionary of `torch.Tensor`) â€” The data to send
    to a given device.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`å¼ é‡`ï¼ˆåµŒå¥—çš„åˆ—è¡¨/å…ƒç»„/å­—å…¸ï¼ŒåŒ…å«`torch.Tensor`ï¼‰- è¦å‘é€åˆ°æŒ‡å®šè®¾å¤‡çš„æ•°æ®ã€‚'
- en: '`device` (`torch.device`) â€” The device to send the data to.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`ï¼ˆ`torch.device`ï¼‰- è¦å‘é€æ•°æ®çš„è®¾å¤‡ã€‚'
- en: Recursively sends the elements in a nested list/tuple/dictionary of tensors
    to a given device.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: é€’å½’åœ°å°†åµŒå¥—çš„åˆ—è¡¨/å…ƒç»„/å­—å…¸ä¸­çš„å…ƒç´ å‘é€åˆ°æŒ‡å®šè®¾å¤‡ã€‚
- en: '#### `accelerate.utils.slice_tensors`'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.slice_tensors`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L585)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L585)'
- en: '[PRE40]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`data` (nested list/tuple/dictionary of `torch.Tensor`) â€” The data to slice.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`æ•°æ®`ï¼ˆåµŒå¥—çš„åˆ—è¡¨/å…ƒç»„/å­—å…¸ï¼ŒåŒ…å«`torch.Tensor`ï¼‰- è¦åˆ‡ç‰‡çš„æ•°æ®ã€‚'
- en: '`tensor_slice` (`slice`) â€” The slice to take.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor_slice`ï¼ˆ`slice`ï¼‰- è¦å–çš„åˆ‡ç‰‡ã€‚'
- en: Recursively takes a slice in a nested list/tuple/dictionary of tensors.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: é€’å½’åœ°åœ¨åµŒå¥—çš„åˆ—è¡¨/å…ƒç»„/å­—å…¸ä¸­çš„å¼ é‡ä¸­å–ä¸€ä¸ªåˆ‡ç‰‡ã€‚
- en: Environment Checks
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¯å¢ƒæ£€æŸ¥
- en: These functionalities check the state of the current working environment including
    information about the operating system itself, what it can support, and if particular
    dependencies are installed.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åŠŸèƒ½æ£€æŸ¥å½“å‰å·¥ä½œç¯å¢ƒçš„çŠ¶æ€ï¼ŒåŒ…æ‹¬æœ‰å…³æ“ä½œç³»ç»Ÿæœ¬èº«çš„ä¿¡æ¯ï¼Œå®ƒå¯ä»¥æ”¯æŒçš„å†…å®¹ï¼Œä»¥åŠç‰¹å®šä¾èµ–é¡¹æ˜¯å¦å·²å®‰è£…ã€‚
- en: '#### `accelerate.utils.is_bf16_available`'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.is_bf16_available`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/imports.py#L130)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/imports.py#L130)'
- en: '[PRE41]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Checks if bf16 is supported, optionally ignoring the TPU
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥æ˜¯å¦æ”¯æŒbf16ï¼Œå¯é€‰æ‹©å¿½ç•¥TPU
- en: '#### `accelerate.utils.is_ipex_available`'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.is_ipex_available`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/imports.py#L255)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/imports.py#L255)'
- en: '[PRE42]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '#### `accelerate.utils.is_mps_available`'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.is_mps_available`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/imports.py#L251)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/imports.py#L251)'
- en: '[PRE43]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '#### `accelerate.utils.is_npu_available`'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.is_npu_available`'
- en: '[PRE44]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Checks if `torch_npu` is installed and potentially if a NPU is in the environment
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥æ˜¯å¦å®‰è£…äº†`torch_npu`ï¼Œå¹¶ä¸”å¯èƒ½ç¯å¢ƒä¸­æ˜¯å¦æœ‰NPU
- en: '#### `accelerate.utils.is_torch_version`'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.is_torch_version`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/versions.py#L46)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/versions.py#L46)'
- en: '[PRE45]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Parameters
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`operation` (`str`) â€” A string representation of an operator, such as `">"`
    or `"<="`'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`operation`ï¼ˆ`str`ï¼‰â€” æ“ä½œç¬¦çš„å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œä¾‹å¦‚`">"`æˆ–`"<="`'
- en: '`version` (`str`) â€” A string version of PyTorch'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`version`ï¼ˆ`str`ï¼‰â€” PyTorchçš„å­—ç¬¦ä¸²ç‰ˆæœ¬'
- en: Compares the current PyTorch version to a given reference with an operation.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å½“å‰çš„PyTorchç‰ˆæœ¬ä¸ç»™å®šçš„å‚è€ƒç‰ˆæœ¬è¿›è¡Œæ¯”è¾ƒã€‚
- en: '#### `accelerate.utils.is_tpu_available`'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.is_tpu_available`'
- en: '[PRE46]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Checks if `torch_xla` is installed and potentially if a TPU is in the environment
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥æ˜¯å¦å®‰è£…äº†`torch_xla`ï¼Œå¹¶ä¸”å¯èƒ½ç¯å¢ƒä¸­æ˜¯å¦æœ‰TPU
- en: '#### `accelerate.utils.is_xpu_available`'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.is_xpu_available`'
- en: '[PRE47]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: check if user disables it explicitly
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ˜ç¡®ç¦ç”¨å®ƒ
- en: Environment Manipulation
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¯å¢ƒæ“ä½œ
- en: '#### `accelerate.utils.patch_environment`'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.patch_environment`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L219)'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L219)'
- en: '[PRE48]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: A context manager that will add each keyword argument passed to `os.environ`
    and remove them when exiting.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œå°†æ¯ä¸ªå…³é”®å­—å‚æ•°æ·»åŠ åˆ°`os.environ`ä¸­ï¼Œå¹¶åœ¨é€€å‡ºæ—¶å°†å®ƒä»¬åˆ é™¤ã€‚
- en: Will convert the values in `kwargs` to strings and upper-case all the keys.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: å°†`kwargs`ä¸­çš„å€¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå¹¶å°†æ‰€æœ‰é”®å¤§å†™ã€‚
- en: 'Example:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE49]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '#### `accelerate.utils.clear_environment`'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.clear_environment`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L186)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L186)'
- en: '[PRE50]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: A context manager that will cache origin `os.environ` and replace it with a
    empty dictionary in this context.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œå°†ç¼“å­˜çš„åŸå§‹`os.environ`æ›¿æ¢ä¸ºä¸€ä¸ªç©ºå­—å…¸åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ã€‚
- en: When this context exits, the cached `os.environ` will be back.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ­¤ä¸Šä¸‹æ–‡é€€å‡ºæ—¶ï¼Œç¼“å­˜çš„`os.environ`å°†æ¢å¤ã€‚
- en: 'Example:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE51]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '#### `accelerate.commands.config.default.write_basic_config`'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.commands.config.default.write_basic_config`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/commands/config/default.py#L29)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/commands/config/default.py#L29)'
- en: '[PRE52]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Parameters
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`mixed_precision` (`str`, *optional*, defaults to â€œnoâ€) â€” Mixed Precision to
    use. Should be one of â€œnoâ€, â€œfp16â€, or â€œbf16â€'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mixed_precision`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸ºâ€œnoâ€ï¼‰â€” è¦ä½¿ç”¨çš„æ··åˆç²¾åº¦ã€‚åº”è¯¥æ˜¯â€œnoâ€ï¼Œâ€œfp16â€æˆ–â€œbf16â€ä¸­çš„ä¸€ä¸ªã€‚'
- en: '`save_location` (`str`, *optional*, defaults to `default_json_config_file`)
    â€” Optional custom save location. Should be passed to `--config_file` when using
    `accelerate launch`. Default location is inside the huggingface cache folder (`~/.cache/huggingface`)
    but can be overriden by setting the `HF_HOME` environmental variable, followed
    by `accelerate/default_config.yaml`.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_location`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`default_json_config_file`ï¼‰â€” å¯é€‰çš„è‡ªå®šä¹‰ä¿å­˜ä½ç½®ã€‚åœ¨ä½¿ç”¨`accelerate
    launch`æ—¶åº”ä¼ é€’ç»™`--config_file`ã€‚é»˜è®¤ä½ç½®ä½äºhuggingfaceç¼“å­˜æ–‡ä»¶å¤¹å†…ï¼ˆ`~/.cache/huggingface`ï¼‰ï¼Œä½†å¯ä»¥é€šè¿‡è®¾ç½®`HF_HOME`ç¯å¢ƒå˜é‡ï¼Œç„¶åè·Ÿéš`accelerate/default_config.yaml`æ¥è¦†ç›–ã€‚'
- en: '`use_xpu` (`bool`, *optional*, defaults to `False`) â€” Whether to use XPU if
    available.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_xpu`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨å¯ç”¨æ—¶ä½¿ç”¨XPUã€‚'
- en: Creates and saves a basic cluster config to be used on a local machine with
    potentially multiple GPUs. Will also set CPU if it is a CPU-only machine.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºå¹¶ä¿å­˜ä¸€ä¸ªåŸºæœ¬çš„é›†ç¾¤é…ç½®ï¼Œç”¨äºåœ¨æœ¬åœ°æœºå™¨ä¸Šä½¿ç”¨å¯èƒ½æœ‰å¤šä¸ªGPUã€‚å¦‚æœæ˜¯ä»…CPUçš„æœºå™¨ï¼Œè¿˜å°†è®¾ç½®CPUã€‚
- en: When setting up ğŸ¤— Accelerate for the first time, rather than running `accelerate
    config` [~utils.write_basic_config] can be used as an alternative for quick configuration.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–æ¬¡è®¾ç½®ğŸ¤— Accelerateæ—¶ï¼Œå¯ä»¥ä½¿ç”¨`accelerate config`è€Œä¸æ˜¯è¿è¡Œ[~utils.write_basic_config]ä½œä¸ºå¿«é€Ÿé…ç½®çš„æ›¿ä»£æ–¹æ³•ã€‚
- en: Memory
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å†…å­˜
- en: '#### `accelerate.find_executable_batch_size`'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.find_executable_batch_size`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/memory.py#L83)'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/memory.py#L83)'
- en: '[PRE53]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Parameters
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`function` (`callable`, *optional*) â€” A function to wrap'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`function`ï¼ˆ`callable`ï¼Œ*å¯é€‰*ï¼‰â€” è¦åŒ…è£…çš„å‡½æ•°'
- en: '`starting_batch_size` (`int`, *optional*) â€” The batch size to try and fit into
    memory'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`starting_batch_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” å°è¯•é€‚åº”å†…å­˜çš„æ‰¹é‡å¤§å°'
- en: A basic decorator that will try to execute `function`. If it fails from exceptions
    related to out-of-memory or CUDNN, the batch size is cut in half and passed to
    `function`
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŸºæœ¬çš„è£…é¥°å™¨ï¼Œå°†å°è¯•æ‰§è¡Œ`function`ã€‚å¦‚æœç”±äºå†…å­˜ä¸è¶³æˆ–CUDNNç›¸å…³çš„å¼‚å¸¸è€Œå¤±è´¥ï¼Œåˆ™å°†æ‰¹é‡å¤§å°å‡åŠå¹¶ä¼ é€’ç»™`function`
- en: '`function` must take in a `batch_size` parameter as its first argument.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '`function`å¿…é¡»å°†`batch_size`å‚æ•°ä½œä¸ºå…¶ç¬¬ä¸€ä¸ªå‚æ•°ã€‚'
- en: 'Example:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE54]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Modeling
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å»ºæ¨¡
- en: These utilities relate to interacting with PyTorch models
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å®ç”¨ç¨‹åºä¸ä¸PyTorchæ¨¡å‹äº¤äº’æœ‰å…³
- en: '#### `accelerate.utils.calculate_maximum_sizes`'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.calculate_maximum_sizes`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1004)'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1004)'
- en: '[PRE55]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Computes the total size of the model and its largest layer
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—æ¨¡å‹åŠå…¶æœ€å¤§å±‚çš„æ€»å¤§å°
- en: '#### `accelerate.utils.compute_module_sizes`'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.compute_module_sizes`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L686)'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L686)'
- en: '[PRE56]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Compute the size of each submodule of a given model.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—ç»™å®šæ¨¡å‹çš„æ¯ä¸ªå­æ¨¡å—çš„å¤§å°ã€‚
- en: '#### `accelerate.utils.extract_model_from_parallel`'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.extract_model_from_parallel`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L56)'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L56)'
- en: '[PRE57]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Parameters
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`torch.nn.Module`) â€” The model to extract.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) â€” è¦æå–çš„æ¨¡å‹ã€‚'
- en: '`keep_fp32_wrapper` (`bool`, *optional*) â€” Whether to remove mixed precision
    hooks from the model.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_fp32_wrapper` (`bool`, *å¯é€‰*) â€” æ˜¯å¦ä»æ¨¡å‹ä¸­åˆ é™¤æ··åˆç²¾åº¦é’©å­ã€‚'
- en: Returns
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`torch.nn.Module`'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Module`'
- en: The extracted model.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: æå–çš„æ¨¡å‹ã€‚
- en: Extract a model from its distributed containers.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å…¶åˆ†å¸ƒå¼å®¹å™¨ä¸­æå–æ¨¡å‹ã€‚
- en: '#### `accelerate.utils.get_balanced_memory`'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.get_balanced_memory`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L872)'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L872)'
- en: '[PRE58]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Parameters
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`torch.nn.Module`) â€” The model to analyze.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) â€” è¦åˆ†æçš„æ¨¡å‹ã€‚'
- en: '`max_memory` (`Dict`, *optional*) â€” A dictionary device identifier to maximum
    memory. Will default to the maximum memory available if unset. Example: `max_memory={0:
    "1GB"}`.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_memory` (`Dict`, *å¯é€‰*) â€” è®¾å¤‡æ ‡è¯†ç¬¦åˆ°æœ€å¤§å†…å­˜çš„å­—å…¸ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†é»˜è®¤ä¸ºå¯ç”¨çš„æœ€å¤§å†…å­˜ã€‚ç¤ºä¾‹ï¼š`max_memory={0:
    "1GB"}`ã€‚'
- en: '`no_split_module_classes` (`List[str]`, *optional*) â€” A list of layer class
    names that should never be split across device (for instance any layer that has
    a residual connection).'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no_split_module_classes` (`List[str]`, *å¯é€‰*) â€” ä¸åº”è·¨è®¾å¤‡æ‹†åˆ†çš„å±‚ç±»ååˆ—è¡¨ï¼ˆä¾‹å¦‚å…·æœ‰æ®‹å·®è¿æ¥çš„ä»»ä½•å±‚ï¼‰ã€‚'
- en: '`dtype` (`str` or `torch.dtype`, *optional*) â€” If provided, the weights will
    be converted to that type when loaded.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype` (`str` æˆ– `torch.dtype`, *å¯é€‰*) â€” å¦‚æœæä¾›ï¼ŒåŠ è½½æ—¶å°†å°†æƒé‡è½¬æ¢ä¸ºè¯¥ç±»å‹ã€‚'
- en: '`special_dtypes` (`Dict[str, Union[str, torch.device]]`, *optional*) â€” If provided,
    special dtypes to consider for some specific weights (will override dtype used
    as default for all weights).'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_dtypes` (`Dict[str, Union[str, torch.device]]`, *å¯é€‰*) â€” å¦‚æœæä¾›ï¼Œç”¨äºæŸäº›ç‰¹å®šæƒé‡çš„ç‰¹æ®Šæ•°æ®ç±»å‹ï¼ˆå°†è¦†ç›–é»˜è®¤ç”¨äºæ‰€æœ‰æƒé‡çš„
    dtypeï¼‰ã€‚'
- en: '`low_zero` (`bool`, *optional*) â€” Minimizes the number of weights on GPU 0,
    which is convenient when itâ€™s used for other operations (like the Transformers
    generate function).'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_zero` (`bool`, *å¯é€‰*) â€” æœ€å°åŒ– GPU 0 ä¸Šçš„æƒé‡æ•°é‡ï¼Œåœ¨å…¶ä»–æ“ä½œï¼ˆå¦‚ Transformers ç”Ÿæˆå‡½æ•°ï¼‰ä¸­ä½¿ç”¨æ—¶å¾ˆæ–¹ä¾¿ã€‚'
- en: Compute a `max_memory` dictionary for [infer_auto_device_map()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.infer_auto_device_map)
    that will balance the use of each available GPU.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸º [infer_auto_device_map()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.infer_auto_device_map)
    è®¡ç®—ä¸€ä¸ª `max_memory` å­—å…¸ï¼Œä»¥å¹³è¡¡æ¯ä¸ªå¯ç”¨ GPU çš„ä½¿ç”¨ã€‚
- en: All computation is done analyzing sizes and dtypes of the model parameters.
    As a result, the model can be on the meta device (as it would if initialized within
    the `init_empty_weights` context manager).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è®¡ç®—éƒ½æ˜¯é€šè¿‡åˆ†ææ¨¡å‹å‚æ•°çš„å¤§å°å’Œæ•°æ®ç±»å‹æ¥å®Œæˆçš„ã€‚å› æ­¤ï¼Œæ¨¡å‹å¯ä»¥ä½äºå…ƒè®¾å¤‡ä¸Šï¼ˆå°±åƒåœ¨ `init_empty_weights` ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­åˆå§‹åŒ–æ—¶ä¸€æ ·ï¼‰ã€‚
- en: '#### `accelerate.utils.get_max_layer_size`'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.get_max_layer_size`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L719)'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L719)'
- en: '[PRE59]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Parameters
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`modules` (`List[Tuple[str, torch.nn.Module]]`) â€” The list of named modules
    where we want to determine the maximum layer size.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modules` (`List[Tuple[str, torch.nn.Module]]`) â€” æˆ‘ä»¬è¦ç¡®å®šæœ€å¤§å±‚å°ºå¯¸çš„å‘½åæ¨¡å—åˆ—è¡¨ã€‚'
- en: '`module_sizes` (`Dict[str, int]`) â€” A dictionary mapping each layer name to
    its size (as generated by `compute_module_sizes`).'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`module_sizes` (`Dict[str, int]`) â€” å°†æ¯ä¸ªå±‚åç§°æ˜ å°„åˆ°å…¶å¤§å°çš„å­—å…¸ï¼ˆç”± `compute_module_sizes`
    ç”Ÿæˆï¼‰ã€‚'
- en: '`no_split_module_classes` (`List[str]`) â€” A list of class names for layers
    we donâ€™t want to be split.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no_split_module_classes` (`List[str]`) â€” ä¸å¸Œæœ›æ‹†åˆ†çš„å±‚ç±»ååˆ—è¡¨ã€‚'
- en: Returns
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`Tuple[int, List[str]]`'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tuple[int, List[str]]`'
- en: The maximum size of a layer with the list of layer names realizing that maximum
    size.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: å…·æœ‰å®ç°æœ€å¤§å°ºå¯¸çš„å±‚åç§°åˆ—è¡¨çš„æœ€å¤§å±‚å°ºå¯¸ã€‚
- en: 'Utility function that will scan a list of named modules and return the maximum
    size used by one full layer. The definition of a layer being:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‰«æå‘½åæ¨¡å—åˆ—è¡¨å¹¶è¿”å›ä¸€ä¸ªå®Œæ•´å±‚ä½¿ç”¨çš„æœ€å¤§å°ºå¯¸çš„å®ç”¨å‡½æ•°ã€‚å±‚çš„å®šä¹‰ä¸ºï¼š
- en: a module with no direct children (just parameters and buffers)
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ²¡æœ‰ç›´æ¥å­çº§ï¼ˆåªæœ‰å‚æ•°å’Œç¼“å†²åŒºï¼‰çš„æ¨¡å—
- en: a module whose class name is in the list `no_split_module_classes`
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»ååœ¨åˆ—è¡¨ `no_split_module_classes` ä¸­çš„æ¨¡å—
- en: '#### `accelerate.infer_auto_device_map`'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.infer_auto_device_map`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1022)'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1022)'
- en: '[PRE60]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Parameters
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`torch.nn.Module`) â€” The model to analyze.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) â€” è¦åˆ†æçš„æ¨¡å‹ã€‚'
- en: '`max_memory` (`Dict`, *optional*) â€” A dictionary device identifier to maximum
    memory. Will default to the maximum memory available if unset. Example: `max_memory={0:
    "1GB"}`.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_memory` (`Dict`, *å¯é€‰*) â€” è®¾å¤‡æ ‡è¯†ç¬¦åˆ°æœ€å¤§å†…å­˜çš„å­—å…¸ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†é»˜è®¤ä¸ºå¯ç”¨çš„æœ€å¤§å†…å­˜ã€‚ç¤ºä¾‹ï¼š`max_memory={0:
    "1GB"}`ã€‚'
- en: '`no_split_module_classes` (`List[str]`, *optional*) â€” A list of layer class
    names that should never be split across device (for instance any layer that has
    a residual connection).'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no_split_module_classes` (`List[str]`, *å¯é€‰*) â€” ä¸åº”è·¨è®¾å¤‡æ‹†åˆ†çš„å±‚ç±»ååˆ—è¡¨ï¼ˆä¾‹å¦‚å…·æœ‰æ®‹å·®è¿æ¥çš„ä»»ä½•å±‚ï¼‰ã€‚'
- en: '`dtype` (`str` or `torch.dtype`, *optional*) â€” If provided, the weights will
    be converted to that type when loaded.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype` (`str` æˆ– `torch.dtype`, *å¯é€‰*) â€” å¦‚æœæä¾›ï¼ŒåŠ è½½æ—¶å°†å°†æƒé‡è½¬æ¢ä¸ºè¯¥ç±»å‹ã€‚'
- en: '`special_dtypes` (`Dict[str, Union[str, torch.device]]`, *optional*) â€” If provided,
    special dtypes to consider for some specific weights (will override dtype used
    as default for all weights).'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_dtypes` (`Dict[str, Union[str, torch.device]]`, *å¯é€‰*) â€” å¦‚æœæä¾›ï¼Œç”¨äºæŸäº›ç‰¹å®šæƒé‡çš„ç‰¹æ®Šæ•°æ®ç±»å‹ï¼ˆå°†è¦†ç›–é»˜è®¤ç”¨äºæ‰€æœ‰æƒé‡çš„
    dtypeï¼‰ã€‚'
- en: '`verbose` (`bool`, *optional*, defaults to `False`) â€” Whether or not to provide
    debugging statements as the function builds the device_map.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨å‡½æ•°æ„å»º device_map æ—¶æä¾›è°ƒè¯•è¯­å¥ã€‚'
- en: '`clean_result` (`bool`, *optional*, defaults to `True`) â€” Clean the resulting
    device_map by grouping all submodules that go on the same device together.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_result` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” é€šè¿‡å°†æ‰€æœ‰æ”¾åœ¨åŒä¸€è®¾å¤‡ä¸Šçš„å­æ¨¡å—åˆ†ç»„æ¥æ¸…ç†ç»“æœçš„ device_mapã€‚'
- en: 'Compute a device map for a given model giving priority to GPUs, then offload
    on CPU and finally offload to disk, such that:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºç»™å®šæ¨¡å‹è®¡ç®—è®¾å¤‡æ˜ å°„ï¼Œä¼˜å…ˆè€ƒè™‘GPUï¼Œç„¶åè½¬ç§»åˆ°CPUï¼Œæœ€åè½¬ç§»åˆ°ç£ç›˜ï¼Œä½¿å¾—ï¼š
- en: we donâ€™t exceed the memory available of any of the GPU.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ä¼šè¶…å‡ºä»»ä½•GPUå¯ç”¨çš„å†…å­˜ã€‚
- en: if offload to the CPU is needed, there is always room left on GPU 0 to put back
    the layer offloaded on CPU that has the largest size.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœéœ€è¦è½¬ç§»åˆ°CPUï¼Œæ€»æ˜¯åœ¨GPU 0ä¸Šç•™æœ‰ç©ºé—´ï¼Œä»¥å°†åœ¨CPUä¸Šè½¬ç§»çš„å…·æœ‰æœ€å¤§å°ºå¯¸çš„å±‚æ”¾å›ã€‚
- en: if offload to the CPU is needed,we donâ€™t exceed the RAM available on the CPU.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœéœ€è¦è½¬ç§»åˆ°CPUï¼Œæˆ‘ä»¬ä¸ä¼šè¶…å‡ºCPUå¯ç”¨çš„RAMã€‚
- en: if offload to the disk is needed, there is always room left on the CPU to put
    back the layer offloaded on disk that has the largest size.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœéœ€è¦è½¬ç§»åˆ°ç£ç›˜ï¼Œæ€»æ˜¯åœ¨CPUä¸Šç•™æœ‰ç©ºé—´ï¼Œä»¥å°†åœ¨ç£ç›˜ä¸Šè½¬ç§»çš„å…·æœ‰æœ€å¤§å°ºå¯¸çš„å±‚æ”¾å›ã€‚
- en: All computation is done analyzing sizes and dtypes of the model parameters.
    As a result, the model can be on the meta device (as it would if initialized within
    the `init_empty_weights` context manager).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è®¡ç®—éƒ½æ˜¯é€šè¿‡åˆ†ææ¨¡å‹å‚æ•°çš„å¤§å°å’Œæ•°æ®ç±»å‹æ¥å®Œæˆçš„ã€‚å› æ­¤ï¼Œæ¨¡å‹å¯ä»¥åœ¨å…ƒè®¾å¤‡ä¸Šï¼ˆå°±åƒåœ¨`init_empty_weights`ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­åˆå§‹åŒ–æ—¶ä¸€æ ·ï¼‰ã€‚
- en: '#### `accelerate.load_checkpoint_in_model`'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.load_checkpoint_in_model`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1442)'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1442)'
- en: '[PRE61]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Parameters
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`torch.nn.Module`) â€” The model in which we want to load a checkpoint.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) â€” æˆ‘ä»¬è¦åŠ è½½æ£€æŸ¥ç‚¹çš„æ¨¡å‹ã€‚'
- en: '`checkpoint` (`str` or `os.PathLike`) â€” The folder checkpoint to load. It can
    be:'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`checkpoint` (`str`æˆ–`os.PathLike`) â€” è¦åŠ è½½çš„æ–‡ä»¶å¤¹æ£€æŸ¥ç‚¹ã€‚å®ƒå¯ä»¥æ˜¯ï¼š'
- en: a path to a file containing a whole model state dict
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒ…å«æ•´ä¸ªæ¨¡å‹çŠ¶æ€å­—å…¸çš„æ–‡ä»¶è·¯å¾„ã€‚
- en: a path to a `.json` file containing the index to a sharded checkpoint
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒ…å«åˆ†ç‰‡æ£€æŸ¥ç‚¹ç´¢å¼•çš„`.json`æ–‡ä»¶è·¯å¾„ã€‚
- en: a path to a folder containing a unique `.index.json` file and the shards of
    a checkpoint.
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«å”¯ä¸€çš„`.index.json`æ–‡ä»¶å’Œæ£€æŸ¥ç‚¹åˆ†ç‰‡çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
- en: a path to a folder containing a unique pytorch_model.bin or a model.safetensors
    file.
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«å”¯ä¸€çš„pytorch_model.binæˆ–model.safetensorsæ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
- en: '`device_map` (`Dict[str, Union[int, str, torch.device]]`, *optional*) â€” A map
    that specifies where each submodule should go. It doesnâ€™t need to be refined to
    each parameter/buffer name, once a given module name is inside, every submodule
    of it will be sent to the same device.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_map` (`Dict[str, Union[int, str, torch.device]]`, *å¯é€‰*) â€” ä¸€ä¸ªæŒ‡å®šæ¯ä¸ªå­æ¨¡å—åº”è¯¥å»å“ªé‡Œçš„æ˜ å°„ã€‚å®ƒä¸éœ€è¦ç»†åŒ–åˆ°æ¯ä¸ªå‚æ•°/ç¼“å†²åŒºåç§°ï¼Œä¸€æ—¦ç»™å®šæ¨¡å—åç§°åœ¨å†…ï¼Œå®ƒçš„æ¯ä¸ªå­æ¨¡å—éƒ½å°†è¢«å‘é€åˆ°ç›¸åŒçš„è®¾å¤‡ã€‚'
- en: '`offload_folder` (`str` or `os.PathLike`, *optional*) â€” If the `device_map`
    contains any value `"disk"`, the folder where we will offload weights.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`offload_folder` (`str`æˆ–`os.PathLike`, *å¯é€‰*) â€” å¦‚æœ`device_map`åŒ…å«ä»»ä½•å€¼ä¸º`"disk"`ï¼Œåˆ™æˆ‘ä»¬å°†è½¬ç§»æƒé‡çš„æ–‡ä»¶å¤¹ã€‚'
- en: '`dtype` (`str` or `torch.dtype`, *optional*) â€” If provided, the weights will
    be converted to that type when loaded.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype` (`str`æˆ–`torch.dtype`, *å¯é€‰*) â€” å¦‚æœæä¾›ï¼ŒåŠ è½½æ—¶æƒé‡å°†è½¬æ¢ä¸ºè¯¥ç±»å‹ã€‚'
- en: '`offload_state_dict` (`bool`, *optional*, defaults to `False`) â€” If `True`,
    will temporarily offload the CPU state dict on the hard drive to avoid getting
    out of CPU RAM if the weight of the CPU state dict + the biggest shard does not
    fit.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`offload_state_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” å¦‚æœä¸º`True`ï¼Œå°†ä¸´æ—¶å°†CPUçŠ¶æ€å­—å…¸è½¬ç§»åˆ°ç¡¬ç›˜ä¸Šï¼Œä»¥é¿å…å¦‚æœCPUçŠ¶æ€å­—å…¸çš„æƒé‡+æœ€å¤§åˆ†ç‰‡çš„æƒé‡ä¸é€‚åˆæ—¶è¶…å‡ºCPU
    RAMã€‚'
- en: '`offload_buffers` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to include the buffers in the weights offloaded to disk.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`offload_buffers` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦å°†ç¼“å†²åŒºåŒ…å«åœ¨è½¬ç§»åˆ°ç£ç›˜çš„æƒé‡ä¸­ã€‚'
- en: '`keep_in_fp32_modules(List[str],` *optional*) â€” A list of the modules that
    we keep in `torch.float32` dtype.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_in_fp32_modules(List[str],` *å¯é€‰*) â€” æˆ‘ä»¬ä¿ç•™åœ¨`torch.float32`æ•°æ®ç±»å‹ä¸­çš„æ¨¡å—åˆ—è¡¨ã€‚'
- en: '`offload_8bit_bnb` (`bool`, *optional*) â€” Whether or not to enable offload
    of 8-bit modules on cpu/disk.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`offload_8bit_bnb` (`bool`, *å¯é€‰*) â€” æ˜¯å¦å¯ç”¨åœ¨cpu/diskä¸Šè½¬ç§»8ä½æ¨¡å—ã€‚'
- en: Loads a (potentially sharded) checkpoint inside a model, potentially sending
    weights to a given device as they are loaded.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨¡å‹ä¸­åŠ è½½ï¼ˆå¯èƒ½æ˜¯åˆ†ç‰‡çš„ï¼‰æ£€æŸ¥ç‚¹ï¼Œå¯èƒ½åœ¨åŠ è½½æ—¶å°†æƒé‡å‘é€åˆ°æŒ‡å®šçš„è®¾å¤‡ã€‚
- en: Once loaded across devices, you still need to call [dispatch_model()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.dispatch_model)
    on your model to make it able to run. To group the checkpoint loading and dispatch
    in one single call, use [load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch).
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è·¨è®¾å¤‡åŠ è½½ï¼Œæ‚¨ä»ç„¶éœ€è¦åœ¨æ¨¡å‹ä¸Šè°ƒç”¨[dispatch_model()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.dispatch_model)ä½¿å…¶èƒ½å¤Ÿè¿è¡Œã€‚è¦å°†æ£€æŸ¥ç‚¹åŠ è½½å’Œåˆ†å‘ç»„åˆåœ¨ä¸€ä¸ªå•ä¸€è°ƒç”¨ä¸­ï¼Œè¯·ä½¿ç”¨[load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch)ã€‚
- en: '#### `accelerate.utils.load_offloaded_weights`'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.load_offloaded_weights`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L842)'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L842)'
- en: '[PRE62]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Parameters
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`torch.nn.Module`) â€” The model to load the weights into.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) â€” è¦åŠ è½½æƒé‡çš„æ¨¡å‹ã€‚'
- en: '`index` (`dict`) â€” A dictionary containing the parameter name and its metadata
    for each parameter that was offloaded from the model.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index` (`dict`) â€” åŒ…å«ä»æ¨¡å‹è½¬ç§»çš„æ¯ä¸ªå‚æ•°çš„å‚æ•°åç§°åŠå…¶å…ƒæ•°æ®çš„å­—å…¸ã€‚'
- en: '`offload_folder` (`str`) â€” The folder where the offloaded weights are stored.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`offload_folder` (`str`) â€” å­˜å‚¨è½¬ç§»æƒé‡çš„æ–‡ä»¶å¤¹ã€‚'
- en: Loads the weights from the offload folder into the model.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æƒé‡ä»è½¬ç§»æ–‡ä»¶å¤¹åŠ è½½åˆ°æ¨¡å‹ä¸­ã€‚
- en: '#### `accelerate.utils.load_state_dict`'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.load_state_dict`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1301)'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1301)'
- en: '[PRE63]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Parameters
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`checkpoint_file` (`str`) â€” The path to the checkpoint to load.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`checkpoint_file` (`str`) â€” è¦åŠ è½½çš„æ£€æŸ¥ç‚¹è·¯å¾„ã€‚'
- en: '`device_map` (`Dict[str, Union[int, str, torch.device]]`, *optional*) â€” A map
    that specifies where each submodule should go. It doesnâ€™t need to be refined to
    each parameter/buffer name, once a given module name is inside, every submodule
    of it will be sent to the same device.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_map` (`Dict[str, Union[int, str, torch.device]]`, *å¯é€‰*) â€” æŒ‡å®šæ¯ä¸ªå­æ¨¡å—åº”è¯¥å»çš„ä½ç½®çš„æ˜ å°„ã€‚ä¸éœ€è¦å¯¹æ¯ä¸ªå‚æ•°/ç¼“å†²åŒºåç§°è¿›è¡Œç»†åŒ–ï¼Œä¸€æ—¦ç»™å®šæ¨¡å—åç§°åœ¨å†…ï¼Œå®ƒçš„æ¯ä¸ªå­æ¨¡å—éƒ½å°†è¢«å‘é€åˆ°ç›¸åŒçš„è®¾å¤‡ã€‚'
- en: Load a checkpoint from a given file. If the checkpoint is in the safetensors
    format and a device map is passed, the weights can be fast-loaded directly on
    the GPU.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç»™å®šæ–‡ä»¶åŠ è½½æ£€æŸ¥ç‚¹ã€‚å¦‚æœæ£€æŸ¥ç‚¹æ˜¯åœ¨safetensorsæ ¼å¼ä¸­å¹¶ä¸”ä¼ é€’äº†è®¾å¤‡æ˜ å°„ï¼Œåˆ™æƒé‡å¯ä»¥ç›´æ¥å¿«é€ŸåŠ è½½åˆ°GPUä¸Šã€‚
- en: '#### `accelerate.utils.offload_state_dict`'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.offload_state_dict`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/offload.py#L85)'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/offload.py#L85)'
- en: '[PRE64]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Parameters
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`save_dir` (`str` or `os.PathLike`) â€” The directory in which to offload the
    state dict.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_dir` (`str` æˆ– `os.PathLike`) â€” è¦å¸è½½çŠ¶æ€å­—å…¸çš„ç›®å½•ã€‚'
- en: '`state_dict` (`Dict[str, torch.Tensor]`) â€” The dictionary of tensors to offload.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state_dict` (`Dict[str, torch.Tensor]`) â€” è¦å¸è½½çš„å¼ é‡å­—å…¸ã€‚'
- en: Offload a state dict in a given folder.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»™å®šæ–‡ä»¶å¤¹ä¸­å¸è½½çŠ¶æ€å­—å…¸ã€‚
- en: '#### `accelerate.utils.retie_parameters`'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.retie_parameters`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L644)'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L644)'
- en: '[PRE65]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Parameters
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`torch.nn.Module`) â€” The model in which to retie parameters.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) â€” è¦é‡æ–°ç»‘å®šå‚æ•°çš„æ¨¡å‹ã€‚'
- en: '`tied_params` (`List[List[str]]`) â€” A mapping parameter name to tied parameter
    name as obtained by `find_tied_parameters`.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tied_params` (`List[List[str]]`) â€” é€šè¿‡`find_tied_parameters`è·å¾—çš„å‚æ•°åç§°åˆ°ç»‘å®šå‚æ•°åç§°çš„æ˜ å°„ã€‚'
- en: Reties tied parameters in a given model if the link was broken (for instance
    when adding hooks).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»™å®šæ¨¡å‹ä¸­é‡æ–°ç»‘å®šç»‘å®šå‚æ•°ï¼Œå¦‚æœé“¾æ¥è¢«æ–­å¼€ï¼ˆä¾‹å¦‚æ·»åŠ é’©å­æ—¶ï¼‰ã€‚
- en: '#### `accelerate.utils.set_module_tensor_to_device`'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.set_module_tensor_to_device`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L275)'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L275)'
- en: '[PRE66]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Parameters
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`module` (`torch.nn.Module`) â€” The module in which the tensor we want to move
    lives.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`module` (`torch.nn.Module`) â€” æˆ‘ä»¬æƒ³è¦ç§»åŠ¨çš„å¼ é‡æ‰€åœ¨çš„æ¨¡å—ã€‚'
- en: '`tensor_name` (`str`) â€” The full name of the parameter/buffer.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor_name` (`str`) â€” å‚æ•°/ç¼“å†²åŒºçš„å®Œæ•´åç§°ã€‚'
- en: '`device` (`int`, `str` or `torch.device`) â€” The device on which to set the
    tensor.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device` (`int`, `str` æˆ– `torch.device`) â€” è¦è®¾ç½®å¼ é‡çš„è®¾å¤‡ã€‚'
- en: '`value` (`torch.Tensor`, *optional*) â€” The value of the tensor (useful when
    going from the meta device to any other device).'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value` (`torch.Tensor`, *å¯é€‰*) â€” å¼ é‡çš„å€¼ï¼ˆåœ¨ä»å…ƒè®¾å¤‡è½¬åˆ°ä»»ä½•å…¶ä»–è®¾å¤‡æ—¶æœ‰ç”¨ï¼‰ã€‚'
- en: '`dtype` (`torch.dtype`, *optional*) â€” If passed along the value of the parameter
    will be cast to this `dtype`. Otherwise, `value` will be cast to the dtype of
    the existing parameter in the model.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype` (`torch.dtype`, *å¯é€‰*) â€” å¦‚æœä¼ é€’äº†å‚æ•°ï¼Œå‚æ•°çš„å€¼å°†è¢«è½¬æ¢ä¸ºè¿™ä¸ª`dtype`ã€‚å¦åˆ™ï¼Œ`value`å°†è¢«è½¬æ¢ä¸ºæ¨¡å‹ä¸­ç°æœ‰å‚æ•°çš„`dtype`ã€‚'
- en: '`fp16_statistics` (`torch.HalfTensor`, *optional*) â€” The list of fp16 statistics
    to set on the module, used for 8 bit model serialization.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fp16_statistics` (`torch.HalfTensor`, *å¯é€‰*) â€” è¦è®¾ç½®åœ¨æ¨¡å—ä¸Šçš„fp16ç»Ÿè®¡ä¿¡æ¯åˆ—è¡¨ï¼Œç”¨äº8ä½æ¨¡å‹åºåˆ—åŒ–ã€‚'
- en: '`tied_params_map` (Dict[int, Dict[torch.device, torch.Tensor]], *optional*,
    defaults to `None`) â€” A map of current data pointers to dictionaries of devices
    to already dispatched tied weights. For a given execution device, this parameter
    is useful to reuse the first available pointer of a shared weight on the device
    for all others, instead of duplicating memory.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tied_params_map` (Dict[int, Dict[torch.device, torch.Tensor]], *å¯é€‰*, é»˜è®¤ä¸º`None`)
    â€” å½“å‰æ•°æ®æŒ‡é’ˆåˆ°å·²åˆ†æ´¾çš„ç»‘å®šæƒé‡è®¾å¤‡å­—å…¸çš„æ˜ å°„ã€‚å¯¹äºç»™å®šçš„æ‰§è¡Œè®¾å¤‡ï¼Œæ­¤å‚æ•°å¯¹äºé‡ç”¨è®¾å¤‡ä¸Šå…±äº«æƒé‡çš„ç¬¬ä¸€ä¸ªå¯ç”¨æŒ‡é’ˆå¯¹äºæ‰€æœ‰å…¶ä»–è®¾å¤‡è€Œè¨€æ˜¯æœ‰ç”¨çš„ï¼Œè€Œä¸æ˜¯å¤åˆ¶å†…å­˜ã€‚'
- en: A helper function to set a given tensor (parameter of buffer) of a module on
    a specific device (note that doing `param.to(device)` creates a new tensor not
    linked to the parameter, which is why we need this function).
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºå°†æ¨¡å—çš„ç»™å®šå¼ é‡ï¼ˆå‚æ•°æˆ–ç¼“å†²åŒºï¼‰è®¾ç½®åœ¨ç‰¹å®šè®¾å¤‡ä¸Šï¼ˆè¯·æ³¨æ„ï¼Œæ‰§è¡Œ`param.to(device)`ä¼šåˆ›å»ºä¸€ä¸ªä¸å‚æ•°ä¸ç›¸å…³è”çš„æ–°å¼ é‡ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦è¿™ä¸ªå‡½æ•°ï¼‰ã€‚
- en: '#### `accelerate.utils.shard_checkpoint`'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.shard_checkpoint`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L193)'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L193)'
- en: '[PRE67]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Parameters
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`state_dict` (`Dict[str, torch.Tensor]`) â€” The state dictionary of a model
    to save.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state_dict` (`Dict[str, torch.Tensor]`) â€” è¦ä¿å­˜çš„æ¨¡å‹çš„çŠ¶æ€å­—å…¸ã€‚'
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"10GB"`) â€” The maximum
    size of each sub-checkpoint. If expressed as a string, needs to be digits followed
    by a unit (like `"5MB"`).'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_shard_size` (`int` æˆ– `str`, *å¯é€‰*, é»˜è®¤ä¸º`"10GB"`) â€” æ¯ä¸ªå­æ£€æŸ¥ç‚¹çš„æœ€å¤§å¤§å°ã€‚å¦‚æœä»¥å­—ç¬¦ä¸²å½¢å¼è¡¨ç¤ºï¼Œéœ€è¦æ˜¯æ•°å­—åè·Ÿä¸€ä¸ªå•ä½ï¼ˆå¦‚`"5MB"`ï¼‰ã€‚'
- en: '`weights_name` (`str`, *optional*, defaults to `"pytorch_model.bin"`) â€” The
    name of the model save file.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weights_name` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`"pytorch_model.bin"`) â€” æ¨¡å‹ä¿å­˜æ–‡ä»¶çš„åç§°ã€‚'
- en: Splits a model state dictionary in sub-checkpoints so that the final size of
    each sub-checkpoint does not exceed a given size.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹çŠ¶æ€å­—å…¸æ‹†åˆ†ä¸ºå­æ£€æŸ¥ç‚¹ï¼Œä»¥ä¾¿æ¯ä¸ªå­æ£€æŸ¥ç‚¹çš„æœ€ç»ˆå¤§å°ä¸è¶…è¿‡ç»™å®šå¤§å°ã€‚
- en: The sub-checkpoints are determined by iterating through the `state_dict` in
    the order of its keys, so there is no optimization made to make each sub-checkpoint
    as close as possible to the maximum size passed. For example, if the limit is
    10GB and we have weights of sizes [6GB, 6GB, 2GB, 6GB, 2GB, 2GB] they will get
    sharded as [6GB], [6+2GB], [6+2+2GB] and not [6+2+2GB], [6+2GB], [6GB].
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: å­æ£€æŸ¥ç‚¹æ˜¯é€šè¿‡æŒ‰ç…§å…¶é”®çš„é¡ºåºè¿­ä»£`state_dict`æ¥ç¡®å®šçš„ï¼Œå› æ­¤æ²¡æœ‰ä¼˜åŒ–ä½¿æ¯ä¸ªå­æ£€æŸ¥ç‚¹å°½å¯èƒ½æ¥è¿‘ä¼ é€’çš„æœ€å¤§å¤§å°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœé™åˆ¶ä¸º10GBï¼Œæˆ‘ä»¬æœ‰å¤§å°ä¸º[6GB,
    6GB, 2GB, 6GB, 2GB, 2GB]çš„æƒé‡ï¼Œå®ƒä»¬å°†è¢«åˆ†å‰²ä¸º[6GB]ï¼Œ[6+2GB]ï¼Œ[6+2+2GB]ï¼Œè€Œä¸æ˜¯[6+2+2GB]ï¼Œ[6+2GB]ï¼Œ[6GB]ã€‚
- en: If one of the modelâ€™s weight is bigger that `max_sahrd_size`, it will end up
    in its own sub-checkpoint which will have a size greater than `max_shard_size`.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹çš„æƒé‡ä¹‹ä¸€å¤§äº`max_shard_size`ï¼Œå®ƒå°†æœ€ç»ˆä½äºè‡ªå·±çš„å­æ£€æŸ¥ç‚¹ä¸­ï¼Œå…¶å¤§å°å¤§äº`max_shard_size`ã€‚
- en: Parallel
  id: totrans-465
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¹¶è¡Œ
- en: These include general utilities that should be used when working in parallel.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åŒ…æ‹¬åº”è¯¥åœ¨å¹¶è¡Œå·¥ä½œæ—¶ä½¿ç”¨çš„é€šç”¨å®ç”¨ç¨‹åºã€‚
- en: '#### `accelerate.utils.extract_model_from_parallel`'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.extract_model_from_parallel`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L56)'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L56)'
- en: '[PRE68]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Parameters
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`torch.nn.Module`) â€” The model to extract.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) â€” è¦æå–çš„æ¨¡å‹ã€‚'
- en: '`keep_fp32_wrapper` (`bool`, *optional*) â€” Whether to remove mixed precision
    hooks from the model.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_fp32_wrapper` (`bool`, *å¯é€‰*) â€” æ˜¯å¦ä»æ¨¡å‹ä¸­åˆ é™¤æ··åˆç²¾åº¦é’©å­ã€‚'
- en: Returns
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`torch.nn.Module`'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Module`'
- en: The extracted model.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: æå–çš„æ¨¡å‹ã€‚
- en: Extract a model from its distributed containers.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å…¶åˆ†å¸ƒå¼å®¹å™¨ä¸­æå–æ¨¡å‹ã€‚
- en: '#### `accelerate.utils.save`'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.save`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L156)'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L156)'
- en: '[PRE69]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Parameters
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`save_on_each_node` (`bool`, *optional*, defaults to `False`) â€” Whether to
    only save on the global main process'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_on_each_node` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä»…åœ¨å…¨å±€ä¸»è¿›ç¨‹ä¸Šä¿å­˜'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `False`) â€” Whether to
    save `obj` using `safetensors` or the traditional PyTorch way (that uses `pickle`).'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä½¿ç”¨ `safetensors` æˆ–ä¼ ç»Ÿçš„
    PyTorch æ–¹å¼ï¼ˆä½¿ç”¨ `pickle`ï¼‰ä¿å­˜ `obj`ã€‚'
- en: Save the data to disk. Use in place of `torch.save()`.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ•°æ®ä¿å­˜åˆ°ç£ç›˜ã€‚ç”¨äºæ›¿ä»£ `torch.save()`ã€‚
- en: '#### `accelerate.utils.wait_for_everyone`'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.wait_for_everyone`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L108)'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L108)'
- en: '[PRE70]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Introduces a blocking point in the script, making sure all processes have reached
    this point before continuing.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è„šæœ¬ä¸­å¼•å…¥ä¸€ä¸ªé˜»å¡ç‚¹ï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹åœ¨ç»§ç»­ä¹‹å‰éƒ½å·²åˆ°è¾¾æ­¤ç‚¹ã€‚
- en: Make sure all processes will reach this instruction otherwise one of your processes
    will hang forever.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿æ‰€æœ‰è¿›ç¨‹å°†åˆ°è¾¾æ­¤æŒ‡ä»¤ï¼Œå¦åˆ™å…¶ä¸­ä¸€ä¸ªè¿›ç¨‹å°†æ°¸è¿œæŒ‚èµ·ã€‚
- en: Random
  id: totrans-489
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éšæœº
- en: These utilities relate to setting and synchronizing of all the random states.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å®ç”¨ç¨‹åºæ¶‰åŠè®¾ç½®å’ŒåŒæ­¥æ‰€æœ‰éšæœºçŠ¶æ€ã€‚
- en: '#### `accelerate.utils.set_seed`'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.set_seed`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/random.py#L31)'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/random.py#L31)'
- en: '[PRE71]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Parameters
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`seed` (`int`) â€” The seed to set.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed` (`int`) â€” è¦è®¾ç½®çš„ç§å­ã€‚'
- en: '`device_specific` (`bool`, *optional*, defaults to `False`) â€” Whether to differ
    the seed on each device slightly with `self.process_index`.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_specific` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ç¨å¾®åœ¨æ¯ä¸ªè®¾å¤‡ä¸Šä½¿ç”¨ `self.process_index`
    ä¸åŒçš„ç§å­ã€‚'
- en: Helper function for reproducible behavior to set the seed in `random`, `numpy`,
    `torch`.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºåœ¨ `random`ã€`numpy`ã€`torch` ä¸­è®¾ç½®ç§å­ä»¥è·å¾—å¯é‡ç°è¡Œä¸ºçš„è¾…åŠ©å‡½æ•°ã€‚
- en: '#### `accelerate.utils.synchronize_rng_state`'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.synchronize_rng_state`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/random.py#L57)'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/random.py#L57)'
- en: '[PRE72]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '#### `accelerate.synchronize_rng_states`'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.synchronize_rng_states`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/random.py#L109)'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/random.py#L109)'
- en: '[PRE73]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: PyTorch XLA
  id: totrans-504
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch XLA
- en: These include utilities that are useful while using PyTorch with XLA.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åŒ…æ‹¬åœ¨ä½¿ç”¨ PyTorch ä¸ XLA æ—¶æœ‰ç”¨çš„å®ç”¨ç¨‹åºã€‚
- en: '#### `accelerate.utils.install_xla`'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.install_xla`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/torch_xla.py#L20)'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/torch_xla.py#L20)'
- en: '[PRE74]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Parameters
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`upgrade` (`bool`, *optional*, defaults to `False`) â€” Whether to upgrade `torch`
    and install the latest `torch_xla` wheels.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`upgrade` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦å‡çº§ `torch` å¹¶å®‰è£…æœ€æ–°çš„ `torch_xla` è½®å­ã€‚'
- en: Helper function to install appropriate xla wheels based on the `torch` version
    in Google Colaboratory.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Google Colaboratory ä¸­æ ¹æ® `torch` ç‰ˆæœ¬å®‰è£…é€‚å½“çš„ xla è½®å­çš„è¾…åŠ©å‡½æ•°ã€‚
- en: 'Example:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE75]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Loading model weights
  id: totrans-514
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½æ¨¡å‹æƒé‡
- en: These include utilities that are useful to load checkpoints.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åŒ…æ‹¬æœ‰ç”¨äºåŠ è½½æ£€æŸ¥ç‚¹çš„å®ç”¨ç¨‹åºã€‚
- en: '#### `accelerate.load_checkpoint_in_model`'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.load_checkpoint_in_model`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1442)'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1442)'
- en: '[PRE76]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Parameters
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`torch.nn.Module`) â€” The model in which we want to load a checkpoint.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) â€” æˆ‘ä»¬è¦åŠ è½½æ£€æŸ¥ç‚¹çš„æ¨¡å‹ã€‚'
- en: '`checkpoint` (`str` or `os.PathLike`) â€” The folder checkpoint to load. It can
    be:'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`checkpoint` (`str` æˆ– `os.PathLike`) â€” è¦åŠ è½½çš„æ–‡ä»¶å¤¹æ£€æŸ¥ç‚¹ã€‚å¯ä»¥æ˜¯ï¼š'
- en: a path to a file containing a whole model state dict
  id: totrans-522
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒ…å«æ•´ä¸ªæ¨¡å‹çŠ¶æ€å­—å…¸çš„æ–‡ä»¶è·¯å¾„
- en: a path to a `.json` file containing the index to a sharded checkpoint
  id: totrans-523
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒ…å«æŒ‡å‘åˆ†ç‰‡æ£€æŸ¥ç‚¹ç´¢å¼•çš„`.json`æ–‡ä»¶çš„è·¯å¾„
- en: a path to a folder containing a unique `.index.json` file and the shards of
    a checkpoint.
  id: totrans-524
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒ…å«å”¯ä¸€çš„ `.index.json` æ–‡ä»¶å’Œæ£€æŸ¥ç‚¹åˆ†ç‰‡çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
- en: a path to a folder containing a unique pytorch_model.bin or a model.safetensors
    file.
  id: totrans-525
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒ…å«å”¯ä¸€çš„ pytorch_model.bin æˆ– model.safetensors æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
- en: '`device_map` (`Dict[str, Union[int, str, torch.device]]`, *optional*) â€” A map
    that specifies where each submodule should go. It doesnâ€™t need to be refined to
    each parameter/buffer name, once a given module name is inside, every submodule
    of it will be sent to the same device.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_map` (`Dict[str, Union[int, str, torch.device]]`, *å¯é€‰*) â€” æŒ‡å®šæ¯ä¸ªå­æ¨¡å—åº”æ”¾ç½®åœ¨ä½•å¤„çš„æ˜ å°„ã€‚å®ƒä¸éœ€è¦è¢«ç»†åŒ–åˆ°æ¯ä¸ªå‚æ•°/ç¼“å†²åŒºåç§°ï¼Œä¸€æ—¦ç»™å®šæ¨¡å—åç§°åœ¨å†…éƒ¨ï¼Œå®ƒçš„æ¯ä¸ªå­æ¨¡å—éƒ½å°†è¢«å‘é€åˆ°ç›¸åŒçš„è®¾å¤‡ã€‚'
- en: '`offload_folder` (`str` or `os.PathLike`, *optional*) â€” If the `device_map`
    contains any value `"disk"`, the folder where we will offload weights.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`offload_folder` (`str` æˆ– `os.PathLike`, *å¯é€‰*) â€” å¦‚æœ `device_map` åŒ…å«ä»»ä½•å€¼ `"disk"`ï¼Œåˆ™æˆ‘ä»¬å°†å¸è½½æƒé‡çš„æ–‡ä»¶å¤¹ã€‚'
- en: '`dtype` (`str` or `torch.dtype`, *optional*) â€” If provided, the weights will
    be converted to that type when loaded.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype` (`str` æˆ– `torch.dtype`, *å¯é€‰*) â€” å¦‚æœæä¾›ï¼ŒåŠ è½½æ—¶å°†å°†æƒé‡è½¬æ¢ä¸ºè¯¥ç±»å‹ã€‚'
- en: '`offload_state_dict` (`bool`, *optional*, defaults to `False`) â€” If `True`,
    will temporarily offload the CPU state dict on the hard drive to avoid getting
    out of CPU RAM if the weight of the CPU state dict + the biggest shard does not
    fit.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`offload_state_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” å¦‚æœä¸º `True`ï¼Œå°†ä¸´æ—¶å°† CPU çŠ¶æ€å­—å…¸è½¬ç§»åˆ°ç¡¬ç›˜ä¸Šï¼Œä»¥é¿å…å¦‚æœ
    CPU çŠ¶æ€å­—å…¸çš„é‡é‡ + æœ€å¤§åˆ†ç‰‡çš„é‡é‡ä¸é€‚åˆ CPU RAMã€‚'
- en: '`offload_buffers` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to include the buffers in the weights offloaded to disk.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`offload_buffers` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦å°†ç¼“å†²åŒºåŒ…å«åœ¨å¸è½½åˆ°ç£ç›˜çš„æƒé‡ä¸­ã€‚'
- en: '`keep_in_fp32_modules(List[str],` *optional*) â€” A list of the modules that
    we keep in `torch.float32` dtype.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_in_fp32_modules(List[str],` *å¯é€‰*) â€” æˆ‘ä»¬ä¿ç•™åœ¨ `torch.float32` æ•°æ®ç±»å‹ä¸­çš„æ¨¡å—åˆ—è¡¨ã€‚'
- en: '`offload_8bit_bnb` (`bool`, *optional*) â€” Whether or not to enable offload
    of 8-bit modules on cpu/disk.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`offload_8bit_bnb` (`bool`, *å¯é€‰*) â€” æ˜¯å¦å¯ç”¨åœ¨cpu/ç£ç›˜ä¸Šå¸è½½8ä½æ¨¡å—ã€‚'
- en: Loads a (potentially sharded) checkpoint inside a model, potentially sending
    weights to a given device as they are loaded.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½ï¼ˆå¯èƒ½æ˜¯åˆ†ç‰‡çš„ï¼‰æ£€æŸ¥ç‚¹åˆ°æ¨¡å‹ä¸­ï¼Œå¯èƒ½åœ¨åŠ è½½æ—¶å°†æƒé‡å‘é€åˆ°ç»™å®šè®¾å¤‡ã€‚
- en: Once loaded across devices, you still need to call [dispatch_model()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.dispatch_model)
    on your model to make it able to run. To group the checkpoint loading and dispatch
    in one single call, use [load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch).
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦åœ¨è®¾å¤‡é—´åŠ è½½å®Œæˆï¼Œæ‚¨ä»ç„¶éœ€è¦è°ƒç”¨[dispatch_model()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.dispatch_model)æ¥ä½¿æ¨¡å‹èƒ½å¤Ÿè¿è¡Œã€‚ä¸ºäº†åœ¨ä¸€ä¸ªå•ä¸€è°ƒç”¨ä¸­ç»„åˆæ£€æŸ¥ç‚¹åŠ è½½å’Œåˆ†å‘ï¼Œå¯ä»¥ä½¿ç”¨[load_checkpoint_and_dispatch()](/docs/accelerate/v0.27.2/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch)ã€‚
- en: Quantization
  id: totrans-535
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡åŒ–
- en: These include utilities that are useful to quantize model.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åŒ…æ‹¬å¯¹é‡åŒ–æ¨¡å‹æœ‰ç”¨çš„å®ç”¨ç¨‹åºã€‚
- en: '#### `accelerate.utils.load_and_quantize_model`'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `accelerate.utils.load_and_quantize_model`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/bnb.py#L44)'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/bnb.py#L44)'
- en: '[PRE77]'
  id: totrans-539
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Parameters
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model` (`torch.nn.Module`) â€” Input model. The model can be already loaded
    or on the meta device'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) â€” è¾“å…¥æ¨¡å‹ã€‚è¯¥æ¨¡å‹å¯ä»¥å·²ç»åŠ è½½æˆ–åœ¨å…ƒè®¾å¤‡ä¸Š'
- en: '`bnb_quantization_config` (`BnbQuantizationConfig`) â€” The bitsandbytes quantization
    parameters'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bnb_quantization_config` (`BnbQuantizationConfig`) â€” ä½å’Œå­—èŠ‚é‡åŒ–å‚æ•°'
- en: '`weights_location` (`str` or `os.PathLike`) â€” The folder weights_location to
    load. It can be:'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weights_location` (`str` or `os.PathLike`) â€” è¦åŠ è½½çš„æƒé‡æ–‡ä»¶å¤¹ã€‚å¯ä»¥æ˜¯ï¼š'
- en: a path to a file containing a whole model state dict
  id: totrans-544
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«æ•´ä¸ªæ¨¡å‹çŠ¶æ€å­—å…¸çš„æ–‡ä»¶è·¯å¾„
- en: a path to a `.json` file containing the index to a sharded checkpoint
  id: totrans-545
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«åˆ†ç‰‡æ£€æŸ¥ç‚¹ç´¢å¼•çš„ `.json` æ–‡ä»¶è·¯å¾„
- en: a path to a folder containing a unique `.index.json` file and the shards of
    a checkpoint.
  id: totrans-546
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«å”¯ä¸€çš„ `.index.json` æ–‡ä»¶å’Œæ£€æŸ¥ç‚¹åˆ†ç‰‡çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
- en: a path to a folder containing a unique pytorch_model.bin file.
  id: totrans-547
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«å”¯ä¸€çš„ pytorch_model.bin æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
- en: '`device_map` (`Dict[str, Union[int, str, torch.device]]`, *optional*) â€” A map
    that specifies where each submodule should go. It doesnâ€™t need to be refined to
    each parameter/buffer name, once a given module name is inside, every submodule
    of it will be sent to the same device.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_map` (`Dict[str, Union[int, str, torch.device]]`, *å¯é€‰*) â€” æŒ‡å®šæ¯ä¸ªå­æ¨¡å—åº”è¯¥å»å“ªé‡Œçš„æ˜ å°„ã€‚å®ƒä¸éœ€è¦è¢«ç»†åŒ–åˆ°æ¯ä¸ªå‚æ•°/ç¼“å†²åŒºåç§°ï¼Œä¸€æ—¦ç»™å®šæ¨¡å—åç§°åœ¨å†…ï¼Œå®ƒçš„æ¯ä¸ªå­æ¨¡å—éƒ½å°†è¢«å‘é€åˆ°ç›¸åŒçš„è®¾å¤‡ã€‚'
- en: '`no_split_module_classes` (`List[str]`, *optional*) â€” A list of layer class
    names that should never be split across device (for instance any layer that has
    a residual connection).'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no_split_module_classes` (`List[str]`, *å¯é€‰*) â€” ä¸åº”è¯¥è·¨è®¾å¤‡åˆ†å‰²çš„å±‚ç±»åç§°åˆ—è¡¨ï¼ˆä¾‹å¦‚å…·æœ‰æ®‹å·®è¿æ¥çš„ä»»ä½•å±‚ï¼‰ã€‚'
- en: '`max_memory` (`Dict`, *optional*) â€” A dictionary device identifier to maximum
    memory. Will default to the maximum memory available if unset.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_memory` (`Dict`, *å¯é€‰*) â€” è®¾å¤‡æ ‡è¯†ç¬¦åˆ°æœ€å¤§å†…å­˜çš„å­—å…¸ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†é»˜è®¤ä¸ºå¯ç”¨çš„æœ€å¤§å†…å­˜ã€‚'
- en: '`offload_folder` (`str` or `os.PathLike`, *optional*) â€” If the `device_map`
    contains any value `"disk"`, the folder where we will offload weights.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`offload_folder` (`str` or `os.PathLike`, *å¯é€‰*) â€” å¦‚æœ `device_map` åŒ…å«ä»»ä½•å€¼ä¸º `"disk"`ï¼Œåˆ™æˆ‘ä»¬å°†å¸è½½æƒé‡çš„æ–‡ä»¶å¤¹ã€‚'
- en: '`offload_state_dict` (`bool`, *optional*, defaults to `False`) â€” If `True`,
    will temporarily offload the CPU state dict on the hard drive to avoid getting
    out of CPU RAM if the weight of the CPU state dict + the biggest shard does not
    fit.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`offload_state_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” å¦‚æœä¸º `True`ï¼Œå°†ä¸´æ—¶å°†CPUçŠ¶æ€å­—å…¸å¸è½½åˆ°ç¡¬ç›˜ä¸Šï¼Œä»¥é¿å…å¦‚æœCPUçŠ¶æ€å­—å…¸çš„é‡é‡
    + æœ€å¤§åˆ†ç‰‡çš„é‡é‡ä¸é€‚åˆæ—¶ä¼šè¶…å‡ºCPU RAMã€‚'
- en: Returns
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`torch.nn.Module`'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Module`'
- en: The quantized model
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: é‡åŒ–åçš„æ¨¡å‹
- en: This function will quantize the input model with the associated config passed
    in `bnb_quantization_config`. If the model is in the meta device, we will load
    and dispatch the weights according to the `device_map` passed. If the model is
    already loaded, we will quantize the model and put the model on the GPU,
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å‡½æ•°å°†ä½¿ç”¨ä¼ é€’ç»™ `bnb_quantization_config` çš„ç›¸å…³é…ç½®å¯¹è¾“å…¥æ¨¡å‹è¿›è¡Œé‡åŒ–ã€‚å¦‚æœæ¨¡å‹åœ¨å…ƒè®¾å¤‡ä¸Šï¼Œæˆ‘ä»¬å°†æ ¹æ®ä¼ é€’çš„ `device_map`
    åŠ è½½å’Œåˆ†å‘æƒé‡ã€‚å¦‚æœæ¨¡å‹å·²ç»åŠ è½½ï¼Œæˆ‘ä»¬å°†é‡åŒ–æ¨¡å‹å¹¶å°†æ¨¡å‹æ”¾åœ¨GPUä¸Šï¼Œ
