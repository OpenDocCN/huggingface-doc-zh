["```py\ngit clone https://github.com/persimmon-ai-labs/adept-inference\nwget path/to/fuyu-8b-model-weights.tar\ntar -xvf fuyu-8b-model-weights.tar\npython src/transformers/models/fuyu/convert_fuyu_weights_to_hf.py  --input_dir /path/to/downloaded/fuyu/weights/ --output_dir /output/path \\\n    --pt_model_path /path/to/fuyu_8b_release/iter_0001251/mp_rank_00/model_optim_rng.pt\n    --ada_lib_path /path/to/adept-inference\n```", "```py\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_chat_model_release.tar\ntar -xvf 8b_base_model_release.tar\n```", "```py\nfrom transformers import FuyuConfig, FuyuForCausalLM\nmodel_config = FuyuConfig()\nmodel = FuyuForCausalLM(model_config).from_pretrained('/output/path')\n```", "```py\nfrom PIL import Image\nfrom transformers import AutoTokenizer\nfrom transformers.models.fuyu.processing_fuyu import FuyuProcessor\nfrom transformers.models.fuyu.image_processing_fuyu import FuyuImageProcessor\n\ntokenizer = AutoTokenizer.from_pretrained('adept-hf-collab/fuyu-8b')\nimage_processor = FuyuImageProcessor()\n\nprocessor = FuyuProcessor(image_processor=image_processor, tokenizer=tokenizer)\ntext_prompt = \"Generate a coco-style caption.\\\\n\"\n\nbus_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bus.png\"\nbus_image_pil = Image.open(io.BytesIO(requests.get(bus_image_url).content))\ninputs_to_model = processor(text=text_prompt, images=image_pil)\n\n```", "```pyAut`.\n\nThis is the configuration class to store the configuration of a [FuyuForCausalLM](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuForCausalLM). It is used to instantiate an Fuyu model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the [adept/fuyu-8b](https://huggingface.co/adept/fuyu-8b).\n\nConfiguration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) for more information.\n\n```", "```py\n\n## FuyuForCausalLM\n\n ### class transformers.FuyuForCausalLM\n\n[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/modeling_fuyu.py#L143)\n\n( config: FuyuConfig )\n\nParameters\n\n*   **config** ([FuyuConfig](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuConfig)) \u2014 Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) method to load the model weights.\n\nFuyu Model with a language modeling head on top for causal language model conditioned on image patches and text. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel). Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\n\nThis model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.\n\n #### forward\n\n[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/modeling_fuyu.py#L210)\n\n( input_ids: LongTensor = None image_patches: Tensor = None image_patches_indices: Tensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None use_cache: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 [transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast) or `tuple(torch.FloatTensor)`\n\nParameters\n\n*   **input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`) \u2014 Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide it.\n\n    Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer). See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__) for details.\n\n    [What are input IDs?](../glossary#input-ids) \n*   **attention_mask** (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*) \u2014 Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n    *   1 for tokens that are **not masked**,\n    *   0 for tokens that are **masked**.\n\n    [What are attention masks?](../glossary#attention-mask)\n\n    Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer). See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__) for details.\n\n    If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n\n    If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask` and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.\n\n    *   1 indicates the head is **not masked**,\n    *   0 indicates the head is **masked**. \n*   **image_patches** (`torch.FloatTensor` of shape `(batch_size, num_total_patches, patch_size_ x patch_size x num_channels)`, *optional*) \u2014 Image patches to be used as continuous embeddings. The patches are flattened and then projected to the hidden size of the model.\n*   **image_patches_indices** (`torch.LongTensor` of shape `(batch_size, num_total_patches + number_of_newline_tokens + number_of_text_tokens, patch_size_ x patch_size x num_channels )`, *optional*) \u2014 Indices indicating at which position the image_patches have to be inserted in input_embeds.\n*   **position_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) \u2014 Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0, config.n_positions - 1]`.\n\n    [What are position IDs?](../glossary#position-ids) \n*   **past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) \u2014 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\n    Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n    If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that don\u2019t have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`. \n*   **inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) \u2014 Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This is useful if you want more control over how to convert `input_ids` indices into associated vectors than the model\u2019s internal embedding lookup matrix.\n*   **use_cache** (`bool`, *optional*) \u2014 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see `past_key_values`).\n*   **output_attentions** (`bool`, *optional*) \u2014 Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned tensors for more detail.\n*   **output_hidden_states** (`bool`, *optional*) \u2014 Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for more detail.\n*   **return_dict** (`bool`, *optional*) \u2014 Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) instead of a plain tuple.\n*   **labels** (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) \u2014 Labels for computing the masked language modeling loss. Indices should either be in `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\nReturns\n\n[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast) or `tuple(torch.FloatTensor)`\n\nA [transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast) or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various elements depending on the configuration ([FuyuConfig](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuConfig)) and inputs.\n\n*   **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) \u2014 Language modeling loss (for next-token prediction).\n\n*   **logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n\n*   **past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) \u2014 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n\n    Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n*   **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) \u2014 Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, + one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n\n    Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n\n*   **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) \u2014 Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.\n\n    Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\nThe [FuyuForCausalLM](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuForCausalLM) forward method, overrides the `__call__` special method.\n\nAlthough the recipe for forward pass needs to be defined within this function, one should call the `Module` instance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\n\nExamples:\n\n```"]