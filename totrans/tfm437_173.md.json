["```py\ngit clone https://github.com/persimmon-ai-labs/adept-inference\nwget path/to/fuyu-8b-model-weights.tar\ntar -xvf fuyu-8b-model-weights.tar\npython src/transformers/models/fuyu/convert_fuyu_weights_to_hf.py  --input_dir /path/to/downloaded/fuyu/weights/ --output_dir /output/path \\\n    --pt_model_path /path/to/fuyu_8b_release/iter_0001251/mp_rank_00/model_optim_rng.pt\n    --ada_lib_path /path/to/adept-inference\n```", "```py\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_chat_model_release.tar\ntar -xvf 8b_base_model_release.tar\n```", "```py\nfrom transformers import FuyuConfig, FuyuForCausalLM\nmodel_config = FuyuConfig()\nmodel = FuyuForCausalLM(model_config).from_pretrained('/output/path')\n```", "```py\nfrom PIL import Image\nfrom transformers import AutoTokenizer\nfrom transformers.models.fuyu.processing_fuyu import FuyuProcessor\nfrom transformers.models.fuyu.image_processing_fuyu import FuyuImageProcessor\n\ntokenizer = AutoTokenizer.from_pretrained('adept-hf-collab/fuyu-8b')\nimage_processor = FuyuImageProcessor()\n\nprocessor = FuyuProcessor(image_processor=image_processor, tokenizer=tokenizer)\ntext_prompt = \"Generate a coco-style caption.\\\\n\"\n\nbus_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bus.png\"\nbus_image_pil = Image.open(io.BytesIO(requests.get(bus_image_url).content))\ninputs_to_model = processor(text=text_prompt, images=image_pil)\n\n```", "```py\n( vocab_size = 262144 hidden_size = 4096 intermediate_size = 16384 num_hidden_layers = 36 num_attention_heads = 64 hidden_act = 'relu2' max_position_embeddings = 16384 image_size = 300 patch_size = 30 num_channels = 3 initializer_range = 0.02 layer_norm_eps = 1e-05 use_cache = True tie_word_embeddings = False rope_theta = 25000.0 rope_scaling = None qk_layernorm = True hidden_dropout = 0.0 attention_dropout = 0.0 partial_rotary_factor = 0.5 pad_token_id = None bos_token_id = 1 eos_token_id = 2 text_config = None **kwargs )\n```", "```pyAut`.\n\nThis is the configuration class to store the configuration of a [FuyuForCausalLM](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuForCausalLM). It is used to instantiate an Fuyu model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the [adept/fuyu-8b](https://huggingface.co/adept/fuyu-8b).\n\nConfiguration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) for more information.\n\n```", "```py\n\n## FuyuForCausalLM\n\n ### `class transformers.FuyuForCausalLM`\n\n[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/modeling_fuyu.py#L143)\n\n```", "```py\n\nParameters\n\n*   `config` ([FuyuConfig](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuConfig)) \u2014 Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) method to load the model weights.\n\nFuyu Model with a language modeling head on top for causal language model conditioned on image patches and text. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel). Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\n\nThis model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.\n\n #### `forward`\n\n[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/modeling_fuyu.py#L210)\n\n```", "```py\n\nParameters\n\n*   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) \u2014 Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide it.\n\n    Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer). See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__) for details.\n\n    [What are input IDs?](../glossary#input-ids) \n*   `attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*) \u2014 Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n    *   1 for tokens that are `not masked`,\n    *   0 for tokens that are `masked`.\n\n    [What are attention masks?](../glossary#attention-mask)\n\n    Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer). See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__) for details.\n\n    If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n\n    If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask` and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.\n\n    *   1 indicates the head is `not masked`,\n    *   0 indicates the head is `masked`. \n*   `image_patches` (`torch.FloatTensor` of shape `(batch_size, num_total_patches, patch_size_ x patch_size x num_channels)`, *optional*) \u2014 Image patches to be used as continuous embeddings. The patches are flattened and then projected to the hidden size of the model.\n*   `image_patches_indices` (`torch.LongTensor` of shape `(batch_size, num_total_patches + number_of_newline_tokens + number_of_text_tokens, patch_size_ x patch_size x num_channels )`, *optional*) \u2014 Indices indicating at which position the image_patches have to be inserted in input_embeds.\n*   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) \u2014 Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0, config.n_positions - 1]`.\n\n    [What are position IDs?](../glossary#position-ids) \n*   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) \u2014 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\n    Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n    If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that don\u2019t have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`. \n*   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) \u2014 Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This is useful if you want more control over how to convert `input_ids` indices into associated vectors than the model\u2019s internal embedding lookup matrix.\n*   `use_cache` (`bool`, *optional*) \u2014 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see `past_key_values`).\n*   `output_attentions` (`bool`, *optional*) \u2014 Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned tensors for more detail.\n*   `output_hidden_states` (`bool`, *optional*) \u2014 Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for more detail.\n*   `return_dict` (`bool`, *optional*) \u2014 Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) instead of a plain tuple.\n*   `labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) \u2014 Labels for computing the masked language modeling loss. Indices should either be in `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\nReturns\n\n[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast) or `tuple(torch.FloatTensor)`\n\nA [transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast) or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various elements depending on the configuration ([FuyuConfig](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuConfig)) and inputs.\n\n*   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) \u2014 Language modeling loss (for next-token prediction).\n\n*   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n\n*   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) \u2014 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n\n    Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n*   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) \u2014 Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, + one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n\n    Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n\n*   `attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) \u2014 Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.\n\n    Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\nThe [FuyuForCausalLM](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuForCausalLM) forward method, overrides the `__call__` special method.\n\nAlthough the recipe for forward pass needs to be defined within this function, one should call the `Module` instance afterwards instead of this since the former takes care of running the pre and post processing steps while the latter silently ignores them.\n\nExamples:\n\n```", "```py\n\n## FuyuImageProcessor\n\n ### `class transformers.FuyuImageProcessor`\n\n[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/image_processing_fuyu.py#L180)\n\n```", "```py\n\nParameters\n\n*   `do_resize` (`bool`, *optional*, defaults to `True`) \u2014 Whether to resize the image to `size`.\n*   `size` (`Dict[str, int]`, *optional*, defaults to `{\"height\" -- 1080, \"width\": 1920}`): Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n*   `resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`) \u2014 `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BILINEAR`.\n*   `do_pad` (`bool`, *optional*, defaults to `True`) \u2014 Whether to pad the image to `size`.\n*   `padding_value` (`float`, *optional*, defaults to 1.0) \u2014 The value to pad the image with.\n*   `padding_mode` (`str`, *optional*, defaults to `\"constant\"`) \u2014 The padding mode to use when padding the image.\n*   `do_normalize` (`bool`, *optional*, defaults to `True`) \u2014 Whether to normalize the image.\n*   `image_mean` (`float`, *optional*, defaults to 0.5) \u2014 The mean to use when normalizing the image.\n*   `image_std` (`float`, *optional*, defaults to 0.5) \u2014 The standard deviation to use when normalizing the image.\n*   `do_rescale` (`bool`, *optional*, defaults to `True`) \u2014 Whether to rescale the image.\n*   `rescale_factor` (`float`, *optional*, defaults to `1 / 255`) \u2014 The factor to use when rescaling the image.\n*   `patch_size` (`Dict[str, int]`, *optional*, defaults to `{\"height\" -- 30, \"width\": 30}`): Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the patches.\n\nThis class should handle the image processing part before the main FuyuForCausalLM. In particular, it should handle:\n\n*   Processing Images: Taking a batch of images as input. If the images are variable-sized, it resizes them based on the desired patch dimensions. The image output is always img_h, img_w of (1080, 1920)\n\n    Then, it patches up these images using the patchify_image function.\n\n*   Creating Image Input IDs: For each patch, a placeholder ID is given to identify where these patches belong in a token sequence. For variable-sized images, each line of patches is terminated with a newline ID.\n\n*   Image Patch Indices: For each image patch, the code maintains an index where these patches should be inserted in a token stream.\n\n #### `__call__`\n\n[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)\n\n```", "```py\n\nPreprocess an image or a batch of images.\n\n## FuyuProcessor\n\n ### `class transformers.FuyuProcessor`\n\n[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/processing_fuyu.py#L309)\n\n```", "```py\n\nParameters\n\n*   `image_processor` ([FuyuImageProcessor](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuImageProcessor)) \u2014 The image processor is a required input.\n*   `tokenizer` ([LlamaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizerFast)) \u2014 The tokenizer is a required input.\n\nConstructs a Fuyu processor which wraps a Fuyu image processor and a Llama tokenizer into a single processor.\n\n[FuyuProcessor](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuProcessor) offers all the functionalities of [FuyuImageProcessor](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuImageProcessor) and [LlamaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizerFast). See the [**call**()](/docs/transformers/v4.37.2/en/model_doc/fuyu#transformers.FuyuProcessor.__call__) and `decode()` for more information.\n\n #### `__call__`\n\n[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/fuyu/processing_fuyu.py#L451)\n\n```"]