["```py\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\n```", "```py\nmodel, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n    model, optimizer, train_dataloader, lr_scheduler\n)\n```", "```py\nvalidation_dataloader = accelerator.prepare(validation_dataloader)\n```", "```py\nfor inputs, targets in validation_dataloader:\n    predictions = model(inputs)\n    # Gather all predictions and targets\n    all_predictions, all_targets = accelerator.gather_for_metrics((predictions, targets))\n    # Example of use with a *Datasets.Metric*\n    metric.add_batch(all_predictions, all_targets)\n```", "```py\naccelerate config\n```", "```py\naccelerate test\n```", "```py\naccelerate test --config_file path_to_config.yaml\n```", "```py\naccelerate launch path_to_script.py --args_for_the_script\n```", "```py\naccelerate launch --config_file path_to_config.yaml path_to_script.py --args_for_the_script\n```", "```py\nfrom accelerate import notebook_launcher\n\nnotebook_launcher(training_function)\n```", "```py\nfrom accelerate import DistributedType\n\nif accelerator.distributed_type == DistributedType.TPU:\n    # do something of static shape\nelse:\n    # go crazy and be dynamic\n```", "```py\nif accelerator.is_local_main_process:\n    # Is executed once per server\n```", "```py\nfrom tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n```", "```py\nif accelerator.is_main_process:\n    # Is executed once only\n```", "```py\naccelerator.wait_for_everyone()\n```", "```py\naccelerator.wait_for_everyone()\naccelerator.save_model(model, save_directory)\n```", "```py\naccelerator.wait_for_everyone()\naccelerator.save_model(model, save_directory, max_shard_size=\"1GB\", safe_serialization=True)\n```", "```py\nunwrapped_model = accelerator.unwrap_model(model)\npath_to_checkpoint = os.path.join(save_directory,\"pytorch_model.bin\")\nunwrapped_model.load_state_dict(torch.load(path_to_checkpoint))\n```", "```py\nload_checkpoint_in_model(unwrapped_model, save_directory, device_map={\"\":device})\n```", "```py\nwith accelerator.autocast():\n    loss = complex_loss_function(outputs, target):\n```", "```py\nif not accelerator.optimizer_step_was_skipped:\n    lr_scheduler.step()\n```", "```py\naccelerator = Accelerator(gradient_accumulation_steps=2)\nmodel, optimizer, training_dataloader = accelerator.prepare(model, optimizer, training_dataloader)\n\nfor input, label in training_dataloader:\n    with accelerator.accumulate(model):\n        predictions = model(input)\n        loss = loss_function(predictions, label)\n        accelerator.backward(loss)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n```"]