- en: Logging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志记录
- en: 'Original text: [https://huggingface.co/docs/trl/logging](https://huggingface.co/docs/trl/logging)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/trl/logging](https://huggingface.co/docs/trl/logging)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: As reinforcement learning algorithms are historically challenging to debug,
    it’s important to pay careful attention to logging. By default, the TRL [PPOTrainer](/docs/trl/v0.7.10/en/trainer#trl.PPOTrainer)
    saves a lot of relevant information to `wandb` or `tensorboard`.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于强化学习算法在历史上很难调试，因此重要的是要仔细关注日志记录。默认情况下，TRL [PPOTrainer](/docs/trl/v0.7.10/en/trainer#trl.PPOTrainer)会将大量相关信息保存到`wandb`或`tensorboard`中。
- en: 'Upon initialization, pass one of these two options to the [PPOConfig](/docs/trl/v0.7.10/en/trainer#trl.PPOConfig):'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化时，将以下两个选项之一传递给[PPOConfig](/docs/trl/v0.7.10/en/trainer#trl.PPOConfig)：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you want to log with tensorboard, add the kwarg `project_kwargs={"logging_dir":
    PATH_TO_LOGS}` to the PPOConfig.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '如果要使用tensorboard进行日志记录，请将`project_kwargs={"logging_dir": PATH_TO_LOGS}`添加到PPOConfig中。'
- en: PPO Logging
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PPO日志记录
- en: 'Here’s a brief explanation for the logged metrics provided in the data:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是提供的数据中记录的指标的简要解释：
- en: 'Key metrics to monitor. We want to maximize the reward, maintain a low KL divergence,
    and maximize entropy:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要监视的关键指标。我们希望最大化奖励，保持低KL散度，并最大化熵：
- en: '`env/reward_mean`: The average reward obtained from the environment. Alias
    `ppo/mean_scores`, which is sed to specifically monitor the reward model.'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`env/reward_mean`: 从环境中获得的平均奖励。别名`ppo/mean_scores''，用于专门监视奖励模型。'
- en: '`env/reward_std`: The standard deviation of the reward obtained from the environment.
    Alias ``ppo/std_scores`, which is sed to specifically monitor the reward model.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`env/reward_std`: 环境获得奖励的标准差。别名``ppo/std_scores''，用于专门监视奖励模型。'
- en: '`env/reward_dist`: The histogram distribution of the reward obtained from the
    environment.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`env/reward_dist`: 从环境中获得的奖励的直方图分布。'
- en: '`objective/kl`: The mean Kullback-Leibler (KL) divergence between the old and
    new policies. It measures how much the new policy deviates from the old policy.
    The KL divergence is used to compute the KL penalty in the objective function.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`objective/kl`: 旧政策和新政策之间的平均Kullback-Leibler（KL）散度。它衡量新政策偏离旧政策的程度。KL散度用于计算目标函数中的KL惩罚。'
- en: '`objective/kl_dist`: The histogram distribution of the `objective/kl`.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`objective/kl_dist`: `objective/kl`的直方图分布。'
- en: '`objective/kl_coef`: The coefficient for Kullback-Leibler (KL) divergence in
    the objective function.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`objective/kl_coef`: 目标函数中Kullback-Leibler（KL）散度的系数。'
- en: '`ppo/mean_non_score_reward`: The **KL penalty** calculated by `objective/kl
    * objective/kl_coef` as the total reward for optimization to prevent the new policy
    from deviating too far from the old policy.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/mean_non_score_reward`: 由`objective/kl * objective/kl_coef`计算的**KL惩罚**，作为优化的总奖励，以防止新政策偏离太远旧政策。'
- en: '`objective/entropy`: The entropy of the model’s policy, calculated by `-logprobs.sum(-1).mean()`.
    High entropy means the model’s actions are more random, which can be beneficial
    for exploration.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`objective/entropy`: 模型策略的熵，通过`-logprobs.sum(-1).mean()`计算。高熵意味着模型的动作更随机，这对于探索可能是有益的。'
- en: 'Training stats:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 训练统计：
- en: '`ppo/learning_rate`: The learning rate for the PPO algorithm.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/learning_rate`: PPO算法的学习率。'
- en: '`ppo/policy/entropy`: The entropy of the model’s policy, calculated by `pd
    = torch.nn.functional.softmax(logits, dim=-1); entropy = torch.logsumexp(logits,
    dim=-1) - torch.sum(pd * logits, dim=-1)`. It measures the randomness of the policy.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/policy/entropy`: 模型策略的熵，通过`pd = torch.nn.functional.softmax(logits, dim=-1);
    entropy = torch.logsumexp(logits, dim=-1) - torch.sum(pd * logits, dim=-1)`计算。它衡量策略的随机性。'
- en: '`ppo/policy/clipfrac`: The fraction of probability ratios (old policy / new
    policy) that fell outside the clipping range in the PPO objective. This can be
    used to monitor the optimization process.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/policy/clipfrac`: 在PPO目标中，概率比率（旧政策/新政策）超出剪切范围的比例。这可以用来监视优化过程。'
- en: '`ppo/policy/approxkl`: The approximate KL divergence between the old and new
    policies, measured by `0.5 * masked_mean((logprobs - old_logprobs) ** 2, mask)`,
    corresponding to the `k2` estimator in [http://joschu.net/blog/kl-approx.html](http://joschu.net/blog/kl-approx.html)'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/policy/approxkl`: 旧政策和新政策之间的近似KL散度，由`0.5 * masked_mean((logprobs - old_logprobs)
    ** 2, mask)`测量，对应于[http://joschu.net/blog/kl-approx.html](http://joschu.net/blog/kl-approx.html)中的`k2`估计器。'
- en: '`ppo/policy/policykl`: Similar to `ppo/policy/approxkl`, but measured by `masked_mean(old_logprobs
    - logprobs, mask)`, corresponding to the `k1` estimator in [http://joschu.net/blog/kl-approx.html](http://joschu.net/blog/kl-approx.html)'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/policy/policykl`: 类似于`ppo/policy/approxkl`，但由`masked_mean(old_logprobs
    - logprobs, mask)`测量，对应于[http://joschu.net/blog/kl-approx.html](http://joschu.net/blog/kl-approx.html)中的`k1`估计器。'
- en: '`ppo/policy/ratio`: The histogram distribution of the ratio between the new
    and old policies, used to compute the PPO objective.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/policy/ratio`: 新旧政策之间比率的直方图分布，用于计算PPO目标。'
- en: '`ppo/policy/advantages_mean`: The average of the GAE (Generalized Advantage
    Estimation) advantage estimates. The advantage function measures how much better
    an action is compared to the average action at a state.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/policy/advantages_mean`: GAE（广义优势估计）优势估计的平均值。优势函数衡量了一个动作相对于状态的平均动作有多好。'
- en: '`ppo/policy/advantages`: The histogram distribution of `ppo/policy/advantages_mean`.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/policy/advantages`: `ppo/policy/advantages_mean`的直方图分布。'
- en: '`ppo/returns/mean`: The mean of the TD(λ) returns, calculated by `returns =
    advantage + values`, another indicator of model performance. See [https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)
    for more details.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/returns/mean`: TD(λ)回报的平均值，通过`returns = advantage + values`计算，这是模型性能的另一个指标。有关更多详细信息，请参见[https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)。'
- en: '`ppo/returns/var`: The variance of the TD(λ) returns, calculated by `returns
    = advantage + values`, another indicator of model performance.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/returns/var`: TD(λ)回报的方差，通过`returns = advantage + values`计算，这是模型性能的另一个指标。'
- en: '`ppo/val/mean`: The mean of the values, used to monitor the value function’s
    performance.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/val/mean`：值的平均值，用于监控值函数的性能。'
- en: '`ppo/val/var` : The variance of the values, used to monitor the value function’s
    performance.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/val/var`：值的方差，用于监控值函数的性能。'
- en: '`ppo/val/var_explained`: The explained variance for the value function, used
    to monitor the value function’s performance.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/val/var_explained`：值函数的解释方差，用于监控值函数的性能。'
- en: '`ppo/val/clipfrac`: The fraction of the value function’s predicted values that
    are clipped.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/val/clipfrac`：值函数预测值被剪切的比例。'
- en: '`ppo/val/vpred`: The predicted values from the value function.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/val/vpred`：值函数的预测值。'
- en: '`ppo/val/error`: The mean squared error between the `ppo/val/vpred` and returns,
    used to monitor the value function’s performance.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/val/error`：`ppo/val/vpred`和returns之间的均方误差，用于监控值函数的性能。'
- en: '`ppo/loss/policy`: The policy loss for the Proximal Policy Optimization (PPO)
    algorithm.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/loss/policy`：Proximal Policy Optimization (PPO)算法的策略损失。'
- en: '`ppo/loss/value`: The loss for the value function in the PPO algorithm. This
    value quantifies how well the function estimates the expected future rewards.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/loss/value`：PPO算法中值函数的损失。该值量化了函数估计未来预期奖励的能力。'
- en: '`ppo/loss/total`: The total loss for the PPO algorithm. It is the sum of the
    policy loss and the value function loss.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/loss/total`：PPO算法的总损失。它是策略损失和值函数损失的总和。'
- en: 'Stats on queries, responses, and logprobs:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 关于查询、响应和logprobs的统计数据：
- en: '`tokens/queries_len_mean`: The average length of the queries tokens.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tokens/queries_len_mean`：查询标记的平均长度。'
- en: '`tokens/queries_len_std`: The standard deviation of the length of the queries
    tokens.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tokens/queries_len_std`：查询标记长度的标准差。'
- en: '`tokens/queries_dist`: The histogram distribution of the length of the queries
    tokens.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tokens/queries_dist`：查询标记长度的直方图分布。'
- en: '`tokens/responses_len_mean`: The average length of the responses tokens.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tokens/responses_len_mean`：响应标记的平均长度。'
- en: '`tokens/responses_len_std`: The standard deviation of the length of the responses
    tokens.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tokens/responses_len_std`：响应标记长度的标准差。'
- en: '`tokens/responses_dist`: The histogram distribution of the length of the responses
    tokens. (Costa: inconsistent naming, should be `tokens/responses_len_dist`)'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tokens/responses_dist`：响应标记长度的直方图分布。（Costa：命名不一致，应该是`tokens/responses_len_dist`）'
- en: '`objective/logprobs`: The histogram distribution of the log probabilities of
    the actions taken by the model.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`objective/logprobs`：模型采取的动作的对数概率的直方图分布。'
- en: '`objective/ref_logprobs`: The histogram distribution of the log probabilities
    of the actions taken by the reference model.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`objective/ref_logprobs`：参考模型采取的动作的对数概率的直方图分布。'
- en: Crucial values
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键数值
- en: 'During training, many values are logged, here are the most important ones:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，记录了许多数值，以下是最重要的数值：
- en: '`env/reward_mean`,`env/reward_std`, `env/reward_dist`: the properties of the
    reward distribution from the “environment” / reward model'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`env/reward_mean`,`env/reward_std`, `env/reward_dist`: 奖励分布的属性来自“环境”/奖励模型'
- en: '`ppo/mean_non_score_reward`: The mean negated KL penalty during training (shows
    the delta between the reference model and the new policy over the batch in the
    step)'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/mean_non_score_reward`：训练过程中的平均负的KL惩罚（显示参考模型和新策略在步骤中批次之间的差异）。'
- en: 'Here are some parameters that are useful to monitor for stability (when these
    diverge or collapse to 0, try tuning variables):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些有用的参数，用于监控稳定性（当这些参数发散或收敛为0时，请尝试调整变量）：
- en: '`ppo/loss/value`: it will spike / NaN when not going well.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/loss/value`：当情况不好时，它会激增/NaN。'
- en: '`ppo/policy/ratio`: `ratio` being 1 is a baseline value, meaning that the probability
    of sampling a token is the same under the new and old policy. If the ratio is
    too high like 200, it means the probability of sampling a token is 200 times higher
    under the new policy than the old policy. This is a sign that the new policy is
    too different from the old policy, which will likely cause overoptimization and
    collapse training later on.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/policy/ratio`：`ratio`为1是一个基准值，意味着在新旧策略下采样标记的概率相同。如果比率太高，比如200，这意味着在新策略下采样标记的概率比旧策略高200倍。这表明新策略与旧策略差异太大，可能会导致过度优化和训练后期崩溃。'
- en: '`ppo/policy/clipfrac` and `ppo/policy/approxkl`: if `ratio` is too high, the
    `ratio` is going to get clipped, resulting in high `clipfrac` and high `approxkl`
    as well.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ppo/policy/clipfrac`和`ppo/policy/approxkl`：如果`ratio`太高，`ratio`将被剪切，导致`clipfrac`和`approxkl`也很高。'
- en: '`objective/kl`: it should stay positive so that the policy is not too far away
    from the reference policy.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`objective/kl`：应保持为正值，以便策略不会远离参考策略。'
- en: '`objective/kl_coef`: The target coefficient with `AdaptiveKLController`. Often
    increases before numerical instabilities.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`objective/kl_coef`：与`AdaptiveKLController`一起的目标系数。通常在数值不稳定之前增加。'
