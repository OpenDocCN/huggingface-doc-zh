# 创建数据集加载脚本

> 原始文本：[https://huggingface.co/docs/datasets/dataset_script](https://huggingface.co/docs/datasets/dataset_script)

如果您的数据集是以下格式之一，则可能不需要数据集加载脚本：CSV、JSON、JSON lines、文本、图像、音频或Parquet。对于这些格式，只要您的数据集存储库具有[所需结构](./repository_structure)，您应该能够使用[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)自动加载数据集。

在下一个主要版本中，🤗 Datasets的新安全功能将默认禁用运行数据集加载脚本，并且您将需要传递`trust_remote_code=True`来加载需要运行数据集脚本的数据集。

编写数据集脚本以加载和共享由不受支持的格式中的数据文件组成或需要更复杂数据准备的数据集。这是一种比使用[数据集卡中的YAML元数据](./repository_structure#define-your-splits-in-yaml)更高级的定义数据集的方式。数据集脚本是一个定义数据集的不同配置和拆分以及如何下载和处理数据的Python文件。

脚本可以从任何网站或相同的数据集存储库下载数据文件。

数据集加载脚本应该与数据集存储库或目录同名。例如，名为`my_dataset`的存储库应包含`my_dataset.py`脚本。这样可以通过以下方式加载：

```py
my_dataset/
├── README.md
└── my_dataset.py
```

```py
>>> from datasets import load_dataset
>>> load_dataset("path/to/my_dataset")
```

以下指南包括有关数据集脚本的说明：

+   添加数据集元数据。

+   下载数据文件。

+   生成样本。

+   生成数据集元数据。

+   将数据集上传到Hub。

打开[SQuAD数据集加载脚本](https://huggingface.co/datasets/squad/blob/main/squad.py)模板，以了解如何共享数据集。

为了帮助您入门，请尝试从数据集加载脚本[模板](https://github.com/huggingface/datasets/blob/main/templates/new_dataset_script.py)开始！

## 添加数据集属性

第一步是在`DatasetBuilder._info()`中添加关于数据集的一些信息或属性。您应该指定的最重要的属性是：

1.  `DatasetInfo.description`提供了数据集的简洁描述。描述告诉用户数据集中包含什么，如何收集数据以及如何用于NLP任务。

1.  `DatasetInfo.features`定义了数据集中每列的名称和类型。这也为每个示例提供了结构，因此可以在列中创建嵌套子字段。查看[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)以获取可以使用的特征类型的完整列表。

```py
datasets.Features(
    {
        "id": datasets.Value("string"),
        "title": datasets.Value("string"),
        "context": datasets.Value("string"),
        "question": datasets.Value("string"),
        "answers": datasets.Sequence(
            {
                "text": datasets.Value("string"),
                "answer_start": datasets.Value("int32"),
            }
        ),
    }
)
```

1.  `DatasetInfo.homepage`包含数据集主页的URL，用户可以在那里找到有关数据集的更多详细信息。

1.  `DatasetInfo.citation`包含数据集的BibTeX引用。

在模板中填写所有这些字段后，它应该看起来像来自SQuAD加载脚本的以下示例：

```py
def _info(self):
    return datasets.DatasetInfo(
        description=_DESCRIPTION,
        features=datasets.Features(
            {
                "id": datasets.Value("string"),
                "title": datasets.Value("string"),
                "context": datasets.Value("string"),
                "question": datasets.Value("string"),
                "answers": datasets.features.Sequence(
                    {"text": datasets.Value("string"), "answer_start": datasets.Value("int32"),}
                ),
            }
        ),
        # No default supervised_keys (as we have to pass both question
        # and context as input).
        supervised_keys=None,
        homepage="https://rajpurkar.github.io/SQuAD-explorer/",
        citation=_CITATION,
    )
```

### 多个配置

在某些情况下，您的数据集可能具有多个配置。例如，[SuperGLUE](https://huggingface.co/datasets/super_glue)数据集是一个包含5个数据集的集合，旨在评估语言理解任务。🤗 Datasets提供了[BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)，允许您为用户创建不同的配置选择。

让我们研究[SuperGLUE加载脚本](https://huggingface.co/datasets/super_glue/blob/main/super_glue.py)，看看您如何定义多个配置。

1.  创建一个包含有关数据集属性的[BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)子类。这些属性可以是数据集的特征、标签类别和数据文件的URL。

```py
class SuperGlueConfig(datasets.BuilderConfig):
    """BuilderConfig for SuperGLUE."""

    def __init__(self, features, data_url, citation, url, label_classes=("False", "True"), **kwargs):
        """BuilderConfig for SuperGLUE.

        Args:
        features: *list[string]*, list of the features that will appear in the
            feature dict. Should not include "label".
        data_url: *string*, url to download the zip file from.
        citation: *string*, citation for the data set.
        url: *string*, url for information about the data set.
        label_classes: *list[string]*, the list of classes for the label if the
            label is present as a string. Non-string labels will be cast to either
            'False' or 'True'.
        **kwargs: keyword arguments forwarded to super.
        """
        # Version history:
        # 1.0.2: Fixed non-nondeterminism in ReCoRD.
        # 1.0.1: Change from the pre-release trial version of SuperGLUE (v1.9) to
        #        the full release (v2.0).
        # 1.0.0: S3 (new shuffling, sharding and slicing mechanism).
        # 0.0.2: Initial version.
        super().__init__(version=datasets.Version("1.0.2"), **kwargs)
        self.features = features
        self.label_classes = label_classes
        self.data_url = data_url
        self.citation = citation
        self.url = url
```

1.  创建配置的实例以指定每个配置的属性值。这使您可以灵活地指定每个配置的名称和描述。这些子类实例应列在`DatasetBuilder.BUILDER_CONFIGS`下：

```py
class SuperGlue(datasets.GeneratorBasedBuilder):
    """The SuperGLUE benchmark."""

    BUILDER_CONFIG_CLASS = SuperGlueConfig

    BUILDER_CONFIGS = [
        SuperGlueConfig(
            name="boolq",
            description=_BOOLQ_DESCRIPTION,
            features=["question", "passage"],
            data_url="https://dl.fbaipublicfiles.com/glue/superglue/data/v2/BoolQ.zip",
            citation=_BOOLQ_CITATION,
            url="https://github.com/google-research-datasets/boolean-questions",
        ),
        ...
        ...
        SuperGlueConfig(
            name="axg",
            description=_AXG_DESCRIPTION,
            features=["premise", "hypothesis"],
            label_classes=["entailment", "not_entailment"],
            data_url="https://dl.fbaipublicfiles.com/glue/superglue/data/v2/AX-g.zip",
            citation=_AXG_CITATION,
            url="https://github.com/rudinger/winogender-schemas",
        ),
```

1.  现在，用户可以使用配置`name`加载数据集的特定配置：

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('super_glue', 'boolq')
```

此外，用户可以通过将构建器配置参数传递给[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)来实例化自定义构建器配置：

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('super_glue', data_url="https://custom_url")
```

### 默认配置

用户在加载具有多个配置的数据集时必须指定配置名称。否则，🤗数据集将引发`ValueError`，并提示用户选择配置名称。您可以通过设置具有`DEFAULT_CONFIG_NAME`属性的默认数据集配置来避免这种情况：

```py
class NewDataset(datasets.GeneratorBasedBuilder):

VERSION = datasets.Version("1.1.0")

BUILDER_CONFIGS = [
    datasets.BuilderConfig(name="first_domain", version=VERSION, description="This part of my dataset covers a first domain"),
    datasets.BuilderConfig(name="second_domain", version=VERSION, description="This part of my dataset covers a second domain"),
]

DEFAULT_CONFIG_NAME = "first_domain"
```

只有在有意义的情况下才使用默认配置。不要设置默认配置，因为用户可能更方便地在加载数据集时不指定配置。例如，多语言数据集通常为每种语言设置单独的配置。一个合适的默认配置可能是一个聚合配置，如果用户没有请求特定语言，则加载数据集的所有语言。

## 下载数据文件并组织拆分

在定义数据集属性之后，下一步是下载数据文件并根据它们的拆分进行组织。

1.  在加载脚本中创建一个指向原始SQuAD数据文件的URL字典：

```py
_URL = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
_URLS = {
    "train": _URL + "train-v1.1.json",
    "dev": _URL + "dev-v1.1.json",
}
```

如果数据文件存储在与数据集脚本相同的文件夹或存储库中，您可以只传递文件的相对路径而不是URL。

1.  [DownloadManager.download_and_extract()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download_and_extract)接受此字典并下载数据文件。一旦文件下载完成，使用[SplitGenerator](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.SplitGenerator)来组织数据集中的每个拆分。这是一个简单的类，包含：

    +   每个拆分的`name`。您应该使用标准的拆分名称：`Split.TRAIN`、`Split.TEST`和`Split.VALIDATION`。

    +   `gen_kwargs`为每个拆分提供要加载的数据文件的文件路径。

您的`DatasetBuilder._split_generator()`现在应该如下所示：

```py
def _split_generators(self, dl_manager: datasets.DownloadManager) -> List[datasets.SplitGenerator]:
    urls_to_download = self._URLS
    downloaded_files = dl_manager.download_and_extract(urls_to_download)

    return [
        datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={"filepath": downloaded_files["train"]}),
        datasets.SplitGenerator(name=datasets.Split.VALIDATION, gen_kwargs={"filepath": downloaded_files["dev"]}),
    ]
```

## 生成样本

此时，您已经有：

+   添加数据集属性。

+   提供了如何下载数据文件的说明。

+   组织拆分。

下一步是实际在每个拆分中生成样本。

1.  `DatasetBuilder._generate_examples`使用`gen_kwargs`提供的文件路径来读取和解析数据文件。您需要编写一个加载数据文件并提取列的函数。

1.  您的函数应该生成一个元组，其中包含一个`id_`和数据集中的一个示例。

```py
def _generate_examples(self, filepath):
    """This function returns the examples in the raw (text) form."""
    logger.info("generating examples from = %s", filepath)
    with open(filepath) as f:
        squad = json.load(f)
        for article in squad["data"]:
            title = article.get("title", "").strip()
            for paragraph in article["paragraphs"]:
                context = paragraph["context"].strip()
                for qa in paragraph["qas"]:
                    question = qa["question"].strip()
                    id_ = qa["id"]

                    answer_starts = [answer["answer_start"] for answer in qa["answers"]]
                    answers = [answer["text"].strip() for answer in qa["answers"]]

                    # Features currently used are "context", "question", and "answers".
                    # Others are extracted here for the ease of future expansions.
                    yield id_, {
                        "title": title,
                        "context": context,
                        "question": question,
                        "id": id_,
                        "answers": {"answer_start": answer_starts, "text": answers,},
                    }
```

## （可选）生成数据集元数据

添加数据集元数据是包含有关数据集信息的重要方式。元数据存储在数据集卡片`README.md`中的YAML中。它包括诸如确认数据集是否正确生成所需的示例数量等信息，以及有关数据集的信息，如其`features`。

运行以下命令生成`README.md`中的数据集元数据，并确保您的新数据集加载脚本正常工作：

```py
datasets-cli test path/to/<your-dataset-loading-script> --save_info --all_configs
```

如果您的数据集加载脚本通过了测试，现在您的数据集文件夹中应该有一个包含一些元数据的`README.md`文件，其中包含一个`dataset_info`字段。

## 上传到Hub

一旦您的脚本准备好了，[创建一个数据集卡片](dataset_card)并[上传到Hub](share)。

恭喜，您现在可以从Hub加载您的数据集了！🥳

```py
>>> from datasets import load_dataset
>>> load_dataset("<username>/my_dataset")
```

## 高级特性

### 分片

如果您的数据集由许多大文件组成，🤗数据集会自动并行运行您的脚本，使其运行速度非常快！例如，如果您有数百或数千个TAR存档文件，或者像[oscar](https://huggingface.co/datasets/oscar/blob/main/oscar.py)这样的JSONL文件。

为了使其工作，我们认为`gen_kwargs`中的文件列表是分片。因此，🤗数据集可以自动启动多个工作进程并行运行`_generate_examples`，每个工作进程都会被分配一个要处理的分片子集。

```py

class MyShardedDataset(datasets.GeneratorBasedBuilder):

    def _split_generators(self, dl_manager: datasets.DownloadManager) -> List[datasets.SplitGenerator]:
        downloaded_files = dl_manager.download([f"data/shard_{i}.jsonl" for i in range(1024)])
        return [
            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={"filepaths": downloaded_files}),
        ]

    def _generate_examples(self, filepaths):
        # Each worker can be given a slice of the original `filepaths` list defined in the `gen_kwargs`
        # so that this code can run in parallel on several shards at the same time
        for filepath in filepaths:
            ...
```

用户还可以在`load_dataset()`中指定`num_proc=`来指定要用作工作进程的进程数。

### ArrowBasedBuilder

对于一些数据集，批量生成数据可能比逐个生成示例要快得多。您可以通过直接生成Arrow表而不是示例来加快数据集生成速度。这在您的数据来自Pandas DataFrames等情况下尤其有用，因为从Pandas转换为Arrow非常简单：

```py
import pyarrow as pa
pa_table = pa.Table.from_pandas(df)
```

要生成Arrow表而不是单个示例，请使您的数据集构建器继承自[ArrowBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.ArrowBasedBuilder)而不是[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder)，并使用`_generate_tables`而不是`_generate_examples`：

```py
class MySuperFastDataset(datasets.ArrowBasedBuilder):

    def _generate_tables(self, filepaths):
        idx = 0
        for filepath in filepaths:
            ...
            yield idx, pa_table
            idx += 1
```

不要忘记保持脚本的内存效率，以防用户在内存较少的机器上运行它们。
