# åˆ›å»ºæ•°æ®é›†åŠ è½½è„šæœ¬

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/datasets/dataset_script](https://huggingface.co/docs/datasets/dataset_script)

å¦‚æœæ‚¨çš„æ•°æ®é›†æ˜¯ä»¥ä¸‹æ ¼å¼ä¹‹ä¸€ï¼Œåˆ™å¯èƒ½ä¸éœ€è¦æ•°æ®é›†åŠ è½½è„šæœ¬ï¼šCSVã€JSONã€JSON linesã€æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘æˆ–Parquetã€‚å¯¹äºè¿™äº›æ ¼å¼ï¼Œåªè¦æ‚¨çš„æ•°æ®é›†å­˜å‚¨åº“å…·æœ‰[æ‰€éœ€ç»“æ„](./repository_structure)ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿä½¿ç”¨[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)è‡ªåŠ¨åŠ è½½æ•°æ®é›†ã€‚

åœ¨ä¸‹ä¸€ä¸ªä¸»è¦ç‰ˆæœ¬ä¸­ï¼ŒğŸ¤— Datasetsçš„æ–°å®‰å…¨åŠŸèƒ½å°†é»˜è®¤ç¦ç”¨è¿è¡Œæ•°æ®é›†åŠ è½½è„šæœ¬ï¼Œå¹¶ä¸”æ‚¨å°†éœ€è¦ä¼ é€’`trust_remote_code=True`æ¥åŠ è½½éœ€è¦è¿è¡Œæ•°æ®é›†è„šæœ¬çš„æ•°æ®é›†ã€‚

ç¼–å†™æ•°æ®é›†è„šæœ¬ä»¥åŠ è½½å’Œå…±äº«ç”±ä¸å—æ”¯æŒçš„æ ¼å¼ä¸­çš„æ•°æ®æ–‡ä»¶ç»„æˆæˆ–éœ€è¦æ›´å¤æ‚æ•°æ®å‡†å¤‡çš„æ•°æ®é›†ã€‚è¿™æ˜¯ä¸€ç§æ¯”ä½¿ç”¨[æ•°æ®é›†å¡ä¸­çš„YAMLå…ƒæ•°æ®](./repository_structure#define-your-splits-in-yaml)æ›´é«˜çº§çš„å®šä¹‰æ•°æ®é›†çš„æ–¹å¼ã€‚æ•°æ®é›†è„šæœ¬æ˜¯ä¸€ä¸ªå®šä¹‰æ•°æ®é›†çš„ä¸åŒé…ç½®å’Œæ‹†åˆ†ä»¥åŠå¦‚ä½•ä¸‹è½½å’Œå¤„ç†æ•°æ®çš„Pythonæ–‡ä»¶ã€‚

è„šæœ¬å¯ä»¥ä»ä»»ä½•ç½‘ç«™æˆ–ç›¸åŒçš„æ•°æ®é›†å­˜å‚¨åº“ä¸‹è½½æ•°æ®æ–‡ä»¶ã€‚

æ•°æ®é›†åŠ è½½è„šæœ¬åº”è¯¥ä¸æ•°æ®é›†å­˜å‚¨åº“æˆ–ç›®å½•åŒåã€‚ä¾‹å¦‚ï¼Œåä¸º`my_dataset`çš„å­˜å‚¨åº“åº”åŒ…å«`my_dataset.py`è„šæœ¬ã€‚è¿™æ ·å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åŠ è½½ï¼š

```py
my_dataset/
â”œâ”€â”€ README.md
â””â”€â”€ my_dataset.py
```

```py
>>> from datasets import load_dataset
>>> load_dataset("path/to/my_dataset")
```

ä»¥ä¸‹æŒ‡å—åŒ…æ‹¬æœ‰å…³æ•°æ®é›†è„šæœ¬çš„è¯´æ˜ï¼š

+   æ·»åŠ æ•°æ®é›†å…ƒæ•°æ®ã€‚

+   ä¸‹è½½æ•°æ®æ–‡ä»¶ã€‚

+   ç”Ÿæˆæ ·æœ¬ã€‚

+   ç”Ÿæˆæ•°æ®é›†å…ƒæ•°æ®ã€‚

+   å°†æ•°æ®é›†ä¸Šä¼ åˆ°Hubã€‚

æ‰“å¼€[SQuADæ•°æ®é›†åŠ è½½è„šæœ¬](https://huggingface.co/datasets/squad/blob/main/squad.py)æ¨¡æ¿ï¼Œä»¥äº†è§£å¦‚ä½•å…±äº«æ•°æ®é›†ã€‚

ä¸ºäº†å¸®åŠ©æ‚¨å…¥é—¨ï¼Œè¯·å°è¯•ä»æ•°æ®é›†åŠ è½½è„šæœ¬[æ¨¡æ¿](https://github.com/huggingface/datasets/blob/main/templates/new_dataset_script.py)å¼€å§‹ï¼

## æ·»åŠ æ•°æ®é›†å±æ€§

ç¬¬ä¸€æ­¥æ˜¯åœ¨`DatasetBuilder._info()`ä¸­æ·»åŠ å…³äºæ•°æ®é›†çš„ä¸€äº›ä¿¡æ¯æˆ–å±æ€§ã€‚æ‚¨åº”è¯¥æŒ‡å®šçš„æœ€é‡è¦çš„å±æ€§æ˜¯ï¼š

1.  `DatasetInfo.description`æä¾›äº†æ•°æ®é›†çš„ç®€æ´æè¿°ã€‚æè¿°å‘Šè¯‰ç”¨æˆ·æ•°æ®é›†ä¸­åŒ…å«ä»€ä¹ˆï¼Œå¦‚ä½•æ”¶é›†æ•°æ®ä»¥åŠå¦‚ä½•ç”¨äºNLPä»»åŠ¡ã€‚

1.  `DatasetInfo.features`å®šä¹‰äº†æ•°æ®é›†ä¸­æ¯åˆ—çš„åç§°å’Œç±»å‹ã€‚è¿™ä¹Ÿä¸ºæ¯ä¸ªç¤ºä¾‹æä¾›äº†ç»“æ„ï¼Œå› æ­¤å¯ä»¥åœ¨åˆ—ä¸­åˆ›å»ºåµŒå¥—å­å­—æ®µã€‚æŸ¥çœ‹[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)ä»¥è·å–å¯ä»¥ä½¿ç”¨çš„ç‰¹å¾ç±»å‹çš„å®Œæ•´åˆ—è¡¨ã€‚

```py
datasets.Features(
    {
        "id": datasets.Value("string"),
        "title": datasets.Value("string"),
        "context": datasets.Value("string"),
        "question": datasets.Value("string"),
        "answers": datasets.Sequence(
            {
                "text": datasets.Value("string"),
                "answer_start": datasets.Value("int32"),
            }
        ),
    }
)
```

1.  `DatasetInfo.homepage`åŒ…å«æ•°æ®é›†ä¸»é¡µçš„URLï¼Œç”¨æˆ·å¯ä»¥åœ¨é‚£é‡Œæ‰¾åˆ°æœ‰å…³æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

1.  `DatasetInfo.citation`åŒ…å«æ•°æ®é›†çš„BibTeXå¼•ç”¨ã€‚

åœ¨æ¨¡æ¿ä¸­å¡«å†™æ‰€æœ‰è¿™äº›å­—æ®µåï¼Œå®ƒåº”è¯¥çœ‹èµ·æ¥åƒæ¥è‡ªSQuADåŠ è½½è„šæœ¬çš„ä»¥ä¸‹ç¤ºä¾‹ï¼š

```py
def _info(self):
    return datasets.DatasetInfo(
        description=_DESCRIPTION,
        features=datasets.Features(
            {
                "id": datasets.Value("string"),
                "title": datasets.Value("string"),
                "context": datasets.Value("string"),
                "question": datasets.Value("string"),
                "answers": datasets.features.Sequence(
                    {"text": datasets.Value("string"), "answer_start": datasets.Value("int32"),}
                ),
            }
        ),
        # No default supervised_keys (as we have to pass both question
        # and context as input).
        supervised_keys=None,
        homepage="https://rajpurkar.github.io/SQuAD-explorer/",
        citation=_CITATION,
    )
```

### å¤šä¸ªé…ç½®

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨çš„æ•°æ®é›†å¯èƒ½å…·æœ‰å¤šä¸ªé…ç½®ã€‚ä¾‹å¦‚ï¼Œ[SuperGLUE](https://huggingface.co/datasets/super_glue)æ•°æ®é›†æ˜¯ä¸€ä¸ªåŒ…å«5ä¸ªæ•°æ®é›†çš„é›†åˆï¼Œæ—¨åœ¨è¯„ä¼°è¯­è¨€ç†è§£ä»»åŠ¡ã€‚ğŸ¤— Datasetsæä¾›äº†[BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)ï¼Œå…è®¸æ‚¨ä¸ºç”¨æˆ·åˆ›å»ºä¸åŒçš„é…ç½®é€‰æ‹©ã€‚

è®©æˆ‘ä»¬ç ”ç©¶[SuperGLUEåŠ è½½è„šæœ¬](https://huggingface.co/datasets/super_glue/blob/main/super_glue.py)ï¼Œçœ‹çœ‹æ‚¨å¦‚ä½•å®šä¹‰å¤šä¸ªé…ç½®ã€‚

1.  åˆ›å»ºä¸€ä¸ªåŒ…å«æœ‰å…³æ•°æ®é›†å±æ€§çš„[BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)å­ç±»ã€‚è¿™äº›å±æ€§å¯ä»¥æ˜¯æ•°æ®é›†çš„ç‰¹å¾ã€æ ‡ç­¾ç±»åˆ«å’Œæ•°æ®æ–‡ä»¶çš„URLã€‚

```py
class SuperGlueConfig(datasets.BuilderConfig):
    """BuilderConfig for SuperGLUE."""

    def __init__(self, features, data_url, citation, url, label_classes=("False", "True"), **kwargs):
        """BuilderConfig for SuperGLUE.

        Args:
        features: *list[string]*, list of the features that will appear in the
            feature dict. Should not include "label".
        data_url: *string*, url to download the zip file from.
        citation: *string*, citation for the data set.
        url: *string*, url for information about the data set.
        label_classes: *list[string]*, the list of classes for the label if the
            label is present as a string. Non-string labels will be cast to either
            'False' or 'True'.
        **kwargs: keyword arguments forwarded to super.
        """
        # Version history:
        # 1.0.2: Fixed non-nondeterminism in ReCoRD.
        # 1.0.1: Change from the pre-release trial version of SuperGLUE (v1.9) to
        #        the full release (v2.0).
        # 1.0.0: S3 (new shuffling, sharding and slicing mechanism).
        # 0.0.2: Initial version.
        super().__init__(version=datasets.Version("1.0.2"), **kwargs)
        self.features = features
        self.label_classes = label_classes
        self.data_url = data_url
        self.citation = citation
        self.url = url
```

1.  åˆ›å»ºé…ç½®çš„å®ä¾‹ä»¥æŒ‡å®šæ¯ä¸ªé…ç½®çš„å±æ€§å€¼ã€‚è¿™ä½¿æ‚¨å¯ä»¥çµæ´»åœ°æŒ‡å®šæ¯ä¸ªé…ç½®çš„åç§°å’Œæè¿°ã€‚è¿™äº›å­ç±»å®ä¾‹åº”åˆ—åœ¨`DatasetBuilder.BUILDER_CONFIGS`ä¸‹ï¼š

```py
class SuperGlue(datasets.GeneratorBasedBuilder):
    """The SuperGLUE benchmark."""

    BUILDER_CONFIG_CLASS = SuperGlueConfig

    BUILDER_CONFIGS = [
        SuperGlueConfig(
            name="boolq",
            description=_BOOLQ_DESCRIPTION,
            features=["question", "passage"],
            data_url="https://dl.fbaipublicfiles.com/glue/superglue/data/v2/BoolQ.zip",
            citation=_BOOLQ_CITATION,
            url="https://github.com/google-research-datasets/boolean-questions",
        ),
        ...
        ...
        SuperGlueConfig(
            name="axg",
            description=_AXG_DESCRIPTION,
            features=["premise", "hypothesis"],
            label_classes=["entailment", "not_entailment"],
            data_url="https://dl.fbaipublicfiles.com/glue/superglue/data/v2/AX-g.zip",
            citation=_AXG_CITATION,
            url="https://github.com/rudinger/winogender-schemas",
        ),
```

1.  ç°åœ¨ï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨é…ç½®`name`åŠ è½½æ•°æ®é›†çš„ç‰¹å®šé…ç½®ï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('super_glue', 'boolq')
```

æ­¤å¤–ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡å°†æ„å»ºå™¨é…ç½®å‚æ•°ä¼ é€’ç»™[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)æ¥å®ä¾‹åŒ–è‡ªå®šä¹‰æ„å»ºå™¨é…ç½®ï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('super_glue', data_url="https://custom_url")
```

### é»˜è®¤é…ç½®

ç”¨æˆ·åœ¨åŠ è½½å…·æœ‰å¤šä¸ªé…ç½®çš„æ•°æ®é›†æ—¶å¿…é¡»æŒ‡å®šé…ç½®åç§°ã€‚å¦åˆ™ï¼ŒğŸ¤—æ•°æ®é›†å°†å¼•å‘`ValueError`ï¼Œå¹¶æç¤ºç”¨æˆ·é€‰æ‹©é…ç½®åç§°ã€‚æ‚¨å¯ä»¥é€šè¿‡è®¾ç½®å…·æœ‰`DEFAULT_CONFIG_NAME`å±æ€§çš„é»˜è®¤æ•°æ®é›†é…ç½®æ¥é¿å…è¿™ç§æƒ…å†µï¼š

```py
class NewDataset(datasets.GeneratorBasedBuilder):

VERSION = datasets.Version("1.1.0")

BUILDER_CONFIGS = [
    datasets.BuilderConfig(name="first_domain", version=VERSION, description="This part of my dataset covers a first domain"),
    datasets.BuilderConfig(name="second_domain", version=VERSION, description="This part of my dataset covers a second domain"),
]

DEFAULT_CONFIG_NAME = "first_domain"
```

åªæœ‰åœ¨æœ‰æ„ä¹‰çš„æƒ…å†µä¸‹æ‰ä½¿ç”¨é»˜è®¤é…ç½®ã€‚ä¸è¦è®¾ç½®é»˜è®¤é…ç½®ï¼Œå› ä¸ºç”¨æˆ·å¯èƒ½æ›´æ–¹ä¾¿åœ°åœ¨åŠ è½½æ•°æ®é›†æ—¶ä¸æŒ‡å®šé…ç½®ã€‚ä¾‹å¦‚ï¼Œå¤šè¯­è¨€æ•°æ®é›†é€šå¸¸ä¸ºæ¯ç§è¯­è¨€è®¾ç½®å•ç‹¬çš„é…ç½®ã€‚ä¸€ä¸ªåˆé€‚çš„é»˜è®¤é…ç½®å¯èƒ½æ˜¯ä¸€ä¸ªèšåˆé…ç½®ï¼Œå¦‚æœç”¨æˆ·æ²¡æœ‰è¯·æ±‚ç‰¹å®šè¯­è¨€ï¼Œåˆ™åŠ è½½æ•°æ®é›†çš„æ‰€æœ‰è¯­è¨€ã€‚

## ä¸‹è½½æ•°æ®æ–‡ä»¶å¹¶ç»„ç»‡æ‹†åˆ†

åœ¨å®šä¹‰æ•°æ®é›†å±æ€§ä¹‹åï¼Œä¸‹ä¸€æ­¥æ˜¯ä¸‹è½½æ•°æ®æ–‡ä»¶å¹¶æ ¹æ®å®ƒä»¬çš„æ‹†åˆ†è¿›è¡Œç»„ç»‡ã€‚

1.  åœ¨åŠ è½½è„šæœ¬ä¸­åˆ›å»ºä¸€ä¸ªæŒ‡å‘åŸå§‹SQuADæ•°æ®æ–‡ä»¶çš„URLå­—å…¸ï¼š

```py
_URL = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
_URLS = {
    "train": _URL + "train-v1.1.json",
    "dev": _URL + "dev-v1.1.json",
}
```

å¦‚æœæ•°æ®æ–‡ä»¶å­˜å‚¨åœ¨ä¸æ•°æ®é›†è„šæœ¬ç›¸åŒçš„æ–‡ä»¶å¤¹æˆ–å­˜å‚¨åº“ä¸­ï¼Œæ‚¨å¯ä»¥åªä¼ é€’æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„è€Œä¸æ˜¯URLã€‚

1.  [DownloadManager.download_and_extract()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download_and_extract)æ¥å—æ­¤å­—å…¸å¹¶ä¸‹è½½æ•°æ®æ–‡ä»¶ã€‚ä¸€æ—¦æ–‡ä»¶ä¸‹è½½å®Œæˆï¼Œä½¿ç”¨[SplitGenerator](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.SplitGenerator)æ¥ç»„ç»‡æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ‹†åˆ†ã€‚è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ç±»ï¼ŒåŒ…å«ï¼š

    +   æ¯ä¸ªæ‹†åˆ†çš„`name`ã€‚æ‚¨åº”è¯¥ä½¿ç”¨æ ‡å‡†çš„æ‹†åˆ†åç§°ï¼š`Split.TRAIN`ã€`Split.TEST`å’Œ`Split.VALIDATION`ã€‚

    +   `gen_kwargs`ä¸ºæ¯ä¸ªæ‹†åˆ†æä¾›è¦åŠ è½½çš„æ•°æ®æ–‡ä»¶çš„æ–‡ä»¶è·¯å¾„ã€‚

æ‚¨çš„`DatasetBuilder._split_generator()`ç°åœ¨åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
def _split_generators(self, dl_manager: datasets.DownloadManager) -> List[datasets.SplitGenerator]:
    urls_to_download = self._URLS
    downloaded_files = dl_manager.download_and_extract(urls_to_download)

    return [
        datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={"filepath": downloaded_files["train"]}),
        datasets.SplitGenerator(name=datasets.Split.VALIDATION, gen_kwargs={"filepath": downloaded_files["dev"]}),
    ]
```

## ç”Ÿæˆæ ·æœ¬

æ­¤æ—¶ï¼Œæ‚¨å·²ç»æœ‰ï¼š

+   æ·»åŠ æ•°æ®é›†å±æ€§ã€‚

+   æä¾›äº†å¦‚ä½•ä¸‹è½½æ•°æ®æ–‡ä»¶çš„è¯´æ˜ã€‚

+   ç»„ç»‡æ‹†åˆ†ã€‚

ä¸‹ä¸€æ­¥æ˜¯å®é™…åœ¨æ¯ä¸ªæ‹†åˆ†ä¸­ç”Ÿæˆæ ·æœ¬ã€‚

1.  `DatasetBuilder._generate_examples`ä½¿ç”¨`gen_kwargs`æä¾›çš„æ–‡ä»¶è·¯å¾„æ¥è¯»å–å’Œè§£ææ•°æ®æ–‡ä»¶ã€‚æ‚¨éœ€è¦ç¼–å†™ä¸€ä¸ªåŠ è½½æ•°æ®æ–‡ä»¶å¹¶æå–åˆ—çš„å‡½æ•°ã€‚

1.  æ‚¨çš„å‡½æ•°åº”è¯¥ç”Ÿæˆä¸€ä¸ªå…ƒç»„ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ª`id_`å’Œæ•°æ®é›†ä¸­çš„ä¸€ä¸ªç¤ºä¾‹ã€‚

```py
def _generate_examples(self, filepath):
    """This function returns the examples in the raw (text) form."""
    logger.info("generating examples from = %s", filepath)
    with open(filepath) as f:
        squad = json.load(f)
        for article in squad["data"]:
            title = article.get("title", "").strip()
            for paragraph in article["paragraphs"]:
                context = paragraph["context"].strip()
                for qa in paragraph["qas"]:
                    question = qa["question"].strip()
                    id_ = qa["id"]

                    answer_starts = [answer["answer_start"] for answer in qa["answers"]]
                    answers = [answer["text"].strip() for answer in qa["answers"]]

                    # Features currently used are "context", "question", and "answers".
                    # Others are extracted here for the ease of future expansions.
                    yield id_, {
                        "title": title,
                        "context": context,
                        "question": question,
                        "id": id_,
                        "answers": {"answer_start": answer_starts, "text": answers,},
                    }
```

## ï¼ˆå¯é€‰ï¼‰ç”Ÿæˆæ•°æ®é›†å…ƒæ•°æ®

æ·»åŠ æ•°æ®é›†å…ƒæ•°æ®æ˜¯åŒ…å«æœ‰å…³æ•°æ®é›†ä¿¡æ¯çš„é‡è¦æ–¹å¼ã€‚å…ƒæ•°æ®å­˜å‚¨åœ¨æ•°æ®é›†å¡ç‰‡`README.md`ä¸­çš„YAMLä¸­ã€‚å®ƒåŒ…æ‹¬è¯¸å¦‚ç¡®è®¤æ•°æ®é›†æ˜¯å¦æ­£ç¡®ç”Ÿæˆæ‰€éœ€çš„ç¤ºä¾‹æ•°é‡ç­‰ä¿¡æ¯ï¼Œä»¥åŠæœ‰å…³æ•°æ®é›†çš„ä¿¡æ¯ï¼Œå¦‚å…¶`features`ã€‚

è¿è¡Œä»¥ä¸‹å‘½ä»¤ç”Ÿæˆ`README.md`ä¸­çš„æ•°æ®é›†å…ƒæ•°æ®ï¼Œå¹¶ç¡®ä¿æ‚¨çš„æ–°æ•°æ®é›†åŠ è½½è„šæœ¬æ­£å¸¸å·¥ä½œï¼š

```py
datasets-cli test path/to/<your-dataset-loading-script> --save_info --all_configs
```

å¦‚æœæ‚¨çš„æ•°æ®é›†åŠ è½½è„šæœ¬é€šè¿‡äº†æµ‹è¯•ï¼Œç°åœ¨æ‚¨çš„æ•°æ®é›†æ–‡ä»¶å¤¹ä¸­åº”è¯¥æœ‰ä¸€ä¸ªåŒ…å«ä¸€äº›å…ƒæ•°æ®çš„`README.md`æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ª`dataset_info`å­—æ®µã€‚

## ä¸Šä¼ åˆ°Hub

ä¸€æ—¦æ‚¨çš„è„šæœ¬å‡†å¤‡å¥½äº†ï¼Œ[åˆ›å»ºä¸€ä¸ªæ•°æ®é›†å¡ç‰‡](dataset_card)å¹¶[ä¸Šä¼ åˆ°Hub](share)ã€‚

æ­å–œï¼Œæ‚¨ç°åœ¨å¯ä»¥ä»HubåŠ è½½æ‚¨çš„æ•°æ®é›†äº†ï¼ğŸ¥³

```py
>>> from datasets import load_dataset
>>> load_dataset("<username>/my_dataset")
```

## é«˜çº§ç‰¹æ€§

### åˆ†ç‰‡

å¦‚æœæ‚¨çš„æ•°æ®é›†ç”±è®¸å¤šå¤§æ–‡ä»¶ç»„æˆï¼ŒğŸ¤—æ•°æ®é›†ä¼šè‡ªåŠ¨å¹¶è¡Œè¿è¡Œæ‚¨çš„è„šæœ¬ï¼Œä½¿å…¶è¿è¡Œé€Ÿåº¦éå¸¸å¿«ï¼ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰æ•°ç™¾æˆ–æ•°åƒä¸ªTARå­˜æ¡£æ–‡ä»¶ï¼Œæˆ–è€…åƒ[oscar](https://huggingface.co/datasets/oscar/blob/main/oscar.py)è¿™æ ·çš„JSONLæ–‡ä»¶ã€‚

ä¸ºäº†ä½¿å…¶å·¥ä½œï¼Œæˆ‘ä»¬è®¤ä¸º`gen_kwargs`ä¸­çš„æ–‡ä»¶åˆ—è¡¨æ˜¯åˆ†ç‰‡ã€‚å› æ­¤ï¼ŒğŸ¤—æ•°æ®é›†å¯ä»¥è‡ªåŠ¨å¯åŠ¨å¤šä¸ªå·¥ä½œè¿›ç¨‹å¹¶è¡Œè¿è¡Œ`_generate_examples`ï¼Œæ¯ä¸ªå·¥ä½œè¿›ç¨‹éƒ½ä¼šè¢«åˆ†é…ä¸€ä¸ªè¦å¤„ç†çš„åˆ†ç‰‡å­é›†ã€‚

```py

class MyShardedDataset(datasets.GeneratorBasedBuilder):

    def _split_generators(self, dl_manager: datasets.DownloadManager) -> List[datasets.SplitGenerator]:
        downloaded_files = dl_manager.download([f"data/shard_{i}.jsonl" for i in range(1024)])
        return [
            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={"filepaths": downloaded_files}),
        ]

    def _generate_examples(self, filepaths):
        # Each worker can be given a slice of the original `filepaths` list defined in the `gen_kwargs`
        # so that this code can run in parallel on several shards at the same time
        for filepath in filepaths:
            ...
```

ç”¨æˆ·è¿˜å¯ä»¥åœ¨`load_dataset()`ä¸­æŒ‡å®š`num_proc=`æ¥æŒ‡å®šè¦ç”¨ä½œå·¥ä½œè¿›ç¨‹çš„è¿›ç¨‹æ•°ã€‚

### ArrowBasedBuilder

å¯¹äºä¸€äº›æ•°æ®é›†ï¼Œæ‰¹é‡ç”Ÿæˆæ•°æ®å¯èƒ½æ¯”é€ä¸ªç”Ÿæˆç¤ºä¾‹è¦å¿«å¾—å¤šã€‚æ‚¨å¯ä»¥é€šè¿‡ç›´æ¥ç”ŸæˆArrowè¡¨è€Œä¸æ˜¯ç¤ºä¾‹æ¥åŠ å¿«æ•°æ®é›†ç”Ÿæˆé€Ÿåº¦ã€‚è¿™åœ¨æ‚¨çš„æ•°æ®æ¥è‡ªPandas DataFramesç­‰æƒ…å†µä¸‹å°¤å…¶æœ‰ç”¨ï¼Œå› ä¸ºä»Pandasè½¬æ¢ä¸ºArrowéå¸¸ç®€å•ï¼š

```py
import pyarrow as pa
pa_table = pa.Table.from_pandas(df)
```

è¦ç”ŸæˆArrowè¡¨è€Œä¸æ˜¯å•ä¸ªç¤ºä¾‹ï¼Œè¯·ä½¿æ‚¨çš„æ•°æ®é›†æ„å»ºå™¨ç»§æ‰¿è‡ª[ArrowBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.ArrowBasedBuilder)è€Œä¸æ˜¯[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder)ï¼Œå¹¶ä½¿ç”¨`_generate_tables`è€Œä¸æ˜¯`_generate_examples`ï¼š

```py
class MySuperFastDataset(datasets.ArrowBasedBuilder):

    def _generate_tables(self, filepaths):
        idx = 0
        for filepath in filepaths:
            ...
            yield idx, pa_table
            idx += 1
```

ä¸è¦å¿˜è®°ä¿æŒè„šæœ¬çš„å†…å­˜æ•ˆç‡ï¼Œä»¥é˜²ç”¨æˆ·åœ¨å†…å­˜è¾ƒå°‘çš„æœºå™¨ä¸Šè¿è¡Œå®ƒä»¬ã€‚
