# 生成工具

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/internal/generation_utils](https://huggingface.co/docs/transformers/v4.37.2/en/internal/generation_utils)

此页面列出了 [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)、[greedy_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search)、[contrastive_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search)、[sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.sample)、[beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)、[beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample)、[group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search) 和 [constrained_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search) 使用的所有实用函数。

大多数情况下，这些只有在研究库中生成方法的代码时才有用。

## 生成输出

[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) 的输出是 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 的子类实例。这个输出是一个数据结构，包含了 [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) 返回的所有信息，但也可以用作元组或字典。

这里有一个例子：

```py
from transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

inputs = tokenizer("Hello, my dog is cute and ", return_tensors="pt")
generation_output = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)
```

`generation_output` 对象是一个 [GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput)，正如我们在下面该类的文档中所看到的，它具有以下属性：

+   `sequences`：生成的 token 序列

+   `scores`（可选）：语言建模头的预测分数，每个生成步骤

+   `hidden_states`（可选）：模型的隐藏状态，每个生成步骤

+   `attentions`（可选）：模型的注意力权重，每个生成步骤

这里我们有 `scores`，因为我们传递了 `output_scores=True`，但我们没有 `hidden_states` 和 `attentions`，因为我们没有传递 `output_hidden_states=True` 或 `output_attentions=True`。

您可以像通常那样访问每个属性，如果该属性未被模型返回，您将得到 `None`。在这里，例如 `generation_output.scores` 是语言建模头生成的所有预测分数，而 `generation_output.attentions` 是 `None`。

当将我们的 `generation_output` 对象用作元组时，它只保留那些没有 `None` 值的属性。在这里，例如，它有两个元素，`loss` 然后 `logits`，所以

```py
generation_output[:2]
```

例如，将返回元组 `(generation_output.sequences, generation_output.scores)`。

当将我们的 `generation_output` 对象用作字典时，它只保留那些没有 `None` 值的属性。在这里，例如，它有两个键，分别是 `sequences` 和 `scores`。

我们在这里记录所有输出类型。

### PyTorch

### `class transformers.generation.GenerateDecoderOnlyOutput`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L96)

```py
( sequences: LongTensor = None scores: Optional = None attentions: Optional = None hidden_states: Optional = None past_key_values: Optional = None )
```

参数

+   `sequences`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`） — 生成的序列。第二维（sequence_length）要么等于 `max_length`，要么如果所有批次由于 `eos_token_id` 提前结束，则要短一些。

+   `scores`（`tuple(torch.FloatTensor)` *可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）— 在每个生成步骤中语言建模头的处理预测分数（SoftMax之前每个词汇标记的分数）。元组`torch.FloatTensor`，最多包含`max_new_tokens`个元素（每个生成的标记一个元素），每个张量的形状为`(batch_size, config.vocab_size)`。

+   `attentions`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 元组（每个生成的标记一个元素）的元组（解码器的每一层一个元素）的`torch.FloatTensor`，形状为`(batch_size, num_heads, generated_length, sequence_length)`。

+   `hidden_states`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 元组（每个生成的标记一个元素）的元组（解码器的每一层一个元素）的`torch.FloatTensor`，形状为`(batch_size, generated_length, hidden_size)`。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）— 注意：一些模型具有不同的`past_key_values`格式，请查阅模型的文档进行确认。通常是一个元组（解码器的每一层一个元素）的元组（两个元素，键张量和值张量）。第一个元组的长度为`config.n_layers`，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`，还有2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

解码器生成模型的输出，在使用非beam方法时。

### `class transformers.generation.GenerateEncoderDecoderOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L131)

```py
( sequences: LongTensor = None scores: Optional = None encoder_attentions: Optional = None encoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None decoder_hidden_states: Optional = None past_key_values: Optional = None )
```

参数

+   `sequences`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。

+   `scores`（`tuple(torch.FloatTensor)` *可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）— 在每个生成步骤中语言建模头的处理预测分数（SoftMax之前每个词汇标记的分数）。元组`torch.FloatTensor`，最多包含`max_new_tokens`个元素（每个生成的标记一个元素），每个张量的形状为`(batch_size, config.vocab_size)`。

+   `encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 元组的`torch.FloatTensor`（解码器的每一层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。

+   `encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 元组的`torch.FloatTensor`（嵌入的输出一个加上每一层的输出一个）的形状为`(batch_size, sequence_length, hidden_size)`。

+   `decoder_attentions`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 元组（每个生成的标记一个元素）的元组（解码器的每一层一个元素）的`torch.FloatTensor`，形状为`(batch_size, num_heads, generated_length, sequence_length)`。

+   `cross_attentions` (`tuple(tuple(torch.FloatTensor))`)，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回 — 每个生成的令牌的元组（每个解码器层一个元素）的`torch.FloatTensor`，形状为`(batch_size, num_heads, generated_length, sequence_length)`。

+   `decoder_hidden_states` (`tuple(tuple(torch.FloatTensor))`)，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回 — 每个生成的令牌的元组（每个解码器层一个元素）的`torch.FloatTensor`的元组，形状为`(batch_size, generated_length, hidden_size)`。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`)，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回 — 注意：一些模型具有不同的`past_key_values`格式，请查阅模型文档确认。通常是一个元组（解码器每一层一个元素）的元组（两个元素，键张量和值张量）。第一个元组的长度为`config.n_layers`，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，并且如果`config.is_encoder_decoder=True`，还有2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

在使用非beam方法时的编码器-解码器生成模型的输出。

### `class transformers.generation.GenerateBeamDecoderOnlyOutput`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L178)

```py
( sequences: LongTensor = None sequences_scores: Optional = None scores: Optional = None beam_indices: Optional = None attentions: Optional = None hidden_states: Optional = None past_key_values: Optional = None )
```

参数

+   `sequences` (`torch.LongTensor`，形状为`(batch_size*num_return_sequences, sequence_length)`) — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前结束，则较短。

+   `sequences_scores` (`torch.FloatTensor`，形状为`(batch_size*num_return_sequences)`)，*可选*，当传递`output_scores=True`或`config.output_scores=True`时返回 — 生成`sequences`的最终beam分数。

+   `scores` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_scores=True`或`config.output_scores=True`时返回 — 每一代步每个词汇令牌的beam转移分数。beam转移分数由tokens的对数概率条件于该beam中先前生成的tokens的对数softmax组成。具有最多`max_new_tokens`元素的`torch.FloatTensor`元组（每个生成的令牌一个元素），每个张量的形状为`(batch_size*num_beams*num_return_sequences, config.vocab_size)`。

+   `beam_indices` (`torch.LongTensor`)，*可选*，当传递`output_scores=True`或`config.output_scores=True`时返回 — 每一代步生成的令牌id的beam索引。形状为`(batch_size*num_return_sequences, sequence_length)`的`torch.LongTensor`。

+   `attentions` (`tuple(tuple(torch.FloatTensor))`)，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回 — 每个生成的令牌的元组（每个解码器层一个元素）的`torch.FloatTensor`的元组，形状为`(batch_size*num_beams, num_heads, generated_length, sequence_length)`。

+   `hidden_states` (`tuple(tuple(torch.FloatTensor))`)，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回 — 每个生成的令牌的元组（每个解码器层一个元素）的`torch.FloatTensor`，形状为`(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor)))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）— 注意：一些模型具有不同的`past_key_values`格式，请查阅模型的文档。通常是一个元组（解码器每一层一个）的元组（两个元素，键张量和值张量）。第一个元组的长度为`config.n_layers`，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`，还有2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

在使用beam方法时，仅解码器生成模型的输出。

### `class transformers.generation.GenerateBeamEncoderDecoderOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L221)

```py
( sequences: LongTensor = None sequences_scores: Optional = None scores: Optional = None beam_indices: Optional = None encoder_attentions: Optional = None encoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None decoder_hidden_states: Optional = None past_key_values: Optional = None )
```

参数

+   `sequences`（`torch.LongTensor`，形状为`(batch_size*num_return_sequences, sequence_length)`）— 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前结束，则较短。

+   `sequences_scores`（`torch.FloatTensor`，形状为`(batch_size*num_return_sequences)`，*可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）— 生成的`sequences`的最终beam分数。

+   `scores`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）— 每一代步骤中每个词汇标记的beam转移分数。Beam转移分数由tokens的log概率组成，条件是该beam中先前生成的tokens的log softmax。`torch.FloatTensor`的元组，最多有`max_new_tokens`个元素（每个生成的token一个元素），每个张量的形状为`(batch_size*num_beams, config.vocab_size)`。

+   `beam_indices`（`torch.LongTensor`，*可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）— 每一代步骤中生成的token id的beam索引。形状为`(batch_size*num_return_sequences, sequence_length)`的`torch.LongTensor`。

+   `encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（解码器每一层一个）。

+   `encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size*num_beams*num_return_sequences, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

+   `decoder_attentions`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 元组（每个生成的token一个元素）的元组（解码器每一层一个）的`torch.FloatTensor`，形状为`(batch_size*num_beams*num_return_sequences, num_heads, generated_length, sequence_length)`。

+   `cross_attentions`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 元组（每个生成的token一个元素）的元组（解码器每一层一个）的`torch.FloatTensor`，形状为`(batch_size, num_heads, generated_length, sequence_length)`。

+   `decoder_hidden_states`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 元组（每个生成的token一个元素）的元组（解码器每一层一个）的`torch.FloatTensor`，形状为`(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor)))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 注意：一些模型具有不同的`past_key_values`格式，请查阅模型文档。通常是一个元组（解码器每层一个元素）的元组（两个元素，键张量和值张量）。第一个元组的长度为`config.n_layers`，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`，还有2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

在使用beam方法时，编码器-解码器生成模型的输出。

### TensorFlow

### `class transformers.generation.TFGreedySearchEncoderDecoderOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L85)

```py
( sequences: Tensor = None scores: Optional = None encoder_attentions: Optional = None encoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None decoder_hidden_states: Optional = None )
```

参数

+   `sequences` (`tf.Tensor`，形状为`(batch_size, sequence_length)`) — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短一些。

+   `scores` (`tuple(tf.Tensor)` *optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回) — 语言建模头的处理预测分数（SoftMax之前每个词汇标记的分数）在每个生成步骤。`tf.Tensor`的元组，最多有`max_new_tokens`个元素（每个生成的标记一个元素），每个张量的形状为`(batch_size, config.vocab_size)`。

+   `encoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。

+   `encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

+   `decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, generated_length, sequence_length)`。

+   `cross_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, generated_length, sequence_length)`。

+   `decoder_hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, generated_length, hidden_size)`。

使用贪婪搜索的编码器-解码器生成模型的输出的基类。可以通过`encoder_attentions`和`encoder_hidden_states`属性（或`decoder_attentions`和`decoder_hidden_states`属性）访问解码器（或编码器）的隐藏状态和注意力权重。

### `class transformers.generation.TFGreedySearchDecoderOnlyOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L57)

```py
( sequences: Tensor = None scores: Optional = None attentions: Optional = None hidden_states: Optional = None )
```

参数

+   `sequences` (`tf.Tensor`，形状为`(batch_size, sequence_length)`) — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短一些。

+   `scores` (`tuple(tf.Tensor)` *optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回) — 每个生成步骤的语言建模头的处理预测分数（SoftMax之前每个词汇标记的分数）。具有最多`max_new_tokens`元素的`tf.Tensor`元组（每个生成的标记一个元素），每个张量的形状为`(batch_size, config.vocab_size)`。

+   `attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 元组（每个生成的标记一个元素）的元组（解码器每一层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, generated_length, sequence_length)`。

+   `hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 元组（每个生成的标记一个元素）的元组（解码器每一层一个元素）的`tf.Tensor`，形状为`(batch_size, generated_length, hidden_size)`。

仅解码器生成模型使用贪婪搜索的输出的基类。

### `class transformers.generation.TFSampleEncoderDecoderOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L155)

```py
( sequences: Tensor = None scores: Optional = None encoder_attentions: Optional = None encoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None decoder_hidden_states: Optional = None )
```

参数

+   `sequences` (`tf.Tensor`，形状为`(batch_size*num_return_sequences, sequence_length)`) — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则较短。

+   `scores` (`tuple(tf.Tensor)` *optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回) — 每个生成步骤的语言建模头的处理预测分数（SoftMax之前每个词汇标记的分数）。具有最多`max_new_tokens`元素的`tf.Tensor`元组（每个生成的标记一个元素），每个张量的形状为`(batch_size*num_return_sequences, config.vocab_size)`。

+   `encoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size*num_return_sequences, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个解码器层一个）。

+   `encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size*num_return_sequences, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

+   `decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 元组（每个生成的标记一个元素）的元组（解码器每一层一个元素）的`tf.Tensor`，形状为`(batch_size*num_return_sequences, num_heads, generated_length, sequence_length)`。

+   `cross_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 元组（每个生成的标记一个元素）的元组（解码器每一层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, generated_length, sequence_length)`。

+   `decoder_hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 元组（每个生成的标记一个元素）的元组（解码器每一层一个元素）的`tf.Tensor`，形状为`(batch_size*num_return_sequences, generated_length, hidden_size)`。

使用采样的编码器-解码器生成模型的输出的基类。可以通过`encoder_attentions`和`encoder_hidden_states`属性（或`decoder_attentions`和`decoder_hidden_states`属性）访问解码器（或编码器）的隐藏状态和注意力权重。

### `class transformers.generation.TFSampleDecoderOnlyOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L127)

```py
( sequences: Tensor = None scores: Optional = None attentions: Optional = None hidden_states: Optional = None )
```

参数

+   `sequences` (`tf.Tensor`，形状为`(batch_size*num_return_sequences, sequence_length)`) — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。

+   `scores` (`tuple(tf.Tensor)`，*optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回) — 语言建模头的预测分数（SoftMax之前的每个词汇标记的分数）在每个生成步骤。`tf.Tensor`的元组，最多有`max_new_tokens`个元素（每个生成的标记一个元素），每个张量的形状为`(batch_size*num_return_sequences, config.vocab_size)`。

+   `attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 元组（每个生成的标记一个元素）的元组（解码器每一层一个元素）的`tf.Tensor`，形状为`(num_return_sequences*batch_size, num_heads, generated_length, sequence_length)`。

+   `hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 元组（每个生成的标记一个元素）的元组（解码器每一层一个元素）的`tf.Tensor`，形状为`(num_return_sequences*batch_size, generated_length, hidden_size)`。

用于仅使用采样的解码器生成模型的输出的基类。

### `class transformers.generation.TFBeamSearchEncoderDecoderOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L232)

```py
( sequences: Tensor = None sequences_scores: Optional = None scores: Optional = None beam_indices: Optional = None encoder_attentions: Optional = None encoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None decoder_hidden_states: Optional = None )
```

参数

+   `sequences` (`tf.Tensor`，形状为`(batch_size*num_return_sequences, sequence_length)`) — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。

+   `sequences_scores` (`tf.Tensor`，形状为`(batch_size*num_return_sequences)`，*optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回) — 生成的`sequences`的最终beam分数。

+   `scores` (`tuple(tf.Tensor)`，*optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回) — 每个生成步骤中每个词汇标记的处理过的beam分数。beam分数由每个词汇标记的log softmax分数和此beam中先前生成标记的log softmax之和组成。`tf.Tensor`的元组，最多有`max_new_tokens`个元素（每个生成的标记一个元素），每个张量的形状为`(batch_size*num_beams, config.vocab_size)`。

+   `beam_indices` (`tf.Tensor`，*optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回) — 每个生成步骤中生成的标记id的beam索引。形状为`(batch_size*num_return_sequences, sequence_length)`的`tf.Tensor`。

+   `encoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（解码器每一层一个）。

+   `encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size*num_beams*num_return_sequences, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

+   `decoder_attentions`（`tuple(tuple(tf.Tensor))`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 元组（每个生成的token一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size*num_beams*num_return_sequences, num_heads, generated_length, sequence_length)`。

+   `cross_attentions`（`tuple(tuple(tf.Tensor))`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 元组（每个生成的token一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, generated_length, sequence_length)`。

+   `decoder_hidden_states`（`tuple(tuple(tf.Tensor))`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 元组（每个生成的token一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`。

基类，用于使用beam搜索的编码器-解码器生成模型的输出。可以通过`encoder_attentions`和`encoder_hidden_states`属性（分别是`decoder_attentions`和`decoder_hidden_states`属性）访问解码器（或编码器）的隐藏状态和注意力权重。

### `class transformers.generation.TFBeamSearchDecoderOnlyOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L197)

```py
( sequences: Tensor = None sequences_scores: Optional = None scores: Optional = None beam_indices: Optional = None attentions: Optional = None hidden_states: Optional = None )
```

参数

+   `sequences`（形状为`(batch_size*num_return_sequences, sequence_length)`的`tf.Tensor`）— 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。

+   `sequences_scores`（形状为`(batch_size*num_return_sequences)`的`tf.Tensor`，*可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）— 生成的`sequences`的最终beam分数。

+   `scores`（`tuple(tf.Tensor)` *可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）— 每个生成步骤中每个词汇标记的处理过的beam分数。beam分数由每个词汇标记的log softmax分数和此beam中先前生成的标记的log softmax之和组成。具有最多`max_new_tokens`元素的`tf.Tensor`元组（每个生成的token一个元素），每个张量的形状为`(batch_size*num_beams*num_return_sequences, config.vocab_size)`。

+   `beam_indices`（`tf.Tensor`，*可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）— 每个生成步骤中生成的token id的beam索引。形状为`(batch_size*num_return_sequences, sequence_length)`的`tf.Tensor`。

+   `attentions`（`tuple(tuple(tf.Tensor))`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 元组（每个生成的token一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size*num_beams, num_heads, generated_length, sequence_length)`。

+   `hidden_states`（`tuple(tuple(tf.Tensor))`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 元组（每个生成的token一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`。

用于使用beam搜索的解码器生成模型的输出的基类。

### `class transformers.generation.TFBeamSampleEncoderDecoderOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L317)

```py
( sequences: Tensor = None sequences_scores: Optional = None scores: Optional = None beam_indices: Optional = None encoder_attentions: Optional = None encoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None decoder_hidden_states: Optional = None )
```

参数

+   `sequences` (`tf.Tensor`，形状为`(batch_size*num_beams, sequence_length)`) — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则较短。

+   `sequences_scores` (`tf.Tensor`，形状为`(batch_size * num_return_sequence)`，*optional*，当传递`output_scores=True`或`config.output_scores=True`时返回) — 生成的`sequences`的最终beam分数。

+   `scores` (`tuple(tf.Tensor)`，*optional*，当传递`output_scores=True`或`config.output_scores=True`时返回) — 每个生成步骤中每个词汇标记的处理beam分数。beam分数由每个词汇标记的log softmax分数和此beam中先前生成标记的log softmax之和组成。具有最多`max_new_tokens`元素的`tf.Tensor`元组（每个生成的标记一个元素），每个张量形状为`(batch_size*num_beams, config.vocab_size)`。

+   `beam_indices` (`tf.Tensor`，*optional*，当传递`output_scores=True`或`config.output_scores=True`时返回) — 每个生成步骤中生成的标记id的beam索引。形状为`(batch_size*num_return_sequences, sequence_length)`的`tf.Tensor`。

+   `encoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个解码器层一个）。

+   `encoder_hidden_states` (`tuple(tf.Tensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size*num_beams, sequence_length, hidden_size)`的`tf.Tensor`元组（嵌入输出和每个层输出各一个）。

+   `decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 元组（每个生成的标记一个元素）的元组（每个解码器层一个）的`tf.Tensor`，形状为`(batch_size*num_beams, num_heads, generated_length, sequence_length)`。

+   `cross_attentions` (`tuple(tuple(tf.Tensor))`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 元组（每个生成的标记一个元素）的元组（每个解码器层一个）的`tf.Tensor`，形状为`(batch_size, num_heads, generated_length, sequence_length)`。

+   `decoder_hidden_states` (`tuple(tuple(tf.Tensor))`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 元组（每个生成的标记一个元素）的元组（每个解码器层一个）的`tf.Tensor`，形状为`(batch_size*num_beams, generated_length, hidden_size)`。

用于使用beam采样的编码器-解码器生成模型输出的基类。可以通过`encoder_attentions`和`encoder_hidden_states`属性（分别为`decoder_attentions`和`decoder_hidden_states`属性）访问解码器（或编码器）的隐藏状态和注意力权重。

### `class transformers.generation.TFBeamSampleDecoderOnlyOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L282)

```py
( sequences: Tensor = None sequences_scores: Optional = None scores: Optional = None beam_indices: Optional = None attentions: Optional = None hidden_states: Optional = None )
```

参数

+   `sequences` (`tf.Tensor`，形状为`(batch_size*num_return_sequences, sequence_length)`) — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则较短。

+   `sequences_scores` (`tf.Tensor`，形状为`(batch_size * num_return_sequence)`，*optional*，当传递`output_scores=True`或`config.output_scores=True`时返回) — 生成的`sequences`的最终beam分数。

+   `scores` (`tuple(tf.Tensor)` *optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回) — 每个生成步骤中每个词汇标记的处理beam分数。beam分数由每个词汇标记的log softmax分数和此beam中先前生成的标记的log softmax之和组成。具有最多`max_new_tokens`元素的`tf.Tensor`元组（每个生成的标记一个元素），每个张量的形状为`(batch_size*num_beams*num_return_sequences, config.vocab_size)`。

+   `beam_indices` (`tf.Tensor`, *optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回) — 每个生成步骤中生成的标记id的beam索引。形状为`(batch_size*num_return_sequences, sequence_length)`的`tf.Tensor`。

+   `attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size*num_beams, num_heads, generated_length, sequence_length)`。

+   `hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size*num_beams, generated_length, hidden_size)`。

用于使用beam sample的仅解码器生成模型的输出的基类。

### `class transformers.generation.TFContrastiveSearchEncoderDecoderOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L393)

```py
( sequences: Tensor = None scores: Optional = None encoder_attentions: Optional = None encoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None decoder_hidden_states: Optional = None )
```

参数

+   `sequences` (`tf.Tensor`，形状为`(batch_size, sequence_length)`) — 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。 

+   `scores` (`tuple(tf.Tensor)` *optional*, 当传递`output_scores=True`或`config.output_scores=True`时返回) — 在每个生成步骤中语言建模头的处理预测分数（SoftMax之前每个词汇标记的分数）。具有最多`max_new_tokens`元素的`tf.Tensor`元组（每个生成的标记一个元素），每个张量的形状为`(batch_size, config.vocab_size)`。

+   `encoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, sequence_length, sequence_length)`。

+   `encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

+   `decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, generated_length, sequence_length)`。

+   `cross_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, num_heads, generated_length, sequence_length)`。

+   `decoder_hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的`tf.Tensor`，形状为`(batch_size, generated_length, hidden_size)`。

用于使用对比搜索的编码器-解码器生成模型的输出的基类。可以通过`encoder_attentions`和`encoder_hidden_states`属性（分别是`decoder_attentions`和`decoder_hidden_states`属性）访问解码器（或编码器）的隐藏状态和注意力权重。

### `class transformers.generation.TFContrastiveSearchDecoderOnlyOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L366)

```py
( sequences: Tensor = None scores: Optional = None attentions: Optional = None hidden_states: Optional = None )
```

参数

+   `sequences`（形状为`(batch_size, sequence_length)`的`tf.Tensor`）— 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前结束，则要短。

+   `scores`（`tuple(tf.Tensor)` *可选*，当传递`output_scores=True`或`config.output_scores=True`时返回）— 语言建模头的处理预测分数（SoftMax之前每个词汇标记的分数）在每个生成步骤。每个生成标记一个元素，每个张量形状为`(batch_size, config.vocab_size)`的`tf.Tensor`的元组，最多有`max_new_tokens`个元素。

+   `attentions`（`tuple(tuple(tf.Tensor))`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的形状为`(batch_size, num_heads, generated_length, sequence_length)`的`tf.Tensor`。

+   `hidden_states`（`tuple(tuple(tf.Tensor))`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 元组（每个生成的标记一个元素）的元组（解码器每层一个元素）的形状为`(batch_size, generated_length, hidden_size)`的`tf.Tensor`。

用于仅使用对比搜索生成模型的输出的基类。

### FLAX

### `class transformers.generation.FlaxSampleOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L68)

```py
( sequences: Array = None )
```

参数

+   `sequences`（形状为`(batch_size, max_length)`的`jnp.ndarray`）— 生成的序列。

Flax基类，用于仅使用抽样生成模型的输出。

#### `替换`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)

```py
( **updates )
```

“返回一个用新值替换指定字段的新对象。

### `class transformers.generation.FlaxGreedySearchOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L54)

```py
( sequences: Array = None )
```

参数

+   `sequences`（形状为`(batch_size, max_length)`的`jnp.ndarray`）— 生成的序列。

Flax基类，用于仅使用贪婪搜索生成模型的输出。

#### `替换`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)

```py
( **updates )
```

“返回一个用新值替换指定字段的新对象。

### `class transformers.generation.FlaxBeamSearchOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L82)

```py
( sequences: Array = None scores: Array = None )
```

参数

+   `sequences`（形状为`(batch_size, max_length)`的`jnp.ndarray`）— 生成的序列。

+   `scores`（形状为`(batch_size,)`的`jnp.ndarray`）— 生成序列的分数（对数概率）。

Flax基类，用于仅使用贪婪搜索生成模型的输出。

#### `替换`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)

```py
( **updates )
```

“返回一个用新值替换指定字段的新对象。

## LogitsProcessor

[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)可以用于修改语言模型头的预测分数以进行生成。

### PyTorch

### `class transformers.AlternatingCodebooksLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L2011)

```py
( input_start_len: int semantic_vocab_size: int codebook_size: int )
```

参数

+   `input_start_len`（`int`）— 初始输入序列的长度。

+   `semantic_vocab_size`（`int`）— 语义部分的词汇表大小，即与语义词汇表相关联的标记数。

+   `codebook_size`（`int`）— 与代码簿相关联的标记数。

[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor) 强制在Bark的两个代码簿之间交替生成。

此对数处理器仅与[Bark](https://huggingface.co/docs/transformers/en/model_doc/bark)的精细子模型兼容。请参阅模型文档以获取示例。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L2040)

```py
( input_ids: LongTensor scores: FloatTensor )
```

### `class transformers.ClassifierFreeGuidanceLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1947)

```py
( guidance_scale )
```

参数

+   `guidance_scale`（浮点数）— 用于分类器自由引导（CFG）的引导比例。通过设置`guidance_scale > 1`来启用CFG。更高的引导比例鼓励模型生成与输入提示更紧密相关的样本，通常以牺牲质量为代价。

[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor) 用于分类器自由引导（CFG）。分数在批处理维度上分割，其中前半部分对应条件对数（从输入提示预测），后半部分对应无条件对数（从空或“null”提示预测）。处理器计算条件和无条件对数之间的加权平均值，由`guidance_scale`参数化。

有关更多信息，请参阅[论文](https://arxiv.org/abs/2306.05284)。

此对数处理器仅与[MusicGen](https://huggingface.co/docs/transformers/main/en/model_doc/musicgen)兼容

示例：

```py
>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration

>>> processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
>>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

>>> inputs = processor(
...     text=["80s pop track with bassy drums and synth", "90s rock song with loud guitars and heavy drums"],
...     padding=True,
...     return_tensors="pt",
... )
>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1995)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores`（形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的对数，或者在使用波束搜索时，可以是每个词汇表标记的对数softmax

返回

`torch.FloatTensor`的形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.EncoderNoRepeatNGramLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L874)

```py
( encoder_ngram_size: int encoder_input_ids: LongTensor )
```

参数

+   `encoder_ngram_size`（`int`）— 所有大小为`ngram_size`的ngram只能出现在编码器输入ID中。

+   `encoder_input_ids`（`int`）— 不应在解码器ID中重复的编码器输入ID。

[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)，其工作方式类似于[NoRepeatNGramLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.NoRepeatNGramLogitsProcessor)，但专门用于防止在提示中重复出现的n-gram。

它旨在通过阻止生成先前对话轮中存在的n-gram来促进语言模型中的喋喋不休。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")
>>> tokenizer = AutoTokenizer.from_pretrained("bigscience/bloomz-560m")

>>> inputs = tokenizer("Alice: I love cats. What do you love?\nBob:", return_tensors="pt")

>>> # With greedy decoding, we see Bob repeating Alice's opinion. If Bob was a chatbot, it would be a poor one.
>>> outputs = model.generate(**inputs)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])
Alice: I love cats. What do you love?
Bob: I love cats. What do you

>>> # With this logits processor, we can prevent Bob from repeating Alice's opinion.
>>> outputs = model.generate(**inputs, encoder_no_repeat_ngram_size=2)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])
Alice: I love cats. What do you love?
Bob: My cats are very cute.
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L923)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入标识？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为 `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的对数，或者在使用波束搜索时，可以是每个词汇表标记的对数softmax

返回

`torch.FloatTensor`，形状为 `(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.EncoderRepetitionPenaltyLogitsProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L342)

```py
( penalty: float encoder_input_ids: LongTensor )
```

参数

+   `penalty` (`float`) — 重复惩罚的参数。1.0 表示没有惩罚。大于 1.0 奖励提示标记。在 0.0 和 1.0 之间惩罚提示标记。

+   `encoder_input_ids` (`torch.LongTensor`) — 应在解码器标识中重复的编码器输入标识。

[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor) 与 [RepetitionPenaltyLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.RepetitionPenaltyLogitsProcessor) 类似，但应用于提示中存在的标记的*反向*惩罚。换句话说，大于 1.0 的惩罚增加了选择提示中存在的标记的几率。

它旨在避免输入驱动任务中的幻觉，如摘要。虽然最初是为编码器-解码器模型设计的，但也可以与仅解码器模型（如LLMs）一起使用。

示例：

```py
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bigscience/bloomz-560m")
>>> model = AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")

>>> inputs = tokenizer(["Alice and Bob. The third member's name was"], return_tensors="pt")
>>> gen_out = model.generate(**inputs)
>>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])
Alice and Bob. The third member's name was not mentioned.

>>> # With the `encoder_repetition_penalty` argument we can trigger this logits processor in `generate`, which can
>>> # promote the use of prompt tokens ("Bob" in this example)
>>> gen_out = model.generate(**inputs, encoder_repetition_penalty=1.2)
>>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])
Alice and Bob. The third member's name was Bob. The third member's name was Bob.
```

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L386)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入标识？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为 `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的对数，或者在使用波束搜索时，可以是每个词汇表标记的对数softmax

返回

`torch.FloatTensor`，形状为 `(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.EpsilonLogitsWarper`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L603)

```py
( epsilon: float filter_value: float = -inf min_tokens_to_keep: int = 1 )
```

参数

+   `epsilon` (`float`) — 如果设置为 > 0，则只保留概率大于 `epsilon` 的最多的标记用于生成。

+   `filter_value` (`float`，*可选*，默认为 -inf) — 所有过滤值将设置为此浮点值。

+   `min_tokens_to_keep` (`int`，*可选*，默认为 1) — 不能被过滤的最小标记数。

[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper) 执行 epsilon-sampling，即限制到概率 `prob >= epsilon` 的标记。如果没有标记满足此约束，则取最大的 min_tokens_to_keep 个标记。有关更多信息，请参阅[截断抽样作为语言模型去平滑](https://arxiv.org/abs/2210.15191)。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed

>>> set_seed(0)
>>> model = AutoModelForCausalLM.from_pretrained("distilgpt2")
>>> tokenizer = AutoTokenizer.from_pretrained("distilgpt2")

>>> inputs = tokenizer("A sequence: 1, 2", return_tensors="pt")

>>> # With sampling, the output is unexpected -- sometimes too unexpected.
>>> outputs = model.generate(**inputs, do_sample=True)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])
A sequence: 1, 2, 0, 2, 2. 2, 2, 2, 2

>>> # With epsilon sampling, the output gets restricted to high-probability tokens. Note that this is similar to
>>> # Top P sampling, which restricts tokens based on their cumulative probability.
>>> # Pro tip: The paper recomends using `epsilon_cutoff` values between 3e-4 and 9e-4
>>> outputs = model.generate(**inputs, do_sample=True, epsilon_cutoff=0.1)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])
A sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9
```

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L656)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入标识？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为 `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的对数，或者在使用波束搜索时，可以是每个词汇表标记的对数softmax

返回

`torch.FloatTensor`，形状为 `(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.EtaLogitsWarper`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L670)

```py
( epsilon: float filter_value: float = -inf min_tokens_to_keep: int = 1 )
```

参数

+   `epsilon` (`float`) — 在范围(0, 1)内的浮点值。用于计算动态截断值`eta`的超参数。根据论文，建议的值范围为3e-4到4e-3，具体取决于模型的大小。

+   `filter_value` (`float`，*可选*，默认为-inf) — 所有低于动态截断值`eta`的值都设置为此浮点值。当需要修改logits以排除生成过程中应完全排除的概率非常低的标记时，此参数很有用。

+   `min_tokens_to_keep` (`int`，*可选*，默认为1) — 指定必须保留的最小标记数，无论它们的概率如何。例如，如果将`min_tokens_to_keep`设置为1，则始终会保留至少一个标记用于生成，即使所有标记的概率都低于截断`eta`。

[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)执行eta采样，一种过滤掉概率低于动态截断值`eta`的标记的技术，该值是基于超参数`epsilon`和标记概率的熵的组合计算得出的，即`eta := min(epsilon, sqrt(epsilon * e^-entropy(probabilities)))`。如果没有标记满足此约束，则保留最大的`min_tokens_to_keep`个标记。它解决了由神经语言模型生成的长文本样本中存在的质量差问题，从而生成更连贯和流畅的文本。有关更多信息，请参阅[截断采样作为语言模型去平滑](https://arxiv.org/abs/2210.15191)。注意：必须将`do_sample`设置为`True`，才能使此`LogitsWarper`正常工作。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed

>>> set_seed(0)
>>> model = AutoModelForCausalLM.from_pretrained("distilgpt2")
>>> tokenizer = AutoTokenizer.from_pretrained("distilgpt2")

>>> inputs = tokenizer("A sequence: 1, 2", return_tensors="pt")

>>> # With sampling, the output is unexpected -- sometimes too unexpected.
>>> outputs = model.generate(**inputs, do_sample=True)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])
A sequence: 1, 2, 0, 2, 2. 2, 2, 2, 2

>>> # With eta sampling, the output gets restricted to high-probability tokens. You can see it as a dynamic form of
>>> # epsilon sampling that adapts its cutoff probability based on the entropy (high entropy = lower cutoff).
>>> # Pro tip: The paper recomends using `eta_cutoff` values between 3e-4 to 4e-3
>>> outputs = model.generate(**inputs, do_sample=True, eta_cutoff=0.1)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])
A sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L733)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log softmax

返回

形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`

处理后的预测分数。

### `class transformers.ExponentialDecayLengthPenalty`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1494)

```py
( exponential_decay_length_penalty: Tuple eos_token_id: Union input_ids_seq_length: int )
```

参数

+   `exponential_decay_length_penalty` (`tuple(int, float)`) — 此元组应包含：`(start_index, decay_factor)`，其中`start_index`表示惩罚开始的位置，`decay_factor`表示指数衰减的因子

+   `eos_token_id` (`Union[int, List[int]]`) — *序列结束*标记的ID。可选择使用列表设置多个*序列结束*标记。

+   `input_ids_seq_length` (`int`) — 输入序列的长度。

[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)在达到`start_index`后指数增加`eos_token_id`的分数。这允许生成较短的序列而不会有硬性截断，从而使`eos_token`能够在有意义的位置被预测。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed

>>> model = AutoModelForCausalLM.from_pretrained("gpt2")
>>> tokenizer = AutoTokenizer.from_pretrained("gpt2")

>>> text = "Just wanted to let you know, I"
>>> inputs = tokenizer(text, return_tensors="pt")

>>> # Let's consider that we want short sentences, so we limit `max_length=30`. However, we observe that the answer
>>> # tends to end abruptly.
>>> set_seed(1)
>>> outputs = model.generate(**inputs, do_sample=True, temperature=0.9, max_length=30, pad_token_id=50256)
>>> print(tokenizer.batch_decode(outputs)[0])
Just wanted to let you know, I received a link to an ebook, the book How To Start A Social Network which was
published in 2010. Although

>>> # To promote the appearance of the EOS token at the right time, we add the `exponential_decay_length_penalty =
>>> # (start_index, decay_factor)`. Instead of cutting at max_tokens, the output comes to an end before and usually
>>> # with more meaning. What happens is that starting from `start_index` the EOS token score will be increased
>>> # by `decay_factor` exponentially. However, if you set a high decay factor, you may also end up with abruptly
>>> # ending sequences.
>>> set_seed(1)
>>> outputs = model.generate(
...     **inputs,
...     do_sample=True,
...     temperature=0.9,
...     max_length=30,
...     pad_token_id=50256,
...     exponential_decay_length_penalty=(15, 1.6),
... )
>>> print(tokenizer.batch_decode(outputs)[0])
Just wanted to let you know, I received a link to an ebook, the book How To Start A Social Network
which<|endoftext|>

>>> # With a small decay factor, you will have a higher chance of getting a meaningful sequence.
>>> set_seed(1)
>>> outputs = model.generate(
...     **inputs,
...     do_sample=True,
...     temperature=0.9,
...     max_length=30,
...     pad_token_id=50256,
...     exponential_decay_length_penalty=(15, 1.01),
... )
>>> print(tokenizer.batch_decode(outputs)[0])
Just wanted to let you know, I received a link to an ebook, the book How To Start A Social Network which was
published in 2010.<|endoftext|>
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1574)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log softmax。

返回

`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.ForcedBOSTokenLogitsProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1378)

```py
( bos_token_id: int )
```

参数

+   `bos_token_id` (`int`) — 强制作为第一个生成的标记的标记ID。

[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor) 会强制指定的标记作为第一个生成的标记。与编码器-解码器模型一起使用。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

>>> model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
>>> tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")

>>> inputs = tokenizer("Translate from English to German: I love cats.", return_tensors="pt")

>>> # By default, it continues generating according to the model's logits
>>> outputs = model.generate(**inputs, max_new_tokens=10)
>>> print(tokenizer.batch_decode(outputs)[0])
<pad> Ich liebe Kitty.</s>

>>> # We can use `forced_bos_token_id` to force the start of generation with an encoder-decoder model
>>> # (including forcing it to end straight away with an EOS token)
>>> outputs = model.generate(**inputs, max_new_tokens=10, forced_bos_token_id=tokenizer.eos_token_id)
>>> print(tokenizer.batch_decode(outputs)[0])
<pad></s>
```

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1413)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log softmax。

返回

`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.ForcedEOSTokenLogitsProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1423)

```py
( max_length: int eos_token_id: Union )
```

参数

+   `max_length` (`int`) — 要生成的序列的最大长度。

+   `eos_token_id` (`Union[int, List[int]]`) — 当达到`max_length`时，强制指定的标记作为最后生成的标记。可选择使用列表设置多个*序列结束*标记。

[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor) 会在达到`max_length`时强制指定的标记作为最后生成的标记。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("distilgpt2")
>>> tokenizer = AutoTokenizer.from_pretrained("distilgpt2")

>>> inputs = tokenizer("A sequence: 1, 2, 3", return_tensors="pt")

>>> # By default, it continues generating according to the model's logits
>>> outputs = model.generate(**inputs, max_new_tokens=10)
>>> print(tokenizer.batch_decode(outputs)[0])
A sequence: 1, 2, 3, 4, 5, 6, 7, 8

>>> # `forced_eos_token_id` ensures the generation ends with a EOS token
>>> outputs = model.generate(**inputs, max_new_tokens=10, forced_eos_token_id=tokenizer.eos_token_id)
>>> print(tokenizer.batch_decode(outputs)[0])
A sequence: 1, 2, 3, 4, 5, 6, 7,<|endoftext|>
```

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1462)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log softmax。

返回

`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.ForceTokensLogitsProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1710)

```py
( force_token_map: List )
```

这个处理器接受一对整数的列表，指示从生成索引到强制生成之前的标记索引的映射。处理器将它们的log概率设置为`inf`，以便在相应的索引处对它们进行采样。最初为[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)创建。

示例：

```py
>>> from transformers import AutoProcessor, WhisperForConditionalGeneration
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("openai/whisper-tiny.en")
>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-tiny.en")
>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> inputs = processor(ds[0]["audio"]["array"], return_tensors="pt")

>>> # This Whisper model forces the generation to start with `50362` at the first position by default, i.e.
>>> # `"forced_decoder_ids": [[1, 50362]]`. This means all other tokens are masked out.
>>> outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)
>>> print(
...     all(outputs.scores[0][0, i] == float("-inf") for i in range(processor.tokenizer.vocab_size) if i != 50362)
... )
True
>>> print(outputs.scores[0][0, 50362])
tensor(0.)

>>> # If we disable `forced_decoder_ids`, we stop seeing that effect
>>> outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True, forced_decoder_ids=None)
>>> print(
...     all(outputs.scores[0][0, i] == float("-inf") for i in range(processor.tokenizer.vocab_size) if i != 50362)
... )
False
>>> print(outputs.scores[0][0, 50362])
tensor(19.3140)
```

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1751)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log softmax。

返回

形状为 `(batch_size, config.vocab_size)` 的 `torch.FloatTensor`

处理后的预测分数。

### `class transformers.HammingDiversityLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1245)

```py
( diversity_penalty: float num_beams: int num_beam_groups: int )
```

参数

+   `diversity_penalty` (`float`) — 如果一个 beam 在特定时间生成与其他组中的任何 beam 相同的标记，则从该 beam 的分数中减去此值。较高的 `diversity_penalty` 将强制在 beam 之间实现更大的多样性。调整此值可以帮助在多样性和自然可能性之间取得平衡。

+   `num_beams` (`int`) — beam 搜索的数量。1 表示没有 beam 搜索。

+   `num_beam_groups` (`int`) — 将 `num_beams` 分成多少组，以确保不同组的 beam 之间的多样性。[此论文](https://arxiv.org/pdf/1610.02424.pdf) 了解更多细节。

[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor) 用于强制进行多样性 beam 搜索。

请注意，此 logits 处理器仅对 [PreTrainedModel.group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search) 有效。有关更多细节，请参阅 [Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/pdf/1610.02424.pdf)。

传统的 beam 搜索经常在不同 beam 之间生成非常相似的序列。`HammingDiversityLogitsProcessor` 通过惩罚在同一时间步生成已被其他 beam 选择的标记的 beam 来解决这个问题。

示例:

```py
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
>>> import torch

>>> # Initialize the model and tokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("t5-base")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

>>> # A long text about the solar system
>>> text = (
...     "The Solar System is a gravitationally bound system comprising the Sun and the objects that orbit it, "
...     "either directly or indirectly. Of the objects that orbit the Sun directly, the largest are the eight "
...     "planets, with the remainder being smaller objects, such as the five dwarf planets and small Solar System "
...     "bodies. The Solar System formed 4.6 billion years ago from the gravitational collapse of a giant "
...     "interstellar molecular cloud."
... )
>>> inputs = tokenizer("summarize: " + text, return_tensors="pt")

>>> # Generate diverse summary
>>> outputs_diverse = model.generate(
...     **inputs,
...     num_beam_groups=2,
...     diversity_penalty=10.0,
...     max_length=100,
...     num_beams=4,
...     num_return_sequences=2,
... )
>>> summaries_diverse = tokenizer.batch_decode(outputs_diverse, skip_special_tokens=True)

>>> # Generate non-diverse summary
>>> outputs_non_diverse = model.generate(
...     **inputs,
...     max_length=100,
...     num_beams=4,
...     num_return_sequences=2,
... )
>>> summary_non_diverse = tokenizer.batch_decode(outputs_non_diverse, skip_special_tokens=True)

>>> # With `diversity_penalty`, the resulting beams are much more diverse
>>> print(summary_non_diverse)
['the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.',
'the Solar System formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.']

>>> print(summaries_diverse)
['the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.',
'the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets. the rest of the objects are smaller objects, such as the five dwarf planets and small solar system bodies.']
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1332)

```py
( input_ids: LongTensor scores: FloatTensor current_tokens: LongTensor beam_group_idx: int ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入 ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为 `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用 beam 搜索时，这些可以是每个词汇表的 logits，或者在使用 beam 搜索时，可以是每个词汇表标记的 log softmax

+   `current_tokens` (`torch.LongTensor`，形状为 `(batch_size)`) — 输入序列标记在词汇表中的索引，对应于当前生成步骤中其他 beam 组选择的标记。

+   `beam_group_idx` (`int`) — 当前正在处理的 beam 组的索引。

返回

形状为 `(batch_size, config.vocab_size)` 的 `torch.FloatTensor`

处理后的预测分数。

### `class transformers.InfNanRemoveLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1473)

```py
( )
```

[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor) 用于移除所有的 `nan` 和 `inf` 值，以避免生成方法失败。请注意，只有在必要时才应该使用 logits 处理器，因为它可能会减慢生成方法的速度。

这个 logits 处理器没有 `generate` 示例，因为不应该有正确的标志组合来保证其使用。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1482)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入 ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为 `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用 beam 搜索时，这些可以是每个词汇表的 logits，或者在使用 beam 搜索时，可以是每个词汇表标记的 log softmax

返回值

形状为 `(batch_size, config.vocab_size)` 的 `torch.FloatTensor`

处理后的预测分数。

### `class transformers.LogitNormalization`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1585)

```py
( )
```

[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)和[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)用于使用log-softmax对分数进行归一化。在应用logits处理器或warper后，在波束搜索期间对分数进行归一化是很重要的，因为此库中使用的搜索算法不会这样做（它只在之前这样做，但它们可能需要重新归一化），但它仍然假设在比较假设时分数已经归一化。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> import torch

>>> model = AutoModelForCausalLM.from_pretrained("distilgpt2")
>>> tokenizer = AutoTokenizer.from_pretrained("distilgpt2")

>>> inputs = tokenizer("A sequence: 1, 2, 3", return_tensors="pt")

>>> # By default, the scores are not normalized -- the sum of their exponentials is NOT a normalized probability
>>> # distribution, summing to 1
>>> outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)
>>> print(torch.sum(torch.exp(outputs.scores[-1])))
tensor(816.3250)

>>> # Normalizing them may have a positive impact on beam methods, or when using the scores on your application
>>> outputs = model.generate(**inputs, renormalize_logits=True, return_dict_in_generate=True, output_scores=True)
>>> print(torch.sum(torch.exp(outputs.scores[-1])))
tensor(1.0000)
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1616)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log softmax。

返回

`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.LogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L44)

```py
( )
```

所有在生成过程中可以应用的logit处理器的抽象基类。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L47)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log softmax。

返回

`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.LogitsProcessorList`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L64)

```py
( iterable = () )
```

这个类可以用来创建一个[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)或[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)列表，以后处理`scores`输入张量。这个类继承自列表，并添加了一个特定的***call***方法来应用每个[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)或[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)到输入中。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L71)

```py
( input_ids: LongTensor scores: FloatTensor **kwargs ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log softmax

+   `kwargs` (`Dict[str, Any]`，*可选*) — 特定于logits处理器的其他kwargs。

返回

`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.LogitsWarper`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L54)

```py
( )
```

所有可以在使用多项式采样进行生成时应用的对数变换器的抽象基类。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L57)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores`（形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的对数，或者在使用波束搜索时，可以是每个词汇表标记的对数softmax

返回

形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`

处理后的预测分数。

### `class transformers.MinLengthLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L102)

```py
( min_length: int eos_token_id: Union )
```

参数

+   `min_length`（`int`）— 小于此长度时，`eos_token_id`的分数将设置为`-float("Inf")`。

+   `eos_token_id`（`Union[int, List[int]]`）— *end-of-sequence*标记的ID。可选地，使用列表设置多个*end-of-sequence*标记。

LogitsProcessor通过将EOS概率设置为0来强制最小长度。请注意，对于像大多数LLMs这样的仅解码器模型，长度包括提示。

示例：

```py
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bigscience/bloomz-560m")
>>> model = AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")

>>> inputs = tokenizer("A number:", return_tensors="pt")
>>> gen_out = model.generate(**inputs)
>>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])
A number: one

>>> # setting `min_length` to a value smaller than the uncontrolled output length has no impact
>>> gen_out = model.generate(**inputs, min_length=3)
>>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])
A number: one

>>> # setting a larger `min_length` will force the model to generate beyond its natural ending point, which is not
>>> # necessarily incorrect
>>> gen_out = model.generate(**inputs, min_length=10)
>>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])
A number: one thousand, nine hundred and ninety-four
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L151)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores`（形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的对数，或者在使用波束搜索时，可以是每个词汇表标记的对数softmax

返回

形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`

处理后的预测分数。

### `class transformers.MinNewTokensLengthLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L160)

```py
( prompt_length_to_skip: int min_new_tokens: int eos_token_id: Union )
```

参数

+   `prompt_length_to_skip`（`int`）— 输入标记的长度。当与`generate`一起使用时，这不是一个有效的参数，因为它会自动分配输入长度。

+   `min_new_tokens`（`int`）— 小于此长度时，`eos_token_id`的分数将设置为`-float("Inf")`。

+   `eos_token_id`（`Union[int, List[int]]`）— *end-of-sequence*标记的ID。可选地，使用列表设置多个*end-of-sequence*标记。

LogitsProcessor通过将EOS（序列结束）标记的概率设置为0来强制新标记的最小长度。与MinLengthLogitsProcessor相反，此处理器忽略提示。

示例：

```py
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bigscience/bloomz-560m")
>>> model = AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")

>>> inputs = tokenizer(["A number:"], return_tensors="pt")
>>> gen_out = model.generate(**inputs)
>>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])
A number: one

>>> # setting `min_new_tokens` will force the model to generate beyond its natural ending point, which is not
>>> # necessarily incorrect
>>> gen_out = model.generate(**inputs, min_new_tokens=2)
>>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])
A number: one thousand
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L212)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores`（形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的对数，或者在使用波束搜索时，可以是每个词汇表标记的对数softmax

返回

形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`

处理后的预测分数。

### `class transformers.NoBadWordsLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1090)

```py
( bad_words_ids: List eos_token_id: Union )
```

参数

+   `bad_words_ids` (`List[List[int]]`) — 不允许生成的标记ID列表。

+   `eos_token_id` (`Union[int, List[int]]`) — *结束序列*标记的ID。可选地，使用列表设置多个*结束序列*标记。

[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)，强制指定的序列永远不会被选中。

为了获取不应出现在生成文本中的单词的标记ID，请确保在初始化分词器时设置`add_prefix_space=True`，并使用`tokenizer(bad_words, add_special_tokens=False).input_ids`。`add_prefix_space`参数仅支持一些慢速分词器，因为快速分词器的前缀行为来自`pre tokenizers`。在这里阅读更多信息(https://huggingface.co/docs/tokenizers/api/pre-tokenizers)。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("gpt2")
>>> tokenizer = AutoTokenizer.from_pretrained("gpt2")
>>> inputs = tokenizer(["In a word, the cake is a"], return_tensors="pt")

>>> output_ids = model.generate(inputs["input_ids"], max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)
>>> print(tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0])
In a word, the cake is a bit of a mess.

>>> # Now let's take the bad words out. Please note that the tokenizer is initialized differently
>>> tokenizer_with_prefix_space = AutoTokenizer.from_pretrained("gpt2", add_prefix_space=True)

>>> def get_tokens_as_list(word_list):
...     "Converts a sequence of words into a list of tokens"
...     tokens_list = []
...     for word in word_list:
...         tokenized_word = tokenizer_with_prefix_space([word], add_special_tokens=False).input_ids[0]
...         tokens_list.append(tokenized_word)
...     return tokens_list

>>> bad_words_ids = get_tokens_as_list(word_list=["mess"])
>>> output_ids = model.generate(
...     inputs["input_ids"], max_new_tokens=5, bad_words_ids=bad_words_ids, pad_token_id=tokenizer.eos_token_id
... )
>>> print(tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0])
In a word, the cake is a bit of a surprise.
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1013)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的logits，或者在使用波束搜索时，可以是每个词汇表标记的log softmax

返回

`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.NoRepeatNGramLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L816)

```py
( ngram_size: int )
```

参数

+   `ngram_size` (`int`) — 所有大小为`ngram_size`的ngrams只能出现一次。

N-grams是从文本序列中获取的“n”个连续单词、字符或标记的组合。给定句子：“她跑得快”，二元组（n=2）将是（“她”，“跑”）和（“跑”，“快”）。在文本生成中，避免单词序列的重复提供了更多样化的输出。这个[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)通过将被禁止的标记的分数设置为负无穷来强制不重复n-grams，从而消除了这些标记在进一步处理分数时的考虑。请注意，对于大多数仅解码器模型（如大多数LLMs），提示也被视为获取n-grams。[Fairseq](https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345)。

谨慎使用n-gram惩罚。例如，在关于纽约市的文章中惩罚2-gram（二元组）可能导致不良结果，其中城市的名称仅出现一次在整个文本中。[参考](https://huggingface.co/blog/how-to-generate)

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("distilgpt2")
>>> tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
>>> inputs = tokenizer(["Today I"], return_tensors="pt")

>>> output = model.generate(**inputs)
>>> print(tokenizer.decode(output[0], skip_special_tokens=True))
Today I’m not sure if I’m going to be able to do it.

>>> # Now let's add ngram size using `no_repeat_ngram_size`. This stops the repetitions ("I’m") in the output.
>>> output = model.generate(**inputs, no_repeat_ngram_size=2)
>>> print(tokenizer.decode(output[0], skip_special_tokens=True))
Today I’m not sure if I can get a better understanding of the nature of this issue
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L863)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的logits，或者在使用波束搜索时，可以是每个词汇表标记的log softmax

返回

`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.PrefixConstrainedLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1177)

```py
( prefix_allowed_tokens_fn: Callable num_beams: int )
```

参数

+   `prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`) — 此函数将波束搜索限制为每个步骤仅允许的标记。此函数接受2个参数`inputs_ids`和批次ID`batch_id`。它必须返回一个列表，其中包含下一代步骤的允许标记，条件是先前生成的标记`inputs_ids`和批次ID`batch_id`。

[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor) 强制执行受限制的生成，对于前缀条件的受限制生成很有用。有关更多信息，请参阅[自回归实体检索](https://arxiv.org/abs/2010.00904)。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")
>>> tokenizer = AutoTokenizer.from_pretrained("bigscience/bloomz-560m")

>>> inputs = tokenizer("Alice and Bob", return_tensors="pt")

>>> # By default, it continues generating according to the model's logits
>>> outputs = model.generate(**inputs, max_new_tokens=5)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])
Alice and Bob are friends

>>> # We can contrain it with `prefix_allowed_tokens_fn` to force a certain behavior based on a prefix.
>>> # For instance, we can force an entire entity to be generated when its beginning is detected.
>>> entity =  tokenizer(" Bob Marley", return_tensors="pt").input_ids[0]  # 3 tokens
>>> def prefix_allowed_tokens_fn(batch_id, input_ids):
...     '''
...     Attempts to generate 'Bob Marley' when 'Bob' is detected.
...     In this case, `batch_id` is not used, but you can set rules for each batch member.
...     '''
...     if input_ids[-1] == entity[0]:
...         return entity[1]
...     elif input_ids[-2] == entity[0] and input_ids[-1] == entity[1]:
...         return entity[2]
...     return list(range(tokenizer.vocab_size))  # If no match, allow all tokens

>>> outputs = model.generate(**inputs, max_new_tokens=5, prefix_allowed_tokens_fn=prefix_allowed_tokens_fn)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])
Alice and Bob Marley
```

### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1228)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log softmax。

返回值

`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.RepetitionPenaltyLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L288)

```py
( penalty: float )
```

参数

+   `penalty` (`float`) — 重复惩罚的参数。1.0表示没有惩罚。大于1.0会惩罚先前生成的标记。在0.0和1.0之间会奖励先前生成的标记。

[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor) 通过惩罚防止先前标记的重复。此惩罚最多每个标记应用一次。请注意，对于大多数仅解码器模型（如大多数LLMs），考虑的标记包括提示。

在原始[论文](https://arxiv.org/pdf/1909.05858.pdf)中，作者建议使用约1.2的惩罚来实现真实生成和减少重复之间的良好平衡。为了惩罚和减少重复，使用大于1.0的`penalty`值，其中较高的值会更强烈地惩罚。为了奖励和鼓励重复，使用0.0和1.0之间的`penalty`值，较低的值会更强烈地奖励。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> # Initializing the model and tokenizer for it
>>> model = AutoModelForCausalLM.from_pretrained("distilgpt2")
>>> tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
>>> inputs = tokenizer(["I'm not going to"], return_tensors="pt")

>>> # This shows a normal generate without any specific parameters
>>> summary_ids = model.generate(**inputs)
>>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0])
I'm not going to be able to do that. I'm going to be able to do that

>>> # This generates a penalty for repeated tokens
>>> penalized_ids = model.generate(**inputs, repetition_penalty=1.1)
>>> print(tokenizer.batch_decode(penalized_ids, skip_special_tokens=True)[0])
I'm not going to be able to do that. I'll just have to go out and play
```

### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L331)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log softmax。

返回值

`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.SequenceBiasLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L942)

```py
( sequence_bias: Dict )
```

参数

+   `sequence_bias` (`Dict[Tuple[int], float]`) — 将标记序列映射到其偏差项的字典。正偏差增加选择该序列的几率，而负偏差则相反。如果序列长度为1，则其偏差将始终应用。否则，仅当所讨论的序列即将完成时（在应用此处理器后的标记选择步骤中）才会应用偏差。

[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)应用于序列的附加偏置。当下一个生成的标记可以完成序列时，将偏置应用于序列的最后一个标记。因此，为了充分利用对具有多个标记的序列进行偏置，考虑使用波束方法（以优雅地解决部分完成的序列具有负偏差的问题）并将偏置应用于它们的前缀（以确保较早地应用偏置）。

为了获取您想要偏置的序列的标记ID，请确保在初始化分词器时设置`add_prefix_space=True`，并使用`tokenizer(bad_words, add_special_tokens=False).input_ids`。`add_prefix_space`参数仅支持一些慢速分词器，因为快速分词器的前缀行为来自`pre tokenizers`。[在这里](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)阅读更多。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("gpt2")
>>> tokenizer = AutoTokenizer.from_pretrained("gpt2")
>>> inputs = tokenizer(["The full name of Donald is Donald"], return_tensors="pt")

>>> summary_ids = model.generate(inputs["input_ids"], max_new_tokens=4)
>>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0])
The full name of Donald is Donald J. Trump Jr

>>> # Now let's control generation through a bias. Please note that the tokenizer is initialized differently!
>>> tokenizer_with_prefix_space = AutoTokenizer.from_pretrained("gpt2", add_prefix_space=True)

>>> def get_tokens_as_tuple(word):
...     return tuple(tokenizer_with_prefix_space([word], add_special_tokens=False).input_ids[0])

>>> # If we add a negative bias without beam search, it may become "stuck" in a prefix without good continuations
>>> sequence_bias = {get_tokens_as_tuple("Trump"): -10.0}
>>> biased_ids = model.generate(inputs["input_ids"], max_new_tokens=4, sequence_bias=sequence_bias)
>>> print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])
The full name of Donald is Donald J. Donald,

>>> biased_ids = model.generate(inputs["input_ids"], max_new_tokens=4, num_beams=4, sequence_bias=sequence_bias)
>>> print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])
The full name of Donald is Donald Rumsfeld,

>>> # We can also add a positive bias to nudge the model towards specific tokens or continuations
>>> sequence_bias = {get_tokens_as_tuple("Donald Duck"): 10.0}
>>> biased_ids = model.generate(inputs["input_ids"], max_new_tokens=4, num_beams=4, sequence_bias=sequence_bias)
>>> print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])
The full name of Donald is Donald Duck.
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1013)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的logits，或者在使用波束搜索时，可以是每个词汇表标记的log softmax

返回

`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.SuppressTokensAtBeginLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1622)

```py
( begin_suppress_tokens begin_index )
```

[SuppressTokensAtBeginLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.SuppressTokensAtBeginLogitsProcessor)在`generate`函数开始生成时立即抑制一系列标记，使用`begin_index`标记。这应该确保由`begin_suppress_tokens`定义的标记在开始时不会被生成。最初为[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)创建。

示例：

```py
>>> from transformers import AutoProcessor, WhisperForConditionalGeneration
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("openai/whisper-tiny.en")
>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-tiny.en")
>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> inputs = processor(ds[0]["audio"]["array"], return_tensors="pt")

>>> # Whisper has `begin_suppress_tokens` set by default (= `[220, 50256]`). 50256 is the EOS token, so this means
>>> # it can't generate and EOS token in the first iteration, but it can in the others.
>>> outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)
>>> print(outputs.scores[1][0, 50256])  # 1 (and not 0) is the first freely generated token
tensor(-inf)
>>> print(outputs.scores[-1][0, 50256])  # in other places we can see some probability mass for EOS
tensor(29.9010)

>>> # If we disable `begin_suppress_tokens`, we can generate EOS in the first iteration.
>>> outputs = model.generate(
...     **inputs, return_dict_in_generate=True, output_scores=True, begin_suppress_tokens=None
... )
>>> print(outputs.scores[1][0, 50256])
tensor(11.2027)
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1664)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的logits，或者在使用波束搜索时，可以是每个词汇表标记的log softmax

返回

`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.SuppressTokensLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1672)

```py
( suppress_tokens )
```

此处理器可用于抑制一系列标记。处理器将将它们的对数概率设置为`-inf`，以便它们不会被生成。最初为[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)创建。

示例：

```py
>>> from transformers import AutoProcessor, WhisperForConditionalGeneration
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("openai/whisper-tiny.en")
>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-tiny.en")
>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> inputs = processor(ds[0]["audio"]["array"], return_tensors="pt")

>>> # Whisper has a long list of suppressed tokens. For instance, in this case, the token 1 is suppressed by default.
>>> outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)
>>> print(outputs.scores[1][0, 1])  # 1 (and not 0) is the first freely generated token
tensor(-inf)

>>> # If we disable `suppress_tokens`, we can generate it.
>>> outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True, suppress_tokens=None)
>>> print(outputs.scores[1][0, 1])
tensor(5.7738)
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1704)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的logits，或者在使用波束搜索时，可以是每个词汇表标记的log softmax

返回

`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.TemperatureLogitsWarper`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L222)

```py
( temperature: float )
```

参数

+   `temperature` (`float`) — 用于调节logits分布的严格正值浮点值。小于 `1` 的值会减少随机性（反之亦然），`0` 相当于将所有概率质量转移到最可能的标记。

[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper) 用于温度（指数缩放输出概率分布），这有效地意味着它可以控制预测标记的随机性。通常与[TopPLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TopPLogitsWarper)和[TopKLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TopKLogitsWarper)一起使用。

确保在 `generate` 参数中包含 `do_sample=True`，否则温度值将不会产生任何效果。

示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed

>>> set_seed(0)  # for reproducibility

>>> tokenizer = AutoTokenizer.from_pretrained("gpt2")
>>> model = AutoModelForCausalLM.from_pretrained("gpt2")
>>> model.config.pad_token_id = model.config.eos_token_id
>>> inputs = tokenizer(["Hugging Face Company is"], return_tensors="pt")

>>> # With temperature=1.0, the default, we consistently get random outputs due to random sampling.
>>> generate_kwargs = {"max_new_tokens": 10, "do_sample": True, "temperature": 1.0, "num_return_sequences": 2}
>>> outputs = model.generate(**inputs, **generate_kwargs)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['Hugging Face Company is a joint venture between GEO Group, one of',
'Hugging Face Company is not an exact science – but what we believe does']

>>> # However, with temperature close to 0, it approximates greedy decoding strategies (invariant)
>>> generate_kwargs["temperature"] = 0.0001
>>> outputs = model.generate(**inputs, **generate_kwargs)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['Hugging Face Company is a company that has been around for over 20 years',
'Hugging Face Company is a company that has been around for over 20 years']
```

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L282)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log softmax

返回

`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.TopKLogitsWarper`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L462)

```py
( top_k: int filter_value: float = -inf min_tokens_to_keep: int = 1 )
```

参数

+   `top_k` (`int`) — 要保留的最高概率词汇标记的数量。

+   `filter_value` (`float`, *可选*, 默认为 -inf) — 所有过滤值将被设置为此浮点值。

+   `min_tokens_to_keep` (`int`, *可选*, 默认为 1) — 不能被过滤的最小标记数量。

[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper) 执行 top-k，即限制为最高概率元素 k。通常与[TemperatureLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TemperatureLogitsWarper)和[TopPLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TopPLogitsWarper)一起使用。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed

>>> set_seed(0)
>>> model = AutoModelForCausalLM.from_pretrained("distilgpt2")
>>> tokenizer = AutoTokenizer.from_pretrained("distilgpt2")

>>> inputs = tokenizer("A sequence: A, B, C, D", return_tensors="pt")

>>> # With sampling, the output is unexpected -- sometimes too unexpected.
>>> outputs = model.generate(**inputs, do_sample=True)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])
A sequence: A, B, C, D, G, H, I. A, M

>>> # With `top_k` sampling, the output gets restricted the k most likely tokens.
>>> # Pro tip: In practice, LLMs use `top_k` in the 5-50 range.
>>> outputs = model.generate(**inputs, do_sample=True, top_k=2)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])
A sequence: A, B, C, D, E, F, G, H, I
```

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L506)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log softmax

返回

`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.TopPLogitsWarper`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L397)

```py
( top_p: float filter_value: float = -inf min_tokens_to_keep: int = 1 )
```

参数

+   `top_p` (`float`) — 如果设置为 < 1，则仅保留概率相加达到 `top_p` 或更高的最可能标记集合用于生成。

+   `filter_value` (`float`, *可选*, 默认为 -inf) — 所有过滤值将被设置为此浮点值。

+   `min_tokens_to_keep` (`int`, *可选*, 默认为 1) — 不能被过滤的最小标记数量。

[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)执行top-p，即限制总和小于等于prob_cut_off的前几个标记。通常与[TemperatureLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TemperatureLogitsWarper)和[TopKLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TopKLogitsWarper)一起使用。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed

>>> set_seed(0)
>>> model = AutoModelForCausalLM.from_pretrained("distilgpt2")
>>> tokenizer = AutoTokenizer.from_pretrained("distilgpt2")

>>> inputs = tokenizer("A sequence: 1, 2", return_tensors="pt")

>>> # With sampling, the output is unexpected -- sometimes too unexpected.
>>> outputs = model.generate(**inputs, do_sample=True)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])
A sequence: 1, 2, 0, 2, 2. 2, 2, 2, 2

>>> # With `top_p` sampling, the output gets restricted to high-probability tokens.
>>> # Pro tip: In practice, LLMs use `top_p` in the 0.9-0.95 range.
>>> outputs = model.generate(**inputs, do_sample=True, top_p=0.1)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])
A sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L446)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的logits，或者在使用波束搜索时，可以是每个词汇表标记的log softmax。

返回

`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.TypicalLogitsWarper`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L515)

```py
( mass: float = 0.9 filter_value: float = -inf min_tokens_to_keep: int = 1 )
```

参数

+   `mass` (`float`, *可选*, 默认为0.9) — 典型p值在0到1之间，默认为0.9。

+   `filter_value` (`float`，*可选*，默认为-inf) — 所有被过滤的值将被设置为此浮点值。

+   `min_tokens_to_keep` (`int`，*可选*，默认为1) — 不能被过滤的最小标记数。

[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)执行典型解码。受到人类如何使用语言的启发，它优先考虑对数概率接近标记概率分布的熵的标记。这意味着在过程中可能会丢弃最有可能的标记。

查看[自然语言生成的典型解码](https://arxiv.org/abs/2202.00666)获取更多信息。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed

>>> model = AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m")
>>> tokenizer = AutoTokenizer.from_pretrained("bigscience/bloomz-560m")

>>> inputs = tokenizer("1, 2, 3", return_tensors="pt")

>>> # We can see that greedy decoding produces a sequence of numbers
>>> outputs = model.generate(**inputs)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])
1, 2, 3, 4, 5, 6, 7, 8, 9, 10,

>>> # For this particular seed, we can see that sampling produces nearly the same low-information (= low entropy)
>>> # sequence
>>> set_seed(18)
>>> outputs = model.generate(**inputs, do_sample=True)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])
1, 2, 3, 4, 5, 6, 7, 8, 9 and 10

>>> # With `typical_p` set, the most obvious sequence is no longer produced, which may be good for your problem
>>> set_seed(18)
>>> outputs = model.generate(
...     **inputs, do_sample=True, typical_p=0.1, return_dict_in_generate=True, output_scores=True
... )
>>> print(tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)[0])
1, 2, 3 and 5

>>> # We can see that the token corresponding to "4" (token 934) in the second position, the most likely token
>>> # as seen with greedy decoding, was entirely blocked out
>>> print(outputs.scores[1][0, 934])
tensor(-inf)
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L579)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇表的logits，或者在使用波束搜索时，可以是每个词汇表标记的log softmax。

返回

`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.UnbatchedClassifierFreeGuidanceLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L2055)

```py
( guidance_scale: float model unconditional_ids: Optional = None unconditional_attention_mask: Optional = None use_cache: Optional = True )
```

参数

+   `guidance_scale` (`float`) — 分类器自由引导（CFG）的引导比例。通过设置`guidance_scale != 1`来启用CFG。较高的引导比例鼓励模型生成与输入提示更紧密相关的样本，通常以牺牲质量为代价。小于1的值具有相反的效果，同时使得与负面提示提供的负面提示ID（如果有）作为正面提示。

+   `model` (`PreTrainedModel`) — 计算无条件分数的模型。假设与计算条件分数的模型相同。两个模型必须使用相同的分词器。

+   `unconditional_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 无条件分支中词汇表中输入序列标记的索引。如果未设置，将默认为提示的最后一个标记。

+   `unconditional_attention_mask` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于无条件ID的注意力掩码。

+   `use_cache`（`bool`，*可选*，默认为`True`）— 是否在负prompt前向传递期间缓存键/值。

用于无分类器引导（CFG）的Logits处理器。处理器通过`guidance_scale`参数化的prompt条件和prompt无条件（或负）logits的分数进行加权平均。无条件分数是通过提示`model`使用`unconditional_ids`分支内部计算的。

有关更多信息，请参阅[论文](https://arxiv.org/abs/2306.17806)。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("gpt2")
>>> tokenizer = AutoTokenizer.from_pretrained("gpt2")
>>> inputs = tokenizer(["Today, a dragon flew over Paris, France,"], return_tensors="pt")
>>> out = model.generate(inputs["input_ids"], guidance_scale=1.5)
>>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]
'Today, a dragon flew over Paris, France, killing at least 50 people and injuring more than 100'

>>> # with a negative prompt
>>> neg_inputs = tokenizer(["A very happy event happened,"], return_tensors="pt")
>>> out = model.generate(inputs["input_ids"], guidance_scale=2, negative_prompt_ids=neg_inputs["input_ids"])
>>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]
'Today, a dragon flew over Paris, France, killing at least 130 people. French media reported that'

>>> # with a positive prompt
>>> neg_inputs = tokenizer(["A very happy event happened,"], return_tensors="pt")
>>> out = model.generate(inputs["input_ids"], guidance_scale=0, negative_prompt_ids=neg_inputs["input_ids"])
>>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]
"Today, a dragon flew over Paris, France, and I'm very happy to be here. I"
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L2161)

```py
( input_ids scores )
```

### `class transformers.WhisperTimeStampLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1761)

```py
( generate_config begin_index: Optional = None _detect_timestamp_from_logprob: Optional = None )
```

参数

+   `generate_config`（`GenerateConfig`）--用于生成输出的生成配置。需要以下参数：eos_token_id（`int`，*optional*，默认为50257）：*sequence*结束标记的id。`no_timestamps_token_id`（`int`，*optional*，默认50363）：``"<|notimestamps|>"``令牌。`max_initial_timestamp_index`（`int`，*optional*，默认1）的id：用于设置初始时间戳的最大值。这用于防止模型预测未来太远的时间戳。

+   `begin_index`（`Optional`，*可选*）— 模型生成的第一个标记的标记索引。

+   `_detect_timestamp_from_logprob`（`bool`，*可选*）— 是否可以从所有时间戳的logprobs中预测时间戳。

[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)用于修改时间戳生成中的logits。当输入标记达到特定阈值时，处理器将分数设置为负无穷大。处理器确保时间戳标记成对出现，通过屏蔽会破坏这种配对模式的logits。这样做是为了保持生成的时间戳的一致性和结构。它还确保当预测任何时间戳标记的采样概率大于任何单个非时间戳标记时，这些非时间戳logits被设置为负无穷大。这样做是为了确保生成时间戳而不是其他潜在标记。

有关更多信息，请参阅[论文](https://arxiv.org/abs/2212.04356)。

示例：

```py
>>> import torch
>>> from transformers import AutoProcessor, WhisperForConditionalGeneration, GenerationConfig
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("openai/whisper-tiny.en")
>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-tiny.en")
>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> inputs = processor(ds[3]["audio"]["array"], return_tensors="pt")
>>> input_features = inputs.input_features

>>> #Displaying timestamps
>>> generated_ids = model.generate(inputs=input_features, return_timestamps=True)
>>> transcription = processor.batch_decode(generated_ids, decode_with_timestamps=True)[0]
>>> print("Transcription:", transcription)
Transcription: <|startoftranscript|><|0.00|> He has grave doubts whether Sir Frederick Layton's work is really Greek after all, and can<|6.44|><|6.44|> discover in it but little of rocky Ithaca.<|9.44|><|endoftext|>

>>> #No timestamps & change EOS:
>>> #This allows the user to select a specific token to terminate the sequence on, in this case it's the word "can"(460)
>>> model.generation_config.eos_token_id = 460
>>> generated_ids = model.generate(inputs=input_features,return_timestamps=False)
>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> print("Transcription:", transcription)
Transcription:  He has grave doubts whether Sir Frederick Layton's work is really Greek after all and can
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1843)

```py
( input_ids: LongTensor scores: FloatTensor ) → export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。[什么是输入ID？](../glossary#input-ids)

+   `scores`（形状为`(batch_size, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的logits，或者在使用波束搜索时，可以是每个词汇标记的log softmax

返回

`torch.FloatTensor`的形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

### TensorFlow

### `class transformers.TFForcedBOSTokenLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L448)

```py
( bos_token_id: int )
```

参数

+   `bos_token_id`（`int`）— 强制作为第一个生成的标记的标记ID。

[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)强制指定的标记作为第一个生成的标记。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L462)

```py
( input_ids: Tensor scores: Tensor cur_len: int )
```

### `class transformers.TFForcedEOSTokenLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L478)

```py
( max_length: int eos_token_id: int )
```

参数

+   `max_length`（`int`）— 要生成的序列的最大长度。

+   `eos_token_id`（`int`）— 在达到`max_length`时强制作为最后生成的标记的标记ID。

[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)强制指定的标记作为达到`max_length`时的最后生成的标记。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L495)

```py
( input_ids: Tensor scores: Tensor cur_len: int )
```

### `class transformers.TFForceTokensLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L551)

```py
( force_token_map: List )
```

这个处理器接受一对整数的列表，指示从生成索引到标记索引的映射，这些将在采样之前被强制。处理器将它们的log概率设置为`0`，并将所有其他标记设置为`-inf`，以便在相应的索引处对它们进行采样。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L567)

```py
( input_ids: Tensor scores: Tensor cur_len: int )
```

### `class transformers.TFLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L53)

```py
( )
```

用于在生成过程中应用的所有logit处理器的抽象基类。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L56)

```py
( input_ids: Tensor scores: Tensor cur_len: int ) → export const metadata = 'undefined';tf.Tensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`tf.Tensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)获取索引。详情请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `scores` (`tf.Tensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用beam search时，这些可以是每个词汇表的logits，或者在使用beam search时，可以是每个词汇表标记的log softmax。

+   `cur_len` (`int`) — 有效输入序列标记的当前长度。在TF实现中，input_ids的序列长度是生成器可以生成的最大长度，我们需要知道哪些标记是有效的。

+   `kwargs` (`Dict[str, Any]`，*可选*) — 其他logits处理器特定的kwargs。

返回

`tf.Tensor`，形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

处理logits的TF方法。

### `class transformers.TFLogitsProcessorList`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L75)

```py
( iterable = () )
```

这个类可用于创建一个[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)列表，以后处理`scores`输入张量。该类继承自列表，并添加了一个特定的***call***方法，以应用每个[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)到输入。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L82)

```py
( input_ids: Tensor scores: Tensor cur_len: int **kwargs ) → export const metadata = 'undefined';tf.Tensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`tf.Tensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)获取索引。详情请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `scores` (`tf.Tensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用beam search时，这些可以是每个词汇表的logits，或者在使用beam search时，可以是每个词汇表标记的log softmax。

+   `cur_len` (`int`) — 有效输入序列标记的当前长度。在 TF 实现中，input_ids 的序列长度是生成的最大长度，我们需要知道哪些标记是有效的。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 额外的 logits 处理器特定 kwargs。

返回

形状为 `(batch_size, config.vocab_size)` 的 `tf.Tensor`

处理后的预测分数。

### `class transformers.TFLogitsWarper`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L64)

```py
( )
```

用于在多项式采样生成期间应用的所有 logits 扭曲的抽象基类。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L67)

```py
( input_ids: Tensor scores: Tensor cur_len: int ) → export const metadata = 'undefined';tf.Tensor of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`tf.Tensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入 ID？](../glossary#input-ids)

+   `scores` (`tf.Tensor`，形状为 `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用波束搜索时，这些可以是每个词汇的 logits，或者在使用波束搜索时，可以是每个词汇标记的 log softmax。

+   `cur_len` (`int`) — 有效输入序列标记的当前长度。在 TF 实现中，input_ids 的序列长度是生成的最大长度，我们需要知道哪些标记是有效的。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 额外的 logits 处理器特定 kwargs。

返回

形状为 `(batch_size, config.vocab_size)` 的 `tf.Tensor`

处理后的预测分数。

用于扭曲 logits 的 TF 方法。

### `class transformers.TFMinLengthLogitsProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L202)

```py
( min_length: int eos_token_id: int )
```

参数

+   `min_length` (`int`) — 将 `eos_token_id` 的分数设置为 `-float("Inf")` 的最小长度。

+   `eos_token_id` (`int`) — *序列结束* 标记的 id。

[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor) 通过将 EOS 概率设置为 0 来强制最小长度。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L228)

```py
( input_ids: Tensor scores: Tensor cur_len: int )
```

### `class transformers.TFNoBadWordsLogitsProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L288)

```py
( bad_words_ids: List eos_token_id: int )
```

参数

+   `bad_words_ids` (`List[List[int]]`) — 不允许生成的标记 id 的列表。为了获取不应出现在生成文本中的单词的标记，请确保在初始化分词器时设置 `add_prefix_space=True`，并使用 `tokenizer(bad_words, add_special_tokens=False).input_ids`。`add_prefix_space` 参数仅支持某些慢速分词器，因为快速分词器的前缀行为来自 `pre tokenizers`。更多信息请阅读[这里](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)。

+   `eos_token_id` (`int`) — *序列结束* 标记的 id。

[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor) 强制指定序列永远不会被采样。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L367)

```py
( input_ids: Tensor scores: Tensor cur_len: int )
```

### `class transformers.TFNoRepeatNGramLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L388)

```py
( ngram_size: int )
```

参数

+   `ngram_size` (`int`) — 所有大小为`ngram_size`的n-gram只能出现一次。

[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor) 强制不重复n-gram。参见[Fairseq](https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345)。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L427)

```py
( input_ids: Tensor scores: Tensor cur_len: int )
```

### `class transformers.TFRepetitionPenaltyLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L238)

```py
( penalty: float )
```

参数

+   `repetition_penalty` (`float`) — 重复惩罚的参数。1.0表示没有惩罚。更多细节请参阅[这篇论文](https://arxiv.org/pdf/1909.05858.pdf)。

[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor) 对重复序列施加指数惩罚。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L280)

```py
( input_ids: Tensor scores: Tensor cur_len: int )
```

### `class transformers.TFSuppressTokensAtBeginLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L511)

```py
( begin_suppress_tokens begin_index )
```

[TFSuppressTokensAtBeginLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFSuppressTokensAtBeginLogitsProcessor) 在`generate`函数开始生成时立即抑制一组标记。这应该确保在生成开始时不会抽样到由`begin_suppress_tokens`定义的标记。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L522)

```py
( input_ids: Tensor scores: Tensor cur_len: int )
```

### `class transformers.TFSuppressTokensLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L535)

```py
( suppress_tokens )
```

这个处理器可以用来抑制一组标记。处理器将把它们的对数概率设置为`-inf`，以便它们不被抽样。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L542)

```py
( input_ids: Tensor scores: Tensor cur_len: int )
```

### `class transformers.TFTemperatureLogitsWarper`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L98)

```py
( temperature: float )
```

参数

+   `temperature` (`float`) — 用于调节logits分布的值。

[TFLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsWarper) 用于温度（指数缩放输出概率分布）。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L113)

```py
( input_ids: Tensor scores: Tensor cur_len: int )
```

### `class transformers.TFTopKLogitsWarper`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L118)

```py
( top_k: int filter_value: float = -inf min_tokens_to_keep: int = 1 )
```

参数

+   `top_k` (`int`) — 保留的最高概率词汇标记数。

+   `filter_value` (`float`, *可选*, 默认为-inf) — 所有被过滤的值将被设置为这个浮点值。

+   `min_tokens_to_keep` (`int`, *可选*, 默认为1) — 不能被过滤的最小标记数。

[TFLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsWarper) 执行top-k，即限制为概率最高的k个元素。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L138)

```py
( input_ids: Tensor scores: Tensor cur_len: int )
```

### `class transformers.TFTopPLogitsWarper`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L146)

```py
( top_p: float filter_value: float = -inf min_tokens_to_keep: int = 1 )
```

参数

+   `top_p` (`float`) — 如果设置为<1，则仅保留概率相加大于`top_p`或更高的最可能标记集。

+   `filter_value` (`float`, *可选*, 默认为-inf) — 所有被过滤的值将被设置为这个浮点值。

+   `min_tokens_to_keep` (`int`, *可选*, 默认为1) — 不能被过滤的最小标记数。

[TFLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsWarper) 执行top-p，即限制总和小于等于prob_cut_off的顶级标记。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L170)

```py
( input_ids: Tensor scores: Tensor cur_len: int )
```

### FLAX

### `class transformers.FlaxForcedBOSTokenLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L194)

```py
( bos_token_id: int )
```

参数

+   `bos_token_id` (`int`) — 强制作为第一个生成的标记的标记id。

[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor) 将指定的标记作为第一个生成的标记。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L206)

```py
( input_ids: Array scores: Array cur_len: int )
```

### `class transformers.FlaxForcedEOSTokenLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L216)

```py
( max_length: int eos_token_id: int )
```

参数

+   `max_length` (`int`) — 要生成的序列的最大长度。

+   `eos_token_id` (`int`) — 当达到`max_length`时，强制作为最后生成的标记的标记id。

[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor) 在达到`max_length`时将指定的标记强制为最后生成的标记。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L231)

```py
( input_ids: Array scores: Array cur_len: int )
```

### `class transformers.FlaxForceTokensLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L315)

```py
( force_token_map )
```

参数

+   `force_token_map` (`list`) — 给出标记id和它们将被强制采样的索引的映射。

[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor) 接受一对整数的列表，指示从生成索引到标记索引的映射，这些标记将在采样之前被强制。处理器将它们的对数概率设置为0，将所有其他标记设置为`-inf`，以便它们在相应的索引处被采样。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L337)

```py
( input_ids: Array scores: Array cur_len: int )
```

### `class transformers.FlaxLogitsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L50)

```py
( )
```

所有在生成期间可以应用的logit处理器的抽象基类。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L53)

```py
( input_ids: Array scores: Array ) → export const metadata = 'undefined';jnp.ndarray of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`jnp.ndarray`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `scores` (`jnp.ndarray`的形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用beam search时，这些可以是每个词汇的logits，或者在使用beam search时，可以是每个词汇标记的log softmax

+   `kwargs` (`Dict[str, Any]`, *optional*) — 特定于logits处理器的额外kwargs。

返回

`jnp.ndarray`的形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

Flax处理logits的方法。

### `class transformers.FlaxLogitsProcessorList`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L72)

```py
( iterable = () )
```

此类可用于创建[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)或[FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)的列表，以随后处理`scores`输入张量。此类继承自列表，并添加了一个特定的***call***方法来应用每个[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)或[FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)到输入中。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L79)

```py
( input_ids: Array scores: Array cur_len: int **kwargs ) → export const metadata = 'undefined';jnp.ndarray of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`jnp.ndarray`的形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `scores` (`jnp.ndarray`的形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用beam search时，这些可以是每个词汇的logits，或者在使用beam search时，可以是每个词汇标记的log softmax

+   `kwargs` (`Dict[str, Any]`, *optional*) — 特定于logits处理器的额外kwargs。

返回

`jnp.ndarray`的形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

### `class transformers.FlaxLogitsWarper`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L61)

```py
( )
```

用于在生成过程中应用多项式采样的所有logit扭曲器的抽象基类。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L64)

```py
( input_ids: Array scores: Array ) → export const metadata = 'undefined';jnp.ndarray of shape (batch_size, config.vocab_size)
```

参数

+   `input_ids` (`jnp.ndarray`的形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `scores` (`jnp.ndarray`的形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。当不使用beam search时，这些可以是每个词汇的logits，或者在使用beam search时，可以是每个词汇标记的log softmax

+   `kwargs` (`Dict[str, Any]`, *optional*) — 特定于logits处理器的额外kwargs。

返回

`jnp.ndarray`的形状为`(batch_size, config.vocab_size)`

处理后的预测分数。

Flax扭曲logits的方法。

### `class transformers.FlaxMinLengthLogitsProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L241)

```py
( min_length: int eos_token_id: int )
```

参数

+   `min_length` (`int`) — 当长度低于此值时，`eos_token_id` 的分数将被设置为 `-float("Inf")`。

+   `eos_token_id` (`int`) — *序列结束* 标记的 id。

[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor) 通过将 EOS 的概率设置为 0 来强制执行最小长度。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L262)

```py
( input_ids: Array scores: Array cur_len: int )
```

### `class transformers.FlaxSuppressTokensAtBeginLogitsProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L271)

```py
( begin_suppress_tokens begin_index )
```

参数

+   `begin_suppress_tokens` (`List[int]`) — 不采样的标记。

+   `begin_index` (`int`) — 抑制标记的索引。

[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor) 在 `generate` 函数开始生成时立即抑制一组标记，使用 `begin_index` 标记。这应该确保由 `begin_suppress_tokens` 定义的标记在生成开始时不被采样。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L288)

```py
( input_ids scores cur_len: int )
```

### `class transformers.FlaxSuppressTokensLogitsProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L296)

```py
( suppress_tokens: list )
```

参数

+   `suppress_tokens` (`list`) — 不采样的标记。

[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor) 在每个解码步骤抑制一组标记。处理器将它们的对数概率设置为 `-inf`，以便它们不被采样。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L309)

```py
( input_ids: Array scores: Array cur_len: int )
```

### `class transformers.FlaxTemperatureLogitsWarper`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L95)

```py
( temperature: float )
```

参数

+   `temperature` (`float`) — 用于调节 logits 分布的值。

[FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper) 用于温度（指数缩放输出概率分布）。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L110)

```py
( input_ids: Array scores: Array cur_len: int )
```

### `class transformers.FlaxTopKLogitsWarper`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L159)

```py
( top_k: int filter_value: float = -inf min_tokens_to_keep: int = 1 )
```

参数

+   `top_k` (`int`) — 保留最高概率词汇标记的数量以进行 top-k 过滤。

+   `filter_value` (`float`, *可选*, 默认为 -inf) — 所有过滤值将被设置为此浮点值。

+   `min_tokens_to_keep` (`int`, *可选*, 默认为 1) — 不能被过滤的最小标记数。

[FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper) 执行 top-k，即限制为概率最高的 k 个元素。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L179)

```py
( input_ids: Array scores: Array cur_len: int )
```

### `class transformers.FlaxTopPLogitsWarper`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L115)

```py
( top_p: float filter_value: float = -inf min_tokens_to_keep: int = 1 )
```

参数

+   `top_p` (`float`) — 如果设置为 < 1，则只保留概率相加达到 `top_p` 或更高的最小一组最可能的标记用于生成。

+   `filter_value` (`float`, *可选*, 默认为 -inf) — 所有过滤值将被设置为此浮点值。

+   `min_tokens_to_keep` (`int`，*可选*，默认为1) — 不能被过滤的最小标记数。

[FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)执行 top-p，即限制总概率小于等于 prob_cut_off 的前 p 个标记。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L139)

```py
( input_ids: Array scores: Array cur_len: int )
```

### `class transformers.FlaxWhisperTimeStampLogitsProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L363)

```py
( generate_config model_config decoder_input_length )
```

参数

+   `generate_config` (`GenerateConfig`) — 用于生成输出的生成配置。需要以下参数：eos_token_id (`int`，*可选*，默认为50257)：*序列结束*标记的id。no_timestamps_token_id (`int`，*可选*，默认为50363)：`"<|notimestamps|>"`标记的id。max_initial_timestamp_index (`int`，*可选*，默认为1)：用于设置初始时间戳的最大值。这用于防止模型预测太遥远的时间戳。

Whisper特定的处理器。此处理器可用于强制一个标记列表。处理器将将它们的对数概率设置为`inf`，以便在相应的索引处对它们进行采样。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L397)

```py
( input_ids scores cur_len )
```

## StoppingCriteria

[StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)可用于更改生成过程何时停止（除了 EOS 标记）。请注意，这仅适用于我们的 PyTorch 实现。

### `class transformers.StoppingCriteria`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L37)

```py
( )
```

所有可以在生成过程中应用的停止标准的抽象基类。

如果您的停止标准取决于`scores`输入，请确保将`return_dict_in_generate=True, output_scores=True`传递给`generate`。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L44)

```py
( input_ids: LongTensor scores: FloatTensor **kwargs )
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入 ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor`，形状为`(batch_size, config.vocab_size)`) — 语言建模头的预测分数。这些可以是 SoftMax 之前每个词汇标记的分数，也可以是 SoftMax 之后每个词汇标记的分数。如果此停止标准取决于`scores`输入，请确保将`return_dict_in_generate=True, output_scores=True`传递给`generate`。

+   `kwargs` (`Dict[str, Any]`，*可选*) — 其他特定停止标准的关键字参数。

### `class transformers.StoppingCriteriaList`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L129)

```py
( iterable = () )
```

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L130)

```py
( input_ids: LongTensor scores: FloatTensor **kwargs )
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。这些可以是SoftMax之前每个词汇标记的分数，也可以是SoftMax之后每个词汇标记的分数。如果此停止标准取决于`scores`输入，请确保您传递`return_dict_in_generate=True, output_scores=True`给`generate`。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 其他特定停止标准的kwargs。

### `class transformers.MaxLengthCriteria`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L49)

```py
( max_length: int max_position_embeddings: Optional = None )
```

参数

+   `max_length` (`int`) — 输出序列在标记数量上可以具有的最大长度。

+   `max_position_embeddings` (`int`, *可选*) — 模型的最大长度，由模型的`config.max_position_embeddings`属性定义。

这个类可以用来在生成的标记数超过`max_length`时停止生成。请注意，对于仅解码器类型的transformers，这将包括初始提示的标记。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L65)

```py
( input_ids: LongTensor scores: FloatTensor **kwargs )
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。这些可以是SoftMax之前每个词汇标记的分数，也可以是SoftMax之后每个词汇标记的分数。如果此停止标准取决于`scores`输入，请确保您传递`return_dict_in_generate=True, output_scores=True`给`generate`。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 其他特定停止标准的kwargs。

### `class transformers.MaxTimeCriteria`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L107)

```py
( max_time: float initial_timestamp: Optional = None )
```

参数

+   `max_time` (`float`) — 生成的最大允许时间（以秒为单位）。

+   `initial_time` (`float`, *可选*, 默认为`time.time()`) — 允许生成的开始时间。 

这个类可以用来在完整生成超过一定时间时停止生成。默认情况下，当初始化此函数时，时间将开始计算。您可以通过传递`initial_time`来覆盖这一点。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L124)

```py
( input_ids: LongTensor scores: FloatTensor **kwargs )
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入 ID？](../glossary#input-ids)

+   `scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) — 语言建模头的预测分数。这些可以是 SoftMax 之前每个词汇标记的分数，也可以是 SoftMax 之后每个词汇标记的分数。如果这个停止标准依赖于 `scores` 输入，确保你传递 `return_dict_in_generate=True, output_scores=True` 给 `generate`。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 其他特定的停止标准参数。

## 约束

[Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint) 可以用来强制生成结果中包含特定的标记或序列。请注意，这仅适用于我们的 PyTorch 实现。

### `class transformers.Constraint`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L5)

```py
( )
```

所有可以在生成过程中应用的约束的抽象基类。它必须定义约束如何被满足。

所有继承 Constraint 的类必须遵循的要求

```py
completed = False
while not completed:
    _, completed = constraint.update(constraint.advance())
```

将始终终止（停止）。

#### `advance`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L48)

```py
( ) → export const metadata = 'undefined';token_ids(torch.tensor)
```

返回

token_ids(`torch.tensor`)

必须是一个可索引的标记列表的张量，而不是某个整数。

调用时，返回一个标记，这个标记会使这个约束更接近被满足一步。

#### `copy`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L113)

```py
( stateful = False ) → export const metadata = 'undefined';constraint(Constraint)
```

返回

constraint(`Constraint`)

与被调用的相同的约束。

创建这个约束的一个新实例。

#### `does_advance`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L60)

```py
( token_id: int )
```

读取一个标记并返回它是否推进了进度。

#### `remaining`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L104)

```py
( )
```

返回 `advance()` 完成这个约束还需要多少步骤。

#### `reset`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L94)

```py
( )
```

重置这个约束的状态到初始化状态。我们会在约束的实现被不想要的标记中断时调用这个方法。

#### `test`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L24)

```py
( )
```

测试这个约束是否已经正确定义。

#### `update`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L69)

```py
( token_id: int ) → export const metadata = 'undefined';stepped(bool)
```

返回

stepped(`bool`)

这个约束是否变得更接近被满足一步。completed(`bool`): 这个约束是否已经被这个生成的标记完全满足。reset (`bool`): 这个约束是否已经被这个生成的标记重置了进度。

读取一个标记并返回指示其推进程度的布尔值。这个函数会更新这个对象的状态，不像 `does_advance(self, token_id: int)`。

这不是为了测试某个特定的标记是否会推进进度；而是为了更新它的状态，就好像它已经被生成了。如果 token_id != desired token（参考 PhrasalConstraint 中的 else 语句），这变得很重要。

### `class transformers.PhrasalConstraint`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L129)

```py
( token_ids: List )
```

参数

+   `token_ids`（`List[int]`）— 必须由输出生成的token的id。

[Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)强制要求输出中包含一个有序的token序列。

### `class transformers.DisjunctiveConstraint`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L261)

```py
( nested_token_ids: List )
```

参数

+   `nested_token_ids`（`List[List[int]]`）— 一个单词列表，其中每个单词都是一个id列表。通过从单词列表中生成一个单词来满足此约束。

一个特殊的[Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)，通过满足几个约束中的一个来实现。

### `class transformers.ConstraintListState`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L351)

```py
( constraints: List )
```

参数

+   `constraints`（`List[Constraint]`）— 必须由beam评分器满足的[Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)对象列表。

用于跟踪beam评分器通过一系列约束的进度的类。

#### `advance`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L383)

```py
( )
```

要生成的token列表，以便我们可以取得进展。这里的“列表”并不意味着将完全满足约束的token列表。

给定约束`c_i = {t_ij | j == # of tokens}`，如果我们不处于通过特定约束`c_i`进行进度的中间阶段，我们返回：

`[t_k1 for k in indices of unfulfilled constraints]`

如果我们处于约束的中间阶段，那么我们返回：`[t_ij]`，其中`i`是正在进行的约束的索引，`j`是约束的下一步。

虽然我们不关心哪个约束先被满足，但如果我们正在满足一个约束，那么这是我们唯一会返回的。

#### `reset`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L418)

```py
( token_ids: Optional )
```

token_ids：到目前为止生成的token，以重置通过约束的进度状态。

## BeamSearch

### `class transformers.BeamScorer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L91)

```py
( )
```

所有用于[beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)和[beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample)的beam评分器的抽象基类。

#### `process`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L97)

```py
( input_ids: LongTensor next_scores: FloatTensor next_tokens: LongTensor next_indices: LongTensor **kwargs ) → export const metadata = 'undefined';UserDict
```

参数

+   `input_ids`（形状为`(batch_size * num_beams, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列token的索引。

    可以使用任何继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)的类来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `next_scores`（形状为`(batch_size, 2 * num_beams)`的`torch.FloatTensor`）— 前`2 * num_beams`个未完成的beam假设的当前分数。

+   `next_tokens`（形状为`(batch_size, 2 * num_beams)`的`torch.LongTensor`）— 与前`2 * num_beams`个未完成的beam假设对应的`input_ids`的tokens。

+   `next_indices`（形状为`(batch_size, 2 * num_beams)`的`torch.LongTensor`）— 指示`next_tokens`对应于哪个beam假设的beam索引。

+   `pad_token_id`（`int`，*可选*）— *填充*标记的id。

+   `eos_token_id`（`Union[int, List[int]]`，*可选*）— *结束序列*标记的id。可选择使用列表设置多个*结束序列*标记。

+   `beam_indices`（`torch.LongTensor`，*可选*）— 指示每个标记对应于哪个beam假设的beam索引。

+   `group_index`（`int`，*可选*）— beam组的索引。与[group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search)一起使用。

返回值

`UserDict`

由上述字段组成的字典：

+   `next_beam_scores`（形状为`(batch_size * num_beams)`的`torch.FloatTensor`）— 所有未完成beam的更新分数。

+   `next_beam_tokens`（形状为`(batch_size * num_beams)`的`torch.FloatTensor`）— 要添加到未完成beam_hypotheses的下一个标记。

+   `next_beam_indices`（形状为`(batch_size * num_beams)`的`torch.FloatTensor`）— 指示下一个标记应添加到哪个beam的beam索引。

#### `finalize`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L109)

```py
( input_ids: LongTensor next_scores: FloatTensor next_tokens: LongTensor next_indices: LongTensor max_length: int **kwargs ) → export const metadata = 'undefined';torch.LongTensor of shape (batch_size * num_return_sequences, sequence_length)
```

参数

+   `input_ids`（形状为`(batch_size * num_beams, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。

    可以使用任何继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)的类来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `final_beam_scores`（形状为`(batch_size * num_beams)`的`torch.FloatTensor`）— 所有未完成beam的最终分数。

+   `final_beam_tokens`（形状为`(batch_size * num_beams)`的`torch.FloatTensor`）— 要添加到未完成beam_hypotheses的最后一个标记。

+   `final_beam_indices`（形状为`(batch_size * num_beams)`的`torch.FloatTensor`）— 指示`final_beam_tokens`应添加到哪个beam的beam索引。

+   `pad_token_id`（`int`，*可选*）— *填充*标记的id。

+   `eos_token_id`（`Union[int, List[int]]`，*可选*）— *结束序列*标记的id。可选择使用列表设置多个*结束序列*标记。

返回值

`torch.LongTensor`的形状为`(batch_size * num_return_sequences, sequence_length)`

生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。

### `class transformers.BeamSearchScorer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L123)

```py
( batch_size: int num_beams: int device: device length_penalty: Optional = 1.0 do_early_stopping: Union = False num_beam_hyps_to_keep: Optional = 1 num_beam_groups: Optional = 1 max_length: Optional = None )
```

参数

+   `batch_size`（`int`）— `input_ids`的批量大小，用于并行运行标准beam搜索解码。

+   `num_beams`（`int`）— beam搜索的beam数量。

+   `device`（`torch.device`）— 定义此`BeamSearchScorer`实例将分配到的设备类型（例如，`"cpu"`或`"cuda"`）。

+   `length_penalty`（`float`，*可选*，默认为1.0）— 用于基于beam的生成的长度的指数惩罚。它作为指数应用于序列长度，然后用于分割序列的分数。由于分数是序列的对数似然（即负数），`length_penalty` > 0.0 促进更长的序列，而`length_penalty` < 0.0 鼓励更短的序列。

+   `do_early_stopping` (`bool`或`str`，*可选*，默认为`False`) — 控制基于beam的方法（如beam-search）的停止条件。接受以下值：`True`，生成在有`num_beams`个完整候选时停止；`False`，应用启发式方法，当很难找到更好的候选时停止生成；`"never"`，仅当不能有更好的候选时，beam搜索过程才会停止（经典beam搜索算法）。

+   `num_beam_hyps_to_keep` (`int`，*可选*，默认为1) — 在调用[finalize()](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamSearchScorer.finalize)时应返回的beam假设数量。

+   `num_beam_groups` (`int`，*可选*，默认为1) — 将`num_beams`分成多个组以确保不同组的beam之间的多样性。有关更多详细信息，请参阅[此论文](https://arxiv.org/pdf/1610.02424.pdf)。

+   `max_length` (`int`，*可选*) — 要生成的序列的最大长度。

[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)实现标准beam搜索解码。

部分改编自[Facebook的XLM beam搜索代码](https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529)。

多样性beam搜索算法和实现的参考[Ashwin Kalyan的DBS实现](https://github.com/ashwinkalyan/dbs/blob/master/dbs/beam_utils.lua)

#### `process`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L215)

```py
( input_ids: LongTensor next_scores: FloatTensor next_tokens: LongTensor next_indices: LongTensor pad_token_id: Optional = None eos_token_id: Union = None beam_indices: Optional = None group_index: Optional = 0 decoder_prompt_len: Optional = 0 )
```

#### `finalize`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L318)

```py
( input_ids: LongTensor final_beam_scores: FloatTensor final_beam_tokens: LongTensor final_beam_indices: LongTensor max_length: int pad_token_id: Optional = None eos_token_id: Union = None beam_indices: Optional = None decoder_prompt_len: Optional = 0 )
```

### `class transformers.ConstrainedBeamSearchScorer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L415)

```py
( batch_size: int num_beams: int constraints: List device: device length_penalty: Optional = 1.0 do_early_stopping: Union = False num_beam_hyps_to_keep: Optional = 1 num_beam_groups: Optional = 1 max_length: Optional = None )
```

参数

+   `batch_size` (`int`) — 并行运行标准beam搜索解码的`input_ids`的批处理大小。

+   `num_beams` (`int`) — beam搜索的beam数量。

+   `constraints` (`List[Constraint]`) — 以`Constraint`对象表示的正约束列表，必须在生成输出中满足。有关更多信息，请阅读[Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)的文档。

+   `device` (`torch.device`) — 定义此`BeamSearchScorer`实例将分配到的设备类型（例如，`"cpu"`或`"cuda"`）。

+   `length_penalty` (`float`，*可选*，默认为1.0) — 用于基于beam的生成的长度的指数惩罚。它作为序列长度的指数应用，然后用于分割序列的分数。由于分数是序列的对数似然（即负数），`length_penalty` > 0.0 促进更长的序列，而`length_penalty` < 0.0 鼓励更短的序列。

+   `do_early_stopping` (`bool`或`str`，*可选*，默认为`False`) — 控制基于beam的方法（如beam-search）的停止条件。接受以下值：`True`，生成在有`num_beams`个完整候选时停止；`False`，应用启发式方法，当很难找到更好的候选时停止生成；`"never"`，仅当不能有更好的候选时，beam搜索过程才会停止（经典beam搜索算法）。

+   `num_beam_hyps_to_keep` (`int`，*可选*，默认为1) — 在调用[finalize()](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamSearchScorer.finalize)时应返回的beam假设数量。

+   `num_beam_groups` (`int`，*可选*，默认为1）— 将`num_beams`分成几组以确保不同组束之间的多样性。有关更多详细信息，请参阅[此论文](https://arxiv.org/pdf/1610.02424.pdf)。

+   `max_length` (`int`，*可选*）— 要生成的序列的最大长度。

[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer) 实现受限束搜索解码。

#### `process`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L509)

```py
( input_ids: LongTensor next_scores: FloatTensor next_tokens: LongTensor next_indices: LongTensor scores_for_all_vocab: FloatTensor pad_token_id: Optional = None eos_token_id: Union = None beam_indices: Optional = None decoder_prompt_len: Optional = 0 ) → export const metadata = 'undefined';UserDict
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size * num_beams, sequence_length)`）— 词汇表中输入序列标记的索引。

    可以使用任何继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)的类来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `next_scores` (`torch.FloatTensor`，形状为`(batch_size, 2 * num_beams)`）— 前`2 * num_beams`个未完成束假设的当前分数。

+   `next_tokens` (`torch.LongTensor`，形状为`(batch_size, 2 * num_beams)`）— 与前`2 * num_beams`个未完成束假设对应的标记的`input_ids`。

+   `next_indices` (`torch.LongTensor`，形状为`(batch_size, 2 * num_beams)`）— 指示`next_tokens`对应的束假设的束索引。

+   `scores_for_all_vocab` (`torch.FloatTensor`，形状为`(batch_size * num_beams, sequence_length)`）— 每个束假设的词汇表中所有标记的分数。

+   `pad_token_id` (`int`，*可选*）— *填充*标记的ID。

+   `eos_token_id` (`Union[int, List[int]]`，*可选*）— *结束序列*标记的ID。可以选择使用列表设置多个*结束序列*标记。

+   `beam_indices` (`torch.LongTensor`，*可选*）— 指示每个标记对应的束假设的束索引。

+   `decoder_prompt_len` (`int`，*可选*）— 包含在输入到解码器中的提示长度。

返回

`UserDict`

由上述字段组成的字典：

+   `next_beam_scores` (`torch.FloatTensor`，形状为`(batch_size * num_beams)`）— 所有未完成束的更新分数。

+   `next_beam_tokens` (`torch.FloatTensor`，形状为`(batch_size * num_beams)`）— 要添加到未完成束假设的下一个标记。

+   `next_beam_indices` (`torch.FloatTensor`，形状为`(batch_size * num_beams)`）— 指示下一个标记应添加到哪个束中的束索引。

#### `finalize`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L807)

```py
( input_ids: LongTensor final_beam_scores: FloatTensor final_beam_tokens: LongTensor final_beam_indices: LongTensor max_length: int pad_token_id: Optional = None eos_token_id: Union = None beam_indices: Optional = None decoder_prompt_len: Optional = 0 )
```

## 实用工具

#### `transformers.top_k_top_p_filtering`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L4610)

```py
( logits: FloatTensor top_k: int = 0 top_p: float = 1.0 filter_value: float = -inf min_tokens_to_keep: int = 1 )
```

参数

+   `top_k` (`int`，*可选*，默认为0）— 如果大于0，则仅保留具有最高概率的前k个标记（top-k过滤）

+   `top_p` (`float`，*可选*，默认为1.0）— 如果小于1.0，则仅保留累积概率大于等于top_p的前几个标记（nucleus过滤）。Nucleus过滤在Holtzman等人的论文中有描述（[http://arxiv.org/abs/1904.09751](http://arxiv.org/abs/1904.09751))。

+   `min_tokens_to_keep` (`int`，*可选*，默认为1）— 输出中每个批次示例保留的最小标记数。

使用top-k和/或nucleus（top-p）过滤对数分布

来自：[https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317](https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317)

#### `transformers.tf_top_k_top_p_filtering`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L3092)

```py
( logits top_k = 0 top_p = 1.0 filter_value = -inf min_tokens_to_keep = 1 )
```

参数

+   `top_k`（`int`，*可选*，默认为0）— 如果> 0，则仅保留具有最高概率的前k个标记（top-k过滤）

+   `top_p`（`float`，*可选*，默认为1.0）— 如果<1.0，则仅保留累积概率>= top_p的前几个标记（nucleus过滤）。Nucleus过滤在Holtzman等人的论文中有描述（[http://arxiv.org/abs/1904.09751](http://arxiv.org/abs/1904.09751)）

+   `min_tokens_to_keep`（`int`，*可选*，默认为1）— 输出中每个批示例要保留的最小标记数。

使用top-k和/或nucleus（top-p）过滤对数分布

来自：[https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317](https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317)

## 流媒体器

### `class transformers.TextStreamer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L38)

```py
( tokenizer: AutoTokenizer skip_prompt: bool = False **decode_kwargs )
```

参数

+   `tokenizer`（`AutoTokenizer`）— 用于解码标记的标记器。

+   `skip_prompt`（`bool`，*可选*，默认为`False`）— 是否跳过提示以执行`.generate()`。例如，对于聊天机器人很有用。

+   `decode_kwargs`（`dict`，*可选*）— 传递给标记器的`decode`方法的其他关键字参数。

简单的文本流媒体器，一旦形成完整单词，就会将标记打印到标准输出。

流媒体类的API仍在开发中，可能会在未来发生变化。

示例：

```py
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

>>> tok = AutoTokenizer.from_pretrained("gpt2")
>>> model = AutoModelForCausalLM.from_pretrained("gpt2")
>>> inputs = tok(["An increasing sequence: one,"], return_tensors="pt")
>>> streamer = TextStreamer(tok)

>>> # Despite returning the usual output, the streamer will also print the generated text to stdout.
>>> _ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)
An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,
```

#### `end`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L116)

```py
( )
```

刷新任何剩余的缓存并将换行符打印到标准输出。

#### `on_finalized_text`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L130)

```py
( text: str stream_end: bool = False )
```

将新文本打印到标准输出。如果流结束，也会打印一个换行符。

#### `put`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L82)

```py
( value )
```

接收标记，解码它们，并在形成完整单词时立即将它们打印到标准输出。

### `class transformers.TextIteratorStreamer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L159)

```py
( tokenizer: AutoTokenizer skip_prompt: bool = False timeout: Optional = None **decode_kwargs )
```

参数

+   `tokenizer`（`AutoTokenizer`）— 用于解码标记的标记器。

+   `skip_prompt`（`bool`，*可选*，默认为`False`）— 是否跳过提示以执行`.generate()`。例如，对于聊天机器人很有用。

+   `timeout`（`float`，*可选*）— 文本队列的超时时间。如果为`None`，队列将无限期阻塞。在单独的线程中调用`.generate()`时，有助于处理异常。

+   `decode_kwargs`（`dict`，*可选*）— 传递给标记器的`decode`方法的其他关键字参数。

将可打印文本存储在队列中的流，供下游应用程序用作迭代器。这对于那些从以非阻塞方式访问生成的文本中受益的应用程序很有用（例如，在交互式Gradio演示中）。

流媒体类的API仍在开发中，可能会在未来发生变化。

示例：

```py
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
>>> from threading import Thread

>>> tok = AutoTokenizer.from_pretrained("gpt2")
>>> model = AutoModelForCausalLM.from_pretrained("gpt2")
>>> inputs = tok(["An increasing sequence: one,"], return_tensors="pt")
>>> streamer = TextIteratorStreamer(tok)

>>> # Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.
>>> generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20)
>>> thread = Thread(target=model.generate, kwargs=generation_kwargs)
>>> thread.start()
>>> generated_text = ""
>>> for new_text in streamer:
...     generated_text += new_text
>>> generated_text
'An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,'
```

#### `on_finalized_text`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L213)

```py
( text: str stream_end: bool = False )
```

将新文本放入队列中。如果流结束，也将停止信号放入队列中。

## 缓存

### `class transformers.Cache`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L6)

```py
( )
```

所有缓存的基类。实际数据结构对于每个子类是特定的。

#### `update`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L11)

```py
( key_states: Tensor value_states: Tensor layer_idx: int cache_kwargs: Optional = None )
```

参数

+   `key_states`（`torch.Tensor`）— 要缓存的新键状态。

+   `value_states`（`torch.Tensor`）— 要缓存的新值状态。

+   `layer_idx`（`int`）— 用于缓存状态的层的索引。

+   `cache_kwargs` (`Dict[str, Any]`, `optional`) — 缓存子类的额外参数。这些参数针对每个子类是特定的，并允许创建新类型的缓存。

使用新的`key_states`和`value_states`更新层`layer_idx`的缓存。

### `class transformers.DynamicCache`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L57)

```py
( )
```

随着生成更多令牌，缓存会动态增长。这是生成模型的默认设置。

它将键和值状态存储为张量列表，每个层一个张量。每个张量的预期形状为`[batch_size, num_heads, seq_len, head_dim]`。

#### `update`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L95)

```py
( key_states: Tensor value_states: Tensor layer_idx: int cache_kwargs: Optional = None )
```

参数

+   `key_states` (`torch.Tensor`) — 要缓存的新键状态。

+   `value_states` (`torch.Tensor`) — 要缓存的新值状态。

+   `layer_idx` (`int`) — 要为其缓存状态的层的索引。

+   `cache_kwargs` (`Dict[str, Any]`, `optional`) — 缓存子类的额外参数。在`DynamicCache`中不使用额外参数。

使用新的`key_states`和`value_states`更新层`layer_idx`的缓存。

#### `get_seq_length`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L132)

```py
( layer_idx: Optional = 0 )
```

返回缓存状态的序列长度。可以选择传递层索引。

#### `reorder_cache`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L142)

```py
( beam_idx: LongTensor )
```

为束搜索重新排序缓存，给定所选的束索引。

#### `to_legacy_cache`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L150)

```py
( )
```

将`DynamicCache`实例转换为其在传统缓存格式中的等效形式。

#### `from_legacy_cache`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L157)

```py
( past_key_values: Optional = None )
```

将传统缓存格式中的缓存转换为等效的`DynamicCache`。

### `class transformers.SinkCache`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L168)

```py
( window_length: int num_sink_tokens: int )
```

参数

+   `window_length` (`int`) — 上下文窗口的长度。

+   `num_sink_tokens` (`int`) — 沉没令牌的数量。有关更多信息，请参阅原始论文。

如[Attention Sinks论文](https://arxiv.org/abs/2309.17453)中描述的缓存。它允许模型在超出其上下文窗口长度的情况下生成，而不会失去对话流畅性。随着丢弃过去的令牌，模型将失去生成依赖于被丢弃上下文的令牌的能力。

它将键和值状态存储为张量列表，每个层一个张量。每个张量的预期形状为`[batch_size, num_heads, seq_len, head_dim]`。

#### `update`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L237)

```py
( key_states: Tensor value_states: Tensor layer_idx: int cache_kwargs: Optional = None )
```

参数

+   `key_states` (`torch.Tensor`) — 要缓存的新键状态。

+   `value_states` (`torch.Tensor`) — 要缓存的新值状态。

+   `layer_idx` (`int`) — 要为其缓存状态的层的索引。

+   `cache_kwargs` (`Dict[str, Any]`, `optional`) — 缓存子类的额外参数。在`SinkCache`中可以使用以下参数：`sin`，`cos`和`partial_rotation_size`。这些参数用于使用RoPE的模型，在令牌移位时重新计算旋转。

使用新的`key_states`和`value_states`更新层`layer_idx`的缓存。

#### `get_seq_length`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L226)

```py
( layer_idx: Optional = 0 )
```

返回缓存状态的序列长度。可以选择传递层索引。

#### `reorder_cache`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L316)

```py
( beam_idx: LongTensor )
```

重新排列缓存以进行波束搜索，给定所选的波束索引。
