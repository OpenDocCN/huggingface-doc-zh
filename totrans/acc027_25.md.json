["```py\naccelerate config\n```", "```py\naccelerate launch my_script.py --args_to_my_script\n```", "```py\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndistributed_type: FSDP\ndowncast_bf16: 'no'\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch_policy: BACKWARD_PRE\n  fsdp_forward_prefetch: false\n  fsdp_cpu_ram_efficient_loading: true\n  fsdp_offload_params: false\n  fsdp_sharding_strategy: FULL_SHARD\n  fsdp_state_dict_type: SHARDED_STATE_DICT\n  fsdp_sync_module_states: true\n  fsdp_transformer_layer_cls_to_wrap: BertLayer\n  fsdp_use_orig_params: true\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```", "```py\naccelerate launch examples/nlp_example.py\n```", "```py\nfrom accelerate import FullyShardedDataParallelPlugin\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n\nfsdp_plugin = FullyShardedDataParallelPlugin(\n    state_dict_config=FullStateDictConfig(offload_to_cpu=False, rank0_only=False),\n    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=False, rank0_only=False),\n)\n\naccelerator = Accelerator(fsdp_plugin=fsdp_plugin)\n```", "```py\naccelerator.save_state(\"ckpt\")\n```", "```py\nls ckpt\n# optimizer_0  pytorch_model_0  random_states_0.pkl  random_states_1.pkl  scheduler.bin\n\ncd ckpt\n\nls optimizer_0\n# __0_0.distcp  __1_0.distcp\n\nls pytorch_model_0\n# __0_0.distcp  __1_0.distcp\n```", "```py\naccelerator.load_state(\"ckpt\")\n```", "```py\n  unwrapped_model.save_pretrained(\n      args.output_dir,\n      is_main_process=accelerator.is_main_process,\n      save_function=accelerator.save,\n+     state_dict=accelerator.get_state_dict(model),\n)\n```"]