- en: Quickstart
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速入门
- en: 'Original text: [https://huggingface.co/docs/optimum-neuron/quickstart](https://huggingface.co/docs/optimum-neuron/quickstart)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/docs/optimum-neuron/quickstart](https://huggingface.co/docs/optimum-neuron/quickstart)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '🤗 Optimum Neuron was designed with one goal in mind: **to make training and
    inference straightforward for any 🤗 Transformers user while leveraging the complete
    power of AWS Accelerators**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Optimum Neuron的设计目标是：**使任何🤗 Transformers用户能够轻松进行训练和推理，同时利用AWS加速器的全部功能**。
- en: Training
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: 'There are two main classes one needs to know:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个主要的类需要了解：
- en: 'NeuronArgumentParser: inherits the original [HfArgumentParser](https://huggingface.co/docs/transformers/main/en/internal/trainer_utils#transformers.HfArgumentParser)
    in Transformers with additional checks on the argument values to make sure that
    they will work well with AWS Trainium instances.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NeuronArgumentParser：继承了Transformers中原始的[HfArgumentParser](https://huggingface.co/docs/transformers/main/en/internal/trainer_utils#transformers.HfArgumentParser)，并对参数值进行额外检查，以确保它们能够很好地与AWS
    Trainium实例配合使用。
- en: '[NeuronTrainer](https://huggingface.co/docs/optimum/neuron/package_reference/trainer):
    the trainer class that takes care of compiling and distributing the model to run
    on Trainium Chips, and performing training and evaluation.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NeuronTrainer](https://huggingface.co/docs/optimum/neuron/package_reference/trainer)：负责编译和分发模型以在Trainium芯片上运行，并执行训练和评估的训练器类。'
- en: The [NeuronTrainer](https://huggingface.co/docs/optimum/neuron/package_reference/trainer)
    is very similar to the [🤗 Transformers Trainer](https://huggingface.co/docs/transformers/main_classes/trainer),
    and adapting a script using the Trainer to make it work with Trainium will mostly
    consist in simply swapping the `Trainer` class for the `NeuronTrainer` one. That’s
    how most of the [example scripts](https://github.com/huggingface/optimum-neuron/tree/main/examples)
    were adapted from their [original counterparts](https://github.com/huggingface/transformers/tree/main/examples/pytorch).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: NeuronTrainer非常类似于🤗 Transformers Trainer，并且通过使用NeuronTrainer来调整脚本以使其与Trainium一起工作，主要是简单地将`Trainer`类替换为`NeuronTrainer`类。这就是大多数[示例脚本](https://github.com/huggingface/optimum-neuron/tree/main/examples)是如何从它们的[原始对应物](https://github.com/huggingface/transformers/tree/main/examples/pytorch)进行调整的。
- en: 'modifications:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 修改：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: All Trainium instances come at least with 2 Neuron Cores. To leverage those
    we need to launch the training whith `torchrun`. Below you see and example of
    how to launch a training script on a `trn1.2xlarge` instance using a `bert-base-uncased`
    model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Trainium实例至少配备2个Neuron核心。为了利用这些核心，我们需要使用`torchrun`启动训练。下面是一个示例，演示如何在`trn1.2xlarge`实例上使用`bert-base-uncased`模型启动训练脚本。
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Inference
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理
- en: 'You can compile and export your 🤗 Transformers models to a serialized format
    before inference on Neuron devices:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Neuron设备上进行推理之前，将🤗 Transformers模型编译并导出为序列化格式：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The command above will export `distilbert-base-uncased-finetuned-sst-2-english`
    with static shapes: `batch_size=1` and `sequence_length=32`, and cast all `matmul`
    operations from FP32 to BF16\. Check out the [exporter guide](https://huggingface.co/docs/optimum-neuron/guides/export_model#exporting-a-model-to-neuron-using-the-cli)
    for more compilation options.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的命令将使用静态形状导出`distilbert-base-uncased-finetuned-sst-2-english`：`batch_size=1`和`sequence_length=32`，并将所有`matmul`操作从FP32转换为BF16。查看[导出器指南](https://huggingface.co/docs/optimum-neuron/guides/export_model#exporting-a-model-to-neuron-using-the-cli)以获取更多编译选项。
- en: 'Then you can run the exported Neuron model on Neuron devices with `NeuronModelForXXX`
    classes which are similar to `AutoModelForXXX` classes in 🤗 Transformers:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用类似于🤗 Transformers中的`AutoModelForXXX`类的`NeuronModelForXXX`类在Neuron设备上运行导出的Neuron模型：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
