- en: Quickstart
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/optimum-neuron/quickstart](https://huggingface.co/docs/optimum-neuron/quickstart)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/optimum.neuron/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/entry/start.abfe5599.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/scheduler.9039eef2.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/singletons.9144bb03.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/paths.e169ac99.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/entry/app.df8ec0a0.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/index.cdcc3d35.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/nodes/0.a52c6f40.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/nodes/19.d6dbb6e7.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/CodeBlock.e3ac94d9.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/Heading.96ce3702.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'ðŸ¤— Optimum Neuron was designed with one goal in mind: **to make training and
    inference straightforward for any ðŸ¤— Transformers user while leveraging the complete
    power of AWS Accelerators**.'
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two main classes one needs to know:'
  prefs: []
  type: TYPE_NORMAL
- en: 'NeuronArgumentParser: inherits the original [HfArgumentParser](https://huggingface.co/docs/transformers/main/en/internal/trainer_utils#transformers.HfArgumentParser)
    in Transformers with additional checks on the argument values to make sure that
    they will work well with AWS Trainium instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[NeuronTrainer](https://huggingface.co/docs/optimum/neuron/package_reference/trainer):
    the trainer class that takes care of compiling and distributing the model to run
    on Trainium Chips, and performing training and evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [NeuronTrainer](https://huggingface.co/docs/optimum/neuron/package_reference/trainer)
    is very similar to the [ðŸ¤— Transformers Trainer](https://huggingface.co/docs/transformers/main_classes/trainer),
    and adapting a script using the Trainer to make it work with Trainium will mostly
    consist in simply swapping the `Trainer` class for the `NeuronTrainer` one. Thatâ€™s
    how most of the [example scripts](https://github.com/huggingface/optimum-neuron/tree/main/examples)
    were adapted from their [original counterparts](https://github.com/huggingface/transformers/tree/main/examples/pytorch).
  prefs: []
  type: TYPE_NORMAL
- en: 'modifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: All Trainium instances come at least with 2 Neuron Cores. To leverage those
    we need to launch the training whith `torchrun`. Below you see and example of
    how to launch a training script on a `trn1.2xlarge` instance using a `bert-base-uncased`
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can compile and export your ðŸ¤— Transformers models to a serialized format
    before inference on Neuron devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The command above will export `distilbert-base-uncased-finetuned-sst-2-english`
    with static shapes: `batch_size=1` and `sequence_length=32`, and cast all `matmul`
    operations from FP32 to BF16\. Check out the [exporter guide](https://huggingface.co/docs/optimum-neuron/guides/export_model#exporting-a-model-to-neuron-using-the-cli)
    for more compilation options.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then you can run the exported Neuron model on Neuron devices with `NeuronModelForXXX`
    classes which are similar to `AutoModelForXXX` classes in ðŸ¤— Transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
