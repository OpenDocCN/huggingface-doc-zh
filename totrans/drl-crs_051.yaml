- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit3/conclusion](https://huggingface.co/learn/deep-rl-course/unit3/conclusion)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Congrats on finishing this chapter!Â There was a lot of information. And congrats
    on finishing the tutorial. Youâ€™ve just trained your first Deep Q-Learning agent
    and shared it on the Hub ðŸ¥³.
  prefs: []
  type: TYPE_NORMAL
- en: Take time to really grasp the material before continuing.
  prefs: []
  type: TYPE_NORMAL
- en: Donâ€™t hesitate to train your agent in other environments (Pong, Seaquest, QBert,
    Ms Pac Man). TheÂ **best way to learn is to try things on your own!**
  prefs: []
  type: TYPE_NORMAL
- en: '![Environments](../Images/bf441b005cda192d0dc86eb42475aeb3.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next unit, **weâ€™re going to learn about Optuna**. One of the most critical
    tasks in Deep Reinforcement Learning is to find a good set of training hyperparameters.
    Optuna is a library that helps you to automate the search.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we would love **to hear what you think of the course and how we can
    improve it**. If you have some feedback then please ðŸ‘‰ [fill this form](https://forms.gle/BzKXWzLAGZESGNaE9)
  prefs: []
  type: TYPE_NORMAL
- en: Keep Learning, stay awesome ðŸ¤—
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
