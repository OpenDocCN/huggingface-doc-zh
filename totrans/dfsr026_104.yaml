- en: LoRA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/loaders/lora](https://huggingface.co/docs/diffusers/api/loaders/lora)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: 'LoRA is a fast and lightweight training method that inserts and trains a significantly
    smaller number of parameters instead of all the model parameters. This produces
    a smaller file (~100 MBs) and makes it easier to quickly train a model to learn
    a new concept. LoRA weights are typically loaded into the UNet, text encoder or
    both. There are two classes for loading LoRA weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LoraLoaderMixin` provides functions for loading and unloading, fusing and
    unfusing, enabling and disabling, and more functions for managing LoRA weights.
    This class can be used with any model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StableDiffusionXLLoraLoaderMixin` is a [Stable Diffusion (SDXL)](../../api/pipelines/stable_diffusion/stable_diffusion_xl)
    version of the `LoraLoaderMixin` class for loading and saving LoRA weights. It
    can only be used with the SDXL model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn more about how to load LoRA weights, see the [LoRA](../../using-diffusers/loading_adapters#lora)
    loading guide.
  prefs: []
  type: TYPE_NORMAL
- en: LoraLoaderMixin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.loaders.LoraLoaderMixin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L72)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Load LoRA layers into [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)
    and [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `delete_adapters`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1284)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`Deletes` the LoRA layers of `adapter_name` for the unet and text-encoder(s).
    â€” adapter_names (`Union[List[str], str]`): The names of the adapter to delete.
    Can be a single string or a list of strings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `disable_lora_for_text_encoder`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1208)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text_encoder` (`torch.nn.Module`, *optional*) â€” The text encoder module to
    disable the LoRA layers for. If `None`, it will try to get the `text_encoder`
    attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disables the LoRA layers for the text encoder.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_lora_for_text_encoder`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1225)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text_encoder` (`torch.nn.Module`, *optional*) â€” The text encoder module to
    enable the LoRA layers for. If `None`, it will try to get the `text_encoder` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enables the LoRA layers for the text encoder.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `fuse_lora`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1000)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`fuse_unet` (`bool`, defaults to `True`) â€” Whether to fuse the UNet LoRA parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fuse_text_encoder` (`bool`, defaults to `True`) â€” Whether to fuse the text
    encoder LoRA parameters. If the text encoder wasnâ€™t monkey-patched with the LoRA
    parameters then it wonâ€™t have any effect.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lora_scale` (`float`, defaults to 1.0) â€” Controls how much to influence the
    outputs with the LoRA parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safe_fusing` (`bool`, defaults to `False`) â€” Whether to check fused weights
    for NaN values before fusing and if values are NaN not fusing them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_names` (`List[str]`, *optional*) â€” Adapter names to be used for fusing.
    If nothing is passed, all active adapters will be fused.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fuses the LoRA parameters into the original parameters of the corresponding
    blocks.
  prefs: []
  type: TYPE_NORMAL
- en: This is an experimental API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#### `get_active_adapters`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1308)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Gets the list of the current active adapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#### `get_list_adapters`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1340)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Gets the current list of all available adapters in the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `load_lora_into_text_encoder`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L484)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`state_dict` (`dict`) â€” A standard state dict containing the lora layer parameters.
    The key should be prefixed with an additional `text_encoder` to distinguish between
    unet lora layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`network_alphas` (`Dict[str, float]`) â€” See `LoRALinearLayer` for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder` (`CLIPTextModel`) â€” The text encoder model to load the LoRA
    layers into.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefix` (`str`) â€” Expected prefix of the `text_encoder` in the `state_dict`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lora_scale` (`float`) â€” How much to scale the output of the lora linear layer
    before it is added with the output of the regular lora layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`low_cpu_mem_usage` (`bool`, *optional*, defaults to `True` if torch version
    >= 1.9.0 else `False`) â€” Speed up model loading only loading the pretrained weights
    and not initializing the weights. This also tries to not use more than 1x model
    size in CPU memory (including peak memory) while loading the model. Only supported
    for PyTorch >= 1.9.0\. If you are using an older version of PyTorch, setting this
    argument to `True` will raise an error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`, *optional*) â€” Adapter name to be used for referencing
    the loaded adapter model. If not specified, it will use `default_{i}` where i
    is the total number of adapters being loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This will load the LoRA layers specified in `state_dict` into `text_encoder`
  prefs: []
  type: TYPE_NORMAL
- en: '#### `load_lora_into_transformer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L667)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`state_dict` (`dict`) â€” A standard state dict containing the lora layer parameters.
    The keys can either be indexed directly into the unet or prefixed with an additional
    `unet` which can be used to distinguish between text encoder lora layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`network_alphas` (`Dict[str, float]`) â€” See `LoRALinearLayer` for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet` (`UNet2DConditionModel`) â€” The UNet model to load the LoRA layers into.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`low_cpu_mem_usage` (`bool`, *optional*, defaults to `True` if torch version
    >= 1.9.0 else `False`) â€” Speed up model loading only loading the pretrained weights
    and not initializing the weights. This also tries to not use more than 1x model
    size in CPU memory (including peak memory) while loading the model. Only supported
    for PyTorch >= 1.9.0\. If you are using an older version of PyTorch, setting this
    argument to `True` will raise an error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`, *optional*) â€” Adapter name to be used for referencing
    the loaded adapter model. If not specified, it will use `default_{i}` where i
    is the total number of adapters being loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This will load the LoRA layers specified in `state_dict` into `transformer`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `load_lora_into_unet`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L375)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`state_dict` (`dict`) â€” A standard state dict containing the lora layer parameters.
    The keys can either be indexed directly into the unet or prefixed with an additional
    `unet` which can be used to distinguish between text encoder lora layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`network_alphas` (`Dict[str, float]`) â€” See `LoRALinearLayer` for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet` (`UNet2DConditionModel`) â€” The UNet model to load the LoRA layers into.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`low_cpu_mem_usage` (`bool`, *optional*, defaults to `True` if torch version
    >= 1.9.0 else `False`) â€” Speed up model loading only loading the pretrained weights
    and not initializing the weights. This also tries to not use more than 1x model
    size in CPU memory (including peak memory) while loading the model. Only supported
    for PyTorch >= 1.9.0\. If you are using an older version of PyTorch, setting this
    argument to `True` will raise an error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`, *optional*) â€” Adapter name to be used for referencing
    the loaded adapter model. If not specified, it will use `default_{i}` where i
    is the total number of adapters being loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This will load the LoRA layers specified in `state_dict` into `unet`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `load_lora_weights`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L83)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained_model_name_or_path_or_dict` (`str` or `os.PathLike` or `dict`)
    â€” See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`dict`, *optional*) â€” See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`, *optional*) â€” Adapter name to be used for referencing
    the loaded adapter model. If not specified, it will use `default_{i}` where i
    is the total number of adapters being loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into
    `self.unet` and `self.text_encoder`.
  prefs: []
  type: TYPE_NORMAL
- en: All kwargs are forwarded to `self.lora_state_dict`.
  prefs: []
  type: TYPE_NORMAL
- en: See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)
    for more details on how the state dict is loaded.
  prefs: []
  type: TYPE_NORMAL
- en: See [load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)
    for more details on how the state dict is loaded into `self.unet`.
  prefs: []
  type: TYPE_NORMAL
- en: See [load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)
    for more details on how the state dict is loaded into `self.text_encoder`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `lora_state_dict`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L138)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained_model_name_or_path_or_dict` (`str` or `os.PathLike` or `dict`)
    â€” Can be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained
    model hosted on the Hub.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* (for example `./my_model_directory`) containing the
    model weights saved with [ModelMixin.save_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.save_pretrained).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A [torch state dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`Union[str, os.PathLike]`, *optional*) â€” Path to a directory where
    a downloaded pretrained model configuration is cached if the standard cache is
    not used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`force_download` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to resume downloading the model weights and configuration files. If set to `False`,
    any incompletely downloaded files are deleted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxies` (`Dict[str, str]`, *optional*) â€” A dictionary of proxy servers to
    use by protocol or endpoint, for example, `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`local_files_only` (`bool`, *optional*, defaults to `False`) â€” Whether to only
    load local model weights and configuration files or not. If set to `True`, the
    model wonâ€™t be downloaded from the Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str` or *bool*, *optional*) â€” The token to use as HTTP bearer authorization
    for remote files. If `True`, the token generated from `diffusers-cli login` (stored
    in `~/.huggingface`) is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*, defaults to `"main"`) â€” The specific model version
    to use. It can be a branch name, a tag name, a commit id, or any identifier allowed
    by Git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subfolder` (`str`, *optional*, defaults to `""`) â€” The subfolder location
    of a model file within a larger model repository on the Hub or locally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`low_cpu_mem_usage` (`bool`, *optional*, defaults to `True` if torch version
    >= 1.9.0 else `False`) â€” Speed up model loading only loading the pretrained weights
    and not initializing the weights. This also tries to not use more than 1x model
    size in CPU memory (including peak memory) while loading the model. Only supported
    for PyTorch >= 1.9.0\. If you are using an older version of PyTorch, setting this
    argument to `True` will raise an error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mirror` (`str`, *optional*) â€” Mirror source to resolve accessibility issues
    if youâ€™re downloading a model in China. We do not guarantee the timeliness or
    safety of the source, and you should refer to the mirror site for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return state dict for lora weights and the network alphas.
  prefs: []
  type: TYPE_NORMAL
- en: We support loading A1111 formatted LoRA checkpoints in a limited capacity.
  prefs: []
  type: TYPE_NORMAL
- en: This function is experimental and might change in the future.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_lora_weights`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L869)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`save_directory` (`str` or `os.PathLike`) â€” Directory to save LoRA parameters
    to. Will be created if it doesnâ€™t exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet_lora_layers` (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`)
    â€” State dict of the LoRA layers corresponding to the `unet`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder_lora_layers` (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`)
    â€” State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly
    pass the text encoder LoRA state dict because it comes from ðŸ¤— Transformers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_main_process` (`bool`, *optional*, defaults to `True`) â€” Whether the process
    calling this is the main process or not. Useful during distributed training and
    you need to call this function on all processes. In this case, set `is_main_process=True`
    only on the main process to avoid race conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save_function` (`Callable`) â€” The function to use to save the state dictionary.
    Useful during distributed training when you need to replace `torch.save` with
    another method. Can be configured with the environment variable `DIFFUSERS_SAVE_MODE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) â€” Whether to
    save the model using `safetensors` or the traditional PyTorch way with `pickle`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the LoRA parameters corresponding to the UNet and text encoder.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_adapters_for_text_encoder`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1166)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`adapter_names` (`List[str]` or `str`) â€” The names of the adapters to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder` (`torch.nn.Module`, *optional*) â€” The text encoder module to
    set the adapter layers for. If `None`, it will try to get the `text_encoder` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder_weights` (`List[float]`, *optional*) â€” The weights to use for
    the text encoder. If `None`, the weights are set to `1.0` for all the adapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sets the adapter layers for the text encoder.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_lora_device`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1363)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`adapter_names` (`List[str]`) â€” List of adapters to send device to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`Union[torch.device, str, int]`) â€” Device to send the adapters to.
    Can be either a torch device, a str or an integer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moves the LoRAs listed in `adapter_names` to a target device. Useful for offloading
    the LoRA to the CPU in case you want to load multiple adapters and free some GPU
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `unfuse_lora`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1106)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`unfuse_unet` (`bool`, defaults to `True`) â€” Whether to unfuse the UNet LoRA
    parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unfuse_text_encoder` (`bool`, defaults to `True`) â€” Whether to unfuse the
    text encoder LoRA parameters. If the text encoder wasnâ€™t monkey-patched with the
    LoRA parameters then it wonâ€™t have any effect.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reverses the effect of [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraLoaderMixin.fuse_lora).
  prefs: []
  type: TYPE_NORMAL
- en: This is an experimental API.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `unload_lora_weights`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L968)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Unloads the LoRA parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: StableDiffusionXLLoraLoaderMixin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.loaders.StableDiffusionXLLoraLoaderMixin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1404)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This class overrides `LoraLoaderMixin` with LoRA loading/saving code thatâ€™s
    specific to SDXL
  prefs: []
  type: TYPE_NORMAL
- en: '#### `load_lora_weights`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L1408)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained_model_name_or_path_or_dict` (`str` or `os.PathLike` or `dict`)
    â€” See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`, *optional*) â€” Adapter name to be used for referencing
    the loaded adapter model. If not specified, it will use `default_{i}` where i
    is the total number of adapters being loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`dict`, *optional*) â€” See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into
    `self.unet` and `self.text_encoder`.
  prefs: []
  type: TYPE_NORMAL
- en: All kwargs are forwarded to `self.lora_state_dict`.
  prefs: []
  type: TYPE_NORMAL
- en: See [lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)
    for more details on how the state dict is loaded.
  prefs: []
  type: TYPE_NORMAL
- en: See [load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)
    for more details on how the state dict is loaded into `self.unet`.
  prefs: []
  type: TYPE_NORMAL
- en: See [load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)
    for more details on how the state dict is loaded into `self.text_encoder`.
  prefs: []
  type: TYPE_NORMAL
