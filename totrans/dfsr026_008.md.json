["```py\n>>> from diffusers import DDPMPipeline\n\n>>> ddpm = DDPMPipeline.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")\n>>> image = ddpm(num_inference_steps=25).images[0]\n>>> image\n```", "```py\n>>> from diffusers import DDPMScheduler, UNet2DModel\n\n>>> scheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cat-256\")\n>>> model = UNet2DModel.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")\n```", "```py\n>>> scheduler.set_timesteps(50)\n```", "```py\n>>> scheduler.timesteps\ntensor([980, 960, 940, 920, 900, 880, 860, 840, 820, 800, 780, 760, 740, 720,\n    700, 680, 660, 640, 620, 600, 580, 560, 540, 520, 500, 480, 460, 440,\n    420, 400, 380, 360, 340, 320, 300, 280, 260, 240, 220, 200, 180, 160,\n    140, 120, 100,  80,  60,  40,  20,   0])\n```", "```py\n>>> import torch\n\n>>> sample_size = model.config.sample_size\n>>> noise = torch.randn((1, 3, sample_size, sample_size), device=\"cuda\")\n```", "```py\n>>> input = noise\n\n>>> for t in scheduler.timesteps:\n...     with torch.no_grad():\n...         noisy_residual = model(input, t).sample\n...     previous_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample\n...     input = previous_noisy_sample\n```", "```py\n>>> from PIL import Image\n>>> import numpy as np\n\n>>> image = (input / 2 + 0.5).clamp(0, 1).squeeze()\n>>> image = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy()\n>>> image = Image.fromarray(image)\n>>> image\n```", "```py\n>>> from PIL import Image\n>>> import torch\n>>> from transformers import CLIPTextModel, CLIPTokenizer\n>>> from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n\n>>> vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_safetensors=True)\n>>> tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\")\n>>> text_encoder = CLIPTextModel.from_pretrained(\n...     \"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\", use_safetensors=True\n... )\n>>> unet = UNet2DConditionModel.from_pretrained(\n...     \"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_safetensors=True\n... )\n```", "```py\n>>> from diffusers import UniPCMultistepScheduler\n\n>>> scheduler = UniPCMultistepScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n```", "```py\n>>> torch_device = \"cuda\"\n>>> vae.to(torch_device)\n>>> text_encoder.to(torch_device)\n>>> unet.to(torch_device)\n```", "```py\n>>> prompt = [\"a photograph of an astronaut riding a horse\"]\n>>> height = 512  # default height of Stable Diffusion\n>>> width = 512  # default width of Stable Diffusion\n>>> num_inference_steps = 25  # Number of denoising steps\n>>> guidance_scale = 7.5  # Scale for classifier-free guidance\n>>> generator = torch.manual_seed(0)  # Seed generator to create the initial latent noise\n>>> batch_size = len(prompt)\n```", "```py\n>>> text_input = tokenizer(\n...     prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n... )\n\n>>> with torch.no_grad():\n...     text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n```", "```py\n>>> max_length = text_input.input_ids.shape[-1]\n>>> uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n>>> uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n```", "```py\n>>> text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n```", "```py\n2 ** (len(vae.config.block_out_channels) - 1) == 8\n```", "```py\n>>> latents = torch.randn(\n...     (batch_size, unet.config.in_channels, height // 8, width // 8),\n...     generator=generator,\n...     device=torch_device,\n... )\n```", "```py\n>>> latents = latents * scheduler.init_noise_sigma\n```", "```py\n>>> from tqdm.auto import tqdm\n\n>>> scheduler.set_timesteps(num_inference_steps)\n\n>>> for t in tqdm(scheduler.timesteps):\n...     # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n...     latent_model_input = torch.cat([latents] * 2)\n\n...     latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n\n...     # predict the noise residual\n...     with torch.no_grad():\n...         noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n...     # perform guidance\n...     noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n...     noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n...     # compute the previous noisy sample x_t -> x_t-1\n...     latents = scheduler.step(noise_pred, t, latents).prev_sample\n```", "```py\n# scale and decode the image latents with vae\nlatents = 1 / 0.18215 * latents\nwith torch.no_grad():\n    image = vae.decode(latents).sample\n```", "```py\n>>> image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n>>> image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n>>> images = (image * 255).round().astype(\"uint8\")\n>>> image = Image.fromarray(image)\n>>> image\n```"]