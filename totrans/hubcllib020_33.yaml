- en: Inference Endpoints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/huggingface_hub/package_reference/inference_endpoints](https://huggingface.co/docs/huggingface_hub/package_reference/inference_endpoints)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Inference Endpoints provides a secure production solution to easily deploy models
    on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference
    Endpoint is built from a model from the [Hub](https://huggingface.co/models).
    This page is a reference for `huggingface_hub`’s integration with Inference Endpoints.
    For more information about the Inference Endpoints product, check out its [official
    documentation](https://huggingface.co/docs/inference-endpoints/index).
  prefs: []
  type: TYPE_NORMAL
- en: Check out the [related guide](../guides/inference_endpoints) to learn how to
    use `huggingface_hub` to manage your Inference Endpoints programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: Inference Endpoints can be fully managed via API. The endpoints are documented
    with [Swagger](https://api.endpoints.huggingface.cloud/). The [InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)
    class is a simple wrapper built on top on this API.
  prefs: []
  type: TYPE_NORMAL
- en: Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A subset of the Inference Endpoint features are implemented in [HfApi](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi):'
  prefs: []
  type: TYPE_NORMAL
- en: '[get_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.get_inference_endpoint)
    and [list_inference_endpoints()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.list_inference_endpoints)
    to get information about your Inference Endpoints'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[create_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.create_inference_endpoint),
    [update_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.update_inference_endpoint)
    and [delete_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.delete_inference_endpoint)
    to deploy and manage Inference Endpoints'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[pause_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.pause_inference_endpoint)
    and [resume_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.resume_inference_endpoint)
    to pause and resume an Inference Endpoint'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[scale_to_zero_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.scale_to_zero_inference_endpoint)
    to manually scale an Endpoint to 0 replicas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: InferenceEndpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main dataclass is [InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint).
    It contains information about a deployed `InferenceEndpoint`, including its configuration
    and current state. Once deployed, you can run inference on the Endpoint using
    the [InferenceEndpoint.client](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.client)
    and [InferenceEndpoint.async_client](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.async_client)
    properties that respectively return an [InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)
    and an [AsyncInferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)
    object.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class huggingface_hub.InferenceEndpoint`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L44)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`name` (`str`) — The unique name of the Inference Endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`namespace` (`str`) — The namespace where the Inference Endpoint is located.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`repository` (`str`) — The name of the model repository deployed on this Inference
    Endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`status` ([InferenceEndpointStatus](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointStatus))
    — The current status of the Inference Endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`url` (`str`, *optional*) — The URL of the Inference Endpoint, if available.
    Only a deployed Inference Endpoint will have a URL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`) — The machine learning framework used for the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`) — The specific model revision deployed on the Inference
    Endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`task` (`str`) — The task associated with the deployed model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`created_at` (`datetime.datetime`) — The timestamp when the Inference Endpoint
    was created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`updated_at` (`datetime.datetime`) — The timestamp of the last update of the
    Inference Endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`type` ([InferenceEndpointType](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointType))
    — The type of the Inference Endpoint (public, protected, private).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`raw` (`Dict`) — The raw dictionary data returned from the API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str`, *optional*) — Authentication token for the Inference Endpoint,
    if set when requesting the API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains information about a deployed Inference Endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_raw`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L126)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Initialize object from raw dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `client`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L145)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceClient)'
  prefs: []
  type: TYPE_NORMAL
- en: an inference client pointing to the deployed endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceEndpointError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointError)'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceEndpointError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointError)
    — If the Inference Endpoint is not yet deployed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns a client to make predictions on this Inference Endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `async_client`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L162)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[AsyncInferenceClient](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)'
  prefs: []
  type: TYPE_NORMAL
- en: an asyncio-compatible inference client pointing to the deployed endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceEndpointError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointError)'
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceEndpointError](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpointError)
    — If the Inference Endpoint is not yet deployed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns a client to make predictions on this Inference Endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `delete`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L346)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Delete the Inference Endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: This operation is not reversible. If you don’t want to be charged for an Inference
    Endpoint, it is preferable to pause it with [InferenceEndpoint.pause()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.pause)
    or scale it to zero with [InferenceEndpoint.scale_to_zero()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.scale_to_zero).
  prefs: []
  type: TYPE_NORMAL
- en: This is an alias for [HfApi.delete_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.delete_inference_endpoint).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `fetch`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L217)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
  prefs: []
  type: TYPE_NORMAL
- en: the same Inference Endpoint, mutated in place with the latest data.
  prefs: []
  type: TYPE_NORMAL
- en: Fetch latest information about the Inference Endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `pause`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L296)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
  prefs: []
  type: TYPE_NORMAL
- en: the same Inference Endpoint, mutated in place with the latest data.
  prefs: []
  type: TYPE_NORMAL
- en: Pause the Inference Endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: A paused Inference Endpoint will not be charged. It can be resumed at any time
    using [InferenceEndpoint.resume()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.resume).
    This is different than scaling the Inference Endpoint to zero with [InferenceEndpoint.scale_to_zero()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.scale_to_zero),
    which would be automatically restarted when a request is made to it.
  prefs: []
  type: TYPE_NORMAL
- en: This is an alias for [HfApi.pause_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.pause_inference_endpoint).
    The current object is mutated in place with the latest data from the server.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `resume`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L314)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
  prefs: []
  type: TYPE_NORMAL
- en: the same Inference Endpoint, mutated in place with the latest data.
  prefs: []
  type: TYPE_NORMAL
- en: Resume the Inference Endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: This is an alias for [HfApi.resume_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.resume_inference_endpoint).
    The current object is mutated in place with the latest data from the server.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `scale_to_zero`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L328)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
  prefs: []
  type: TYPE_NORMAL
- en: the same Inference Endpoint, mutated in place with the latest data.
  prefs: []
  type: TYPE_NORMAL
- en: Scale Inference Endpoint to zero.
  prefs: []
  type: TYPE_NORMAL
- en: An Inference Endpoint scaled to zero will not be charged. It will be resume
    on the next request to it, with a cold start delay. This is different than pausing
    the Inference Endpoint with [InferenceEndpoint.pause()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.pause),
    which would require a manual resume with [InferenceEndpoint.resume()](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint.resume).
  prefs: []
  type: TYPE_NORMAL
- en: This is an alias for [HfApi.scale_to_zero_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.scale_to_zero_inference_endpoint).
    The current object is mutated in place with the latest data from the server.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `update`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L228)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`accelerator` (`str`, *optional*) — The hardware accelerator to be used for
    inference (e.g. `"cpu"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`instance_size` (`str`, *optional*) — The size or type of the instance to be
    used for hosting the model (e.g. `"large"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`instance_type` (`str`, *optional*) — The cloud instance type where the Inference
    Endpoint will be deployed (e.g. `"c6i"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_replica` (`int`, *optional*) — The minimum number of replicas (instances)
    to keep running for the Inference Endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_replica` (`int`, *optional*) — The maximum number of replicas (instances)
    to scale to for the Inference Endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`repository` (`str`, *optional*) — The name of the model repository associated
    with the Inference Endpoint (e.g. `"gpt2"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The machine learning framework used for the
    model (e.g. `"custom"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*) — The specific model revision to deploy on the
    Inference Endpoint (e.g. `"6c0e6080953db56375760c0471a8c5f2929baf11"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`task` (`str`, *optional*) — The task on which to deploy the model (e.g. `"text-classification"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
  prefs: []
  type: TYPE_NORMAL
- en: the same Inference Endpoint, mutated in place with the latest data.
  prefs: []
  type: TYPE_NORMAL
- en: Update the Inference Endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: This method allows the update of either the compute configuration, the deployed
    model, or both. All arguments are optional but at least one must be provided.
  prefs: []
  type: TYPE_NORMAL
- en: This is an alias for [HfApi.update_inference_endpoint()](/docs/huggingface_hub/v0.20.3/en/package_reference/hf_api#huggingface_hub.HfApi.update_inference_endpoint).
    The current object is mutated in place with the latest data from the server.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `wait`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L179)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`timeout` (`int`, *optional*) — The maximum time to wait for the Inference
    Endpoint to be deployed, in seconds. If `None`, will wait indefinitely.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`refresh_every` (`int`, *optional*) — The time to wait between each fetch of
    the Inference Endpoint status, in seconds. Defaults to 5s.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)'
  prefs: []
  type: TYPE_NORMAL
- en: the same Inference Endpoint, mutated in place with the latest data.
  prefs: []
  type: TYPE_NORMAL
- en: Wait for the Inference Endpoint to be deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Information from the server will be fetched every 1s. If the Inference Endpoint
    is not deployed after `timeout` seconds, a `InferenceEndpointTimeoutError` will
    be raised. The [InferenceEndpoint](/docs/huggingface_hub/v0.20.3/en/package_reference/inference_endpoints#huggingface_hub.InferenceEndpoint)
    will be mutated in place with the latest data.
  prefs: []
  type: TYPE_NORMAL
- en: InferenceEndpointStatus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class huggingface_hub.InferenceEndpointStatus`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L27)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: An enumeration.
  prefs: []
  type: TYPE_NORMAL
- en: InferenceEndpointType
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class huggingface_hub.InferenceEndpointType`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L38)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: An enumeration.
  prefs: []
  type: TYPE_NORMAL
- en: InferenceEndpointError
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class huggingface_hub.InferenceEndpointError`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/huggingface_hub/blob/v0.20.3/src/huggingface_hub/_inference_endpoints.py#L19)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Generic exception when dealing with Inference Endpoints.
  prefs: []
  type: TYPE_NORMAL
