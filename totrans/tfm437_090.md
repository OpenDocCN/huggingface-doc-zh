# 测试

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/testing](https://huggingface.co/docs/transformers/v4.37.2/en/testing)

让我们看看🤗 Transformers模型是如何测试的，以及您如何编写新测试并改进现有测试。

存储库中有2个测试套件：

1.  `tests` — 用于一般API的测试

1.  `examples` — 主要用于不属于API的各种应用的测试

## 如何测试transformers

1.  一旦提交了PR，它将通过9个CircleCi作业进行测试。对该PR的每个新提交都会重新测试。这些作业在此[配置文件](https://github.com/huggingface/transformers/tree/main/.circleci/config.yml)中定义，因此如果需要，您可以在您的机器上重现相同的环境。

    这些CI作业不运行`@slow`测试。

1.  由[github actions](https://github.com/huggingface/transformers/actions)运行3个作业：

    +   [torch hub集成](https://github.com/huggingface/transformers/tree/main/.github/workflows/github-torch-hub.yml)：检查torch hub集成是否正常工作。

    +   [自托管（推送）](https://github.com/huggingface/transformers/tree/main/.github/workflows/self-push.yml)：仅在`main`上的提交上在GPU上运行快速测试。仅在`main`上的提交更新了以下文件夹中的代码时才运行：`src`，`tests`，`.github`（以防止在添加模型卡、笔记本等时运行）。

    +   [自托管的运行器](https://github.com/huggingface/transformers/tree/main/.github/workflows/self-scheduled.yml)：在`tests`和`examples`中的GPU上运行正常和慢速测试：

```py
RUN_SLOW=1 pytest tests/
RUN_SLOW=1 pytest examples/
```

结果可以在[此处](https://github.com/huggingface/transformers/actions)观察。

## 运行测试

### 选择要运行的测试

本文详细介绍了如何运行测试。如果阅读完所有内容后，您需要更多细节，您可以在[此处](https://docs.pytest.org/en/latest/usage.html)找到它们。

以下是运行测试的一些最有用的方法。

运行所有：

```py
pytest
```

或：

```py
make test
```

请注意，后者被定义为：

```py
python -m pytest -n auto --dist=loadfile -s -v ./tests/
```

告诉pytest：

+   运行与CPU核心数量相同的测试进程（如果RAM不足可能会太多！）

+   确保同一文件中的所有测试将由同一个测试进程运行

+   不捕获输出

+   以详细模式运行

### 获取所有测试的列表

测试套件的所有测试：

```py
pytest --collect-only -q
```

给定测试文件的所有测试：

```py
pytest tests/test_optimization.py --collect-only -q
```

### 运行特定测试模块

要运行单个测试模块：

```py
pytest tests/utils/test_logging.py
```

### 运行特定测试

由于大多数测试中使用了unittest，要运行特定的子测试，您需要知道包含这些测试的unittest类的名称。例如，可能是：

```py
pytest tests/test_optimization.py::OptimizationTest::test_adam_w
```

这里：

+   `tests/test_optimization.py` - 具有测试的文件

+   `OptimizationTest` - 类的名称

+   `test_adam_w` - 特定测试函数的名称

如果文件包含多个类，您可以选择仅运行给定类的测试。例如：

```py
pytest tests/test_optimization.py::OptimizationTest
```

将运行该类中的所有测试。

如前所述，您可以通过运行以下内容查看`OptimizationTest`类中包含的所有测试：

```py
pytest tests/test_optimization.py::OptimizationTest --collect-only -q
```

您可以通过关键字表达式运行测试。

仅运行包含`adam`名称的测试：

```py
pytest -k adam tests/test_optimization.py
```

逻辑`and`和`or`可用于指示是否应匹配所有关键字或任一关键字。`not`可用于否定。

要运行除包含`adam`的名称的测试之外的所有测试：

```py
pytest -k "not adam" tests/test_optimization.py
```

您可以将这两种模式结合在一起：

```py
pytest -k "ada and not adam" tests/test_optimization.py
```

例如，要同时运行`test_adafactor`和`test_adam_w`，您可以使用：

```py
pytest -k "test_adam_w or test_adam_w" tests/test_optimization.py
```

请注意，我们在这里使用`or`，因为我们希望关键字中的任何一个匹配以包括两者。

如果要仅包含包含两种模式的测试，应使用`and`：

```py
pytest -k "test and ada" tests/test_optimization.py
```

### 运行加速测试

有时您需要在模型上运行`accelerate`测试。为此，您只需将`-m accelerate_tests`添加到您的命令中，例如，如果您想在`OPT`上运行这些测试：

```py
RUN_SLOW=1 pytest -m accelerate_tests tests/models/opt/test_modeling_opt.py 
```

### 运行文档测试

为了测试文档示例是否正确，您应该检查`doctests`是否通过。例如，让我们使用[`WhisperModel.forward`的文档字符串](https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/modeling_whisper.py#L1017-L1035)：

```py
r"""
Returns:

Example:
    ```python

    >>> import torch

    >>> from transformers import WhisperModel, WhisperFeatureExtractor

    从数据集导入数据集

    >>> model = WhisperModel.from_pretrained("openai/whisper-base")

    >>> feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-base")

    >>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")

    >>> inputs = feature_extractor(ds[0]["audio"]["array"], return_tensors="pt")

    >>> input_features = inputs.input_features

    >>> decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id

    >>> last_hidden_state = model(input_features, decoder_input_ids=decoder_input_ids).last_hidden_state

    >>> list(last_hidden_state.shape)

    [1, 2, 512]

    ```py"""

```

只需运行以下行，自动测试所需文件中的每个文档字符串示例：

```py
pytest --doctest-modules <path_to_file_or_dir>
```

如果文件具有markdown扩展名，应添加`--doctest-glob="*.md"`参数。

### 仅运行修改后的测试

您可以通过使用[pytest-picked](https://github.com/anapaulagomes/pytest-picked)来运行与未暂存文件或当前分支（根据Git）相关的测试。这是一个快速测试您的更改是否破坏了任何内容的好方法，因为它不会运行与您未触及的文件相关的测试。

```py
pip install pytest-picked
```

```py
pytest --picked
```

将从已修改但尚未提交的文件和文件夹中运行所有测试。

### 在源代码修改时自动重新运行失败的测试

[pytest-xdist](https://github.com/pytest-dev/pytest-xdist)提供了一个非常有用的功能，可以检测所有失败的测试，然后等待您修改文件并持续重新运行这些失败的测试，直到它们通过，同时您修复它们。这将重复进行，直到所有测试通过，之后再次执行完整运行。

```py
pip install pytest-xdist
```

要进入模式：`pytest -f`或`pytest --looponfail`

通过查看`looponfailroots`根目录及其所有内容（递归）来检测文件更改。如果此值的默认值对您不起作用，您可以通过在`setup.cfg`中设置配置选项来更改项目中的值：

```py
[tool:pytest]
looponfailroots = transformers tests
```

或`pytest.ini`/`tox.ini`文件：

```py
[pytest]
looponfailroots = transformers tests
```

这将导致仅查找相对于ini文件目录指定的相应目录中的文件更改。

[pytest-watch](https://github.com/joeyespo/pytest-watch)是此功能的另一种实现。

### 跳过一个测试模块

如果您想运行所有测试模块，除了一些，您可以通过提供一个明确的测试运行列表来排除它们。例如，要运行除了`test_modeling_*.py`测试之外的所有测试：

```py
pytest *ls -1 tests/*py | grep -v test_modeling*
```

### 清除状态

CI构建和隔离重要时（针对速度），应清除缓存：

```py
pytest --cache-clear tests
```

### 并行运行测试

如前所述，`make test`通过`pytest-xdist`插件（`-n X`参数，例如`-n 2`以运行2个并行作业）并行运行测试。

`pytest-xdist`的`--dist=`选项允许控制如何对测试进行分组。`--dist=loadfile`将位于一个文件中的测试放在同一个进程中。

由于执行测试的顺序不同且不可预测，如果使用`pytest-xdist`运行测试套件会产生失败（意味着我们有一些未检测到的耦合测试），请使用[pytest-replay](https://github.com/ESSS/pytest-replay)以相同顺序重放测试，这应该有助于以某种方式将失败序列减少到最小。

### 测试顺序和重复

最好多次重复测试，按顺序、随机或成组进行，以检测任何潜在的相互依赖和与状态相关的错误（拆除）。直接的多次重复只是用来检测由于DL的随机性而暴露出的一些问题。

#### 重复测试

+   [pytest-flakefinder](https://github.com/dropbox/pytest-flakefinder)：

```py
pip install pytest-flakefinder
```

然后多次运行每个测试（默认为50次）：

```py
pytest --flake-finder --flake-runs=5 tests/test_failing_test.py
```

此插件不与`pytest-xdist`的`-n`标志一起使用。

还有另一个插件`pytest-repeat`，但它与`unittest`不兼容。

#### 以随机顺序运行测试

```py
pip install pytest-random-order
```

重要提示：存在`pytest-random-order`将自动随机化测试，无需更改配置或命令行选项。

正如前面所解释的，这允许检测耦合测试 - 一个测试的状态影响另一个测试的状态。当安装了`pytest-random-order`时，它将打印用于该会话的随机种子，例如：

```py
pytest tests
[...]
Using --random-order-bucket=module
Using --random-order-seed=573663
```

因此，如果给定的特定顺序失败，您可以通过添加确切的种子来重现它，例如：

```py
pytest --random-order-seed=573663
[...]
Using --random-order-bucket=module
Using --random-order-seed=573663
```

只有在使用完全相同的测试列表（或根本没有列表）时，它才会重现确切的顺序。一旦开始手动缩小列表，就不能再依赖种子，而必须按照它们失败的确切顺序手动列出它们，并告诉pytest不要随机化它们，而是使用`--random-order-bucket=none`，例如：

```py
pytest --random-order-bucket=none tests/test_a.py tests/test_c.py tests/test_b.py
```

要禁用所有测试的洗牌：

```py
pytest --random-order-bucket=none
```

默认情况下，隐含`--random-order-bucket=module`，这将在模块级别对文件进行洗牌。它还可以在`class`、`package`、`global`和`none`级别进行洗牌。有关完整详情，请参阅其[文档](https://github.com/jbasko/pytest-random-order)。

另一个随机化的替代方案是：[`pytest-randomly`](https://github.com/pytest-dev/pytest-randomly)。这个模块具有非常相似的功能/接口，但它没有`pytest-random-order`中可用的桶模式。一旦安装，它也会强制自身。

### 外观和感觉变化

#### pytest-sugar

[pytest-sugar](https://github.com/Frozenball/pytest-sugar) 是一个插件，可以改善外观和感觉，添加进度条，并立即显示失败的测试和断言。安装后会自动激活。

```py
pip install pytest-sugar
```

要在没有它的情况下运行测试，请运行：

```py
pytest -p no:sugar
```

或者卸载它。

#### 报告每个子测试的名称及其进度

通过`pytest`运行单个或一组测试（在`pip install pytest-pspec`之后）：

```py
pytest --pspec tests/test_optimization.py
```

#### 立即显示失败的测试

[pytest-instafail](https://github.com/pytest-dev/pytest-instafail) 立即显示失败和错误，而不是等到测试会话结束。

```py
pip install pytest-instafail
```

```py
pytest --instafail
```

### 要GPU还是不要GPU

在启用GPU的设置中，要在仅CPU模式下测试，请添加`CUDA_VISIBLE_DEVICES=""`：

```py
CUDA_VISIBLE_DEVICES="" pytest tests/utils/test_logging.py
```

或者如果您有多个GPU，可以指定`pytest`要使用的GPU。例如，如果您有GPU `0` 和 `1`，则可以仅使用第二个GPU运行：

```py
CUDA_VISIBLE_DEVICES="1" pytest tests/utils/test_logging.py
```

这在您想要在不同的GPU上运行不同任务时非常方便。

一些测试必须在仅CPU上运行，其他测试可以在CPU或GPU或TPU上运行，另一些测试可以在多个GPU上运行。以下跳过装饰器用于设置测试的CPU/GPU/TPU要求：

+   `require_torch` - 此测试仅在torch下运行

+   `require_torch_gpu` - 与`require_torch`相同，至少需要1个GPU

+   `require_torch_multi_gpu` - 与`require_torch`相同，至少需要2个GPU

+   `require_torch_non_multi_gpu` - 与`require_torch`相同，需要0个或1个GPU

+   `require_torch_up_to_2_gpus` - 与`require_torch`相同，需要0个或1个或2个GPU

+   `require_torch_tpu` - 与`require_torch`相同，至少需要1个TPU

让我们在下表中描述GPU要求：

| n个GPU | 装饰器 | | --------+-------------------------------- | | `>= 0` | `@require_torch` | | `>= 1` | `@require_torch_gpu` | | `>= 2` | `@require_torch_multi_gpu` | | `< 2` | `@require_torch_non_multi_gpu` | | `< 3` | `@require_torch_up_to_2_gpus` |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |

例如，这是一个必须在有2个或更多个GPU可用且已安装pytorch时运行的测试：

```py
@require_torch_multi_gpu
def test_example_with_multi_gpu():
```

如果一个测试需要`tensorflow`，请使用`require_tf`装饰器。例如：

```py
@require_tf
def test_tf_thing_with_tensorflow():
```

这些装饰器可以叠加。例如，如果一个测试很慢并且在pytorch下至少需要一个GPU，这是如何设置的：

```py
@require_torch_gpu
@slow
def test_example_slow_on_gpu():
```

一些装饰器如`@parametrized`会重写测试名称，因此`@require_*`跳过装饰器必须在最后列出才能正常工作。以下是正确使用的示例：

```py
@parameterized.expand(...)
@require_torch_multi_gpu
def test_integration_foo():
```

这个顺序问题在`@pytest.mark.parametrize`中不存在，您可以将其放在最前面或最后面，它仍然有效。但它只适用于非单元测试。

在测试中：

+   有多少个GPU可用：

```py
from transformers.testing_utils import get_gpu_count

n_gpu = get_gpu_count()  # works with torch and tf
```

### 使用特定的PyTorch后端或设备进行测试

要在特定的torch设备上运行测试套件，请添加`TRANSFORMERS_TEST_DEVICE="$device"`，其中`$device`是目标后端。例如，要仅在CPU上测试：

```py
TRANSFORMERS_TEST_DEVICE="cpu" pytest tests/utils/test_logging.py
```

此变量对于测试自定义或不太常见的PyTorch后端（如`mps`）很有用。它还可以用于通过定位特定GPU或在仅CPU模式下进行测试来实现与`CUDA_VISIBLE_DEVICES`相同的效果。

在第一次导入`torch`后，某些设备将需要额外的导入。这可以使用环境变量`TRANSFORMERS_TEST_BACKEND`指定：

```py
TRANSFORMERS_TEST_BACKEND="torch_npu" pytest tests/utils/test_logging.py
```

替代后端可能还需要替换特定于设备的函数。例如，`torch.cuda.manual_seed`可能需要替换为特定于设备的种子设置器，如`torch.npu.manual_seed`，以正确设置设备上的随机种子。在运行测试套件时指定新的后端和后端特定设备函数时，创建一个Python设备规范文件，格式如下：

```py
import torch
import torch_npu
# !! Further additional imports can be added here !!

# Specify the device name (eg. 'cuda', 'cpu', 'npu')
DEVICE_NAME = 'npu'

# Specify device-specific backends to dispatch to.
# If not specified, will fallback to 'default' in 'testing_utils.py`
MANUAL_SEED_FN = torch.npu.manual_seed
EMPTY_CACHE_FN = torch.npu.empty_cache
DEVICE_COUNT_FN = torch.npu.device_count
```

此格式还允许指定所需的任何其他导入。要使用此文件替换测试套件中的等效方法，请将环境变量`TRANSFORMERS_TEST_DEVICE_SPEC`设置为规范文件的路径。

目前，只支持`MANUAL_SEED_FN`、`EMPTY_CACHE_FN`和`DEVICE_COUNT_FN`用于特定设备的分发。

### 分布式训练

`pytest`不能直接处理分布式训练。如果尝试这样做-子进程不会做正确的事情，最终会认为它们是`pytest`并开始循环运行测试套件。但是，如果生成一个正常进程，然后生成多个工作进程并管理IO管道，则可以正常工作。

以下是一些使用它的测试：

+   [test_trainer_distributed.py](https://github.com/huggingface/transformers/tree/main/tests/trainer/test_trainer_distributed.py)

+   [test_deepspeed.py](https://github.com/huggingface/transformers/tree/main/tests/deepspeed/test_deepspeed.py)

要直接跳转到执行点，请在这些测试中搜索`execute_subprocess_async`调用。

您至少需要2个GPU才能看到这些测试的运行情况：

```py
CUDA_VISIBLE_DEVICES=0,1 RUN_SLOW=1 pytest -sv tests/test_trainer_distributed.py
```

### 输出捕获

在测试执行期间，发送到`stdout`和`stderr`的任何输出都将被捕获。如果测试或设置方法失败，则通常会显示其相应的捕获输出以及失败的回溯。

要禁用输出捕获并正常获取`stdout`和`stderr`，请使用`-s`或`--capture=no`：

```py
pytest -s tests/utils/test_logging.py
```

将测试结果发送到JUnit格式的输出：

```py
py.test tests --junitxml=result.xml
```

### 颜色控制

要没有颜色（例如，白色背景上的黄色不可读）：

```py
pytest --color=no tests/utils/test_logging.py
```

### 将测试报告发送到在线粘贴服务

为每个测试失败创建一个URL：

```py
pytest --pastebin=failed tests/utils/test_logging.py
```

这将向远程粘贴服务提交测试运行信息，并为每个失败提供一个URL。您可以像往常一样选择测试，或者例如添加`-x`，如果您只想发送一个特定的失败。

为整个测试会话日志创建一个URL：

```py
pytest --pastebin=all tests/utils/test_logging.py
```

## 编写测试

🤗 transformers测试基于`unittest`，但由`pytest`运行，因此大多数情况下可以使用两个系统的功能。

您可以在[这里](https://docs.pytest.org/en/stable/unittest.html)阅读支持的功能，但要记住的重要事情是大多数`pytest`固定装置不起作用。也不是参数化，但我们使用模块`parameterized`以类似的方式工作。

### 参数化

经常需要多次运行相同的测试，但使用不同的参数。可以从测试内部完成，但是那样就无法仅为一个参数集运行该测试。

```py
# test_this1.py
import unittest
from parameterized import parameterized

class TestMathUnitTest(unittest.TestCase):
 @parameterized.expand( [
            ("negative", -1.5, -2.0),
            ("integer", 1, 1.0),
            ("large fraction", 1.6, 1),
        ] )
    def test_floor(self, name, input, expected):
        assert_equal(math.floor(input), expected)
```

现在，默认情况下，此测试将运行3次，每次将`test_floor`的最后3个参数分配给参数列表中的相应参数。

您可以仅运行`negative`和`integer`参数集：

```py
pytest -k "negative and integer" tests/test_mytest.py
```

或除了`negative`子测试外的所有子测试，使用：

```py
pytest -k "not negative" tests/test_mytest.py
```

除了使用刚提到的`-k`过滤器，您可以找出每个子测试的确切名称，并使用其确切名称运行任何或所有子测试。

```py
pytest test_this1.py --collect-only -q
```

它将列出：

```py
test_this1.py::TestMathUnitTest::test_floor_0_negative
test_this1.py::TestMathUnitTest::test_floor_1_integer
test_this1.py::TestMathUnitTest::test_floor_2_large_fraction
```

现在您可以仅运行2个特定的子测试：

```py
pytest test_this1.py::TestMathUnitTest::test_floor_0_negative  test_this1.py::TestMathUnitTest::test_floor_1_integer
```

模块[parameterized](https://pypi.org/project/parameterized/)已经是`transformers`的开发依赖项，适用于`unittests`和`pytest`测试。

但是，如果测试不是`unittest`，则可以使用`pytest.mark.parametrize`（或者您可能会看到它在一些现有测试中使用，主要在`examples`下）。

以下是相同的示例，这次使用`pytest`的`parametrize`标记：

```py
# test_this2.py
import pytest

@pytest.mark.parametrize( "name, input, expected",
    [
        ("negative", -1.5, -2.0),
        ("integer", 1, 1.0),
        ("large fraction", 1.6, 1),
    ], )
def test_floor(name, input, expected):
    assert_equal(math.floor(input), expected)
```

与`parameterized`一样，使用`pytest.mark.parametrize`可以对要运行的子测试进行精细控制，如果`-k`过滤器无法完成任务。除此之外，此参数化函数为子测试创建了一组略有不同的名称。以下是它们的样子：

```py
pytest test_this2.py --collect-only -q
```

它将列出：

```py
test_this2.py::test_floor[integer-1-1.0]
test_this2.py::test_floor[negative--1.5--2.0]
test_this2.py::test_floor[large fraction-1.6-1]
```

现在您可以仅运行特定的测试：

```py
pytest test_this2.py::test_floor[negative--1.5--2.0] test_this2.py::test_floor[integer-1-1.0]
```

与前面的示例相同。

### 文件和目录

在测试中，我们经常需要知道事物相对于当前测试文件的位置，这并不是微不足道的，因为测试可能会从多个目录调用，或者可能位于具有不同深度的子目录中。一个辅助类`transformers.test_utils.TestCasePlus`通过整理所有基本路径并提供易于访问的访问器来解决这个问题：

+   `pathlib`对象（全部完全解析）：

    +   `test_file_path` - 当前测试文件路径，即`__file__`

    +   `test_file_dir` - 包含当前测试文件的目录

    +   `tests_dir` - `tests`测试套件的目录

    +   `examples_dir` - `examples`测试套件的目录

    +   `repo_root_dir` - 仓库的目录

    +   `src_dir` - `src`的目录（即`transformers`子目录所在的地方）

+   字符串化路径---与上述相同，但这些返回路径作为字符串，而不是`pathlib`对象：

    +   `test_file_path_str`

    +   `test_file_dir_str`

    +   `tests_dir_str`

    +   `examples_dir_str`

    +   `repo_root_dir_str`

    +   `src_dir_str`

要开始使用这些，您只需要确保测试位于`transformers.test_utils.TestCasePlus`的子类中。例如：

```py
from transformers.testing_utils import TestCasePlus

class PathExampleTest(TestCasePlus):
    def test_something_involving_local_locations(self):
        data_dir = self.tests_dir / "fixtures/tests_samples/wmt_en_ro"
```

如果您不需要通过`pathlib`操纵路径，或者只需要路径作为字符串，您可以始终在`pathlib`对象上调用`str()`或使用以`_str`结尾的访问器。例如：

```py
from transformers.testing_utils import TestCasePlus

class PathExampleTest(TestCasePlus):
    def test_something_involving_stringified_locations(self):
        examples_dir = self.examples_dir_str
```

### 临时文件和目录

使用唯一的临时文件和目录对于并行测试运行至关重要，以便测试不会覆盖彼此的数据。此外，我们希望在每个创建它们的测试结束时删除临时文件和目录。因此，使用像`tempfile`这样满足这些需求的软件包是至关重要的。

然而，在调试测试时，您需要能够看到临时文件或目录中的内容，并且希望知道其确切路径，而不是在每次测试重新运行时随机化。

辅助类`transformers.test_utils.TestCasePlus`最适合用于这些目的。它是`unittest.TestCase`的子类，因此我们可以在测试模块中轻松继承它。

以下是其用法示例：

```py
from transformers.testing_utils import TestCasePlus

class ExamplesTests(TestCasePlus):
    def test_whatever(self):
        tmp_dir = self.get_auto_remove_tmp_dir()
```

此代码创建一个唯一的临时目录，并将`tmp_dir`设置为其位置。

+   创建一个唯一的临时目录：

```py
def test_whatever(self):
    tmp_dir = self.get_auto_remove_tmp_dir()
```

`tmp_dir`将包含创建的临时目录的路径。它将在测试结束时自动删除。

+   创建我选择的临时目录，在测试开始之前确保它为空，并在测试结束后不清空它。

```py
def test_whatever(self):
    tmp_dir = self.get_auto_remove_tmp_dir("./xxx")
```

当您想要监视特定目录并确保之前的测试没有在其中留下任何数据时，这很有用。

+   您可以通过直接覆盖`before`和`after`参数来覆盖默认行为，从而导致以下行为之一：

    +   `before=True`：临时目录将始终在测试开始时清除。

    +   `before=False`：如果临时目录已经存在，则任何现有文件将保留在那里。

    +   `after=True`：临时目录将始终在测试结束时被删除。

    +   `after=False`：临时目录将始终在测试结束时保持不变。

为了安全运行等效于`rm -r`的操作，只允许项目存储库检出的子目录，如果使用了显式的`tmp_dir`，则不会错误地删除任何`/tmp`或类似的文件系统重要部分。即请始终传递以`./`开头的路径。

每个测试可以注册多个临时目录，它们都将自动删除，除非另有要求。

### 临时sys.path覆盖

如果需要临时覆盖`sys.path`以从另一个测试中导入，例如，可以使用`ExtendSysPath`上下文管理器。示例：

```py
import os
from transformers.testing_utils import ExtendSysPath

bindir = os.path.abspath(os.path.dirname(__file__))
with ExtendSysPath(f"{bindir}/.."):
    from test_trainer import TrainerIntegrationCommon  # noqa
```

### 跳过测试

当发现错误并编写新测试但尚未修复错误时，这很有用。为了能够将其提交到主存储库，我们需要确保在`make test`期间跳过它。

方法：

+   **skip**表示您期望测试仅在满足某些条件时才通过，否则pytest应该跳过运行测试。常见示例是在非Windows平台上跳过仅适用于Windows的测试，或者跳过依赖于当前不可用的外部资源的测试（例如数据库）。

+   **xfail**表示您期望由于某种原因测试失败。一个常见示例是测试尚未实现的功能，或者尚未修复的错误。当一个测试尽管预期失败（标记为pytest.mark.xfail）仍然通过时，它是一个xpass，并将在测试摘要中报告。

两者之间的一个重要区别是，`skip`不运行测试，而`xfail`会运行。因此，如果导致一些糟糕状态的有缺陷代码会影响其他测试，请不要使用`xfail`。

#### 实施

+   以下是如何无条件跳过整个测试：

```py
@unittest.skip("this bug needs to be fixed")
def test_feature_x():
```

或通过pytest：

```py
@pytest.mark.skip(reason="this bug needs to be fixed")
```

或`xfail`方式：

```py
@pytest.mark.xfail
def test_feature_x():
```

以下是如何根据测试内部检查跳过测试的方法：

```py
def test_feature_x():
    if not has_something():
        pytest.skip("unsupported configuration")
```

或整个模块：

```py
import pytest

if not pytest.config.getoption("--custom-flag"):
    pytest.skip("--custom-flag is missing, skipping tests", allow_module_level=True)
```

或`xfail`方式：

```py
def test_feature_x():
    pytest.xfail("expected to fail until bug XYZ is fixed")
```

+   以下是如何跳过模块中的所有测试，如果某个导入丢失：

```py
docutils = pytest.importorskip("docutils", minversion="0.3")
```

+   根据条件跳过测试：

```py
@pytest.mark.skipif(sys.version_info < (3,6), reason="requires python3.6 or higher")
def test_feature_x():
```

或：

```py
@unittest.skipIf(torch_device == "cpu", "Can't do half precision")
def test_feature_x():
```

或跳过整个模块：

```py
@pytest.mark.skipif(sys.platform == 'win32', reason="does not run on windows")
class TestClass():
    def test_feature_x(self):
```

更多详细信息、示例和方法请参见[这里](https://docs.pytest.org/en/latest/skipping.html)。

### 慢速测试

测试库不断增长，一些测试需要几分钟才能运行，因此我们无法等待一个小时才能完成CI的测试套件。因此，除了一些必要测试的例外，慢速测试应该标记为下面的示例：

```py
from transformers.testing_utils import slow
@slow
def test_integration_foo():
```

一旦将测试标记为`@slow`，要运行这些测试，请设置`RUN_SLOW=1`环境变量，例如：

```py
RUN_SLOW=1 pytest tests
```

一些装饰器如`@parameterized`会重写测试名称，因此`@slow`和其他跳过装饰器`@require_*`必须在最后列出才能正常工作。以下是正确使用的示例：

```py
@parameteriz ed.expand(...)
@slow
def test_integration_foo():
```

正如本文档开头所解释的，慢速测试会定期运行，而不是在PR的CI检查中运行。因此，可能会在提交PR时错过一些问题并合并。这些问题将在下一个定期CI作业中被捕获。但这也意味着在提交PR之前在您的计算机上运行慢速测试非常重要。

以下是选择哪些测试应标记为慢速的大致决策机制：

如果测试侧重于库的内部组件（例如，建模文件、标记文件、流水线），那么我们应该在非慢速测试套件中运行该测试。如果侧重于库的其他方面，例如文档或示例，则应在慢速测试套件中运行这些测试。然后，为了完善这种方法，我们应该有例外情况：

+   所有需要下载一组大量权重或大于~50MB的数据集（例如，模型或分词器集成测试，管道集成测试）的测试都应该设置为慢速。如果要添加新模型，应该创建并上传到hub一个其微型版本（具有随机权重）用于集成测试。这将在以下段落中讨论。

+   所有需要进行训练但没有专门优化为快速的测试都应该设置为慢速。

+   如果其中一些应该是非慢速测试的测试非常慢，可以引入异常，并将它们设置为`@slow`。自动建模测试，将大文件保存和加载到磁盘上，是一个被标记为`@slow`的测试的很好的例子。

+   如果一个测试在CI上完成时间不到1秒（包括下载），那么它应该是一个正常的测试。

总的来说，所有非慢速测试都需要完全覆盖不同的内部功能，同时保持快速。例如，通过使用具有随机权重的特别创建的微型模型进行测试，可以实现显著的覆盖范围。这些模型具有最小数量的层（例如2），词汇量（例如1000）等。然后`@slow`测试可以使用大型慢速模型进行定性测试。要查看这些的用法，只需搜索带有*tiny*的模型：

```py
grep tiny tests examples
```

这里是一个[脚本](https://github.com/huggingface/transformers/tree/main/scripts/fsmt/fsmt-make-tiny-model.py)的示例，创建了微型模型[stas/tiny-wmt19-en-de](https://huggingface.co/stas/tiny-wmt19-en-de)。您可以轻松调整它以适应您特定模型的架构。

如果例如下载一个巨大模型的开销很大，那么很容易错误地测量运行时间，但如果您在本地测试它，下载的文件将被缓存，因此下载时间不会被测量。因此，应该查看CI日志中的执行速度报告（`pytest --durations=0 tests`的输出）。

该报告还有助于找到未标记为慢的慢异常值，或者需要重新编写以提高速度的测试。如果您注意到CI上的测试套件开始变慢，此报告的顶部列表将显示最慢的测试。

### 测试stdout/stderr输出

为了测试写入`stdout`和/或`stderr`的函数，测试可以使用`pytest`的[capsys系统](https://docs.pytest.org/en/latest/capture.html)来访问这些流。以下是实现这一目标的方法：

```py
import sys

def print_to_stdout(s):
    print(s)

def print_to_stderr(s):
    sys.stderr.write(s)

def test_result_and_stdout(capsys):
    msg = "Hello"
    print_to_stdout(msg)
    print_to_stderr(msg)
    out, err = capsys.readouterr()  # consume the captured output streams
    # optional: if you want to replay the consumed streams:
    sys.stdout.write(out)
    sys.stderr.write(err)
    # test:
    assert msg in out
    assert msg in err
```

当然，大多数情况下，`stderr`将作为异常的一部分出现，因此在这种情况下必须使用try/except：

```py
def raise_exception(msg):
    raise ValueError(msg)

def test_something_exception():
    msg = "Not a good value"
    error = ""
    try:
        raise_exception(msg)
    except Exception as e:
        error = str(e)
        assert msg in error, f"{msg} is in the exception:\n{error}"
```

另一种捕获stdout的方法是通过`contextlib.redirect_stdout`：

```py
from io import StringIO
from contextlib import redirect_stdout

def print_to_stdout(s):
    print(s)

def test_result_and_stdout():
    msg = "Hello"
    buffer = StringIO()
    with redirect_stdout(buffer):
        print_to_stdout(msg)
    out = buffer.getvalue()
    # optional: if you want to replay the consumed streams:
    sys.stdout.write(out)
    # test:
    assert msg in out
```

捕获stdout的一个重要潜在问题是它可能包含`\r`字符，这些字符在正常的`print`中会重置到目前为止已经打印的所有内容。对于`pytest`没有问题，但对于`pytest -s`，这些字符会包含在缓冲区中，因此为了能够在有和没有`-s`的情况下运行测试，必须对捕获的输出进行额外的清理，使用`re.sub(r'~.*\r', '', buf, 0, re.M)`。

但是，我们有一个辅助上下文管理器包装器，可以自动处理所有这些，无论它是否包含一些`\r`，所以这很简单：

```py
from transformers.testing_utils import CaptureStdout

with CaptureStdout() as cs:
    function_that_writes_to_stdout()
print(cs.out)
```

这里是一个完整的测试示例：

```py
from transformers.testing_utils import CaptureStdout

msg = "Secret message\r"
final = "Hello World"
with CaptureStdout() as cs:
    print(msg + final)
assert cs.out == final + "\n", f"captured: {cs.out}, expecting {final}"
```

如果您想捕获`stderr`，请改用`CaptureStderr`类：

```py
from transformers.testing_utils import CaptureStderr

with CaptureStderr() as cs:
    function_that_writes_to_stderr()
print(cs.err)
```

如果需要同时捕获两个流，请使用父类`CaptureStd`：

```py
from transformers.testing_utils import CaptureStd

with CaptureStd() as cs:
    function_that_writes_to_stdout_and_stderr()
print(cs.err, cs.out)
```

此外，为了帮助调试测试问题，默认情况下这些上下文管理器会在退出上下文时自动重放捕获的流。

### 捕获记录器流

如果您需要验证记录器的输出，可以使用`CaptureLogger`：

```py
from transformers import logging
from transformers.testing_utils import CaptureLogger

msg = "Testing 1, 2, 3"
logging.set_verbosity_info()
logger = logging.get_logger("transformers.models.bart.tokenization_bart")
with CaptureLogger(logger) as cl:
    logger.info(msg)
assert cl.out, msg + "\n"
```

### 使用环境变量进行测试

如果您想测试特定测试的环境变量的影响，可以使用辅助装饰器`transformers.testing_utils.mockenv`

```py
from transformers.testing_utils import mockenv

class HfArgumentParserTest(unittest.TestCase):
 @mockenv(TRANSFORMERS_VERBOSITY="error")
    def test_env_override(self):
        env_level_str = os.getenv("TRANSFORMERS_VERBOSITY", None)
```

有时需要调用外部程序，这需要在`os.environ`中设置`PYTHONPATH`以包括多个本地路径。一个辅助类`transformers.test_utils.TestCasePlus`来帮助：

```py
from transformers.testing_utils import TestCasePlus

class EnvExampleTest(TestCasePlus):
    def test_external_prog(self):
        env = self.get_env()
        # now call the external program, passing `env` to it
```

根据测试文件是在`tests`测试套件还是`examples`下，它将正确设置`env[PYTHONPATH]`以包括这两个目录之一，并且还将`src`目录设置为确保针对当前存储库进行测试，最后还将设置`env[PYTHONPATH]`在调用测试之前已经设置的任何内容。

这个辅助方法创建了`os.environ`对象的副本，因此原始对象保持不变。

### 获得可重现的结果

在某些情况下，您可能希望为测试去除随机性。要获得相同的可重现结果集，您需要修复种子：

```py
seed = 42

# python RNG
import random

random.seed(seed)

# pytorch RNGs
import torch

torch.manual_seed(seed)
torch.backends.cudnn.deterministic = True
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# numpy RNG
import numpy as np

np.random.seed(seed)

# tf RNG
tf.random.set_seed(seed)
```

### 调试测试

要在警告点启动调试器，请执行以下操作：

```py
pytest tests/utils/test_logging.py -W error::UserWarning --pdb
```

## 使用github actions工作流

要触发自动推送工作流CI作业，必须：

1.  在`transformers`源上创建一个新分支（不是分叉！）。

1.  分支名称必须以`ci_`或`ci-`开头（`main`也会触发它，但我们不能在`main`上创建PR）。它还仅针对特定路径触发 - 如果自从编写本文以来发生了更改，您可以在[此处](https://github.com/huggingface/transformers/blob/main/.github/workflows/self-push.yml)找到最新的定义，位于*push:*下。

1.  从此分支创建一个PR。

1.  然后您可以在[这里](https://github.com/huggingface/transformers/actions/workflows/self-push.yml)看到该作业。如果有积压，它可能不会立即运行。

## 测试实验性CI功能

测试CI功能可能会有潜在问题，因为它可能会干扰正常的CI功能。因此，如果要添加新的CI功能，应按以下步骤进行。

1.  创建一个新的专用作业，测试需要测试的内容

1.  新作业必须始终成功，以便为我们提供绿色的 ✓（下面有详细信息）。

1.  让它运行几天，看看各种不同类型的PR是否可以运行在上面（用户分支，非分叉分支，源自github.com UI直接文件编辑的分支，各种强制推送等等 - 有很多），同时监视实验性作业的日志（而不是整体作业绿色，因为它故意总是绿色）

1.  当一切都很稳定时，然后将新更改合并到现有作业中。

这样，对CI功能本身的实验就不会干扰正常的工作流程。

现在我们如何确保工作始终成功，同时新的CI功能正在开发中？

一些CI，如TravisCI支持ignore-step-failure，并将整体作业报告为成功，但截至目前，CircleCI和Github Actions不支持该功能。

因此，可以使用以下解决方法：

1.  在运行命令的开头使用`set +euo pipefail`来抑制bash脚本中的大多数潜在故障。

1.  最后一个命令必须成功：`echo "done"`或只需`true`即可

这是一个例子：

```py
- run:
    name: run CI experiment
    command: |
        set +euo pipefail
        echo "setting run-all-despite-any-errors-mode"
        this_command_will_fail
        echo "but bash continues to run"
        # emulate another failure
        false
        # but the last command must be a success
        echo "during experiment do not remove: reporting success to CI, even if there were failures"
```

对于简单的命令，您也可以这样做：

```py
cmd_that_may_fail || true
```

当结果令人满意时，将实验步骤或作业与其余正常作业集成在一起，同时删除`set +euo pipefail`或您可能添加的任何其他内容，以确保实验性作业不会干扰正常的CI功能。

如果只能为实验步骤设置类似于`allow-failure`的内容，并且让它失败而不影响PR的整体状态，那么整个过程将变得更加容易。但正如前面提到的，CircleCI和Github Actions目前不支持这一点。

您可以为此功能投票，并查看CI特定线程的进展：

+   [Github Actions：](https://github.com/actions/toolkit/issues/399)

+   [CircleCI：](https://ideas.circleci.com/ideas/CCI-I-344)
