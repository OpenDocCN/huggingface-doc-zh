["```py\n( prediction_length: Optional = None context_length: Optional = None distribution_output: str = 'student_t' loss: str = 'nll' input_size: int = 1 lags_sequence: List = None scaling: Union = 'mean' num_dynamic_real_features: int = 0 num_static_real_features: int = 0 num_static_categorical_features: int = 0 num_time_features: int = 0 cardinality: Optional = None embedding_dimension: Optional = None d_model: int = 64 encoder_ffn_dim: int = 32 decoder_ffn_dim: int = 32 encoder_attention_heads: int = 2 decoder_attention_heads: int = 2 encoder_layers: int = 2 decoder_layers: int = 2 is_encoder_decoder: bool = True activation_function: str = 'gelu' dropout: float = 0.05 encoder_layerdrop: float = 0.1 decoder_layerdrop: float = 0.1 attention_dropout: float = 0.1 activation_dropout: float = 0.1 num_parallel_samples: int = 100 init_std: float = 0.02 use_cache = True attention_type: str = 'prob' sampling_factor: int = 5 distil: bool = True **kwargs )\n```", "```py\n>>> from transformers import InformerConfig, InformerModel\n\n>>> # Initializing an Informer configuration with 12 time steps for prediction\n>>> configuration = InformerConfig(prediction_length=12)\n\n>>> # Randomly initializing a model (with random weights) from the configuration\n>>> model = InformerModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config: InformerConfig )\n```", "```py\n( past_values: Tensor past_time_features: Tensor past_observed_mask: Tensor static_categorical_features: Optional = None static_real_features: Optional = None future_values: Optional = None future_time_features: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None use_cache: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqTSModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from huggingface_hub import hf_hub_download\n>>> import torch\n>>> from transformers import InformerModel\n\n>>> file = hf_hub_download(\n...     repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=\"train-batch.pt\", repo_type=\"dataset\"\n... )\n>>> batch = torch.load(file)\n\n>>> model = InformerModel.from_pretrained(\"huggingface/informer-tourism-monthly\")\n\n>>> # during training, one provides both past and future values\n>>> # as well as possible additional features\n>>> outputs = model(\n...     past_values=batch[\"past_values\"],\n...     past_time_features=batch[\"past_time_features\"],\n...     past_observed_mask=batch[\"past_observed_mask\"],\n...     static_categorical_features=batch[\"static_categorical_features\"],\n...     static_real_features=batch[\"static_real_features\"],\n...     future_values=batch[\"future_values\"],\n...     future_time_features=batch[\"future_time_features\"],\n... )\n\n>>> last_hidden_state = outputs.last_hidden_state\n```", "```py\n( config: InformerConfig )\n```", "```py\n( past_values: Tensor past_time_features: Tensor past_observed_mask: Tensor static_categorical_features: Optional = None static_real_features: Optional = None future_values: Optional = None future_time_features: Optional = None future_observed_mask: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None use_cache: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqTSModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from huggingface_hub import hf_hub_download\n>>> import torch\n>>> from transformers import InformerForPrediction\n\n>>> file = hf_hub_download(\n...     repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=\"train-batch.pt\", repo_type=\"dataset\"\n... )\n>>> batch = torch.load(file)\n\n>>> model = InformerForPrediction.from_pretrained(\n...     \"huggingface/informer-tourism-monthly\"\n... )\n\n>>> # during training, one provides both past and future values\n>>> # as well as possible additional features\n>>> outputs = model(\n...     past_values=batch[\"past_values\"],\n...     past_time_features=batch[\"past_time_features\"],\n...     past_observed_mask=batch[\"past_observed_mask\"],\n...     static_categorical_features=batch[\"static_categorical_features\"],\n...     static_real_features=batch[\"static_real_features\"],\n...     future_values=batch[\"future_values\"],\n...     future_time_features=batch[\"future_time_features\"],\n... )\n\n>>> loss = outputs.loss\n>>> loss.backward()\n\n>>> # during inference, one only provides past values\n>>> # as well as possible additional features\n>>> # the model autoregressively generates future values\n>>> outputs = model.generate(\n...     past_values=batch[\"past_values\"],\n...     past_time_features=batch[\"past_time_features\"],\n...     past_observed_mask=batch[\"past_observed_mask\"],\n...     static_categorical_features=batch[\"static_categorical_features\"],\n...     static_real_features=batch[\"static_real_features\"],\n...     future_time_features=batch[\"future_time_features\"],\n... )\n\n>>> mean_prediction = outputs.sequences.mean(dim=1)\n```"]