# TRL - Transformer Reinforcement Learning

> 原文：[`huggingface.co/docs/trl/index`](https://huggingface.co/docs/trl/index)

![](img/4b294f1850c2a0865587ccc36b23d404.png)

TRL 是一个全栈库，我们提供一套工具来使用强化学习训练变压器语言模型，从监督微调步骤（SFT）、奖励建模步骤（RM）到近端策略优化（PPO）步骤。该库与🤗 [transformers](https://github.com/huggingface/transformers)集成。

![](img/6bff4454a0be1455f1b3b09f74640ec7.png)

根据您的需求查看文档中的适当部分：

## API 文档

+   模型类: *每个公共模型类的简要概述*

+   `SFTTrainer`: *使用`SFTTrainer`轻松监督微调您的模型*

+   `RewardTrainer`: *使用`RewardTrainer`轻松训练您的奖励模型*

+   `PPOTrainer`: *使用 PPO 算法进一步微调监督微调的模型*

+   最佳 N 采样: *使用最佳 N 采样作为从活跃模型中采样预测的替代方式*

+   `DPOTrainer`: *使用`DPOTrainer`进行直接偏好优化训练*

+   `TextEnvironment`: *使用 RL 工具训练您的模型的文本环境*

## 示例

+   情感微调: *微调您的模型以生成积极的电影内容*

+   使用 PEFT 进行训练: *使用 PEFT 和适配器进行内存高效的 RLHF 训练*

+   去毒化 LLMs: *通过 RLHF 去毒化您的语言模型*

+   使用 Llama 模型: *在 Stack 交换数据集上进行端到端的 RLHF 训练*

+   使用工具进行学习: *使用`TextEnvironments`的演练*

+   多适配器训练: *使用单个基础模型和多个适配器进行内存高效的端到端训练*

## 博客文章

![缩略图

从人类反馈中阐释强化学习](https://huggingface.co/blog/rlhf) ![缩略图

在 24GB 消费级 GPU 上使用 RLHF 对 20B LLM 进行微调](https://huggingface.co/blog/trl-peft) ![缩略图

StackLLaMA：使用 RLHF 进行 LLaMA 的实践指南](https://huggingface.co/blog/stackllama) ![缩略图

使用 DPO 对 Llama 2 进行微调](https://huggingface.co/blog/dpo-trl) ![缩略图

使用 TRL 通过 DDPO 微调稳定扩散模型](https://huggingface.co/blog/trl-ddpo)
