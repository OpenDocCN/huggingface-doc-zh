# Fully Sharded Data Parallel

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/accelerate/usage_guides/fsdp](https://huggingface.co/docs/accelerate/usage_guides/fsdp)

ä¸ºäº†åŠ é€Ÿåœ¨æ›´å¤§çš„æ‰¹é‡å¤§å°ä¸Šè®­ç»ƒåºå¤§çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®Œå…¨åˆ†ç‰‡çš„æ•°æ®å¹¶è¡Œæ¨¡å‹ã€‚è¿™ç§æ•°æ®å¹¶è¡ŒèŒƒå¼é€šè¿‡åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°æ¥å®ç°æ›´å¤šæ•°æ®å’Œæ›´å¤§æ¨¡å‹çš„æ‹Ÿåˆã€‚è¦äº†è§£æ›´å¤šä¿¡æ¯å’Œå¥½å¤„ï¼Œè¯·æŸ¥çœ‹[å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œåšå®¢](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)ã€‚æˆ‘ä»¬å·²ç»é›†æˆäº†æœ€æ–°çš„PyTorchå®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œï¼ˆFSDPï¼‰è®­ç»ƒåŠŸèƒ½ã€‚æ‚¨åªéœ€è¦é€šè¿‡é…ç½®å¯ç”¨å®ƒã€‚

## å¼€ç®±å³ç”¨çš„å·¥ä½œåŸç†

åœ¨æ‚¨çš„æœºå™¨ä¸Šè¿è¡Œï¼š

```py
accelerate config
```

å¹¶å›ç­”æå‡ºçš„é—®é¢˜ã€‚è¿™å°†ç”Ÿæˆä¸€ä¸ªé…ç½®æ–‡ä»¶ï¼Œå°†åœ¨æ‰§è¡Œæ—¶è‡ªåŠ¨ä½¿ç”¨ä»¥æ­£ç¡®è®¾ç½®é»˜è®¤é€‰é¡¹

```py
accelerate launch my_script.py --args_to_my_script
```

ä¾‹å¦‚ï¼Œè¿™æ˜¯å¦‚ä½•åœ¨å¯ç”¨FSDPçš„æƒ…å†µä¸‹è¿è¡Œ`examples/nlp_example.py`ï¼ˆä»å­˜å‚¨åº“çš„æ ¹ç›®å½•ï¼‰çš„æ–¹å¼ï¼š

```py
compute_environment: LOCAL_MACHINE
debug: false
distributed_type: FSDP
downcast_bf16: 'no'
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch_policy: BACKWARD_PRE
  fsdp_forward_prefetch: false
  fsdp_cpu_ram_efficient_loading: true
  fsdp_offload_params: false
  fsdp_sharding_strategy: FULL_SHARD
  fsdp_state_dict_type: SHARDED_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_transformer_layer_cls_to_wrap: BertLayer
  fsdp_use_orig_params: true
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 2
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

```py
accelerate launch examples/nlp_example.py
```

ç›®å‰ï¼Œ`Accelerate`é€šè¿‡CLIæ”¯æŒä»¥ä¸‹é…ç½®ï¼š

`fsdp_sharding_strategy`ï¼š[1] FULL_SHARDï¼ˆåˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°ï¼‰ï¼Œ[2] SHARD_GRAD_OPï¼ˆåˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ï¼‰ï¼Œ[3] NO_SHARDï¼ˆDDPï¼‰ï¼Œ[4] HYBRID_SHARDï¼ˆåœ¨æ¯ä¸ªèŠ‚ç‚¹å†…åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°ï¼Œè€Œæ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰å®Œæ•´å‰¯æœ¬ï¼‰ï¼Œ[5] HYBRID_SHARD_ZERO2ï¼ˆåœ¨æ¯ä¸ªèŠ‚ç‚¹å†…åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ï¼Œè€Œæ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰å®Œæ•´å‰¯æœ¬ï¼‰ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…å®˜æ–¹[PyTorchæ–‡æ¡£](https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.ShardingStrategy)ã€‚

`fsdp_offload_params`ï¼šå†³å®šæ˜¯å¦å°†å‚æ•°å’Œæ¢¯åº¦å¸è½½åˆ°CPU

`fsdp_auto_wrap_policy`ï¼š[1] TRANSFORMER_BASED_WRAPï¼Œ[2] SIZE_BASED_WRAPï¼Œ[3] NO_WRAP

`fsdp_transformer_layer_cls_to_wrap`ï¼šä»…é€‚ç”¨äºğŸ¤— Transformersã€‚å½“ä½¿ç”¨`fsdp_auto_wrap_policy=TRANSFORMER_BASED_WRAP`æ—¶ï¼Œç”¨æˆ·å¯ä»¥æä¾›ä¸€ä¸ªé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼Œå…¶ä¸­åŒ…å«è¦åŒ…è£…çš„å˜å‹å™¨å±‚ç±»åç§°ï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰ï¼Œä¾‹å¦‚`BertLayer`ï¼Œ`GPTJBlock`ï¼Œ`T5Block`ï¼Œ`BertLayerï¼ŒBertEmbeddingsï¼ŒBertSelfOutput`ã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå…±äº«æƒé‡çš„å­æ¨¡å—ï¼ˆä¾‹å¦‚åµŒå…¥å±‚ï¼‰ä¸åº”è¯¥å‡ºç°åœ¨ä¸åŒçš„FSDPåŒ…è£…å•å…ƒä¸­ã€‚ä½¿ç”¨æ­¤ç­–ç•¥ï¼Œæ¯ä¸ªåŒ…å«å¤šå¤´æ³¨æ„åŠ›å’Œå‡ ä¸ªMLPå±‚çš„å—éƒ½ä¼šè¿›è¡ŒåŒ…è£…ã€‚å…¶ä½™å±‚ï¼ŒåŒ…æ‹¬å…±äº«çš„åµŒå…¥å±‚ï¼Œéƒ½æ–¹ä¾¿åœ°åŒ…è£…åœ¨åŒä¸€ä¸ªæœ€å¤–å±‚çš„FSDPå•å…ƒä¸­ã€‚å› æ­¤ï¼Œå¯¹äºåŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼Œè¯·ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚æ‚¨å¯ä»¥é€šè¿‡å›ç­”`yes`æ¥ä½¿ç”¨ğŸ¤— Transformeræ¨¡å‹çš„`model._no_split_modules`ï¼Œä»¥ä½¿ç”¨`model._no_split_modules`ã€‚åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œå®ƒå°†å°è¯•ä½¿ç”¨`model._no_split_modules`ã€‚

`fsdp_min_num_params`ï¼šåœ¨ä½¿ç”¨`fsdp_auto_wrap_policy=SIZE_BASED_WRAP`æ—¶ï¼Œå‚æ•°æ•°é‡çš„æœ€å°å€¼ã€‚

`fsdp_backward_prefetch_policy`: [1] BACKWARD_PRE, [2] BACKWARD_POST, [3] NO_PREFETCH

`fsdp_forward_prefetch`ï¼šå¦‚æœä¸ºTrueï¼Œåˆ™FSDPåœ¨å‰å‘ä¼ é€’æ‰§è¡Œæ—¶æ˜¾å¼é¢„å–ä¸‹ä¸€ä¸ªå³å°†åˆ°æ¥çš„å…¨èšåˆã€‚åº”ä»…ç”¨äºé™æ€å›¾æ¨¡å‹ï¼Œå› ä¸ºé¢„å–éµå¾ªç¬¬ä¸€æ¬¡è¿­ä»£çš„æ‰§è¡Œé¡ºåºã€‚å³ï¼Œå¦‚æœå­æ¨¡å—çš„é¡ºåºåœ¨æ¨¡å‹æ‰§è¡Œè¿‡ç¨‹ä¸­åŠ¨æ€å˜åŒ–ï¼Œè¯·ä¸è¦å¯ç”¨æ­¤åŠŸèƒ½ã€‚

`fsdp_state_dict_type`ï¼š[1] FULL_STATE_DICTï¼Œ[2] LOCAL_STATE_DICTï¼Œ[3] SHARDED_STATE_DICT

`fsdp_use_orig_params`ï¼šå¦‚æœä¸ºTrueï¼Œåˆ™å…è®¸åœ¨åˆå§‹åŒ–æœŸé—´ä½¿ç”¨éå‡åŒ€çš„`requires_grad`ï¼Œè¿™æ„å‘³ç€æ”¯æŒäº¤æ›¿å†»ç»“å’Œå¯è®­ç»ƒå‚æ•°ã€‚åœ¨è¯¸å¦‚å‚æ•°é«˜æ•ˆå¾®è°ƒç­‰æƒ…å†µä¸‹ï¼Œè¿™ä¸ªè®¾ç½®éå¸¸æœ‰ç”¨ï¼Œå¦‚[æ­¤å¸–å­](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019)ä¸­æ‰€è®¨è®ºçš„ã€‚æ­¤é€‰é¡¹è¿˜å…è®¸ä¸€ä¸ªæ‹¥æœ‰å¤šä¸ªä¼˜åŒ–å™¨å‚æ•°ç»„ã€‚åœ¨ä½¿ç”¨FSDPä¹‹å‰åˆ›å»ºä¼˜åŒ–å™¨æ—¶ï¼Œåº”å°†æ­¤è®¾ç½®ä¸º`True`ã€‚

`fsdp_cpu_ram_efficient_loading`ï¼šä»…é€‚ç”¨äºğŸ¤— Transformersæ¨¡å‹ã€‚å¦‚æœä¸ºTrueï¼Œåˆ™åªæœ‰ç¬¬ä¸€ä¸ªè¿›ç¨‹åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œè€Œæ‰€æœ‰å…¶ä»–è¿›ç¨‹éƒ½å…·æœ‰ç©ºæƒé‡ã€‚å¦‚æœåœ¨é€šè¿‡`from_pretrained`æ–¹æ³•åŠ è½½é¢„è®­ç»ƒğŸ¤— Transformersæ¨¡å‹æ—¶é‡åˆ°é”™è¯¯ï¼Œåˆ™åº”å°†å…¶è®¾ç½®ä¸ºFalseã€‚å½“æ­¤è®¾ç½®ä¸ºTrueæ—¶ï¼Œ`fsdp_sync_module_states`ä¹Ÿå¿…é¡»ä¸ºTrueï¼Œå¦åˆ™é™¤ä¸»è¿›ç¨‹å¤–çš„æ‰€æœ‰è¿›ç¨‹éƒ½å°†å…·æœ‰éšæœºæƒé‡ï¼Œå¯¼è‡´è®­ç»ƒæœŸé—´å‡ºç°æ„å¤–è¡Œä¸ºã€‚ä¸ºä½¿å…¶æ­£å¸¸å·¥ä½œï¼Œè¯·ç¡®ä¿åœ¨è°ƒç”¨Transformersçš„`from_pretrained`æ–¹æ³•ä¹‹å‰åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„ã€‚åœ¨ä½¿ç”¨ğŸ¤— Trainer APIæ—¶ï¼Œå½“æ‚¨åˆ›å»º`TrainingArguments`ç±»çš„å®ä¾‹æ—¶ï¼Œåˆ†å¸ƒå¼è¿›ç¨‹ç»„å°†è¢«åˆå§‹åŒ–ã€‚

`fsdp_sync_module_states`ï¼šå¦‚æœä¸ºTrueï¼Œåˆ™æ¯ä¸ªå•ç‹¬åŒ…è£…çš„FSDPå•å…ƒå°†ä»æ’å0å¹¿æ’­æ¨¡å—å‚æ•°ã€‚

ä¸ºäº†è·å¾—é¢å¤–å’Œæ›´å¾®å¦™çš„æ§åˆ¶ï¼Œæ‚¨å¯ä»¥é€šè¿‡`FullyShardedDataParallelPlugin`æŒ‡å®šå…¶ä»–FSDPå‚æ•°ã€‚åˆ›å»º`FullyShardedDataParallelPlugin`å¯¹è±¡æ—¶ï¼Œå°†å…¶ä¼ é€’ç»™é‚£äº›ä¸æ˜¯åŠ é€Ÿé…ç½®çš„ä¸€éƒ¨åˆ†æˆ–è€…å¦‚æœæ‚¨æƒ³è¦è¦†ç›–å®ƒä»¬çš„å‚æ•°ã€‚FSDPå‚æ•°å°†æ ¹æ®åŠ é€Ÿé…ç½®æ–‡ä»¶æˆ–å¯åŠ¨å‘½ä»¤å‚æ•°é€‰æ‹©ï¼Œå¹¶ä¸”æ‚¨å°†ç›´æ¥é€šè¿‡`FullyShardedDataParallelPlugin`å¯¹è±¡ä¼ é€’çš„å…¶ä»–å‚æ•°å°†è®¾ç½®/è¦†ç›–å®ƒã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š

```py
from accelerate import FullyShardedDataParallelPlugin
from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig

fsdp_plugin = FullyShardedDataParallelPlugin(
    state_dict_config=FullStateDictConfig(offload_to_cpu=False, rank0_only=False),
    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=False, rank0_only=False),
)

accelerator = Accelerator(fsdp_plugin=fsdp_plugin)
```

## ä¿å­˜å’ŒåŠ è½½

åœ¨ä½¿ç”¨FSDPæ¨¡å‹æ—¶ï¼Œå»ºè®®çš„æ–°çš„æ£€æŸ¥ç‚¹æ–¹å¼æ˜¯åœ¨è®¾ç½®åŠ é€Ÿé…ç½®æ—¶å°†`SHARDED_STATE_DICT`è®¾ç½®ä¸º`StateDictType`ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨åŠ é€Ÿçš„`save_state`å®ç”¨ç¨‹åºä¿å­˜çš„ä»£ç ç‰‡æ®µã€‚

```py
accelerator.save_state("ckpt")
```

æ£€æŸ¥æ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹ä»¥æŸ¥çœ‹æ¯ä¸ªè¿›ç¨‹çš„æ¨¡å‹å’Œä¼˜åŒ–å™¨ä½œä¸ºåˆ†ç‰‡ï¼š

```py
ls ckpt
# optimizer_0  pytorch_model_0  random_states_0.pkl  random_states_1.pkl  scheduler.bin

cd ckpt

ls optimizer_0
# __0_0.distcp  __1_0.distcp

ls pytorch_model_0
# __0_0.distcp  __1_0.distcp
```

è¦åŠ è½½å®ƒä»¬ä»¥æ¢å¤è®­ç»ƒï¼Œè¯·ä½¿ç”¨åŠ é€Ÿçš„`load_state`å®ç”¨ç¨‹åº

```py
accelerator.load_state("ckpt")
```

åœ¨ä½¿ç”¨transformersçš„`save_pretrained`æ—¶ï¼Œå°†`state_dict=accelerator.get_state_dict(model)`ä¼ é€’ä»¥ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š

```py
  unwrapped_model.save_pretrained(
      args.output_dir,
      is_main_process=accelerator.is_main_process,
      save_function=accelerator.save,
+     state_dict=accelerator.get_state_dict(model),
)
```

### çŠ¶æ€å­—å…¸

`accelerator.get_state_dict`å°†ä½¿ç”¨`FullStateDictConfig(offload_to_cpu=True, rank0_only=True)`ä¸Šä¸‹æ–‡ç®¡ç†å™¨è°ƒç”¨åº•å±‚çš„`model.state_dict`å®ç°ï¼Œä»…ä¸ºæ’å0è·å–çŠ¶æ€å­—å…¸ï¼Œå¹¶å°†å…¶å¸è½½åˆ°CPUã€‚

ç„¶åå¯ä»¥å°†`state`ä¼ é€’ç»™`save_pretrained`æ–¹æ³•ã€‚æœ‰å‡ ç§`StateDictType`å’Œ`FullStateDictConfig`çš„æ¨¡å¼å¯ç”¨äºæ§åˆ¶`state_dict`çš„è¡Œä¸ºã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[PyTorchæ–‡æ¡£](https://pytorch.org/docs/stable/fsdp.html)ã€‚

## FSDPåˆ†ç‰‡ç­–ç•¥ä¸DeepSpeed ZeROé˜¶æ®µä¹‹é—´çš„æ˜ å°„

+   `FULL_SHARD`æ˜ å°„åˆ°DeepSpeedçš„`ZeRO Stage-3`ã€‚åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°ã€‚

+   `SHARD_GRAD_OP`æ˜ å°„åˆ°DeepSpeedçš„`ZeRO Stage-2`ã€‚åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ã€‚

+   `NO_SHARD`æ˜ å°„åˆ°`ZeRO Stage-0`ã€‚æ²¡æœ‰åˆ†ç‰‡ï¼Œæ¯ä¸ªGPUéƒ½æœ‰æ¨¡å‹ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦çš„å®Œæ•´å‰¯æœ¬ã€‚

+   `HYBRID_SHARD`æ˜ å°„åˆ°`ZeRO++ Stage-3`ï¼Œå…¶ä¸­`zero_hpz_partition_size=<num_gpus_per_node>`ã€‚åœ¨è¿™é‡Œï¼Œè¿™å°†åœ¨æ¯ä¸ªèŠ‚ç‚¹å†…å¯¹ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°è¿›è¡Œåˆ†ç‰‡ï¼Œè€Œæ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰å®Œæ•´çš„å‰¯æœ¬ã€‚

## éœ€è¦æ³¨æ„çš„ä¸€äº›æ³¨æ„äº‹é¡¹

+   åœ¨å¤šä¸ªæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå°†ä¼˜åŒ–å™¨æŒ‰ç…§ç›¸åº”æ¨¡å‹çš„é¡ºåºä¼ é€’ç»™å‡†å¤‡è°ƒç”¨ï¼Œå¦åˆ™`accelerator.save_state()`å’Œ`accelerator.load_state()`å°†å¯¼è‡´é”™è¯¯/æ„å¤–è¡Œä¸ºã€‚

+   æ­¤åŠŸèƒ½ä¸ğŸ¤—`Transformers`åº“ä¸­`run_translation.py`è„šæœ¬ä¸­çš„`--predict_with_generate`ä¸å…¼å®¹ã€‚

å¯¹äºæ›´å¤šæ§åˆ¶ï¼Œç”¨æˆ·å¯ä»¥åˆ©ç”¨`FullyShardedDataParallelPlugin`ã€‚åœ¨åˆ›å»ºæ­¤ç±»çš„å®ä¾‹åï¼Œç”¨æˆ·å¯ä»¥å°†å…¶ä¼ é€’ç»™Acceleratorç±»çš„å®ä¾‹åŒ–ã€‚æœ‰å…³è¿™äº›é€‰é¡¹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…PyTorch [FullyShardedDataParallel](https://github.com/pytorch/pytorch/blob/0df2e863fbd5993a7b9e652910792bd21a516ff3/torch/distributed/fsdp/fully_sharded_data_parallel.py#L236) ä»£ç ã€‚
