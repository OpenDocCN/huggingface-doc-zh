# Fully Sharded Data Parallel

> 原始文本：[https://huggingface.co/docs/accelerate/usage_guides/fsdp](https://huggingface.co/docs/accelerate/usage_guides/fsdp)

为了加速在更大的批量大小上训练庞大的模型，我们可以使用完全分片的数据并行模型。这种数据并行范式通过分片优化器状态、梯度和参数来实现更多数据和更大模型的拟合。要了解更多信息和好处，请查看[完全分片数据并行博客](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)。我们已经集成了最新的PyTorch完全分片数据并行（FSDP）训练功能。您只需要通过配置启用它。

## 开箱即用的工作原理

在您的机器上运行：

```py
accelerate config
```

并回答提出的问题。这将生成一个配置文件，将在执行时自动使用以正确设置默认选项

```py
accelerate launch my_script.py --args_to_my_script
```

例如，这是如何在启用FSDP的情况下运行`examples/nlp_example.py`（从存储库的根目录）的方式：

```py
compute_environment: LOCAL_MACHINE
debug: false
distributed_type: FSDP
downcast_bf16: 'no'
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch_policy: BACKWARD_PRE
  fsdp_forward_prefetch: false
  fsdp_cpu_ram_efficient_loading: true
  fsdp_offload_params: false
  fsdp_sharding_strategy: FULL_SHARD
  fsdp_state_dict_type: SHARDED_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_transformer_layer_cls_to_wrap: BertLayer
  fsdp_use_orig_params: true
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 2
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

```py
accelerate launch examples/nlp_example.py
```

目前，`Accelerate`通过CLI支持以下配置：

`fsdp_sharding_strategy`：[1] FULL_SHARD（分片优化器状态、梯度和参数），[2] SHARD_GRAD_OP（分片优化器状态和梯度），[3] NO_SHARD（DDP），[4] HYBRID_SHARD（在每个节点内分片优化器状态、梯度和参数，而每个节点都有完整副本），[5] HYBRID_SHARD_ZERO2（在每个节点内分片优化器状态和梯度，而每个节点都有完整副本）。有关更多信息，请参阅官方[PyTorch文档](https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.ShardingStrategy)。

`fsdp_offload_params`：决定是否将参数和梯度卸载到CPU

`fsdp_auto_wrap_policy`：[1] TRANSFORMER_BASED_WRAP，[2] SIZE_BASED_WRAP，[3] NO_WRAP

`fsdp_transformer_layer_cls_to_wrap`：仅适用于🤗 Transformers。当使用`fsdp_auto_wrap_policy=TRANSFORMER_BASED_WRAP`时，用户可以提供一个逗号分隔的字符串，其中包含要包装的变压器层类名称（区分大小写），例如`BertLayer`，`GPTJBlock`，`T5Block`，`BertLayer，BertEmbeddings，BertSelfOutput`。这很重要，因为共享权重的子模块（例如嵌入层）不应该出现在不同的FSDP包装单元中。使用此策略，每个包含多头注意力和几个MLP层的块都会进行包装。其余层，包括共享的嵌入层，都方便地包装在同一个最外层的FSDP单元中。因此，对于基于变压器的模型，请使用此功能。您可以通过回答`yes`来使用🤗 Transformer模型的`model._no_split_modules`，以使用`model._no_split_modules`。在可能的情况下，它将尝试使用`model._no_split_modules`。

`fsdp_min_num_params`：在使用`fsdp_auto_wrap_policy=SIZE_BASED_WRAP`时，参数数量的最小值。

`fsdp_backward_prefetch_policy`: [1] BACKWARD_PRE, [2] BACKWARD_POST, [3] NO_PREFETCH

`fsdp_forward_prefetch`：如果为True，则FSDP在前向传递执行时显式预取下一个即将到来的全聚合。应仅用于静态图模型，因为预取遵循第一次迭代的执行顺序。即，如果子模块的顺序在模型执行过程中动态变化，请不要启用此功能。

`fsdp_state_dict_type`：[1] FULL_STATE_DICT，[2] LOCAL_STATE_DICT，[3] SHARDED_STATE_DICT

`fsdp_use_orig_params`：如果为True，则允许在初始化期间使用非均匀的`requires_grad`，这意味着支持交替冻结和可训练参数。在诸如参数高效微调等情况下，这个设置非常有用，如[此帖子](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019)中所讨论的。此选项还允许一个拥有多个优化器参数组。在使用FSDP之前创建优化器时，应将此设置为`True`。

`fsdp_cpu_ram_efficient_loading`：仅适用于🤗 Transformers模型。如果为True，则只有第一个进程加载预训练模型检查点，而所有其他进程都具有空权重。如果在通过`from_pretrained`方法加载预训练🤗 Transformers模型时遇到错误，则应将其设置为False。当此设置为True时，`fsdp_sync_module_states`也必须为True，否则除主进程外的所有进程都将具有随机权重，导致训练期间出现意外行为。为使其正常工作，请确保在调用Transformers的`from_pretrained`方法之前初始化分布式进程组。在使用🤗 Trainer API时，当您创建`TrainingArguments`类的实例时，分布式进程组将被初始化。

`fsdp_sync_module_states`：如果为True，则每个单独包装的FSDP单元将从排名0广播模块参数。

为了获得额外和更微妙的控制，您可以通过`FullyShardedDataParallelPlugin`指定其他FSDP参数。创建`FullyShardedDataParallelPlugin`对象时，将其传递给那些不是加速配置的一部分或者如果您想要覆盖它们的参数。FSDP参数将根据加速配置文件或启动命令参数选择，并且您将直接通过`FullyShardedDataParallelPlugin`对象传递的其他参数将设置/覆盖它。

以下是一个示例：

```py
from accelerate import FullyShardedDataParallelPlugin
from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig

fsdp_plugin = FullyShardedDataParallelPlugin(
    state_dict_config=FullStateDictConfig(offload_to_cpu=False, rank0_only=False),
    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=False, rank0_only=False),
)

accelerator = Accelerator(fsdp_plugin=fsdp_plugin)
```

## 保存和加载

在使用FSDP模型时，建议的新的检查点方式是在设置加速配置时将`SHARDED_STATE_DICT`设置为`StateDictType`。以下是使用加速的`save_state`实用程序保存的代码片段。

```py
accelerator.save_state("ckpt")
```

检查检查点文件夹以查看每个进程的模型和优化器作为分片：

```py
ls ckpt
# optimizer_0  pytorch_model_0  random_states_0.pkl  random_states_1.pkl  scheduler.bin

cd ckpt

ls optimizer_0
# __0_0.distcp  __1_0.distcp

ls pytorch_model_0
# __0_0.distcp  __1_0.distcp
```

要加载它们以恢复训练，请使用加速的`load_state`实用程序

```py
accelerator.load_state("ckpt")
```

在使用transformers的`save_pretrained`时，将`state_dict=accelerator.get_state_dict(model)`传递以保存模型状态字典。以下是一个示例：

```py
  unwrapped_model.save_pretrained(
      args.output_dir,
      is_main_process=accelerator.is_main_process,
      save_function=accelerator.save,
+     state_dict=accelerator.get_state_dict(model),
)
```

### 状态字典

`accelerator.get_state_dict`将使用`FullStateDictConfig(offload_to_cpu=True, rank0_only=True)`上下文管理器调用底层的`model.state_dict`实现，仅为排名0获取状态字典，并将其卸载到CPU。

然后可以将`state`传递给`save_pretrained`方法。有几种`StateDictType`和`FullStateDictConfig`的模式可用于控制`state_dict`的行为。有关更多信息，请参阅[PyTorch文档](https://pytorch.org/docs/stable/fsdp.html)。

## FSDP分片策略与DeepSpeed ZeRO阶段之间的映射

+   `FULL_SHARD`映射到DeepSpeed的`ZeRO Stage-3`。分片优化器状态、梯度和参数。

+   `SHARD_GRAD_OP`映射到DeepSpeed的`ZeRO Stage-2`。分片优化器状态和梯度。

+   `NO_SHARD`映射到`ZeRO Stage-0`。没有分片，每个GPU都有模型、优化器状态和梯度的完整副本。

+   `HYBRID_SHARD`映射到`ZeRO++ Stage-3`，其中`zero_hpz_partition_size=<num_gpus_per_node>`。在这里，这将在每个节点内对优化器状态、梯度和参数进行分片，而每个节点都有完整的副本。

## 需要注意的一些注意事项

+   在多个模型的情况下，将优化器按照相应模型的顺序传递给准备调用，否则`accelerator.save_state()`和`accelerator.load_state()`将导致错误/意外行为。

+   此功能与🤗`Transformers`库中`run_translation.py`脚本中的`--predict_with_generate`不兼容。

对于更多控制，用户可以利用`FullyShardedDataParallelPlugin`。在创建此类的实例后，用户可以将其传递给Accelerator类的实例化。有关这些选项的更多信息，请参阅PyTorch [FullyShardedDataParallel](https://github.com/pytorch/pytorch/blob/0df2e863fbd5993a7b9e652910792bd21a516ff3/torch/distributed/fsdp/fully_sharded_data_parallel.py#L236) 代码。
