- en: Evaluate predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/datasets/metrics](https://huggingface.co/docs/datasets/metrics)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Metrics is deprecated in ðŸ¤— Datasets. To learn more about how to use metrics,
    take a look at the library ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)!
    In addition to metrics, you can find more tools for evaluating models and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ¤— Datasets provides various common and NLP-specific [metrics](https://huggingface.co/metrics)
    for you to measure your models performance. In this section of the tutorials,
    you will load a metric and use it to evaluate your models predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see what metrics are available with [list_metrics()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.list_metrics):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Load metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is very easy to load a metric with ðŸ¤— Datasets. In fact, you will notice
    that it is very similar to loading a dataset! Load a metric from the Hub with
    [load_metric()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_metric):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This will load the metric associated with the MRPC dataset from the GLUE benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: Select a configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are using a benchmark dataset, you need to select a metric that is associated
    with the configuration you are using. Select a metric configuration by providing
    the configuration name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Metrics object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you begin using a [Metric](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric)
    object, you should get to know it a little better. As with a dataset, you can
    return some basic information about a metric. For example, access the `inputs_description`
    parameter in [datasets.MetricInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.MetricInfo)
    to get more information about a metrics expected input format and some usage examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Notice for the MRPC configuration, the metric expects the input format to be
    zero or one. For a complete list of attributes you can return with your metric,
    take a look at [MetricInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.MetricInfo).
  prefs: []
  type: TYPE_NORMAL
- en: Compute metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you have loaded a metric, you are ready to use it to evaluate a models
    predictions. Provide the model predictions and references to [compute()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric.compute):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
