# 加载方法

> 原文链接：[`huggingface.co/docs/datasets/package_reference/loading_methods`](https://huggingface.co/docs/datasets/package_reference/loading_methods)

列出和加载数据集和指标的方法：

## 数据集

#### `datasets.list_datasets`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/inspect.py#L52)

```py
( with_community_datasets = True with_details = False )
```

参数

+   `with_community_datasets` (`bool`, *optional*, 默认为`True`) — 包括社区提供的数据集。

+   `with_details` (`bool`, *optional*, 默认为`False`) — 返回数据集的完整详情而不仅仅是简短名称。

列出 HF Hub 上所有可用的数据集脚本。

示例：

```py
>>> from datasets import list_datasets
>>> list_datasets()
['acronym_identification',
 'ade_corpus_v2',
 'adversarial_qa',
 'aeslc',
 'afrikaans_ner_corpus',
 'ag_news',
 ...
]
```

#### `datasets.load_dataset`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/load.py#L2276)

```py
( path: str name: Optional = None data_dir: Optional = None data_files: Union = None split: Union = None cache_dir: Optional = None features: Optional = None download_config: Optional = None download_mode: Union = None verification_mode: Union = None ignore_verifications = 'deprecated' keep_in_memory: Optional = None save_infos: bool = False revision: Union = None token: Union = None use_auth_token = 'deprecated' task = 'deprecated' streaming: bool = False num_proc: Optional = None storage_options: Optional = None trust_remote_code: bool = None **config_kwargs ) → export const metadata = 'undefined';Dataset or DatasetDict
```

参数

+   `path` (`str`) — 数据集的路径或名称。根据`path`，使用的数据集构建器来自通用数据集脚本（JSON、CSV、Parquet、文本等）或数据集目录内的数据集脚本（一个 Python 文件）。

    对于本地数据集：

    +   如果`path`是本地目录（仅包含数据文件）-> 根据目录内容加载通用数据集构建器（csv、json、文本等）例如`'./path/to/directory/with/my/csv/data'`。

    +   如果`path`是本地数据集脚本或包含本地数据集脚本的目录（如果脚本与目录同名）-> 从数据集脚本加载数据集构建器 例如`'./dataset/squad'`或`'./dataset/squad/squad.py'`。

    对于 HF Hub 上的数据集（使用`huggingface_hub.list_datasets`列出所有可用数据集）

    +   如果`path`是 HF Hub 上的数据集存储库（仅包含数据文件）-> 根据存储库内容加载通用数据集构建器（csv、文本等）例如`'username/dataset_name'`，HF Hub 上包含您数据文件的数据集存储库。

    +   如果`path`是 HF Hub 上带有数据集脚本的数据集存储库（如果脚本与目录同名）-> 从数据集存储库中的数据集脚本加载数据集构建器 例如`glue`、`squad`、`'username/dataset_name'`，HF Hub 上包含数据集脚本`'dataset_name.py'`的数据集存储库。

+   `name` (`str`, *optional*) — 定义数据集配置的名称。

+   `data_dir` (`str`, *optional*) — 定义数据集配置的`data_dir`。如果为通用构建器（csv、文本等）或 Hub 数据集指定了`data_files`且为`None`，则行为等同于将`os.path.join(data_dir, **)`作为`data_files`传递以引用目录中的所有文件。

+   `data_files` (`str` 或 `Sequence` 或 `Mapping`, *optional*) — 源数据文件的路径。

+   `split` (`Split` 或 `str`) — 要加载的数据拆分。如果为`None`，将返回包含所有拆分的`dict`（通常为`datasets.Split.TRAIN`和`datasets.Split.TEST`）。如果给定，将返回单个数据集。可以像在 tensorflow-datasets 中一样组合和指定拆分。

+   `cache_dir` (`str`, *optional*) — 读取/写入数据的目录。默认为`"~/.cache/huggingface/datasets"`。

+   `features` (`Features`, *optional*) — 设置此数据集使用的特征类型。

+   `download_config` (DownloadConfig, *optional*) — 特定的下载配置参数。

+   `download_mode` (DownloadMode 或 `str`, 默认为`REUSE_DATASET_IF_EXISTS`) — 下载/生成模式。

+   `verification_mode` (VerificationMode 或 `str`, 默认为`BASIC_CHECKS`) — 确定对下载/处理的数据集信息运行的检查（校验和/大小/拆分等）的验证模式。

    添加于 2.9.1

+   `ignore_verifications`（`bool`，默认为`False`）— 忽略下载/处理的数据集信息的验证（校验和/大小/拆分/...）。

    2.9.1 中已弃用

    `ignore_verifications`在版本 2.9.1 中已弃用，并将在 3.0.0 中删除。请改用`verification_mode`。

+   `keep_in_memory`（`bool`，默认为`None`）— 是否将数据集保存在内存中。如果为`None`，则除非通过将`datasets.config.IN_MEMORY_MAX_SIZE`设置为非零来明确启用，否则数据集不会保存在内存中。在 improve performance 部分中查看更多详细信息。

+   `save_infos`（`bool`，默认为`False`）— 保存数据集信息（校验和/大小/拆分/...）。

+   `revision`（Version 或`str`，*可选*）— 要加载的数据集脚本的版本。由于数据集在 Datasets Hub 上有自己的 git 存储库，默认版本“main”对应于它们的“main”分支。您可以通过使用数据集存储库的提交 SHA 或 git 标签指定与默认“main”不同的版本。

+   `token`（`str`或`bool`，*可选*）— 用作 Datasets Hub 上远程文件的 Bearer 令牌的可选字符串或布尔值。如果为`True`或未指定，则将从`"~/.huggingface"`获取令牌。

+   `use_auth_token`（`str`或`bool`，*可选*）— 用作 Datasets Hub 上远程文件的 Bearer 令牌的可选字符串或布尔值。如果为`True`或未指定，则将从`"~/.huggingface"`获取令牌。

    2.14.0 中已弃用

    `use_auth_token`已在版本 2.14.0 中弃用，改用`token`，并将在 3.0.0 中删除。

+   `task`（`str`）— 在训练和评估期间为数据集准备的任务。将数据集的 Features 转换为`datasets.tasks`中详细说明的标准化列名和类型。

    2.13.0 中已弃用

    `task`在版本 2.13.0 中已弃用，并将在 3.0.0 中删除。

+   `streaming`（`bool`，默认为`False`）— 如果设置为`True`，则不下载数据文件。相反，它在迭代数据集时逐步流式传输数据。在这种情况下，将返回 IterableDataset 或 IterableDatasetDict。

    请注意，流式传输适用于支持像 txt、csv、jsonl 等可以迭代的数据格式的数据集。例如，Json 文件可能会完全下载。还支持从远程 zip 或 gzip 文件进行流式传输，但不支持其他压缩格式，如 rar 和 xz。tgz 格式不允许流式传输。

+   `num_proc`（`int`，*可选*，默认为`None`）— 下载和本地生成数据集时的进程数。默认情况下禁用多进程。

    2.7.0 中添加

+   `storage_options`（`dict`，*可选*，默认为`None`）— **实验性**。要传递给数据集文件系统后端的键/值对，如果有的话。

    2.11.0 中添加

+   `trust_remote_code`（`bool`，默认为`True`）— 是否允许使用数据集脚本在 Hub 上定义数据集。此选项应仅对您信任的存储库设置为`True`，并且您已阅读了代码，因为它将在本地机器上执行 Hub 上存在的代码。

    `trust_remote_code`将在下一个主要版本中默认为 False。

    2.16.0 中添加

+   *`*config_kwargs`（额外的关键字参数）— 要传递给`BuilderConfig`并在 DatasetBuilder 中使用的关键字参数。

返回

Dataset 或 DatasetDict

+   如果`split`不是`None`：请求的数据集，

+   如果`split`为`None`，则返回一个包含每个拆分的 DatasetDict。

或 IterableDataset 或 IterableDatasetDict：如果`streaming=True`

+   如果`split`不是`None`，则请求数据集。

+   如果`split`为`None`，则返回一个`~datasets.streaming.IterableDatasetDict`，其中包含每个拆分。

从 Hugging Face Hub 加载数据集，或者加载本地数据集。

您可以在[Hub](https://huggingface.co/datasets)上找到数据集列表，或使用`huggingface_hub.list_datasets`。

数据集是一个包含以下内容的目录：

+   一些通用格式的数据文件（JSON、CSV、Parquet、文本等）。

+   以及可选的数据集脚本，如果需要一些代码来读取数据文件。这用于加载任何类型的格式或结构。

请注意，数据集脚本也可以从任何地方下载和读取数据文件 - 如果您的数据文件已经存在在线。

此函数在后台执行以下操作：

1.  如果库中尚未缓存数据集脚本，则从`path`下载并导入数据集脚本。

    如果数据集没有数据集脚本，则会导入一个通用数据集脚本（JSON、CSV、Parquet、文本等）。

    数据集脚本是定义数据集构建器的小型 Python 脚本。它们定义了数据集的引用、信息和格式，包含了原始数据文件的路径或 URL 以及从原始数据文件加载示例的代码。

    您可以在 Datasets [Hub](https://huggingface.co/datasets)中找到完整的数据集列表。

1.  运行数据集脚本，该脚本将：

    +   从原始 URL 下载数据集文件（参见脚本），如果本地尚未可用或已缓存。

    +   为缓存处理并缓存数据集中的类型化 Arrow 表。

        Arrow 表是任意长的、带类型的表，可以存储嵌套对象，并且可以映射到 numpy/pandas/python 通用类型。它们可以直接从磁盘访问，加载到 RAM 中，甚至通过网络流式传输。

1.  返回从`split`中请求的拆分构建的数据集（默认：全部）。

还允许从本地目录或 Hugging Face Hub 上的数据集存储库加载数据集，而无需数据集脚本。在这种情况下，它会自动从目录或数据集存储库中加载所有数据文件。

示例：

从 Hugging Face Hub 加载数据集：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset('rotten_tomatoes', split='train')

# Map data files to splits
>>> data_files = {'train': 'train.csv', 'test': 'test.csv'}
>>> ds = load_dataset('namespace/your_dataset_name', data_files=data_files)
```

加载本地数据集：

```py
# Load a CSV file
>>> from datasets import load_dataset
>>> ds = load_dataset('csv', data_files='path/to/local/my_dataset.csv')

# Load a JSON file
>>> from datasets import load_dataset
>>> ds = load_dataset('json', data_files='path/to/local/my_dataset.json')

# Load from a local loading script
>>> from datasets import load_dataset
>>> ds = load_dataset('path/to/local/loading_script/loading_script.py', split='train')
```

加载一个 IterableDataset：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset('rotten_tomatoes', split='train', streaming=True)
```

使用`ImageFolder`数据集构建器加载图像数据集：

```py
>>> from datasets import load_dataset
>>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')
```

#### `datasets.load_from_disk`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/load.py#L2600)

```py
( dataset_path: str fs = 'deprecated' keep_in_memory: Optional = None storage_options: Optional = None ) → export const metadata = 'undefined';Dataset or DatasetDict
```

参数

+   `dataset_path`（`str`）—数据集将从中加载的 Dataset 或 DatasetDict 目录的路径（例如`"dataset/train"`）或远程 URI（例如`"s3://my-bucket/dataset/train"`）。

+   `fs`（`~filesystems.S3FileSystem`或`fsspec.spec.AbstractFileSystem`，*可选*）—用于从远程文件系统下载文件的远程文件系统的实例。

    在 2.9.0 中弃用

    `fs`在 2.9.0 版本中已弃用，并将在 3.0.0 中移除。请改用`storage_options`，例如`storage_options=fs.storage_options`。

+   `keep_in_memory`（`bool`，默认为`None`）—是否将数据集复制到内存中。如果为`None`，除非通过将`datasets.config.IN_MEMORY_MAX_SIZE`设置为非零来明确启用，否则数据集不会被复制到内存中。在 improve performance 部分中查看更多详细信息。

+   `storage_options`（`dict`，*可选*）—要传递给文件系统后端的键/值对，如果有的话。

    2.9.0 版本中添加

返回

Dataset 或 DatasetDict

+   如果`dataset_path`是数据集目录的路径，则返回请求的数据集。

+   如果`dataset_path`是数据集字典目录的路径，则返回一个包含每个拆分的 DatasetDict。

加载先前使用 save_to_disk()保存的数据集，从数据集目录中加载，或者使用`fsspec.spec.AbstractFileSystem`的任何实现从文件系统加载。

示例：

```py
>>> from datasets import load_from_disk
>>> ds = load_from_disk('path/to/dataset/directory')
```

#### `datasets.load_dataset_builder`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/load.py#L2089)

```py
( path: str name: Optional = None data_dir: Optional = None data_files: Union = None cache_dir: Optional = None features: Optional = None download_config: Optional = None download_mode: Union = None revision: Union = None token: Union = None use_auth_token = 'deprecated' storage_options: Optional = None trust_remote_code: Optional = None _require_default_config_name = True **config_kwargs )
```

参数

+   `path` (`str`) — 数据集的路径或名称。根据`path`，所使用的数据集构建器来自通用数据集脚本（JSON、CSV、Parquet、文本等）或数据集目录中的数据集脚本（python 文件）。

    对于本地数据集：

    +   如果`path`是本地目录（仅包含数据文件）-> 根据目录内容加载通用数据集构建器（csv、json、文本等），例如`'./path/to/directory/with/my/csv/data'`。

    +   如果`path`是本地数据集脚本或包含本地数据集脚本的目录（如果脚本与目录同名）-> 从数据集脚本加载数据集构建器，例如`'./dataset/squad'`或`'./dataset/squad/squad.py'`。

    对于 Hugging Face Hub 上的数据集（使用`huggingface_hub.list_datasets`列出所有可用数据集）

    +   如果`path`是 HF Hub 上的数据集存储库（仅包含数据文件）-> 根据存储库内容加载通用数据集构建器（csv、文本等），例如`'username/dataset_name'`，包含您的数据文件的 HF Hub 上的数据集存储库。

    +   如果`path`是 HF Hub 上带有数据集脚本的数据集存储库（如果脚本与目录同名）-> 从数据集存储库中的数据集脚本加载数据集构建器，例如`glue`、`squad`、`'username/dataset_name'`，包含数据集脚本`'dataset_name.py'`的 HF Hub 上的数据集存储库。

+   `name` (`str`, *可选*) — 定义数据集配置的名称。

+   `data_dir` (`str`, *可选*) — 定义数据集配置的`data_dir`。如果为通用构建器（csv、文本等）或 Hub 数据集指定了`data_dir`，并且`data_files`为`None`，则行为等同于将`os.path.join(data_dir, **)`作为`data_files`传递，以引用目录中的所有文件。

+   `data_files` (`str`或`Sequence`或`Mapping`，*可选*) — 源数据文件的路径。

+   `cache_dir` (`str`, *可选*) — 读取/写入数据的目录。默认为`"~/.cache/huggingface/datasets"`。

+   `features`（Features，*可选*） — 设置用于此数据集的特征类型。

+   `download_config`（DownloadConfig，*可选*） — 特定的下载配置参数。

+   `download_mode`（DownloadMode 或`str`，默认为`REUSE_DATASET_IF_EXISTS`） — 下载/生成模式。

+   `revision`（Version 或`str`，*可选*） — 要加载的数据集脚本的版本。由于数据集在 Datasets Hub 上有自己的 git 存储库，默认版本“main”对应于它们的“main”分支。您可以通过使用数据集存储库的提交 SHA 或 git 标签指定与默认“main”不同的版本。

+   `token` (`str`或`bool`，*可选*) — 用作远程文件在 Datasets Hub 上的 Bearer 令牌的可选字符串或布尔值。如果为`True`，或未指定，则将从`"~/.huggingface"`获取令牌。

+   `use_auth_token` (`str` 或 `bool`, *可选*) — 用作远程文件在数据集 Hub 上的 Bearer 令牌的可选字符串或布尔值。如果为 `True`，或未指定，将从 `"~/.huggingface"` 获取令牌。

    在 2.14.0 中已弃用

    `use_auth_token` 在 2.14.0 中已弃用，推荐使用 `token`，并将在 3.0.0 中移除。

+   `storage_options` (`dict`, *可选*, 默认为 `None`) — **实验性**。要传递给数据集文件系统后端的键/值对，如果有的话。

    在 2.11.0 中添加

+   `trust_remote_code` (`bool`, 默认为 `True`) — 是否允许在 Hub 上使用数据集脚本定义数据集。此选项应仅在您信任的存储库中设置为 `True`，并且您已阅读了代码，因为它将在本地机器上执行 Hub 上存在的代码。

    `trust_remote_code` 将在下一个主要版本中默认为 False。

    在 2.16.0 中添加

+   *`*config_kwargs` (额外的关键字参数) — 要传递给 BuilderConfig 并在 DatasetBuilder 中使用的关键字参数。

从 Hugging Face Hub 加载数据集构建器，或本地数据集。数据集构建器可用于检查构建数据集所需的一般信息（缓存目录、配置、数据集信息等），而无需下载数据集本身。

您可以在 [Hub](https://huggingface.co/datasets) 上找到数据集列表，或使用 `huggingface_hub.list_datasets`。

数据集是包含以下内容的目录：

+   一些通用格式的数据文件（JSON、CSV、Parquet、文本等）

+   以及可选的数据集脚本，如果需要一些代码来读取数据文件。这用于加载任何格式或结构。

请注意，数据集脚本也可以从任何地方下载和读取数据文件 - 如果您的数据文件已经存在在线。

示例：

```py
>>> from datasets import load_dataset_builder
>>> ds_builder = load_dataset_builder('rotten_tomatoes')
>>> ds_builder.info.features
{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),
 'text': Value(dtype='string', id=None)}
```

#### `datasets.get_dataset_config_names`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/inspect.py#L291)

```py
( path: str revision: Union = None download_config: Optional = None download_mode: Union = None dynamic_modules_path: Optional = None data_files: Union = None **download_kwargs )
```

参数

+   `path` (`str`) — 包含数据集构建器的数据集处理脚本的路径。可以是：

    +   用于处理脚本的本地路径或包含脚本的目录的本地路径（如果脚本与目录同名），例如 `'./dataset/squad'` 或 `'./dataset/squad/squad.py'`

    +   在 Hugging Face Hub 上的数据集标识符（使用 datasets.list_datasets() 列出所有可用数据集和 ID）例如 `'squad'`、`'glue'` 或 `'openai/webtext'`

+   `revision` (`Union[str, datasets.Version]`, *可选*) — 如果指定，数据集模块将从此版本的数据集存储库加载。默认情况下：

    +   它设置为本地版本的库。

    +   如果在本地版本的库中不可用，它还将尝试从主分支加载。指定与本地版本的库不同的版本可能会导致兼容性问题。

+   `download_config` (DownloadConfig, *可选*) — 特定的下载配置参数。

+   `download_mode` (DownloadMode 或 `str`, 默认为 `REUSE_DATASET_IF_EXISTS`) — 下载/生成模式。

+   `dynamic_modules_path` (`str`, 默认为 `~/.cache/huggingface/modules/datasets_modules`) — 动态模块保存的可选路径。它必须已经使用 `init_dynamic_modules` 进行初始化。默认情况下，数据集和指标存储在 `datasets_modules` 模块内。

+   `data_files` (`Union[Dict, List, str]`, *可选*) — 定义数据集配置的 data_files。

+   *`*download_kwargs`（额外的关键字参数） — 用于覆盖提供的 `download_config` 中属性的 DownloadConfig 的可选属性，例如 `token`。

获取特定数据集的可用配置名称列表。

示例：

```py
>>> from datasets import get_dataset_config_names
>>> get_dataset_config_names("glue")
['cola',
 'sst2',
 'mrpc',
 'qqp',
 'stsb',
 'mnli',
 'mnli_mismatched',
 'mnli_matched',
 'qnli',
 'rte',
 'wnli',
 'ax']
```

#### `datasets.get_dataset_infos`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/inspect.py#L205)

```py
( path: str data_files: Union = None download_config: Optional = None download_mode: Union = None revision: Union = None token: Union = None use_auth_token = 'deprecated' **config_kwargs )
```

参数

+   `path` (`str`) — 数据集处理脚本的路径与数据集构建器。可以是：

    +   本地处理脚本的路径或包含脚本的目录（如果脚本与目录同名），例如 `'./dataset/squad'` 或 `'./dataset/squad/squad.py'`

    +   Hugging Face Hub 上的数据集标识符（使用 datasets.list_datasets() 列出所有可用数据集和标识符），例如 `'squad'`、`'glue'` 或 `'openai/webtext'`

+   `revision` (`Union[str, datasets.Version]`, *optional*) — 如果指定，数据集模块将从该版本的数据集存储库中加载。默认情况下：

    +   它设置为库的本地版本。

    +   如果在库的本地版本中不可用，它还会尝试从主分支加载。指定与库的本地版本不同的版本可能会导致兼容性问题。

+   `download_config` (DownloadConfig, *optional*) — 具体的下载配置参数。

+   `download_mode` (DownloadMode 或 `str`，默认为 `REUSE_DATASET_IF_EXISTS`) — 下载/生成模式。

+   `data_files` (`Union[Dict, List, str]`, *optional*) — 定义数据集配置的数据文件。

+   `token` (`str` 或 `bool`, *optional*) — 用作数据集中远程文件的 Bearer token 的可选字符串或布尔值。如果为 `True`，或未指定，将从 `"~/.huggingface"` 获取令牌。

+   `use_auth_token` (`str` 或 `bool`, *optional*) — 用作数据集中远程文件的 Bearer token 的可选字符串或布尔值。如果为 `True`，或未指定，将从 `"~/.huggingface"` 获取令牌。

    在 2.14.0 中已弃用

    `use_auth_token` 在 2.14.0 版本中已弃用，将在 3.0.0 中移除。

+   *`*config_kwargs`（额外的关键字参数） — 用于覆盖提供的属性的构建器类的可选属性。

获取关于数据集的元信息，返回一个将配置名称映射到 DatasetInfoDict 的字典。

示例：

```py
>>> from datasets import get_dataset_infos
>>> get_dataset_infos('rotten_tomatoes')
{'default': DatasetInfo(description="Movie Review Dataset.
 is a dataset of containing 5,331 positive and 5,331 negative processed
ences from Rotten Tomatoes movie reviews...), ...}
```

#### `datasets.get_dataset_split_names`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/inspect.py#L507)

```py
( path: str config_name: Optional = None data_files: Union = None download_config: Optional = None download_mode: Union = None revision: Union = None token: Union = None use_auth_token = 'deprecated' **config_kwargs )
```

参数

+   `path` (`str`) — 数据集处理脚本的路径与数据集构建器。可以是：

    +   本地处理脚本的路径或包含脚本的目录（如果脚本与目录同名），例如 `'./dataset/squad'` 或 `'./dataset/squad/squad.py'`

    +   Hugging Face Hub 上的数据集标识符（使用 datasets.list_datasets() 列出所有可用数据集和标识符），例如 `'squad'`、`'glue'` 或 `'openai/webtext'`

+   `config_name` (`str`, *optional*) — 定义数据集配置的名称。

+   `data_files` (`str` 或 `Sequence` 或 `Mapping`, *optional*) — 源数据文件的路径。

+   `download_config` (DownloadConfig, *optional*) — 具体的下载配置参数。

+   `download_mode` (DownloadMode 或 `str`，默认为 `REUSE_DATASET_IF_EXISTS`) — 下载/生成模式。

+   `revision` (Version 或 `str`, *optional*) — 要加载的数据集脚本的版本。由于数据集在数据集 Hub 上有自己的 git 存储库，默认版本“main”对应于它们的“main”分支。您可以通过使用数据集存储库的提交 SHA 或 git 标签指定与默认“main”不同的版本。

+   `token` (`str` 或 `bool`, *optional*) — 用作数据集中远程文件的 Bearer 令牌的可选字符串或布尔值。如果为 `True`，或未指定，将从 `"~/.huggingface"` 获取令牌。

+   `use_auth_token` (`str` 或 `bool`, *optional*) — 用作数据集中远程文件的 Bearer 令牌的可选字符串或布尔值。如果为 `True`，或未指定，将从 `"~/.huggingface"` 获取令牌。

    在 2.14.0 中已弃用

    `use_auth_token` 在版本 2.14.0 中已弃用，推荐使用 `token`，并将在 3.0.0 中移除。

+   *`*config_kwargs` (额外的关键字参数) — 用于构建器类的可选属性，如果提供了这些属性，则会覆盖这些属性。

获取特定配置和数据集的可用拆分列表。

示例：

```py
>>> from datasets import get_dataset_split_names
>>> get_dataset_split_names('rotten_tomatoes')
['train', 'validation', 'test']
```

#### `datasets.inspect_dataset`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/inspect.py#L124)

```py
( path: str local_path: str download_config: Optional = None **download_kwargs )
```

参数

+   `path` (`str`) — 数据集处理脚本的路径，与数据集构建器一起。可以是：

    +   处理脚本的本地路径或包含脚本的目录的本地路径（如果脚本与目录同名），例如 `'./dataset/squad'` 或 `'./dataset/squad/squad.py'`。

    +   Hugging Face Hub 上的数据集标识符（使用 list_datasets() 列出所有可用的数据集和 ID）例如 `'squad'`, `'glue'` 或 `'openai/webtext'`。

+   `local_path` (`str`) — 将数据集脚本复制到的本地文件夹的路径。

+   `download_config` (DownloadConfig, *optional*) — 具体的下载配置参数。

+   *`*download_kwargs` (额外的关键字参数) — 用于 DownloadConfig 的可选参数，如果提供了这些参数，则会覆盖 `download_config` 的属性。

通过将数据集脚本复制到本地驱动器上的本地路径来允许检查/修改数据集脚本。

## 指标

指标在 🤗 Datasets 中已弃用。要了解有关如何使用指标的更多信息，请查看库 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index)! 除了指标之外，您还可以找到更多用于评估模型和数据集的工具。

#### `datasets.list_metrics`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/inspect.py#L85)

```py
( with_community_metrics = True with_details = False )
```

参数

+   `with_community_metrics` (`bool`, optional, default `True`) — 包括社区提供的指标。

+   `with_details` (`bool`, optional, default `False`) — 返回指标的完整详情而不仅仅是简称。

列出在 Hugging Face Hub 上可用的所有指标脚本。

在 2.5.0 中已弃用

请改用 *evaluate.list_evaluation_modules*，来自新库 🤗 Evaluate: [`huggingface.co/docs/evaluate`](https://huggingface.co/docs/evaluate)

示例：

```py
>>> from datasets import list_metrics
>>> list_metrics()
['accuracy',
 'bertscore',
 'bleu',
 'bleurt',
 'cer',
 'chrf',
 ...
]
```

#### `datasets.load_metric`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/load.py#L1992)

```py
( path: str config_name: Optional = None process_id: int = 0 num_process: int = 1 cache_dir: Optional = None experiment_id: Optional = None keep_in_memory: bool = False download_config: Optional = None download_mode: Union = None revision: Union = None trust_remote_code: Optional = None **metric_init_kwargs )
```

参数

+   `path` (`str`) — 包含指标构建器的指标处理脚本的路径。可以是：

    +   处理脚本的本地路径或包含脚本的目录的本地路径（如果脚本与目录同名），例如 `'./metrics/rouge'` 或 `'./metrics/rogue/rouge.py'`

    +   HuggingFace 数据集存储库中的指标标识符（使用 `datasets.list_metrics()` 列出所有可用的指标）例如 `'rouge'` 或 `'bleu'`

+   `config_name` (`str`, optional) — 为指标选择一个配置（例如，GLUE 指标为每个子集都有一个配置）

+   `process_id`（`int`，可选） — 用于分布式评估：进程的 id

+   `num_process`（`int`，可选） — 用于分布式评估：进程的总数

+   `cache_dir`（可选 str） — 用于存储临时预测和参考的路径（默认为*~/.cache/huggingface/metrics/*）

+   `experiment_id`（`str`） — 特定的实验 id。如果几个分布式评估共享相同的文件系统，则会使用此选项。这对于在分布式设置中计算指标很有用（特别是像 F1 这样的非加法指标）。

+   `keep_in_memory`（bool） — 是否将临时结果存储在内存中（默认为 False）

+   `download_config`（可选`datasets.DownloadConfig` — 特定的下载配置参数。

+   `download_mode`（DownloadMode 或`str`，默认为`REUSE_DATASET_IF_EXISTS`） — 下载/生成模式。

+   `revision`（可选`Union[str, datasets.Version]`） — 如果指定，模块将从数据集存储库中加载此版本。默认情况下，它设置为库的本地版本。指定与本地库版本不同的版本可能会导致兼容性问题。

+   `trust_remote_code`（*bool*，默认为*True*） — 是否允许使用数据集脚本在 Hub 上定义数据集。此选项应仅对您信任的存储库设置为*True*，并且您已经阅读了代码，因为它将在本地机器上执行 Hub 上存在的代码。

    *trust_remote_code* 在下一个主要版本中默认为 False。

    在 2.16.0 中添加

加载一个*datasets.Metric*。

在 2.5.0 中弃用

请改用*evaluate.load*，从新库🤗 Evaluate：[`huggingface.co/docs/evaluate`](https://huggingface.co/docs/evaluate)

示例：

```py
>>> from datasets import load_metric
>>> accuracy = load_metric('accuracy')
>>> accuracy.compute(references=[1, 0], predictions=[1, 1])
{'accuracy': 0.5}
```

#### `datasets.inspect_metric`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/inspect.py#L161)

```py
( path: str local_path: str download_config: Optional = None **download_kwargs )
```

参数

+   `path`（`str`） — 包含数据集构建器的数据集处理脚本的路径。可以是：

    +   处理脚本的本地路径或包含脚本的目录（如果脚本与目录同名），例如`'./dataset/squad'`或`'./dataset/squad/squad.py'`

    +   Hugging Face Hub 上的数据集标识符（使用`datasets.list_datasets()`列出所有可用数据集和 id）例如`'squad'`，`'glue'`或`'openai/webtext'`

+   `local_path`（`str`） — 用于将数据集脚本复制到的本地文件夹的路径。

+   `download_config`（可选`datasets.DownloadConfig`） — 特定的下载配置参数。

+   *`*download_kwargs`（额外的关键字参数） — 可选属性，用于覆盖 download_config 中的属性。

通过将指标脚本复制到本地驱动器上的 local_path 来允许检查/修改指标脚本。

在 2.5.0 中弃用

请改用*evaluate.inspect_evaluation_module*，从新库🤗 Evaluate 替代：[`huggingface.co/docs/evaluate`](https://huggingface.co/docs/evaluate)

## 从文件

用于加载数据文件的配置。在加载本地文件或数据集存储库时使用：

+   本地文件：`load_dataset("parquet", data_dir="path/to/data/dir")`

+   数据集存储库：`load_dataset("allenai/c4")`

您可以传递参数给`load_dataset`以配置数据加载。例如，您可以指定`sep`参数来定义用于加载数据的 CsvConfig：

```py
load_dataset("csv", data_dir="path/to/data/dir", sep="\t")
```

### 文本

### `class datasets.packaged_modules.text.TextConfig`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/text/text.py#L17)

```py
( name: str = 'default' version: Union = 0.0.0 data_dir: Optional = None data_files: Union = None description: Optional = None features: Optional = None encoding: str = 'utf-8' errors: dataclasses.InitVar[typing.Optional[str]] = 'deprecated' encoding_errors: Optional = None chunksize: int = 10485760 keep_linebreaks: bool = False sample_by: str = 'line' )
```

文本文件的 BuilderConfig。

### `class datasets.packaged_modules.text.Text`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/text/text.py#L39)

```py
( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )
```

### CSV

### `class datasets.packaged_modules.csv.CsvConfig`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/csv/csv.py#L23)

```py
( name: str = 'default' version: Union = 0.0.0 data_dir: Optional = None data_files: Union = None description: Optional = None sep: str = ',' delimiter: Optional = None header: Union = 'infer' names: Optional = None column_names: Optional = None index_col: Union = None usecols: Union = None prefix: Optional = None mangle_dupe_cols: bool = True engine: Optional = None converters: Dict = None true_values: Optional = None false_values: Optional = None skipinitialspace: bool = False skiprows: Union = None nrows: Optional = None na_values: Union = None keep_default_na: bool = True na_filter: bool = True verbose: bool = False skip_blank_lines: bool = True thousands: Optional = None decimal: str = '.' lineterminator: Optional = None quotechar: str = '"' quoting: int = 0 escapechar: Optional = None comment: Optional = None encoding: Optional = None dialect: Optional = None error_bad_lines: bool = True warn_bad_lines: bool = True skipfooter: int = 0 doublequote: bool = True memory_map: bool = False float_precision: Optional = None chunksize: int = 10000 features: Optional = None encoding_errors: Optional = 'strict' on_bad_lines: Literal = 'error' date_format: Optional = None )
```

CSV 的 BuilderConfig。

### `class datasets.packaged_modules.csv.Csv`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/csv/csv.py#L137)

```py
( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )
```

### JSON

### `class datasets.packaged_modules.json.JsonConfig`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/json/json.py#L18)

```py
( name: str = 'default' version: Union = 0.0.0 data_dir: Optional = None data_files: Union = None description: Optional = None features: Optional = None encoding: str = 'utf-8' encoding_errors: Optional = None field: Optional = None use_threads: bool = True block_size: Optional = None chunksize: int = 10485760 newlines_in_values: Optional = None )
```

JSON 的 BuilderConfig。

### `class datasets.packaged_modules.json.Json`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/json/json.py#L32)

```py
( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )
```

### Parquet

### `class datasets.packaged_modules.parquet.ParquetConfig`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/parquet/parquet.py#L15)

```py
( name: str = 'default' version: Union = 0.0.0 data_dir: Optional = None data_files: Union = None description: Optional = None batch_size: int = 10000 columns: Optional = None features: Optional = None )
```

Parquet 的 BuilderConfig。

### `class datasets.packaged_modules.parquet.Parquet`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/parquet/parquet.py#L24)

```py
( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )
```

### Arrow

### `class datasets.packaged_modules.arrow.ArrowConfig`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/arrow/arrow.py#L14)

```py
( name: str = 'default' version: Union = 0.0.0 data_dir: Optional = None data_files: Union = None description: Optional = None features: Optional = None )
```

Arrow 的 BuilderConfig。

### `class datasets.packaged_modules.arrow.Arrow`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/arrow/arrow.py#L21)

```py
( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )
```

### SQL

### `class datasets.packaged_modules.sql.SqlConfig`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/sql/sql.py#L23)

```py
( name: str = 'default' version: Union = 0.0.0 data_dir: Optional = None data_files: Union = None description: Optional = None sql: Union = None con: Union = None index_col: Union = None coerce_float: bool = True params: Union = None parse_dates: Union = None columns: Optional = None chunksize: Optional = 10000 features: Optional = None )
```

SQL 的 BuilderConfig。

### `class datasets.packaged_modules.sql.Sql`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/sql/sql.py#L90)

```py
( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )
```

### 图像

### `class datasets.packaged_modules.imagefolder.ImageFolderConfig`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/imagefolder/imagefolder.py#L12)

```py
( name: str = 'default' version: Union = 0.0.0 data_dir: Optional = None data_files: Union = None description: Optional = None features: Optional = None drop_labels: bool = None drop_metadata: bool = None )
```

ImageFolder 的 BuilderConfig。

### `class datasets.packaged_modules.imagefolder.ImageFolder`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/imagefolder/imagefolder.py#L19)

```py
( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )
```

### 音频

### `class datasets.packaged_modules.audiofolder.AudioFolderConfig`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/audiofolder/audiofolder.py#L12)

```py
( name: str = 'default' version: Union = 0.0.0 data_dir: Optional = None data_files: Union = None description: Optional = None features: Optional = None drop_labels: bool = None drop_metadata: bool = None )
```

音频文件夹的 Builder 配置。

### `class datasets.packaged_modules.audiofolder.AudioFolder`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/audiofolder/audiofolder.py#L19)

```py
( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )
```

### WebDataset

### `class datasets.packaged_modules.webdataset.WebDataset`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/packaged_modules/webdataset/webdataset.py#L15)

```py
( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )
```
