- en: CLAP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CLAP model was proposed in [Large Scale Contrastive Language-Audio pretraining
    with feature fusion and keyword-to-caption augmentation](https://arxiv.org/pdf/2211.06687.pdf)
    by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo
    Dubnov.
  prefs: []
  type: TYPE_NORMAL
- en: CLAP (Contrastive Language-Audio Pretraining) is a neural network trained on
    a variety of (audio, text) pairs. It can be instructed in to predict the most
    relevant text snippet, given an audio, without directly optimizing for the task.
    The CLAP model uses a SWINTransformer to get audio features from a log-Mel spectrogram
    input, and a RoBERTa model to get text features. Both the text and audio features
    are then projected to a latent space with identical dimension. The dot product
    between the projected audio and text features is then used as a similar score.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Contrastive learning has shown remarkable success in the field of multimodal
    representation learning. In this paper, we propose a pipeline of contrastive language-audio
    pretraining to develop an audio representation by combining audio data with natural
    language descriptions. To accomplish this target, we first release LAION-Audio-630K,
    a large collection of 633,526 audio-text pairs from different data sources. Second,
    we construct a contrastive language-audio pretraining model by considering different
    audio encoders and text encoders. We incorporate the feature fusion mechanism
    and keyword-to-caption augmentation into the model design to further enable the
    model to process audio inputs of variable lengths and enhance the performance.
    Third, we perform comprehensive experiments to evaluate our model across three
    tasks: text-to-audio retrieval, zero-shot audio classification, and supervised
    audio classification. The results demonstrate that our model achieves superior
    performance in text-to-audio retrieval task. In audio classification tasks, the
    model achieves state-of-the-art performance in the zeroshot setting and is able
    to obtain performance comparable to models’ results in the non-zero-shot setting.
    LAION-Audio-6*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [Younes Belkada](https://huggingface.co/ybelkada)
    and [Arthur Zucker](https://huggingface.co/ArthurZ) . The original code can be
    found [here](https://github.com/LAION-AI/Clap).
  prefs: []
  type: TYPE_NORMAL
- en: ClapConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClapConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/configuration_clap.py#L334)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text_config` (`dict`, *optional*) — Dictionary of configuration options used
    to initialize [ClapTextConfig](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audio_config` (`dict`, *optional*) — Dictionary of configuration options used
    to initialize [ClapAudioConfig](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapAudioConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logit_scale_init_value` (`float`, *optional*, defaults to 14.29) — The inital
    value of the *logit_scale* paramter. Default is used as per the original CLAP
    implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`projection_dim` (`int`, *optional*, defaults to 512) — Dimentionality of text
    and audio projection layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`projection_hidden_act` (`str`, *optional*, defaults to `"relu"`) — Activation
    function for the projection layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — Factor to scale
    the initialization of the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (*optional*) — Dictionary of keyword arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ClapConfig](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapConfig)
    is the configuration class to store the configuration of a [ClapModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel).
    It is used to instantiate a CLAP model according to the specified arguments, defining
    the text model and audio model configs. Instantiating a configuration with the
    defaults will yield a similar configuration to that of the CLAP [laion/clap-htsat-fused](https://huggingface.co/laion/clap-htsat-fused)
    architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_text_audio_configs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/configuration_clap.py#L422)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ClapConfig](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapConfig)'
  prefs: []
  type: TYPE_NORMAL
- en: An instance of a configuration object
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a [ClapConfig](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapConfig)
    (or a derived class) from clap text model configuration and clap audio model configuration.
  prefs: []
  type: TYPE_NORMAL
- en: ClapTextConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClapTextConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/configuration_clap.py#L32)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) — Vocabulary size of the
    CLAP model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [ClapTextModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (often named feed-forward) layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"relu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"relu"`, `"relu"`, `"silu"` and `"relu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — The
    dropout ratio for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`type_vocab_size` (`int`, *optional*, defaults to 2) — The vocabulary size
    of the `token_type_ids` passed when calling [ClapTextModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`position_embedding_type` (`str`, *optional*, defaults to `"absolute"`) — Type
    of position embedding. Choose one of `"absolute"`, `"relative_key"`, `"relative_key_query"`.
    For positional embeddings use `"absolute"`. For more information on `"relative_key"`,
    please refer to [Self-Attention with Relative Position Representations (Shaw et
    al.)](https://arxiv.org/abs/1803.02155). For more information on `"relative_key_query"`,
    please refer to *Method 4* in [Improve Transformer Models with Better Relative
    Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_decoder` (`bool`, *optional*, defaults to `False`) — Whether the model
    is used as a decoder or not. If `False`, the model is used as an encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models). Only relevant
    if `config.is_decoder=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`projection_hidden_act` (`str`, *optional*, defaults to `"relu"`) — The non-linear
    activation function (function or string) in the projection layer. If string, `"gelu"`,
    `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`projection_dim` (`int`, *optional*, defaults to 512) — Dimension of the projection
    head of the `ClapTextModelWithProjection`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [ClapTextModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModel).
    It is used to instantiate a CLAP model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the CLAP [calp-hsat-fused](https://huggingface.co/laion/clap-hsat-fused)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ClapAudioConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClapAudioConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/configuration_clap.py#L164)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`window_size` (`int`, *optional*, defaults to 8) — Image size of the spectrogram'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_mel_bins` (`int`, *optional*, defaults to 64) — Number of mel features
    used per frames. Should correspond to the value used in the `ClapProcessor` class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spec_size` (`int`, *optional*, defaults to 256) — Desired input size of the
    spectrogram that the model supports. It can be different from the output of the
    `ClapFeatureExtractor`, in which case the input features will be resized. Corresponds
    to the `image_size` of the audio models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str`, *optional*, defaults to `"gelu"`) — The non-linear activation
    function (function or string) in the encoder and pooler. If string, `"gelu"`,
    `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_size` (`int`, *optional*, defaults to 4) — Patch size for the audio
    spectrogram'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_stride` (`list`, *optional*, defaults to `[4, 4]`) — Patch stride for
    the audio spectrogram'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_classes` (`int`, *optional*, defaults to 527) — Number of classes used
    for the head training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Hidden size of the output
    of the audio encoder. Correspond to the dimension of the penultimate layer’s output,which
    is sent to the projection MLP layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`projection_dim` (`int`, *optional*, defaults to 512) — Hidden size of the
    projection layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`depths` (`list`, *optional*, defaults to `[2, 2, 6, 2]`) — Depths used for
    the Swin Layers of the audio model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`list`, *optional*, defaults to `[4, 8, 16, 32]`) —
    Number of attention heads used for the Swin Layers of the audio model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enable_fusion` (`bool`, *optional*, defaults to `False`) — Whether or not
    to enable patch fusion. This is the main contribution of the authors, and should
    give the best results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probabilitiy for all fully connected layers in the encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fusion_type` (`[type]`, *optional*) — Fusion type used for the patch fusion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_embed_input_channels` (`int`, *optional*, defaults to 1) — Number of
    channels used for the input spectrogram'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flatten_patch_embeds` (`bool`, *optional*, defaults to `True`) — Whether or
    not to flatten the patch embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_embeds_hidden_size` (`int`, *optional*, defaults to 96) — Hidden size
    of the patch embeddings. It is used as the number of output channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enable_patch_layer_norm` (`bool`, *optional*, defaults to `True`) — Whether
    or not to enable layer normalization for the patch embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_path_rate` (`float`, *optional*, defaults to 0.0) — Drop path rate for
    the patch fusion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — The
    dropout ratio for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — Whether or not to add
    a bias to the query, key, value projections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlp_ratio` (`float`, *optional*, defaults to 4.0) — Ratio of the mlp hidden
    dim to embedding dim.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aff_block_r` (`int`, *optional*, defaults to 4) — downsize_ratio used in the
    AudioFF block'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 4) — Number of hidden layers
    in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`projection_hidden_act` (`str`, *optional*, defaults to `"relu"`) — The non-linear
    activation function (function or string) in the projection layer. If string, `"gelu"`,
    `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`[type]`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — A factor for
    initializing all weight matrices (should be kept to 1, used internally for initialization
    testing).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [ClapAudioModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapAudioModel).
    It is used to instantiate a CLAP audio encoder according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the audio encoder of the CLAP [laion/clap-htsat-fused](https://huggingface.co/laion/clap-htsat-fused)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ClapFeatureExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClapFeatureExtractor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/feature_extraction_clap.py#L33)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_size` (`int`, *optional*, defaults to 64) — The feature dimension
    of the extracted Mel spectrograms. This corresponds to the number of mel filters
    (`n_mels`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*, defaults to 48000) — The sampling rate
    at which the audio files should be digitalized expressed in hertz (Hz). This only
    serves to warn users if the audio fed to the feature extractor does not have the
    same sampling rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hop_length` (`int`,*optional*, defaults to 480) — Length of the overlaping
    windows for the STFT used to obtain the Mel Spectrogram. The audio will be split
    in smaller `frames` with a step of `hop_length` between each frame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length_s` (`int`, *optional*, defaults to 10) — The maximum input length
    of the model in seconds. This is used to pad the audio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fft_window_size` (`int`, *optional*, defaults to 1024) — Size of the window
    (in samples) on which the Fourier transform is applied. This controls the frequency
    resolution of the spectrogram. 400 means that the fourrier transform is computed
    on windows of 400 samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding_value` (`float`, *optional*, defaults to 0.0) — Padding value used
    to pad the audio. Should correspond to silences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_attention_mask` (`bool`, *optional*, defaults to `False`) — Whether
    or not the model should return the attention masks coresponding to the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frequency_min` (`float`, *optional*, defaults to 0) — The lowest frequency
    of interest. The STFT will not be computed for values below this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frequency_max` (`float`, *optional*, defaults to 14000) — The highest frequency
    of interest. The STFT will not be computed for values above this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_db` (`float`, *optional*) — The highest decibel value used to convert
    the mel spectrogram to the log scale. For more details see the `audio_utils.power_to_db`
    function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`str`, *optional*, defaults to `"fusion"`) — Truncation pattern
    for long audio inputs. Two patterns are available:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fusion` will use `_random_mel_fusion`, which stacks 3 random crops from the
    mel spectrogram and a downsampled version of the entire mel spectrogram. If `config.fusion`
    is set to True, shorter audios also need to to return 4 mels, which will just
    be a copy of the original mel obtained from the padded audio.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rand_trunc` will select a random crop of the mel spectrogram.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`str`, *optional*, defaults to `"repeatpad"`) — Padding pattern
    for shorter audio inputs. Three patterns were originally implemented:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`repeatpad`: the audio is repeated, and then padded to fit the `max_length`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`repeat`: the audio is repeated and then cut to fit the `max_length`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad`: the audio is padded.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a CLAP feature extractor.
  prefs: []
  type: TYPE_NORMAL
- en: This feature extractor inherits from [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: This class extracts mel-filter bank features from raw speech using a custom
    numpy implementation of the *Short Time Fourier Transform* (STFT) which should
    match pytorch’s `torch.stft` equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `to_dict`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/feature_extraction_clap.py#L138)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict[str, Any]`'
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary of all the attributes that make up this configuration instance, excpet
    for the mel filter banks, which do not need to be saved or printed as they are
    too long.
  prefs: []
  type: TYPE_NORMAL
- en: Serializes this instance to a Python dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: ClapProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClapProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/processing_clap.py#L23)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_extractor` ([ClapFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor))
    — The audio processor is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([RobertaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizerFast))
    — The tokenizer is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a CLAP processor which wraps a CLAP feature extractor and a RoBerta
    tokenizer into a single processor.
  prefs: []
  type: TYPE_NORMAL
- en: '[ClapProcessor](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapProcessor)
    offers all the functionalities of [ClapFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor)
    and [RobertaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizerFast).
    See the `__call__()` and [decode()](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapProcessor.decode)
    for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `batch_decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/processing_clap.py#L99)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This method forwards all its arguments to RobertaTokenizerFast’s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/processing_clap.py#L106)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This method forwards all its arguments to RobertaTokenizerFast’s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  prefs: []
  type: TYPE_NORMAL
- en: ClapModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClapModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L1920)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([ClapConfig](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L2050)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Input audio features. This should be returnes by the [ClapFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor)
    class that you can also retrieve from [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor).
    See `ClapFeatureExtractor.__call__()` for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_loss` (`bool`, *optional*) — Whether or not to return the contrastive
    loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.clap.modeling_clap.ClapOutput` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.clap.modeling_clap.ClapOutput` or a tuple of `torch.FloatTensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration (`<class 'transformers.models.clap.configuration_clap.ClapConfig'>`)
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss`
    is `True`) — Contrastive loss for audio-text similarity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_per_audio:(torch.FloatTensor` of shape `(audio_batch_size, text_batch_size)`)
    — The scaled dot product scores between `audio_embeds` and `text_embeds`. This
    represents the audio-text similarity scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_per_text:(torch.FloatTensor` of shape `(text_batch_size, audio_batch_size)`)
    — The scaled dot product scores between `text_embeds` and `audio_embeds`. This
    represents the text-audio similarity scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_embeds(torch.FloatTensor` of shape `(batch_size, output_dim`) — The text
    embeddings obtained by applying the projection layer to the pooled output of [ClapTextModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audio_embeds(torch.FloatTensor` of shape `(batch_size, output_dim`) — The
    audio embeddings obtained by applying the projection layer to the pooled output
    of [ClapAudioModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapAudioModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_model_output(BaseModelOutputWithPooling):` The output of the [ClapTextModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audio_model_output(BaseModelOutputWithPooling):` The output of the [ClapAudioModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapAudioModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [ClapModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#### `get_text_features`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L1956)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`)
  prefs: []
  type: TYPE_NORMAL
- en: The text embeddings obtained by applying the projection layer to the pooled
    output of [ClapTextModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModel).
  prefs: []
  type: TYPE_NORMAL
- en: The [ClapModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '#### `get_audio_features`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L2004)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Input audio features. This should be returnes by the [ClapFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor)
    class that you can also retrieve from [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor).
    See `ClapFeatureExtractor.__call__()` for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_longer` (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*) —
    Whether the audio clip is longer than `max_length`. If `True`, a feature fusion
    will be enabled to enhance the features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: audio_features (`torch.FloatTensor` of shape `(batch_size, output_dim`)
  prefs: []
  type: TYPE_NORMAL
- en: The audio embeddings obtained by applying the projection layer to the pooled
    output of [ClapAudioModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapAudioModel).
  prefs: []
  type: TYPE_NORMAL
- en: The [ClapModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: ClapTextModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClapTextModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L1751)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The model can behave as an encoder (with only self-attention) as well as a decoder,
    in which case a layer of cross-attention is added between the self-attention layers,
    following the architecture described in *Attention is all you need*_ by Ashish
    Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
    Lukasz Kaiser and Illia Polosukhin.
  prefs: []
  type: TYPE_NORMAL
- en: To behave as an decoder the model needs to be initialized with the `is_decoder`
    argument of the configuration set to `True`. To be used in a Seq2Seq model, the
    model needs to initialized with both `is_decoder` argument and `add_cross_attention`
    set to `True`; an `encoder_hidden_states` is then expected as an input to the
    forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: '.. _*Attention is all you need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L1789)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*): Sequence of hidden-states at the output of the last
    layer of the encoder. Used in the cross-attention if the model is configured as
    a decoder. encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size,
    sequence_length)`, *optional*): Mask to avoid performing attention on the padding
    token indices of the encoder input. This mask is used in the cross-attention if
    the model is configured as a decoder. Mask values selected in `[0, 1]`:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '0 for tokens that are `masked`. past_key_values (`tuple(tuple(torch.FloatTensor))`
    of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size,
    num_heads, sequence_length - 1, embed_size_per_head)`): Contains precomputed key
    and value hidden states of the attention blocks. Can be used to speed up decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `past_key_values` are used, the user can optionally input only the last
    `decoder_input_ids` (those that don’t have their past key value states given to
    this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids` of shape
    `(batch_size, sequence_length)`. use_cache (`bool`, *optional*): If set to `True`,
    `past_key_values` key value states are returned and can be used to speed up decoding
    (see `past_key_values`).'
  prefs: []
  type: TYPE_NORMAL
- en: ClapTextModelWithProjection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClapTextModelWithProjection`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L2148)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([ClapConfig](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CLAP Text Model with a projection layer on top (a linear layer on top of the
    pooled output).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L2170)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.clap.modeling_clap.ClapTextModelOutput` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.clap.modeling_clap.ClapTextModelOutput` or a tuple of
    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.clap.configuration_clap.ClapTextConfig'>`)
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`text_embeds` (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional*
    returned when model is initialized with `with_projection=True`) — The text embeddings
    obtained by applying the projection layer to the pooler_output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ClapTextModelWithProjection](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModelWithProjection)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: ClapAudioModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClapAudioModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L1693)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L1706)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Input audio features. This should be returnes by the [ClapFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor)
    class that you can also retrieve from [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor).
    See `ClapFeatureExtractor.__call__()` for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_longer` (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*) —
    Whether the audio clip is longer than `max_length`. If `True`, a feature fusion
    will be enabled to enhance the features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.clap.configuration_clap.ClapAudioConfig'>`)
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ClapAudioModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapAudioModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: ClapAudioModelWithProjection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClapAudioModelWithProjection`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L2224)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([ClapConfig](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CLAP Audio Model with a projection layer on top (a linear layer on top of the
    pooled output).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L2244)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Input audio features. This should be returnes by the [ClapFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor)
    class that you can also retrieve from [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor).
    See `ClapFeatureExtractor.__call__()` for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_longer` (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*) —
    Whether the audio clip is longer than `max_length`. If `True`, a feature fusion
    will be enabled to enhance the features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.clap.modeling_clap.ClapAudioModelOutput` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.clap.modeling_clap.ClapAudioModelOutput` or a tuple of
    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.clap.configuration_clap.ClapAudioConfig'>`)
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`audio_embeds` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`) —
    The Audio embeddings obtained by applying the projection layer to the pooler_output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ClapAudioModelWithProjection](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapAudioModelWithProjection)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
