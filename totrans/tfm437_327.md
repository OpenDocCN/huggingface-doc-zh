# WavLM

> 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wavlm](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wavlm)

## 概述

WavLM模型是由Sanyuan Chen、Chengyi Wang、Zhengyang Chen、Yu Wu、Shujie Liu、Zhuo Chen、Jinyu Li、Naoyuki Kanda、Takuya Yoshioka、Xiong Xiao、Jian Wu、Long Zhou、Shuo Ren、Yanmin Qian、Yao Qian、Jian Wu、Michael Zeng、Furu Wei在[《WavLM: 大规模自监督预训练用于全栈语音处理》](https://arxiv.org/abs/2110.13900)中提出的。

该论文的摘要如下：

*自监督学习（SSL）在语音识别中取得了巨大成功，但对其他语音处理任务的探索有限。由于语音信号包含说话人身份、语用学、口语内容等多方面信息，为所有语音任务学习通用表示是具有挑战性的。在本文中，我们提出了一个新的预训练模型WavLM，用于解决全栈下游语音任务。WavLM基于HuBERT框架构建，重点放在口语内容建模和说话人身份保留上。我们首先为Transformer结构配备了门控相对位置偏差，以提高其在识别任务上的能力。为了更好地区分说话人，我们提出了一种话语混合训练策略，其中额外的重叠话语是无监督创建的，并在模型训练过程中加以整合。最后，我们将训练数据集从60k小时扩大到94k小时。WavLM Large在SUPERB基准测试中取得了最先进的性能，并为各种语音处理任务在其代表性基准测试中带来了显著改进。*

相关检查点可以在[https://huggingface.co/models?other=wavlm](https://huggingface.co/models?other=wavlm)下找到。

该模型由[patrickvonplaten](https://huggingface.co/patrickvonplaten)贡献。作者的代码可以在[这里](https://github.com/microsoft/unilm/tree/master/wavlm)找到。

## 使用提示

+   WavLM是一个接受与语音信号的原始波形对应的浮点数组的语音模型。请使用[Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)进行特征提取。

+   WavLM模型可以使用连接主义时间分类（CTC）进行微调，因此模型输出必须使用[Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer)进行解码。

+   WavLM在说话人验证、说话人识别和说话人分割任务中表现特别好。

## 资源

+   [音频分类任务指南](../tasks/audio_classification)

+   [自动语音识别任务指南](../tasks/asr)

## WavLMConfig

### `class transformers.WavLMConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wavlm/configuration_wavlm.py#L32)

```py
( vocab_size = 32 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout = 0.1 activation_dropout = 0.1 attention_dropout = 0.1 feat_proj_dropout = 0.0 final_dropout = 0.1 layerdrop = 0.1 initializer_range = 0.02 layer_norm_eps = 1e-05 feat_extract_norm = 'group' feat_extract_activation = 'gelu' conv_dim = (512, 512, 512, 512, 512, 512, 512) conv_stride = (5, 2, 2, 2, 2, 2, 2) conv_kernel = (10, 3, 3, 3, 3, 2, 2) conv_bias = False num_conv_pos_embeddings = 128 num_conv_pos_embedding_groups = 16 num_buckets = 320 max_bucket_distance = 800 do_stable_layer_norm = False apply_spec_augment = True mask_time_prob = 0.05 mask_time_length = 10 mask_time_min_masks = 2 mask_feature_prob = 0.0 mask_feature_length = 10 num_codevectors_per_group = 320 num_codevector_groups = 2 contrastive_logits_temperature = 0.1 num_negatives = 100 codevector_dim = 256 proj_codevector_dim = 256 diversity_loss_weight = 0.1 ctc_loss_reduction = 'mean' ctc_zero_infinity = False use_weighted_layer_sum = False classifier_proj_size = 256 tdnn_dim = (512, 512, 512, 512, 1500) tdnn_kernel = (5, 3, 3, 1, 1) tdnn_dilation = (1, 2, 3, 1, 1) xvector_output_dim = 512 num_ctc_classes = 80 pad_token_id = 0 bos_token_id = 1 eos_token_id = 2 add_adapter = False adapter_kernel_size = 3 adapter_stride = 2 num_adapter_layers = 3 output_hidden_size = None **kwargs )
```

参数

+   `vocab_size`（`int`，*可选*，默认为32）— WavLM模型的词汇大小。定义了在调用[WavLMModel](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMModel)时可以表示的不同令牌数量。模型的词汇大小。定义了可以由传递给[WavLMModel](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMModel)的*inputs_ids*表示的不同令牌。

+   `hidden_size`（`int`，*可选*，默认为768）— 编码器层和池化层的维度。

+   `num_hidden_layers`（`int`，*可选*，默认为12）— Transformer编码器中的隐藏层数。

+   `num_attention_heads`（`int`，*可选*，默认为12）— Transformer编码器中每个注意力层的注意力头数。

+   `intermediate_size`（`int`，*可选*，默认为3072）— Transformer编码器中“中间”（即前馈）层的维度。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`, `"relu"`, `"selu"` 和 `"gelu_new"`。

+   `hidden_dropout` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的丢弃概率。

+   `activation_dropout` (`float`, *optional*, defaults to 0.1) — 全连接层内部激活的丢弃比例。

+   `attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力概率的丢弃比例。

+   `final_dropout` (`float`, *optional*, defaults to 0.1) — [WavLMForCTC](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMForCTC) 最终投影层的丢弃概率。

+   `layerdrop` (`float`, *optional*, defaults to 0.1) — LayerDrop 概率。详情请参考 [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层规范化层使用的 epsilon 值。

+   `feat_extract_norm` (`str`, *optional*, defaults to `"group"`) — 用于特征编码器中的1D卷积层的规范化方式。可以选择 `"group"` 表示仅对第一个1D卷积层进行分组规范化，或者选择 `"layer"` 表示对所有1D卷积层进行层规范化。

+   `feat_proj_dropout` (`float`, *optional*, defaults to 0.0) — 特征编码器输出的丢弃概率。

+   `feat_extract_activation` (`str,` optional`, defaults to` “gelu”`) -- 特征提取器中1D卷积层的非线性激活函数（函数或字符串）。如果是字符串，支持` “gelu”`,` “relu”`,` “selu”`和`“gelu_new”`。

+   `conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512, 512, 512, 512, 512, 512)`) — 定义特征编码器中每个1D卷积层的输入和输出通道数的整数元组。*conv_dim* 的长度定义了1D卷积层的数量。

+   `conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2, 2, 2, 2, 2, 2)`) — 定义特征编码器中每个1D卷积层的步幅的整数元组。*conv_stride* 的长度定义了卷积层的数量，并且必须与 *conv_dim* 的长度匹配。

+   `conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3, 3, 3, 3, 3, 3)`) — 定义特征编码器中每个1D卷积层的内核大小的整数元组。*conv_kernel* 的长度定义了卷积层的数量，并且必须与 *conv_dim* 的长度匹配。

+   `conv_bias` (`bool`, *optional*, defaults to `False`) — 1D卷积层是否带有偏置。

+   `num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — 卷积位置嵌入的数量。定义了1D卷积位置嵌入层的内核大小。

+   `num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — 1D卷积位置嵌入层的组数。

+   `do_stable_layer_norm` (`bool`, *optional*, defaults to `False`) — 是否应用 Transformer 编码器的 *stable* 层规范化架构。`do_stable_layer_norm is True` 表示在注意力层之前应用层规范化，而 `do_stable_layer_norm is False` 表示在注意力层之后应用层规范化。

+   `apply_spec_augment` (`bool`, *optional*, defaults to `True`) — 是否对特征编码器的输出应用 *SpecAugment* 数据增强。详情请参考 [SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779)。

+   `mask_time_prob` (`float`, *optional*, defaults to 0.05) — 沿时间轴的每个特征向量被选择为要屏蔽的向量跨度的起始的概率。大约会有 `mask_time_prob * sequence_length // mask_time_length` 个特征向量沿时间轴被屏蔽。仅在 `apply_spec_augment` 为真时相关。

+   `mask_time_length` (`int`, *optional*, defaults to 10) — 沿时间轴的向量跨度长度。

+   `mask_time_min_masks` (`int`, *optional*, defaults to 2), — 沿时间轴生成的长度为 `mask_feature_length` 的最小掩码数量，每个时间步，与 `mask_feature_prob` 无关。仅在 ”mask_time_prob*len(time_axis)/mask_time_length < mask_time_min_masks” 时相关。

+   `mask_feature_prob` (`float`, *optional*, defaults to 0.0) — 沿特征轴的每个特征向量被选择为要屏蔽的向量跨度的起始的概率。大约会有 `mask_time_prob * hidden_size // mask_time_length` 个特征向量沿时间轴被屏蔽。仅在 `apply_spec_augment` 为真时相关。

+   `mask_feature_length` (`int`, *optional*, defaults to 10) — 沿特征轴的向量跨度长度。

+   `num_codevectors_per_group` (`int`, *optional*, defaults to 320) — 每个量化码书（组）中的条目数。

+   `num_codevector_groups` (`int`, *optional*, defaults to 2) — 产品码矢量量化的码矢量组数。

+   `contrastive_logits_temperature` (`float`, *optional*, defaults to 0.1) — 对比损失中的温度 *kappa*。

+   `num_negatives` (`int`, *optional*, defaults to 100) — 对比损失的负样本数量。

+   `codevector_dim` (`int`, *optional*, defaults to 256) — 量化特征向量的维度。

+   `proj_codevector_dim` (`int`, *optional*, defaults to 256) — 量化和变换特征的最终投影的维度。

+   `diversity_loss_weight` (`int`, *optional*, defaults to 0.1) — 码书多样性损失组件的权重。

+   `ctc_loss_reduction` (`str`, *optional*, defaults to `"mean"`) — 指定应用于 `torch.nn.CTCLoss` 输出的减少方式。仅在训练 [WavLMForCTC](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMForCTC) 实例时相关。

+   `ctc_zero_infinity` (`bool`, *optional*, defaults to `False`) — 是否将 `torch.nn.CTCLoss` 的无限损失和相关梯度置零。当输入太短无法与目标对齐时主要会出现无限损失。仅在训练 [WavLMForCTC](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMForCTC) 实例时相关。

+   `use_weighted_layer_sum` (`bool`, *optional*, defaults to `False`) — 是否使用具有学习权重的层输出的加权平均。仅在使用 [WavLMForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMForSequenceClassification) 实例时相关。

+   `classifier_proj_size` (`int`, *optional*, defaults to 256) — 分类前的投影维度，用于标记均值池化。

+   `tdnn_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512, 512, 512, 1500)`) — *XVector* 模型中 *TDNN* 模块中每个一维卷积层的输出通道数的整数元组。*tdnn_dim* 的长度定义了 *TDNN* 层的数量。

+   `tdnn_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 3, 3, 1, 1)`) — *XVector* 模型中 *TDNN* 模块中每个一维卷积层的内核大小的整数元组。*tdnn_kernel* 的长度必须与 *tdnn_dim* 的长度相匹配。

+   `tdnn_dilation` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(1, 2, 3, 1, 1)`) — *XVector* 模型中 *TDNN* 模块中每个一维卷积层的膨胀因子的整数元组。*tdnn_dilation* 的长度必须与 *tdnn_dim* 的长度相匹配。

+   `xvector_output_dim`（`int`，*可选*，默认为512）— *XVector*嵌入向量的维度。

+   `add_adapter`（`bool`，*可选*，默认为`False`）— 是否应在Wav2Vec2编码器顶部堆叠卷积网络。对于Warm-starting Wav2Vec2用于SpeechEncoderDecoder模型非常有用。

+   `adapter_kernel_size`（`int`，*可选*，默认为3）— 适配器网络中卷积层的核大小。仅在`add_adapter`为True时相关。

+   `adapter_stride`（`int`，*可选*，默认为2）— 适配器网络中卷积层的步幅。仅在`add_adapter`为True时相关。

+   `num_adapter_layers`（`int`，*可选*，默认为3）— 适配器网络中应使用的卷积层的数量。仅在`add_adapter`为True时相关。

+   `output_hidden_size`（`int`，*可选*）— 编码器输出层的维度。如果未定义，则默认为*hidden-size*。仅在`add_adapter`为True时相关。

这是用于存储[WavLMModel](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMModel)配置的配置类。根据指定的参数实例化一个WavLM模型，定义模型架构。使用默认值实例化配置将产生类似于WavLM [microsoft/wavlm-base](https://huggingface.co/microsoft/wavlm-base)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

示例：

```py
>>> from transformers import WavLMConfig, WavLMModel

>>> # Initializing a WavLM facebook/wavlm-base-960h style configuration
>>> configuration = WavLMConfig()

>>> # Initializing a model (with random weights) from the facebook/wavlm-base-960h style configuration
>>> model = WavLMModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## WavLMModel

### `class transformers.WavLMModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wavlm/modeling_wavlm.py#L1094)

```py
( config: WavLMConfig )
```

参数

+   `config`（[WavLMConfig](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

WavLM模型转换器裸输出原始隐藏状态，没有任何特定的头部。WavLM是由Sanyuan Chen、Chengyi Wang、Zhengyang Chen、Yu Wu、Shujie Liu、Zhuo Chen、Jinyu Li、Naoyuki Kanda、Takuya Yoshioka、Xiong Xiao、Jian Wu、Long Zhou、Shuo Ren、Yanmin Qian、Yao Qian、Jian Wu、Michael Zeng、Xiangzhan Yu、Furu Wei在[《WavLM: 使用标记和未标记数据进行统一语音表示学习》](https://arxiv.org/abs/2110.13900)中提出的。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存等）。

此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wavlm/modeling_wavlm.py#L1185)

```py
( input_values: Optional attention_mask: Optional = None mask_time_indices: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.Wav2Vec2BaseModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_values` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 输入原始语音波形的浮点值。值可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得，例如通过soundfile库（`pip install soundfile`）。要准备好数组为`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。

+   `attention_mask` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于避免在填充标记索引上执行卷积和注意力的蒙版。蒙版值选择在`[0, 1]`中：

    +   对于未被蒙版的标记，为1，

    +   对于被蒙版的标记，为0。

    [注意力蒙版是什么？](../glossary#attention-mask)

    只有当相应的处理器具有`config.return_attention_mask == True`时才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask == False`的模型，当进行批量推断时，应避免传递`attention_mask`以避免性能下降。对于这样的模型，`input_values`应简单地填充为0并在不传递`attention_mask`的情况下传递。请注意，这些模型根据`input_values`是否填充会产生略有不同的结果。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)或`tuple(torch.FloatTensor)`

[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，这取决于配置（[WavLMConfig](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMConfig)）和输入。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层输出的隐藏状态序列。

+   `extract_features` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, conv_dim[-1])`) — 模型最后一个卷积层提取的特征向量序列。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。

[WavLMModel](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMModel)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, WavLMModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> processor = AutoProcessor.from_pretrained("patrickvonplaten/wavlm-libri-clean-100h-base-plus")
>>> model = WavLMModel.from_pretrained("patrickvonplaten/wavlm-libri-clean-100h-base-plus")

>>> # audio file is decoded on the fly
>>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 292, 768]
```

## WavLMForCTC

### `class transformers.WavLMForCTC`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wavlm/modeling_wavlm.py#L1246)

```py
( config target_lang: Optional = None )
```

参数

+   `config` ([WavLMConfig](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

WavLM 模型在 Connectionist Temporal Classification (CTC) 上具有 `语言建模` 头部。WavLM 是由 Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu, Furu Wei 在 [WavLM: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2110.13900) 中提出的。

此模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为其所有模型实现的通用方法（例如下载或保存等）。

此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 的子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wavlm/modeling_wavlm.py#L1323)

```py
( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_values` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length)`) — 输入原始语音波形的浮点值。值可以通过将 `.flac` 或 `.wav` 音频文件加载到类型为 `List[float]` 或 `numpy.ndarray` 的数组中获得，例如通过 soundfile 库（`pip install soundfile`）。要准备好数组以获得 `input_values`，应使用 [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor) 进行填充和转换为类型为 `torch.FloatTensor` 的张量。有关详细信息，请参阅 [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。

+   `attention_mask` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*可选*) — 用于避免在填充标记索引上执行卷积和注意力的掩码。选择在 `[0, 1]` 中的掩码值。

    +   对于未被 `masked` 的标记为 1，

    +   对于被 `masked` 的标记为 0。

    [什么是注意力掩码？](../glossary#attention-mask)

    `attention_mask` 只有在相应的处理器具有 `config.return_attention_mask == True` 时才应传递。对于所有处理器的 `config.return_attention_mask == False` 的模型，当进行批量推理时，应 `不` 传递 `attention_mask` 以避免性能下降。对于这种模型，`input_values` 应该简单地用 0 填充并在不传递 `attention_mask` 的情况下传递。请注意，这些模型根据 `input_values` 是否填充而产生略有不同的结果。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels`（形状为`(batch_size, target_length)`的`torch.LongTensor`，*可选*）— 用于连接主义时间分类的标签。请注意，`target_length`必须小于或等于输出logits的序列长度。索引在`[-100, 0, ..., config.vocab_size - 1]`中选择。所有设置为`-100`的标签都被忽略（掩码），损失仅计算在`[0, ..., config.vocab_size - 1]`中的标签。

返回

[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`，则根据配置（[WavLMConfig](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMConfig)）和输入包含各种元素。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 语言建模损失（用于下一个标记预测）。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有一个嵌入层，+ 一个用于每一层的输出）。

    每层模型输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[WavLMForCTC](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMForCTC)前向方法，覆盖`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行预处理和后处理步骤，而后者则会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, WavLMForCTC
>>> from datasets import load_dataset
>>> import torch

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> processor = AutoProcessor.from_pretrained("patrickvonplaten/wavlm-libri-clean-100h-base-plus")
>>> model = WavLMForCTC.from_pretrained("patrickvonplaten/wavlm-libri-clean-100h-base-plus")

>>> # audio file is decoded on the fly
>>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
>>> predicted_ids = torch.argmax(logits, dim=-1)

>>> # transcribe speech
>>> transcription = processor.batch_decode(predicted_ids)
>>> transcription[0]
'mister quilter is the aposle of the middle classes and we are glad to welcome his gospel'

>>> inputs["labels"] = processor(text=dataset[0]["text"], return_tensors="pt").input_ids

>>> # compute loss
>>> loss = model(**inputs).loss
>>> round(loss.item(), 2)
12.51
```

## WavLMForSequenceClassification

### `class transformers.WavLMForSequenceClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wavlm/modeling_wavlm.py#L1403)

```py
( config )
```

参数

+   `config`（[WavLMConfig](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

WavLM模型在顶部具有一个序列分类头（一个线性层在池化输出上方）用于类似SUPERB关键词检测的任务。

WavLM是由Sanyuan Chen、Chengyi Wang、Zhengyang Chen、Yu Wu、Shujie Liu、Zhuo Chen、Jinyu Li、Naoyuki Kanda、Takuya Yoshioka、Xiong Xiao、Jian Wu、Long Zhou、Shuo Ren、Yanmin Qian、Yao Qian、Jian Wu、Michael Zeng、Xiangzhan Yu、Furu Wei在[《WavLM: Unified Speech Representation Learning with Labeled and Unlabeled Data》](https://arxiv.org/abs/2110.13900)中提出的。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库实现的所有模型的通用方法（如下载或保存等）。

这个模型是PyTorch的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以了解所有与一般用法和行为相关的事项。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wavlm/modeling_wavlm.py#L1458)

```py
( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_values` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 输入原始语音波形的浮点值。值可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得，例如通过声音文件库（`pip install soundfile`）。为了准备数组为`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。

+   `attention_mask` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选在`[0, 1]`之间：

    +   对于未被`masked`的标记为1。

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

    只有当相应的处理器具有`config.return_attention_mask == True`时，才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask == False`的模型，当进行批量推断时，应避免传递`attention_mask`以避免性能下降。对于这些模型，`input_values`应简单地用0填充并在不带`attention_mask`的情况下传递。请注意，这些模型根据`input_values`是否填充会产生略有不同的结果。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size,)`，*可选*) — 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput) 或 `tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[WavLMConfig](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMConfig)）和输入的不同元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 分类（如果config.num_labels==1则为回归）损失。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果config.num_labels==1则为回归）得分（SoftMax之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在自注意力头中用于计算加权平均值的注意力softmax后的注意力权重。

[WavLMForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMForSequenceClassification)的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoFeatureExtractor, WavLMForSequenceClassification
>>> from datasets import load_dataset
>>> import torch

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("patrickvonplaten/wavlm-libri-clean-100h-base-plus")
>>> model = WavLMForSequenceClassification.from_pretrained("patrickvonplaten/wavlm-libri-clean-100h-base-plus")

>>> # audio file is decoded on the fly
>>> inputs = feature_extractor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_ids = torch.argmax(logits, dim=-1).item()
>>> predicted_label = model.config.id2label[predicted_class_ids]

>>> # compute loss - target_label is e.g. "down"
>>> target_label = model.config.id2label[0]
>>> inputs["labels"] = torch.tensor([model.config.label2id[target_label]])
>>> loss = model(**inputs).loss
```

## WavLMForAudioFrameClassification

### `class transformers.WavLMForAudioFrameClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wavlm/modeling_wavlm.py#L1528)

```py
( config )
```

参数

+   `config`（[WavLMConfig](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMConfig））-模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

WavLM模型在顶部带有帧分类头，用于说话人分离等任务。

WavLM是由Sanyuan Chen、Chengyi Wang、Zhengyang Chen、Yu Wu、Shujie Liu、Zhuo Chen、Jinyu Li、Naoyuki Kanda、Takuya Yoshioka、Xiong Xiao、Jian Wu、Long Zhou、Shuo Ren、Yanmin Qian、Yao Qian、Jian Wu、Michael Zeng、Xiangzhan Yu、Furu Wei在[《WavLM: 统一的带标记和未标记数据的语音表示学习》](https://arxiv.org/abs/2110.13900)中提出的。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存等）。

此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wavlm/modeling_wavlm.py#L1579)

```py
( input_values: Optional attention_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_values`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）-输入原始语音波形的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，例如通过soundfile库（`pip install soundfile`）。要将数组准备成`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选在`[0, 1]`之间：

    +   对于被“未掩码”的标记为1，

    +   对于被“掩码”的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

    只有当相应的处理器具有`config.return_attention_mask == True`时才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask == False`的模型，应避免传递`attention_mask`以避免在进行批量推断时性能下降。对于这样的模型，`input_values`应该简单地用0填充并在不传递`attention_mask`的情况下传递。请注意，这些模型根据`input_values`是否填充会产生略有不同的结果。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels` (`torch.LongTensor` of shape `(batch_size,)`, *可选*) — 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或`tuple(torch.FloatTensor)`

[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置([WavLMConfig](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMConfig))和输入的各种元素。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *可选*，当提供`labels`时返回) — 分类损失。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`) — 分类得分（SoftMax之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层，则为嵌入输出的输出 + 每层的输出）。

    模型在每一层输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[WavLMForAudioFrameClassification](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例而不是此函数，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoFeatureExtractor, WavLMForAudioFrameClassification
>>> from datasets import load_dataset
>>> import torch

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("microsoft/wavlm-base-plus-sd")
>>> model = WavLMForAudioFrameClassification.from_pretrained("microsoft/wavlm-base-plus-sd")

>>> # audio file is decoded on the fly
>>> inputs = feature_extractor(dataset[0]["audio"]["array"], return_tensors="pt", sampling_rate=sampling_rate)
>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> probabilities = torch.sigmoid(logits[0])
>>> # labels is a one-hot array of shape (num_frames, num_speakers)
>>> labels = (probabilities > 0.5).long()
>>> labels[0].tolist()
[0, 0]
```

## WavLMForXVector

### `class transformers.WavLMForXVector`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wavlm/modeling_wavlm.py#L1692)

```py
( config )
```

参数

+   `config` ([WavLMConfig](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

WavLM模型在顶部具有XVector特征提取头，用于说话者验证等任务。

WavLM是由Sanyuan Chen、Chengyi Wang、Zhengyang Chen、Yu Wu、Shujie Liu、Zhuo Chen、Jinyu Li、Naoyuki Kanda、Takuya Yoshioka、Xiong Xiao、Jian Wu、Long Zhou、Shuo Ren、Yanmin Qian、Yao Qian、Jian Wu、Michael Zeng、Xiangzhan Yu、Furu Wei提出的[WavLM: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2110.13900)。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存等）。

此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wavlm/modeling_wavlm.py#L1761)

```py
( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.XVectorOutput or tuple(torch.FloatTensor)
```

参数

+   `input_values` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 输入原始语音波形的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，例如通过soundfile库（`pip install soundfile`）。要准备数组为`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。

+   `attention_mask` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于避免在填充标记索引上执行卷积和注意力的遮罩。选择的遮罩值在`[0, 1]`中。

    +   对于未被遮罩的标记，

    +   对于被遮罩的标记为0。

    [什么是注意力遮罩？](../glossary#attention-mask)

    只有在相应的处理器具有`config.return_attention_mask == True`时才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask == False`的模型，应避免传递`attention_mask`以避免在进行批量推断时性能下降。对于这样的模型，`input_values`应简单地用0填充并在不传递`attention_mask`的情况下传递。请注意，这些模型根据`input_values`是否填充会产生略有不同的结果。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size,)`，*可选*) — 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`中。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.XVectorOutput) 或 `tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.XVectorOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包括根据配置([WavLMConfig](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMConfig))和输入而变化的各种元素。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *可选*, 当提供`labels`时返回) — 分类损失。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, config.xvector_output_dim)`) — AMSoftmax之前的分类隐藏状态。

+   `embeddings` (`torch.FloatTensor` of shape `(batch_size, config.xvector_output_dim)`) — 用于基于向量相似性检索的话语嵌入。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[WavLMForXVector](/docs/transformers/v4.37.2/en/model_doc/wavlm#transformers.WavLMForXVector)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传播的配方需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoFeatureExtractor, WavLMForXVector
>>> from datasets import load_dataset
>>> import torch

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("microsoft/wavlm-base-plus-sv")
>>> model = WavLMForXVector.from_pretrained("microsoft/wavlm-base-plus-sv")

>>> # audio file is decoded on the fly
>>> inputs = feature_extractor(
...     [d["array"] for d in dataset[:2]["audio"]], sampling_rate=sampling_rate, return_tensors="pt", padding=True
... )
>>> with torch.no_grad():
...     embeddings = model(**inputs).embeddings

>>> embeddings = torch.nn.functional.normalize(embeddings, dim=-1).cpu()

>>> # the resulting embeddings can be used for cosine similarity-based retrieval
>>> cosine_sim = torch.nn.CosineSimilarity(dim=-1)
>>> similarity = cosine_sim(embeddings[0], embeddings[1])
>>> threshold = 0.7  # the optimal threshold is dataset-dependent
>>> if similarity < threshold:
...     print("Speakers are not the same!")
>>> round(similarity.item(), 2)
0.97
```
