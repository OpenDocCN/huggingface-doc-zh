# xFormers

> 原文链接：[https://huggingface.co/docs/diffusers/optimization/xformers](https://huggingface.co/docs/diffusers/optimization/xformers)

我们推荐使用[xFormers](https://github.com/facebookresearch/xformers)进行推理和训练。在我们的测试中，注意力块中进行的优化使得速度更快，内存消耗更少。

从`pip`安装xFormers：

```py
pip install xformers
```

xFormers的`pip`包需要最新版本的PyTorch。如果您需要使用之前的PyTorch版本，我们建议从源代码中[安装xFormers](https://github.com/facebookresearch/xformers#installing-xformers)。

安装完xFormers后，您可以使用`enable_xformers_memory_efficient_attention()`来进行更快的推理和减少内存消耗，如本[部分](memory#memory-efficient-attention)所示。

根据这个[问题](https://github.com/huggingface/diffusers/issues/2234#issuecomment-1416931212)，xFormers `v0.0.16`在某些GPU上不能用于训练（微调或DreamBooth）。如果您遇到这个问题，请按照问题评论中的指示安装开发版本。
