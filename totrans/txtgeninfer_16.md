# é‡åŒ–

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/text-generation-inference/conceptual/quantization](https://huggingface.co/docs/text-generation-inference/conceptual/quantization)

TGIæä¾›äº†GPTQå’Œbits-and-bytesé‡åŒ–ï¼Œä»¥é‡åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ã€‚

## ä½¿ç”¨GPTQè¿›è¡Œé‡åŒ–

GPTQæ˜¯ä¸€ç§åè®­ç»ƒé‡åŒ–æ–¹æ³•ï¼Œå¯ä»¥ä½¿æ¨¡å‹æ›´å°ã€‚å®ƒé€šè¿‡æ‰¾åˆ°è¯¥æƒé‡çš„å‹ç¼©ç‰ˆæœ¬æ¥é‡åŒ–å±‚ï¼Œè¿™å°†äº§ç”Ÿä¸€ä¸ªæœ€å°å‡æ–¹è¯¯å·®ï¼Œå¦‚ä¸‹æ‰€ç¤ºğŸ‘‡

ç»™å®šä¸€ä¸ªå¸¦æœ‰æƒé‡çŸ©é˜µ<math><semantics><mrow><msub><mi>W</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">W_{l}</annotation></semantics></math>Wlå’Œå±‚è¾“å…¥<math><semantics><mrow><msub><mi>X</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">X_{l}</annotation></semantics></math>Xlçš„å±‚<math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math>lï¼Œæ‰¾åˆ°é‡åŒ–æƒé‡<math><semantics><mrow><mi>h</mi><mi>a</mi><mi>t</mi><msub><mi>W</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">\\hat{W}_{l}</annotation></semantics></math>hatWlï¼š <math display="block"><semantics><mrow><mo stretchy="false">(</mo><msup><msub><mover accent="true"><mi>W</mi><mo>^</mo></mover><mi>l</mi></msub><mo lspace="0em" rspace="0em">âˆ—</mo></msup><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>i</mi><msub><mi>n</mi><mover accent="true"><msub><mi>W</mi><mi>l</mi></msub><mo>^</mo></mover></msub><mi mathvariant="normal">âˆ£</mi><mi mathvariant="normal">âˆ£</mi><msub><mi>W</mi><mi>l</mi></msub><mi>X</mi><mo>âˆ’</mo><msub><mover accent="true"><mi>W</mi><mo>^</mo></mover><mi>l</mi></msub><mi>X</mi><mi mathvariant="normal">âˆ£</mi><msubsup><mi mathvariant="normal">âˆ£</mi><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">({\hat{W}_{l}}^{*} = argmin_{\hat{W_{l}}} ||W_{l}X-\hat{W}_{l}X||^{2}_{2})</annotation></semantics></math>(W^lâ€‹âˆ—=argminWlâ€‹^â€‹â€‹âˆ£âˆ£Wlâ€‹Xâˆ’W^lâ€‹Xâˆ£âˆ£22â€‹)

TGIå…è®¸æ‚¨è¿è¡Œå·²ç»GPTQé‡åŒ–çš„æ¨¡å‹ï¼ˆæŸ¥çœ‹å¯ç”¨æ¨¡å‹[æ­¤å¤„](https://huggingface.co/models?search=gptq)ï¼‰æˆ–ä½¿ç”¨é‡åŒ–è„šæœ¬é‡åŒ–æ‚¨é€‰æ‹©çš„æ¨¡å‹ã€‚æ‚¨å¯ä»¥é€šè¿‡ç®€å•åœ°ä¼ é€’â€”quantizeæ¥è¿è¡Œä¸€ä¸ªé‡åŒ–æ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºğŸ‘‡

```py
docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id $model --quantize gptq
```

è¯·æ³¨æ„ï¼ŒTGIçš„GPTQå®ç°ä¸ä½¿ç”¨[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)ä½œä¸ºåº•å±‚ã€‚ä½†æ˜¯ï¼Œä½¿ç”¨AutoGPTQæˆ–Optimumé‡åŒ–çš„æ¨¡å‹ä»ç„¶å¯ä»¥è¢«TGIæä¾›æœåŠ¡ã€‚

è¦ä½¿ç”¨GPTQå¯¹ç»™å®šæ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œåªéœ€è¿è¡Œ

```py
text-generation-server quantize tiiuae/falcon-40b /data/falcon-40b-gptq
# Add --upload-to-model-id MYUSERNAME/falcon-40b to push the created model to the hub directly
```

è¿™å°†åˆ›å»ºä¸€ä¸ªåŒ…å«é‡åŒ–æ–‡ä»¶çš„æ–°ç›®å½•ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ“ä½œï¼Œ

```py
text-generation-launcher --model-id /data/falcon-40b-gptq/ --sharded true --num-shard 2 --quantize gptq
```

æ‚¨å¯ä»¥é€šè¿‡è¿è¡Œ`text-generation-server quantize --help`æ¥äº†è§£æ›´å¤šæœ‰å…³é‡åŒ–é€‰é¡¹çš„ä¿¡æ¯ã€‚

å¦‚æœæ‚¨å¸Œæœ›å¯¹GPTQæ¨¡å‹è¿›è¡Œæ›´å¤šæ“ä½œï¼ˆä¾‹å¦‚åœ¨é¡¶éƒ¨è®­ç»ƒä¸€ä¸ªé€‚é…å™¨ï¼‰ï¼Œæ‚¨å¯ä»¥é˜…è¯»æœ‰å…³transformers GPTQé›†æˆçš„ä¿¡æ¯[æ­¤å¤„](https://huggingface.co/blog/gptq-integration)ã€‚æ‚¨å¯ä»¥ä»[è®ºæ–‡](https://arxiv.org/pdf/2210.17323.pdf)ä¸­äº†è§£æ›´å¤šå…³äºGPTQçš„ä¿¡æ¯ã€‚

## ä½¿ç”¨bitsandbytesè¿›è¡Œé‡åŒ–

bitsandbytesæ˜¯ä¸€ä¸ªç”¨äºå°†8ä½å’Œ4ä½é‡åŒ–åº”ç”¨äºæ¨¡å‹çš„åº“ã€‚ä¸GPTQé‡åŒ–ä¸åŒï¼Œbitsandbytesä¸éœ€è¦æ ¡å‡†æ•°æ®é›†æˆ–ä»»ä½•åå¤„ç†-æƒé‡ä¼šåœ¨åŠ è½½æ—¶è‡ªåŠ¨é‡åŒ–ã€‚ä½†æ˜¯ï¼Œä½¿ç”¨bitsandbytesè¿›è¡Œæ¨æ–­æ¯”GPTQæˆ–FP16ç²¾åº¦è¦æ…¢ã€‚

8ä½é‡åŒ–ä½¿å¾—å¤šåäº¿å‚æ•°è§„æ¨¡çš„æ¨¡å‹å¯ä»¥é€‚åº”æ›´å°çš„ç¡¬ä»¶è€Œä¸ä¼šå¤ªå¤§ç¨‹åº¦åœ°é™ä½æ€§èƒ½ã€‚åœ¨TGIä¸­ï¼Œæ‚¨å¯ä»¥é€šè¿‡æ·»åŠ `--quantize bitsandbytes`æ¥ä½¿ç”¨8ä½é‡åŒ–ï¼Œå¦‚ä¸‹æ‰€ç¤ºğŸ‘‡

```py
docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id $model --quantize bitsandbytes
```

4ä½é‡åŒ–ä¹Ÿå¯ä»¥ä½¿ç”¨bitsandbytesã€‚æ‚¨å¯ä»¥é€‰æ‹©ä»¥ä¸‹4ä½æ•°æ®ç±»å‹ä¹‹ä¸€ï¼š4ä½æµ®ç‚¹æ•°ï¼ˆ`fp4`ï¼‰æˆ–4ä½`NormalFloat`ï¼ˆ`nf4`ï¼‰ã€‚è¿™äº›æ•°æ®ç±»å‹æ˜¯åœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒçš„èƒŒæ™¯ä¸‹å¼•å…¥çš„ï¼Œä½†æ‚¨å¯ä»¥é€šè¿‡åœ¨åŠ è½½æ—¶è‡ªåŠ¨è½¬æ¢æ¨¡å‹æƒé‡æ¥å°†å…¶åº”ç”¨äºæ¨æ–­ã€‚

åœ¨ TGI ä¸­ï¼Œæ‚¨å¯ä»¥é€šè¿‡æ·»åŠ  `--quantize bitsandbytes-nf4` æˆ– `--quantize bitsandbytes-fp4` æ¥ä½¿ç”¨ 4 ä½é‡åŒ–ï¼Œå°±åƒä¸‹é¢è¿™æ ·ğŸ‘‡

```py
docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id $model --quantize bitsandbytes-nf4
```

æ‚¨å¯ä»¥é€šè¿‡é˜…è¯»è¿™ç¯‡[åšå®¢æ–‡ç« ](https://huggingface.co/blog/hf-bitsandbytes-integration)äº†è§£æ›´å¤šå…³äº 8 ä½é‡åŒ–çš„ä¿¡æ¯ï¼Œä»¥åŠé€šè¿‡é˜…è¯»[è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/4bit-transformers-bitsandbytes)äº†è§£ 4 ä½é‡åŒ–ã€‚
