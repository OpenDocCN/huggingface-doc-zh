# 使用🤗 Tokenizers 中的分词器

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/fast_tokenizers`](https://huggingface.co/docs/transformers/v4.37.2/en/fast_tokenizers)

PreTrainedTokenizerFast 依赖于 [🤗 Tokenizers](https://huggingface.co/docs/tokenizers) 库。从🤗 Tokenizers 库获得的分词器可以非常简单地加载到🤗 Transformers 中。

在进入具体内容之前，让我们首先通过几行代码创建一个虚拟的分词器：

```py
>>> from tokenizers import Tokenizer
>>> from tokenizers.models import BPE
>>> from tokenizers.trainers import BpeTrainer
>>> from tokenizers.pre_tokenizers import Whitespace

>>> tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
>>> trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

>>> tokenizer.pre_tokenizer = Whitespace()
>>> files = [...]
>>> tokenizer.train(files, trainer)
```

我们现在有一个在我们定义的文件上训练过的分词器。我们可以继续在该运行时中使用它，或者将其保存到一个 JSON 文件中以供将来重复使用。

## 直接从分词器对象加载

让我们看看如何在🤗 Transformers 库中利用这个分词器对象。PreTrainedTokenizerFast 类允许通过接受实例化的 *tokenizer* 对象作为参数来轻松实例化：

```py
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
```

这个对象现在可以与🤗 Transformers 分词器共享的所有方法一起使用！请前往分词器页面获取更多信息。

## 从一个 JSON 文件加载

为了从一个 JSON 文件中加载一个分词器，让我们首先保存我们的分词器：

```py
>>> tokenizer.save("tokenizer.json")
```

我们保存这个文件的路径可以通过 `tokenizer_file` 参数传递给 PreTrainedTokenizerFast 初始化方法：

```py
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")
```

这个对象现在可以与🤗 Transformers 分词器共享的所有方法一起使用！请前往分词器页面获取更多信息。
