["```py\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-moe-54b\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-moe-54b\")\n\n>>> article = \"Previously, Ring's CEO, Jamie Siminoff, remarked the company started when his doorbell wasn't audible from his shop in his garage.\"\n>>> inputs = tokenizer(article, return_tensors=\"pt\")\n\n>>> translated_tokens = model.generate(\n...     **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"fra_Latn\"], max_length=50\n... )\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n\"Auparavant, le PDG de Ring, Jamie Siminoff, a fait remarquer que la soci\u00e9t\u00e9 avait commenc\u00e9 lorsque sa sonnette n'\u00e9tait pas audible depuis son magasin dans son garage.\"\n```", "```py\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-moe-54b\", src_lang=\"ron_Latn\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-moe-54b\")\n\n>>> article = \"\u015eeful ONU spune c\u0103 nu exist\u0103 o solu\u0163ie militar\u0103 \u00een Siria\"\n>>> inputs = tokenizer(article, return_tensors=\"pt\")\n\n>>> translated_tokens = model.generate(\n...     **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"deu_Latn\"], max_length=30\n... )\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n```", "```py\n( vocab_size = 128112 max_position_embeddings = 1024 encoder_layers = 12 encoder_ffn_dim = 4096 encoder_attention_heads = 16 decoder_layers = 12 decoder_ffn_dim = 4096 decoder_attention_heads = 16 encoder_layerdrop = 0.05 decoder_layerdrop = 0.05 use_cache = True is_encoder_decoder = True activation_function = 'relu' d_model = 1024 dropout = 0.1 attention_dropout = 0.1 activation_dropout = 0.0 init_std = 0.02 decoder_start_token_id = 2 scale_embedding = True router_bias = False router_dtype = 'float32' router_ignore_padding_tokens = False num_experts = 128 expert_capacity = 64 encoder_sparse_step = 4 decoder_sparse_step = 4 router_z_loss_coef = 0.001 router_aux_loss_coef = 0.001 second_expert_policy = 'all' normalize_router_prob_before_dropping = False batch_prioritized_routing = False moe_eval_capacity_token_fraction = 1.0 moe_token_dropout = 0.2 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 output_router_logits = False **kwargs )\n```", "```py\n>>> from transformers import NllbMoeModel, NllbMoeConfig\n\n>>> # Initializing a NllbMoe facebook/nllb-moe-54b style configuration\n>>> configuration = NllbMoeConfig()\n\n>>> # Initializing a model from the facebook/nllb-moe-54b style configuration\n>>> model = NllbMoeModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config: NllbMoeConfig )\n```", "```py\n( router_logits: Tensor input_dtype: dtype = torch.float32 padding_mask: Optional = None )\n```", "```py\n( hidden_states: Tensor padding_mask: Optional = None ) \u2192 export const metadata = 'undefined';top_1_mask (torch.Tensor of shape (batch_size, sequence_length))\n```", "```py\n( config: NllbMoeConfig ffn_dim: int expert_class: Module = <class 'transformers.models.nllb_moe.modeling_nllb_moe.NllbMoeDenseActDense'> )\n```", "```py\n( hidden_states: Tensor padding_mask: Optional = False ) \u2192 export const metadata = 'undefined';hidden_states (torch.Tensor of shape (batch_size, sequence_length, hidden_dim))\n```", "```py\n( config: NllbMoeConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None output_router_logits: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqMoEModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, NllbMoeModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/random-nllb-moe-2-experts\")\n>>> model = SwitchTransformersModel.from_pretrained(\"hf-internal-testing/random-nllb-moe-2-experts\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n\n>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for NllbMoeModel\n>>> decoder_input_ids = model._shift_right(decoder_input_ids)\n\n>>> # forward pass\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: NllbMoeConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None output_router_logits: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqMoEOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, NllbMoeForConditionalGeneration\n\n>>> model = NllbMoeForConditionalGeneration.from_pretrained(\"facebook/nllb-moe-54b\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-moe-54b\")\n\n>>> text_to_translate = \"Life is like a box of chocolates\"\n>>> model_inputs = tokenizer(text_to_translate, return_tensors=\"pt\")\n\n>>> # translate to French\n>>> gen_tokens = model.generate(**model_inputs, forced_bos_token_id=tokenizer.get_lang_id(\"eng_Latn\"))\n>>> print(tokenizer.batch_decode(gen_tokens, skip_special_tokens=True))\n```"]