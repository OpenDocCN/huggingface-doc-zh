["```py\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-moe-54b\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-moe-54b\")\n\n>>> article = \"Previously, Ring's CEO, Jamie Siminoff, remarked the company started when his doorbell wasn't audible from his shop in his garage.\"\n>>> inputs = tokenizer(article, return_tensors=\"pt\")\n\n>>> translated_tokens = model.generate(\n...     **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"fra_Latn\"], max_length=50\n... )\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n\"Auparavant, le PDG de Ring, Jamie Siminoff, a fait remarquer que la soci\u00e9t\u00e9 avait commenc\u00e9 lorsque sa sonnette n'\u00e9tait pas audible depuis son magasin dans son garage.\"\n```", "```py\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-moe-54b\", src_lang=\"ron_Latn\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-moe-54b\")\n\n>>> article = \"\u015eeful ONU spune c\u0103 nu exist\u0103 o solu\u0163ie militar\u0103 \u00een Siria\"\n>>> inputs = tokenizer(article, return_tensors=\"pt\")\n\n>>> translated_tokens = model.generate(\n...     **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"deu_Latn\"], max_length=30\n... )\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n```", "```py\n>>> from transformers import NllbMoeModel, NllbMoeConfig\n\n>>> # Initializing a NllbMoe facebook/nllb-moe-54b style configuration\n>>> configuration = NllbMoeConfig()\n\n>>> # Initializing a model from the facebook/nllb-moe-54b style configuration\n>>> model = NllbMoeModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoTokenizer, NllbMoeModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/random-nllb-moe-2-experts\")\n>>> model = SwitchTransformersModel.from_pretrained(\"hf-internal-testing/random-nllb-moe-2-experts\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n\n>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for NllbMoeModel\n>>> decoder_input_ids = model._shift_right(decoder_input_ids)\n\n>>> # forward pass\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, NllbMoeForConditionalGeneration\n\n>>> model = NllbMoeForConditionalGeneration.from_pretrained(\"facebook/nllb-moe-54b\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-moe-54b\")\n\n>>> text_to_translate = \"Life is like a box of chocolates\"\n>>> model_inputs = tokenizer(text_to_translate, return_tensors=\"pt\")\n\n>>> # translate to French\n>>> gen_tokens = model.generate(**model_inputs, forced_bos_token_id=tokenizer.get_lang_id(\"eng_Latn\"))\n>>> print(tokenizer.batch_decode(gen_tokens, skip_special_tokens=True))\n```"]