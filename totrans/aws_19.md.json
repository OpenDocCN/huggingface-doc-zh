["```py\nfrom optimum.neuron import NeuronTrainingArguments, NeuronTrainer\n\n# To enable ZeRO-1, set the `zero_1` argument to `True` in the training arguments.\ntraining_args = NeuronTrainingArguments(\n    ...,\n    zero_1=True,\n)\n\ntrainer = NeuronTrainer(\n    model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n\ntrainer.train()\n```", "```py\ntorchrun --nproc_per_node=2 examples/language-modeling/run_clm.py \\\n    --model_name_or_path TinyLlama/TinyLlama-1.1B-Chat-v0.6 \\\n    --dataset_name wikitext \\\n    --dataset_config_name wikitext-2-raw-v1 \\\n    --do_train \\\n    --per_device_train_batch_size 1 \\\n    --block_size 1024 \\\n    --bf16 \\\n    --zero_1 \\\n    --output_dir my_training/\n\n```", "```py\nfrom torch.optim import AdamW\nfrom optimum.neuron.distributed import make_optimizer_constructor_lazy\n\nlazy_adamw = make_optimizer_constructor_lazy(AdamW)\n```", "```py\naccelerator = NeuronAccelerator(\n    ...\n    zero_1=True,\n)\n\nmodel = ...\nlazy_optimizer = lazy_adamw(...) # Actually instantiate the optimizer.\n\nmodel, optimizer = accelerator.prepare(model, lazy_optimizer)\n```", "```py\nfrom optimum.neuron import NeuronTrainingArguments, NeuronTrainer\nfrom optimum.neuron.distributed import lazy_load_for_parallelism\n\n# Specify the `tensor_parallel_size` in the training arguments.\ntraining_args = NeuronTrainingArguments(\n    ...,\n    tensor_parallel_size=8,\n    disable_embedding_parallelization=False, # It is `False` by default.\n    disable_sequence_parallel=False, # It is `False` by default.\n)\n\nwith lazy_load_for_parallelism(tensor_parallel_size=training_args.tensor_parallel_size):\n    model = ...\n\ntrainer = NeuronTrainer(\n    model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n\ntrainer.train()\n```", "```py\ntorchrun --nproc_per_node=2 examples/language-modeling/run_clm.py \\\n    --model_name_or_path TinyLlama/TinyLlama-1.1B-Chat-v0.6 \\\n    --dataset_name wikitext \\\n    --dataset_config_name wikitext-2-raw-v1 \\\n    --do_train \\\n    --per_device_train_batch_size 1 \\\n    --block_size 1024 \\\n    --bf16 \\\n    --tensor_parallel_size 2 \\\n    --output_dir my_training/\n```", "```py\nfrom torch.optim import AdamW\nfrom optimum.neuron import NeuronAccelerator\nfrom optimum.neuron.accelerate.utils import ModelParallelismPlugin\nfrom optimum.neuron.distributed import lazy_load_for_parallelism\n\ntensor_parallel_size = 8\nmp_plugin = ModelParallelismPlugin(\n    tensor_parallel_size,\n    parallelize_embeddings=True,\n    sequence_parallel_enabled=True,\n    checkpoint_dir=None, # Can be specified when resuming from checkpoint.\n)\n\naccelerator = NeuronAccelerator(\n    ...\n    mp_plugin=mp_plugin,\n)\n\nwith lazy_load_for_parallelism(tensor_parallel_size=tensor_parallel_size):\n    model = ...\n\nlazy_adamw = make_optimizer_constructor_lazy(AdamW)\nlazy_optimizer = lazy_adamw(...) # Actually instantiate the optimizer.\n\nmodel, optimizer = accelerator.prepare(model, lazy_optimizer)\n```", "```py\noptimum-cli neuron consolidate --help\n\nusage: optimum-cli neuron consolidate [-h] [-f {pytorch,safetensors}] checkpoint_dir output_dir\n\npositional arguments:\n  checkpoint_dir        The path to the directory containing the checkpoints.\n  output_dir            The path to the output directory containing the consolidated checkpoint.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -f {pytorch,safetensors}, --format {pytorch,safetensors}\n                        The format used to save the consolidated checkpoint.\n\n```", "```py\nmy_training/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 all_results.json \n\u251c\u2500\u2500 checkpoint-10 \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 config.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 scheduler.pt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 special_tokens_map.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tensor_parallel_shards\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tokenizer.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tokenizer.model\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tokenizer_config.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 trainer_state.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 training_args.bin\n\u251c\u2500\u2500 config.json\n\u251c\u2500\u2500 special_tokens_map.json\n\u251c\u2500\u2500 tensor_parallel_shards\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tp_rank_00_pp_rank_00\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tp_rank_01_pp_rank_00\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tp_rank_02_pp_rank_00\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tp_rank_03_pp_rank_00\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tp_rank_04_pp_rank_00\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tp_rank_05_pp_rank_00\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tp_rank_06_pp_rank_00\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tp_rank_07_pp_rank_00\n\u251c\u2500\u2500 tokenizer.json\n\u251c\u2500\u2500 tokenizer.model\n\u251c\u2500\u2500 tokenizer_config.json\n\u251c\u2500\u2500 train_results.json\n\u251c\u2500\u2500 trainer_state.json\n\u2514\u2500\u2500 training_args.bin\n```", "```py\noptimum-cli neuron consolidate my_training my_training_consolidated_checkpoint\n```"]