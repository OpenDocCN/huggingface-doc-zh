# 激活函数

> 原文链接: [https://huggingface.co/docs/diffusers/api/activations](https://huggingface.co/docs/diffusers/api/activations)

为🤗 Diffusers中支持各种模型的自定义激活函数。

## GELU

### `class diffusers.models.activations.GELU`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/activations.py#L50)

```py
( dim_in: int dim_out: int approximate: str = 'none' bias: bool = True )
```

参数

+   `dim_in` (`int`) — 输入通道的数量。

+   `dim_out` (`int`) — 输出通道的数量。

+   `approximate` (`str`, *可选*, 默认为`"none"`) — 如果为`"tanh"`，则使用tanh近似。

+   `bias` (`bool`, 默认为True) — 是否在线性层中使用偏置。

具有tanh近似支持的GELU激活函数，使用`approximate="tanh"`。

## GEGLU

### `class diffusers.models.activations.GEGLU`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/activations.py#L78)

```py
( dim_in: int dim_out: int bias: bool = True )
```

参数

+   `dim_in` (`int`) — 输入通道的数量。

+   `dim_out` (`int`) — 输出通道的数量。

+   `bias` (`bool`, 默认为True) — 是否在线性层中使用偏置。

门控线性单元激活函数的一种变体。

## ApproximateGELU

### `class diffusers.models.activations.ApproximateGELU`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/activations.py#L106)

```py
( dim_in: int dim_out: int bias: bool = True )
```

参数

+   `dim_in` (`int`) — 输入通道的数量。

+   `dim_out` (`int`) — 输出通道的数量。

+   `bias` (`bool`, 默认为True) — 是否在线性层中使用偏置。

高斯误差线性单元（GELU）的近似形式。更多细节请参见这篇[论文](https://arxiv.org/abs/1606.08415)的第2节。
