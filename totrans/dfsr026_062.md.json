["```py\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```", "```py\ncd examples/controlnet\npip install -r requirements.txt\n```", "```py\naccelerate config\n```", "```py\naccelerate config default\n```", "```py\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```", "```py\naccelerate launch train_controlnet.py \\\n  --mixed_precision=\"fp16\"\n```", "```py\naccelerate launch train_controlnet.py \\\n  --snr_gamma=5.0\n```", "```py\nconditioning_image_transforms = transforms.Compose(\n    [\n        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n        transforms.CenterCrop(args.resolution),\n        transforms.ToTensor(),\n    ]\n)\n```", "```py\nif args.controlnet_model_name_or_path:\n    logger.info(\"Loading existing controlnet weights\")\n    controlnet = ControlNetModel.from_pretrained(args.controlnet_model_name_or_path)\nelse:\n    logger.info(\"Initializing controlnet weights from unet\")\n    controlnet = ControlNetModel.from_unet(unet)\n```", "```py\nparams_to_optimize = controlnet.parameters()\noptimizer = optimizer_class(\n    params_to_optimize,\n    lr=args.learning_rate,\n    betas=(args.adam_beta1, args.adam_beta2),\n    weight_decay=args.adam_weight_decay,\n    eps=args.adam_epsilon,\n)\n```", "```py\nencoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\ncontrolnet_image = batch[\"conditioning_pixel_values\"].to(dtype=weight_dtype)\n\ndown_block_res_samples, mid_block_res_sample = controlnet(\n    noisy_latents,\n    timesteps,\n    encoder_hidden_states=encoder_hidden_states,\n    controlnet_cond=controlnet_image,\n    return_dict=False,\n)\n```", "```py\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png\n```", "```py\npip install bitsandbytes\n```", "```py\naccelerate launch train_controlnet.py \\\n  --gradient_checkpointing \\\n  --use_8bit_adam \\\n```", "```py\nexport MODEL_DIR=\"runwayml/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"path/to/save/model\"\n\naccelerate launch train_controlnet.py \\\n --pretrained_model_name_or_path=$MODEL_DIR \\\n --output_dir=$OUTPUT_DIR \\\n --dataset_name=fusing/fill50k \\\n --resolution=512 \\\n --learning_rate=1e-5 \\\n --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n --train_batch_size=1 \\\n --gradient_accumulation_steps=4 \\\n --push_to_hub\n```", "```py\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\nfrom diffusers.utils import load_image\nimport torch\n\ncontrolnet = ControlNetModel.from_pretrained(\"path/to/controlnet\", torch_dtype=torch.float16)\npipeline = StableDiffusionControlNetPipeline.from_pretrained(\n    \"path/to/base/model\", controlnet=controlnet, torch_dtype=torch.float16\n).to(\"cuda\")\n\ncontrol_image = load_image(\"./conditioning_image_1.png\")\nprompt = \"pale golden rod circle with old lace background\"\n\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=20, generator=generator, image=control_image).images[0]\nimage.save(\"./output.png\")\n```"]