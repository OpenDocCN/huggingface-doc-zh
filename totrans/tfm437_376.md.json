["```py\n>>> from transformers import ViTConfig, BertConfig, VisionTextDualEncoderConfig, VisionTextDualEncoderModel\n\n>>> # Initializing a BERT and ViT configuration\n>>> config_vision = ViTConfig()\n>>> config_text = BertConfig()\n\n>>> config = VisionTextDualEncoderConfig.from_vision_text_configs(config_vision, config_text, projection_dim=512)\n\n>>> # Initializing a BERT and ViT model (with random weights)\n>>> model = VisionTextDualEncoderModel(config=config)\n\n>>> # Accessing the model configuration\n>>> config_vision = model.config.vision_config\n>>> config_text = model.config.text_config\n\n>>> # Saving the model, including its configuration\n>>> model.save_pretrained(\"vit-bert\")\n\n>>> # loading model and config from pretrained folder\n>>> vision_text_config = VisionTextDualEncoderConfig.from_pretrained(\"vit-bert\")\n>>> model = VisionTextDualEncoderModel.from_pretrained(\"vit-bert\", config=vision_text_config)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import (\n...     VisionTextDualEncoderModel,\n...     VisionTextDualEncoderProcessor,\n...     AutoImageProcessor,\n...     AutoTokenizer,\n... )\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n>>> processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n>>> model = VisionTextDualEncoderModel.from_vision_text_pretrained(\n...     \"google/vit-base-patch16-224\", \"bert-base-uncased\"\n... )\n\n>>> # contrastive training\n>>> urls = [\n...     \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n...     \"https://farm3.staticflickr.com/2674/5850229113_4fe05d5265_z.jpg\",\n... ]\n>>> images = [Image.open(requests.get(url, stream=True).raw) for url in urls]\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=images, return_tensors=\"pt\", padding=True\n... )\n>>> outputs = model(\n...     input_ids=inputs.input_ids,\n...     attention_mask=inputs.attention_mask,\n...     pixel_values=inputs.pixel_values,\n...     return_loss=True,\n... )\n>>> loss, logits_per_image = outputs.loss, outputs.logits_per_image  # this is the image-text similarity score\n\n>>> # save and load from pretrained\n>>> model.save_pretrained(\"vit-bert\")\n>>> model = VisionTextDualEncoderModel.from_pretrained(\"vit-bert\")\n\n>>> # inference\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> import jax\n>>> from transformers import (\n...     FlaxVisionTextDualEncoderModel,\n...     VisionTextDualEncoderProcessor,\n...     AutoImageProcessor,\n...     AutoTokenizer,\n... )\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> image_processor = AutoImageProcesor.from_pretrained(\"google/vit-base-patch16-224\")\n>>> processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n>>> model = FlaxVisionTextDualEncoderModel.from_vision_text_pretrained(\n...     \"google/vit-base-patch16-224\", \"bert-base-uncased\"\n... )\n\n>>> # contrastive training\n>>> urls = [\n...     \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n...     \"https://farm3.staticflickr.com/2674/5850229113_4fe05d5265_z.jpg\",\n... ]\n>>> images = [Image.open(requests.get(url, stream=True).raw) for url in urls]\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=images, return_tensors=\"np\", padding=True\n... )\n>>> outputs = model(\n...     input_ids=inputs.input_ids,\n...     attention_mask=inputs.attention_mask,\n...     pixel_values=inputs.pixel_values,\n... )\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n\n>>> # save and load from pretrained\n>>> model.save_pretrained(\"vit-bert\")\n>>> model = FlaxVisionTextDualEncoderModel.from_pretrained(\"vit-bert\")\n\n>>> # inference\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = jax.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import (\n...     TFVisionTextDualEncoderModel,\n...     VisionTextDualEncoderProcessor,\n...     AutoImageProcessor,\n...     AutoTokenizer,\n... )\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n>>> processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n>>> model = TFVisionTextDualEncoderModel.from_vision_text_pretrained(\n...     \"google/vit-base-patch16-224\", \"bert-base-uncased\"\n... )\n\n>>> # contrastive training\n>>> urls = [\n...     \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n...     \"https://farm3.staticflickr.com/2674/5850229113_4fe05d5265_z.jpg\",\n... ]\n>>> images = [Image.open(requests.get(url, stream=True).raw) for url in urls]\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=images, return_tensors=\"np\", padding=True\n... )\n>>> outputs = model(\n...     input_ids=inputs.input_ids,\n...     attention_mask=inputs.attention_mask,\n...     pixel_values=inputs.pixel_values,\n...     return_loss=True,\n... )\n>>> loss, logits_per_image = outputs.loss, outputs.logits_per_image  # this is the image-text similarity score\n\n>>> # save and load from pretrained\n>>> model.save_pretrained(\"vit-bert\")\n>>> model = TFVisionTextDualEncoderModel.from_pretrained(\"vit-bert\")\n\n>>> # inference\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n```"]