["```py\n( projection_dim = 512 logit_scale_init_value = 2.6592 **kwargs )\n```", "```py\n>>> from transformers import ViTConfig, BertConfig, VisionTextDualEncoderConfig, VisionTextDualEncoderModel\n\n>>> # Initializing a BERT and ViT configuration\n>>> config_vision = ViTConfig()\n>>> config_text = BertConfig()\n\n>>> config = VisionTextDualEncoderConfig.from_vision_text_configs(config_vision, config_text, projection_dim=512)\n\n>>> # Initializing a BERT and ViT model (with random weights)\n>>> model = VisionTextDualEncoderModel(config=config)\n\n>>> # Accessing the model configuration\n>>> config_vision = model.config.vision_config\n>>> config_text = model.config.text_config\n\n>>> # Saving the model, including its configuration\n>>> model.save_pretrained(\"vit-bert\")\n\n>>> # loading model and config from pretrained folder\n>>> vision_text_config = VisionTextDualEncoderConfig.from_pretrained(\"vit-bert\")\n>>> model = VisionTextDualEncoderModel.from_pretrained(\"vit-bert\", config=vision_text_config)\n```", "```py\n( vision_config: PretrainedConfig text_config: PretrainedConfig **kwargs ) \u2192 export const metadata = 'undefined';VisionTextDualEncoderConfig\n```", "```py\n( image_processor = None tokenizer = None **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config: Optional = None vision_model: Optional = None text_model: Optional = None )\n```", "```py\n( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None position_ids: Optional = None return_loss: Optional = None token_type_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.clip.modeling_clip.CLIPOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import (\n...     VisionTextDualEncoderModel,\n...     VisionTextDualEncoderProcessor,\n...     AutoImageProcessor,\n...     AutoTokenizer,\n... )\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n>>> processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n>>> model = VisionTextDualEncoderModel.from_vision_text_pretrained(\n...     \"google/vit-base-patch16-224\", \"bert-base-uncased\"\n... )\n\n>>> # contrastive training\n>>> urls = [\n...     \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n...     \"https://farm3.staticflickr.com/2674/5850229113_4fe05d5265_z.jpg\",\n... ]\n>>> images = [Image.open(requests.get(url, stream=True).raw) for url in urls]\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=images, return_tensors=\"pt\", padding=True\n... )\n>>> outputs = model(\n...     input_ids=inputs.input_ids,\n...     attention_mask=inputs.attention_mask,\n...     pixel_values=inputs.pixel_values,\n...     return_loss=True,\n... )\n>>> loss, logits_per_image = outputs.loss, outputs.logits_per_image  # this is the image-text similarity score\n\n>>> # save and load from pretrained\n>>> model.save_pretrained(\"vit-bert\")\n>>> model = VisionTextDualEncoderModel.from_pretrained(\"vit-bert\")\n\n>>> # inference\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n( config: VisionTextDualEncoderConfig input_shape: Optional = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids pixel_values attention_mask = None position_ids = None token_type_ids = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> import jax\n>>> from transformers import (\n...     FlaxVisionTextDualEncoderModel,\n...     VisionTextDualEncoderProcessor,\n...     AutoImageProcessor,\n...     AutoTokenizer,\n... )\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> image_processor = AutoImageProcesor.from_pretrained(\"google/vit-base-patch16-224\")\n>>> processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n>>> model = FlaxVisionTextDualEncoderModel.from_vision_text_pretrained(\n...     \"google/vit-base-patch16-224\", \"bert-base-uncased\"\n... )\n\n>>> # contrastive training\n>>> urls = [\n...     \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n...     \"https://farm3.staticflickr.com/2674/5850229113_4fe05d5265_z.jpg\",\n... ]\n>>> images = [Image.open(requests.get(url, stream=True).raw) for url in urls]\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=images, return_tensors=\"np\", padding=True\n... )\n>>> outputs = model(\n...     input_ids=inputs.input_ids,\n...     attention_mask=inputs.attention_mask,\n...     pixel_values=inputs.pixel_values,\n... )\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n\n>>> # save and load from pretrained\n>>> model.save_pretrained(\"vit-bert\")\n>>> model = FlaxVisionTextDualEncoderModel.from_pretrained(\"vit-bert\")\n\n>>> # inference\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = jax.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n```", "```py\n( config: Optional[VisionTextDualEncoderConfig] = None vision_model: Optional[TFPreTrainedModel] = None text_model: Optional[TFPreTrainedModel] = None )\n```", "```py\n( input_ids: tf.Tensor | None = None pixel_values: tf.Tensor | None = None attention_mask: tf.Tensor | None = None position_ids: tf.Tensor | None = None return_loss: Optional[bool] = None token_type_ids: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.models.clip.modeling_tf_clip.TFCLIPOutput or tuple(tf.Tensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import (\n...     TFVisionTextDualEncoderModel,\n...     VisionTextDualEncoderProcessor,\n...     AutoImageProcessor,\n...     AutoTokenizer,\n... )\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n>>> processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n>>> model = TFVisionTextDualEncoderModel.from_vision_text_pretrained(\n...     \"google/vit-base-patch16-224\", \"bert-base-uncased\"\n... )\n\n>>> # contrastive training\n>>> urls = [\n...     \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n...     \"https://farm3.staticflickr.com/2674/5850229113_4fe05d5265_z.jpg\",\n... ]\n>>> images = [Image.open(requests.get(url, stream=True).raw) for url in urls]\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=images, return_tensors=\"np\", padding=True\n... )\n>>> outputs = model(\n...     input_ids=inputs.input_ids,\n...     attention_mask=inputs.attention_mask,\n...     pixel_values=inputs.pixel_values,\n...     return_loss=True,\n... )\n>>> loss, logits_per_image = outputs.loss, outputs.logits_per_image  # this is the image-text similarity score\n\n>>> # save and load from pretrained\n>>> model.save_pretrained(\"vit-bert\")\n>>> model = TFVisionTextDualEncoderModel.from_pretrained(\"vit-bert\")\n\n>>> # inference\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n```"]