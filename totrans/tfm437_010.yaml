- en: Train with a script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/run_scripts](https://huggingface.co/docs/transformers/v4.37.2/en/run_scripts)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/336.76f009a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Markdown.fef84341.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/stores.c16bc1a5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: Along with the ðŸ¤— Transformers [notebooks](./noteboks/README), there are also
    example scripts demonstrating how to train a model for a task with [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch),
    [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow),
    or [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax).
  prefs: []
  type: TYPE_NORMAL
- en: You will also find scripts weâ€™ve used in our [research projects](https://github.com/huggingface/transformers/tree/main/examples/research_projects)
    and [legacy examples](https://github.com/huggingface/transformers/tree/main/examples/legacy)
    which are mostly community contributed. These scripts are not actively maintained
    and require a specific version of ðŸ¤— Transformers that will most likely be incompatible
    with the latest version of the library.
  prefs: []
  type: TYPE_NORMAL
- en: The example scripts are not expected to work out-of-the-box on every problem,
    and you may need to adapt the script to the problem youâ€™re trying to solve. To
    help you with this, most of the scripts fully expose how data is preprocessed,
    allowing you to edit it as necessary for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: For any feature youâ€™d like to implement in an example script, please discuss
    it on the [forum](https://discuss.huggingface.co/) or in an [issue](https://github.com/huggingface/transformers/issues)
    before submitting a Pull Request. While we welcome bug fixes, it is unlikely we
    will merge a Pull Request that adds more functionality at the cost of readability.
  prefs: []
  type: TYPE_NORMAL
- en: This guide will show you how to run an example summarization training script
    in [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization)
    and [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization).
    All examples are expected to work with both frameworks unless otherwise specified.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To successfully run the latest version of the example scripts, you have to
    **install ðŸ¤— Transformers from source** in a new virtual environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For older versions of the example scripts, click on the toggle below:'
  prefs: []
  type: TYPE_NORMAL
- en: <details data-svelte-h="svelte-145d76l"><summary>Examples for older versions
    of ðŸ¤— Transformers</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[v4.5.1](https://github.com/huggingface/transformers/tree/v4.5.1/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v4.4.2](https://github.com/huggingface/transformers/tree/v4.4.2/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v4.3.3](https://github.com/huggingface/transformers/tree/v4.3.3/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v4.2.2](https://github.com/huggingface/transformers/tree/v4.2.2/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v4.1.1](https://github.com/huggingface/transformers/tree/v4.1.1/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v4.0.1](https://github.com/huggingface/transformers/tree/v4.0.1/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v3.5.1](https://github.com/huggingface/transformers/tree/v3.5.1/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v3.4.0](https://github.com/huggingface/transformers/tree/v3.4.0/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v3.3.1](https://github.com/huggingface/transformers/tree/v3.3.1/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v3.2.0](https://github.com/huggingface/transformers/tree/v3.2.0/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v3.1.0](https://github.com/huggingface/transformers/tree/v3.1.0/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v3.0.2](https://github.com/huggingface/transformers/tree/v3.0.2/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v2.11.0](https://github.com/huggingface/transformers/tree/v2.11.0/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v2.10.0](https://github.com/huggingface/transformers/tree/v2.10.0/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v2.9.1](https://github.com/huggingface/transformers/tree/v2.9.1/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v2.8.0](https://github.com/huggingface/transformers/tree/v2.8.0/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v2.7.0](https://github.com/huggingface/transformers/tree/v2.7.0/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v2.6.0](https://github.com/huggingface/transformers/tree/v2.6.0/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v2.5.1](https://github.com/huggingface/transformers/tree/v2.5.1/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v2.4.0](https://github.com/huggingface/transformers/tree/v2.4.0/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v2.3.0](https://github.com/huggingface/transformers/tree/v2.3.0/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v2.2.0](https://github.com/huggingface/transformers/tree/v2.2.0/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v2.1.1](https://github.com/huggingface/transformers/tree/v2.1.0/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v2.0.0](https://github.com/huggingface/transformers/tree/v2.0.0/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v1.2.0](https://github.com/huggingface/transformers/tree/v1.2.0/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v1.1.0](https://github.com/huggingface/transformers/tree/v1.1.0/examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[v1.0.0](https://github.com/huggingface/transformers/tree/v1.0.0/examples)</details>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then switch your current clone of ðŸ¤— Transformers to a specific version, like
    v3.5.1 for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After youâ€™ve setup the correct library version, navigate to the example folder
    of your choice and install the example specific requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Run a script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: The example script downloads and preprocesses a dataset from the ðŸ¤— [Datasets](https://huggingface.co/docs/datasets/)
    library. Then the script fine-tunes a dataset with the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
    on an architecture that supports summarization. The following example shows how
    to fine-tune [T5-small](https://huggingface.co/t5-small) on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail)
    dataset. The T5 model requires an additional `source_prefix` argument due to how
    it was trained. This prompt lets T5 know this is a summarization task.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: The example script downloads and preprocesses a dataset from the ðŸ¤— [Datasets](https://huggingface.co/docs/datasets/)
    library. Then the script fine-tunes a dataset using Keras on an architecture that
    supports summarization. The following example shows how to fine-tune [T5-small](https://huggingface.co/t5-small)
    on the [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) dataset.
    The T5 model requires an additional `source_prefix` argument due to how it was
    trained. This prompt lets T5 know this is a summarization task.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Distributed training and mixed precision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
    supports distributed training and mixed precision, which means you can also use
    it in a script. To enable both of these features:'
  prefs: []
  type: TYPE_NORMAL
- en: Add the `fp16` argument to enable mixed precision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the number of GPUs to use with the `nproc_per_node` argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow scripts utilize a [`MirroredStrategy`](https://www.tensorflow.org/guide/distributed_training#mirroredstrategy)
    for distributed training, and you donâ€™t need to add any additional arguments to
    the training script. The TensorFlow script will use multiple GPUs by default if
    they are available.
  prefs: []
  type: TYPE_NORMAL
- en: Run a script on a TPU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Processing Units (TPUs) are specifically designed to accelerate performance.
    PyTorch supports TPUs with the [XLA](https://www.tensorflow.org/xla) deep learning
    compiler (see [here](https://github.com/pytorch/xla/blob/master/README.md) for
    more details). To use a TPU, launch the `xla_spawn.py` script and use the `num_cores`
    argument to set the number of TPU cores you want to use.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Processing Units (TPUs) are specifically designed to accelerate performance.
    TensorFlow scripts utilize a [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy)
    for training on TPUs. To use a TPU, pass the name of the TPU resource to the `tpu`
    argument.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Run a script with ðŸ¤— Accelerate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ðŸ¤— [Accelerate](https://huggingface.co/docs/accelerate) is a PyTorch-only library
    that offers a unified method for training a model on several types of setups (CPU-only,
    multiple GPUs, TPUs) while maintaining complete visibility into the PyTorch training
    loop. Make sure you have ðŸ¤— Accelerate installed if you donâ€™t already have it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: As Accelerate is rapidly developing, the git version of accelerate must
    be installed to run the scripts'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: 'Instead of the `run_summarization.py` script, you need to use the `run_summarization_no_trainer.py`
    script. ðŸ¤— Accelerate supported scripts will have a `task_no_trainer.py` file in
    the folder. Begin by running the following command to create and save a configuration
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Test your setup to make sure it is configured correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you are ready to launch the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Use a custom dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The summarization script supports custom datasets as long as they are a CSV
    or JSON Line file. When you use your own dataset, you need to specify several
    additional arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train_file` and `validation_file` specify the path to your training and validation
    files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_column` is the input text to summarize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summary_column` is the target text to output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A summarization script using a custom dataset would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Test a script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is often a good idea to run your script on a smaller number of dataset examples
    to ensure everything works as expected before committing to an entire dataset
    which may take hours to complete. Use the following arguments to truncate the
    dataset to a maximum number of samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_train_samples`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_eval_samples`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_predict_samples`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Not all example scripts support the `max_predict_samples` argument. If you
    arenâ€™t sure whether your script supports this argument, add the `-h` argument
    to check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Resume training from checkpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another helpful option to enable is resuming training from a previous checkpoint.
    This will ensure you can pick up where you left off without starting over if your
    training gets interrupted. There are two methods to resume training from a checkpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first method uses the `output_dir previous_output_dir` argument to resume
    training from the latest checkpoint stored in `output_dir`. In this case, you
    should remove `overwrite_output_dir`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The second method uses the `resume_from_checkpoint path_to_specific_checkpoint`
    argument to resume training from a specific checkpoint folder.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Share your model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All scripts can upload your final model to the [Model Hub](https://huggingface.co/models).
    Make sure you are logged into Hugging Face before you begin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Then add the `push_to_hub` argument to the script. This argument will create
    a repository with your Hugging Face username and the folder name specified in
    `output_dir`.
  prefs: []
  type: TYPE_NORMAL
- en: To give your repository a specific name, use the `push_to_hub_model_id` argument
    to add it. The repository will be automatically listed under your namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows how to upload a model with a specific repository
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
