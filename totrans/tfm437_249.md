# XLM-V

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlm-v](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlm-v)

## 概述

XLM-V是一个多语言语言模型，具有一个由Common Crawl的2.5TB数据训练的一百万标记词汇（与XLM-R相同）。它在Davis Liang、Hila Gonen、Yuning Mao、Rui Hou、Naman Goyal、Marjan Ghazvininejad、Luke Zettlemoyer和Madian Khabsa的论文[XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://arxiv.org/abs/2301.10472)中介绍。

从XLM-V论文的摘要中：

*大型多语言语言模型通常依赖于跨100多种语言共享的单一词汇表。随着这些模型的参数数量和深度增加，词汇表大小基本保持不变。这种词汇瓶颈限制了诸如XLM-R之类的多语言模型的表征能力。在本文中，我们介绍了一种通过减少语言之间的标记共享并分配词汇容量来实现对非常大型多语言词汇的扩展的新方法，以实现对每种单独语言的足够覆盖。使用我们的词汇表进行标记化通常比XLM-R更具语义意义且更短。利用这个改进的词汇表，我们训练了XLM-V，一个具有一百万标记词汇的多语言语言模型。XLM-V在我们测试的每个任务上都优于XLM-R，包括自然语言推理（XNLI）、问答（MLQA、XQuAD、TyDiQA）和命名实体识别（WikiAnn）等低资源任务（Americas NLI、MasakhaNER）。*

该模型由[stefan-it](https://huggingface.co/stefan-it)贡献，包括对XLM-V在下游任务上的详细实验。实验存储库可以在[这里](https://github.com/stefan-it/xlm-v-experiments)找到。

## 使用提示

+   XLM-V与XLM-RoBERTa模型架构兼容，只需将模型权重从[`fairseq`](https://github.com/facebookresearch/fairseq)库转换即可。

+   `XLMTokenizer`实现用于加载词汇表并执行标记化。

XLM-V（基础大小）模型可在[`facebook/xlm-v-base`](https://huggingface.co/facebook/xlm-v-base)标识符下找到。

XLM-V架构与XLM-RoBERTa相同，请参考[XLM-RoBERTa文档](xlm-roberta)以获取API参考和示例。
