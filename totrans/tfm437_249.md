# XLM-V

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlm-v`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlm-v)

## 概述

XLM-V 是一个多语言语言模型，具有一个由 Common Crawl 的 2.5TB 数据训练的一百万标记词汇（与 XLM-R 相同）。它在 Davis Liang、Hila Gonen、Yuning Mao、Rui Hou、Naman Goyal、Marjan Ghazvininejad、Luke Zettlemoyer 和 Madian Khabsa 的论文[XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://arxiv.org/abs/2301.10472)中介绍。

从 XLM-V 论文的摘要中：

*大型多语言语言模型通常依赖于跨 100 多种语言共享的单一词汇表。随着这些模型的参数数量和深度增加，词汇表大小基本保持不变。这种词汇瓶颈限制了诸如 XLM-R 之类的多语言模型的表征能力。在本文中，我们介绍了一种通过减少语言之间的标记共享并分配词汇容量来实现对非常大型多语言词汇的扩展的新方法，以实现对每种单独语言的足够覆盖。使用我们的词汇表进行标记化通常比 XLM-R 更具语义意义且更短。利用这个改进的词汇表，我们训练了 XLM-V，一个具有一百万标记词汇的多语言语言模型。XLM-V 在我们测试的每个任务上都优于 XLM-R，包括自然语言推理（XNLI）、问答（MLQA、XQuAD、TyDiQA）和命名实体识别（WikiAnn）等低资源任务（Americas NLI、MasakhaNER）。*

该模型由[stefan-it](https://huggingface.co/stefan-it)贡献，包括对 XLM-V 在下游任务上的详细实验。实验存储库可以在[这里](https://github.com/stefan-it/xlm-v-experiments)找到。

## 使用提示

+   XLM-V 与 XLM-RoBERTa 模型架构兼容，只需将模型权重从[`fairseq`](https://github.com/facebookresearch/fairseq)库转换即可。

+   `XLMTokenizer`实现用于加载词汇表并执行标记化。

XLM-V（基础大小）模型可在[`facebook/xlm-v-base`](https://huggingface.co/facebook/xlm-v-base)标识符下找到。

XLM-V 架构与 XLM-RoBERTa 相同，请参考 XLM-RoBERTa 文档以获取 API 参考和示例。
