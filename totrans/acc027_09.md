# ä»Jupyterç¯å¢ƒå¯åŠ¨å¤šGPUè®­ç»ƒ

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/accelerate/basic_tutorials/notebook](https://huggingface.co/docs/accelerate/basic_tutorials/notebook)

æœ¬æ•™ç¨‹å°†æ•™æ‚¨å¦‚ä½•ä½¿ç”¨ğŸ¤— Accelerateä»Jupyter Notebookåœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸Šå¾®è°ƒè®¡ç®—æœºè§†è§‰æ¨¡å‹ã€‚æ‚¨è¿˜å°†å­¦ä¹ å¦‚ä½•è®¾ç½®ç¡®ä¿ç¯å¢ƒæ­£ç¡®é…ç½®ã€æ•°æ®å·²æ­£ç¡®å‡†å¤‡ä»¥åŠæœ€ç»ˆå¦‚ä½•å¯åŠ¨è®­ç»ƒæ‰€éœ€çš„ä¸€äº›è¦æ±‚ã€‚

æœ¬æ•™ç¨‹ä¹Ÿå¯ä½œä¸ºJupyter Notebook [åœ¨è¿™é‡Œ](https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_cv_example.ipynb)æŸ¥çœ‹

## é…ç½®ç¯å¢ƒ

åœ¨æ‰§è¡Œä»»ä½•è®­ç»ƒä¹‹å‰ï¼Œç³»ç»Ÿä¸­å¿…é¡»å­˜åœ¨ä¸€ä¸ªğŸ¤— Accelerateé…ç½®æ–‡ä»¶ã€‚é€šå¸¸å¯ä»¥é€šè¿‡åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤å¹¶å›ç­”æç¤ºæ¥å®Œæˆï¼š

```py
accelerate config
```

ä½†æ˜¯ï¼Œå¦‚æœé€šç”¨é»˜è®¤å€¼æ˜¯å¯ä»¥æ¥å—çš„ï¼Œå¹¶ä¸”æ‚¨*ä¸*åœ¨TPUä¸Šè¿è¡Œï¼ŒğŸ¤—Accelerateæœ‰ä¸€ä¸ªå®ç”¨ç¨‹åºï¼Œå¯ä»¥é€šè¿‡[utils.write_basic_config()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.commands.config.default.write_basic_config)å¿«é€Ÿå°†æ‚¨çš„GPUé…ç½®å†™å…¥é…ç½®æ–‡ä»¶ã€‚

ä»¥ä¸‹ä»£ç å°†åœ¨å†™å…¥é…ç½®åé‡æ–°å¯åŠ¨Jupyterï¼Œå› ä¸ºè°ƒç”¨äº†CUDAä»£ç æ¥æ‰§è¡Œæ­¤æ“ä½œã€‚

åœ¨å¤šGPUç³»ç»Ÿä¸Šï¼ŒCUDAä¸èƒ½è¢«åˆå§‹åŒ–å¤šæ¬¡ã€‚åœ¨ç¬”è®°æœ¬ä¸­è¿›è¡Œè°ƒè¯•å¹¶è°ƒç”¨CUDAæ˜¯å¯ä»¥çš„ï¼Œä½†ä¸ºäº†æœ€ç»ˆè®­ç»ƒï¼Œéœ€è¦æ‰§è¡Œå®Œå…¨æ¸…ç†å’Œé‡æ–°å¯åŠ¨ã€‚

```py
import os
from accelerate.utils import write_basic_config

write_basic_config()  # Write a config file
os._exit(00)  # Restart the notebook
```

## å‡†å¤‡æ•°æ®é›†å’Œæ¨¡å‹

æ¥ä¸‹æ¥ï¼Œæ‚¨åº”è¯¥å‡†å¤‡æ‚¨çš„æ•°æ®é›†ã€‚å¦‚å‰æ‰€è¿°ï¼Œåœ¨å‡†å¤‡`DataLoaders`å’Œæ¨¡å‹æ—¶åº”æ ¼å¤–å°å¿ƒï¼Œä»¥ç¡®ä¿**ä»»ä½•**ä¸œè¥¿éƒ½ä¸ä¼šæ”¾åœ¨*ä»»ä½•* GPUä¸Šã€‚

å¦‚æœæ‚¨è¿™æ ·åšï¼Œå»ºè®®å°†è¯¥ç‰¹å®šä»£ç æ”¾å…¥ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œå¹¶ä»ç¬”è®°æœ¬å¯åŠ¨å™¨ç•Œé¢ä¸­è°ƒç”¨è¯¥å‡½æ•°ï¼Œç¨åå°†æ˜¾ç¤ºè¯¥ç•Œé¢ã€‚

ç¡®ä¿æ ¹æ®[è¿™é‡Œ](https://github.com/huggingface/accelerate/tree/main/examples#simple-vision-example)çš„è¯´æ˜ä¸‹è½½æ•°æ®é›†

```py
import os, re, torch, PIL
import numpy as np

from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import DataLoader, Dataset
from torchvision.transforms import Compose, RandomResizedCrop, Resize, ToTensor

from accelerate import Accelerator
from accelerate.utils import set_seed
from timm import create_model
```

é¦–å…ˆï¼Œæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®æ–‡ä»¶åæå–ç±»åï¼š

```py
import os

data_dir = "../../images"
fnames = os.listdir(data_dir)
fname = fnames[0]
print(fname)
```

```py
beagle_32.jpg
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ ‡ç­¾æ˜¯`beagle`ã€‚ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¯ä»¥ä»æ–‡ä»¶åä¸­æå–æ ‡ç­¾ï¼š

```py
import re

def extract_label(fname):
    stem = fname.split(os.path.sep)[-1]
    return re.search(r"^(.*)_\d+\.jpg$", stem).groups()[0]
```

```py
extract_label(fname)
```

æ‚¨å¯ä»¥çœ‹åˆ°å®ƒæ­£ç¡®è¿”å›äº†æˆ‘ä»¬æ–‡ä»¶çš„æ­£ç¡®åç§°ï¼š

```py
"beagle"
```

æ¥ä¸‹æ¥åº”è¯¥åˆ¶ä½œä¸€ä¸ª`Dataset`ç±»æ¥å¤„ç†è·å–å›¾åƒå’Œæ ‡ç­¾ï¼š

```py
class PetsDataset(Dataset):
    def __init__(self, file_names, image_transform=None, label_to_id=None):
        self.file_names = file_names
        self.image_transform = image_transform
        self.label_to_id = label_to_id

    def __len__(self):
        return len(self.file_names)

    def __getitem__(self, idx):
        fname = self.file_names[idx]
        raw_image = PIL.Image.open(fname)
        image = raw_image.convert("RGB")
        if self.image_transform is not None:
            image = self.image_transform(image)
        label = extract_label(fname)
        if self.label_to_id is not None:
            label = self.label_to_id[label]
        return {"image": image, "label": label}
```

ç°åœ¨å¼€å§‹æ„å»ºæ•°æ®é›†ã€‚åœ¨è®­ç»ƒå‡½æ•°ä¹‹å¤–ï¼Œæ‚¨å¯ä»¥æ‰¾åˆ°å¹¶å£°æ˜æ‰€æœ‰çš„æ–‡ä»¶åå’Œæ ‡ç­¾ï¼Œå¹¶å°†å®ƒä»¬ç”¨ä½œå¯åŠ¨å‡½æ•°å†…çš„å‚è€ƒï¼š

```py
fnames = [os.path.join("../../images", fname) for fname in fnames if fname.endswith(".jpg")]
```

æ¥ä¸‹æ¥æ”¶é›†æ‰€æœ‰çš„æ ‡ç­¾ï¼š

```py
all_labels = [extract_label(fname) for fname in fnames]
id_to_label = list(set(all_labels))
id_to_label.sort()
label_to_id = {lbl: i for i, lbl in enumerate(id_to_label)}
```

æ¥ä¸‹æ¥ï¼Œæ‚¨åº”è¯¥åˆ¶ä½œä¸€ä¸ª`get_dataloaders`å‡½æ•°ï¼Œè¯¥å‡½æ•°å°†ä¸ºæ‚¨è¿”å›æ„å»ºçš„æ•°æ®åŠ è½½å™¨ã€‚å¦‚å‰æ‰€è¿°ï¼Œå¦‚æœåœ¨æ„å»º`DataLoaders`æ—¶æ•°æ®ä¼šè‡ªåŠ¨å‘é€åˆ°GPUæˆ–TPUè®¾å¤‡ï¼Œåˆ™å¿…é¡»ä½¿ç”¨æ­¤æ–¹æ³•æ„å»ºå®ƒä»¬ã€‚

```py
def get_dataloaders(batch_size: int = 64):
    "Builds a set of dataloaders with a batch_size"
    random_perm = np.random.permutation(len(fnames))
    cut = int(0.8 * len(fnames))
    train_split = random_perm[:cut]
    eval_split = random_perm[cut:]

    # For training a simple RandomResizedCrop will be used
    train_tfm = Compose([RandomResizedCrop((224, 224), scale=(0.5, 1.0)), ToTensor()])
    train_dataset = PetsDataset([fnames[i] for i in train_split], image_transform=train_tfm, label_to_id=label_to_id)

    # For evaluation a deterministic Resize will be used
    eval_tfm = Compose([Resize((224, 224)), ToTensor()])
    eval_dataset = PetsDataset([fnames[i] for i in eval_split], image_transform=eval_tfm, label_to_id=label_to_id)

    # Instantiate dataloaders
    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=4)
    eval_dataloader = DataLoader(eval_dataset, shuffle=False, batch_size=batch_size * 2, num_workers=4)
    return train_dataloader, eval_dataloader
```

æœ€åï¼Œæ‚¨åº”è¯¥å¯¼å…¥ç¨åè¦ä½¿ç”¨çš„è°ƒåº¦ç¨‹åºï¼š

```py
from torch.optim.lr_scheduler import CosineAnnealingLR
```

## ç¼–å†™è®­ç»ƒå‡½æ•°

ç°åœ¨å¯ä»¥æ„å»ºè®­ç»ƒå¾ªç¯ã€‚[notebook_launcher()](/docs/accelerate/v0.27.2/en/package_reference/launchers#accelerate.notebook_launcher)é€šè¿‡ä¼ å…¥ä¸€ä¸ªè¦è°ƒç”¨çš„å‡½æ•°æ¥å·¥ä½œï¼Œè¯¥å‡½æ•°å°†åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­è¿è¡Œã€‚

è¿™æ˜¯ä¸€ä¸ªç”¨äºåŠ¨ç‰©åˆ†ç±»é—®é¢˜çš„åŸºæœ¬è®­ç»ƒå¾ªç¯ï¼š

ä»£ç å·²ç»åˆ†å‰²å¼€æ¥ï¼Œä»¥ä¾¿å¯¹æ¯ä¸ªéƒ¨åˆ†è¿›è¡Œè§£é‡Šã€‚å¯ä»¥å¤åˆ¶å¹¶ç²˜è´´çš„å®Œæ•´ç‰ˆæœ¬å°†åœ¨æœ€åæä¾›

```py
def training_loop(mixed_precision="fp16", seed: int = 42, batch_size: int = 64):
    set_seed(seed)
    accelerator = Accelerator(mixed_precision=mixed_precision)
```

é¦–å…ˆï¼Œæ‚¨åº”è¯¥å°½æ—©åœ¨è®­ç»ƒå¾ªç¯ä¸­è®¾ç½®ç§å­å¹¶åˆ›å»ºä¸€ä¸ª[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)å¯¹è±¡ã€‚

å¦‚æœåœ¨TPUä¸Šè®­ç»ƒï¼Œæ‚¨çš„è®­ç»ƒå¾ªç¯åº”å°†æ¨¡å‹ä½œä¸ºå‚æ•°ä¼ å…¥ï¼Œå¹¶ä¸”åº”åœ¨è®­ç»ƒå¾ªç¯å‡½æ•°ä¹‹å¤–å®ä¾‹åŒ–ã€‚è¯·å‚é˜…[TPUæœ€ä½³å®è·µ](../concept_guides/training_tpu)ä»¥äº†è§£åŸå› 

æ¥ä¸‹æ¥ï¼Œæ‚¨åº”è¯¥æ„å»ºæ‚¨çš„æ•°æ®åŠ è½½å™¨å¹¶åˆ›å»ºæ‚¨çš„æ¨¡å‹ï¼š

```py
    train_dataloader, eval_dataloader = get_dataloaders(batch_size)
    model = create_model("resnet50d", pretrained=True, num_classes=len(label_to_id))
```

åœ¨è¿™é‡Œæ„å»ºæ¨¡å‹ï¼Œä»¥ä¾¿ç§å­ä¹Ÿæ§åˆ¶æ–°çš„æƒé‡åˆå§‹åŒ–

åœ¨æ­¤ç¤ºä¾‹ä¸­è¿›è¡Œè¿ç§»å­¦ä¹ æ—¶ï¼Œæ¨¡å‹çš„ç¼–ç å™¨å¼€å§‹æ—¶æ˜¯å†»ç»“çš„ï¼Œå› æ­¤æ¨¡å‹çš„å¤´éƒ¨åªèƒ½æœ€åˆè¿›è¡Œè®­ç»ƒï¼š

```py
    for param in model.parameters():
        param.requires_grad = False
    for param in model.get_classifier().parameters():
        param.requires_grad = True
```

å¯¹å›¾åƒæ‰¹æ¬¡è¿›è¡Œå½’ä¸€åŒ–å°†ä½¿è®­ç»ƒé€Ÿåº¦ç¨å¿«ï¼š

```py
    mean = torch.tensor(model.default_cfg["mean"])[None, :, None, None]
    std = torch.tensor(model.default_cfg["std"])[None, :, None, None]
```

ä¸ºäº†ä½¿è¿™äº›å¸¸é‡åœ¨æ´»åŠ¨è®¾å¤‡ä¸Šå¯ç”¨ï¼Œæ‚¨åº”è¯¥å°†å…¶è®¾ç½®ä¸ºåŠ é€Ÿå™¨çš„è®¾å¤‡ï¼š

```py
    mean = mean.to(accelerator.device)
    std = std.to(accelerator.device)
```

æ¥ä¸‹æ¥å®ä¾‹åŒ–ç”¨äºè®­ç»ƒçš„å…¶ä½™PyTorchç±»ï¼š

```py
    optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-2 / 25)
    lr_scheduler = OneCycleLR(optimizer=optimizer, max_lr=3e-2, epochs=5, steps_per_epoch=len(train_dataloader))
```

åœ¨å°†æ‰€æœ‰å†…å®¹ä¼ é€’ç»™[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)ä¹‹å‰ã€‚

æ²¡æœ‰ç‰¹å®šçš„é¡ºåºéœ€è¦è®°ä½ï¼Œæ‚¨åªéœ€è¦æŒ‰ç…§ä¸prepareæ–¹æ³•ä¸­ç»™å‡ºçš„ç›¸åŒé¡ºåºè§£åŒ…å¯¹è±¡ã€‚

```py
    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
    )
```

ç°åœ¨è®­ç»ƒæ¨¡å‹ï¼š

```py
    for epoch in range(5):
        model.train()
        for batch in train_dataloader:
            inputs = (batch["image"] - mean) / std
            outputs = model(inputs)
            loss = torch.nn.functional.cross_entropy(outputs, batch["label"])
            accelerator.backward(loss)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
```

è¯„ä¼°å¾ªç¯ä¸è®­ç»ƒå¾ªç¯ç›¸æ¯”ä¼šç¨æœ‰ä¸åŒã€‚ä¼ é€’çš„å…ƒç´ æ•°é‡ä»¥åŠæ¯ä¸ªæ‰¹æ¬¡çš„æ€»ä½“å‡†ç¡®ç‡å°†æ·»åŠ åˆ°ä¸¤ä¸ªå¸¸é‡ä¸­ï¼š

```py
        model.eval()
        accurate = 0
        num_elems = 0
```

æ¥ä¸‹æ¥æ˜¯æ‚¨æ ‡å‡†PyTorchå¾ªç¯çš„å…¶ä½™éƒ¨åˆ†ï¼š

```py
        for batch in eval_dataloader:
            inputs = (batch["image"] - mean) / std
            with torch.no_grad():
                outputs = model(inputs)
            predictions = outputs.argmax(dim=-1)
```

æœ€åæ˜¯æœ€åä¸€ä¸ªä¸»è¦åŒºåˆ«ã€‚

åœ¨è¿›è¡Œåˆ†å¸ƒå¼è¯„ä¼°æ—¶ï¼Œéœ€è¦é€šè¿‡[gather()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather)ä¼ é€’é¢„æµ‹å’Œæ ‡ç­¾ï¼Œä»¥ä¾¿æ‰€æœ‰æ•°æ®éƒ½åœ¨å½“å‰è®¾å¤‡ä¸Šå¯ç”¨ï¼Œå¹¶ä¸”å¯ä»¥å®ç°æ­£ç¡®è®¡ç®—çš„æŒ‡æ ‡ï¼š

```py
            accurate_preds = accelerator.gather(predictions) == accelerator.gather(batch["label"])
            num_elems += accurate_preds.shape[0]
            accurate += accurate_preds.long().sum()
```

ç°åœ¨æ‚¨åªéœ€è¦è®¡ç®—æ­¤é—®é¢˜çš„å®é™…æŒ‡æ ‡ï¼Œå¹¶å¯ä»¥åœ¨ä¸»è¿›ç¨‹ä¸Šä½¿ç”¨[print()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.print)æ‰“å°å®ƒï¼š

```py
        eval_metric = accurate.item() / num_elems
        accelerator.print(f"epoch {epoch}: {100 * eval_metric:.2f}")
```

ä¸‹é¢æä¾›äº†å®Œæ•´ç‰ˆæœ¬çš„è®­ç»ƒå¾ªç¯ï¼š

```py
def training_loop(mixed_precision="fp16", seed: int = 42, batch_size: int = 64):
    set_seed(seed)
    # Initialize accelerator
    accelerator = Accelerator(mixed_precision=mixed_precision)
    # Build dataloaders
    train_dataloader, eval_dataloader = get_dataloaders(batch_size)

    # Instantiate the model (you build the model here so that the seed also controls new weight initaliziations)
    model = create_model("resnet50d", pretrained=True, num_classes=len(label_to_id))

    # Freeze the base model
    for param in model.parameters():
        param.requires_grad = False
    for param in model.get_classifier().parameters():
        param.requires_grad = True

    # You can normalize the batches of images to be a bit faster
    mean = torch.tensor(model.default_cfg["mean"])[None, :, None, None]
    std = torch.tensor(model.default_cfg["std"])[None, :, None, None]

    # To make these constants available on the active device, set it to the accelerator device
    mean = mean.to(accelerator.device)
    std = std.to(accelerator.device)

    # Instantiate the optimizer
    optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-2 / 25)

    # Instantiate the learning rate scheduler
    lr_scheduler = OneCycleLR(optimizer=optimizer, max_lr=3e-2, epochs=5, steps_per_epoch=len(train_dataloader))

    # Prepare everything
    # There is no specific order to remember, you just need to unpack the objects in the same order you gave them to the
    # prepare method.
    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
    )

    # Now you train the model
    for epoch in range(5):
        model.train()
        for batch in train_dataloader:
            inputs = (batch["image"] - mean) / std
            outputs = model(inputs)
            loss = torch.nn.functional.cross_entropy(outputs, batch["label"])
            accelerator.backward(loss)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()

        model.eval()
        accurate = 0
        num_elems = 0
        for batch in eval_dataloader:
            inputs = (batch["image"] - mean) / std
            with torch.no_grad():
                outputs = model(inputs)
            predictions = outputs.argmax(dim=-1)
            accurate_preds = accelerator.gather(predictions) == accelerator.gather(batch["label"])
            num_elems += accurate_preds.shape[0]
            accurate += accurate_preds.long().sum()

        eval_metric = accurate.item() / num_elems
        # Use accelerator.print to print only on the main process.
        accelerator.print(f"epoch {epoch}: {100 * eval_metric:.2f}")
```

## ä½¿ç”¨notebook_launcher

å‰©ä¸‹çš„å°±æ˜¯ä½¿ç”¨[notebook_launcher()](/docs/accelerate/v0.27.2/en/package_reference/launchers#accelerate.notebook_launcher)ã€‚

æ‚¨ä¼ å…¥å‡½æ•°ã€å‚æ•°ï¼ˆä½œä¸ºå…ƒç»„ï¼‰ä»¥åŠè¦è®­ç»ƒçš„è¿›ç¨‹æ•°ã€‚ï¼ˆæœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[æ–‡æ¡£](../package_reference/launchers)ï¼‰

```py
from accelerate import notebook_launcher
```

```py
args = ("fp16", 42, 64)
notebook_launcher(training_loop, args, num_processes=2)
```

åœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œæ—¶ï¼Œæ‚¨éœ€è¦åœ¨æ¯ä¸ªèŠ‚ç‚¹è®¾ç½®ä¸€ä¸ªJupyterä¼šè¯ï¼Œå¹¶åŒæ—¶è¿è¡Œå¯åŠ¨å•å…ƒæ ¼ã€‚

å¯¹äºåŒ…å«æ¯å°è®¡ç®—æœºæœ‰8ä¸ªGPUçš„2ä¸ªèŠ‚ç‚¹ï¼ˆè®¡ç®—æœºï¼‰çš„ç¯å¢ƒï¼Œä¸»è®¡ç®—æœºçš„IPåœ°å€ä¸ºâ€œ172.31.43.8â€ï¼Œä¼šæ˜¯è¿™æ ·çš„ï¼š

```py
notebook_launcher(training_loop, args, master_addr="172.31.43.8", node_rank=0, num_nodes=2, num_processes=8)
```

åœ¨å¦ä¸€å°æœºå™¨ä¸Šçš„ç¬¬äºŒä¸ªJupyterä¼šè¯ä¸­ï¼š

æ³¨æ„`node_rank`å·²æ›´æ”¹

```py
notebook_launcher(training_loop, args, master_addr="172.31.43.8", node_rank=1, num_nodes=2, num_processes=8)
```

åœ¨TPUä¸Šè¿è¡Œæ—¶ï¼Œä¼šæ˜¯è¿™æ ·çš„ï¼š

```py
model = create_model("resnet50d", pretrained=True, num_classes=len(label_to_id))

args = (model, "fp16", 42, 64)
notebook_launcher(training_loop, args, num_processes=8)
```

åœ¨è¿è¡Œæ—¶ï¼Œå®ƒå°†æ‰“å°è¿›åº¦ï¼Œå¹¶è¯´æ˜æ‚¨è¿è¡Œåœ¨å¤šå°‘è®¾å¤‡ä¸Šã€‚æœ¬æ•™ç¨‹ä½¿ç”¨äº†ä¸¤ä¸ªGPUï¼š

```py
Launching training on 2 GPUs.
epoch 0: 88.12
epoch 1: 91.73
epoch 2: 92.58
epoch 3: 93.90
epoch 4: 94.71
```

å°±æ˜¯è¿™æ ·ï¼

## è°ƒè¯•

åœ¨è¿è¡Œ`notebook_launcher`æ—¶å¸¸è§çš„é—®é¢˜æ˜¯æ”¶åˆ°CUDAå·²ç»åˆå§‹åŒ–çš„é—®é¢˜ã€‚è¿™é€šå¸¸æºäºç¬”è®°æœ¬ä¸­çš„å¯¼å…¥æˆ–å…ˆå‰ä»£ç è°ƒç”¨PyTorchçš„`torch.cuda`å­åº“ã€‚ä¸ºäº†å¸®åŠ©ç¼©å°é—®é¢˜èŒƒå›´ï¼Œæ‚¨å¯ä»¥åœ¨ç¯å¢ƒä¸­ä½¿ç”¨`ACCELERATE_DEBUG_MODE=yes`å¯åŠ¨`notebook_launcher`ï¼Œå¹¶åœ¨ç”Ÿæˆæ—¶è¿›è¡Œé¢å¤–æ£€æŸ¥ï¼Œä»¥ç¡®ä¿å¯ä»¥åˆ›å»ºå¸¸è§„è¿›ç¨‹å¹¶æ— é—®é¢˜åœ°åˆ©ç”¨CUDAã€‚ï¼ˆæ‚¨çš„CUDAä»£ç ä»ç„¶å¯ä»¥åœ¨ä¹‹åè¿è¡Œï¼‰ã€‚

## ç»“è®º

æœ¬ç¬”è®°æœ¬å±•ç¤ºäº†å¦‚ä½•ä»Jupyter Notebookå†…æ‰§è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚è¯·è®°ä½ä¸€äº›å…³é”®è¦ç‚¹ï¼š

+   ç¡®ä¿ä¿å­˜ä½¿ç”¨CUDAçš„ä»»ä½•ä»£ç ï¼ˆæˆ–CUDAå¯¼å…¥ï¼‰ä»¥ä¼ é€’ç»™[notebook_launcher()](/docs/accelerate/v0.27.2/en/package_reference/launchers#accelerate.notebook_launcher)çš„å‡½æ•°ã€‚

+   å°†`num_processes`è®¾ç½®ä¸ºç”¨äºè®­ç»ƒçš„è®¾å¤‡æ•°é‡ï¼ˆä¾‹å¦‚GPUã€CPUã€TPUç­‰çš„æ•°é‡ï¼‰

+   å¦‚æœä½¿ç”¨TPUï¼Œè¯·åœ¨è®­ç»ƒå¾ªç¯å‡½æ•°ä¹‹å¤–å£°æ˜æ‚¨çš„æ¨¡å‹
