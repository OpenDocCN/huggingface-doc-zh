["```py\n>>> from transformers import NllbTokenizer\n\n>>> tokenizer = NllbTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n>>> tokenizer(\"How was your day?\").input_ids\n[13374, 1398, 4260, 4039, 248130, 2, 256047]\n\n>>> # 2: '</s>'\n>>> # 256047 : 'eng_Latn'\n```", "```py\n>>> from transformers import NllbTokenizer\n\n>>> tokenizer = NllbTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n>>> tokenizer(\"How was your day?\").input_ids\n[256047, 13374, 1398, 4260, 4039, 248130, 2]\n```", "```py\n>>> from transformers import NllbTokenizer\n\n>>> tokenizer = NllbTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\", legacy_behaviour=True)\n```", "```py\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n\n>>> article = \"UN Chief says there is no military solution in Syria\"\n>>> inputs = tokenizer(article, return_tensors=\"pt\")\n\n>>> translated_tokens = model.generate(\n...     **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"fra_Latn\"], max_length=30\n... )\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\nLe chef de l'ONU dit qu'il n'y a pas de solution militaire en Syrie\n```", "```py\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\n...     \"facebook/nllb-200-distilled-600M\", token=True, src_lang=\"ron_Latn\"\n... )\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", token=True)\n\n>>> article = \"\u015eeful ONU spune c\u0103 nu exist\u0103 o solu\u0163ie militar\u0103 \u00een Siria\"\n>>> inputs = tokenizer(article, return_tensors=\"pt\")\n\n>>> translated_tokens = model.generate(\n...     **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"deu_Latn\"], max_length=30\n... )\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\nUN-Chef sagt, es gibt keine milit\u00e4rische L\u00f6sung in Syrien\n```", "```py\n>>> from transformers import NllbTokenizer\n\n>>> tokenizer = NllbTokenizer.from_pretrained(\n...     \"facebook/nllb-200-distilled-600M\", src_lang=\"eng_Latn\", tgt_lang=\"fra_Latn\"\n... )\n>>> example_english_phrase = \" UN Chief Says There Is No Military Solution in Syria\"\n>>> expected_translation_french = \"Le chef de l'ONU affirme qu'il n'y a pas de solution militaire en Syrie.\"\n>>> inputs = tokenizer(example_english_phrase, text_target=expected_translation_french, return_tensors=\"pt\")\n```", "```py\n>>> from transformers import NllbTokenizerFast\n\n>>> tokenizer = NllbTokenizerFast.from_pretrained(\n...     \"facebook/nllb-200-distilled-600M\", src_lang=\"eng_Latn\", tgt_lang=\"fra_Latn\"\n... )\n>>> example_english_phrase = \" UN Chief Says There Is No Military Solution in Syria\"\n>>> expected_translation_french = \"Le chef de l'ONU affirme qu'il n'y a pas de solution militaire en Syrie.\"\n>>> inputs = tokenizer(example_english_phrase, text_target=expected_translation_french, return_tensors=\"pt\")\n```"]