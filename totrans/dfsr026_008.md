# 理解管道、模型和调度器

> 原文链接：[https://huggingface.co/docs/diffusers/using-diffusers/write_own_pipeline](https://huggingface.co/docs/diffusers/using-diffusers/write_own_pipeline)

扩散器旨在成为一个用户友好和灵活的工具箱，用于构建适合您用例的扩散系统。工具箱的核心是模型和调度器。虽然 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline) 将这些组件捆绑在一起以方便使用，但您也可以解除管道并单独使用模型和调度器来创建新的扩散系统。

在本教程中，您将学习如何使用模型和调度器来组装一个用于推断的扩散系统，从基本管道开始，然后逐步过渡到稳定扩散管道。

## 分解基本管道

管道是运行模型进行推断的一种快速简便的方法，只需不超过四行代码即可生成图像：

```py
>>> from diffusers import DDPMPipeline

>>> ddpm = DDPMPipeline.from_pretrained("google/ddpm-cat-256", use_safetensors=True).to("cuda")
>>> image = ddpm(num_inference_steps=25).images[0]
>>> image
```

![从DDPMPipeline创建的猫图像](../Images/14589ab570d5bde87da475dacae4b73e.png)

这太容易了，但管道是如何做到的呢？让我们分解管道并看看底层发生了什么。

在上面的示例中，管道包含一个 [UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel) 模型和一个 [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)。管道通过取与所需输出相同大小的随机噪声，并将其通过模型多次传递来去噪图像。在每个时间步中，模型预测 *噪声残差*，调度器使用它来预测一个更少嘈杂的图像。管道重复此过程，直到达到指定的推断步数的末尾。

要分别使用模型和调度器重新创建管道，请编写我们自己的去噪过程。

1.  加载模型和调度器：

```py
>>> from diffusers import DDPMScheduler, UNet2DModel

>>> scheduler = DDPMScheduler.from_pretrained("google/ddpm-cat-256")
>>> model = UNet2DModel.from_pretrained("google/ddpm-cat-256", use_safetensors=True).to("cuda")
```

1.  设置运行去噪过程的时间步数：

```py
>>> scheduler.set_timesteps(50)
```

1.  设置调度器时间步数会创建一个张量，其中包含均匀间隔的元素，例如此示例中的 50 个。每个元素对应于模型去噪图像的时间步。稍后创建去噪循环时，您将遍历此张量以去噪图像：

```py
>>> scheduler.timesteps
tensor([980, 960, 940, 920, 900, 880, 860, 840, 820, 800, 780, 760, 740, 720,
    700, 680, 660, 640, 620, 600, 580, 560, 540, 520, 500, 480, 460, 440,
    420, 400, 380, 360, 340, 320, 300, 280, 260, 240, 220, 200, 180, 160,
    140, 120, 100,  80,  60,  40,  20,   0])
```

1.  创建与所需输出相同形状的一些随机噪声：

```py
>>> import torch

>>> sample_size = model.config.sample_size
>>> noise = torch.randn((1, 3, sample_size, sample_size), device="cuda")
```

1.  现在编写一个循环以遍历时间步。在每个时间步中，模型执行 [UNet2DModel.forward()](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel.forward) 操作并返回嘈杂的残差。调度器的 [step()](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler.step) 方法获取嘈杂的残差、时间步和输入，并预测前一个时间步的图像。此输出成为去噪循环中模型的下一个输入，并将重复，直到达到 `timesteps` 数组的末尾。

```py
>>> input = noise

>>> for t in scheduler.timesteps:
...     with torch.no_grad():
...         noisy_residual = model(input, t).sample
...     previous_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample
...     input = previous_noisy_sample
```

这是整个去噪过程，您可以使用相同的模式编写任何扩散系统。

1.  最后一步是将去噪输出转换为图像：

```py
>>> from PIL import Image
>>> import numpy as np

>>> image = (input / 2 + 0.5).clamp(0, 1).squeeze()
>>> image = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy()
>>> image = Image.fromarray(image)
>>> image
```

在下一节中，您将将您的技能付诸实践，并分解更复杂的稳定扩散管道。步骤多少是相同的。您将初始化必要的组件，并设置时间步数以创建一个 `timestep` 数组。`timestep` 数组在去噪循环中使用，对于该数组中的每个元素，模型会预测一个更少嘈杂的图像。去噪循环会遍历 `timestep`，在每个时间步中，它会输出一个嘈杂的残差，调度器使用它来预测前一个时间步的更少嘈杂的图像。这个过程会重复，直到达到 `timestep` 数组的末尾。

让我们试一试！

## 分解稳定扩散管道

稳定扩散是一种文本到图像的*潜在扩散*模型。它被称为潜在扩散模型，因为它使用图像的较低维表示而不是实际的像素空间，这使其更节省内存。编码器将图像压缩为较小的表示，解码器将压缩表示转换回图像。对于文本到图像模型，您将需要一个分词器和一个编码器来生成文本嵌入。从前面的示例中，您已经知道需要一个UNet模型和一个调度器。

正如您所看到的，这已经比仅包含UNet模型的DDPM管线更复杂。稳定扩散模型有三个单独的预训练模型。

💡 阅读[稳定扩散是如何工作的？](https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work)博客，了解有关VAE、UNet和文本编码器模型如何工作的更多细节。

现在您知道稳定扩散管线所需的内容，使用[from_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.from_pretrained)方法加载所有这些组件。您可以在预训练的[`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)检查点中找到它们，每个组件都存储在一个单独的子文件夹中：

```py
>>> from PIL import Image
>>> import torch
>>> from transformers import CLIPTextModel, CLIPTokenizer
>>> from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler

>>> vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae", use_safetensors=True)
>>> tokenizer = CLIPTokenizer.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="tokenizer")
>>> text_encoder = CLIPTextModel.from_pretrained(
...     "CompVis/stable-diffusion-v1-4", subfolder="text_encoder", use_safetensors=True
... )
>>> unet = UNet2DConditionModel.from_pretrained(
...     "CompVis/stable-diffusion-v1-4", subfolder="unet", use_safetensors=True
... )
```

将默认的[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)替换为[UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)，看看如何轻松地插入不同的调度器：

```py
>>> from diffusers import UniPCMultistepScheduler

>>> scheduler = UniPCMultistepScheduler.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="scheduler")
```

为了加快推理速度，将模型移动到GPU上，因为与调度器不同，它们具有可训练的权重：

```py
>>> torch_device = "cuda"
>>> vae.to(torch_device)
>>> text_encoder.to(torch_device)
>>> unet.to(torch_device)
```

### 创建文本嵌入

下一步是对文本进行标记化以生成嵌入。文本用于调节UNet模型并引导扩散过程朝着类似于输入提示的方向发展。

💡 `guidance_scale`参数确定在生成图像时应给予提示的权重有多大。

如果您想生成其他内容，请随意选择任何提示！

```py
>>> prompt = ["a photograph of an astronaut riding a horse"]
>>> height = 512  # default height of Stable Diffusion
>>> width = 512  # default width of Stable Diffusion
>>> num_inference_steps = 25  # Number of denoising steps
>>> guidance_scale = 7.5  # Scale for classifier-free guidance
>>> generator = torch.manual_seed(0)  # Seed generator to create the initial latent noise
>>> batch_size = len(prompt)
```

标记化文本并从提示生成嵌入：

```py
>>> text_input = tokenizer(
...     prompt, padding="max_length", max_length=tokenizer.model_max_length, truncation=True, return_tensors="pt"
... )

>>> with torch.no_grad():
...     text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]
```

您还需要生成*无条件文本嵌入*，这些是填充令牌的嵌入。这些需要与条件`text_embeddings`具有相同的形状（`batch_size`和`seq_length`）：

```py
>>> max_length = text_input.input_ids.shape[-1]
>>> uncond_input = tokenizer([""] * batch_size, padding="max_length", max_length=max_length, return_tensors="pt")
>>> uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]
```

让我们将条件和无条件嵌入连接成一个批次，以避免进行两次前向传递：

```py
>>> text_embeddings = torch.cat([uncond_embeddings, text_embeddings])
```

### 创建随机噪声

接下来，生成一些初始随机噪声作为扩散过程的起点。这是图像的潜在表示，它将逐渐去噪。此时，`latent`图像比最终图像尺寸要小，但这没关系，因为模型稍后会将其转换为最终的512x512图像尺寸。

💡 高度和宽度除以8，因为`vae`模型有3个下采样层。您可以通过运行以下内容来检查：

```py
2 ** (len(vae.config.block_out_channels) - 1) == 8
```

```py
>>> latents = torch.randn(
...     (batch_size, unet.config.in_channels, height // 8, width // 8),
...     generator=generator,
...     device=torch_device,
... )
```

### 去噪图像

首先，通过初始噪声分布*sigma*对输入进行缩放，噪声比例值，这对于改进的调度器如[UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)是必需的：

```py
>>> latents = latents * scheduler.init_noise_sigma
```

最后一步是创建去噪循环，逐渐将`latents`中的纯噪声转换为由您的提示描述的图像。记住，去噪循环需要做三件事：

1.  设置调度器在去噪过程中要使用的时间步长。

1.  迭代时间步长。

1.  在每个时间步骤中，调用UNet模型来预测噪声残差，并将其传递给调度器以计算先前的嘈杂样本。

```py
>>> from tqdm.auto import tqdm

>>> scheduler.set_timesteps(num_inference_steps)

>>> for t in tqdm(scheduler.timesteps):
...     # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.
...     latent_model_input = torch.cat([latents] * 2)

...     latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)

...     # predict the noise residual
...     with torch.no_grad():
...         noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample

...     # perform guidance
...     noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
...     noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

...     # compute the previous noisy sample x_t -> x_t-1
...     latents = scheduler.step(noise_pred, t, latents).prev_sample
```

### 解码图像

最后一步是使用`vae`将潜在表示解码为图像，并使用`sample`获取解码输出：

```py
# scale and decode the image latents with vae
latents = 1 / 0.18215 * latents
with torch.no_grad():
    image = vae.decode(latents).sample
```

最后，将图像转换为`PIL.Image`以查看生成的图像！

```py
>>> image = (image / 2 + 0.5).clamp(0, 1).squeeze()
>>> image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()
>>> images = (image * 255).round().astype("uint8")
>>> image = Image.fromarray(image)
>>> image
```

![](../Images/4e7ad063cb2c61e9b5a381072c11e2f2.png)

## 下一步

从基本到复杂的管道，您已经看到，编写自己的扩散系统所需的只是一个去噪环。该循环应设置调度器的时间步长，对其进行迭代，并在调用 UNet 模型以预测噪声残差并将其传递给调度器以计算先前的嘈杂样本之间进行交替。

这确实是 🧨 Diffusers 的设计目的：使使用模型和调度器编写自己的扩散系统变得直观和简单。

接下来，您可以自由地：

+   学习如何[构建和贡献管道](../using-diffusers/contribute_pipeline)给 🧨 Diffusers。我们迫不及待地想看看您会想出什么！

+   在库中探索[现有的管道](../api/pipelines/overview)，看看您是否可以解构并从头开始构建一个管道，分别使用模型和调度器。
