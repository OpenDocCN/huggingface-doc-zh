- en: IDEFICS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/idefics](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/idefics)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The IDEFICS model was proposed in [OBELICS: An Open Web-Scale Filtered Dataset
    of Interleaved Image-Text Documents](https://huggingface.co/papers/2306.16527)
    by Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh,
    Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela,
    Matthieu Cord, Victor Sanh'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Large multimodal models trained on natural documents, which interleave images
    and text, outperform models trained on image-text pairs on various multimodal
    benchmarks that require reasoning over one or multiple images to generate a text.
    However, the datasets used to train these models have not been released, and the
    collection process has not been fully specified. We introduce the OBELICS dataset,
    an open web-scale filtered dataset of interleaved image-text documents comprising
    141 million web pages extracted from Common Crawl, 353 million associated images,
    and 115 billion text tokens. We describe the dataset creation process, present
    comprehensive filtering rules, and provide an analysis of the dataset’s content.
    To show the viability of OBELISC, we train an 80 billion parameters vision and
    language model on the dataset and obtain competitive performance on various multimodal
    benchmarks. We release the code to reproduce the dataset along with the dataset
    itself.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This model was contributed by [HuggingFaceM4](https://huggingface.co/HuggingFaceM4).
    The original code can be found [here](INSERT%20LINK%20TO%20GITHUB%20REPO%20HERE).
    (TODO: don’t have a public link yet).'
  prefs: []
  type: TYPE_NORMAL
- en: IDEFICS modeling code in Transformers is for finetuning and inferencing the
    pre-trained IDEFICS models.
  prefs: []
  type: TYPE_NORMAL
- en: To train a new IDEFICS model from scratch use the m4 codebase (a link will be
    provided once it’s made public)
  prefs: []
  type: TYPE_NORMAL
- en: IdeficsConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.IdeficsConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/configuration_idefics.py#L161)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`additional_vocab_size` (`int`, *optional`, defaults to 0) — Additional vocabulary
    size of the model, typically for the special ”” token. Additional vocab tokens
    are always trainable whereas regular vocab tokens can be frozen or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 32000) — Vocabulary size of the
    Idefics model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [~IdeficsModel](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 4096) — Dimension of the hidden
    representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intermediate_size` (`int`, *optional*, defaults to 11008) — Dimension of the
    MLP representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 32) — Number of hidden
    layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 32) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"silu"`) — The
    non-linear activation function (function or string) in the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha_initializer` (`str`, *optional*, defaults to `"zeros"`) — Initialization
    type for the alphas.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alphas_initializer_range` (`float`, *optional*, defaults to 0.0) — The standard
    deviation of the truncated_normal_initializer for initializing the alphas in the
    Gated Cross Attention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha_type` (`str`, *optional*, defaults to `"float"`) — Whether the gating
    alphas should be vectors or single floats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rms_norm_eps` (`float`, *optional*, defaults to 1e-6) — The epsilon used by
    the rms normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models). Only relevant
    if `config.is_decoder=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*, defaults to 0) — Padding token id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token_id` (`int`, *optional*, defaults to 1) — Beginning of stream token
    id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`int`, *optional*, defaults to 2) — End of stream token id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tie_word_embeddings(bool,` *optional*, defaults to `False`) — Whether to tie
    weight embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_layer_interval` (`int`, *optional*, default to 1) — Interval for cross
    attention (from text to image) layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qk_layer_norms` (`bool`, *optional*, defaults to `False`) — Whether to add
    layer norm after q and k'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`freeze_text_layers` (`bool`, *optional*, defaults to `True`) — Whether to
    freeze text layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`freeze_text_module_exceptions` (`bool`, *optional*, defaults to `[]`) — Exceptions
    to freezing text layers when `freeze_text_layers` is `True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`freeze_lm_head` (`bool`, *optional*, defaults to `False`) — Whether to freeze
    lm head'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`freeze_vision_layers` (`bool`, *optional*, defaults to `True`) — Whether to
    freeze vision layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`freeze_vision_module_exceptions` (`bool`, *optional*, defaults to `[]`) —
    Exceptions to freezing vision layers when `freeze_vision_layers` is `True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_resampler` (`bool`, *optional*, defaults to `False`) — Whether to use
    the Resampler'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vision_config` (`IdeficsVisionConfig`, *optional*) — Custom vision config
    or dict'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`perceiver_config` (`IdeficsPerceiverConfig`, *optional*) — Custom perceiver
    config or dict'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [IdeficsModel](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsModel).
    It is used to instantiate an Idefics model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Idefics-9B.
  prefs: []
  type: TYPE_NORMAL
- en: e.g. [HuggingFaceM4/idefics-9b](https://huggingface.co/HuggingFaceM4/idefics-9b)
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: IdeficsModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.IdeficsModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/modeling_idefics.py#L1064)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([IdeficsConfig](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights. config — IdeficsConfig'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare LLaMA Model outputting raw hidden-states without any specific head
    on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer decoder consisting of `config.num_hidden_layers` layers. Each layer
    is a `IdeficsDecoderLayer`
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/modeling_idefics.py#L1145)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`. [What are position
    IDs?](../glossary#position-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [IdeficsModel](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: IdeficsForVisionText2Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.IdeficsForVisionText2Text`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/modeling_idefics.py#L1403)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/modeling_idefics.py#L1461)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `modeling_opt._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.n_positions - 1]`. [What are position
    IDs?](../glossary#position-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Args — labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*): Labels for computing the masked language modeling loss. Indices should
    either be in `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring).
    Tokens with indices set to `-100` are ignored (masked), the loss is only computed
    for the tokens with labels in `[0, ..., config.vocab_size]`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast`
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([IdeficsConfig](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`image_hidden_states` (`tuple(torch.FloatTensor)`, *optional*) — Tuple of `torch.FloatTensor`
    (one for the output of the image embeddings, `(batch_size, num_images, sequence_length,
    hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: image_hidden_states of the model produced by the vision encoder, and optionally
    by the perceiver
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [IdeficsForVisionText2Text](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsForVisionText2Text)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: IdeficsImageProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.IdeficsImageProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/image_processing_idefics.py#L51)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`image_size` (`int`, *optional*, defaults to 224) — Resize to image size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IDEFICS_STANDARD_MEAN`)
    — Mean to use if normalizing the image. This is a float or list of floats the
    length of the number of channels in the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method. Can be overridden by the `image_mean` parameter
    in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IDEFICS_STANDARD_STD`)
    — Standard deviation to use if normalizing the image. This is a float or list
    of floats the length of the number of channels in the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method. Can be overridden by
    the `image_std` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_num_channels` (`int`, *optional*, defaults to 3) — Number of image channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a Idefics image processor.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `preprocess`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/image_processing_idefics.py#L87)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`ImageInput`) — A list of images to preprocess.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_size` (`int`, *optional*, defaults to `self.image_size`) — Resize to
    image size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_num_channels` (`int`, *optional*, defaults to `self.image_num_channels`)
    — Number of image channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IDEFICS_STANDARD_MEAN`)
    — Mean to use if normalizing the image. This is a float or list of floats the
    length of the number of channels in the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method. Can be overridden by the `image_mean` parameter
    in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IDEFICS_STANDARD_STD`)
    — Standard deviation to use if normalizing the image. This is a float or list
    of floats the length of the number of channels in the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method. Can be overridden by
    the `image_std` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transform` (`Callable`, *optional*, defaults to `None`) — A custom transform
    function that accepts a single image can be passed for training. For example,
    `torchvision.Compose` can be used to compose multiple transforms. If `None` -
    an inference mode is assumed - and then a preset of inference-specific transforms
    will be applied to the images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess a batch of images.
  prefs: []
  type: TYPE_NORMAL
- en: IdeficsProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.IdeficsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/processing_idefics.py#L108)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`image_processor` (`IdeficsImageProcessor`) — An instance of [IdeficsImageProcessor](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsImageProcessor).
    The image processor is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`LlamaTokenizerFast`) — An instance of [LlamaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizerFast).
    The tokenizer is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_size` (`int`, *optional*, defaults to 224) — Image size (assuming a
    square image)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a IDEFICS processor which wraps a LLama tokenizer and IDEFICS image
    processor into a single processor.
  prefs: []
  type: TYPE_NORMAL
- en: '[IdeficsProcessor](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsProcessor)
    offers all the functionalities of [IdeficsImageProcessor](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsImageProcessor)
    and [LlamaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizerFast).
    See the docstring of [**call**()](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsProcessor.__call__)
    and `decode()` for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/processing_idefics.py#L149)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompts` (`Union[List[TextInput], [List[List[TextInput]]]]`) — either a single
    prompt or a batched list of prompts - see the detailed description immediately
    after the end of the arguments doc section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — Maximum length of the returned list and
    optionally padding length (see above).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`bool`, *optional*) — Activates truncation to cut input sequences
    longer than `max_length` to `max_length`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transform` (`Callable`, *optional*) — A custom transform function that accepts
    a single image can be passed for training. For example, `torchvision.Compose`
    can be used to compose multiple functions. If `None` a preset inference-specific
    set of transforms will be applied to the images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_eos_token` (`bool`, *optional*, defaults to `False`) — Adds `eos_token`
    at the end of the final prompt if True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_end_of_utterance_token` (`bool`, *optional*) — Whether to automatically
    add `<end_of_utterance>` after each prompt’s text input (unless followed by an
    image). If `None` the tokenizer will be checked instead and if this token is found
    in `additional_special_tokens` then the value will be `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`debug` (`bool`, *optional*, defaults to `False`) — `True` value will help
    debug prompt generation by dumping useful information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or `TensorType`, *optional*, defaults to `TensorType.PYTORCH`)
    — The type of tensors to return. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: a dict with entries
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids`, `attention_mask`, `pixel_values`, `image_attention_mask` which
    can be directly passed to `model.generate`'
  prefs: []
  type: TYPE_NORMAL
- en: This method takes batched or non-batched prompts made of text and images and
    converts them into prompts that the model was trained on and prepares the image
    pixel values for the model to process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Detailed explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: Each entry in `prompts` is either a text to be passed as is or an image that
    will be processed.
  prefs: []
  type: TYPE_NORMAL
- en: An image can be either an image object (`PIL.Image`) or a url from which the
    image can be retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: When the processor encounters an image it’ll inject `<fake_token_around_image><image><fake_token_around_image>`
    entry into the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example the `prompts` will be converted into:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: and the two images will be massaged using [IdeficsImageProcessor.**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    method and placed inside the `pixel_values` dict entry of the return value.
  prefs: []
  type: TYPE_NORMAL
- en: This example also examplifies that images can be passed as objects or as text
    urls. It can be seen that the first image is passed as object and the second one
    as a url.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do training do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In order to help debug prompt generation enable `debug=True` which will show
    you what’s happening.
  prefs: []
  type: TYPE_NORMAL
