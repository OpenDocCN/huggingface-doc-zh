- en: LayoutXLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutxlm](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutxlm)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/161.c2908755.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LayoutXLM was proposed in [LayoutXLM: Multimodal Pre-training for Multilingual
    Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) by Yiheng
    Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang,
    Furu Wei. It’s a multilingual extension of the [LayoutLMv2 model](https://arxiv.org/abs/2012.14740)
    trained on 53 languages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Multimodal pre-training with text, layout, and image has achieved SOTA performance
    for visually-rich document understanding tasks recently, which demonstrates the
    great potential for joint learning across different modalities. In this paper,
    we present LayoutXLM, a multimodal pre-trained model for multilingual document
    understanding, which aims to bridge the language barriers for visually-rich document
    understanding. To accurately evaluate LayoutXLM, we also introduce a multilingual
    form understanding benchmark dataset named XFUN, which includes form understanding
    samples in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese),
    and key-value pairs are manually labeled for each language. Experiment results
    show that the LayoutXLM model has significantly outperformed the existing SOTA
    cross-lingual pre-trained models on the XFUN dataset.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code can be found [here](https://github.com/microsoft/unilm).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips and examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One can directly plug in the weights of LayoutXLM into a LayoutLMv2 model,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that LayoutXLM has its own tokenizer, based on [LayoutXLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer)/[LayoutXLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast).
    You can initialize it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Similar to LayoutLMv2, you can use [LayoutXLMProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor)
    (which internally applies [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)
    and [LayoutXLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer)/[LayoutXLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast)
    in sequence) to prepare all data for the model.
  prefs: []
  type: TYPE_NORMAL
- en: As LayoutXLM’s architecture is equivalent to that of LayoutLMv2, one can refer
    to [LayoutLMv2’s documentation page](layoutlmv2) for all tips, code examples and
    notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: LayoutXLMTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.LayoutXLMTokenizer'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L149)'
  prefs: []
  type: TYPE_NORMAL
- en: '( vocab_file bos_token = ''<s>'' eos_token = ''</s>'' sep_token = ''</s>''
    cls_token = ''<s>'' unk_token = ''<unk>'' pad_token = ''<pad>'' mask_token = ''<mask>''
    cls_token_box = [0, 0, 0, 0] sep_token_box = [1000, 1000, 1000, 1000] pad_token_box
    = [0, 0, 0, 0] pad_token_label = -100 only_label_first_subword = True sp_model_kwargs:
    Optional = None **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab_file** (`str`) — Path to the vocabulary file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bos_token** (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**eos_token** (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**sep_token** (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_token** (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unk_token** (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_token** (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mask_token** (`str`, *optional*, defaults to `"<mask>"`) — The token used
    for masking values. This is the token used when training this model with masked
    language modeling. This is the token which the model will try to predict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_token_box** (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — The
    bounding box to use for the special [CLS] token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sep_token_box** (`List[int]`, *optional*, defaults to `[1000, 1000, 1000,
    1000]`) — The bounding box to use for the special [SEP] token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_token_box** (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — The
    bounding box to use for the special [PAD] token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_token_label** (`int`, *optional*, defaults to -100) — The label to use
    for padding tokens. Defaults to -100, which is the `ignore_index` of PyTorch’s
    CrossEntropyLoss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**only_label_first_subword** (`bool`, *optional*, defaults to `True`) — Whether
    or not to only label the first subword, in case word labels are provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sp_model_kwargs** (`dict`, *optional*) — Will be passed to the `SentencePieceProcessor.__init__()`
    method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python)
    can be used, among other things, to set:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enable_sampling`: Enable subword regularization.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size = {0,1}`: No sampling is performed.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size > 1`: samples from the nbest_size results.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size < 0`: assuming that nbest_size is infinite and samples from the
    all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha`: Smoothing parameter for unigram sampling, and dropout probability
    of merge operations for BPE-dropout.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sp_model** (`SentencePieceProcessor`) — The *SentencePiece* processor that
    is used for every conversion (string, tokens and IDs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapted from [RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)
    and [XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer).
    Based on [SentencePiece](https://github.com/google/sentencepiece).
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L442)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: Union text_pair: Union = None boxes: Union = None word_labels: Union
    = None add_special_tokens: bool = True padding: Union = False truncation: Union
    = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional
    = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask:
    Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask:
    bool = False return_offsets_mapping: bool = False return_length: bool = False
    verbose: bool = True **kwargs ) → [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`, `List[str]`, `List[List[str]]`) — The sequence or batch of
    sequences to be encoded. Each sequence can be a string, a list of strings (words
    of a single example or questions of a batch of examples) or a list of list of
    strings (batch of words).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_pair** (`List[str]`, `List[List[str]]`) — The sequence or batch of sequences
    to be encoded. Each sequence should be a list of strings (pretokenized string).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**boxes** (`List[List[int]]`, `List[List[List[int]]]`) — Word-level bounding
    boxes. Each bounding box should be normalized to be on a 0-1000 scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**word_labels** (`List[int]`, `List[List[int]]`, *optional*) — Word-level integer
    labels (for token classification tasks such as FUNSD, CORD).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**add_special_tokens** (`bool`, *optional*, defaults to `True`) — Whether or
    not to encode the sequences with the special tokens relative to their model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**padding** (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**truncation** (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_length** (`int`, *optional*) — Controls the maximum length to use by
    one of the truncation/padding parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**stride** (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_to_multiple_of** (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_tensors** (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_token_type_ids** (`bool`, *optional*) — Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizer’s default, defined by the `return_outputs` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**return_attention_mask** (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizer’s default, defined by the `return_outputs` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**return_overflowing_tokens** (`bool`, *optional*, defaults to `False`) — Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_special_tokens_mask** (`bool`, *optional*, defaults to `False`) —
    Whether or not to return special tokens mask information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_offsets_mapping** (`bool`, *optional*, defaults to `False`) — Whether
    or not to return `(char_start, char_end)` for each token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Python’s tokenizer, this method will raise `NotImplementedError`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**return_length** (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the lengths of the encoded inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**verbose** (`bool`, *optional*, defaults to `True`) — Whether or not to print
    more information and warnings. **kwargs — passed to the `self.tokenize()` method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** — List of token ids to be fed to a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**bbox** — List of bounding boxes to be fed to a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** — List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *“token_type_ids”* is in `self.model_input_names`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**labels** — List of labels to be fed to a model. (when `word_labels` is specified).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**overflowing_tokens** — List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_truncated_tokens** — Number of tokens truncated (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**special_tokens_mask** — List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**length** — The length of the inputs (when `return_length=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences with word-level normalized bounding boxes
    and optional labels.
  prefs: []
  type: TYPE_NORMAL
- en: '#### build_inputs_with_special_tokens'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L314)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. An XLM-RoBERTa sequence has
    the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'single sequence: `<s> X </s>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'pair of sequences: `<s> A </s></s> B </s>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### get_special_tokens_mask'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L340)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens:
    bool = False ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**already_has_special_tokens** (`bool`, *optional*, defaults to `False`) —
    Whether or not the token list is already formatted with special tokens for the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '#### create_token_type_ids_from_sequences'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L368)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of zeros.
  prefs: []
  type: TYPE_NORMAL
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. XLM-RoBERTa does not make use of token type ids, therefore a list of zeros
    is returned.
  prefs: []
  type: TYPE_NORMAL
- en: '#### save_vocabulary'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L425)'
  prefs: []
  type: TYPE_NORMAL
- en: '( save_directory: str filename_prefix: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: LayoutXLMTokenizerFast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.LayoutXLMTokenizerFast'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py#L152)'
  prefs: []
  type: TYPE_NORMAL
- en: ( vocab_file = None tokenizer_file = None bos_token = '<s>' eos_token = '</s>'
    sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token
    = '<mask>' cls_token_box = [0, 0, 0, 0] sep_token_box = [1000, 1000, 1000, 1000]
    pad_token_box = [0, 0, 0, 0] pad_token_label = -100 only_label_first_subword =
    True **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab_file** (`str`) — Path to the vocabulary file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bos_token** (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**eos_token** (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**sep_token** (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_token** (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**unk_token** (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_token** (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mask_token** (`str`, *optional*, defaults to `"<mask>"`) — The token used
    for masking values. This is the token used when training this model with masked
    language modeling. This is the token which the model will try to predict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cls_token_box** (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — The
    bounding box to use for the special [CLS] token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sep_token_box** (`List[int]`, *optional*, defaults to `[1000, 1000, 1000,
    1000]`) — The bounding box to use for the special [SEP] token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_token_box** (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — The
    bounding box to use for the special [PAD] token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_token_label** (`int`, *optional*, defaults to -100) — The label to use
    for padding tokens. Defaults to -100, which is the `ignore_index` of PyTorch’s
    CrossEntropyLoss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**only_label_first_subword** (`bool`, *optional*, defaults to `True`) — Whether
    or not to only label the first subword, in case word labels are provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**additional_special_tokens** (`List[str]`, *optional*, defaults to `["<s>NOTUSED",
    "</s>NOTUSED"]`) — Additional special tokens used by the tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a “fast” LayoutXLM tokenizer (backed by HuggingFace’s *tokenizers*
    library). Adapted from [RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)
    and [XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer).
    Based on [BPE](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models).
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py#L272)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text: Union text_pair: Union = None boxes: Union = None word_labels: Union
    = None add_special_tokens: bool = True padding: Union = False truncation: Union
    = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional
    = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask:
    Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask:
    bool = False return_offsets_mapping: bool = False return_length: bool = False
    verbose: bool = True **kwargs ) → [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**text** (`str`, `List[str]`, `List[List[str]]`) — The sequence or batch of
    sequences to be encoded. Each sequence can be a string, a list of strings (words
    of a single example or questions of a batch of examples) or a list of list of
    strings (batch of words).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_pair** (`List[str]`, `List[List[str]]`) — The sequence or batch of sequences
    to be encoded. Each sequence should be a list of strings (pretokenized string).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**boxes** (`List[List[int]]`, `List[List[List[int]]]`) — Word-level bounding
    boxes. Each bounding box should be normalized to be on a 0-1000 scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**word_labels** (`List[int]`, `List[List[int]]`, *optional*) — Word-level integer
    labels (for token classification tasks such as FUNSD, CORD).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**add_special_tokens** (`bool`, *optional*, defaults to `True`) — Whether or
    not to encode the sequences with the special tokens relative to their model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**padding** (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**truncation** (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_length** (`int`, *optional*) — Controls the maximum length to use by
    one of the truncation/padding parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**stride** (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_to_multiple_of** (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_tensors** (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_token_type_ids** (`bool`, *optional*) — Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizer’s default, defined by the `return_outputs` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**return_attention_mask** (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizer’s default, defined by the `return_outputs` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**return_overflowing_tokens** (`bool`, *optional*, defaults to `False`) — Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_special_tokens_mask** (`bool`, *optional*, defaults to `False`) —
    Whether or not to return special tokens mask information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_offsets_mapping** (`bool`, *optional*, defaults to `False`) — Whether
    or not to return `(char_start, char_end)` for each token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Python’s tokenizer, this method will raise `NotImplementedError`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**return_length** (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the lengths of the encoded inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**verbose** (`bool`, *optional*, defaults to `True`) — Whether or not to print
    more information and warnings. **kwargs — passed to the `self.tokenize()` method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** — List of token ids to be fed to a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**bbox** — List of bounding boxes to be fed to a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** — List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *“token_type_ids”* is in `self.model_input_names`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attention_mask** — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**labels** — List of labels to be fed to a model. (when `word_labels` is specified).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**overflowing_tokens** — List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_truncated_tokens** — Number of tokens truncated (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**special_tokens_mask** — List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**length** — The length of the inputs (when `return_length=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences with word-level normalized bounding boxes
    and optional labels.
  prefs: []
  type: TYPE_NORMAL
- en: LayoutXLMProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.LayoutXLMProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/processing_layoutxlm.py#L26)'
  prefs: []
  type: TYPE_NORMAL
- en: ( image_processor = None tokenizer = None **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image_processor** (`LayoutLMv2ImageProcessor`, *optional*) — An instance
    of [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor).
    The image processor is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tokenizer** (`LayoutXLMTokenizer` or `LayoutXLMTokenizerFast`, *optional*)
    — An instance of [LayoutXLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer)
    or [LayoutXLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast).
    The tokenizer is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a LayoutXLM processor which combines a LayoutXLM image processor
    and a LayoutXLM tokenizer into a single processor.
  prefs: []
  type: TYPE_NORMAL
- en: '[LayoutXLMProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor)
    offers all the functionalities you need to prepare data for the model.'
  prefs: []
  type: TYPE_NORMAL
- en: It first uses [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)
    to resize document images to a fixed size, and optionally applies OCR to get words
    and normalized bounding boxes. These are then provided to [LayoutXLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer)
    or [LayoutXLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast),
    which turns the words and bounding boxes into token-level `input_ids`, `attention_mask`,
    `token_type_ids`, `bbox`. Optionally, one can provide integer `word_labels`, which
    are turned into token-level `labels` for token classification tasks (such as FUNSD,
    CORD).
  prefs: []
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/processing_layoutxlm.py#L67)'
  prefs: []
  type: TYPE_NORMAL
- en: '( images text: Union = None text_pair: Union = None boxes: Union = None word_labels:
    Union = None add_special_tokens: bool = True padding: Union = False truncation:
    Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional
    = None return_token_type_ids: Optional = None return_attention_mask: Optional
    = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool
    = False return_offsets_mapping: bool = False return_length: bool = False verbose:
    bool = True return_tensors: Union = None **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: This method first forwards the `images` argument to `~LayoutLMv2ImagePrpcessor.__call__`.
    In case `LayoutLMv2ImagePrpcessor` was initialized with `apply_ocr` set to `True`,
    it passes the obtained words and bounding boxes along with the additional arguments
    to [**call**()](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer.__call__)
    and returns the output, together with resized `images`. In case `LayoutLMv2ImagePrpcessor`
    was initialized with `apply_ocr` set to `False`, it passes the words (`text`/`text_pair`)
    and `boxes` specified by the user along with the additional arguments to [__call__()](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer.__call__)
    and returns the output, together with resized `images`.
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the docstring of the above two methods for more information.
  prefs: []
  type: TYPE_NORMAL
