- en: Image-to-image
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ°å›¾åƒ
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/img2img](https://huggingface.co/docs/diffusers/using-diffusers/img2img)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/using-diffusers/img2img](https://huggingface.co/docs/diffusers/using-diffusers/img2img)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Image-to-image is similar to [text-to-image](conditional_image_generation),
    but in addition to a prompt, you can also pass an initial image as a starting
    point for the diffusion process. The initial image is encoded to latent space
    and noise is added to it. Then the latent diffusion model takes a prompt and the
    noisy latent image, predicts the added noise, and removes the predicted noise
    from the initial latent image to get the new latent image. Lastly, a decoder decodes
    the new latent image back into an image.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ°å›¾åƒç±»ä¼¼äº [æ–‡æœ¬åˆ°å›¾åƒ](conditional_image_generation)ï¼Œä½†é™¤äº†ä¸€ä¸ªæç¤ºå¤–ï¼Œæ‚¨è¿˜å¯ä»¥ä¼ é€’ä¸€ä¸ªåˆå§‹å›¾åƒä½œä¸ºæ‰©æ•£è¿‡ç¨‹çš„èµ·ç‚¹ã€‚åˆå§‹å›¾åƒè¢«ç¼–ç åˆ°æ½œåœ¨ç©ºé—´ä¸­ï¼Œå¹¶å‘å…¶æ·»åŠ å™ªå£°ã€‚ç„¶åï¼Œæ½œåœ¨æ‰©æ•£æ¨¡å‹æ¥æ”¶ä¸€ä¸ªæç¤ºå’Œå˜ˆæ‚çš„æ½œåœ¨å›¾åƒï¼Œé¢„æµ‹æ·»åŠ çš„å™ªå£°ï¼Œå¹¶ä»åˆå§‹æ½œåœ¨å›¾åƒä¸­å»é™¤é¢„æµ‹çš„å™ªå£°ä»¥è·å¾—æ–°çš„æ½œåœ¨å›¾åƒã€‚æœ€åï¼Œè§£ç å™¨å°†æ–°çš„æ½œåœ¨å›¾åƒè§£ç å›å›¾åƒã€‚
- en: 'With ğŸ¤— Diffusers, this is as easy as 1-2-3:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ ğŸ¤— Diffusersï¼Œè¿™å°±åƒ 1-2-3 ä¸€æ ·ç®€å•ï¼š
- en: 'Load a checkpoint into the [AutoPipelineForImage2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForImage2Image)
    class; this pipeline automatically handles loading the correct pipeline class
    based on the checkpoint:'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ£€æŸ¥ç‚¹åŠ è½½åˆ° [AutoPipelineForImage2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForImage2Image)
    ç±»ä¸­ï¼›æ­¤ç®¡é“ä¼šæ ¹æ®æ£€æŸ¥ç‚¹è‡ªåŠ¨å¤„ç†åŠ è½½æ­£ç¡®çš„ç®¡é“ç±»ï¼š
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Youâ€™ll notice throughout the guide, we use [enable_model_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/gligen#diffusers.StableDiffusionGLIGENTextImagePipeline.enable_model_cpu_offload)
    and [enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_xformers_memory_efficient_attention),
    to save memory and increase inference speed. If youâ€™re using PyTorch 2.0, then
    you donâ€™t need to call [enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_xformers_memory_efficient_attention)
    on your pipeline because itâ€™ll already be using PyTorch 2.0â€™s native [scaled-dot
    product attention](../optimization/torch2.0#scaled-dot-product-attention).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•´ä¸ªæŒ‡å—ä¸­ï¼Œæ‚¨ä¼šæ³¨æ„åˆ°æˆ‘ä»¬ä½¿ç”¨ [enable_model_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/gligen#diffusers.StableDiffusionGLIGENTextImagePipeline.enable_model_cpu_offload)
    å’Œ [enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_xformers_memory_efficient_attention)ï¼Œä»¥èŠ‚çœå†…å­˜å¹¶å¢åŠ æ¨ç†é€Ÿåº¦ã€‚å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯
    PyTorch 2.0ï¼Œåˆ™ä¸éœ€è¦åœ¨ç®¡é“ä¸Šè°ƒç”¨ [enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_xformers_memory_efficient_attention)ï¼Œå› ä¸ºå®ƒå·²ç»åœ¨ä½¿ç”¨
    PyTorch 2.0 çš„æœ¬æœº [scaled-dot product attention](../optimization/torch2.0#scaled-dot-product-attention)ã€‚
- en: 'Load an image to pass to the pipeline:'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åŠ è½½ä¸€ä¸ªå›¾åƒä¼ é€’ç»™ç®¡é“ï¼š
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Pass a prompt and image to the pipeline to generate an image:'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¼ é€’ä¸€ä¸ªæç¤ºå’Œå›¾åƒç»™ç®¡é“ä»¥ç”Ÿæˆä¸€ä¸ªå›¾åƒï¼š
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/b7dea39428bc1d2cd88a6400783552cb.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7dea39428bc1d2cd88a6400783552cb.png)'
- en: initial image
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹å›¾åƒ
- en: '![](../Images/54142d2106911436dd03998048a92fad.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54142d2106911436dd03998048a92fad.png)'
- en: generated image
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆçš„å›¾åƒ
- en: Popular models
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æµè¡Œçš„æ¨¡å‹
- en: The most popular image-to-image models are [Stable Diffusion v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5),
    [Stable Diffusion XL (SDXL)](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0),
    and [Kandinsky 2.2](https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder).
    The results from the Stable Diffusion and Kandinsky models vary due to their architecture
    differences and training process; you can generally expect SDXL to produce higher
    quality images than Stable Diffusion v1.5\. Letâ€™s take a quick look at how to
    use each of these models and compare their results.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å—æ¬¢è¿çš„å›¾åƒåˆ°å›¾åƒæ¨¡å‹æ˜¯ [ç¨³å®šæ‰©æ•£ v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5)ã€[ç¨³å®šæ‰©æ•£
    XL (SDXL)](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) å’Œ
    [Kandinsky 2.2](https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder)ã€‚ç”±äºå®ƒä»¬çš„æ¶æ„å·®å¼‚å’Œè®­ç»ƒè¿‡ç¨‹çš„ä¸åŒï¼Œç¨³å®šæ‰©æ•£å’Œ
    Kandinsky æ¨¡å‹çš„ç»“æœä¼šæœ‰æ‰€ä¸åŒï¼›é€šå¸¸å¯ä»¥é¢„æœŸ SDXL ä¼šäº§ç”Ÿæ¯”ç¨³å®šæ‰©æ•£ v1.5 æ›´é«˜è´¨é‡çš„å›¾åƒã€‚è®©æˆ‘ä»¬å¿«é€Ÿçœ‹çœ‹å¦‚ä½•ä½¿ç”¨è¿™äº›æ¨¡å‹å¹¶æ¯”è¾ƒå®ƒä»¬çš„ç»“æœã€‚
- en: Stable Diffusion v1.5
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¨³å®šæ‰©æ•£ v1.5
- en: 'Stable Diffusion v1.5 is a latent diffusion model initialized from an earlier
    checkpoint, and further finetuned for 595K steps on 512x512 images. To use this
    pipeline for image-to-image, youâ€™ll need to prepare an initial image to pass to
    the pipeline. Then you can pass a prompt and the image to the pipeline to generate
    a new image:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç¨³å®šæ‰©æ•£ v1.5 æ˜¯ä¸€ä¸ªä»æ—©æœŸæ£€æŸ¥ç‚¹åˆå§‹åŒ–çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå¹¶åœ¨ 512x512 å›¾åƒä¸Šè¿›ä¸€æ­¥å¾®è°ƒäº† 595K æ­¥ã€‚è¦å°†æ­¤ç®¡é“ç”¨äºå›¾åƒåˆ°å›¾åƒï¼Œæ‚¨éœ€è¦å‡†å¤‡ä¸€ä¸ªåˆå§‹å›¾åƒä¼ é€’ç»™ç®¡é“ã€‚ç„¶åï¼Œæ‚¨å¯ä»¥ä¼ é€’ä¸€ä¸ªæç¤ºå’Œå›¾åƒç»™ç®¡é“ä»¥ç”Ÿæˆä¸€ä¸ªæ–°å›¾åƒï¼š
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/36dc16e92be7b4f357b7201ad98da118.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36dc16e92be7b4f357b7201ad98da118.png)'
- en: initial image
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹å›¾åƒ
- en: '![](../Images/0145794767dd4ef55a2071169ec77aa2.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0145794767dd4ef55a2071169ec77aa2.png)'
- en: generated image
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆçš„å›¾åƒ
- en: Stable Diffusion XL (SDXL)
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¨³å®šæ‰©æ•£ XL (SDXL)
- en: SDXL is a more powerful version of the Stable Diffusion model. It uses a larger
    base model, and an additional refiner model to increase the quality of the base
    modelâ€™s output. Read the [SDXL](sdxl) guide for a more detailed walkthrough of
    how to use this model, and other techniques it uses to produce high quality images.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL æ˜¯ç¨³å®šæ‰©æ•£æ¨¡å‹çš„æ›´å¼ºå¤§ç‰ˆæœ¬ã€‚å®ƒä½¿ç”¨ä¸€ä¸ªæ›´å¤§çš„åŸºç¡€æ¨¡å‹ï¼Œä»¥åŠä¸€ä¸ªé¢å¤–çš„ç»†åŒ–æ¨¡å‹æ¥æé«˜åŸºç¡€æ¨¡å‹è¾“å‡ºçš„è´¨é‡ã€‚é˜…è¯» [SDXL](sdxl) æŒ‡å—ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨æ­¤æ¨¡å‹ä»¥åŠå®ƒç”¨äºç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„å…¶ä»–æŠ€æœ¯ã€‚
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/2fe2edb85787ba08991e059ff912c98c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fe2edb85787ba08991e059ff912c98c.png)'
- en: initial image
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹å›¾åƒ
- en: '![](../Images/7ec0479f09a14b51ed59ccd01d6f93c9.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ec0479f09a14b51ed59ccd01d6f93c9.png)'
- en: generated image
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆçš„å›¾åƒ
- en: Kandinsky 2.2
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kandinsky 2.2
- en: The Kandinsky model is different from the Stable Diffusion models because it
    uses an image prior model to create image embeddings. The embeddings help create
    a better alignment between text and images, allowing the latent diffusion model
    to generate better images.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Kandinsky æ¨¡å‹ä¸ç¨³å®šæ‰©æ•£æ¨¡å‹ä¸åŒï¼Œå› ä¸ºå®ƒä½¿ç”¨å›¾åƒå…ˆéªŒæ¨¡å‹æ¥åˆ›å»ºå›¾åƒåµŒå…¥ã€‚è¿™äº›åµŒå…¥æœ‰åŠ©äºåœ¨æ–‡æœ¬å’Œå›¾åƒä¹‹é—´åˆ›å»ºæ›´å¥½çš„å¯¹é½ï¼Œä½¿æ½œåœ¨æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ›´å¥½çš„å›¾åƒã€‚
- en: 'The simplest way to use Kandinsky 2.2 is:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Kandinsky 2.2 çš„æœ€ç®€å•æ–¹æ³•æ˜¯ï¼š
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/36dc16e92be7b4f357b7201ad98da118.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36dc16e92be7b4f357b7201ad98da118.png)'
- en: initial image
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹å›¾åƒ
- en: '![](../Images/97dcd8328035f392ab7d774997117ed0.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97dcd8328035f392ab7d774997117ed0.png)'
- en: generated image
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆçš„å›¾åƒ
- en: Configure pipeline parameters
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é…ç½®ç®¡é“å‚æ•°
- en: There are several important parameters you can configure in the pipeline thatâ€™ll
    affect the image generation process and image quality. Letâ€™s take a closer look
    at what these parameters do and how changing them affects the output.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é…ç½®ç®¡é“ä¸­çš„å‡ ä¸ªé‡è¦å‚æ•°ï¼Œè¿™äº›å‚æ•°å°†å½±å“å›¾åƒç”Ÿæˆè¿‡ç¨‹å’Œå›¾åƒè´¨é‡ã€‚è®©æˆ‘ä»¬æ›´ä»”ç»†åœ°çœ‹çœ‹è¿™äº›å‚æ•°çš„ä½œç”¨ä»¥åŠå¦‚ä½•æ›´æ”¹å®ƒä»¬ä¼šå½±å“è¾“å‡ºã€‚
- en: Strength
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¼ºåº¦
- en: '`strength` is one of the most important parameters to consider and itâ€™ll have
    a huge impact on your generated image. It determines how much the generated image
    resembles the initial image. In other words:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`strength`æ˜¯è¦è€ƒè™‘çš„æœ€é‡è¦å‚æ•°ä¹‹ä¸€ï¼Œå®ƒå°†å¯¹ç”Ÿæˆçš„å›¾åƒäº§ç”Ÿå·¨å¤§å½±å“ã€‚å®ƒå†³å®šäº†ç”Ÿæˆçš„å›¾åƒä¸åˆå§‹å›¾åƒçš„ç›¸ä¼¼ç¨‹åº¦ã€‚æ¢å¥è¯è¯´ï¼š'
- en: ğŸ“ˆ a higher `strength` value gives the model more â€œcreativityâ€ to generate an
    image thatâ€™s different from the initial image; a `strength` value of 1.0 means
    the initial image is more or less ignored
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ“ˆè¾ƒé«˜çš„`strength`å€¼ä½¿æ¨¡å‹æ›´å…·â€œåˆ›é€ åŠ›â€ï¼Œå¯ä»¥ç”Ÿæˆä¸åˆå§‹å›¾åƒä¸åŒçš„å›¾åƒï¼›`strength`å€¼ä¸º1.0æ„å‘³ç€åˆå§‹å›¾åƒæˆ–å¤šæˆ–å°‘è¢«å¿½ç•¥
- en: ğŸ“‰ a lower `strength` value means the generated image is more similar to the
    initial image
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ“‰è¾ƒä½çš„`strength`å€¼æ„å‘³ç€ç”Ÿæˆçš„å›¾åƒæ›´ç±»ä¼¼äºåˆå§‹å›¾åƒ
- en: The `strength` and `num_inference_steps` parameters are related because `strength`
    determines the number of noise steps to add. For example, if the `num_inference_steps`
    is 50 and `strength` is 0.8, then this means adding 40 (50 * 0.8) steps of noise
    to the initial image and then denoising for 40 steps to get the newly generated
    image.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`strength`å’Œ`num_inference_steps`å‚æ•°ç›¸å…³ï¼Œå› ä¸º`strength`ç¡®å®šè¦æ·»åŠ çš„å™ªå£°æ­¥æ•°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ`num_inference_steps`ä¸º50ï¼Œ`strength`ä¸º0.8ï¼Œåˆ™è¿™æ„å‘³ç€å‘åˆå§‹å›¾åƒæ·»åŠ 40ï¼ˆ50
    * 0.8ï¼‰æ­¥å™ªå£°ï¼Œç„¶åå»å™ª40æ­¥ä»¥è·å¾—æ–°ç”Ÿæˆçš„å›¾åƒã€‚'
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/b18d4d9ef1fda116b32a37eb344dd61d.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b18d4d9ef1fda116b32a37eb344dd61d.png)'
- en: strength = 0.4
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: strength = 0.4
- en: '![](../Images/caabae187343ac728e132d95979f20d9.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/caabae187343ac728e132d95979f20d9.png)'
- en: strength = 0.6
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: strength = 0.6
- en: '![](../Images/6dd3ccf15afa5473f942682270943955.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6dd3ccf15afa5473f942682270943955.png)'
- en: strength = 1.0
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: strength = 1.0
- en: Guidance scale
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æŒ‡å¯¼å°ºåº¦
- en: The `guidance_scale` parameter is used to control how closely aligned the generated
    image and text prompt are. A higher `guidance_scale` value means your generated
    image is more aligned with the prompt, while a lower `guidance_scale` value means
    your generated image has more space to deviate from the prompt.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`guidance_scale`å‚æ•°ç”¨äºæ§åˆ¶ç”Ÿæˆçš„å›¾åƒå’Œæ–‡æœ¬æç¤ºä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚è¾ƒé«˜çš„`guidance_scale`å€¼æ„å‘³ç€æ‚¨ç”Ÿæˆçš„å›¾åƒä¸æç¤ºæ›´åŠ å¯¹é½ï¼Œè€Œè¾ƒä½çš„`guidance_scale`å€¼æ„å‘³ç€æ‚¨ç”Ÿæˆçš„å›¾åƒæœ‰æ›´å¤šçš„ç©ºé—´åç¦»æç¤ºã€‚'
- en: You can combine `guidance_scale` with `strength` for even more precise control
    over how expressive the model is. For example, combine a high `strength + guidance_scale`
    for maximum creativity or use a combination of low `strength` and low `guidance_scale`
    to generate an image that resembles the initial image but is not as strictly bound
    to the prompt.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥å°†`guidance_scale`ä¸`strength`ç»“åˆä½¿ç”¨ï¼Œä»¥æ›´ç²¾ç¡®åœ°æ§åˆ¶æ¨¡å‹çš„è¡¨ç°åŠ›ã€‚ä¾‹å¦‚ï¼Œç»“åˆé«˜`strength + guidance_scale`ä»¥è·å¾—æœ€å¤§åˆ›é€ åŠ›ï¼Œæˆ–è€…ä½¿ç”¨ä½`strength`å’Œä½`guidance_scale`çš„ç»„åˆç”Ÿæˆç±»ä¼¼äºåˆå§‹å›¾åƒä½†ä¸ä¸¥æ ¼å—é™äºæç¤ºçš„å›¾åƒã€‚
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/8f312f9d19115eba39cce061ca99ee11.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f312f9d19115eba39cce061ca99ee11.png)'
- en: guidance_scale = 0.1
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: guidance_scale = 0.1
- en: '![](../Images/ee6aee8a1bb3c419a6a94dd5b4a870b8.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee6aee8a1bb3c419a6a94dd5b4a870b8.png)'
- en: guidance_scale = 5.0
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: guidance_scale = 5.0
- en: '![](../Images/94e53acaee818fe2a3dddf5671819e4e.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94e53acaee818fe2a3dddf5671819e4e.png)'
- en: guidance_scale = 10.0
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: guidance_scale = 10.0
- en: Negative prompt
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è´Ÿæç¤º
- en: A negative prompt conditions the model to *not* include things in an image,
    and it can be used to improve image quality or modify an image. For example, you
    can improve image quality by including negative prompts like â€œpoor detailsâ€ or
    â€œblurryâ€ to encourage the model to generate a higher quality image. Or you can
    modify an image by specifying things to exclude from an image.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è´Ÿæç¤ºæ¡ä»¶æ¨¡å‹*ä¸*åœ¨å›¾åƒä¸­åŒ…å«æŸäº›å†…å®¹ï¼Œå¯ç”¨äºæ”¹å–„å›¾åƒè´¨é‡æˆ–ä¿®æ”¹å›¾åƒã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥é€šè¿‡åŒ…å«â€œç»†èŠ‚å·®â€æˆ–â€œæ¨¡ç³Šâ€ç­‰è´Ÿæç¤ºæ¥æ”¹å–„å›¾åƒè´¨é‡ï¼Œä»¥é¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´é«˜è´¨é‡çš„å›¾åƒã€‚æˆ–è€…æ‚¨å¯ä»¥é€šè¿‡æŒ‡å®šè¦ä»å›¾åƒä¸­æ’é™¤çš„å†…å®¹æ¥ä¿®æ”¹å›¾åƒã€‚
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/b0371dfac3fc42c122a50da2e48f809a.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0371dfac3fc42c122a50da2e48f809a.png)'
- en: negative_prompt = "ugly, deformed, disfigured, poor details, bad anatomy"
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: negative_prompt = "ä¸‘é™‹ï¼Œç•¸å½¢ï¼Œæ¯å®¹ï¼Œç»†èŠ‚å·®ï¼Œè§£å‰–ä¸è‰¯"
- en: '![](../Images/420cf48fd86187c0fb09679e8d6a92bd.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/420cf48fd86187c0fb09679e8d6a92bd.png)'
- en: negative_prompt = "jungle"
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: negative_prompt = "ä¸›æ—"
- en: Chained image-to-image pipelines
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é“¾å¼å›¾åƒåˆ°å›¾åƒç®¡é“
- en: There are some other interesting ways you can use an image-to-image pipeline
    aside from just generating an image (although that is pretty cool too). You can
    take it a step further and chain it with other pipelines.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ç”Ÿæˆå›¾åƒä¹‹å¤–ï¼Œæ‚¨è¿˜å¯ä»¥ä»¥å…¶ä»–æœ‰è¶£çš„æ–¹å¼ä½¿ç”¨å›¾åƒåˆ°å›¾åƒç®¡é“ï¼ˆå°½ç®¡ç”Ÿæˆå›¾åƒä¹Ÿå¾ˆé…·ï¼‰ã€‚æ‚¨å¯ä»¥è¿›ä¸€æ­¥å°†å…¶ä¸å…¶ä»–ç®¡é“é“¾æ¥èµ·æ¥ã€‚
- en: Text-to-image-to-image
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°å›¾åƒåˆ°å›¾åƒ
- en: Chaining a text-to-image and image-to-image pipeline allows you to generate
    an image from text and use the generated image as the initial image for the image-to-image
    pipeline. This is useful if you want to generate an image entirely from scratch.
    For example, letâ€™s chain a Stable Diffusion and a Kandinsky model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒåˆ°å›¾åƒç®¡é“é“¾æ¥åœ¨ä¸€èµ·ï¼Œå¯ä»¥ä»æ–‡æœ¬ç”Ÿæˆå›¾åƒï¼Œå¹¶å°†ç”Ÿæˆçš„å›¾åƒç”¨ä½œå›¾åƒåˆ°å›¾åƒç®¡é“çš„åˆå§‹å›¾åƒã€‚å¦‚æœæ‚¨æƒ³å®Œå…¨ä»å¤´å¼€å§‹ç”Ÿæˆå›¾åƒï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬é“¾æ¥ä¸€ä¸ªç¨³å®šæ‰©æ•£å’Œä¸€ä¸ªåº·å®šæ–¯åŸºæ¨¡å‹ã€‚
- en: 'Start by generating an image with the text-to-image pipeline:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒç®¡é“ç”Ÿæˆä¸€ä¸ªå›¾åƒï¼š
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now you can pass this generated image to the image-to-image pipeline:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥å°†ç”Ÿæˆçš„å›¾åƒä¼ é€’ç»™å›¾åƒåˆ°å›¾åƒç®¡é“ï¼š
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Image-to-image-to-image
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ°å›¾åƒåˆ°å›¾åƒ
- en: You can also chain multiple image-to-image pipelines together to create more
    interesting images. This can be useful for iteratively performing style transfer
    on an image, generating short GIFs, restoring color to an image, or restoring
    missing areas of an image.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥å°†å¤šä¸ªå›¾åƒåˆ°å›¾åƒç®¡é“é“¾æ¥åœ¨ä¸€èµ·ï¼Œä»¥åˆ›å»ºæ›´æœ‰è¶£çš„å›¾åƒã€‚è¿™å¯¹äºåœ¨å›¾åƒä¸Šè¿­ä»£æ‰§è¡Œé£æ ¼è½¬ç§»ã€ç”ŸæˆçŸ­GIFã€æ¢å¤å›¾åƒçš„é¢œè‰²æˆ–æ¢å¤å›¾åƒä¸­ç¼ºå¤±çš„åŒºåŸŸéå¸¸æœ‰ç”¨ã€‚
- en: 'Start by generating an image:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆç”Ÿæˆä¸€ä¸ªå›¾åƒï¼š
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It is important to specify `output_type="latent"` in the pipeline to keep all
    the outputs in latent space to avoid an unnecessary decode-encode step. This only
    works if the chained pipelines are using the same VAE.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Pass the latent output from this pipeline to the next pipeline to generate
    an image in a [comic book art style](https://huggingface.co/ogkalu/Comic-Diffusion):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Repeat one more time to generate the final image in a [pixel art style](https://huggingface.co/kohbanye/pixel-art-style):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Image-to-upscaler-to-super-resolution
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another way you can chain your image-to-image pipeline is with an upscaler and
    super-resolution pipeline to really increase the level of details in an image.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with an image-to-image pipeline:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It is important to specify `output_type="latent"` in the pipeline to keep all
    the outputs in *latent* space to avoid an unnecessary decode-encode step. This
    only works if the chained pipelines are using the same VAE.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Chain it to an upscaler pipeline to increase the image resolution:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, chain it to a super-resolution pipeline to further enhance the resolution:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Control image generation
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Trying to generate an image that looks exactly the way you want can be difficult,
    which is why controlled generation techniques and models are so useful. While
    you can use the `negative_prompt` to partially control image generation, there
    are more robust methods like prompt weighting and ControlNets.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Prompt weighting
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompt weighting allows you to scale the representation of each concept in a
    prompt. For example, in a prompt like â€œAstronaut in a jungle, cold color palette,
    muted colors, detailed, 8kâ€, you can choose to increase or decrease the embeddings
    of â€œastronautâ€ and â€œjungleâ€. The [Compel](https://github.com/damian0815/compel)
    library provides a simple syntax for adjusting prompt weights and generating the
    embeddings. You can learn how to create the embeddings in the [Prompt weighting](weighted_prompts)
    guide.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[AutoPipelineForImage2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForImage2Image)
    has a `prompt_embeds` (and `negative_prompt_embeds` if youâ€™re using a negative
    prompt) parameter where you can pass the embeddings which replaces the `prompt`
    parameter.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ControlNet
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ControlNets provide a more flexible and accurate way to control image generation
    because you can use an additional conditioning image. The conditioning image can
    be a canny image, depth map, image segmentation, and even scribbles! Whatever
    type of conditioning image you choose, the ControlNet generates an image that
    preserves the information in it.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: For example, letâ€™s condition an image with a depth map to keep the spatial information
    in the image.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Load a ControlNet model conditioned on depth maps and the [AutoPipelineForImage2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForImage2Image):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now generate a new image conditioned on the depth map, initial image, and prompt:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/36dc16e92be7b4f357b7201ad98da118.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: initial image
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/795c4016502683bcda2f00fa33a84c2d.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: depth image
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e425aa0e6403b0304599567d11b91a4d.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: ControlNet image
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s apply a new [style](https://huggingface.co/nitrosocke/elden-ring-diffusion)
    to the image generated from the ControlNet by chaining it with an image-to-image
    pipeline:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/3d19fb4449a5e38b2448796e3bf6cf11.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: Optimize
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Running diffusion models is computationally expensive and intensive, but with
    a few optimization tricks, it is entirely possible to run them on consumer and
    free-tier GPUs. For example, you can use a more memory-efficient form of attention
    such as PyTorch 2.0â€™s [scaled-dot product attention](../optimization/torch2.0#scaled-dot-product-attention)
    or [xFormers](../optimization/xformers) (you can use one or the other, but thereâ€™s
    no need to use both). You can also offload the model to the GPU while the other
    pipeline components wait on the CPU.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'With [`torch.compile`](../optimization/torch2.0#torchcompile), you can boost
    your inference speed even more by wrapping your UNet with it:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[`torch.compile`](../optimization/torch2.0#torchcompile)ï¼Œæ‚¨å¯ä»¥é€šè¿‡å°†UNetä¸å…¶åŒ…è£…æ¥è¿›ä¸€æ­¥æé«˜æ¨ç†é€Ÿåº¦ï¼š
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: To learn more, take a look at the [Reduce memory usage](../optimization/memory)
    and [Torch 2.0](../optimization/torch2.0) guides.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[å‡å°‘å†…å­˜ä½¿ç”¨](../optimization/memory)å’Œ[Torch 2.0](../optimization/torch2.0)æŒ‡å—ã€‚
