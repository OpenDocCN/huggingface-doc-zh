- en: Image-to-image
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像到图像
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/img2img](https://huggingface.co/docs/diffusers/using-diffusers/img2img)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/using-diffusers/img2img](https://huggingface.co/docs/diffusers/using-diffusers/img2img)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Image-to-image is similar to [text-to-image](conditional_image_generation),
    but in addition to a prompt, you can also pass an initial image as a starting
    point for the diffusion process. The initial image is encoded to latent space
    and noise is added to it. Then the latent diffusion model takes a prompt and the
    noisy latent image, predicts the added noise, and removes the predicted noise
    from the initial latent image to get the new latent image. Lastly, a decoder decodes
    the new latent image back into an image.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图像到图像类似于 [文本到图像](conditional_image_generation)，但除了一个提示外，您还可以传递一个初始图像作为扩散过程的起点。初始图像被编码到潜在空间中，并向其添加噪声。然后，潜在扩散模型接收一个提示和嘈杂的潜在图像，预测添加的噪声，并从初始潜在图像中去除预测的噪声以获得新的潜在图像。最后，解码器将新的潜在图像解码回图像。
- en: 'With 🤗 Diffusers, this is as easy as 1-2-3:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 🤗 Diffusers，这就像 1-2-3 一样简单：
- en: 'Load a checkpoint into the [AutoPipelineForImage2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForImage2Image)
    class; this pipeline automatically handles loading the correct pipeline class
    based on the checkpoint:'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将检查点加载到 [AutoPipelineForImage2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForImage2Image)
    类中；此管道会根据检查点自动处理加载正确的管道类：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You’ll notice throughout the guide, we use [enable_model_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/gligen#diffusers.StableDiffusionGLIGENTextImagePipeline.enable_model_cpu_offload)
    and [enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_xformers_memory_efficient_attention),
    to save memory and increase inference speed. If you’re using PyTorch 2.0, then
    you don’t need to call [enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_xformers_memory_efficient_attention)
    on your pipeline because it’ll already be using PyTorch 2.0’s native [scaled-dot
    product attention](../optimization/torch2.0#scaled-dot-product-attention).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个指南中，您会注意到我们使用 [enable_model_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/gligen#diffusers.StableDiffusionGLIGENTextImagePipeline.enable_model_cpu_offload)
    和 [enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_xformers_memory_efficient_attention)，以节省内存并增加推理速度。如果您使用的是
    PyTorch 2.0，则不需要在管道上调用 [enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_xformers_memory_efficient_attention)，因为它已经在使用
    PyTorch 2.0 的本机 [scaled-dot product attention](../optimization/torch2.0#scaled-dot-product-attention)。
- en: 'Load an image to pass to the pipeline:'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一个图像传递给管道：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Pass a prompt and image to the pipeline to generate an image:'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 传递一个提示和图像给管道以生成一个图像：
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/b7dea39428bc1d2cd88a6400783552cb.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7dea39428bc1d2cd88a6400783552cb.png)'
- en: initial image
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 初始图像
- en: '![](../Images/54142d2106911436dd03998048a92fad.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54142d2106911436dd03998048a92fad.png)'
- en: generated image
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图像
- en: Popular models
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流行的模型
- en: The most popular image-to-image models are [Stable Diffusion v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5),
    [Stable Diffusion XL (SDXL)](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0),
    and [Kandinsky 2.2](https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder).
    The results from the Stable Diffusion and Kandinsky models vary due to their architecture
    differences and training process; you can generally expect SDXL to produce higher
    quality images than Stable Diffusion v1.5\. Let’s take a quick look at how to
    use each of these models and compare their results.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的图像到图像模型是 [稳定扩散 v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5)、[稳定扩散
    XL (SDXL)](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) 和
    [Kandinsky 2.2](https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder)。由于它们的架构差异和训练过程的不同，稳定扩散和
    Kandinsky 模型的结果会有所不同；通常可以预期 SDXL 会产生比稳定扩散 v1.5 更高质量的图像。让我们快速看看如何使用这些模型并比较它们的结果。
- en: Stable Diffusion v1.5
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 稳定扩散 v1.5
- en: 'Stable Diffusion v1.5 is a latent diffusion model initialized from an earlier
    checkpoint, and further finetuned for 595K steps on 512x512 images. To use this
    pipeline for image-to-image, you’ll need to prepare an initial image to pass to
    the pipeline. Then you can pass a prompt and the image to the pipeline to generate
    a new image:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散 v1.5 是一个从早期检查点初始化的潜在扩散模型，并在 512x512 图像上进一步微调了 595K 步。要将此管道用于图像到图像，您需要准备一个初始图像传递给管道。然后，您可以传递一个提示和图像给管道以生成一个新图像：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/36dc16e92be7b4f357b7201ad98da118.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36dc16e92be7b4f357b7201ad98da118.png)'
- en: initial image
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 初始图像
- en: '![](../Images/0145794767dd4ef55a2071169ec77aa2.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0145794767dd4ef55a2071169ec77aa2.png)'
- en: generated image
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图像
- en: Stable Diffusion XL (SDXL)
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 稳定扩散 XL (SDXL)
- en: SDXL is a more powerful version of the Stable Diffusion model. It uses a larger
    base model, and an additional refiner model to increase the quality of the base
    model’s output. Read the [SDXL](sdxl) guide for a more detailed walkthrough of
    how to use this model, and other techniques it uses to produce high quality images.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL 是稳定扩散模型的更强大版本。它使用一个更大的基础模型，以及一个额外的细化模型来提高基础模型输出的质量。阅读 [SDXL](sdxl) 指南，了解如何使用此模型以及它用于生成高质量图像的其他技术。
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/2fe2edb85787ba08991e059ff912c98c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fe2edb85787ba08991e059ff912c98c.png)'
- en: initial image
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 初始图像
- en: '![](../Images/7ec0479f09a14b51ed59ccd01d6f93c9.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ec0479f09a14b51ed59ccd01d6f93c9.png)'
- en: generated image
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图像
- en: Kandinsky 2.2
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kandinsky 2.2
- en: The Kandinsky model is different from the Stable Diffusion models because it
    uses an image prior model to create image embeddings. The embeddings help create
    a better alignment between text and images, allowing the latent diffusion model
    to generate better images.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Kandinsky 模型与稳定扩散模型不同，因为它使用图像先验模型来创建图像嵌入。这些嵌入有助于在文本和图像之间创建更好的对齐，使潜在扩散模型能够生成更好的图像。
- en: 'The simplest way to use Kandinsky 2.2 is:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Kandinsky 2.2 的最简单方法是：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/36dc16e92be7b4f357b7201ad98da118.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36dc16e92be7b4f357b7201ad98da118.png)'
- en: initial image
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 初始图像
- en: '![](../Images/97dcd8328035f392ab7d774997117ed0.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97dcd8328035f392ab7d774997117ed0.png)'
- en: generated image
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图像
- en: Configure pipeline parameters
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置管道参数
- en: There are several important parameters you can configure in the pipeline that’ll
    affect the image generation process and image quality. Let’s take a closer look
    at what these parameters do and how changing them affects the output.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以配置管道中的几个重要参数，这些参数将影响图像生成过程和图像质量。让我们更仔细地看看这些参数的作用以及如何更改它们会影响输出。
- en: Strength
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强度
- en: '`strength` is one of the most important parameters to consider and it’ll have
    a huge impact on your generated image. It determines how much the generated image
    resembles the initial image. In other words:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`strength`是要考虑的最重要参数之一，它将对生成的图像产生巨大影响。它决定了生成的图像与初始图像的相似程度。换句话说：'
- en: 📈 a higher `strength` value gives the model more “creativity” to generate an
    image that’s different from the initial image; a `strength` value of 1.0 means
    the initial image is more or less ignored
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 📈较高的`strength`值使模型更具“创造力”，可以生成与初始图像不同的图像；`strength`值为1.0意味着初始图像或多或少被忽略
- en: 📉 a lower `strength` value means the generated image is more similar to the
    initial image
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 📉较低的`strength`值意味着生成的图像更类似于初始图像
- en: The `strength` and `num_inference_steps` parameters are related because `strength`
    determines the number of noise steps to add. For example, if the `num_inference_steps`
    is 50 and `strength` is 0.8, then this means adding 40 (50 * 0.8) steps of noise
    to the initial image and then denoising for 40 steps to get the newly generated
    image.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`strength`和`num_inference_steps`参数相关，因为`strength`确定要添加的噪声步数。例如，如果`num_inference_steps`为50，`strength`为0.8，则这意味着向初始图像添加40（50
    * 0.8）步噪声，然后去噪40步以获得新生成的图像。'
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/b18d4d9ef1fda116b32a37eb344dd61d.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b18d4d9ef1fda116b32a37eb344dd61d.png)'
- en: strength = 0.4
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: strength = 0.4
- en: '![](../Images/caabae187343ac728e132d95979f20d9.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/caabae187343ac728e132d95979f20d9.png)'
- en: strength = 0.6
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: strength = 0.6
- en: '![](../Images/6dd3ccf15afa5473f942682270943955.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6dd3ccf15afa5473f942682270943955.png)'
- en: strength = 1.0
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: strength = 1.0
- en: Guidance scale
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指导尺度
- en: The `guidance_scale` parameter is used to control how closely aligned the generated
    image and text prompt are. A higher `guidance_scale` value means your generated
    image is more aligned with the prompt, while a lower `guidance_scale` value means
    your generated image has more space to deviate from the prompt.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`guidance_scale`参数用于控制生成的图像和文本提示之间的对齐程度。较高的`guidance_scale`值意味着您生成的图像与提示更加对齐，而较低的`guidance_scale`值意味着您生成的图像有更多的空间偏离提示。'
- en: You can combine `guidance_scale` with `strength` for even more precise control
    over how expressive the model is. For example, combine a high `strength + guidance_scale`
    for maximum creativity or use a combination of low `strength` and low `guidance_scale`
    to generate an image that resembles the initial image but is not as strictly bound
    to the prompt.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将`guidance_scale`与`strength`结合使用，以更精确地控制模型的表现力。例如，结合高`strength + guidance_scale`以获得最大创造力，或者使用低`strength`和低`guidance_scale`的组合生成类似于初始图像但不严格受限于提示的图像。
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/8f312f9d19115eba39cce061ca99ee11.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f312f9d19115eba39cce061ca99ee11.png)'
- en: guidance_scale = 0.1
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: guidance_scale = 0.1
- en: '![](../Images/ee6aee8a1bb3c419a6a94dd5b4a870b8.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee6aee8a1bb3c419a6a94dd5b4a870b8.png)'
- en: guidance_scale = 5.0
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: guidance_scale = 5.0
- en: '![](../Images/94e53acaee818fe2a3dddf5671819e4e.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94e53acaee818fe2a3dddf5671819e4e.png)'
- en: guidance_scale = 10.0
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: guidance_scale = 10.0
- en: Negative prompt
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负提示
- en: A negative prompt conditions the model to *not* include things in an image,
    and it can be used to improve image quality or modify an image. For example, you
    can improve image quality by including negative prompts like “poor details” or
    “blurry” to encourage the model to generate a higher quality image. Or you can
    modify an image by specifying things to exclude from an image.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 负提示条件模型*不*在图像中包含某些内容，可用于改善图像质量或修改图像。例如，您可以通过包含“细节差”或“模糊”等负提示来改善图像质量，以鼓励模型生成更高质量的图像。或者您可以通过指定要从图像中排除的内容来修改图像。
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/b0371dfac3fc42c122a50da2e48f809a.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0371dfac3fc42c122a50da2e48f809a.png)'
- en: negative_prompt = "ugly, deformed, disfigured, poor details, bad anatomy"
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: negative_prompt = "丑陋，畸形，毁容，细节差，解剖不良"
- en: '![](../Images/420cf48fd86187c0fb09679e8d6a92bd.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/420cf48fd86187c0fb09679e8d6a92bd.png)'
- en: negative_prompt = "jungle"
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: negative_prompt = "丛林"
- en: Chained image-to-image pipelines
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链式图像到图像管道
- en: There are some other interesting ways you can use an image-to-image pipeline
    aside from just generating an image (although that is pretty cool too). You can
    take it a step further and chain it with other pipelines.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 除了生成图像之外，您还可以以其他有趣的方式使用图像到图像管道（尽管生成图像也很酷）。您可以进一步将其与其他管道链接起来。
- en: Text-to-image-to-image
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本到图像到图像
- en: Chaining a text-to-image and image-to-image pipeline allows you to generate
    an image from text and use the generated image as the initial image for the image-to-image
    pipeline. This is useful if you want to generate an image entirely from scratch.
    For example, let’s chain a Stable Diffusion and a Kandinsky model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本到图像和图像到图像管道链接在一起，可以从文本生成图像，并将生成的图像用作图像到图像管道的初始图像。如果您想完全从头开始生成图像，这将非常有用。例如，让我们链接一个稳定扩散和一个康定斯基模型。
- en: 'Start by generating an image with the text-to-image pipeline:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先使用文本到图像管道生成一个图像：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now you can pass this generated image to the image-to-image pipeline:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以将生成的图像传递给图像到图像管道：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Image-to-image-to-image
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像到图像到图像
- en: You can also chain multiple image-to-image pipelines together to create more
    interesting images. This can be useful for iteratively performing style transfer
    on an image, generating short GIFs, restoring color to an image, or restoring
    missing areas of an image.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将多个图像到图像管道链接在一起，以创建更有趣的图像。这对于在图像上迭代执行风格转移、生成短GIF、恢复图像的颜色或恢复图像中缺失的区域非常有用。
- en: 'Start by generating an image:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先生成一个图像：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It is important to specify `output_type="latent"` in the pipeline to keep all
    the outputs in latent space to avoid an unnecessary decode-encode step. This only
    works if the chained pipelines are using the same VAE.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Pass the latent output from this pipeline to the next pipeline to generate
    an image in a [comic book art style](https://huggingface.co/ogkalu/Comic-Diffusion):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Repeat one more time to generate the final image in a [pixel art style](https://huggingface.co/kohbanye/pixel-art-style):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Image-to-upscaler-to-super-resolution
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another way you can chain your image-to-image pipeline is with an upscaler and
    super-resolution pipeline to really increase the level of details in an image.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with an image-to-image pipeline:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It is important to specify `output_type="latent"` in the pipeline to keep all
    the outputs in *latent* space to avoid an unnecessary decode-encode step. This
    only works if the chained pipelines are using the same VAE.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Chain it to an upscaler pipeline to increase the image resolution:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, chain it to a super-resolution pipeline to further enhance the resolution:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Control image generation
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Trying to generate an image that looks exactly the way you want can be difficult,
    which is why controlled generation techniques and models are so useful. While
    you can use the `negative_prompt` to partially control image generation, there
    are more robust methods like prompt weighting and ControlNets.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Prompt weighting
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompt weighting allows you to scale the representation of each concept in a
    prompt. For example, in a prompt like “Astronaut in a jungle, cold color palette,
    muted colors, detailed, 8k”, you can choose to increase or decrease the embeddings
    of “astronaut” and “jungle”. The [Compel](https://github.com/damian0815/compel)
    library provides a simple syntax for adjusting prompt weights and generating the
    embeddings. You can learn how to create the embeddings in the [Prompt weighting](weighted_prompts)
    guide.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[AutoPipelineForImage2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForImage2Image)
    has a `prompt_embeds` (and `negative_prompt_embeds` if you’re using a negative
    prompt) parameter where you can pass the embeddings which replaces the `prompt`
    parameter.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ControlNet
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ControlNets provide a more flexible and accurate way to control image generation
    because you can use an additional conditioning image. The conditioning image can
    be a canny image, depth map, image segmentation, and even scribbles! Whatever
    type of conditioning image you choose, the ControlNet generates an image that
    preserves the information in it.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s condition an image with a depth map to keep the spatial information
    in the image.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Load a ControlNet model conditioned on depth maps and the [AutoPipelineForImage2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForImage2Image):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now generate a new image conditioned on the depth map, initial image, and prompt:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/36dc16e92be7b4f357b7201ad98da118.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: initial image
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/795c4016502683bcda2f00fa33a84c2d.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: depth image
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e425aa0e6403b0304599567d11b91a4d.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: ControlNet image
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply a new [style](https://huggingface.co/nitrosocke/elden-ring-diffusion)
    to the image generated from the ControlNet by chaining it with an image-to-image
    pipeline:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/3d19fb4449a5e38b2448796e3bf6cf11.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: Optimize
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Running diffusion models is computationally expensive and intensive, but with
    a few optimization tricks, it is entirely possible to run them on consumer and
    free-tier GPUs. For example, you can use a more memory-efficient form of attention
    such as PyTorch 2.0’s [scaled-dot product attention](../optimization/torch2.0#scaled-dot-product-attention)
    or [xFormers](../optimization/xformers) (you can use one or the other, but there’s
    no need to use both). You can also offload the model to the GPU while the other
    pipeline components wait on the CPU.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'With [`torch.compile`](../optimization/torch2.0#torchcompile), you can boost
    your inference speed even more by wrapping your UNet with it:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[`torch.compile`](../optimization/torch2.0#torchcompile)，您可以通过将UNet与其包装来进一步提高推理速度：
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: To learn more, take a look at the [Reduce memory usage](../optimization/memory)
    and [Torch 2.0](../optimization/torch2.0) guides.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多信息，请查看[减少内存使用](../optimization/memory)和[Torch 2.0](../optimization/torch2.0)指南。
