- en: Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/diffusers/api/pipelines/overview](https://huggingface.co/docs/diffusers/api/pipelines/overview)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/55.f6d2a0c2.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Tip.230e2334.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Docstring.93f6f462.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/CodeBlock.57fe6e13.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/ExampleCodeBlock.658f5cd6.js">
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines provide a simple way to run state-of-the-art diffusion models in inference
    by bundling all of the necessary components (multiple independently-trained models,
    schedulers, and processors) into a single end-to-end class. Pipelines are flexible
    and they can be adapted to use different schedulers or even model components.
  prefs: []
  type: TYPE_NORMAL
- en: All pipelines are built from the base [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    class which provides basic functionality for loading, downloading, and saving
    all the components. Specific pipeline types (for example [StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline))
    loaded with [from_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained)
    are automatically detected and the pipeline components are loaded and passed to
    the `__init__` function of the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: You shouldn’t use the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    class for training. Individual components (for example, [UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel)
    and [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    of diffusion pipelines are usually trained individually, so we suggest directly
    working with them instead.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines do not offer any training functionality. You’ll notice PyTorch’s autograd
    is disabled by decorating the `__call__()` method with a [`torch.no_grad`](https://pytorch.org/docs/stable/generated/torch.no_grad.html)
    decorator because pipelines should not be used for training. If you’re interested
    in training, please take a look at the [Training](../../training/overview) guides
    instead!
  prefs: []
  type: TYPE_NORMAL
- en: The table below lists all the pipelines currently available in 🤗 Diffusers and
    the tasks they support. Click on a pipeline to view its abstract and published
    paper.
  prefs: []
  type: TYPE_NORMAL
- en: '| Pipeline | Tasks |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [AltDiffusion](alt_diffusion) | image2image |'
  prefs: []
  type: TYPE_TB
- en: '| [AnimateDiff](animatediff) | text2video |'
  prefs: []
  type: TYPE_TB
- en: '| [Attend-and-Excite](attend_and_excite) | text2image |'
  prefs: []
  type: TYPE_TB
- en: '| [Audio Diffusion](audio_diffusion) | image2audio |'
  prefs: []
  type: TYPE_TB
- en: '| [AudioLDM](audioldm) | text2audio |'
  prefs: []
  type: TYPE_TB
- en: '| [AudioLDM2](audioldm2) | text2audio |'
  prefs: []
  type: TYPE_TB
- en: '| [BLIP Diffusion](blip_diffusion) | text2image |'
  prefs: []
  type: TYPE_TB
- en: '| [Consistency Models](consistency_models) | unconditional image generation
    |'
  prefs: []
  type: TYPE_TB
- en: '| [ControlNet](controlnet) | text2image, image2image, inpainting |'
  prefs: []
  type: TYPE_TB
- en: '| [ControlNet with Stable Diffusion XL](controlnet_sdxl) | text2image |'
  prefs: []
  type: TYPE_TB
- en: '| [ControlNet-XS](controlnetxs) | text2image |'
  prefs: []
  type: TYPE_TB
- en: '| [ControlNet-XS with Stable Diffusion XL](controlnetxs_sdxl) | text2image
    |'
  prefs: []
  type: TYPE_TB
- en: '| [Cycle Diffusion](cycle_diffusion) | image2image |'
  prefs: []
  type: TYPE_TB
- en: '| [Dance Diffusion](dance_diffusion) | unconditional audio generation |'
  prefs: []
  type: TYPE_TB
- en: '| [DDIM](ddim) | unconditional image generation |'
  prefs: []
  type: TYPE_TB
- en: '| [DDPM](ddpm) | unconditional image generation |'
  prefs: []
  type: TYPE_TB
- en: '| [DeepFloyd IF](deepfloyd_if) | text2image, image2image, inpainting, super-resolution
    |'
  prefs: []
  type: TYPE_TB
- en: '| [DiffEdit](diffedit) | inpainting |'
  prefs: []
  type: TYPE_TB
- en: '| [DiT](dit) | text2image |'
  prefs: []
  type: TYPE_TB
- en: '| [GLIGEN](stable_diffusion/gligen) | text2image |'
  prefs: []
  type: TYPE_TB
- en: '| [InstructPix2Pix](pix2pix) | image editing |'
  prefs: []
  type: TYPE_TB
- en: '| [Kandinsky 2.1](kandinsky) | text2image, image2image, inpainting, interpolation
    |'
  prefs: []
  type: TYPE_TB
- en: '| [Kandinsky 2.2](kandinsky_v22) | text2image, image2image, inpainting |'
  prefs: []
  type: TYPE_TB
- en: '| [Kandinsky 3](kandinsky3) | text2image, image2image |'
  prefs: []
  type: TYPE_TB
- en: '| [Latent Consistency Models](latent_consistency_models) | text2image |'
  prefs: []
  type: TYPE_TB
- en: '| [Latent Diffusion](latent_diffusion) | text2image, super-resolution |'
  prefs: []
  type: TYPE_TB
- en: '| [LDM3D](stable_diffusion/ldm3d_diffusion) | text2image, text-to-3D, text-to-pano,
    upscaling |'
  prefs: []
  type: TYPE_TB
- en: '| [MultiDiffusion](panorama) | text2image |'
  prefs: []
  type: TYPE_TB
- en: '| [MusicLDM](musicldm) | text2audio |'
  prefs: []
  type: TYPE_TB
- en: '| [Paint by Example](paint_by_example) | inpainting |'
  prefs: []
  type: TYPE_TB
- en: '| [ParaDiGMS](paradigms) | text2image |'
  prefs: []
  type: TYPE_TB
- en: '| [Pix2Pix Zero](pix2pix_zero) | image editing |'
  prefs: []
  type: TYPE_TB
- en: '| [PixArt-α](pixart) | text2image |'
  prefs: []
  type: TYPE_TB
- en: '| [PNDM](pndm) | unconditional image generation |'
  prefs: []
  type: TYPE_TB
- en: '| [RePaint](repaint) | inpainting |'
  prefs: []
  type: TYPE_TB
- en: '| [Score SDE VE](score_sde_ve) | unconditional image generation |'
  prefs: []
  type: TYPE_TB
- en: '| [Self-Attention Guidance](self_attention_guidance) | text2image |'
  prefs: []
  type: TYPE_TB
- en: '| [Semantic Guidance](semantic_stable_diffusion) | text2image |'
  prefs: []
  type: TYPE_TB
- en: '| [Shap-E](shap_e) | text-to-3D, image-to-3D |'
  prefs: []
  type: TYPE_TB
- en: '| [Spectrogram Diffusion](spectrogram_diffusion) |  |'
  prefs: []
  type: TYPE_TB
- en: '| [Stable Diffusion](stable_diffusion/overview) | text2image, image2image,
    depth2image, inpainting, image variation, latent upscaler, super-resolution |'
  prefs: []
  type: TYPE_TB
- en: '| [Stable Diffusion Model Editing](model_editing) | model editing |'
  prefs: []
  type: TYPE_TB
- en: '| [Stable Diffusion XL](stable_diffusion/stable_diffusion_xl) | text2image,
    image2image, inpainting |'
  prefs: []
  type: TYPE_TB
- en: '| [Stable Diffusion XL Turbo](stable_diffusion/sdxl_turbo) | text2image, image2image,
    inpainting |'
  prefs: []
  type: TYPE_TB
- en: '| [Stable unCLIP](stable_unclip) | text2image, image variation |'
  prefs: []
  type: TYPE_TB
- en: '| [Stochastic Karras VE](stochastic_karras_ve) | unconditional image generation
    |'
  prefs: []
  type: TYPE_TB
- en: '| [T2I-Adapter](stable_diffusion/adapter) | text2image |'
  prefs: []
  type: TYPE_TB
- en: '| [Text2Video](text_to_video) | text2video, video2video |'
  prefs: []
  type: TYPE_TB
- en: '| [Text2Video-Zero](text_to_video_zero) | text2video |'
  prefs: []
  type: TYPE_TB
- en: '| [unCLIP](unclip) | text2image, image variation |'
  prefs: []
  type: TYPE_TB
- en: '| [Unconditional Latent Diffusion](latent_diffusion_uncond) | unconditional
    image generation |'
  prefs: []
  type: TYPE_TB
- en: '| [UniDiffuser](unidiffuser) | text2image, image2text, image variation, text
    variation, unconditional image generation, unconditional audio generation |'
  prefs: []
  type: TYPE_TB
- en: '| [Value-guided planning](value_guided_sampling) | value guided sampling |'
  prefs: []
  type: TYPE_TB
- en: '| [Versatile Diffusion](versatile_diffusion) | text2image, image variation
    |'
  prefs: []
  type: TYPE_TB
- en: '| [VQ Diffusion](vq_diffusion) | text2image |'
  prefs: []
  type: TYPE_TB
- en: '| [Wuerstchen](wuerstchen) | text2image |'
  prefs: []
  type: TYPE_TB
- en: DiffusionPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.DiffusionPipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L569)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Base class for all pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    stores all components (models, schedulers, and processors) for diffusion pipelines
    and provides methods for loading, downloading and saving models. It also includes
    methods to:'
  prefs: []
  type: TYPE_NORMAL
- en: move all PyTorch modules to the device of your choice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: enable/disable the progress bar for the denoising iteration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**config_name** (`str`) — The configuration filename that stores the class
    and module names of all the diffusion pipeline’s components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**_optional_components** (`List[str]`) — List of all optional components that
    don’t have to be passed to the pipeline to function (should be overridden by subclasses).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: ( *args **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Call self as a function.
  prefs: []
  type: TYPE_NORMAL
- en: '#### device'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L901)'
  prefs: []
  type: TYPE_NORMAL
- en: ( ) → `torch.device`
  prefs: []
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.device`'
  prefs: []
  type: TYPE_NORMAL
- en: The torch device on which the pipeline is located.
  prefs: []
  type: TYPE_NORMAL
- en: '#### to'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L742)'
  prefs: []
  type: TYPE_NORMAL
- en: ( *args **kwargs ) → [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**dtype** (`torch.dtype`, *optional*) — Returns a pipeline with the specified
    [`dtype`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (`torch.Device`, *optional*) — Returns a pipeline with the specified
    [`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**silence_dtype_warnings** (`str`, *optional*, defaults to `False`) — Whether
    to omit warnings if the target `dtype` is not compatible with the target `device`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)'
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline converted to specified `dtype` and/or `dtype`.
  prefs: []
  type: TYPE_NORMAL
- en: Performs Pipeline dtype and/or device conversion. A torch.dtype and torch.device
    are inferred from the arguments of `self.to(*args, **kwargs).`
  prefs: []
  type: TYPE_NORMAL
- en: If the pipeline already has the correct torch.dtype and torch.device, then it
    is returned as is. Otherwise, the returned pipeline is a copy of self with the
    desired torch.dtype and torch.device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the ways to call `to`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`to(dtype, silence_dtype_warnings=False) → DiffusionPipeline` to return a pipeline
    with the specified [`dtype`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`to(device, silence_dtype_warnings=False) → DiffusionPipeline` to return a
    pipeline with the specified [`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`to(device=None, dtype=None, silence_dtype_warnings=False) → DiffusionPipeline`
    to return a pipeline with the specified [`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device)
    and [`dtype`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### components'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L1941)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: The `self.components` property can be useful to run different pipelines with
    the same weights and configurations without reallocating additional memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns (`dict`): A dictionary containing all the modules needed to initialize
    the pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#### disable_attention_slicing'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Disable sliced attention computation. If `enable_attention_slicing` was previously
    called, attention is computed in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### disable_xformers_memory_efficient_attention'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Disable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).
  prefs: []
  type: TYPE_NORMAL
- en: '#### download'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L1552)'
  prefs: []
  type: TYPE_NORMAL
- en: ( pretrained_model_name **kwargs ) → `os.PathLike`
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**pretrained_model_name** (`str` or `os.PathLike`, *optional*) — A string,
    the *repository id* (for example `CompVis/ldm-text2im-large-256`) of a pretrained
    pipeline hosted on the Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**custom_pipeline** (`str`, *optional*) — Can be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *repository id* (for example `CompVis/ldm-text2im-large-256`)
    of a pretrained pipeline hosted on the Hub. The repository must contain a file
    called `pipeline.py` that defines the custom pipeline.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *file name* of a community pipeline hosted on GitHub under [Community](https://github.com/huggingface/diffusers/tree/main/examples/community).
    Valid file names must match the file name and not the pipeline script (`clip_guided_stable_diffusion`
    instead of `clip_guided_stable_diffusion.py`). Community pipelines are always
    loaded from the current `main` branch of GitHub.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* (`./my_pipeline_directory/`) containing a custom pipeline.
    The directory must contain a file called `pipeline.py` that defines the custom
    pipeline.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 🧪 This is an experimental feature and may change in the future.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For more information on how to load and create custom pipelines, take a look
    at [How to contribute a community pipeline](https://huggingface.co/docs/diffusers/main/en/using-diffusers/contribute_pipeline).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**force_download** (`bool`, *optional*, defaults to `False`) — Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**resume_download** (`bool`, *optional*, defaults to `False`) — Whether or
    not to resume downloading the model weights and configuration files. If set to
    `False`, any incompletely downloaded files are deleted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**proxies** (`Dict[str, str]`, *optional*) — A dictionary of proxy servers
    to use by protocol or endpoint, for example, `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_loading_info(`bool`,** *optional*, defaults to `False`) — Whether
    or not to also return a dictionary containing missing keys, unexpected keys and
    error messages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**local_files_only** (`bool`, *optional*, defaults to `False`) — Whether to
    only load local model weights and configuration files or not. If set to `True`,
    the model won’t be downloaded from the Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token** (`str` or *bool*, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, the token generated from `diffusers-cli login` (stored
    in `~/.huggingface`) is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**revision** (`str`, *optional*, defaults to `"main"`) — The specific model
    version to use. It can be a branch name, a tag name, a commit id, or any identifier
    allowed by Git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**custom_revision** (`str`, *optional*, defaults to `"main"`) — The specific
    model version to use. It can be a branch name, a tag name, or a commit id similar
    to `revision` when loading a custom pipeline from the Hub. It can be a 🤗 Diffusers
    version when loading a custom pipeline from GitHub, otherwise it defaults to `"main"`
    when loading from the Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mirror** (`str`, *optional*) — Mirror source to resolve accessibility issues
    if you’re downloading a model in China. We do not guarantee the timeliness or
    safety of the source, and you should refer to the mirror site for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**variant** (`str`, *optional*) — Load weights from a specified variant filename
    such as `"fp16"` or `"ema"`. This is ignored when loading `from_flax`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_safetensors** (`bool`, *optional*, defaults to `None`) — If set to `None`,
    the safetensors weights are downloaded if they’re available **and** if the safetensors
    library is installed. If set to `True`, the model is forcibly loaded from safetensors
    weights. If set to `False`, safetensors weights are not loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_onnx** (`bool`, *optional*, defaults to `False`) — If set to `True`,
    ONNX weights will always be downloaded if present. If set to `False`, ONNX weights
    will never be downloaded. By default `use_onnx` defaults to the `_is_onnx` class
    attribute which is `False` for non-ONNX pipelines and `True` for ONNX pipelines.
    ONNX weights include both files ending with `.onnx` and `.pb`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**trust_remote_code** (`bool`, *optional*, defaults to `False`) — Whether or
    not to allow for custom pipelines and components defined on the Hub in their own
    files. This option should only be set to `True` for repositories you trust and
    in which you have read the code, as it will execute code present on the Hub on
    your local machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`os.PathLike`'
  prefs: []
  type: TYPE_NORMAL
- en: A path to the downloaded pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Download and cache a PyTorch diffusion pipeline from pretrained pipeline weights.
  prefs: []
  type: TYPE_NORMAL
- en: To use private or [gated models](https://huggingface.co/docs/hub/models-gated#gated-models),
    log-in with `huggingface-cli login`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### enable_attention_slicing'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)'
  prefs: []
  type: TYPE_NORMAL
- en: '( slice_size: Union = ''auto'' )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**slice_size** (`str` or `int`, *optional*, defaults to `"auto"`) — When `"auto"`,
    halves the input to the attention heads, so attention will be computed in two
    steps. If `"max"`, maximum amount of memory will be saved by running only one
    slice at a time. If a number is provided, uses as many slices as `attention_head_dim
    // slice_size`. In this case, `attention_head_dim` must be a multiple of `slice_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable sliced attention computation. When this option is enabled, the attention
    module splits the input tensor in slices to compute attention in several steps.
    For more than one attention head, the computation is performed sequentially over
    each head. This is useful to save some memory in exchange for a small speed decrease.
  prefs: []
  type: TYPE_NORMAL
- en: ⚠️ Don’t enable attention slicing if you’re already using `scaled_dot_product_attention`
    (SDPA) from PyTorch 2.0 or xFormers. These attention computations are already
    very memory efficient so you won’t need to enable this function. If you enable
    attention slicing with SDPA or xFormers, it can lead to serious slow downs!
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#### enable_model_cpu_offload'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L1410)'
  prefs: []
  type: TYPE_NORMAL
- en: '( gpu_id: Optional = None device: Union = ''cuda'' )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**gpu_id** (`int`, *optional*) — The ID of the accelerator that shall be used
    in inference. If not specified, it will default to 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (`torch.Device` or `str`, *optional*, defaults to “cuda”) — The
    PyTorch device type of the accelerator that shall be used in inference. If not
    specified, it will default to “cuda”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offloads all models to CPU using accelerate, reducing memory usage with a low
    impact on performance. Compared to `enable_sequential_cpu_offload`, this method
    moves one whole model at a time to the GPU when its `forward` method is called,
    and the model remains in GPU until the next model runs. Memory savings are lower
    than with `enable_sequential_cpu_offload`, but performance is much better due
    to the iterative execution of the `unet`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### enable_sequential_cpu_offload'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L1499)'
  prefs: []
  type: TYPE_NORMAL
- en: '( gpu_id: Optional = None device: Union = ''cuda'' )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**gpu_id** (`int`, *optional*) — The ID of the accelerator that shall be used
    in inference. If not specified, it will default to 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device** (`torch.Device` or `str`, *optional*, defaults to “cuda”) — The
    PyTorch device type of the accelerator that shall be used in inference. If not
    specified, it will default to “cuda”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offloads all models to CPU using 🤗 Accelerate, significantly reducing memory
    usage. When called, the state dicts of all `torch.nn.Module` components (except
    those in `self._exclude_from_cpu_offload`) are saved to CPU and then moved to
    `torch.device('meta')` and loaded to GPU only when their specific submodule has
    its `forward` method called. Offloading happens on a submodule basis. Memory savings
    are higher than with `enable_model_cpu_offload`, but performance is lower.
  prefs: []
  type: TYPE_NORMAL
- en: '#### enable_xformers_memory_efficient_attention'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)'
  prefs: []
  type: TYPE_NORMAL
- en: '( attention_op: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**attention_op** (`Callable`, *optional*) — Override the default `None` operator
    for use as `op` argument to the [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)
    function of xFormers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).
    When this option is enabled, you should observe lower GPU memory usage and a potential
    speed up during inference. Speed up during training is not guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: ⚠️ When memory efficient attention and sliced attention are both enabled, memory
    efficient attention takes precedent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#### from_pretrained'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L931)'
  prefs: []
  type: TYPE_NORMAL
- en: '( pretrained_model_name_or_path: Union **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**pretrained_model_name_or_path** (`str` or `os.PathLike`, *optional*) — Can
    be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *repo id* (for example `CompVis/ldm-text2im-large-256`) of a pretrained
    pipeline hosted on the Hub.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* (for example `./my_pipeline_directory/`) containing
    pipeline weights saved using [save_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline.save_pretrained).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**torch_dtype** (`str` or `torch.dtype`, *optional*) — Override the default
    `torch.dtype` and load the model with another dtype. If “auto” is passed, the
    dtype is automatically derived from the model’s weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**custom_pipeline** (`str`, *optional*) —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 🧪 This is an experimental feature and may change in the future.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Can be either:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A string, the *repo id* (for example `hf-internal-testing/diffusers-dummy-pipeline`)
    of a custom pipeline hosted on the Hub. The repository must contain a file called
    pipeline.py that defines the custom pipeline.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *file name* of a community pipeline hosted on GitHub under [Community](https://github.com/huggingface/diffusers/tree/main/examples/community).
    Valid file names must match the file name and not the pipeline script (`clip_guided_stable_diffusion`
    instead of `clip_guided_stable_diffusion.py`). Community pipelines are always
    loaded from the current main branch of GitHub.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a directory (`./my_pipeline_directory/`) containing a custom pipeline.
    The directory must contain a file called `pipeline.py` that defines the custom
    pipeline.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on how to load and create custom pipelines, please have
    a look at [Loading and Adding Custom Pipelines](https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**force_download** (`bool`, *optional*, defaults to `False`) — Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cache_dir** (`Union[str, os.PathLike]`, *optional*) — Path to a directory
    where a downloaded pretrained model configuration is cached if the standard cache
    is not used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**resume_download** (`bool`, *optional*, defaults to `False`) — Whether or
    not to resume downloading the model weights and configuration files. If set to
    `False`, any incompletely downloaded files are deleted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**proxies** (`Dict[str, str]`, *optional*) — A dictionary of proxy servers
    to use by protocol or endpoint, for example, `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_loading_info(`bool`,** *optional*, defaults to `False`) — Whether
    or not to also return a dictionary containing missing keys, unexpected keys and
    error messages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**local_files_only** (`bool`, *optional*, defaults to `False`) — Whether to
    only load local model weights and configuration files or not. If set to `True`,
    the model won’t be downloaded from the Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token** (`str` or *bool*, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, the token generated from `diffusers-cli login` (stored
    in `~/.huggingface`) is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**revision** (`str`, *optional*, defaults to `"main"`) — The specific model
    version to use. It can be a branch name, a tag name, a commit id, or any identifier
    allowed by Git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**custom_revision** (`str`, *optional*, defaults to `"main"`) — The specific
    model version to use. It can be a branch name, a tag name, or a commit id similar
    to `revision` when loading a custom pipeline from the Hub. It can be a 🤗 Diffusers
    version when loading a custom pipeline from GitHub, otherwise it defaults to `"main"`
    when loading from the Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mirror** (`str`, *optional*) — Mirror source to resolve accessibility issues
    if you’re downloading a model in China. We do not guarantee the timeliness or
    safety of the source, and you should refer to the mirror site for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**device_map** (`str` or `Dict[str, Union[int, str, torch.device]]`, *optional*)
    — A map that specifies where each submodule should go. It doesn’t need to be defined
    for each parameter/buffer name; once a given module name is inside, every submodule
    of it will be sent to the same device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set `device_map="auto"` to have 🤗 Accelerate automatically compute the most
    optimized `device_map`. For more information about each option see [designing
    a device map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**max_memory** (`Dict`, *optional*) — A dictionary device identifier for the
    maximum memory. Will default to the maximum memory available for each GPU and
    the available CPU RAM if unset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**offload_folder** (`str` or `os.PathLike`, *optional*) — The path to offload
    weights if device_map contains the value `"disk"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**offload_state_dict** (`bool`, *optional*) — If `True`, temporarily offloads
    the CPU state dict to the hard drive to avoid running out of CPU RAM if the weight
    of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults
    to `True` when there is some disk offload.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**low_cpu_mem_usage** (`bool`, *optional*, defaults to `True` if torch version
    >= 1.9.0 else `False`) — Speed up model loading only loading the pretrained weights
    and not initializing the weights. This also tries to not use more than 1x model
    size in CPU memory (including peak memory) while loading the model. Only supported
    for PyTorch >= 1.9.0\. If you are using an older version of PyTorch, setting this
    argument to `True` will raise an error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_safetensors** (`bool`, *optional*, defaults to `None`) — If set to `None`,
    the safetensors weights are downloaded if they’re available **and** if the safetensors
    library is installed. If set to `True`, the model is forcibly loaded from safetensors
    weights. If set to `False`, safetensors weights are not loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_onnx** (`bool`, *optional*, defaults to `None`) — If set to `True`, ONNX
    weights will always be downloaded if present. If set to `False`, ONNX weights
    will never be downloaded. By default `use_onnx` defaults to the `_is_onnx` class
    attribute which is `False` for non-ONNX pipelines and `True` for ONNX pipelines.
    ONNX weights include both files ending with `.onnx` and `.pb`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (remaining dictionary of keyword arguments, *optional*) — Can be
    used to overwrite load and saveable variables (the pipeline components of the
    specific pipeline class). The overwritten components are passed directly to the
    pipelines `__init__` method. See example below for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**variant** (`str`, *optional*) — Load weights from a specified variant filename
    such as `"fp16"` or `"ema"`. This is ignored when loading `from_flax`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate a PyTorch diffusion pipeline from pretrained pipeline weights.
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline is set in evaluation mode (`model.eval()`) by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you get the error message below, you need to finetune the weights for your
    downstream task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To use private or [gated](https://huggingface.co/docs/hub/models-gated#gated-models)
    models, log-in with `huggingface-cli login`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#### maybe_free_model_hooks'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L1480)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Function that offloads all components, removes all model hooks that were added
    when using `enable_model_cpu_offload` and then applies them again. In case the
    model has not been offloaded this function is a no-op. Make sure to add this function
    to the end of the `__call__` function of your pipeline so that it functions correctly
    when applying enable_model_cpu_offload.
  prefs: []
  type: TYPE_NORMAL
- en: '#### numpy_to_pil'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L1977)'
  prefs: []
  type: TYPE_NORMAL
- en: ( images )
  prefs: []
  type: TYPE_NORMAL
- en: Convert a NumPy image or a batch of images to a PIL image.
  prefs: []
  type: TYPE_NORMAL
- en: '#### save_pretrained'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L624)'
  prefs: []
  type: TYPE_NORMAL
- en: '( save_directory: Union safe_serialization: bool = True variant: Optional =
    None push_to_hub: bool = False **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**save_directory** (`str` or `os.PathLike`) — Directory to save a pipeline
    to. Will be created if it doesn’t exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**safe_serialization** (`bool`, *optional*, defaults to `True`) — Whether to
    save the model using `safetensors` or the traditional PyTorch way with `pickle`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**variant** (`str`, *optional*) — If specified, weights are saved in the format
    `pytorch_model.<variant>.bin`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**push_to_hub** (`bool`, *optional*, defaults to `False`) — Whether or not
    to push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (`Dict[str, Any]`, *optional*) — Additional keyword arguments passed
    along to the [push_to_hub()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.utils.PushToHubMixin.push_to_hub)
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save all saveable variables of the pipeline to a directory. A pipeline variable
    can be saved and loaded if its class implements both a save and loading method.
    The pipeline is easily reloaded using the [from_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained)
    class method.
  prefs: []
  type: TYPE_NORMAL
- en: FlaxDiffusionPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.FlaxDiffusionPipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_flax_utils.py#L101)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Base class for Flax-based pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '[FlaxDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.FlaxDiffusionPipeline)
    stores all components (models, schedulers, and processors) for diffusion pipelines
    and provides methods for loading, downloading and saving models. It also includes
    methods to:'
  prefs: []
  type: TYPE_NORMAL
- en: enable/disable the progress bar for the denoising iteration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**config_name** (`str`) — The configuration filename that stores the class
    and module names of all the diffusion pipeline’s components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### from_pretrained'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_flax_utils.py#L229)'
  prefs: []
  type: TYPE_NORMAL
- en: '( pretrained_model_name_or_path: Union **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**pretrained_model_name_or_path** (`str` or `os.PathLike`, *optional*) — Can
    be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *repo id* (for example `runwayml/stable-diffusion-v1-5`) of a
    pretrained pipeline hosted on the Hub.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* (for example `./my_model_directory`) containing the
    model weights saved using [save_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.FlaxDiffusionPipeline.save_pretrained).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dtype** (`str` or `jnp.dtype`, *optional*) — Override the default `jnp.dtype`
    and load the model under this dtype. If `"auto"`, the dtype is automatically derived
    from the model’s weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**force_download** (`bool`, *optional*, defaults to `False`) — Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**resume_download** (`bool`, *optional*, defaults to `False`) — Whether or
    not to resume downloading the model weights and configuration files. If set to
    `False`, any incompletely downloaded files are deleted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**proxies** (`Dict[str, str]`, *optional*) — A dictionary of proxy servers
    to use by protocol or endpoint, for example, `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_loading_info(`bool`,** *optional*, defaults to `False`) — Whether
    or not to also return a dictionary containing missing keys, unexpected keys and
    error messages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**local_files_only** (`bool`, *optional*, defaults to `False`) — Whether to
    only load local model weights and configuration files or not. If set to `True`,
    the model won’t be downloaded from the Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token** (`str` or *bool*, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, the token generated from `diffusers-cli login` (stored
    in `~/.huggingface`) is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**revision** (`str`, *optional*, defaults to `"main"`) — The specific model
    version to use. It can be a branch name, a tag name, a commit id, or any identifier
    allowed by Git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mirror** (`str`, *optional*) — Mirror source to resolve accessibility issues
    if you’re downloading a model in China. We do not guarantee the timeliness or
    safety of the source, and you should refer to the mirror site for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (remaining dictionary of keyword arguments, *optional*) — Can be
    used to overwrite load and saveable variables (the pipeline components) of the
    specific pipeline class. The overwritten components are passed directly to the
    pipelines `__init__` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate a Flax-based diffusion pipeline from pretrained pipeline weights.
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline is set in evaluation mode (`model.eval()) by default and dropout
    modules are deactivated.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you get the error message below, you need to finetune the weights for your
    downstream task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To use private or [gated models](https://huggingface.co/docs/hub/models-gated#gated-models),
    log-in with `huggingface-cli login`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#### numpy_to_pil'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_flax_utils.py#L588)'
  prefs: []
  type: TYPE_NORMAL
- en: ( images )
  prefs: []
  type: TYPE_NORMAL
- en: Convert a NumPy image or a batch of images to a PIL image.
  prefs: []
  type: TYPE_NORMAL
- en: '#### save_pretrained'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_flax_utils.py#L151)'
  prefs: []
  type: TYPE_NORMAL
- en: '( save_directory: Union params: Union push_to_hub: bool = False **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**save_directory** (`str` or `os.PathLike`) — Directory to which to save. Will
    be created if it doesn’t exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**push_to_hub** (`bool`, *optional*, defaults to `False`) — Whether or not
    to push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (`Dict[str, Any]`, *optional*) — Additional keyword arguments passed
    along to the [push_to_hub()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.utils.PushToHubMixin.push_to_hub)
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save all saveable variables of the pipeline to a directory. A pipeline variable
    can be saved and loaded if its class implements both a save and loading method.
    The pipeline is easily reloaded using the [from_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.FlaxDiffusionPipeline.from_pretrained)
    class method.
  prefs: []
  type: TYPE_NORMAL
- en: PushToHubMixin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.utils.PushToHubMixin'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/utils/hub_utils.py#L351)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: A Mixin to push a model, scheduler, or pipeline to the Hugging Face Hub.
  prefs: []
  type: TYPE_NORMAL
- en: '#### push_to_hub'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/utils/hub_utils.py#L380)'
  prefs: []
  type: TYPE_NORMAL
- en: '( repo_id: str commit_message: Optional = None private: Optional = None token:
    Optional = None create_pr: bool = False safe_serialization: bool = True variant:
    Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**repo_id** (`str`) — The name of the repository you want to push your model,
    scheduler, or pipeline files to. It should contain your organization name when
    pushing to an organization. `repo_id` can also be a path to a local directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**commit_message** (`str`, *optional*) — Message to commit while pushing. Default
    to `"Upload {object}"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**private** (`bool`, *optional*) — Whether or not the repository created should
    be private.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token** (`str`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. The token generated when running `huggingface-cli login` (stored
    in `~/.huggingface`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**create_pr** (`bool`, *optional*, defaults to `False`) — Whether or not to
    create a PR with the uploaded files or directly commit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**safe_serialization** (`bool`, *optional*, defaults to `True`) — Whether or
    not to convert the model weights to the `safetensors` format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**variant** (`str`, *optional*) — If specified, weights are saved in the format
    `pytorch_model.<variant>.bin`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upload model, scheduler, or pipeline files to the 🤗 Hugging Face Hub.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
