# FLAN-UL2

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/flan-ul2`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flan-ul2)

## 概述

Flan-UL2 是基于 T5 架构的编码器解码器模型。它使用与去年早些时候发布的 UL2 模型相同的配置。它经过“Flan”提示调整和数据集收集进行微调。与`Flan-T5`类似，可以直接使用 FLAN-UL2 权重而无需微调模型：

根据原始博客，以下是显著的改进：

+   原始的 UL2 模型只使用了 512 的感受野进行训练，这使得它对于大量 N-shot 提示不理想。

+   Flan-UL2 检查点使用 2048 的感受野，使其更适用于少样本上下文学习。

+   原始的 UL2 模型还有模式切换令牌，这对于获得良好性能是相当必要的。然而，它们有点繁琐，因为这经常需要在推理或微调过程中进行一些更改。在这次更新/更改中，我们继续训练 UL2 20B 额外的 100k 步（使用小批量）来忘记“模式令牌”，然后应用 Flan 指令调整。这个 Flan-UL2 检查点不再需要模式令牌。Google 发布了以下变体：

原始检查点可以在[这里](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints)找到。

## 在资源有限的设备上运行

该模型非常庞大（半精度约 40GB），因此如果您只想运行模型，请确保以 8 位加载您的模型，并使用`device_map="auto"`确保您没有任何 OOM 问题！

```py
>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

>>> model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-ul2", load_in_8bit=True, device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("google/flan-ul2")

>>> inputs = tokenizer("A step by step recipe to make bolognese pasta:", return_tensors="pt")
>>> outputs = model.generate(**inputs)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['In a large skillet, brown the ground beef and onion over medium heat. Add the garlic']
```

请参考 T5 的文档页面获取 API 参考、提示、代码示例和笔记本。
