# FLAN-UL2

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flan-ul2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flan-ul2)

## 概述

Flan-UL2是基于T5架构的编码器解码器模型。它使用与去年早些时候发布的UL2模型相同的配置。它经过“Flan”提示调整和数据集收集进行微调。与`Flan-T5`类似，可以直接使用FLAN-UL2权重而无需微调模型：

根据原始博客，以下是显著的改进：

+   原始的UL2模型只使用了512的感受野进行训练，这使得它对于大量N-shot提示不理想。

+   Flan-UL2检查点使用2048的感受野，使其更适用于少样本上下文学习。

+   原始的UL2模型还有模式切换令牌，这对于获得良好性能是相当必要的。然而，它们有点繁琐，因为这经常需要在推理或微调过程中进行一些更改。在这次更新/更改中，我们继续训练UL2 20B额外的100k步（使用小批量）来忘记“模式令牌”，然后应用Flan指令调整。这个Flan-UL2检查点不再需要模式令牌。Google发布了以下变体：

原始检查点可以在[这里](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints)找到。

## 在资源有限的设备上运行

该模型非常庞大（半精度约40GB），因此如果您只想运行模型，请确保以8位加载您的模型，并使用`device_map="auto"`确保您没有任何OOM问题！

```py
>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

>>> model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-ul2", load_in_8bit=True, device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("google/flan-ul2")

>>> inputs = tokenizer("A step by step recipe to make bolognese pasta:", return_tensors="pt")
>>> outputs = model.generate(**inputs)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['In a large skillet, brown the ground beef and onion over medium heat. Add the garlic']
```

请参考[T5的文档页面](t5)获取API参考、提示、代码示例和笔记本。
