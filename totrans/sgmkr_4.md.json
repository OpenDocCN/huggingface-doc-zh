["```py\nfrom sagemaker.huggingface import HuggingFaceModel\n\n# create Hugging Face Model Class and deploy it as SageMaker endpoint\nhuggingface_model = HuggingFaceModel(...).deploy()\n```", "```py\npip install sagemaker --upgrade\n```", "```py\nimport sagemaker\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n```", "```py\nimport sagemaker\nimport boto3\n\niam_client = boto3.client('iam')\nrole = iam_client.get_role(RoleName='role-name-of-your-iam-role-with-right-permissions')['Role']['Arn']\nsess = sagemaker.Session()\n```", "```py\nfrom sagemaker.huggingface import HuggingFace\n\n############ pseudo code start ############\n\n# create Hugging Face Estimator for training\nhuggingface_estimator = HuggingFace(....)\n\n# start the train job with our uploaded datasets as input\nhuggingface_estimator.fit(...)\n\n############ pseudo code end ############\n\n# deploy model to SageMaker Inference\npredictor = hf_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")\n\n# example request: you always need to define \"inputs\"\ndata = {\n   \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n}\n\n# request\npredictor.predict(data)\n```", "```py\n# delete endpoint\npredictor.delete_endpoint()\n```", "```py\nfrom sagemaker.huggingface.model import HuggingFaceModel\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n   model_data=\"s3://models/my-bert-model/model.tar.gz\",  # path to your trained SageMaker model\n   role=role,                                            # IAM role with permissions to create an endpoint\n   transformers_version=\"4.26\",                           # Transformers version used\n   pytorch_version=\"1.13\",                                # PyTorch version used\n   py_version='py39',                                    # Python version used\n)\n\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n   initial_instance_count=1,\n   instance_type=\"ml.m5.xlarge\"\n)\n\n# example request: you always need to define \"inputs\"\ndata = {\n   \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n}\n\n# request\npredictor.predict(data)\n```", "```py\n# delete endpoint\npredictor.delete_endpoint()\n```", "```py\nmodel.tar.gz/\n|- pytorch_model.bin\n|- vocab.txt\n|- tokenizer_config.json\n|- config.json\n|- special_tokens_map.json\n```", "```py\ngit lfs install\ngit clone git@hf.co:{repository}\n```", "```py\ncd {repository}\ntar zcvf model.tar.gz *\n```", "```py\naws s3 cp model.tar.gz <s3://{my-s3-path}>\n```", "```py\nfrom sagemaker.huggingface.model import HuggingFaceModel\n\n# Hub model configuration <https://huggingface.co/models>\nhub = {\n  'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad', # model_id from hf.co/models\n  'HF_TASK':'question-answering'                           # NLP task you want to use for predictions\n}\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n   env=hub,                                                # configuration for loading model from Hub\n   role=role,                                              # IAM role with permissions to create an endpoint\n   transformers_version=\"4.26\",                             # Transformers version used\n   pytorch_version=\"1.13\",                                  # PyTorch version used\n   py_version='py39',                                      # Python version used\n)\n\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n   initial_instance_count=1,\n   instance_type=\"ml.m5.xlarge\"\n)\n\n# example request: you always need to define \"inputs\"\ndata = {\n\"inputs\": {\n\t\"question\": \"What is used for inference?\",\n\t\"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n\t}\n}\n\n# request\npredictor.predict(data)\n```", "```py\n# delete endpoint\npredictor.delete_endpoint()\n```", "```py\nbatch_job = huggingface_estimator.transformer(\n    instance_count=1,\n    instance_type='ml.p3.2xlarge',\n    strategy='SingleRecord')\n\nbatch_job.transform(\n    data='s3://s3-uri-to-batch-data',\n    content_type='application/json',    \n    split_type='Line')\n```", "```py\nfrom sagemaker.huggingface.model import HuggingFaceModel\n\n# Hub model configuration <https://huggingface.co/models>\nhub = {\n\t'HF_MODEL_ID':'distilbert-base-uncased-finetuned-sst-2-english',\n\t'HF_TASK':'text-classification'\n}\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n   env=hub,                                                # configuration for loading model from Hub\n   role=role,                                              # IAM role with permissions to create an endpoint\n   transformers_version=\"4.26\",                             # Transformers version used\n   pytorch_version=\"1.13\",                                  # PyTorch version used\n   py_version='py39',                                      # Python version used\n)\n\n# create transformer to run a batch job\nbatch_job = huggingface_model.transformer(\n    instance_count=1,\n    instance_type='ml.p3.2xlarge',\n    strategy='SingleRecord'\n)\n\n# starts batch transform job and uses S3 data as input\nbatch_job.transform(\n    data='s3://sagemaker-s3-demo-test/samples/input.jsonl',\n    content_type='application/json',    \n    split_type='Line'\n)\n```", "```py\n{\"inputs\":\"this movie is terrible\"}\n{\"inputs\":\"this movie is amazing\"}\n{\"inputs\":\"SageMaker is pretty cool\"}\n{\"inputs\":\"SageMaker is pretty cool\"}\n{\"inputs\":\"this movie is terrible\"}\n{\"inputs\":\"this movie is amazing\"}\n```", "```py\nmodel.tar.gz/\n|- pytorch_model.bin\n|- ....\n|- code/\n  |- inference.py\n  |- requirements.txt \n```", "```py\nfrom sagemaker_huggingface_inference_toolkit import decoder_encoder\n\ndef model_fn(model_dir):\n    # implement custom code to load the model\n    loaded_model = ...\n\n    return loaded_model \n\ndef input_fn(input_data, content_type):\n    # decode the input data  (e.g. JSON string -> dict)\n    data = decoder_encoder.decode(input_data, content_type)\n    return data\n\ndef predict_fn(data, model):\n    # call your custom model with the data\n    outputs = model(data , ... )\n    return predictions\n\ndef output_fn(prediction, accept):\n    # convert the model output to the desired output format (e.g. dict -> JSON string)\n    response = decoder_encoder.encode(prediction, accept)\n    return response\n```", "```py\nfrom sagemaker_huggingface_inference_toolkit import decoder_encoder\n\ndef model_fn(model_dir):\n    # implement custom code to load the model\n    loaded_model = ...\n\n    return loaded_model \n\ndef transform_fn(model, input_data, content_type, accept):\n     # decode the input data (e.g. JSON string -> dict)\n    data = decoder_encoder.decode(input_data, content_type)\n\n    # call your custom model with the data\n    outputs = model(data , ... ) \n\n    # convert the model output to the desired output format (e.g. dict -> JSON string)\n    response = decoder_encoder.encode(output, accept)\n\n    return response\n```"]