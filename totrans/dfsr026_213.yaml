- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/activations](https://huggingface.co/docs/diffusers/api/activations)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Customized activation functions for supporting various models in ðŸ¤— Diffusers.
  prefs: []
  type: TYPE_NORMAL
- en: GELU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.activations.GELU`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/activations.py#L50)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dim_in` (`int`) â€” The number of channels in the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dim_out` (`int`) â€” The number of channels in the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`approximate` (`str`, *optional*, defaults to `"none"`) â€” If `"tanh"`, use
    tanh approximation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias` (`bool`, defaults to True) â€” Whether to use a bias in the linear layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GELU activation function with tanh approximation support with `approximate="tanh"`.
  prefs: []
  type: TYPE_NORMAL
- en: GEGLU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.activations.GEGLU`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/activations.py#L78)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dim_in` (`int`) â€” The number of channels in the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dim_out` (`int`) â€” The number of channels in the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias` (`bool`, defaults to True) â€” Whether to use a bias in the linear layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [variant](https://arxiv.org/abs/2002.05202) of the gated linear unit activation
    function.
  prefs: []
  type: TYPE_NORMAL
- en: ApproximateGELU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.activations.ApproximateGELU`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/activations.py#L106)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dim_in` (`int`) â€” The number of channels in the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dim_out` (`int`) â€” The number of channels in the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias` (`bool`, defaults to True) â€” Whether to use a bias in the linear layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The approximate form of the Gaussian Error Linear Unit (GELU). For more details,
    see section 2 of this [paper](https://arxiv.org/abs/1606.08415).
  prefs: []
  type: TYPE_NORMAL
