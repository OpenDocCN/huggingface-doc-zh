- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/diffusers/api/activations](https://huggingface.co/docs/diffusers/api/activations)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/2.61123bb5.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Docstring.93f6f462.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
  prefs: []
  type: TYPE_NORMAL
- en: Customized activation functions for supporting various models in ðŸ¤— Diffusers.
  prefs: []
  type: TYPE_NORMAL
- en: GELU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.activations.GELU`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/activations.py#L50)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dim_in` (`int`) â€” The number of channels in the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dim_out` (`int`) â€” The number of channels in the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`approximate` (`str`, *optional*, defaults to `"none"`) â€” If `"tanh"`, use
    tanh approximation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias` (`bool`, defaults to True) â€” Whether to use a bias in the linear layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GELU activation function with tanh approximation support with `approximate="tanh"`.
  prefs: []
  type: TYPE_NORMAL
- en: GEGLU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.activations.GEGLU`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/activations.py#L78)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dim_in` (`int`) â€” The number of channels in the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dim_out` (`int`) â€” The number of channels in the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias` (`bool`, defaults to True) â€” Whether to use a bias in the linear layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [variant](https://arxiv.org/abs/2002.05202) of the gated linear unit activation
    function.
  prefs: []
  type: TYPE_NORMAL
- en: ApproximateGELU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.activations.ApproximateGELU`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/activations.py#L106)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dim_in` (`int`) â€” The number of channels in the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dim_out` (`int`) â€” The number of channels in the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias` (`bool`, defaults to True) â€” Whether to use a bias in the linear layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The approximate form of the Gaussian Error Linear Unit (GELU). For more details,
    see section 2 of this [paper](https://arxiv.org/abs/1606.08415).
  prefs: []
  type: TYPE_NORMAL
