["```py\npip install optimum[exporters]\n```", "```py\noptimum-cli export onnx --help\n```", "```py\noptimum-cli export onnx --model distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/\n```", "```py\nValidating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...\n\t-[\u2713] ONNX model output names match reference model (start_logits, end_logits)\n\t- Validating ONNX Model output \"start_logits\":\n\t\t-[\u2713] (2, 16) matches (2, 16)\n\t\t-[\u2713] all values close (atol: 0.0001)\n\t- Validating ONNX Model output \"end_logits\":\n\t\t-[\u2713] (2, 16) matches (2, 16)\n\t\t-[\u2713] all values close (atol: 0.0001)\nThe ONNX export succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx\n```", "```py\noptimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/\n```", "```py\n>>> from transformers import AutoTokenizer\n>>> from optimum.onnxruntime import ORTModelForQuestionAnswering\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n>>> model = ORTModelForQuestionAnswering.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n>>> inputs = tokenizer(\"What am I using?\", \"Using DistilBERT with ONNX Runtime!\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n```", "```py\noptimum-cli export onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/\n```", "```py\n>>> from optimum.onnxruntime import ORTModelForSequenceClassification\n>>> from transformers import AutoTokenizer\n\n>>> model_checkpoint = \"distilbert_base_uncased_squad\"\n>>> save_directory = \"onnx/\"\n\n>>> # Load a model from transformers and export it to ONNX\n>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)\n>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n>>> # Save the onnx model and tokenizer\n>>> ort_model.save_pretrained(save_directory)\n>>> tokenizer.save_pretrained(save_directory)\n```", "```py\npip install transformers[onnx]\n```", "```py\npython -m transformers.onnx --model=distilbert-base-uncased onnx/\n```", "```py\n>>> from transformers import AutoTokenizer\n>>> from onnxruntime import InferenceSession\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n>>> session = InferenceSession(\"onnx/model.onnx\")\n>>> # ONNX Runtime expects NumPy arrays as input\n>>> inputs = tokenizer(\"Using DistilBERT with ONNX Runtime!\", return_tensors=\"np\")\n>>> outputs = session.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))\n```", "```py\n>>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig\n\n>>> config = DistilBertConfig()\n>>> onnx_config = DistilBertOnnxConfig(config)\n>>> print(list(onnx_config.outputs.keys()))\n[\"last_hidden_state\"]\n```", "```py\npython -m transformers.onnx --model=keras-io/transformers-qa onnx/\n```", "```py\npython -m transformers.onnx --model=local-pt-checkpoint onnx/\n```"]