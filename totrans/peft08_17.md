# 混合适配器类型

> 原文链接: [`huggingface.co/docs/peft/developer_guides/mixed_models`](https://huggingface.co/docs/peft/developer_guides/mixed_models)

通常情况下，在🤗 PEFT 中无法混合不同类型的适配器。您可以创建一个具有两个不同 LoRA 适配器的 PEFT 模型（可以具有不同的配置选项），但无法组合 LoRA 和 LoHa 适配器。但是，使用 PeftMixedModel，只要适配器类型兼容，就可以实现这一点。允许混合适配器类型的主要目的是为了结合推理的训练适配器。虽然可以训练混合适配器模型，但尚未经过测试，不建议使用。

要将不同类型的适配器加载到 PEFT 模型中，请使用 PeftMixedModel 而不是 PeftModel：

```py
from peft import PeftMixedModel

base_model = ...  # load the base model, e.g. from transformers
# load first adapter, which will be called "default"
peft_model = PeftMixedModel.from_pretrained(base_model, <path_to_adapter1>)
peft_model.load_adapter(<path_to_adapter2>, adapter_name="other")
peft_model.set_adapter(["default", "other"])
```

必须使用 set_adapter()方法来激活两个适配器，否则只有第一个适配器会处于活动状态。您可以通过反复调用 add_adapter()来继续添加更多适配器。

PeftMixedModel 不支持保存和加载混合适配器。适配器应该已经训练好，加载模型需要每次运行一个脚本。

## 提示

+   并非所有适配器类型都可以组合。请参阅[`peft.tuners.mixed.COMPATIBLE_TUNER_TYPES`](https://github.com/huggingface/peft/blob/1c1c7fdaa6e6abaa53939b865dee1eded82ad032/src/peft/tuners/mixed/model.py#L35)以获取兼容类型的列表。如果尝试组合不兼容的适配器类型，将会引发错误。

+   可以混合多个相同类型的适配器，这对于将具有非常不同配置的适配器组合在一起非常有用。

+   如果要组合许多不同的适配器，最有效的方法是依次添加相同类型的适配器。例如，按照 LoRA1、LoRA2、LoHa1、LoHa2 的顺序添加，而不是 LoRA1、LoHa1、LoRA2 和 LoHa2。虽然顺序可能会影响输出，但并没有固定的*最佳*顺序，因此最好选择最快的顺序。
