# 稳定扩散2

> 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_2](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_2)

稳定扩散2是一个基于原始[稳定扩散](https://stability.ai/blog/stable-diffusion-public-release)工作构建的文本到图像*潜在扩散*模型，由[Stability AI](https://stability.ai/)和[LAION](https://laion.ai/)的Robin Rombach和Katherine Crowson领导。

*稳定扩散2.0版本包括使用全新文本编码器（OpenCLIP）训练的稳定文本到图像模型，由LAION开发，得到了Stability AI的支持，与早期V1版本相比，大大提高了生成图像的质量。此版本中的文本到图像模型可以生成默认分辨率为512x512像素和768x768像素的图像。这些模型是在由Stability AI的DeepFloyd团队创建的[LAION-5B数据集](https://laion.ai/blog/laion-5b/)的美学子集上进行训练的，然后使用[LAION的NSFW过滤器](https://openreview.net/forum?id=M3Y74vmsMcY)进一步过滤以删除成人内容。*

有关稳定扩散2的工作原理及其与原始稳定扩散的区别的更多详细信息，请参阅官方[公告帖](https://stability.ai/blog/stable-diffusion-v2-release)。

稳定扩散2的架构与原始[稳定扩散模型](./text2img)几乎相同，因此请查看其API文档以了解如何使用稳定扩散2。我们建议使用[DPMSolverMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler)，因为它在速度/质量的权衡方面提供了合理的选择，并且可以在至少20个步骤下运行。

稳定扩散2可用于文本到图像、补洞、超分辨率和深度到图像等任务：

| 任务 | 仓库 |
| --- | --- |
| 文本到图像（512x512） | [stabilityai/stable-diffusion-2-base](https://huggingface.co/stabilityai/stable-diffusion-2-base) |
| 文本到图像（768x768） | [stabilityai/stable-diffusion-2](https://huggingface.co/stabilityai/stable-diffusion-2) |
| 补洞 | [stabilityai/stable-diffusion-2-inpainting](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting) |
| 超分辨率 | [stable-diffusion-x4-upscaler](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler) |
| 深度到图像 | [stabilityai/stable-diffusion-2-depth](https://huggingface.co/stabilityai/stable-diffusion-2-depth) |

以下是如何为每个任务使用稳定扩散2的一些示例：

请务必查看稳定扩散[Tips](overview#tips)部分，了解如何探索调度器速度和质量之间的权衡，以及如何有效地重用管道组件！

如果您有兴趣使用任务的官方检查点之一，请探索[CompVis](https://huggingface.co/CompVis)、[Runway](https://huggingface.co/runwayml)和[Stability AI](https://huggingface.co/stabilityai) Hub组织！

## 文本到图像

```py
from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler
import torch

repo_id = "stabilityai/stable-diffusion-2-base"
pipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, revision="fp16")

pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
pipe = pipe.to("cuda")

prompt = "High quality photo of an astronaut riding a horse in space"
image = pipe(prompt, num_inference_steps=25).images[0]
image
```

## 补洞

```py
import torch
from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler
from diffusers.utils import load_image, make_image_grid

img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"

init_image = load_image(img_url).resize((512, 512))
mask_image = load_image(mask_url).resize((512, 512))

repo_id = "stabilityai/stable-diffusion-2-inpainting"
pipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, revision="fp16")

pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
pipe = pipe.to("cuda")

prompt = "Face of a yellow cat, high resolution, sitting on a park bench"
image = pipe(prompt=prompt, image=init_image, mask_image=mask_image, num_inference_steps=25).images[0]
make_image_grid([init_image, mask_image, image], rows=1, cols=3)
```

## 超分辨率

```py
from diffusers import StableDiffusionUpscalePipeline
from diffusers.utils import load_image, make_image_grid
import torch

# load model and scheduler
model_id = "stabilityai/stable-diffusion-x4-upscaler"
pipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipeline = pipeline.to("cuda")

# let's download an  image
url = "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png"
low_res_img = load_image(url)
low_res_img = low_res_img.resize((128, 128))
prompt = "a white cat"
upscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]
make_image_grid([low_res_img.resize((512, 512)), upscaled_image.resize((512, 512))], rows=1, cols=2)
```

## 深度到图像

```py
import torch
from diffusers import StableDiffusionDepth2ImgPipeline
from diffusers.utils import load_image, make_image_grid

pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-depth",
    torch_dtype=torch.float16,
).to("cuda")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
init_image = load_image(url)
prompt = "two tigers"
negative_prompt = "bad, deformed, ugly, bad anotomy"
image = pipe(prompt=prompt, image=init_image, negative_prompt=negative_prompt, strength=0.7).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```
