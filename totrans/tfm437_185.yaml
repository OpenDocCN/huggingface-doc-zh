- en: Jukebox
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/jukebox](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/jukebox)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Jukebox model was proposed in [Jukebox: A generative model for music](https://arxiv.org/pdf/2005.00341.pdf)
    by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford,
    Ilya Sutskever. It introduces a generative music model which can produce minute
    long samples that can be conditioned on an artist, genres and lyrics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We introduce Jukebox, a model that generates music with singing in the raw
    audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE
    to compress it to discrete codes, and modeling those using autoregressive Transformers.
    We show that the combined model at scale can generate high-fidelity and diverse
    songs with coherence up to multiple minutes. We can condition on artist and genre
    to steer the musical and vocal style, and on unaligned lyrics to make the singing
    more controllable. We are releasing thousands of non cherry-picked samples, along
    with model weights and code.*'
  prefs: []
  type: TYPE_NORMAL
- en: As shown on the following figure, Jukebox is made of 3 `priors` which are decoder
    only models. They follow the architecture described in [Generating Long Sequences
    with Sparse Transformers](https://arxiv.org/abs/1904.10509), modified to support
    longer context length. First, a autoencoder is used to encode the text lyrics.
    Next, the first (also called `top_prior`) prior attends to the last hidden states
    extracted from the lyrics encoder. The priors are linked to the previous priors
    respectively via an `AudioConditionner` module. The`AudioConditioner` upsamples
    the outputs of the previous prior to raw tokens at a certain audio frame per second
    resolution. The metadata such as *artist, genre and timing* are passed to each
    prior, in the form of a start token and positional embedding for the timing data.
    The hidden states are mapped to the closest codebook vector from the VQVAE in
    order to convert them to raw audio.
  prefs: []
  type: TYPE_NORMAL
- en: '![JukeboxModel](../Images/5772cc27b249201f12c4d49bc48dfd5e.png)'
  prefs: []
  type: TYPE_IMG
- en: This model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ).
    The original code can be found [here](https://github.com/openai/jukebox).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This model only supports inference. This is for a few reasons, mostly because
    it requires a crazy amount of memory to train. Feel free to open a PR and add
    what’s missing to have a full integration with the hugging face traineer!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This model is very slow, and takes 8h to generate a minute long audio using
    the 5b top prior on a V100 GPU. In order automaticallay handle the device on which
    the model should execute, use `accelerate`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Contrary to the paper, the order of the priors goes from `0` to `1` as it felt
    more intuitive : we sample starting from `0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Primed sampling (conditioning the sampling on raw audio) requires more memory
    than ancestral sampling and should be used with `fp16` set to `True`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ).
    The original code can be found [here](https://github.com/openai/jukebox).
  prefs: []
  type: TYPE_NORMAL
- en: JukeboxConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.JukeboxConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/configuration_jukebox.py#L495)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vqvae_config` (`JukeboxVQVAEConfig`, *optional*) — Configuration for the `JukeboxVQVAE`
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_config_list` (`List[JukeboxPriorConfig]`, *optional*) — List of the
    configs for each of the `JukeboxPrior` of the model. The original architecture
    uses 3 priors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nb_priors` (`int`, *optional*, defaults to 3) — Number of prior models that
    will sequentially sample tokens. Each prior is conditional auto regressive (decoder)
    model, apart from the top prior, which can include a lyric encoder. The available
    models were trained using a top prior and 2 upsampler priors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*, defaults to 44100) — Sampling rate of the
    raw audio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timing_dims` (`int`, *optional*, defaults to 64) — Dimensions of the JukeboxRangeEmbedding
    layer which is equivalent to traditional positional embedding layer. The timing
    embedding layer converts the absolute and relative position in the currently sampled
    audio to a tensor of length `timing_dims` that will be added to the music tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_duration` (`int`, *optional*, defaults to 0) — Minimum duration of the
    audios to generate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_duration` (`float`, *optional*, defaults to 600.0) — Maximum duration
    of the audios to generate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_nb_genres` (`int`, *optional*, defaults to 5) — Maximum number of genres
    that can be used to condition a single sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata_conditioning` (`bool`, *optional*, defaults to `True`) — Whether
    or not to use metadata conditioning, corresponding to the artist, the genre and
    the min/maximum duration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [JukeboxModel](/docs/transformers/v4.37.2/en/model_doc/jukebox#transformers.JukeboxModel).
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information. Instantiating a configuration with the defaults will yield
    a similar configuration to that of [openai/jukebox-1b-lyrics](https://huggingface.co/openai/jukebox-1b-lyrics)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The downsampling and stride are used to determine downsampling of the input
    sequence. For example, downsampling = (5,3), and strides = (2, 2) will downsample
    the audio by 2^5 = 32 to get the first level of codes, and 2**8 = 256 to get the
    second level codes. This is mostly true for training the top level prior and the
    upsamplers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_configs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/configuration_jukebox.py#L598)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[JukeboxConfig](/docs/transformers/v4.37.2/en/model_doc/jukebox#transformers.JukeboxConfig)'
  prefs: []
  type: TYPE_NORMAL
- en: An instance of a configuration object
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a [JukeboxConfig](/docs/transformers/v4.37.2/en/model_doc/jukebox#transformers.JukeboxConfig)
    (or a derived class) from clip text model configuration and clip vision model
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: JukeboxPriorConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.JukeboxPriorConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/configuration_jukebox.py#L143)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`act_fn` (`str`, *optional*, defaults to `"quick_gelu"`) — Activation function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alignment_head` (`int`, *optional*, defaults to 2) — Head that is responsible
    of the alignment between lyrics and music. Only used to compute the lyric to audio
    alignment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alignment_layer` (`int`, *optional*, defaults to 68) — Index of the layer
    that is responsible of the alignment between lyrics and music. Only used to compute
    the lyric to audio alignment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_multiplier` (`float`, *optional*, defaults to 0.25) — Multiplier
    coefficient used to define the hidden dimension of the attention layers. 0.25
    means that 0.25*width of the model will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_pattern` (`str`, *optional*, defaults to `"enc_dec_with_lyrics"`)
    — Which attention pattern to use for the decoder/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attn_dropout` (`int`, *optional*, defaults to 0) — Dropout probability for
    the post-attention layer dropout in the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attn_res_scale` (`bool`, *optional*, defaults to `False`) — Whether or not
    to scale the residuals in the attention conditioner block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`blocks` (`int`, *optional*, defaults to 64) — Number of blocks used in the
    `block_attn`. A sequence of length seq_len is factored as `[blocks, seq_len //
    blocks]` in the `JukeboxAttention` layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conv_res_scale` (`int`, *optional*) — Whether or not to scale the residuals
    in the conditioner block. Since the top level prior does not have a conditioner,
    the default value is to None and should not be modified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_layers` (`int`, *optional*, defaults to 72) — Number of layers of the
    transformer architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`emb_dropout` (`int`, *optional*, defaults to 0) — Embedding dropout used in
    the lyric decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_config` (`JukeboxPriorConfig`, *optional*) — Configuration of the
    encoder which models the prior on the lyrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_loss_fraction` (`float`, *optional*, defaults to 0.4) — Multiplication
    factor used in front of the lyric encoder loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 2048) — Hidden dimension of the
    attention layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_scale` (`float`, *optional*, defaults to 0.2) — Initialization scales
    for the prior modules.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_encoder_decoder` (`bool`, *optional*, defaults to `True`) — Whether or
    not the prior is an encoder-decoder model. In case it is not, and `nb_relevant_lyric_tokens`
    is greater than 0, the `encoder` args should be specified for the lyric encoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask` (`bool`, *optional*, defaults to `False`) — Whether or not to mask the
    previous positions in the attention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_duration` (`int`, *optional*, defaults to 600) — Maximum supported duration
    of the generated song in seconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_nb_genres` (`int`, *optional*, defaults to 1) — Maximum number of genres
    that can be used to condition the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`merged_decoder` (`bool`, *optional*, defaults to `True`) — Whether or not
    the decoder and the encoder inputs are merged. This is used for the separated
    encoder-decoder architecture'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata_conditioning` (`bool`, *optional*, defaults to `True)` — Whether
    or not to condition on the artist and genre metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata_dims` (`List[int]`, *optional*, defaults to `[604, 7898]`) — Number
    of genres and the number of artists that were used to train the embedding layers
    of the prior models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_duration` (`int`, *optional*, defaults to 0) — Minimum duration of the
    generated audio on which the model was trained.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlp_multiplier` (`float`, *optional*, defaults to 1.0) — Multiplier coefficient
    used to define the hidden dimension of the MLP layers. 0.25 means that 0.25*width
    of the model will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`music_vocab_size` (`int`, *optional*, defaults to 2048) — Number of different
    music tokens. Should be similar to the `JukeboxVQVAEConfig.nb_discrete_codes`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_ctx` (`int`, *optional*, defaults to 6144) — Number of context tokens for
    each prior. The context tokens are the music tokens that are attended to when
    generating music tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_heads` (`int`, *optional*, defaults to 2) — Number of attention heads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nb_relevant_lyric_tokens` (`int`, *optional*, defaults to 384) — Number of
    lyric tokens that are used when sampling a single window of length `n_ctx`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`res_conv_depth` (`int`, *optional*, defaults to 3) — Depth of the `JukeboxDecoderConvBock`
    used to upsample the previously sampled audio in the `JukeboxMusicTokenConditioner`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`res_conv_width` (`int`, *optional*, defaults to 128) — Width of the `JukeboxDecoderConvBock`
    used to upsample the previously sampled audio in the `JukeboxMusicTokenConditioner`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`res_convolution_multiplier` (`int`, *optional*, defaults to 1) — Multiplier
    used to scale the `hidden_dim` of the `JukeboxResConv1DBlock`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`res_dilation_cycle` (`int`, *optional*) — Dilation cycle used to define the
    `JukeboxMusicTokenConditioner`. Usually similar to the ones used in the corresponding
    level of the VQVAE. The first prior does not use it as it is not conditioned on
    upper level tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`res_dilation_growth_rate` (`int`, *optional*, defaults to 1) — Dilation grow
    rate used between each convolutionnal block of the `JukeboxMusicTokenConditioner`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`res_downs_t` (`List[int]`, *optional*, defaults to `[3, 2, 2]`) — Downsampling
    rates used in the audio conditioning network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`res_strides_t` (`List[int]`, *optional*, defaults to `[2, 2, 2]`) — Striding
    used in the audio conditioning network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resid_dropout` (`int`, *optional*, defaults to 0) — Residual dropout used
    in the attention pattern.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*, defaults to 44100) — Sampling rate used
    for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spread` (`int`, *optional*) — Spread used in the `summary_spread_attention`
    pattern'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timing_dims` (`int`, *optional*, defaults to 64) — Dimension of the timing
    embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zero_out` (`bool`, *optional*, defaults to `False`) — Whether or not to zero
    out convolution weights when initializing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [JukeboxPrior](/docs/transformers/v4.37.2/en/model_doc/jukebox#transformers.JukeboxPrior).
    It is used to instantiate a `JukeboxPrior` according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the top level prior from the [openai/jukebox-1b-lyrics]([https://huggingface.co/openai/jukebox](https://huggingface.co/openai/jukebox)
    -1b-lyrics) architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: JukeboxVQVAEConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.JukeboxVQVAEConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/configuration_jukebox.py#L372)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`act_fn` (`str`, *optional*, defaults to `"relu"`) — Activation function of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nb_discrete_codes` (`int`, *optional*, defaults to 2048) — Number of codes
    of the VQVAE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`commit` (`float`, *optional*, defaults to 0.02) — Commit loss multiplier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conv_input_shape` (`int`, *optional*, defaults to 1) — Number of audio channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conv_res_scale` (`bool`, *optional*, defaults to `False`) — Whether or not
    to scale the residuals of the `JukeboxResConv1DBlock`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embed_dim` (`int`, *optional*, defaults to 64) — Embedding dimension of the
    codebook vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hop_fraction` (`List[int]`, *optional*, defaults to `[0.125, 0.5, 0.5]`) —
    Fraction of non-intersecting window used when continuing the sampling process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`levels` (`int`, *optional*, defaults to 3) — Number of hierarchical levels
    that used in the VQVAE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lmu` (`float`, *optional*, defaults to 0.99) — Used in the codebook update,
    exponential moving average coefficient. For more detail refer to Appendix A.1
    of the original [VQVAE paper](https://arxiv.org/pdf/1711.00937v2.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multipliers` (`List[int]`, *optional*, defaults to `[2, 1, 1]`) — Depth and
    width multipliers used for each level. Used on the `res_conv_width` and `res_conv_depth`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`res_conv_depth` (`int`, *optional*, defaults to 4) — Depth of the encoder
    and decoder block. If no `multipliers` are used, this is the same for each level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`res_conv_width` (`int`, *optional*, defaults to 32) — Width of the encoder
    and decoder block. If no `multipliers` are used, this is the same for each level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`res_convolution_multiplier` (`int`, *optional*, defaults to 1) — Scaling factor
    of the hidden dimension used in the `JukeboxResConv1DBlock`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`res_dilation_cycle` (`int`, *optional*) — Dilation cycle value used in the
    `JukeboxResnet`. If an int is used, each new Conv1 block will have a depth reduced
    by a power of `res_dilation_cycle`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`res_dilation_growth_rate` (`int`, *optional*, defaults to 3) — Resnet dilation
    growth rate used in the VQVAE (dilation_growth_rate ** depth)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`res_downs_t` (`List[int]`, *optional*, defaults to `[3, 2, 2]`) — Downsampling
    rate for each level of the hierarchical VQ-VAE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`res_strides_t` (`List[int]`, *optional*, defaults to `[2, 2, 2]`) — Stride
    used for each level of the hierarchical VQ-VAE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample_length` (`int`, *optional*, defaults to 1058304) — Provides the max
    input shape of the VQVAE. Is used to compute the input shape of each level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_scale` (`float`, *optional*, defaults to 0.2) — Initialization scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zero_out` (`bool`, *optional*, defaults to `False`) — Whether or not to zero
    out convolution weights when initializing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [JukeboxVQVAE](/docs/transformers/v4.37.2/en/model_doc/jukebox#transformers.JukeboxVQVAE).
    It is used to instantiate a `JukeboxVQVAE` according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the VQVAE from [openai/jukebox-1b-lyrics](https://huggingface.co/openai/jukebox-1b-lyrics)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: JukeboxTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.JukeboxTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/tokenization_jukebox.py#L59)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`artists_file` (`str`) — Path to the vocabulary file which contains a mapping
    between artists and ids. The default file supports both “v2” and “v3”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`genres_file` (`str`) — Path to the vocabulary file which contain a mapping
    between genres and ids.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lyrics_file` (`str`) — Path to the vocabulary file which contains the accepted
    characters for the lyrics tokenization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`version` (`List[str]`, `optional`, default to `["v3", "v2", "v2"]`) — List
    of the tokenizer versions. The `5b-lyrics`’s top level prior model was trained
    using `v3` instead of `v2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_genres` (`int`, `optional`, defaults to 1) — Maximum number of genres to
    use for composition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_n_lyric_tokens` (`int`, `optional`, defaults to 512) — Maximum number
    of lyric tokens to keep.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The unknown
    token. A token that is not in the vocabulary cannot be converted to an ID and
    is set to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--未知令牌。词汇表中没有的令牌无法转换为ID，而是设置为该令牌。'
- en: 'Constructs a Jukebox tokenizer. Jukebox can be conditioned on 3 different inputs
    :'
  prefs: []
  type: TYPE_NORMAL
- en: Artists, unique ids are associated to each artist from the provided dictionary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Genres, unique ids are associated to each genre from the provided dictionary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lyrics, character based tokenization. Must be initialized with the list of characters
    that are inside the vocabulary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This tokenizer does not require training. It should be able to process a different
    number of inputs: as the conditioning of the model can be done on the three different
    queries. If None is provided, defaults values will be used.:'
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the number of genres on which the model should be conditioned (`n_genres`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You can get around that behavior by passing `add_prefix_space=True` when instantiating
    this tokenizer or when you call it on some text, but since the model was not pretrained
    this way, it might yield a decrease in performance.
  prefs: []
  type: TYPE_NORMAL
- en: If nothing is provided, the genres and the artist will either be selected randomly
    or set to None
  prefs: []
  type: TYPE_NORMAL
- en: 'This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to: this superclass
    for more information regarding those methods.'
  prefs: []
  type: TYPE_NORMAL
- en: However the code does not allow that and only supports composing from various
    genres.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_vocabulary`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/tokenization_jukebox.py#L372)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`save_directory` (`str`) — A path to the directory where to saved. It will
    be created if it doesn’t exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filename_prefix` (`Optional[str]`, *optional*) — A prefix to add to the names
    of the files saved by the tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saves the tokenizer’s vocabulary dictionary to the provided save_directory.
  prefs: []
  type: TYPE_NORMAL
- en: JukeboxModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.JukeboxModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2291)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` (`JukeboxConfig`) — Model configuration class with all the parameters
    of the model. Initializing with a config file does not load the weights associated
    with the model, only the configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The bare JUKEBOX Model used for music generation. 4 sampling techniques are
    supported : `primed_sample`, `upsample`, `continue_sample` and `ancestral_sample`.
    It does not have a `forward` method as the training is not end to end. If you
    want to fine-tune the model, it is recommended to use the `JukeboxPrior` class
    and train each prior individually.'
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `ancestral_sample`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2573)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`labels` (`List[torch.LongTensor]`) — List of length `n_sample`, and shape
    `(self.levels, 4 + self.config.max_nb_genre + lyric_sequence_length)` metadata
    such as `artist_id`, `genre_id` and the full list of lyric tokens which are used
    to condition the generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_samples` (`int`, *optional*, default to 1) — Number of samples to be generated
    in parallel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates music tokens based on the provided `labels. Will start at the desired
    prior level and automatically upsample the sequence. If you want to create the
    audio, you should call` model.decode(tokens)`, which will use the VQ-VAE decoder
    to convert the music tokens to raw audio.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#### `primed_sample`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2649)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`raw_audio` (`List[torch.Tensor]` of length `n_samples` ) — A list of raw audio
    that will be used as conditioning information for each samples that will be generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`List[torch.LongTensor]` of length `n_sample`, and shape `(self.levels,
    self.config.max_nb_genre + lyric_sequence_length)` — List of metadata such as
    `artist_id`, `genre_id` and the full list of lyric tokens which are used to condition
    the generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_kwargs` (`Dict[Any]`) — Various additional sampling arguments that
    are used by the `_sample` function. A detail list of the arguments can bee seen
    in the `_sample` function documentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Generate a raw audio conditioned on the provided `raw_audio` which is used
    as conditioning at each of the generation levels. The audio is encoded to music
    tokens using the 3 levels of the VQ-VAE. These tokens are used: as conditioning
    for each level, which means that no ancestral sampling is required.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `continue_sample`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2619)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`music_tokens` (`List[torch.LongTensor]` of length `self.levels` ) — A sequence
    of music tokens which will be used as context to continue the sampling process.
    Should have `self.levels` tensors, each corresponding to the generation at a certain
    level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`List[torch.LongTensor]` of length `n_sample`, and shape `(self.levels,
    self.config.max_nb_genre + lyric_sequence_length)` — List of metadata such as
    `artist_id`, `genre_id` and the full list of lyric tokens which are used to condition
    the generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_kwargs` (`Dict[Any]`) — Various additional sampling arguments that
    are used by the `_sample` function. A detail list of the arguments can bee seen
    in the `_sample` function documentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates a continuation of the previously generated tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `upsample`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2634)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`music_tokens` (`List[torch.LongTensor]` of length `self.levels` ) — A sequence
    of music tokens which will be used as context to continue the sampling process.
    Should have `self.levels` tensors, each corresponding to the generation at a certain
    level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`List[torch.LongTensor]` of length `n_sample`, and shape `(self.levels,
    self.config.max_nb_genre + lyric_sequence_length)` — List of metadata such as
    `artist_id`, `genre_id` and the full list of lyric tokens which are used to condition
    the generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_kwargs` (`Dict[Any]`) — Various additional sampling arguments that
    are used by the `_sample` function. A detail list of the arguments can bee seen
    in the `_sample` function documentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upsamples a sequence of music tokens using the prior at level `level`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `_sample`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2434)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`music_tokens` (`List[torch.LongTensor]`) — A sequence of music tokens of length
    `self.levels` which will be used as context to continue the sampling process.
    Should have `self.levels` tensors, each corresponding to the generation at a certain
    level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`List[torch.LongTensor]`) — List of length `n_sample`, and shape
    `(self.levels, 4 + self.config.max_nb_genre + lyric_sequence_length)` metadata
    such as `artist_id`, `genre_id` and the full list of lyric tokens which are used
    to condition the generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample_levels` (`List[int]`) — List of the desired levels at which the sampling
    will be done. A level is equivalent to the index of the prior in the list of priors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metas` (`List[Any]`, *optional*) — Metadatas used to generate the `labels`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunk_size` (`int`, *optional*, defaults to 32) — Size of a chunk of audio,
    used to fill up the memory in chuncks to prevent OOM erros. Bigger chunks means
    faster memory filling but more consumption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_temperature` (`float`, *optional*, defaults to 0.98) — Temperature
    used to ajust the randomness of the sampling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lower_batch_size` (`int`, *optional*, defaults to 16) — Maximum batch size
    for the lower level priors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_batch_size` (`int`, *optional*, defaults to 16) — Maximum batch size for
    the top level priors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample_length_in_seconds` (`int`, *optional*, defaults to 24) — Desired length
    of the generation in seconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`compute_alignments` (`bool`, *optional*, defaults to `False`) — Whether or
    not to compute the alignment between the lyrics and the audio using the top_prior'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample_tokens` (`int`, *optional*) — Precise number of tokens that should
    be sampled at each level. This is mostly useful for running dummy experiments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offset` (`int`, *optional*, defaults to 0) — Audio offset used as conditioning,
    corresponds to the starting sample in the music. If the offset is greater than
    0, the lyrics will be shifted take that intoaccount'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save_results` (`bool`, *optional*, defaults to `True`) — Whether or not to
    save the intermediate results. If `True`, will generate a folder named with the
    start time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample_length` (`int`, *optional*) — Desired length of the generation in samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core sampling function used to generate music tokens. Iterates over the provided
    list of levels, while saving the generated raw audio at each step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns: torch.Tensor'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: JukeboxPrior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.JukeboxPrior`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L1769)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` (`JukeboxPriorConfig`) — Model configuration class with all the parameters
    of the model. Initializing with a config file does not load the weights associated
    with the model, only the configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`level` (`int`, *optional*) — Current level of the Prior. Should be in range
    `[0,nb_priors]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nb_priors` (`int`, *optional*, defaults to 3) — Total number of priors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vqvae_encoder` (`Callable`, *optional*) — Encoding method of the VQVAE encoder
    used in the forward pass of the model. Passing functions instead of the vqvae
    module to avoid getting the parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vqvae_decoder` (`Callable`, *optional*) — Decoding method of the VQVAE decoder
    used in the forward pass of the model. Passing functions instead of the vqvae
    module to avoid getting the parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The JukeboxPrior class, which is a wrapper around the various conditioning and
    the transformer. JukeboxPrior can be seen as language models trained on music.
    They model the next `music token` prediction task. If a (lyric) `encoderù is defined,
    it also models the` next character` prediction on the lyrics. Can be conditionned
    on timing, artist, genre, lyrics and codes from lower-levels Priors.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `sample`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2059)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`n_samples` (`int`) — Number of samples to generate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`music_tokens` (`List[torch.LongTensor]`, *optional*) — Previously gemerated
    tokens at the current level. Used as context for the generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`music_tokens_conds` (`List[torch.FloatTensor]`, *optional*) — Upper-level
    music tokens generated by the previous prior model. Is `None` if the generation
    is not conditionned on the upper-level tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata` (`List[torch.LongTensor]`, *optional*) — List containing the metatdata
    tensor with the artist, genre and the lyric tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temp` (`float`, *optional*, defaults to 1.0) — Sampling temperature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_k` (`int`, *optional*, defaults to 0) — Top k probabilities used for filtering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_p` (`float`, *optional*, defaults to 0.0) — Top p probabilities used for
    filtering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunk_size` (`int`, *optional*) — Size of the chunks used to prepare the cache
    of the transformer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample_tokens` (`int`, *optional*) — Number of tokens to sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ancestral/Prime sampling a window of tokens using the provided conditioning
    and metadatas.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2227)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_states` (`torch.Tensor`) — Hidden states which should be raw audio'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata` (`List[torch.LongTensor]`, *optional*) — List containing the metadata
    conditioning tensorwith the lyric and the metadata tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decode` (`bool`, *optional*, defaults to `False`) — Whether or not to decode
    the encoded to tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get_preds` (`bool`, *optional*, defaults to `False`) — Whether or not to return
    the actual predicitons of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encode the hidden states using the `vqvae` encoder, and then predicts the next
    token in the `forward_tokens` function. The loss is the sum of the `encoder` loss
    and the `decoder` loss.
  prefs: []
  type: TYPE_NORMAL
- en: JukeboxVQVAE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.JukeboxVQVAE`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L595)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` (`JukeboxConfig`) — Model configuration class with all the parameters
    of the model. Initializing with a config file does not load the weights associated
    with the model, only the configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hierarchical VQ-VAE model used in Jukebox. This model follows the Hierarchical
    VQVAE paper from [Will Williams, Sam Ringer, Tom Ash, John Hughes, David MacLeod,
    Jamie Dougherty](https://arxiv.org/abs/2002.08111).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L739)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`raw_audio` (`torch.FloatTensor`) — Audio input which will be encoded and decoded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forward pass of the VQ-VAE, encodes the `raw_audio` to latent states, which
    are then decoded for each level. The commit loss, which ensure that the encoder’s
    computed embeddings are close to the codebook vectors, is computed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '#### `encode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L709)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_audio` (`torch.Tensor`) — Raw audio which will be encoded to its discrete
    representation using the codebook. The closest `code` form the codebook will be
    computed for each sequence of samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_level` (`int`, *optional*, defaults to 0) — Level at which the encoding
    process will start. Default to 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_level` (`int`, *optional*) — Level at which the encoding process will
    start. Default to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bs_chunks` (int, *optional*, defaults to 1) — Number of chunks of raw audio
    to process at the same time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforms the `input_audio` to a discrete representation made out of `music_tokens`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L673)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`music_tokens` (`torch.LongTensor`) — Tensor of music tokens which will be
    decoded to raw audio by using the codebook. Each music token should be an index
    to a corresponding `code` vector in the codebook.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_level` (`int`, *optional*) — Level at which the decoding process will
    start. Default to 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_level` (`int`, *optional*) — Level at which the decoding process will
    start. Default to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bs_chunks` (int, *optional*) — Number of chunks to process at the same time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforms the input `music_tokens` to their `raw_audio` representation.
  prefs: []
  type: TYPE_NORMAL
