- en: Adding support for new architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/optimum-neuron/community/contributing](https://huggingface.co/docs/optimum-neuron/community/contributing)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: '***NOTE:*** ‚ùóThis section does not apply to the decoder model‚Äôs inference with
    autoregressive sampling integrated through `transformers-neuronx`. If you want
    to add support for these models, please open an issue on the Optimum Neuron GitHub
    repo, and ping maintainers for help.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You want to export and run a new model on AWS Inferentia or Trainium? Check
    the guideline, and submit a pull request to [ü§ó Optimum Neuron‚Äôs GitHub repo](https://github.com/huggingface/optimum-neuron/)!
  prefs: []
  type: TYPE_NORMAL
- en: 'To support a new model architecture in the Optimum Neuron library here are
    some steps to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement a custom Neuron configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Export and validate the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Contribute to the GitHub repo.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a custom Neuron configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To support the export of a new model to a Neuron compatible format, the first
    thing to do is to define a Neuron configuration, describing how to export the
    PyTorch model by specifying:'
  prefs: []
  type: TYPE_NORMAL
- en: The input names.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output names.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The dummy inputs used to trace the model: the Neuron Compiler records the computational
    graph via tracing and works on the resulting `TorchScript` module.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The compilation arguments used to control the trade-off between hardware efficiency
    (latency, throughput) and accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Depending on the choice of model and task, we represent the data above with
    configuration classes. Each configuration class is associated with a specific
    model architecture, and follows the naming convention `ArchitectureNameNeuronConfig`.
    For instance, the configuration that specifies the Neuron export of BERT models
    is `BertNeuronConfig`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since many architectures share similar properties for their Neuron configuration,
    ü§ó Optimum adopts a 3-level class hierarchy:'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract and generic base classes. These handle all the fundamental features,
    while being agnostic to the modality (text, image, audio, etc).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Middle-end classes. These are aware of the modality. Multiple config classes
    could exist for the same modality, depending on the inputs they support. They
    specify which input generators should be used for generating the dummy inputs,
    but remain model-agnostic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model-specific classes like the `BertNeuronConfig` mentioned above. These are
    the ones actually used to export models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Example: Adding support for ESM models'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here we take the support of [ESM models](https://huggingface.co/docs/transformers/model_doc/esm#esm)
    as an example. Let‚Äôs create an `EsmNeuronConfig` class in the `optimum/exporters/neuron/model_configs.py`.
  prefs: []
  type: TYPE_NORMAL
- en: When an Esm model interprets as a text encoder, we are able to inherit from
    the middle-end class [`TextEncoderNeuronConfig`](https://github.com/huggingface/optimum-neuron/blob/v0.0.18/optimum/exporters/neuron/config.py#L36).
    Since the modeling and configuration of Esm is almost the same as BERT when it
    is interpreted as an encoder, we can use the `NormalizedConfigManager` with `model_type=bert`
    to normalize the configuration to generate dummy inputs for tracing the model.
  prefs: []
  type: TYPE_NORMAL
- en: And one last step, since `optimum-neuron` is an extension of `optimum`, we need
    to register the Neuron config that we create to the [TasksManager](https://huggingface.co/docs/optimum/main/en/exporters/task_manager#optimum.exporters.TasksManager)
    with the `register_in_tasks_manager` decorator by specifying the model type and
    supported tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Export and validate the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the Neuron configuration class that you implemented, now do a quick test
    if it works as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: Export
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'During the export [`validate_model_outputs`](https://github.com/huggingface/optimum-neuron/blob/7b18de9ddfa5c664c94051304c651eaf855c3e0b/optimum/exporters/neuron/convert.py#L136)
    will be called to validate the outputs of your exported Neuron model by comparing
    them to the results of PyTorch on the CPU. You could also validate the model manually
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Inference (optional)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Contribute to the GitHub repo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are almost all set. Now submit a pull request to make your work accessible
    to all community members!
  prefs: []
  type: TYPE_NORMAL
- en: Open an issue in the [Optimum Neuron GitHub repo](https://github.com/huggingface/optimum-neuron/issues)
    to describe the new feature and make it visible to Optimum Neuron‚Äôs maintainers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add the model to the exporter test in [`optimum-neuron/tests/exporters/exporters_utils.py`](https://github.com/huggingface/optimum-neuron/blob/v0.0.18/tests/exporters/exporters_utils.py)
    and the inference test in [`optimum-neuron/tests/inference/inference_utils.py`](https://github.com/huggingface/optimum-neuron/blob/v0.0.18/tests/inference/inference_utils.py).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open a pull request! (Don‚Äôt forget to link it to the issue you opened, so that
    the maintainers could better track it and provide help when needed.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We usually test smaller checkpoints to accelerate the CIs, you could find tiny
    models for testing under the [`Hugging Face Internal Testing Organization`](https://huggingface.co/hf-internal-testing).
  prefs: []
  type: TYPE_NORMAL
- en: You have made a new model accessible on Neuron for the community! Thanks for
    joining us in the endeavor of democratizing good machine learning ü§ó.
  prefs: []
  type: TYPE_NORMAL
