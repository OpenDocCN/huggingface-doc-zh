["```py\nfrom datasets import load_dataset\n\n# prompts = load_dataset(\"nateraw/parti-prompts\", split=\"train\")\n# prompts = prompts.shuffle()\n# sample_prompts = [prompts[i][\"Prompt\"] for i in range(5)]\n\n# Fixing these sample prompts in the interest of reproducibility.\nsample_prompts = [\n    \"a corgi\",\n    \"a hot air balloon with a yin-yang symbol, with the moon visible in the daytime sky\",\n    \"a car with no windows\",\n    \"a cube made of porcupine\",\n    'The saying \"BE EXCELLENT TO EACH OTHER\" written on a red brick wall with a graffiti image of a green alien wearing a tuxedo. A yellow fire hydrant is on a sidewalk in the foreground.',\n]\n```", "```py\nimport torch\n\nseed = 0\ngenerator = torch.manual_seed(seed)\n\nimages = sd_pipeline(sample_prompts, num_images_per_prompt=1, generator=generator).images\n```", "```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_ckpt = \"CompVis/stable-diffusion-v1-4\"\nsd_pipeline = StableDiffusionPipeline.from_pretrained(model_ckpt, torch_dtype=torch.float16).to(\"cuda\")\n```", "```py\nprompts = [\n    \"a photo of an astronaut riding a horse on mars\",\n    \"A high tech solarpunk utopia in the Amazon rainforest\",\n    \"A pikachu fine dining with a view to the Eiffel Tower\",\n    \"A mecha robot in a favela in expressionist style\",\n    \"an insect robot preparing a delicious meal\",\n    \"A small cabin on top of a snowy mountain in the style of Disney, artstation\",\n]\n\nimages = sd_pipeline(prompts, num_images_per_prompt=1, output_type=\"np\").images\n\nprint(images.shape)\n# (6, 512, 512, 3)\n```", "```py\nfrom torchmetrics.functional.multimodal import clip_score\nfrom functools import partial\n\nclip_score_fn = partial(clip_score, model_name_or_path=\"openai/clip-vit-base-patch16\")\n\ndef calculate_clip_score(images, prompts):\n    images_int = (images * 255).astype(\"uint8\")\n    clip_score = clip_score_fn(torch.from_numpy(images_int).permute(0, 3, 1, 2), prompts).detach()\n    return round(float(clip_score), 4)\n\nsd_clip_score = calculate_clip_score(images, prompts)\nprint(f\"CLIP score: {sd_clip_score}\")\n# CLIP score: 35.7038\n```", "```py\nseed = 0\ngenerator = torch.manual_seed(seed)\n\nimages = sd_pipeline(prompts, num_images_per_prompt=1, generator=generator, output_type=\"np\").images\n```", "```py\nmodel_ckpt_1_5 = \"runwayml/stable-diffusion-v1-5\"\nsd_pipeline_1_5 = StableDiffusionPipeline.from_pretrained(model_ckpt_1_5, torch_dtype=weight_dtype).to(device)\n\nimages_1_5 = sd_pipeline_1_5(prompts, num_images_per_prompt=1, generator=generator, output_type=\"np\").images\n```", "```py\nsd_clip_score_1_4 = calculate_clip_score(images, prompts)\nprint(f\"CLIP Score with v-1-4: {sd_clip_score_1_4}\")\n# CLIP Score with v-1-4: 34.9102\n\nsd_clip_score_1_5 = calculate_clip_score(images_1_5, prompts)\nprint(f\"CLIP Score with v-1-5: {sd_clip_score_1_5}\")\n# CLIP Score with v-1-5: 36.2137\n```", "```py\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"sayakpaul/instructpix2pix-demo\", split=\"train\")\ndataset.features\n```", "```py\n{'input': Value(dtype='string', id=None),\n 'edit': Value(dtype='string', id=None),\n 'output': Value(dtype='string', id=None),\n 'image': Image(decode=True, id=None)}\n```", "```py\nidx = 0\nprint(f\"Original caption: {dataset[idx]['input']}\")\nprint(f\"Edit instruction: {dataset[idx]['edit']}\")\nprint(f\"Modified caption: {dataset[idx]['output']}\")\n```", "```py\nOriginal caption: 2\\. FAROE ISLANDS: An archipelago of 18 mountainous isles in the North Atlantic Ocean between Norway and Iceland, the Faroe Islands has 'everything you could hope for', according to Big 7 Travel. It boasts 'crystal clear waterfalls, rocky cliffs that seem to jut out of nowhere and velvety green hills'\nEdit instruction: make the isles all white marble\nModified caption: 2\\. WHITE MARBLE ISLANDS: An archipelago of 18 mountainous white marble isles in the North Atlantic Ocean between Norway and Iceland, the White Marble Islands has 'everything you could hope for', according to Big 7 Travel. It boasts 'crystal clear waterfalls, rocky cliffs that seem to jut out of nowhere and velvety green hills'\n```", "```py\ndataset[idx][\"image\"]\n```", "```py\nfrom diffusers import StableDiffusionInstructPix2PixPipeline\n\ninstruct_pix2pix_pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n    \"timbrooks/instruct-pix2pix\", torch_dtype=torch.float16\n).to(device)\n```", "```py\nimport numpy as np\n\ndef edit_image(input_image, instruction):\n    image = instruct_pix2pix_pipeline(\n        instruction,\n        image=input_image,\n        output_type=\"np\",\n        generator=generator,\n    ).images[0]\n    return image\n\ninput_images = []\noriginal_captions = []\nmodified_captions = []\nedited_images = []\n\nfor idx in range(len(dataset)):\n    input_image = dataset[idx][\"image\"]\n    edit_instruction = dataset[idx][\"edit\"]\n    edited_image = edit_image(input_image, edit_instruction)\n\n    input_images.append(np.array(input_image))\n    original_captions.append(dataset[idx][\"input\"])\n    modified_captions.append(dataset[idx][\"output\"])\n    edited_images.append(edited_image)\n```", "```py\nfrom transformers import (\n    CLIPTokenizer,\n    CLIPTextModelWithProjection,\n    CLIPVisionModelWithProjection,\n    CLIPImageProcessor,\n)\n\nclip_id = \"openai/clip-vit-large-patch14\"\ntokenizer = CLIPTokenizer.from_pretrained(clip_id)\ntext_encoder = CLIPTextModelWithProjection.from_pretrained(clip_id).to(device)\nimage_processor = CLIPImageProcessor.from_pretrained(clip_id)\nimage_encoder = CLIPVisionModelWithProjection.from_pretrained(clip_id).to(device)\n```", "```py\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DirectionalSimilarity(nn.Module):\n    def __init__(self, tokenizer, text_encoder, image_processor, image_encoder):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.text_encoder = text_encoder\n        self.image_processor = image_processor\n        self.image_encoder = image_encoder\n\n    def preprocess_image(self, image):\n        image = self.image_processor(image, return_tensors=\"pt\")[\"pixel_values\"]\n        return {\"pixel_values\": image.to(device)}\n\n    def tokenize_text(self, text):\n        inputs = self.tokenizer(\n            text,\n            max_length=self.tokenizer.model_max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        return {\"input_ids\": inputs.input_ids.to(device)}\n\n    def encode_image(self, image):\n        preprocessed_image = self.preprocess_image(image)\n        image_features = self.image_encoder(**preprocessed_image).image_embeds\n        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n        return image_features\n\n    def encode_text(self, text):\n        tokenized_text = self.tokenize_text(text)\n        text_features = self.text_encoder(**tokenized_text).text_embeds\n        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n        return text_features\n\n    def compute_directional_similarity(self, img_feat_one, img_feat_two, text_feat_one, text_feat_two):\n        sim_direction = F.cosine_similarity(img_feat_two - img_feat_one, text_feat_two - text_feat_one)\n        return sim_direction\n\n    def forward(self, image_one, image_two, caption_one, caption_two):\n        img_feat_one = self.encode_image(image_one)\n        img_feat_two = self.encode_image(image_two)\n        text_feat_one = self.encode_text(caption_one)\n        text_feat_two = self.encode_text(caption_two)\n        directional_similarity = self.compute_directional_similarity(\n            img_feat_one, img_feat_two, text_feat_one, text_feat_two\n        )\n        return directional_similarity\n```", "```py\ndir_similarity = DirectionalSimilarity(tokenizer, text_encoder, image_processor, image_encoder)\nscores = []\n\nfor i in range(len(input_images)):\n    original_image = input_images[i]\n    original_caption = original_captions[i]\n    edited_image = edited_images[i]\n    modified_caption = modified_captions[i]\n\n    similarity_score = dir_similarity(original_image, edited_image, original_caption, modified_caption)\n    scores.append(float(similarity_score.detach().cpu()))\n\nprint(f\"CLIP directional similarity: {np.mean(scores)}\")\n# CLIP directional similarity: 0.0797976553440094\n```", "```py\nfrom zipfile import ZipFile\nimport requests\n\ndef download(url, local_filepath):\n    r = requests.get(url)\n    with open(local_filepath, \"wb\") as f:\n        f.write(r.content)\n    return local_filepath\n\ndummy_dataset_url = \"https://hf.co/datasets/sayakpaul/sample-datasets/resolve/main/sample-imagenet-images.zip\"\nlocal_filepath = download(dummy_dataset_url, dummy_dataset_url.split(\"/\")[-1])\n\nwith ZipFile(local_filepath, \"r\") as zipper:\n    zipper.extractall(\".\")\n```", "```py\nfrom PIL import Image\nimport os\n\ndataset_path = \"sample-imagenet-images\"\nimage_paths = sorted([os.path.join(dataset_path, x) for x in os.listdir(dataset_path)])\n\nreal_images = [np.array(Image.open(path).convert(\"RGB\")) for path in image_paths]\n```", "```py\nfrom torchvision.transforms import functional as F\n\ndef preprocess_image(image):\n    image = torch.tensor(image).unsqueeze(0)\n    image = image.permute(0, 3, 1, 2) / 255.0\n    return F.center_crop(image, (256, 256))\n\nreal_images = torch.cat([preprocess_image(image) for image in real_images])\nprint(real_images.shape)\n# torch.Size([10, 3, 256, 256])\n```", "```py\nfrom diffusers import DiTPipeline, DPMSolverMultistepScheduler\n\ndit_pipeline = DiTPipeline.from_pretrained(\"facebook/DiT-XL-2-256\", torch_dtype=torch.float16)\ndit_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(dit_pipeline.scheduler.config)\ndit_pipeline = dit_pipeline.to(\"cuda\")\n\nwords = [\n    \"cassette player\",\n    \"chainsaw\",\n    \"chainsaw\",\n    \"church\",\n    \"gas pump\",\n    \"gas pump\",\n    \"gas pump\",\n    \"parachute\",\n    \"parachute\",\n    \"tench\",\n]\n\nclass_ids = dit_pipeline.get_label_ids(words)\noutput = dit_pipeline(class_labels=class_ids, generator=generator, output_type=\"np\")\n\nfake_images = output.images\nfake_images = torch.tensor(fake_images)\nfake_images = fake_images.permute(0, 3, 1, 2)\nprint(fake_images.shape)\n# torch.Size([10, 3, 256, 256])\n```", "```py\nfrom torchmetrics.image.fid import FrechetInceptionDistance\n\nfid = FrechetInceptionDistance(normalize=True)\nfid.update(real_images, real=True)\nfid.update(fake_images, real=False)\n\nprint(f\"FID: {float(fid.compute())}\")\n# FID: 177.7147216796875\n```"]