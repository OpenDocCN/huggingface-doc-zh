["```py\ncurl localhost:3000/v1/chat/completions \\\n    -X POST \\\n    -d '{\n  \"model\": \"tgi\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful assistant.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What is deep learning?\"\n    }\n  ],\n  \"stream\": true,\n  \"max_tokens\": 20\n}' \\\n    -H 'Content-Type: application/json'\n```", "```py\nfrom openai import OpenAI\n\n# init the client but point it to TGI\nclient = OpenAI(\n    base_url=\"http://localhost:3000/v1\",\n    api_key=\"-\"\n)\n\nchat_completion = client.chat.completions.create(\n    model=\"tgi\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n        {\"role\": \"user\", \"content\": \"What is deep learning?\"}\n    ],\n    stream=True\n)\n\n# iterate and print stream\nfor message in chat_completion:\n    print(message)\n```", "```py\nfrom openai import OpenAI\n\n# init the client but point it to TGI\nclient = OpenAI(\n    base_url=\"http://localhost:3000/v1\",\n    api_key=\"-\"\n)\n\nchat_completion = client.chat.completions.create(\n    model=\"tgi\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n        {\"role\": \"user\", \"content\": \"What is deep learning?\"}\n    ],\n    stream=False\n)\n\nprint(chat_completion)\n```", "```py\nfrom openai import OpenAI\n\n# init the client but point it to TGI\nclient = OpenAI(\n    # replace with your endpoint url, make sure to include \"v1/\" at the end\n    base_url=\"https://vlzz10eq3fol3429.us-east-1.aws.endpoints.huggingface.cloud/v1/\",\n    # replace with your API key\n    api_key=\"hf_XXX\"\n)\n\nchat_completion = client.chat.completions.create(\n    model=\"tgi\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n        {\"role\": \"user\", \"content\": \"What is deep learning?\"}\n    ],\n    stream=True\n)\n\n# iterate and print stream\nfor message in chat_completion:\n    print(message.choices[0].delta.content, end=\"\")\n```", "```py\nimport json\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n\ntry:\n role = sagemaker.get_execution_role()\nexcept ValueError:\n iam = boto3.client('iam')\n role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n# Hub Model configuration. https://huggingface.co/models\nhub = {\n 'HF_MODEL_ID':'HuggingFaceH4/zephyr-7b-beta',\n 'SM_NUM_GPUS': json.dumps(1),\n 'MESSAGES_API_ENABLED': True\n}\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n image_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.4.0\"),\n env=hub,\n role=role,\n)\n\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n initial_instance_count=1,\n instance_type=\"ml.g5.2xlarge\",\n container_startup_health_check_timeout=300,\n  )\n\n# send request\npredictor.predict({\n\"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n        {\"role\": \"user\", \"content\": \"What is deep learning?\"}\n    ]\n})\n```"]