- en: ImageGPT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/imagegpt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/imagegpt)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ImageGPT model was proposed in [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt)
    by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya
    Sutskever. ImageGPT (iGPT) is a GPT-2-like model trained to predict the next pixel
    value, allowing for both unconditional and conditional image generation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '*Inspired by progress in unsupervised representation learning for natural language,
    we examine whether similar models can learn useful representations for images.
    We train a sequence Transformer to auto-regressively predict pixels, without incorporating
    knowledge of the 2D input structure. Despite training on low-resolution ImageNet
    without labels, we find that a GPT-2 scale model learns strong image representations
    as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10,
    we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide
    ResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised
    pre-trained models. We are also competitive with self-supervised benchmarks on
    ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0% top-1
    accuracy on a linear probe of our features.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![drawing](../Images/7dfe07e48e8dd3472559e9232735486e.png) Summary of the approach.
    Taken from the [original paper](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr), based
    on [this issue](https://github.com/openai/image-gpt/issues/7). The original code
    can be found [here](https://github.com/openai/image-gpt).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ImageGPT is almost exactly the same as [GPT-2](gpt2), with the exception that
    a different activation function is used (namely “quick gelu”), and the layer normalization
    layers don’t mean center the inputs. ImageGPT also doesn’t have tied input- and
    output embeddings.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the time- and memory requirements of the attention mechanism of Transformers
    scales quadratically in the sequence length, the authors pre-trained ImageGPT
    on smaller input resolutions, such as 32x32 and 64x64\. However, feeding a sequence
    of 32x32x3=3072 tokens from 0..255 into a Transformer is still prohibitively large.
    Therefore, the authors applied k-means clustering to the (R,G,B) pixel values
    with k=512\. This way, we only have a 32*32 = 1024-long sequence, but now of integers
    in the range 0..511\. So we are shrinking the sequence length at the cost of a
    bigger embedding matrix. In other words, the vocabulary size of ImageGPT is 512,
    + 1 for a special “start of sentence” (SOS) token, used at the beginning of every
    sequence. One can use [ImageGPTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTImageProcessor)
    to prepare images for the model.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite being pre-trained entirely unsupervised (i.e. without the use of any
    labels), ImageGPT produces fairly performant image features useful for downstream
    tasks, such as image classification. The authors showed that the features in the
    middle of the network are the most performant, and can be used as-is to train
    a linear model (such as a sklearn logistic regression model for example). This
    is also referred to as “linear probing”. Features can be easily obtained by first
    forwarding the image through the model, then specifying `output_hidden_states=True`,
    and then average-pool the hidden states at whatever layer you like.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, one can further fine-tune the entire model on a downstream dataset,
    similar to BERT. For this, you can use [ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ImageGPT comes in different sizes: there’s ImageGPT-small, ImageGPT-medium
    and ImageGPT-large. The authors did also train an XL variant, which they didn’t
    release. The differences in size are summarized in the following table:'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ImageGPT有不同的大小：有ImageGPT-small、ImageGPT-medium和ImageGPT-large。作者还训练了一个XL变体，但他们没有发布。大小上的差异总结在以下表中：
- en: '| **Model variant** | **Depths** | **Hidden sizes** | **Decoder hidden size**
    | **Params (M)** | **ImageNet-1k Top 1** |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| **模型变体** | **深度** | **隐藏大小** | **解码器隐藏大小** | **参数（M）** | **ImageNet-1k Top
    1** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| MiT-b0 | [2, 2, 2, 2] | [32, 64, 160, 256] | 256 | 3.7 | 70.5 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| MiT-b0 | [2, 2, 2, 2] | [32, 64, 160, 256] | 256 | 3.7 | 70.5 |'
- en: '| MiT-b1 | [2, 2, 2, 2] | [64, 128, 320, 512] | 256 | 14.0 | 78.7 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| MiT-b1 | [2, 2, 2, 2] | [64, 128, 320, 512] | 256 | 14.0 | 78.7 |'
- en: '| MiT-b2 | [3, 4, 6, 3] | [64, 128, 320, 512] | 768 | 25.4 | 81.6 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| MiT-b2 | [3, 4, 6, 3] | [64, 128, 320, 512] | 768 | 25.4 | 81.6 |'
- en: '| MiT-b3 | [3, 4, 18, 3] | [64, 128, 320, 512] | 768 | 45.2 | 83.1 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| MiT-b3 | [3, 4, 18, 3] | [64, 128, 320, 512] | 768 | 45.2 | 83.1 |'
- en: '| MiT-b4 | [3, 8, 27, 3] | [64, 128, 320, 512] | 768 | 62.6 | 83.6 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| MiT-b4 | [3, 8, 27, 3] | [64, 128, 320, 512] | 768 | 62.6 | 83.6 |'
- en: '| MiT-b5 | [3, 6, 40, 3] | [64, 128, 320, 512] | 768 | 82.0 | 83.8 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| MiT-b5 | [3, 6, 40, 3] | [64, 128, 320, 512] | 768 | 82.0 | 83.8 |'
- en: Resources
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with ImageGPT.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一个官方的Hugging Face和社区（由🌎表示）资源列表，帮助您开始使用ImageGPT。
- en: Image Classification
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类
- en: Demo notebooks for ImageGPT can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ImageGPT).
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ImageGPT的演示笔记本可以在[这里](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ImageGPT)找到。
- en: '[ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)由这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)支持。'
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另请参阅：[图像分类任务指南](../tasks/image_classification)
- en: If you’re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we’ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣提交资源以包含在这里，请随时打开一个Pull Request，我们将进行审核！资源应该展示一些新东西，而不是重复现有资源。
- en: ImageGPTConfig
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageGPTConfig
- en: '### `class transformers.ImageGPTConfig`'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ImageGPTConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/configuration_imagegpt.py#L37)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/configuration_imagegpt.py#L37)'
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 512) — Vocabulary size of the
    GPT-2 model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)
    or `TFImageGPTModel`.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`，*可选*，默认为512) — GPT-2模型的词汇大小。定义了在调用[ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)或`TFImageGPTModel`时可以表示的不同令牌数量。 '
- en: '`n_positions` (`int`, *optional*, defaults to 32*32) — The maximum sequence
    length that this model might ever be used with. Typically set this to something
    large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_positions` (`int`，*可选*，默认为32*32) — 此模型可能会使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512或1024或2048）。'
- en: '`n_embd` (`int`, *optional*, defaults to 512) — Dimensionality of the embeddings
    and hidden states.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_embd` (`int`，*可选*，默认为512) — 嵌入和隐藏状态的维度。'
- en: '`n_layer` (`int`, *optional*, defaults to 24) — Number of hidden layers in
    the Transformer encoder.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_layer` (`int`，*可选*，默认为24) — Transformer编码器中的隐藏层数。'
- en: '`n_head` (`int`, *optional*, defaults to 8) — Number of attention heads for
    each attention layer in the Transformer encoder.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_head` (`int`，*可选*，默认为8) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`n_inner` (`int`, *optional*, defaults to None) — Dimensionality of the inner
    feed-forward layers. `None` will set it to 4 times n_embd'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_inner` (`int`，*可选*，默认为None) — 内部前馈层的维度。`None`将将其设置为n_embd的4倍。'
- en: '`activation_function` (`str`, *optional*, defaults to `"quick_gelu"`) — Activation
    function (can be one of the activation functions defined in src/transformers/activations.py).
    Defaults to “quick_gelu”.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_function` (`str`，*可选*，默认为`"quick_gelu"`) — 激活函数（可以是src/transformers/activations.py中定义的激活函数之一）。默认为“quick_gelu”。'
- en: '`resid_pdrop` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resid_pdrop` (`float`，*可选*，默认为0.1) — 嵌入、编码器和池化器中所有全连接层的丢失概率。'
- en: '`embd_pdrop` (`int`, *optional*, defaults to 0.1) — The dropout ratio for the
    embeddings.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embd_pdrop` (`int`，*可选*，默认为0.1) — 嵌入的丢失比率。'
- en: '`attn_pdrop` (`float`, *optional*, defaults to 0.1) — The dropout ratio for
    the attention.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attn_pdrop` (`float`，*可选*，默认为0.1) — 注意力的丢失比率。'
- en: '`layer_norm_epsilon` (`float`, *optional*, defaults to 1e-5) — The epsilon
    to use in the layer normalization layers.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_epsilon` (`float`，*可选*，默认为1e-5) — 在层归一化层中使用的epsilon。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`，*可选*，默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`scale_attn_weights` (`bool`, *optional*, defaults to `True`) — Scale attention
    weights by dividing by sqrt(hidden_size)..'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale_attn_weights` (`bool`，*可选*，默认为`True`) — 通过除以sqrt(hidden_size)来缩放注意力权重。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *可选*, 默认为`True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。'
- en: '`scale_attn_by_inverse_layer_idx` (`bool`, *optional*, defaults to `False`)
    — Whether to additionally scale attention weights by `1 / layer_idx + 1`.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale_attn_by_inverse_layer_idx` (`bool`, *可选*, 默认为`False`) — 是否额外按`1 / layer_idx
    + 1`缩放注意力权重。'
- en: '`reorder_and_upcast_attn` (`bool`, *optional*, defaults to `False`) — Whether
    to scale keys (K) prior to computing attention (dot-product) and upcast attention
    dot-product/softmax to float() when training with mixed precision.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reorder_and_upcast_attn` (`bool`, *可选*, 默认为`False`) — 是否在计算注意力（点积）之前缩放键（K），并在使用混合精度训练时将注意力点积/softmax向上转换为float()。'
- en: This is the configuration class to store the configuration of a [ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)
    or a `TFImageGPTModel`. It is used to instantiate a GPT-2 model according to the
    specified arguments, defining the model architecture. Instantiating a configuration
    with the defaults will yield a similar configuration to that of the ImageGPT [openai/imagegpt-small](https://huggingface.co/openai/imagegpt-small)
    architecture.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是配置类，用于存储[ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)或`TFImageGPTModel`的配置。根据指定的参数实例化一个GPT-2模型，定义模型架构。使用默认值实例化配置将产生类似于ImageGPT
    [openai/imagegpt-small](https://huggingface.co/openai/imagegpt-small)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ImageGPTFeatureExtractor
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageGPTFeatureExtractor
- en: '### `class transformers.ImageGPTFeatureExtractor`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ImageGPTFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/feature_extraction_imagegpt.py#L26)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/feature_extraction_imagegpt.py#L26)'
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `__call__`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Preprocess an image or a batch of images.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理一张图像或一批图像。
- en: ImageGPTImageProcessor
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageGPTImageProcessor
- en: '### `class transformers.ImageGPTImageProcessor`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ImageGPTImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/image_processing_imagegpt.py#L58)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/image_processing_imagegpt.py#L58)'
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`clusters` (`np.ndarray` or `List[List[int]]`, *optional*) — The color clusters
    to use, of shape `(n_clusters, 3)` when color quantizing. Can be overriden by
    `clusters` in `preprocess`.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clusters` (`np.ndarray`或`List[List[int]]`, *可选*) — 在颜色量化时要使用的颜色簇，形状为`(n_clusters,
    3)`。可以被`preprocess`中的`clusters`覆盖。'
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Whether to resize the
    image’s dimensions to `(size["height"], size["width"])`. Can be overridden by
    `do_resize` in `preprocess`.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *可选*, 默认为`True`) — 是否将图像的尺寸调整为`(size["height"], size["width"])`。可以被`preprocess`中的`do_resize`覆盖。'
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"height" -- 256, "width":
    256}`): Size of the image after resizing. Can be overridden by `size` in `preprocess`.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]` *可选*, 默认为`{"height" -- 256, "width": 256}`): 调整大小后的图像尺寸。可以被`preprocess`中的`size`覆盖。'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`)
    — Resampling filter to use if resizing the image. Can be overridden by `resample`
    in `preprocess`.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *可选*, 默认为`Resampling.BILINEAR`) — 如果调整图像大小，要使用的重采样滤波器。可以被`preprocess`中的`resample`覆盖。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the image pixel value to between [-1, 1]. Can be overridden by `do_normalize`
    in `preprocess`.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *可选*, 默认为`True`) — 是否将图像像素值归一化为[-1, 1]之间。可以被`preprocess`中的`do_normalize`覆盖。'
- en: '`do_color_quantize` (`bool`, *optional*, defaults to `True`) — Whether to color
    quantize the image. Can be overridden by `do_color_quantize` in `preprocess`.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_color_quantize` (`bool`, *可选*, 默认为`True`) — 是否对图像进行颜色量化。可以被`preprocess`中的`do_color_quantize`覆盖。'
- en: Constructs a ImageGPT image processor. This image processor can be used to resize
    images to a smaller resolution (such as 32x32 or 64x64), normalize them and finally
    color quantize them to obtain sequences of “pixel values” (color clusters).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个ImageGPT图像处理器。此图像处理器可用于将图像调整为较小分辨率（如32x32或64x64），对其进行归一化，最后进行颜色量化，以获得“像素值”（颜色簇）序列。
- en: '#### `preprocess`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/image_processing_imagegpt.py#L175)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/image_processing_imagegpt.py#L175)'
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`ImageInput`) — Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_normalize=False`.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) — 要预处理的图像。期望像素值从0到255的单个图像或图像批次。如果传入像素值在0到1之间的图像，请设置`do_normalize=False`。'
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) — Whether to
    resize the image.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *可选*, 默认为`self.do_resize`) — 是否调整图像大小。'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — Size of the
    image after resizing.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`, *可选*, 默认为`self.size`) — 调整大小后的图像尺寸。'
- en: '`resample` (`int`, *optional*, defaults to `self.resample`) — Resampling filter
    to use if resizing the image. This can be one of the enum `PILImageResampling`,
    Only has an effect if `do_resize` is set to `True`.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`int`，*可选*，默认为 `self.resample`) — 如果调整图像大小，则要使用的重采样滤波器。可以是枚举 `PILImageResampling`
    中的一个，仅在 `do_resize` 设置为 `True` 时有效。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — Whether
    to normalize the image'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`，*可选*，默认为 `self.do_normalize`) — 是否对图像进行归一化'
- en: '`do_color_quantize` (`bool`, *optional*, defaults to `self.do_color_quantize`)
    — Whether to color quantize the image.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_color_quantize` (`bool`，*可选*，默认为 `self.do_color_quantize`) — 是否对图像进行颜色量化。'
- en: '`clusters` (`np.ndarray` or `List[List[int]]`, *optional*, defaults to `self.clusters`)
    — Clusters used to quantize the image of shape `(n_clusters, 3)`. Only has an
    effect if `do_color_quantize` is set to `True`.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clusters` (`np.ndarray` 或 `List[List[int]]`，*可选*，默认为 `self.clusters`) — 用于对图像进行量化的聚类，形状为
    `(n_clusters, 3)`。仅在 `do_color_quantize` 设置为 `True` 时有效。'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 `TensorType`，*可选*) — 返回的张量类型。可以是以下之一：'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '未设置: 返回一个 `np.ndarray` 列表。'
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW` 或 `''tf''`: 返回类型为 `tf.Tensor` 的批次。'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH` 或 `''pt''`: 返回类型为 `torch.Tensor` 的批次。'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY` 或 `''np''`: 返回类型为 `np.ndarray` 的批次。'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX` 或 `''jax''`: 返回类型为 `jax.numpy.ndarray` 的批次。'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`ChannelDimension` 或 `str`，*可选*，默认为 `ChannelDimension.FIRST`)
    — 输出图像的通道维度格式。可以是以下之一：'
- en: '`ChannelDimension.FIRST`: image in (num_channels, height, width) format.'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.FIRST`: 图像格式为 (通道数, 高度, 宽度)。'
- en: '`ChannelDimension.LAST`: image in (height, width, num_channels) format. Only
    has an effect if `do_color_quantize` is set to `False`.'
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.LAST`: 图像格式为 (高度, 宽度, 通道数)。仅在 `do_color_quantize` 设置为 `False`
    时有效。'
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format` (`ChannelDimension` 或 `str`，*可选*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` 或 `ChannelDimension.FIRST`: 图像格式为 (通道数, 高度, 宽度)。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` 或 `ChannelDimension.LAST`: 图像格式为 (高度, 宽度, 通道数)。'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"` 或 `ChannelDimension.NONE`: 图像格式为 (高度, 宽度)。'
- en: Preprocess an image or batch of images.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理图像或图像批次。
- en: ImageGPTModel
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageGPTModel
- en: '### `class transformers.ImageGPTModel`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ImageGPTModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L608)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L608)'
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The bare ImageGPT Model transformer outputting raw hidden-states without any
    specific head on top.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的 ImageGPT 模型变压器，输出没有特定头部的原始隐藏状态。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有信息。
- en: '#### `forward`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L645)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L645)'
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[-2]`
    (`sequence_length` of input past key value states). Indices of input sequence
    tokens in the vocabulary.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`） — 如果
    `past_key_values` 为 `None`，则 `input_ids_length` = `sequence_length`，否则为 `past_key_values[0][0].shape[-2]`（输入过去关键值状态的序列长度）。词汇表中输入序列标记的索引。'
- en: If `past_key_values` is used, only `input_ids` that do not have their past calculated
    should be passed as `input_ids`.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了 `past_key_values`，则只应将未计算其过去的 `input_ids` 作为 `input_ids` 传递。
- en: Indices can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取索引。有关详细信息，请参见[ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`)
    — Contains precomputed hidden-states (key and values in the attention blocks)
    as computed by the model (see `past_key_values` output below). Can be used to
    speed up sequential decoding. The `input_ids` which have their past given to this
    model should not be passed as `input_ids` as they have already been computed.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（长度为`config.n_layers`的`Tuple[Tuple[torch.Tensor]]`）- 包含由模型计算的预先计算的隐藏状态（注意力块中的键和值），可以用于加速顺序解码。将过去给定给该模型的`input_ids`不应作为`input_ids`传递，因为它们已经被计算过。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被`masked`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被`masked`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是attention masks？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    段标记索引，用于指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是token type IDs？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是position IDs？](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）-
    用于使自注意力模块的选定头部失效的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部是`masked`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: If `past_key_values` is used, optionally only the last `inputs_embeds` have
    to be input (see `past_key_values`).
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，则可以选择仅输入最后的`inputs_embeds`（请参见`past_key_values`）。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*）- 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（请参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for language modeling. Note that the labels **are shifted** inside the
    model, i.e. you can set `labels = input_ids` Indices are selected in `[-100, 0,
    ..., config.vocab_size]` All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 用于语言建模的标签。请注意，模型内部的标签**已经被移位**，即您可以设置`labels
    = input_ids`。索引在`[-100, 0, ..., config.vocab_size]`中选择。所有设置为`-100`的标签都被忽略（被`masked`），损失仅计算标签在`[0,
    ..., config.vocab_size]`中的情况。'
- en: Returns
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)
    or `tuple(torch.FloatTensor)`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    and inputs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)）和输入的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的输出的隐藏状态序列。'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，则只输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and optionally if `config.is_encoder_decoder=True`
    2 additional tensors of shape `(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`还有2个额外的形状为`(batch_size,
    num_heads, encoder_sequence_length, embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and optionally if `config.is_encoder_decoder=True` in the cross-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块中的键和值，以及在交叉注意力块中可选地使用`config.is_encoder_decoder=True`）可用于加速顺序解码（查看`past_key_values`输入）。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有嵌入层，+
    一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`和`config.add_cross_attention=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: The [ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)
    forward method, overrides the `__call__` special method.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ImageGPTForCausalImageModeling
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageGPTForCausalImageModeling
- en: '### `class transformers.ImageGPTForCausalImageModeling`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ImageGPTForCausalImageModeling`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L876)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L876)'
- en: '[PRE9]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)）
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The ImageGPT Model transformer with a language modeling head on top (linear
    layer with weights tied to the input embeddings).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ImageGPT模型变压器，顶部带有语言建模头（线性层，其权重与输入嵌入层绑定）。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L940)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[-2]`
    (`sequence_length` of input past key value states). Indices of input sequence
    tokens in the vocabulary.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, only `input_ids` that do not have their past calculated
    should be passed as `input_ids`.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`)
    — Contains precomputed hidden-states (key and values in the attention blocks)
    as computed by the model (see `past_key_values` output below). Can be used to
    speed up sequential decoding. The `input_ids` which have their past given to this
    model should not be passed as `input_ids` as they have already been computed.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `inputs_embeds` have
    to be input (see `past_key_values`).
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for language modeling. Note that the labels **are shifted** inside the
    model, i.e. you can set `labels = input_ids` Indices are selected in `[-100, 0,
    ..., config.vocab_size]` All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`的形状为`(batch_size, sequence_length)`，*optional*)
    — 语言建模的标签。请注意，标签**在模型内部被移位**，即您可以设置`labels = input_ids`，索引在`[-100, 0, ..., config.vocab_size]`中选择。所有设置为`-100`的标签都被忽略（掩码），损失仅计算在`[0,
    ..., config.vocab_size]`中的标签。'
- en: Returns
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)
    or `tuple(torch.FloatTensor)`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)或`torch.FloatTensor`元组'
- en: A [transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    and inputs.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`，则根据配置（[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)）和输入包含各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) —
    语言建模损失（用于下一个标记预测）。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`的形状为`(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — `torch.FloatTensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+ 一个用于每个层的输出）的形状为`(batch_size, sequence_length,
    hidden_size)`。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — `torch.FloatTensor`元组（每层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — `torch.FloatTensor`元组（每层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Cross attentions weights after the attention softmax, used to compute the weighted
    average in the cross-attention heads.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的交叉注意力权重，用于计算交叉注意力头中的加权平均值。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `torch.FloatTensor`
    tuples of length `config.n_layers`, with each tuple containing the cached key,
    value states of the self-attention and the cross-attention layers if model is
    used in encoder-decoder setting. Only relevant if `config.is_decoder = True`.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的`torch.FloatTensor`元组，每个元组包含自注意力和交叉注意力层的缓存键、值状态，如果模型用于编码器-解码器设置，则相关。仅在`config.is_decoder
    = True`时相关。'
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: The [ImageGPTForCausalImageModeling](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForCausalImageModeling)
    forward method, overrides the `__call__` special method.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImageGPTForCausalImageModeling](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForCausalImageModeling)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ImageGPTForImageClassification
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageGPTForImageClassification
- en: '### `class transformers.ImageGPTForImageClassification`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ImageGPTForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L1076)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ImageGPT Model transformer with an image classification head on top (linear
    layer). [ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)
    average-pools the hidden states in order to do the classification.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L1093)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[-2]`
    (`sequence_length` of input past key value states). Indices of input sequence
    tokens in the vocabulary.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, only `input_ids` that do not have their past calculated
    should be passed as `input_ids`.
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`)
    — Contains precomputed hidden-states (key and values in the attention blocks)
    as computed by the model (see `past_key_values` output below). Can be used to
    speed up sequential decoding. The `input_ids` which have their past given to this
    model should not be passed as `input_ids` as they have already been computed.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*）—
    可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: If `past_key_values` is used, optionally only the last `inputs_embeds` have
    to be input (see `past_key_values`).
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，则可能只需输入最后的`inputs_embeds`（参见`past_key_values`）。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*）— 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（`torch.LongTensor`，形状为`(batch_size,)`，*可选*）— 用于计算序列分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.modeling_outputs.SequenceClassifierOutputWithPast` or `tuple(torch.FloatTensor)`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_outputs.SequenceClassifierOutputWithPast`或`tuple(torch.FloatTensor)`'
- en: A `transformers.modeling_outputs.SequenceClassifierOutputWithPast` or a tuple
    of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    and inputs.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.modeling_outputs.SequenceClassifierOutputWithPast`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)）和输入的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回）— 分类（如果config.num_labels==1则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`）— 分类（如果config.num_labels==1则为回归）得分（SoftMax之前）。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）—
    长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块中的键和值），可用于加速顺序解码。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出加上每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行前处理和后处理步骤，而后者则会默默地忽略它们。
- en: 'Examples:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
