- en: ImageGPT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/imagegpt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/imagegpt)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ImageGPT model was proposed in [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt)
    by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya
    Sutskever. ImageGPT (iGPT) is a GPT-2-like model trained to predict the next pixel
    value, allowing for both unconditional and conditional image generation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '*Inspired by progress in unsupervised representation learning for natural language,
    we examine whether similar models can learn useful representations for images.
    We train a sequence Transformer to auto-regressively predict pixels, without incorporating
    knowledge of the 2D input structure. Despite training on low-resolution ImageNet
    without labels, we find that a GPT-2 scale model learns strong image representations
    as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10,
    we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide
    ResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised
    pre-trained models. We are also competitive with self-supervised benchmarks on
    ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0% top-1
    accuracy on a linear probe of our features.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![drawing](../Images/7dfe07e48e8dd3472559e9232735486e.png) Summary of the approach.
    Taken from the [original paper](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr), based
    on [this issue](https://github.com/openai/image-gpt/issues/7). The original code
    can be found [here](https://github.com/openai/image-gpt).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ImageGPT is almost exactly the same as [GPT-2](gpt2), with the exception that
    a different activation function is used (namely â€œquick geluâ€), and the layer normalization
    layers donâ€™t mean center the inputs. ImageGPT also doesnâ€™t have tied input- and
    output embeddings.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the time- and memory requirements of the attention mechanism of Transformers
    scales quadratically in the sequence length, the authors pre-trained ImageGPT
    on smaller input resolutions, such as 32x32 and 64x64\. However, feeding a sequence
    of 32x32x3=3072 tokens from 0..255 into a Transformer is still prohibitively large.
    Therefore, the authors applied k-means clustering to the (R,G,B) pixel values
    with k=512\. This way, we only have a 32*32 = 1024-long sequence, but now of integers
    in the range 0..511\. So we are shrinking the sequence length at the cost of a
    bigger embedding matrix. In other words, the vocabulary size of ImageGPT is 512,
    + 1 for a special â€œstart of sentenceâ€ (SOS) token, used at the beginning of every
    sequence. One can use [ImageGPTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTImageProcessor)
    to prepare images for the model.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite being pre-trained entirely unsupervised (i.e. without the use of any
    labels), ImageGPT produces fairly performant image features useful for downstream
    tasks, such as image classification. The authors showed that the features in the
    middle of the network are the most performant, and can be used as-is to train
    a linear model (such as a sklearn logistic regression model for example). This
    is also referred to as â€œlinear probingâ€. Features can be easily obtained by first
    forwarding the image through the model, then specifying `output_hidden_states=True`,
    and then average-pool the hidden states at whatever layer you like.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, one can further fine-tune the entire model on a downstream dataset,
    similar to BERT. For this, you can use [ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ImageGPT comes in different sizes: thereâ€™s ImageGPT-small, ImageGPT-medium
    and ImageGPT-large. The authors did also train an XL variant, which they didnâ€™t
    release. The differences in size are summarized in the following table:'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ImageGPTæœ‰ä¸åŒçš„å¤§å°ï¼šæœ‰ImageGPT-smallã€ImageGPT-mediumå’ŒImageGPT-largeã€‚ä½œè€…è¿˜è®­ç»ƒäº†ä¸€ä¸ªXLå˜ä½“ï¼Œä½†ä»–ä»¬æ²¡æœ‰å‘å¸ƒã€‚å¤§å°ä¸Šçš„å·®å¼‚æ€»ç»“åœ¨ä»¥ä¸‹è¡¨ä¸­ï¼š
- en: '| **Model variant** | **Depths** | **Hidden sizes** | **Decoder hidden size**
    | **Params (M)** | **ImageNet-1k Top 1** |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| **æ¨¡å‹å˜ä½“** | **æ·±åº¦** | **éšè—å¤§å°** | **è§£ç å™¨éšè—å¤§å°** | **å‚æ•°ï¼ˆMï¼‰** | **ImageNet-1k Top
    1** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| MiT-b0 | [2, 2, 2, 2] | [32, 64, 160, 256] | 256 | 3.7 | 70.5 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| MiT-b0 | [2, 2, 2, 2] | [32, 64, 160, 256] | 256 | 3.7 | 70.5 |'
- en: '| MiT-b1 | [2, 2, 2, 2] | [64, 128, 320, 512] | 256 | 14.0 | 78.7 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| MiT-b1 | [2, 2, 2, 2] | [64, 128, 320, 512] | 256 | 14.0 | 78.7 |'
- en: '| MiT-b2 | [3, 4, 6, 3] | [64, 128, 320, 512] | 768 | 25.4 | 81.6 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| MiT-b2 | [3, 4, 6, 3] | [64, 128, 320, 512] | 768 | 25.4 | 81.6 |'
- en: '| MiT-b3 | [3, 4, 18, 3] | [64, 128, 320, 512] | 768 | 45.2 | 83.1 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| MiT-b3 | [3, 4, 18, 3] | [64, 128, 320, 512] | 768 | 45.2 | 83.1 |'
- en: '| MiT-b4 | [3, 8, 27, 3] | [64, 128, 320, 512] | 768 | 62.6 | 83.6 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| MiT-b4 | [3, 8, 27, 3] | [64, 128, 320, 512] | 768 | 62.6 | 83.6 |'
- en: '| MiT-b5 | [3, 6, 40, 3] | [64, 128, 320, 512] | 768 | 82.0 | 83.8 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| MiT-b5 | [3, 6, 40, 3] | [64, 128, 320, 512] | 768 | 82.0 | 83.8 |'
- en: Resources
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with ImageGPT.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå®˜æ–¹çš„Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ImageGPTã€‚
- en: Image Classification
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ†ç±»
- en: Demo notebooks for ImageGPT can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ImageGPT).
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ImageGPTçš„æ¼”ç¤ºç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ImageGPT)æ‰¾åˆ°ã€‚
- en: '[ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)æ”¯æŒã€‚'
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦è¯·å‚é˜…ï¼š[å›¾åƒåˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/image_classification)
- en: If youâ€™re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨è¿™é‡Œï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªPull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æ ¸ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: ImageGPTConfig
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageGPTConfig
- en: '### `class transformers.ImageGPTConfig`'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ImageGPTConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/configuration_imagegpt.py#L37)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/configuration_imagegpt.py#L37)'
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_size` (`int`, *optional*, defaults to 512) â€” Vocabulary size of the
    GPT-2 model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)
    or `TFImageGPTModel`.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º512) â€” GPT-2æ¨¡å‹çš„è¯æ±‡å¤§å°ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨[ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)æˆ–`TFImageGPTModel`æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒä»¤ç‰Œæ•°é‡ã€‚ '
- en: '`n_positions` (`int`, *optional*, defaults to 32*32) â€” The maximum sequence
    length that this model might ever be used with. Typically set this to something
    large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_positions` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º32*32) â€” æ­¤æ¨¡å‹å¯èƒ½ä¼šä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸å°†å…¶è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚ï¼Œ512æˆ–1024æˆ–2048ï¼‰ã€‚'
- en: '`n_embd` (`int`, *optional*, defaults to 512) â€” Dimensionality of the embeddings
    and hidden states.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_embd` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º512) â€” åµŒå…¥å’Œéšè—çŠ¶æ€çš„ç»´åº¦ã€‚'
- en: '`n_layer` (`int`, *optional*, defaults to 24) â€” Number of hidden layers in
    the Transformer encoder.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_layer` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º24) â€” Transformerç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°ã€‚'
- en: '`n_head` (`int`, *optional*, defaults to 8) â€” Number of attention heads for
    each attention layer in the Transformer encoder.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_head` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º8) â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`n_inner` (`int`, *optional*, defaults to None) â€” Dimensionality of the inner
    feed-forward layers. `None` will set it to 4 times n_embd'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_inner` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸ºNone) â€” å†…éƒ¨å‰é¦ˆå±‚çš„ç»´åº¦ã€‚`None`å°†å°†å…¶è®¾ç½®ä¸ºn_embdçš„4å€ã€‚'
- en: '`activation_function` (`str`, *optional*, defaults to `"quick_gelu"`) â€” Activation
    function (can be one of the activation functions defined in src/transformers/activations.py).
    Defaults to â€œquick_geluâ€.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_function` (`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"quick_gelu"`) â€” æ¿€æ´»å‡½æ•°ï¼ˆå¯ä»¥æ˜¯src/transformers/activations.pyä¸­å®šä¹‰çš„æ¿€æ´»å‡½æ•°ä¹‹ä¸€ï¼‰ã€‚é»˜è®¤ä¸ºâ€œquick_geluâ€ã€‚'
- en: '`resid_pdrop` (`float`, *optional*, defaults to 0.1) â€” The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resid_pdrop` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.1) â€” åµŒå…¥ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¤±æ¦‚ç‡ã€‚'
- en: '`embd_pdrop` (`int`, *optional*, defaults to 0.1) â€” The dropout ratio for the
    embeddings.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embd_pdrop` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.1) â€” åµŒå…¥çš„ä¸¢å¤±æ¯”ç‡ã€‚'
- en: '`attn_pdrop` (`float`, *optional*, defaults to 0.1) â€” The dropout ratio for
    the attention.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attn_pdrop` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.1) â€” æ³¨æ„åŠ›çš„ä¸¢å¤±æ¯”ç‡ã€‚'
- en: '`layer_norm_epsilon` (`float`, *optional*, defaults to 1e-5) â€” The epsilon
    to use in the layer normalization layers.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_epsilon` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1e-5) â€” åœ¨å±‚å½’ä¸€åŒ–å±‚ä¸­ä½¿ç”¨çš„epsilonã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`scale_attn_weights` (`bool`, *optional*, defaults to `True`) â€” Scale attention
    weights by dividing by sqrt(hidden_size)..'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale_attn_weights` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`) â€” é€šè¿‡é™¤ä»¥sqrt(hidden_size)æ¥ç¼©æ”¾æ³¨æ„åŠ›æƒé‡ã€‚'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) â€” Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚'
- en: '`scale_attn_by_inverse_layer_idx` (`bool`, *optional*, defaults to `False`)
    â€” Whether to additionally scale attention weights by `1 / layer_idx + 1`.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale_attn_by_inverse_layer_idx` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦é¢å¤–æŒ‰`1 / layer_idx
    + 1`ç¼©æ”¾æ³¨æ„åŠ›æƒé‡ã€‚'
- en: '`reorder_and_upcast_attn` (`bool`, *optional*, defaults to `False`) â€” Whether
    to scale keys (K) prior to computing attention (dot-product) and upcast attention
    dot-product/softmax to float() when training with mixed precision.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reorder_and_upcast_attn` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦åœ¨è®¡ç®—æ³¨æ„åŠ›ï¼ˆç‚¹ç§¯ï¼‰ä¹‹å‰ç¼©æ”¾é”®ï¼ˆKï¼‰ï¼Œå¹¶åœ¨ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒæ—¶å°†æ³¨æ„åŠ›ç‚¹ç§¯/softmaxå‘ä¸Šè½¬æ¢ä¸ºfloat()ã€‚'
- en: This is the configuration class to store the configuration of a [ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)
    or a `TFImageGPTModel`. It is used to instantiate a GPT-2 model according to the
    specified arguments, defining the model architecture. Instantiating a configuration
    with the defaults will yield a similar configuration to that of the ImageGPT [openai/imagegpt-small](https://huggingface.co/openai/imagegpt-small)
    architecture.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯é…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨[ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)æˆ–`TFImageGPTModel`çš„é…ç½®ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªGPT-2æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºImageGPT
    [openai/imagegpt-small](https://huggingface.co/openai/imagegpt-small)æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ImageGPTFeatureExtractor
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageGPTFeatureExtractor
- en: '### `class transformers.ImageGPTFeatureExtractor`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ImageGPTFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/feature_extraction_imagegpt.py#L26)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/feature_extraction_imagegpt.py#L26)'
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `__call__`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Preprocess an image or a batch of images.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†ä¸€å¼ å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚
- en: ImageGPTImageProcessor
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageGPTImageProcessor
- en: '### `class transformers.ImageGPTImageProcessor`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ImageGPTImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/image_processing_imagegpt.py#L58)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/image_processing_imagegpt.py#L58)'
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`clusters` (`np.ndarray` or `List[List[int]]`, *optional*) â€” The color clusters
    to use, of shape `(n_clusters, 3)` when color quantizing. Can be overriden by
    `clusters` in `preprocess`.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clusters` (`np.ndarray`æˆ–`List[List[int]]`, *å¯é€‰*) â€” åœ¨é¢œè‰²é‡åŒ–æ—¶è¦ä½¿ç”¨çš„é¢œè‰²ç°‡ï¼Œå½¢çŠ¶ä¸º`(n_clusters,
    3)`ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`clusters`è¦†ç›–ã€‚'
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) â€” Whether to resize the
    imageâ€™s dimensions to `(size["height"], size["width"])`. Can be overridden by
    `do_resize` in `preprocess`.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦å°†å›¾åƒçš„å°ºå¯¸è°ƒæ•´ä¸º`(size["height"], size["width"])`ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`do_resize`è¦†ç›–ã€‚'
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"height" -- 256, "width":
    256}`): Size of the image after resizing. Can be overridden by `size` in `preprocess`.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]` *å¯é€‰*, é»˜è®¤ä¸º`{"height" -- 256, "width": 256}`): è°ƒæ•´å¤§å°åçš„å›¾åƒå°ºå¯¸ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`size`è¦†ç›–ã€‚'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`)
    â€” Resampling filter to use if resizing the image. Can be overridden by `resample`
    in `preprocess`.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *å¯é€‰*, é»˜è®¤ä¸º`Resampling.BILINEAR`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œè¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`resample`è¦†ç›–ã€‚'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) â€” Whether to normalize
    the image pixel value to between [-1, 1]. Can be overridden by `do_normalize`
    in `preprocess`.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦å°†å›¾åƒåƒç´ å€¼å½’ä¸€åŒ–ä¸º[-1, 1]ä¹‹é—´ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`do_normalize`è¦†ç›–ã€‚'
- en: '`do_color_quantize` (`bool`, *optional*, defaults to `True`) â€” Whether to color
    quantize the image. Can be overridden by `do_color_quantize` in `preprocess`.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_color_quantize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œé¢œè‰²é‡åŒ–ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`do_color_quantize`è¦†ç›–ã€‚'
- en: Constructs a ImageGPT image processor. This image processor can be used to resize
    images to a smaller resolution (such as 32x32 or 64x64), normalize them and finally
    color quantize them to obtain sequences of â€œpixel valuesâ€ (color clusters).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªImageGPTå›¾åƒå¤„ç†å™¨ã€‚æ­¤å›¾åƒå¤„ç†å™¨å¯ç”¨äºå°†å›¾åƒè°ƒæ•´ä¸ºè¾ƒå°åˆ†è¾¨ç‡ï¼ˆå¦‚32x32æˆ–64x64ï¼‰ï¼Œå¯¹å…¶è¿›è¡Œå½’ä¸€åŒ–ï¼Œæœ€åè¿›è¡Œé¢œè‰²é‡åŒ–ï¼Œä»¥è·å¾—â€œåƒç´ å€¼â€ï¼ˆé¢œè‰²ç°‡ï¼‰åºåˆ—ã€‚
- en: '#### `preprocess`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/image_processing_imagegpt.py#L175)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/image_processing_imagegpt.py#L175)'
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`images` (`ImageInput`) â€” Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_normalize=False`.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) â€” è¦é¢„å¤„ç†çš„å›¾åƒã€‚æœŸæœ›åƒç´ å€¼ä»0åˆ°255çš„å•ä¸ªå›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡ã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨0åˆ°1ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½®`do_normalize=False`ã€‚'
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) â€” Whether to
    resize the image.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`self.do_resize`) â€” æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) â€” Size of the
    image after resizing.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`, *å¯é€‰*, é»˜è®¤ä¸º`self.size`) â€” è°ƒæ•´å¤§å°åçš„å›¾åƒå°ºå¯¸ã€‚'
- en: '`resample` (`int`, *optional*, defaults to `self.resample`) â€” Resampling filter
    to use if resizing the image. This can be one of the enum `PILImageResampling`,
    Only has an effect if `do_resize` is set to `True`.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.resample`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥æ˜¯æšä¸¾ `PILImageResampling`
    ä¸­çš„ä¸€ä¸ªï¼Œä»…åœ¨ `do_resize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚'
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) â€” Whether
    to normalize the image'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_normalize`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–'
- en: '`do_color_quantize` (`bool`, *optional*, defaults to `self.do_color_quantize`)
    â€” Whether to color quantize the image.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_color_quantize` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_color_quantize`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œé¢œè‰²é‡åŒ–ã€‚'
- en: '`clusters` (`np.ndarray` or `List[List[int]]`, *optional*, defaults to `self.clusters`)
    â€” Clusters used to quantize the image of shape `(n_clusters, 3)`. Only has an
    effect if `do_color_quantize` is set to `True`.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clusters` (`np.ndarray` æˆ– `List[List[int]]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.clusters`) â€” ç”¨äºå¯¹å›¾åƒè¿›è¡Œé‡åŒ–çš„èšç±»ï¼Œå½¢çŠ¶ä¸º
    `(n_clusters, 3)`ã€‚ä»…åœ¨ `do_color_quantize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) â€” The type of tensors
    to return. Can be one of:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` æˆ– `TensorType`ï¼Œ*å¯é€‰*) â€” è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'æœªè®¾ç½®: è¿”å›ä¸€ä¸ª `np.ndarray` åˆ—è¡¨ã€‚'
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW` æˆ– `''tf''`: è¿”å›ç±»å‹ä¸º `tf.Tensor` çš„æ‰¹æ¬¡ã€‚'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH` æˆ– `''pt''`: è¿”å›ç±»å‹ä¸º `torch.Tensor` çš„æ‰¹æ¬¡ã€‚'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY` æˆ– `''np''`: è¿”å›ç±»å‹ä¸º `np.ndarray` çš„æ‰¹æ¬¡ã€‚'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX` æˆ– `''jax''`: è¿”å›ç±»å‹ä¸º `jax.numpy.ndarray` çš„æ‰¹æ¬¡ã€‚'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    â€” The channel dimension format for the output image. Can be one of:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`ChannelDimension` æˆ– `str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `ChannelDimension.FIRST`)
    â€” è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: '`ChannelDimension.FIRST`: image in (num_channels, height, width) format.'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.FIRST`: å›¾åƒæ ¼å¼ä¸º (é€šé“æ•°, é«˜åº¦, å®½åº¦)ã€‚'
- en: '`ChannelDimension.LAST`: image in (height, width, num_channels) format. Only
    has an effect if `do_color_quantize` is set to `False`.'
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.LAST`: å›¾åƒæ ¼å¼ä¸º (é«˜åº¦, å®½åº¦, é€šé“æ•°)ã€‚ä»…åœ¨ `do_color_quantize` è®¾ç½®ä¸º `False`
    æ—¶æœ‰æ•ˆã€‚'
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) â€” The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format` (`ChannelDimension` æˆ– `str`ï¼Œ*å¯é€‰*) â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œåˆ™ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` æˆ– `ChannelDimension.FIRST`: å›¾åƒæ ¼å¼ä¸º (é€šé“æ•°, é«˜åº¦, å®½åº¦)ã€‚'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` æˆ– `ChannelDimension.LAST`: å›¾åƒæ ¼å¼ä¸º (é«˜åº¦, å®½åº¦, é€šé“æ•°)ã€‚'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"` æˆ– `ChannelDimension.NONE`: å›¾åƒæ ¼å¼ä¸º (é«˜åº¦, å®½åº¦)ã€‚'
- en: Preprocess an image or batch of images.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†å›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡ã€‚
- en: ImageGPTModel
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageGPTModel
- en: '### `class transformers.ImageGPTModel`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ImageGPTModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L608)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L608)'
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare ImageGPT Model transformer outputting raw hidden-states without any
    specific head on top.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„ ImageGPT æ¨¡å‹å˜å‹å™¨ï¼Œè¾“å‡ºæ²¡æœ‰ç‰¹å®šå¤´éƒ¨çš„åŸå§‹éšè—çŠ¶æ€ã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L645)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L645)'
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[-2]`
    (`sequence_length` of input past key value states). Indices of input sequence
    tokens in the vocabulary.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ `torch.LongTensor`ï¼‰ â€” å¦‚æœ
    `past_key_values` ä¸º `None`ï¼Œåˆ™ `input_ids_length` = `sequence_length`ï¼Œå¦åˆ™ä¸º `past_key_values[0][0].shape[-2]`ï¼ˆè¾“å…¥è¿‡å»å…³é”®å€¼çŠ¶æ€çš„åºåˆ—é•¿åº¦ï¼‰ã€‚è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: If `past_key_values` is used, only `input_ids` that do not have their past calculated
    should be passed as `input_ids`.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨äº† `past_key_values`ï¼Œåˆ™åªåº”å°†æœªè®¡ç®—å…¶è¿‡å»çš„ `input_ids` ä½œä¸º `input_ids` ä¼ é€’ã€‚
- en: Indices can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`)
    â€” Contains precomputed hidden-states (key and values in the attention blocks)
    as computed by the model (see `past_key_values` output below). Can be used to
    speed up sequential decoding. The `input_ids` which have their past given to this
    model should not be passed as `input_ids` as they have already been computed.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`ï¼ˆé•¿åº¦ä¸º`config.n_layers`çš„`Tuple[Tuple[torch.Tensor]]`ï¼‰- åŒ…å«ç”±æ¨¡å‹è®¡ç®—çš„é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ä»¥ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚å°†è¿‡å»ç»™å®šç»™è¯¥æ¨¡å‹çš„`input_ids`ä¸åº”ä½œä¸º`input_ids`ä¼ é€’ï¼Œå› ä¸ºå®ƒä»¬å·²ç»è¢«è®¡ç®—è¿‡ã€‚'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºè¢«`masked`çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯attention masksï¼Ÿ](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰-
    æ®µæ ‡è®°ç´¢å¼•ï¼Œç”¨äºæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0å¯¹åº”äº*å¥å­A*æ ‡è®°ï¼Œ
- en: 1 corresponds to a *sentence B* token.
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1å¯¹åº”äº*å¥å­B*æ ‡è®°ã€‚
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯token type IDsï¼Ÿ](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰-
    æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯position IDsï¼Ÿ](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨æ˜¯`masked`ã€‚
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰-
    å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: If `past_key_values` is used, optionally only the last `inputs_embeds` have
    to be input (see `past_key_values`).
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`inputs_embeds`ï¼ˆè¯·å‚è§`past_key_values`ï¼‰ã€‚
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆè¯·å‚è§`past_key_values`ï¼‰ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Labels for language modeling. Note that the labels **are shifted** inside the
    model, i.e. you can set `labels = input_ids` Indices are selected in `[-100, 0,
    ..., config.vocab_size]` All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè¯­è¨€å»ºæ¨¡çš„æ ‡ç­¾ã€‚è¯·æ³¨æ„ï¼Œæ¨¡å‹å†…éƒ¨çš„æ ‡ç­¾**å·²ç»è¢«ç§»ä½**ï¼Œå³æ‚¨å¯ä»¥è®¾ç½®`labels
    = input_ids`ã€‚ç´¢å¼•åœ¨`[-100, 0, ..., config.vocab_size]`ä¸­é€‰æ‹©ã€‚æ‰€æœ‰è®¾ç½®ä¸º`-100`çš„æ ‡ç­¾éƒ½è¢«å¿½ç•¥ï¼ˆè¢«`masked`ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—æ ‡ç­¾åœ¨`[0,
    ..., config.vocab_size]`ä¸­çš„æƒ…å†µã€‚'
- en: Returns
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)
    or `tuple(torch.FloatTensor)`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    and inputs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™åªè¾“å‡ºå½¢çŠ¶ä¸º`(batch_size, 1, hidden_size)`çš„åºåˆ—çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ã€‚
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and optionally if `config.is_encoder_decoder=True`
    2 additional tensors of shape `(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›)
    â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡ï¼Œå¦‚æœ`config.is_encoder_decoder=True`è¿˜æœ‰2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size,
    num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and optionally if `config.is_encoder_decoder=True` in the cross-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼Œä»¥åŠåœ¨äº¤å‰æ³¨æ„åŠ›å—ä¸­å¯é€‰åœ°ä½¿ç”¨`config.is_encoder_decoder=True`ï¼‰å¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆæŸ¥çœ‹`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œ+
    ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`)
    â€” Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`å’Œ`config.add_cross_attention=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)
    forward method, overrides the `__call__` special method.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ImageGPTForCausalImageModeling
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageGPTForCausalImageModeling
- en: '### `class transformers.ImageGPTForCausalImageModeling`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ImageGPTForCausalImageModeling`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L876)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L876)'
- en: '[PRE9]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)ï¼‰
    â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The ImageGPT Model transformer with a language modeling head on top (linear
    layer with weights tied to the input embeddings).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ImageGPTæ¨¡å‹å˜å‹å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´ï¼ˆçº¿æ€§å±‚ï¼Œå…¶æƒé‡ä¸è¾“å…¥åµŒå…¥å±‚ç»‘å®šï¼‰ã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L940)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[-2]`
    (`sequence_length` of input past key value states). Indices of input sequence
    tokens in the vocabulary.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, only `input_ids` that do not have their past calculated
    should be passed as `input_ids`.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`)
    â€” Contains precomputed hidden-states (key and values in the attention blocks)
    as computed by the model (see `past_key_values` output below). Can be used to
    speed up sequential decoding. The `input_ids` which have their past given to this
    model should not be passed as `input_ids` as they have already been computed.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `inputs_embeds` have
    to be input (see `past_key_values`).
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Labels for language modeling. Note that the labels **are shifted** inside the
    model, i.e. you can set `labels = input_ids` Indices are selected in `[-100, 0,
    ..., config.vocab_size]` All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*)
    â€” è¯­è¨€å»ºæ¨¡çš„æ ‡ç­¾ã€‚è¯·æ³¨æ„ï¼Œæ ‡ç­¾**åœ¨æ¨¡å‹å†…éƒ¨è¢«ç§»ä½**ï¼Œå³æ‚¨å¯ä»¥è®¾ç½®`labels = input_ids`ï¼Œç´¢å¼•åœ¨`[-100, 0, ..., config.vocab_size]`ä¸­é€‰æ‹©ã€‚æ‰€æœ‰è®¾ç½®ä¸º`-100`çš„æ ‡ç­¾éƒ½è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—åœ¨`[0,
    ..., config.vocab_size]`ä¸­çš„æ ‡ç­¾ã€‚'
- en: Returns
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)
    or `tuple(torch.FloatTensor)`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)æˆ–`torch.FloatTensor`å…ƒç»„'
- en: A [transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    and inputs.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼Œåˆ™æ ¹æ®é…ç½®ï¼ˆ[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)ï¼‰å’Œè¾“å…¥åŒ…å«å„ç§å…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Language modeling loss (for next-token prediction).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾›`labels`æ—¶è¿”å›) â€”
    è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`)
    â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” `torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥å±‚çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œ+ ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” `torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” `torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`ã€‚'
- en: Cross attentions weights after the attention softmax, used to compute the weighted
    average in the cross-attention heads.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„äº¤å‰æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `torch.FloatTensor`
    tuples of length `config.n_layers`, with each tuple containing the cached key,
    value states of the self-attention and the cross-attention layers if model is
    used in encoder-decoder setting. Only relevant if `config.is_decoder = True`.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›)
    â€” é•¿åº¦ä¸º`config.n_layers`çš„`torch.FloatTensor`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›å±‚çš„ç¼“å­˜é”®ã€å€¼çŠ¶æ€ï¼Œå¦‚æœæ¨¡å‹ç”¨äºç¼–ç å™¨-è§£ç å™¨è®¾ç½®ï¼Œåˆ™ç›¸å…³ã€‚ä»…åœ¨`config.is_decoder
    = True`æ—¶ç›¸å…³ã€‚'
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: The [ImageGPTForCausalImageModeling](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForCausalImageModeling)
    forward method, overrides the `__call__` special method.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImageGPTForCausalImageModeling](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForCausalImageModeling)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE11]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ImageGPTForImageClassification
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageGPTForImageClassification
- en: '### `class transformers.ImageGPTForImageClassification`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ImageGPTForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L1076)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ImageGPT Model transformer with an image classification head on top (linear
    layer). [ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)
    average-pools the hidden states in order to do the classification.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L1093)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[-2]`
    (`sequence_length` of input past key value states). Indices of input sequence
    tokens in the vocabulary.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, only `input_ids` that do not have their past calculated
    should be passed as `input_ids`.
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`)
    â€” Contains precomputed hidden-states (key and values in the attention blocks)
    as computed by the model (see `past_key_values` output below). Can be used to
    speed up sequential decoding. The `input_ids` which have their past given to this
    model should not be passed as `input_ids` as they have already been computed.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰â€”
    å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒæ¥å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚'
- en: If `past_key_values` is used, optionally only the last `inputs_embeds` have
    to be input (see `past_key_values`).
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™å¯èƒ½åªéœ€è¾“å…¥æœ€åçš„`inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,)`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels
    > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.modeling_outputs.SequenceClassifierOutputWithPast` or `tuple(torch.FloatTensor)`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_outputs.SequenceClassifierOutputWithPast`æˆ–`tuple(torch.FloatTensor)`'
- en: A `transformers.modeling_outputs.SequenceClassifierOutputWithPast` or a tuple
    of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    and inputs.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª`transformers.modeling_outputs.SequenceClassifierOutputWithPast`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” åˆ†ç±»ï¼ˆå¦‚æœconfig.num_labels==1åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€”
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`ï¼‰â€” åˆ†ç±»ï¼ˆå¦‚æœconfig.num_labels==1åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰â€”
    é•¿åº¦ä¸º`config.n_layers`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡ºåŠ ä¸Šæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤ä¹‹åè°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE14]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
