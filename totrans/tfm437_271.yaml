- en: ImageGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/imagegpt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/imagegpt)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ImageGPT model was proposed in [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt)
    by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya
    Sutskever. ImageGPT (iGPT) is a GPT-2-like model trained to predict the next pixel
    value, allowing for both unconditional and conditional image generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Inspired by progress in unsupervised representation learning for natural language,
    we examine whether similar models can learn useful representations for images.
    We train a sequence Transformer to auto-regressively predict pixels, without incorporating
    knowledge of the 2D input structure. Despite training on low-resolution ImageNet
    without labels, we find that a GPT-2 scale model learns strong image representations
    as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10,
    we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide
    ResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised
    pre-trained models. We are also competitive with self-supervised benchmarks on
    ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0% top-1
    accuracy on a linear probe of our features.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![drawing](../Images/7dfe07e48e8dd3472559e9232735486e.png) Summary of the approach.
    Taken from the [original paper](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr), based
    on [this issue](https://github.com/openai/image-gpt/issues/7). The original code
    can be found [here](https://github.com/openai/image-gpt).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ImageGPT is almost exactly the same as [GPT-2](gpt2), with the exception that
    a different activation function is used (namely ‚Äúquick gelu‚Äù), and the layer normalization
    layers don‚Äôt mean center the inputs. ImageGPT also doesn‚Äôt have tied input- and
    output embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the time- and memory requirements of the attention mechanism of Transformers
    scales quadratically in the sequence length, the authors pre-trained ImageGPT
    on smaller input resolutions, such as 32x32 and 64x64\. However, feeding a sequence
    of 32x32x3=3072 tokens from 0..255 into a Transformer is still prohibitively large.
    Therefore, the authors applied k-means clustering to the (R,G,B) pixel values
    with k=512\. This way, we only have a 32*32 = 1024-long sequence, but now of integers
    in the range 0..511\. So we are shrinking the sequence length at the cost of a
    bigger embedding matrix. In other words, the vocabulary size of ImageGPT is 512,
    + 1 for a special ‚Äústart of sentence‚Äù (SOS) token, used at the beginning of every
    sequence. One can use [ImageGPTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTImageProcessor)
    to prepare images for the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite being pre-trained entirely unsupervised (i.e. without the use of any
    labels), ImageGPT produces fairly performant image features useful for downstream
    tasks, such as image classification. The authors showed that the features in the
    middle of the network are the most performant, and can be used as-is to train
    a linear model (such as a sklearn logistic regression model for example). This
    is also referred to as ‚Äúlinear probing‚Äù. Features can be easily obtained by first
    forwarding the image through the model, then specifying `output_hidden_states=True`,
    and then average-pool the hidden states at whatever layer you like.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, one can further fine-tune the entire model on a downstream dataset,
    similar to BERT. For this, you can use [ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ImageGPT comes in different sizes: there‚Äôs ImageGPT-small, ImageGPT-medium
    and ImageGPT-large. The authors did also train an XL variant, which they didn‚Äôt
    release. The differences in size are summarized in the following table:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **Model variant** | **Depths** | **Hidden sizes** | **Decoder hidden size**
    | **Params (M)** | **ImageNet-1k Top 1** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MiT-b0 | [2, 2, 2, 2] | [32, 64, 160, 256] | 256 | 3.7 | 70.5 |'
  prefs: []
  type: TYPE_TB
- en: '| MiT-b1 | [2, 2, 2, 2] | [64, 128, 320, 512] | 256 | 14.0 | 78.7 |'
  prefs: []
  type: TYPE_TB
- en: '| MiT-b2 | [3, 4, 6, 3] | [64, 128, 320, 512] | 768 | 25.4 | 81.6 |'
  prefs: []
  type: TYPE_TB
- en: '| MiT-b3 | [3, 4, 18, 3] | [64, 128, 320, 512] | 768 | 45.2 | 83.1 |'
  prefs: []
  type: TYPE_TB
- en: '| MiT-b4 | [3, 8, 27, 3] | [64, 128, 320, 512] | 768 | 62.6 | 83.6 |'
  prefs: []
  type: TYPE_TB
- en: '| MiT-b5 | [3, 6, 40, 3] | [64, 128, 320, 512] | 768 | 82.0 | 83.8 |'
  prefs: []
  type: TYPE_TB
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A list of official Hugging Face and community (indicated by üåé) resources to
    help you get started with ImageGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Image Classification
  prefs: []
  type: TYPE_NORMAL
- en: Demo notebooks for ImageGPT can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ImageGPT).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you‚Äôre interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we‚Äôll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  prefs: []
  type: TYPE_NORMAL
- en: ImageGPTConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ImageGPTConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/configuration_imagegpt.py#L37)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 512) ‚Äî Vocabulary size of the
    GPT-2 model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)
    or `TFImageGPTModel`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_positions` (`int`, *optional*, defaults to 32*32) ‚Äî The maximum sequence
    length that this model might ever be used with. Typically set this to something
    large just in case (e.g., 512 or 1024 or 2048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_embd` (`int`, *optional*, defaults to 512) ‚Äî Dimensionality of the embeddings
    and hidden states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_layer` (`int`, *optional*, defaults to 24) ‚Äî Number of hidden layers in
    the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_head` (`int`, *optional*, defaults to 8) ‚Äî Number of attention heads for
    each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_inner` (`int`, *optional*, defaults to None) ‚Äî Dimensionality of the inner
    feed-forward layers. `None` will set it to 4 times n_embd'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation_function` (`str`, *optional*, defaults to `"quick_gelu"`) ‚Äî Activation
    function (can be one of the activation functions defined in src/transformers/activations.py).
    Defaults to ‚Äúquick_gelu‚Äù.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resid_pdrop` (`float`, *optional*, defaults to 0.1) ‚Äî The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embd_pdrop` (`int`, *optional*, defaults to 0.1) ‚Äî The dropout ratio for the
    embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attn_pdrop` (`float`, *optional*, defaults to 0.1) ‚Äî The dropout ratio for
    the attention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_epsilon` (`float`, *optional*, defaults to 1e-5) ‚Äî The epsilon
    to use in the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) ‚Äî The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scale_attn_weights` (`bool`, *optional*, defaults to `True`) ‚Äî Scale attention
    weights by dividing by sqrt(hidden_size)..'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not the model
    should return the last key/values attentions (not used by all models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scale_attn_by_inverse_layer_idx` (`bool`, *optional*, defaults to `False`)
    ‚Äî Whether to additionally scale attention weights by `1 / layer_idx + 1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reorder_and_upcast_attn` (`bool`, *optional*, defaults to `False`) ‚Äî Whether
    to scale keys (K) prior to computing attention (dot-product) and upcast attention
    dot-product/softmax to float() when training with mixed precision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)
    or a `TFImageGPTModel`. It is used to instantiate a GPT-2 model according to the
    specified arguments, defining the model architecture. Instantiating a configuration
    with the defaults will yield a similar configuration to that of the ImageGPT [openai/imagegpt-small](https://huggingface.co/openai/imagegpt-small)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ImageGPTFeatureExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ImageGPTFeatureExtractor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/feature_extraction_imagegpt.py#L26)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Preprocess an image or a batch of images.
  prefs: []
  type: TYPE_NORMAL
- en: ImageGPTImageProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ImageGPTImageProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/image_processing_imagegpt.py#L58)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`clusters` (`np.ndarray` or `List[List[int]]`, *optional*) ‚Äî The color clusters
    to use, of shape `(n_clusters, 3)` when color quantizing. Can be overriden by
    `clusters` in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) ‚Äî Whether to resize the
    image‚Äôs dimensions to `(size["height"], size["width"])`. Can be overridden by
    `do_resize` in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"height" -- 256, "width":
    256}`): Size of the image after resizing. Can be overridden by `size` in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`)
    ‚Äî Resampling filter to use if resizing the image. Can be overridden by `resample`
    in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) ‚Äî Whether to normalize
    the image pixel value to between [-1, 1]. Can be overridden by `do_normalize`
    in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_color_quantize` (`bool`, *optional*, defaults to `True`) ‚Äî Whether to color
    quantize the image. Can be overridden by `do_color_quantize` in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a ImageGPT image processor. This image processor can be used to resize
    images to a smaller resolution (such as 32x32 or 64x64), normalize them and finally
    color quantize them to obtain sequences of ‚Äúpixel values‚Äù (color clusters).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `preprocess`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/image_processing_imagegpt.py#L175)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`ImageInput`) ‚Äî Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_normalize=False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) ‚Äî Whether to
    resize the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) ‚Äî Size of the
    image after resizing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample` (`int`, *optional*, defaults to `self.resample`) ‚Äî Resampling filter
    to use if resizing the image. This can be one of the enum `PILImageResampling`,
    Only has an effect if `do_resize` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) ‚Äî Whether
    to normalize the image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_color_quantize` (`bool`, *optional*, defaults to `self.do_color_quantize`)
    ‚Äî Whether to color quantize the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clusters` (`np.ndarray` or `List[List[int]]`, *optional*, defaults to `self.clusters`)
    ‚Äî Clusters used to quantize the image of shape `(n_clusters, 3)`. Only has an
    effect if `do_color_quantize` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or `TensorType`, *optional*) ‚Äî The type of tensors
    to return. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unset: Return a list of `np.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    ‚Äî The channel dimension format for the output image. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChannelDimension.FIRST`: image in (num_channels, height, width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChannelDimension.LAST`: image in (height, width, num_channels) format. Only
    has an effect if `do_color_quantize` is set to `False`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) ‚Äî The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess an image or batch of images.
  prefs: []
  type: TYPE_NORMAL
- en: ImageGPTModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ImageGPTModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L608)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    ‚Äî Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare ImageGPT Model transformer outputting raw hidden-states without any
    specific head on top.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L645)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) ‚Äî
    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[-2]`
    (`sequence_length` of input past key value states). Indices of input sequence
    tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, only `input_ids` that do not have their past calculated
    should be passed as `input_ids`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`)
    ‚Äî Contains precomputed hidden-states (key and values in the attention blocks)
    as computed by the model (see `past_key_values` output below). Can be used to
    speed up sequential decoding. The `input_ids` which have their past given to this
    model should not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) ‚Äî Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) ‚Äî Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model‚Äôs internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `inputs_embeds` have
    to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) ‚Äî If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) ‚Äî Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) ‚Äî Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) ‚Äî Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    ‚Äî Labels for language modeling. Note that the labels **are shifted** inside the
    model, i.e. you can set `labels = input_ids` Indices are selected in `[-100, 0,
    ..., config.vocab_size]` All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) ‚Äî Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) ‚Äî Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and optionally if `config.is_encoder_decoder=True`
    2 additional tensors of shape `(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and optionally if `config.is_encoder_decoder=True` in the cross-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`)
    ‚Äî Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder‚Äôs cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: ImageGPTForCausalImageModeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ImageGPTForCausalImageModeling`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L876)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    ‚Äî Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ImageGPT Model transformer with a language modeling head on top (linear
    layer with weights tied to the input embeddings).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L940)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) ‚Äî
    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[-2]`
    (`sequence_length` of input past key value states). Indices of input sequence
    tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, only `input_ids` that do not have their past calculated
    should be passed as `input_ids`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`)
    ‚Äî Contains precomputed hidden-states (key and values in the attention blocks)
    as computed by the model (see `past_key_values` output below). Can be used to
    speed up sequential decoding. The `input_ids` which have their past given to this
    model should not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) ‚Äî Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) ‚Äî Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model‚Äôs internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `inputs_embeds` have
    to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) ‚Äî If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) ‚Äî Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) ‚Äî Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) ‚Äî Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    ‚Äî Labels for language modeling. Note that the labels **are shifted** inside the
    model, i.e. you can set `labels = input_ids` Indices are selected in `[-100, 0,
    ..., config.vocab_size]` All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) ‚Äî Language modeling loss (for next-token prediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    ‚Äî Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross attentions weights after the attention softmax, used to compute the weighted
    average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) ‚Äî Tuple of `torch.FloatTensor`
    tuples of length `config.n_layers`, with each tuple containing the cached key,
    value states of the self-attention and the cross-attention layers if model is
    used in encoder-decoder setting. Only relevant if `config.is_decoder = True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ImageGPTForCausalImageModeling](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForCausalImageModeling)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ImageGPTForImageClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ImageGPTForImageClassification`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L1076)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    ‚Äî Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ImageGPT Model transformer with an image classification head on top (linear
    layer). [ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)
    average-pools the hidden states in order to do the classification.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L1093)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) ‚Äî
    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[-2]`
    (`sequence_length` of input past key value states). Indices of input sequence
    tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, only `input_ids` that do not have their past calculated
    should be passed as `input_ids`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`)
    ‚Äî Contains precomputed hidden-states (key and values in the attention blocks)
    as computed by the model (see `past_key_values` output below). Can be used to
    speed up sequential decoding. The `input_ids` which have their past given to this
    model should not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) ‚Äî Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) ‚Äî Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model‚Äôs internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `inputs_embeds` have
    to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) ‚Äî If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) ‚Äî Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) ‚Äî Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) ‚Äî Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) ‚Äî Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.modeling_outputs.SequenceClassifierOutputWithPast` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.modeling_outputs.SequenceClassifierOutputWithPast` or a tuple
    of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) ‚Äî Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) ‚Äî
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) ‚Äî Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
