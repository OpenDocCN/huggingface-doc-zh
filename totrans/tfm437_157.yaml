- en: DialoGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dialogpt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dialogpt)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/110.bca970d1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DialoGPT was proposed in [DialoGPT: Large-Scale Generative Pre-training for
    Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe
    Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng
    Gao, Jingjing Liu, Bill Dolan. It’s a GPT2 Model trained on 147M conversation-like
    exchanges extracted from Reddit.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We present a large, tunable neural conversational response generation model,
    DialoGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like
    exchanges extracted from Reddit comment chains over a period spanning from 2005
    through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain
    a performance close to human both in terms of automatic and human evaluation in
    single-turn dialogue settings. We show that conversational systems that leverage
    DialoGPT generate more relevant, contentful and context-consistent responses than
    strong baseline systems. The pre-trained model and training pipeline are publicly
    released to facilitate research into neural response generation and the development
    of more intelligent open-domain dialogue systems.*'
  prefs: []
  type: TYPE_NORMAL
- en: The original code can be found [here](https://github.com/microsoft/DialoGPT).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DialoGPT is a model with absolute position embeddings so it’s usually advised
    to pad the inputs on the right rather than the left.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DialoGPT was trained with a causal language modeling (CLM) objective on conversational
    data and is therefore powerful at response generation in open-domain dialogue
    systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DialoGPT enables the user to create a chat bot in just 10 lines of code as shown
    on [DialoGPT’s model card](https://huggingface.co/microsoft/DialoGPT-medium).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to train or fine-tune DialoGPT, one can use causal language modeling
    training. To cite the official paper: *We follow the OpenAI GPT-2 to model a multiturn
    dialogue session as a long text and frame the generation task as language modeling.
    We first concatenate all dialog turns within a dialogue session into a long text
    x_1,…, x_N (N is the sequence length), ended by the end-of-text token.* For more
    information please confer to the original paper.'
  prefs: []
  type: TYPE_NORMAL
- en: DialoGPT’s architecture is based on the GPT2 model, refer to [GPT2’s documentation
    page](gpt2) for API reference and examples.
  prefs: []
  type: TYPE_NORMAL
