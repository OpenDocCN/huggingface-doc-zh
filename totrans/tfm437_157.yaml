- en: DialoGPT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DialoGPT
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dialogpt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dialogpt)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dialogpt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dialogpt)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'DialoGPT was proposed in [DialoGPT: Large-Scale Generative Pre-training for
    Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe
    Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng
    Gao, Jingjing Liu, Bill Dolan. It’s a GPT2 Model trained on 147M conversation-like
    exchanges extracted from Reddit.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: DialoGPT是由Yizhe Zhang、Siqi Sun、Michel Galley、Yen-Chun Chen、Chris Brockett、Xiang
    Gao、Jianfeng Gao、Jingjing Liu、Bill Dolan在[《DialoGPT:大规模生成式预训练用于对话回复生成》](https://arxiv.org/abs/1911.00536)中提出的。它是一个在Reddit上提取的147M对话式交流数据上训练的GPT2模型。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*We present a large, tunable neural conversational response generation model,
    DialoGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like
    exchanges extracted from Reddit comment chains over a period spanning from 2005
    through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain
    a performance close to human both in terms of automatic and human evaluation in
    single-turn dialogue settings. We show that conversational systems that leverage
    DialoGPT generate more relevant, contentful and context-consistent responses than
    strong baseline systems. The pre-trained model and training pipeline are publicly
    released to facilitate research into neural response generation and the development
    of more intelligent open-domain dialogue systems.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们提出了一个大型、可调节的神经对话回复生成模型DialoGPT（对话生成式预训练变压器）。在Reddit评论链中提取的147M对话式交流数据上训练，跨越2005年至2017年，DialoGPT扩展了Hugging
    Face PyTorch变压器，以在单轮对话设置中实现接近人类的自动和人类评估性能。我们展示了利用DialoGPT的对话系统生成比强基线系统更相关、内容更丰富和上下文更一致的回复。预训练模型和训练流程已公开发布，以促进神经回复生成研究和更智能的开放领域对话系统的发展。*'
- en: The original code can be found [here](https://github.com/microsoft/DialoGPT).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 原始代码可以在[这里](https://github.com/microsoft/DialoGPT)找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: DialoGPT is a model with absolute position embeddings so it’s usually advised
    to pad the inputs on the right rather than the left.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DialoGPT是一个带有绝对位置嵌入的模型，因此通常建议在右侧填充输入而不是左侧。
- en: DialoGPT was trained with a causal language modeling (CLM) objective on conversational
    data and is therefore powerful at response generation in open-domain dialogue
    systems.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DialoGPT在对话数据上使用因果语言建模（CLM）目标进行训练，因此在开放领域对话系统中的回复生成方面非常强大。
- en: DialoGPT enables the user to create a chat bot in just 10 lines of code as shown
    on [DialoGPT’s model card](https://huggingface.co/microsoft/DialoGPT-medium).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DialoGPT使用户可以仅用10行代码创建一个聊天机器人，如[DialoGPT的模型卡片](https://huggingface.co/microsoft/DialoGPT-medium)所示。
- en: 'Training:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 训练：
- en: 'In order to train or fine-tune DialoGPT, one can use causal language modeling
    training. To cite the official paper: *We follow the OpenAI GPT-2 to model a multiturn
    dialogue session as a long text and frame the generation task as language modeling.
    We first concatenate all dialog turns within a dialogue session into a long text
    x_1,…, x_N (N is the sequence length), ended by the end-of-text token.* For more
    information please confer to the original paper.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练或微调DialoGPT，可以使用因果语言建模训练。引用官方论文中的话：*我们遵循OpenAI GPT-2，将多轮对话会话建模为长文本，并将生成任务构建为语言建模。我们首先将对话会话中的所有对话轮次连接成一个长文本x_1,…,
    x_N（N为序列长度），以结束文本标记结束。*更多信息请参考原始论文。
- en: DialoGPT’s architecture is based on the GPT2 model, refer to [GPT2’s documentation
    page](gpt2) for API reference and examples.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: DialoGPT的架构基于GPT2模型，请参考[GPT2的文档页面](gpt2)获取API参考和示例。
