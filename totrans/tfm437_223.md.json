["```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> device = \"cuda\" # the device to load the model onto\n\n>>> model = AutoModelForCausalLM.from_pretrained(\"Qwen2/Qwen2-7B-Chat-beta\", device_map=\"auto\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen2/Qwen2-7B-Chat-beta\")\n\n>>> prompt = \"Give me a short introduction to large language model.\"\n\n>>> messages = [{\"role\": \"user\", \"content\": prompt}]\n\n>>> text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n>>> model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n>>> generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)\n\n>>> generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n\n>>> response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```", "```py\n( vocab_size = 151936 hidden_size = 4096 intermediate_size = 22016 num_hidden_layers = 32 num_attention_heads = 32 num_key_value_heads = 32 hidden_act = 'silu' max_position_embeddings = 32768 initializer_range = 0.02 rms_norm_eps = 1e-06 use_cache = True tie_word_embeddings = False rope_theta = 10000.0 use_sliding_window = False sliding_window = 4096 max_window_layers = 28 attention_dropout = 0.0 **kwargs )\n```", "```py\n>>> from transformers import Qwen2Model, Qwen2Config\n\n>>> # Initializing a Qwen2 style configuration\n>>> configuration = Qwen2Config()\n\n>>> # Initializing a model from the Qwen2-7B style configuration\n>>> model = Qwen2Model(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file merges_file errors = 'replace' unk_token = '<|endoftext|>' bos_token = None eos_token = '<|endoftext|>' pad_token = '<|endoftext|>' clean_up_tokenization_spaces = False split_special_tokens = False **kwargs )\n```", "```py\n>>> from transformers import Qwen2Tokenizer\n\n>>> tokenizer = Qwen2Tokenizer.from_pretrained(\"Qwen/Qwen-tokenizer\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[9707, 1879]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[21927, 1879]\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( vocab_file = None merges_file = None tokenizer_file = None unk_token = '<|endoftext|>' bos_token = None eos_token = '<|endoftext|>' pad_token = '<|endoftext|>' **kwargs )\n```", "```py\n>>> from transformers import Qwen2TokenizerFast\n\n>>> tokenizer = Qwen2TokenizerFast.from_pretrained(\"Qwen/Qwen-tokenizer\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[9707, 1879]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[21927, 1879]\n```", "```py\n( config: Qwen2Config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, Qwen2ForCausalLM\n\n>>> model = Qwen2ForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n>>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n\n>>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n>>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> # Generate\n>>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n>>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```"]