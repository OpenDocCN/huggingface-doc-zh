- en: Optimize inference using torch.compile()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/perf_torch_compile](https://huggingface.co/docs/transformers/v4.37.2/en/perf_torch_compile)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/320.0e13e266.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: This guide aims to provide a benchmark on the inference speed-ups introduced
    with [`torch.compile()`](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)Â for
    [computer vision models in ðŸ¤— Transformers](https://huggingface.co/models?pipeline_tag=image-classification&library=transformers&sort=trending).
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of torch.compile
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Depending on the model and the GPU, `torch.compile()` yields up to 30% speed-up
    during inference. To use `torch.compile()`, simply install any version of `torch`
    above 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compiling a model takes time, so itâ€™s useful if you are compiling the model
    only once instead of every time you infer. To compile any computer vision model
    of your choice, call `torch.compile()` on the model as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`compile()`Â comes with multiple modes for compiling, which essentially differ
    in compilation time and inference overhead. `max-autotune`Â takes longer than `reduce-overhead`Â but
    results in faster inference. Default mode is fastest for compilation but is not
    as efficient compared to `reduce-overhead` for inference time. In this guide,
    we used the default mode. You can learn more about it [here](https://pytorch.org/get-started/pytorch-2.0/#user-experience).'
  prefs: []
  type: TYPE_NORMAL
- en: We benchmarked `torch.compile` with different computer vision models, tasks,
    types of hardware, and batch sizes on `torch`Â version 2.0.1.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Below you can find the benchmarking code for each task. We warm up the GPU before
    inference and take the mean time of 300 inferences, using the same image each
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Image Classification with ViT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Object Detection with DETR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Image Segmentation with Segformer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Below you can find the list of the models we benchmarked.
  prefs: []
  type: TYPE_NORMAL
- en: '**Image Classification**'
  prefs: []
  type: TYPE_NORMAL
- en: '[google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[microsoft/beit-base-patch16-224-pt22k-ft22k](https://huggingface.co/microsoft/beit-base-patch16-224-pt22k-ft22k)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[facebook/convnext-large-224](https://huggingface.co/facebook/convnext-large-224)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[microsoft/resnet-50](https://huggingface.co/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image Segmentation**'
  prefs: []
  type: TYPE_NORMAL
- en: '[nvidia/segformer-b0-finetuned-ade-512-512](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[facebook/mask2former-swin-tiny-coco-panoptic](https://huggingface.co/facebook/mask2former-swin-tiny-coco-panoptic)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[facebook/maskformer-swin-base-ade](https://huggingface.co/facebook/maskformer-swin-base-ade)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[google/deeplabv3_mobilenet_v2_1.0_513](https://huggingface.co/google/deeplabv3_mobilenet_v2_1.0_513)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object Detection**'
  prefs: []
  type: TYPE_NORMAL
- en: '[google/owlvit-base-patch32](https://huggingface.co/google/owlvit-base-patch32)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[facebook/detr-resnet-101](https://huggingface.co/facebook/detr-resnet-101)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[microsoft/conditional-detr-resnet-50](https://huggingface.co/microsoft/conditional-detr-resnet-50)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Below you can find visualization of inference durations with and without `torch.compile()`Â and
    percentage improvements for each model in different hardware and batch sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/371979e4a66b66d493287c3d7f51d2d8.png)![](../Images/02da43bec655f1f8fc466bdbe14fa012.png)![](../Images/9109d187b7a12ee73d95da7b45b39cb2.png)![](../Images/87a432d9807c5b0267d63ddf71148c01.png)![](../Images/eb2dfd0de63fee7e557533652fc75bef.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Duration Comparison on V100 with Batch Size of 1](../Images/28839892a7c0c288676c2bec3735258a.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Percentage Improvement on T4 with Batch Size of 4](../Images/a404e9b598e7cf47c5e67e3c81e06dd5.png)'
  prefs: []
  type: TYPE_IMG
- en: Below you can find inference durations in milliseconds for each model with and
    without `compile()`. Note that OwlViT results in OOM in larger batch sizes.
  prefs: []
  type: TYPE_NORMAL
- en: 'A100 (batch size: 1)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| **Task/Model** | **torch 2.0 - no compile** | **torch 2.0 - compile** |'
  prefs: []
  type: TYPE_TB
- en: '| :-: | :-: | :-: |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ViT | 9.325 | 7.584 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Segformer | 11.759 | 10.500 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/OwlViT | 24.978 | 18.420 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | 11.282 | 8.448 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | 34.619 | 19.040 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ConvNeXT | 10.410 | 10.208 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ResNet | 6.531 | 4.124 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Mask2former | 60.188 | 49.117 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Maskformer | 75.764 | 59.487 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/MobileNet | 8.583 | 3.974 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Resnet-101 | 36.276 | 18.197 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Conditional-DETR | 31.219 | 17.993 |'
  prefs: []
  type: TYPE_TB
- en: 'A100 (batch size: 4)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| **Task/Model** | **torch 2.0 - no compile** | **torch 2.0 - compile** |'
  prefs: []
  type: TYPE_TB
- en: '| :-: | :-: | :-: |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ViT | 14.832 | 14.499 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Segformer | 18.838 | 16.476 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | 13.205 | 13.048 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | 48.657 | 32.418 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ConvNeXT | 22.940 | 21.631 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ResNet | 6.657 | 4.268 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Mask2former | 74.277 | 61.781 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Maskformer | 180.700 | 159.116 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/MobileNet | 14.174 | 8.515 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Resnet-101 | 68.101 | 44.998 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Conditional-DETR | 56.470 | 35.552 |'
  prefs: []
  type: TYPE_TB
- en: 'A100 (batch size: 16)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| **Task/Model** | **torch 2.0 - no compile** | **torch 2.0 - compile** |'
  prefs: []
  type: TYPE_TB
- en: '| :-: | :-: | :-: |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ViT | 40.944 | 40.010 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Segformer | 37.005 | 31.144 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | 41.854 | 41.048 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | 164.382 | 161.902 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ConvNeXT | 82.258 | 75.561 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ResNet | 7.018 | 5.024 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Mask2former | 178.945 | 154.814 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Maskformer | 638.570 | 579.826 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/MobileNet | 51.693 | 30.310 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Resnet-101 | 232.887 | 155.021 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Conditional-DETR | 180.491 | 124.032 |'
  prefs: []
  type: TYPE_TB
- en: 'V100 (batch size: 1)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| **Task/Model** | **torch 2.0 - no compile** | **torch 2.0 - compile** |'
  prefs: []
  type: TYPE_TB
- en: '| :-: | :-: | :-: |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ViT | 10.495 | 6.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Segformer | 13.321 | 5.862 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/OwlViT | 25.769 | 22.395 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | 11.347 | 7.234 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | 33.951 | 19.388 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ConvNeXT | 11.623 | 10.412 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ResNet | 6.484 | 3.820 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Mask2former | 64.640 | 49.873 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Maskformer | 95.532 | 72.207 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/MobileNet | 9.217 | 4.753 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Resnet-101 | 52.818 | 28.367 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Conditional-DETR | 39.512 | 20.816 |'
  prefs: []
  type: TYPE_TB
- en: 'V100 (batch size: 4)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| **Task/Model** | **torch 2.0 - no compile** | **torch 2.0 - compile** |'
  prefs: []
  type: TYPE_TB
- en: '| :-: | :-: | :-: |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ViT | 15.181 | 14.501 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Segformer | 16.787 | 16.188 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | 15.171 | 14.753 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | 88.529 | 64.195 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ConvNeXT | 29.574 | 27.085 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ResNet | 6.109 | 4.731 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Mask2former | 90.402 | 76.926 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Maskformer | 234.261 | 205.456 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/MobileNet | 24.623 | 14.816 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Resnet-101 | 134.672 | 101.304 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Conditional-DETR | 97.464 | 69.739 |'
  prefs: []
  type: TYPE_TB
- en: 'V100 (batch size: 16)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| **Task/Model** | **torch 2.0 - no compile** | **torch 2.0 - compile** |'
  prefs: []
  type: TYPE_TB
- en: '| :-: | :-: | :-: |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ViT | 52.209 | 51.633 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Segformer | 61.013 | 55.499 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | 53.938 | 53.581 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | OOM | OOM |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ConvNeXT | 109.682 | 100.771 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ResNet | 14.857 | 12.089 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Mask2former | 249.605 | 222.801 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Maskformer | 831.142 | 743.645 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/MobileNet | 93.129 | 55.365 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Resnet-101 | 482.425 | 361.843 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Conditional-DETR | 344.661 | 255.298 |'
  prefs: []
  type: TYPE_TB
- en: 'T4 (batch size: 1)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| **Task/Model** | **torch 2.0 - no compile** | **torch 2.0 - compile** |'
  prefs: []
  type: TYPE_TB
- en: '| :-: | :-: | :-: |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ViT | 16.520 | 15.786 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Segformer | 16.116 | 14.205 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/OwlViT | 53.634 | 51.105 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | 16.464 | 15.710 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | 73.100 | 53.99 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ConvNeXT | 32.932 | 30.845 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ResNet | 6.031 | 4.321 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Mask2former | 79.192 | 66.815 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Maskformer | 200.026 | 188.268 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/MobileNet | 18.908 | 11.997 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Resnet-101 | 106.622 | 82.566 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Conditional-DETR | 77.594 | 56.984 |'
  prefs: []
  type: TYPE_TB
- en: 'T4 (batch size: 4)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| **Task/Model** | **torch 2.0 - no compile** | **torch 2.0 - compile** |'
  prefs: []
  type: TYPE_TB
- en: '| :-: | :-: | :-: |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ViT | 43.653 | 43.626 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Segformer | 45.327 | 42.445 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | 52.007 | 51.354 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | 277.850 | 268.003 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ConvNeXT | 119.259 | 105.580 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ResNet | 13.039 | 11.388 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Mask2former | 201.540 | 184.670 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Maskformer | 764.052 | 711.280 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/MobileNet | 74.289 | 48.677 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Resnet-101 | 421.859 | 357.614 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Conditional-DETR | 289.002 | 226.945 |'
  prefs: []
  type: TYPE_TB
- en: 'T4 (batch size: 16)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| **Task/Model** | **torch 2.0 - no compile** | **torch 2.0 - compile** |'
  prefs: []
  type: TYPE_TB
- en: '| :-: | :-: | :-: |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ViT | 163.914 | 160.907 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Segformer | 192.412 | 163.620 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | 188.978 | 187.976 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | OOM | OOM |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ConvNeXT | 422.886 | 388.078 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ResNet | 44.114 | 37.604 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Mask2former | 756.337 | 695.291 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/Maskformer | 2842.940 | 2656.88 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/MobileNet | 299.003 | 201.942 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Resnet-101 | 1619.505 | 1262.758 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Conditional-DETR | 1137.513 | 897.390 |'
  prefs: []
  type: TYPE_TB
- en: PyTorch Nightly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We also benchmarked on PyTorch nightly (2.1.0dev, find the wheel [here](https://download.pytorch.org/whl/nightly/cu118))
    and observed improvement in latency both for uncompiled and compiled models.
  prefs: []
  type: TYPE_NORMAL
- en: A100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| **Task/Model** | **Batch Size** | **torch 2.0 - no compile** | **torch 2.0
    - compile** |'
  prefs: []
  type: TYPE_TB
- en: '| :-: | :-: | :-: | :-: |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | Unbatched | 12.462 | 6.954 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | 4 | 14.109 | 12.851 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | 16 | 42.179 | 42.147 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | Unbatched | 30.484 | 15.221 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | 4 | 46.816 | 30.942 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | 16 | 163.749 | 163.706 |'
  prefs: []
  type: TYPE_TB
- en: T4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| **Task/Model** | **Batch Size** | **torch 2.0 - no compile** | **torch 2.0
    - compile** |'
  prefs: []
  type: TYPE_TB
- en: '| :-: | :-: | :-: | :-: |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | Unbatched | 14.408 | 14.052 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | 4 | 47.381 | 46.604 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | 16 | 42.179 | 42.147 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | Unbatched | 68.382 | 53.481 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | 4 | 269.615 | 204.785 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | 16 | OOM | OOM |'
  prefs: []
  type: TYPE_TB
- en: '###Â V100'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Task/Model** | **Batch Size** | **torch 2.0 - no compile** | **torch 2.0
    - compile** |'
  prefs: []
  type: TYPE_TB
- en: '| :-: | :-: | :-: | :-: |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | Unbatched | 13.477 | 7.926 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | 4 | 15.103 | 14.378 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/BeiT | 16 | 52.517 | 51.691 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | Unbatched | 28.706 | 19.077 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | 4 | 88.402 | 62.949 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/DETR | 16 | OOM | OOM |'
  prefs: []
  type: TYPE_TB
- en: Reduce Overhead
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We benchmarked `reduce-overhead` compilation mode for A100 and T4 in Nightly.
  prefs: []
  type: TYPE_NORMAL
- en: A100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| **Task/Model** | **Batch Size** | **torch 2.0 - no compile** | **torch 2.0
    - compile** |'
  prefs: []
  type: TYPE_TB
- en: '| :-: | :-: | :-: | :-: |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ConvNeXT | Unbatched | 11.758 | 7.335 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ConvNeXT | 4 | 23.171 | 21.490 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ResNet | Unbatched | 7.435 | 3.801 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ResNet | 4 | 7.261 | 2.187 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Conditional-DETR | Unbatched | 32.823 | 11.627 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Conditional-DETR | 4 | 50.622 | 33.831 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/MobileNet | Unbatched | 9.869 | 4.244 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/MobileNet | 4 | 14.385 | 7.946 |'
  prefs: []
  type: TYPE_TB
- en: T4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| **Task/Model** | **Batch Size** | **torch 2.0 - no compile** | **torch 2.0
    - compile** |'
  prefs: []
  type: TYPE_TB
- en: '| :-: | :-: | :-: | :-: |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ConvNeXT | Unbatched | 32.137 | 31.84 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ConvNeXT | 4 | 120.944 | 110.209 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ResNet | Unbatched | 9.761 | 7.698 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Classification/ResNet | 4 | 15.215 | 13.871 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Conditional-DETR | Unbatched | 72.150 | 57.660 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection/Conditional-DETR | 4 | 301.494 | 247.543 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/MobileNet | Unbatched | 22.266 | 19.339 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Segmentation/MobileNet | 4 | 78.311 | 50.983 |'
  prefs: []
  type: TYPE_TB
