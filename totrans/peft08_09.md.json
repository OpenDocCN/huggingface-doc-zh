["```py\npip install -q peft transformers datasets\n```", "```py\nfrom datasets import load_dataset\n\nds = load_dataset(\"ought/raft\", \"twitter_complaints\")\n\nclasses = [k.replace(\"_\", \" \") for k in ds[\"train\"].features[\"Label\"].names]\nds = ds.map(\n    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n    batched=True,\n    num_proc=1,\n)\nds[\"train\"][0]\n{\"Tweet text\": \"@HMRCcustomers No this is my first job\", \"ID\": 0, \"Label\": 2, \"text_label\": \"no complaint\"}\n```", "```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\ntarget_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\nprint(target_max_length)\n```", "```py\nimport torch\n\nmax_length = 64\n\ndef preprocess_function(examples, text_column=\"Tweet text\", label_column=\"text_label\"):\n    batch_size = len(examples[text_column])\n    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n    targets = [str(x) for x in examples[label_column]]\n    model_inputs = tokenizer(inputs)\n    labels = tokenizer(targets)\n    classes = [k.replace(\"_\", \" \") for k in ds[\"train\"].features[\"Label\"].names]\n    for i in range(batch_size):\n        sample_input_ids = model_inputs[\"input_ids\"][i]\n        label_input_ids = labels[\"input_ids\"][i]\n        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n            max_length - len(sample_input_ids)\n        ) + sample_input_ids\n        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n            \"attention_mask\"\n        ][i]\n        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n```", "```py\nprocessed_ds = ds.map(\n    preprocess_function,\n    batched=True,\n    num_proc=1,\n    remove_columns=ds[\"train\"].column_names,\n    load_from_cache_file=False,\n    desc=\"Running tokenizer on dataset\",\n)\n```", "```py\nfrom torch.utils.data import DataLoader\nfrom transformers import default_data_collator\n\ntrain_ds = processed_ds[\"train\"]\neval_ds = processed_ds[\"test\"]\n\nbatch_size = 16\n\ntrain_dataloader = DataLoader(train_ds, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\neval_dataloader = DataLoader(eval_ds, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n```", "```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-560m\")\n```", "```py\nfrom peft import PromptEncoderConfig, get_peft_model\n\npeft_config = PromptEncoderConfig(task_type=\"CAUSAL_LM\", num_virtual_tokens=20, encoder_hidden_size=128)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\"trainable params: 300,288 || all params: 559,514,880 || trainable%: 0.05366935013417338\"\n```", "```py\nfrom transformers import get_linear_schedule_with_warmup\n\nlr = 3e-2\nnum_epochs = 50\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=(len(train_dataloader) * num_epochs),\n)\n```", "```py\nfrom tqdm import tqdm\n\ndevice = \"cuda\"\nmodel = model.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_loss += loss.detach().float()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n    model.eval()\n    eval_loss = 0\n    eval_preds = []\n    for step, batch in enumerate(tqdm(eval_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n        loss = outputs.loss\n        eval_loss += loss.detach().float()\n        eval_preds.extend(\n            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n        )\n\n    eval_epoch_loss = eval_loss / len(eval_dataloader)\n    eval_ppl = torch.exp(eval_epoch_loss)\n    train_epoch_loss = total_loss / len(train_dataloader)\n    train_ppl = torch.exp(train_epoch_loss)\n    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n```", "```py\nfrom huggingface_hub import notebook_login\n\naccount = <your-hf-account-name>\npeft_model_id = f\"{account}/bloomz-560-m-peft-method\"\nmodel.push_to_hub(peft_model_id)\n```", "```py\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\"peft_model_id\").to(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n\ni = 15\ninputs = tokenizer(f'{text_column} : {ds[\"test\"][i][\"Tweet text\"]} Label : ', return_tensors=\"pt\")\nprint(ds[\"test\"][i][\"Tweet text\"])\n\"@NYTsupport i have complained a dozen times &amp; yet my papers are still thrown FAR from my door. Why is this so hard to resolve?\"\n```", "```py\nwith torch.no_grad():\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\n\"['Tweet text : @NYTsupport i have complained a dozen times &amp; yet my papers are still thrown FAR from my door. Why is this so hard to resolve? Label : complaint']\"\n```"]