- en: Value-guided planning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»·å€¼å¼•å¯¼è§„åˆ’
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/value_guided_sampling](https://huggingface.co/docs/diffusers/api/pipelines/value_guided_sampling)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/api/pipelines/value_guided_sampling](https://huggingface.co/docs/diffusers/api/pipelines/value_guided_sampling)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ§ª This is an experimental pipeline for reinforcement learning!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ§ª è¿™æ˜¯ä¸€ä¸ªç”¨äºå¼ºåŒ–å­¦ä¹ çš„å®éªŒæ€§ç®¡é“ï¼
- en: This pipeline is based on the [Planning with Diffusion for Flexible Behavior
    Synthesis](https://huggingface.co/papers/2205.09991) paper by Michael Janner,
    Yilun Du, Joshua B. Tenenbaum, Sergey Levine.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç®¡é“åŸºäºMichael Jannerã€Yilun Duã€Joshua B. Tenenbaumã€Sergey Levineçš„[Planning with
    Diffusion for Flexible Behavior Synthesis](https://huggingface.co/papers/2205.09991)è®ºæ–‡ã€‚
- en: 'The abstract from the paper is:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è®ºæ–‡çš„æ‘˜è¦æ˜¯ï¼š
- en: '*Model-based reinforcement learning methods often use learning only for the
    purpose of estimating an approximate dynamics model, offloading the rest of the
    decision-making work to classical trajectory optimizers. While conceptually simple,
    this combination has a number of empirical shortcomings, suggesting that learned
    models may not be well-suited to standard trajectory optimization. In this paper,
    we consider what it would look like to fold as much of the trajectory optimization
    pipeline as possible into the modeling problem, such that sampling from the model
    and planning with it become nearly identical. The core of our technical approach
    lies in a diffusion probabilistic model that plans by iteratively denoising trajectories.
    We show how classifier-guided sampling and image inpainting can be reinterpreted
    as coherent planning strategies, explore the unusual and useful properties of
    diffusion-based planning methods, and demonstrate the effectiveness of our framework
    in control settings that emphasize long-horizon decision-making and test-time
    flexibility.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸ä»…ç”¨äºä¼°è®¡è¿‘ä¼¼åŠ¨æ€æ¨¡å‹çš„ç›®çš„ï¼Œå°†å†³ç­–å·¥ä½œçš„å…¶ä½™éƒ¨åˆ†è½¬ç§»åˆ°ç»å…¸è½¨è¿¹ä¼˜åŒ–å™¨ã€‚è™½ç„¶åœ¨æ¦‚å¿µä¸Šç®€å•ï¼Œä½†è¿™ç§ç»„åˆå­˜åœ¨è®¸å¤šç»éªŒä¸Šçš„ç¼ºé™·ï¼Œè¡¨æ˜å­¦ä¹ æ¨¡å‹å¯èƒ½ä¸é€‚åˆæ ‡å‡†è½¨è¿¹ä¼˜åŒ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘å°†å°½å¯èƒ½å¤šçš„è½¨è¿¹ä¼˜åŒ–ç®¡é“æŠ˜å åˆ°å»ºæ¨¡é—®é¢˜ä¸­ï¼Œä½¿å¾—ä»æ¨¡å‹ä¸­é‡‡æ ·å’Œè§„åˆ’å‡ ä¹ç›¸åŒã€‚æˆ‘ä»¬çš„æŠ€æœ¯æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºé€šè¿‡è¿­ä»£å»å™ªè½¨è¿¹è¿›è¡Œè§„åˆ’çš„æ‰©æ•£æ¦‚ç‡æ¨¡å‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†åˆ†ç±»å™¨å¼•å¯¼é‡‡æ ·å’Œå›¾åƒä¿®å¤å¦‚ä½•è¢«é‡æ–°è§£é‡Šä¸ºä¸€è‡´çš„è§„åˆ’ç­–ç•¥ï¼Œæ¢ç´¢äº†åŸºäºæ‰©æ•£çš„è§„åˆ’æ–¹æ³•çš„ä¸å¯»å¸¸å’Œæœ‰ç”¨çš„ç‰¹æ€§ï¼Œå¹¶å±•ç¤ºäº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨å¼ºè°ƒé•¿æœŸå†³ç­–å’Œæµ‹è¯•æ—¶é—´çµæ´»æ€§çš„æ§åˆ¶è®¾ç½®ä¸­çš„æœ‰æ•ˆæ€§ã€‚*'
- en: You can find additional information about the model on the [project page](https://diffusion-planning.github.io/),
    the [original codebase](https://github.com/jannerm/diffuser), or try it out in
    a demo [notebook](https://colab.research.google.com/drive/1rXm8CX4ZdN5qivjJ2lhwhkOmt_m0CvU0#scrollTo=6HXJvhyqcITc&uniqifier=1).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨[é¡¹ç›®é¡µé¢](https://diffusion-planning.github.io/)ã€[åŸå§‹ä»£ç åº“](https://github.com/jannerm/diffuser)æˆ–åœ¨æ¼”ç¤º[ç¬”è®°æœ¬](https://colab.research.google.com/drive/1rXm8CX4ZdN5qivjJ2lhwhkOmt_m0CvU0#scrollTo=6HXJvhyqcITc&uniqifier=1)ä¸­æ‰¾åˆ°æœ‰å…³è¯¥æ¨¡å‹çš„å…¶ä»–ä¿¡æ¯ã€‚
- en: The script to run the model is available [here](https://github.com/huggingface/diffusers/tree/main/examples/reinforcement_learning).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œæ¨¡å‹çš„è„šæœ¬åœ¨[æ­¤å¤„](https://github.com/huggingface/diffusers/tree/main/examples/reinforcement_learning)å¯ç”¨ã€‚
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·åŠ¡å¿…æŸ¥çœ‹è°ƒåº¦ç¨‹åº[æŒ‡å—](../../using-diffusers/schedulers)ä»¥äº†è§£å¦‚ä½•åœ¨è°ƒåº¦ç¨‹åºé€Ÿåº¦å’Œè´¨é‡ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œå¹¶æŸ¥çœ‹[è·¨ç®¡é“é‡ç”¨ç»„ä»¶](../../using-diffusers/loading#reuse-components-across-pipelines)éƒ¨åˆ†ï¼Œä»¥äº†è§£å¦‚ä½•æœ‰æ•ˆåœ°å°†ç›¸åŒç»„ä»¶åŠ è½½åˆ°å¤šä¸ªç®¡é“ä¸­ã€‚
- en: ValueGuidedRLPipeline
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ValueGuidedRLPipeline
- en: '### `class diffusers.experimental.ValueGuidedRLPipeline`'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.experimental.ValueGuidedRLPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/experimental/rl/value_guided_sampling.py#L25)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/experimental/rl/value_guided_sampling.py#L25)'
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`value_function` ([UNet1DModel](/docs/diffusers/v0.26.3/en/api/models/unet#diffusers.UNet1DModel))
    â€” A specialized UNet for fine-tuning trajectories base on reward.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value_function`ï¼ˆ[UNet1DModel](/docs/diffusers/v0.26.3/en/api/models/unet#diffusers.UNet1DModel)ï¼‰-
    ç”¨äºæ ¹æ®å¥–åŠ±å¾®è°ƒè½¨è¿¹çš„ä¸“é—¨UNetã€‚'
- en: '`unet` ([UNet1DModel](/docs/diffusers/v0.26.3/en/api/models/unet#diffusers.UNet1DModel))
    â€” UNet architecture to denoise the encoded trajectories.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet`ï¼ˆ[UNet1DModel](/docs/diffusers/v0.26.3/en/api/models/unet#diffusers.UNet1DModel)ï¼‰-
    ç”¨äºå»å™ªç¼–ç è½¨è¿¹çš„UNetæ¶æ„ã€‚'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    â€” A scheduler to be used in combination with `unet` to denoise the encoded trajectories.
    Default for this application is [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler`ï¼ˆ[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)ï¼‰-
    ä¸`unet`ç»“åˆä½¿ç”¨ä»¥å»å™ªç¼–ç è½¨è¿¹çš„è°ƒåº¦ç¨‹åºã€‚æ­¤åº”ç”¨çš„é»˜è®¤å€¼ä¸º[DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)ã€‚'
- en: '`env` () â€” An environment following the OpenAI gym API to act in. For now only
    Hopper has pretrained models.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`env`ï¼ˆï¼‰- éµå¾ªOpenAI gym APIä»¥è¡ŒåŠ¨çš„ç¯å¢ƒã€‚ç›®å‰åªæœ‰Hopperæœ‰é¢„è®­ç»ƒæ¨¡å‹ã€‚'
- en: Pipeline for value-guided sampling from a diffusion model trained to predict
    sequences of states.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è®­ç»ƒä»¥é¢„æµ‹çŠ¶æ€åºåˆ—çš„æ‰©æ•£æ¨¡å‹ä¸­è¿›è¡Œä»·å€¼å¼•å¯¼é‡‡æ ·çš„ç®¡é“ã€‚
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚
