- en: Value-guided planning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 价值引导规划
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/value_guided_sampling](https://huggingface.co/docs/diffusers/api/pipelines/value_guided_sampling)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/value_guided_sampling](https://huggingface.co/docs/diffusers/api/pipelines/value_guided_sampling)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 🧪 This is an experimental pipeline for reinforcement learning!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 🧪 这是一个用于强化学习的实验性管道！
- en: This pipeline is based on the [Planning with Diffusion for Flexible Behavior
    Synthesis](https://huggingface.co/papers/2205.09991) paper by Michael Janner,
    Yilun Du, Joshua B. Tenenbaum, Sergey Levine.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 该管道基于Michael Janner、Yilun Du、Joshua B. Tenenbaum、Sergey Levine的[Planning with
    Diffusion for Flexible Behavior Synthesis](https://huggingface.co/papers/2205.09991)论文。
- en: 'The abstract from the paper is:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要是：
- en: '*Model-based reinforcement learning methods often use learning only for the
    purpose of estimating an approximate dynamics model, offloading the rest of the
    decision-making work to classical trajectory optimizers. While conceptually simple,
    this combination has a number of empirical shortcomings, suggesting that learned
    models may not be well-suited to standard trajectory optimization. In this paper,
    we consider what it would look like to fold as much of the trajectory optimization
    pipeline as possible into the modeling problem, such that sampling from the model
    and planning with it become nearly identical. The core of our technical approach
    lies in a diffusion probabilistic model that plans by iteratively denoising trajectories.
    We show how classifier-guided sampling and image inpainting can be reinterpreted
    as coherent planning strategies, explore the unusual and useful properties of
    diffusion-based planning methods, and demonstrate the effectiveness of our framework
    in control settings that emphasize long-horizon decision-making and test-time
    flexibility.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于模型的强化学习方法通常仅用于估计近似动态模型的目的，将决策工作的其余部分转移到经典轨迹优化器。虽然在概念上简单，但这种组合存在许多经验上的缺陷，表明学习模型可能不适合标准轨迹优化。在本文中，我们考虑将尽可能多的轨迹优化管道折叠到建模问题中，使得从模型中采样和规划几乎相同。我们的技术方法的核心在于通过迭代去噪轨迹进行规划的扩散概率模型。我们展示了分类器引导采样和图像修复如何被重新解释为一致的规划策略，探索了基于扩散的规划方法的不寻常和有用的特性，并展示了我们的框架在强调长期决策和测试时间灵活性的控制设置中的有效性。*'
- en: You can find additional information about the model on the [project page](https://diffusion-planning.github.io/),
    the [original codebase](https://github.com/jannerm/diffuser), or try it out in
    a demo [notebook](https://colab.research.google.com/drive/1rXm8CX4ZdN5qivjJ2lhwhkOmt_m0CvU0#scrollTo=6HXJvhyqcITc&uniqifier=1).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[项目页面](https://diffusion-planning.github.io/)、[原始代码库](https://github.com/jannerm/diffuser)或在演示[笔记本](https://colab.research.google.com/drive/1rXm8CX4ZdN5qivjJ2lhwhkOmt_m0CvU0#scrollTo=6HXJvhyqcITc&uniqifier=1)中找到有关该模型的其他信息。
- en: The script to run the model is available [here](https://github.com/huggingface/diffusers/tree/main/examples/reinforcement_learning).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 运行模型的脚本在[此处](https://github.com/huggingface/diffusers/tree/main/examples/reinforcement_learning)可用。
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 请务必查看调度程序[指南](../../using-diffusers/schedulers)以了解如何在调度程序速度和质量之间进行权衡，并查看[跨管道重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分，以了解如何有效地将相同组件加载到多个管道中。
- en: ValueGuidedRLPipeline
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ValueGuidedRLPipeline
- en: '### `class diffusers.experimental.ValueGuidedRLPipeline`'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.experimental.ValueGuidedRLPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/experimental/rl/value_guided_sampling.py#L25)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/experimental/rl/value_guided_sampling.py#L25)'
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`value_function` ([UNet1DModel](/docs/diffusers/v0.26.3/en/api/models/unet#diffusers.UNet1DModel))
    — A specialized UNet for fine-tuning trajectories base on reward.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value_function`（[UNet1DModel](/docs/diffusers/v0.26.3/en/api/models/unet#diffusers.UNet1DModel)）-
    用于根据奖励微调轨迹的专门UNet。'
- en: '`unet` ([UNet1DModel](/docs/diffusers/v0.26.3/en/api/models/unet#diffusers.UNet1DModel))
    — UNet architecture to denoise the encoded trajectories.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet`（[UNet1DModel](/docs/diffusers/v0.26.3/en/api/models/unet#diffusers.UNet1DModel)）-
    用于去噪编码轨迹的UNet架构。'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded trajectories.
    Default for this application is [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)）-
    与`unet`结合使用以去噪编码轨迹的调度程序。此应用的默认值为[DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)。'
- en: '`env` () — An environment following the OpenAI gym API to act in. For now only
    Hopper has pretrained models.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`env`（）- 遵循OpenAI gym API以行动的环境。目前只有Hopper有预训练模型。'
- en: Pipeline for value-guided sampling from a diffusion model trained to predict
    sequences of states.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练以预测状态序列的扩散模型中进行价值引导采样的管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以了解为所有管道实现的通用方法（下载、保存、在特定设备上运行等）。
