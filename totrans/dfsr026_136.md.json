["```py\n>>> # !pip install opencv-python transformers accelerate\n>>> from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n>>> from diffusers.utils import load_image\n>>> import numpy as np\n>>> import torch\n\n>>> import cv2\n>>> from PIL import Image\n\n>>> # download an image\n>>> image = load_image(\n...     \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n... )\n>>> image = np.array(image)\n\n>>> # get canny image\n>>> image = cv2.Canny(image, 100, 200)\n>>> image = image[:, :, None]\n>>> image = np.concatenate([image, image, image], axis=2)\n>>> canny_image = Image.fromarray(image)\n\n>>> # load control net and stable diffusion v1-5\n>>> controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n>>> pipe = StableDiffusionControlNetPipeline.from_pretrained(\n...     \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n... )\n\n>>> # speed up diffusion process with faster scheduler and memory optimization\n>>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n>>> # remove following line if xformers is not installed\n>>> pipe.enable_xformers_memory_efficient_attention()\n\n>>> pipe.enable_model_cpu_offload()\n\n>>> # generate image\n>>> generator = torch.manual_seed(0)\n>>> image = pipe(\n...     \"futuristic-looking woman\", num_inference_steps=20, generator=generator, image=canny_image\n... ).images[0]\n```", "```py\n>>> import torch\n>>> from diffusers import StableDiffusionPipeline\n\n>>> pipe = StableDiffusionPipeline.from_pretrained(\n...     \"runwayml/stable-diffusion-v1-5\",\n...     torch_dtype=torch.float16,\n...     use_safetensors=True,\n... )\n\n>>> prompt = \"a photo of an astronaut riding a horse on mars\"\n>>> pipe.enable_attention_slicing()\n>>> image = pipe(prompt).images[0]\n```", "```py\n>>> import torch\n>>> from diffusers import DiffusionPipeline\n>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n\n>>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n>>> pipe = pipe.to(\"cuda\")\n>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n>>> # Workaround for not accepting attention shape using VAE for Flash Attention\n>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n```", "```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\npipe.load_textual_inversion(\"sd-concepts-library/cat-toy\")\n\nprompt = \"A <cat-toy> backpack\"\n\nimage = pipe(prompt, num_inference_steps=50).images[0]\nimage.save(\"cat-backpack.png\")\n```", "```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\npipe.load_textual_inversion(\"./charturnerv2.pt\", token=\"charturnerv2\")\n\nprompt = \"charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details.\"\n\nimage = pipe(prompt, num_inference_steps=50).images[0]\nimage.save(\"character.png\")\n```", "```py\n>>> # !pip install opencv-python transformers accelerate\n>>> from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultistepScheduler\n>>> from diffusers.utils import load_image\n>>> import numpy as np\n>>> import torch\n\n>>> import cv2\n>>> from PIL import Image\n\n>>> # download an image\n>>> image = load_image(\n...     \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n... )\n>>> np_image = np.array(image)\n\n>>> # get canny image\n>>> np_image = cv2.Canny(np_image, 100, 200)\n>>> np_image = np_image[:, :, None]\n>>> np_image = np.concatenate([np_image, np_image, np_image], axis=2)\n>>> canny_image = Image.fromarray(np_image)\n\n>>> # load control net and stable diffusion v1-5\n>>> controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n>>> pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n...     \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n... )\n\n>>> # speed up diffusion process with faster scheduler and memory optimization\n>>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n>>> pipe.enable_model_cpu_offload()\n\n>>> # generate image\n>>> generator = torch.manual_seed(0)\n>>> image = pipe(\n...     \"futuristic-looking woman\",\n...     num_inference_steps=20,\n...     generator=generator,\n...     image=image,\n...     control_image=canny_image,\n... ).images[0]\n```", "```py\n>>> import torch\n>>> from diffusers import StableDiffusionPipeline\n\n>>> pipe = StableDiffusionPipeline.from_pretrained(\n...     \"runwayml/stable-diffusion-v1-5\",\n...     torch_dtype=torch.float16,\n...     use_safetensors=True,\n... )\n\n>>> prompt = \"a photo of an astronaut riding a horse on mars\"\n>>> pipe.enable_attention_slicing()\n>>> image = pipe(prompt).images[0]\n```", "```py\n>>> import torch\n>>> from diffusers import DiffusionPipeline\n>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n\n>>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n>>> pipe = pipe.to(\"cuda\")\n>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n>>> # Workaround for not accepting attention shape using VAE for Flash Attention\n>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n```", "```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\npipe.load_textual_inversion(\"sd-concepts-library/cat-toy\")\n\nprompt = \"A <cat-toy> backpack\"\n\nimage = pipe(prompt, num_inference_steps=50).images[0]\nimage.save(\"cat-backpack.png\")\n```", "```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\npipe.load_textual_inversion(\"./charturnerv2.pt\", token=\"charturnerv2\")\n\nprompt = \"charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details.\"\n\nimage = pipe(prompt, num_inference_steps=50).images[0]\nimage.save(\"character.png\")\n```", "```py\n>>> # !pip install transformers accelerate\n>>> from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, DDIMScheduler\n>>> from diffusers.utils import load_image\n>>> import numpy as np\n>>> import torch\n\n>>> init_image = load_image(\n...     \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/boy.png\"\n... )\n>>> init_image = init_image.resize((512, 512))\n\n>>> generator = torch.Generator(device=\"cpu\").manual_seed(1)\n\n>>> mask_image = load_image(\n...     \"https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/boy_mask.png\"\n... )\n>>> mask_image = mask_image.resize((512, 512))\n\n>>> def make_canny_condition(image):\n...     image = np.array(image)\n...     image = cv2.Canny(image, 100, 200)\n...     image = image[:, :, None]\n...     image = np.concatenate([image, image, image], axis=2)\n...     image = Image.fromarray(image)\n...     return image\n\n>>> control_image = make_canny_condition(init_image)\n\n>>> controlnet = ControlNetModel.from_pretrained(\n...     \"lllyasviel/control_v11p_sd15_inpaint\", torch_dtype=torch.float16\n... )\n>>> pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n...     \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n... )\n\n>>> pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n>>> pipe.enable_model_cpu_offload()\n\n>>> # generate image\n>>> image = pipe(\n...     \"a handsome man with ray-ban sunglasses\",\n...     num_inference_steps=20,\n...     generator=generator,\n...     eta=1.0,\n...     image=init_image,\n...     mask_image=mask_image,\n...     control_image=control_image,\n... ).images[0]\n```", "```py\n>>> import torch\n>>> from diffusers import StableDiffusionPipeline\n\n>>> pipe = StableDiffusionPipeline.from_pretrained(\n...     \"runwayml/stable-diffusion-v1-5\",\n...     torch_dtype=torch.float16,\n...     use_safetensors=True,\n... )\n\n>>> prompt = \"a photo of an astronaut riding a horse on mars\"\n>>> pipe.enable_attention_slicing()\n>>> image = pipe(prompt).images[0]\n```", "```py\n>>> import torch\n>>> from diffusers import DiffusionPipeline\n>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n\n>>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n>>> pipe = pipe.to(\"cuda\")\n>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n>>> # Workaround for not accepting attention shape using VAE for Flash Attention\n>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n```", "```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\npipe.load_textual_inversion(\"sd-concepts-library/cat-toy\")\n\nprompt = \"A <cat-toy> backpack\"\n\nimage = pipe(prompt, num_inference_steps=50).images[0]\nimage.save(\"cat-backpack.png\")\n```", "```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\npipe.load_textual_inversion(\"./charturnerv2.pt\", token=\"charturnerv2\")\n\nprompt = \"charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details.\"\n\nimage = pipe(prompt, num_inference_steps=50).images[0]\nimage.save(\"character.png\")\n```", "```py\n>>> import jax\n>>> import numpy as np\n>>> import jax.numpy as jnp\n>>> from flax.jax_utils import replicate\n>>> from flax.training.common_utils import shard\n>>> from diffusers.utils import load_image, make_image_grid\n>>> from PIL import Image\n>>> from diffusers import FlaxStableDiffusionControlNetPipeline, FlaxControlNetModel\n\n>>> def create_key(seed=0):\n...     return jax.random.PRNGKey(seed)\n\n>>> rng = create_key(0)\n\n>>> # get canny image\n>>> canny_image = load_image(\n...     \"https://huggingface.co/datasets/YiYiXu/test-doc-assets/resolve/main/blog_post_cell_10_output_0.jpeg\"\n... )\n\n>>> prompts = \"best quality, extremely detailed\"\n>>> negative_prompts = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n\n>>> # load control net and stable diffusion v1-5\n>>> controlnet, controlnet_params = FlaxControlNetModel.from_pretrained(\n...     \"lllyasviel/sd-controlnet-canny\", from_pt=True, dtype=jnp.float32\n... )\n>>> pipe, params = FlaxStableDiffusionControlNetPipeline.from_pretrained(\n...     \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, revision=\"flax\", dtype=jnp.float32\n... )\n>>> params[\"controlnet\"] = controlnet_params\n\n>>> num_samples = jax.device_count()\n>>> rng = jax.random.split(rng, jax.device_count())\n\n>>> prompt_ids = pipe.prepare_text_inputs([prompts] * num_samples)\n>>> negative_prompt_ids = pipe.prepare_text_inputs([negative_prompts] * num_samples)\n>>> processed_image = pipe.prepare_image_inputs([canny_image] * num_samples)\n\n>>> p_params = replicate(params)\n>>> prompt_ids = shard(prompt_ids)\n>>> negative_prompt_ids = shard(negative_prompt_ids)\n>>> processed_image = shard(processed_image)\n\n>>> output = pipe(\n...     prompt_ids=prompt_ids,\n...     image=processed_image,\n...     params=p_params,\n...     prng_seed=rng,\n...     num_inference_steps=50,\n...     neg_prompt_ids=negative_prompt_ids,\n...     jit=True,\n... ).images\n\n>>> output_images = pipe.numpy_to_pil(np.asarray(output.reshape((num_samples,) + output.shape[-3:])))\n>>> output_images = make_image_grid(output_images, num_samples // 4, 4)\n>>> output_images.save(\"generated_image.png\")\n```"]