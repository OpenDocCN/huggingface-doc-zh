["```py\n>>> import evaluate\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n\n>>> dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"validation\")\n>>> model = (\n...     LongT5ForConditionalGeneration.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n...     .to(\"cuda\")\n...     .half()\n... )\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n\n>>> def generate_answers(batch):\n...     inputs_dict = tokenizer(\n...         batch[\"article\"], max_length=16384, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n...     )\n...     input_ids = inputs_dict.input_ids.to(\"cuda\")\n...     attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n...     output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=512, num_beams=2)\n...     batch[\"predicted_abstract\"] = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n...     return batch\n\n>>> result = dataset.map(generate_answer, batched=True, batch_size=2)\n>>> rouge = evaluate.load(\"rouge\")\n>>> rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"])\n```", "```py\n( vocab_size = 32128 d_model = 512 d_kv = 64 d_ff = 2048 num_layers = 6 num_decoder_layers = None num_heads = 8 local_radius = 127 global_block_size = 16 relative_attention_num_buckets = 32 relative_attention_max_distance = 128 dropout_rate = 0.1 layer_norm_epsilon = 1e-06 initializer_factor = 1.0 feed_forward_proj = 'relu' is_encoder_decoder = True encoder_attention_type = 'local' use_cache = True pad_token_id = 0 eos_token_id = 1 **kwargs )\n```", "```py\n( config: LongT5Config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LongT5Model\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-local-base\")\n>>> model = LongT5Model.from_pretrained(\"google/long-t5-local-base\")\n\n>>> # Let's try a very long encoder input.\n>>> input_ids = tokenizer(\n...     100 * \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n\n>>> # forward pass\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: LongT5Config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n>>> model = LongT5ForConditionalGeneration.from_pretrained(\n...     \"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\"\n... )\n\n>>> # Let's try a very long input.\n>>> inputs = tokenizer(100 * \"studies have shown that owning a dog is good for you \", return_tensors=\"pt\")\n>>> input_ids = inputs.input_ids\n\n>>> outputs = model.generate(input_ids)\n>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\nabstractthe aim of this article is to provide an overview of the literature on the role of dog\n```", "```py\n( config: LongT5Config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-local-base\")\n>>> model = LongT5EncoderModel.from_pretrained(\"google/long-t5-local-base\")\n>>> input_ids = tokenizer(\n...     100 * \"Studies have been shown that owning a dog is good for you \", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> outputs = model(input_ids=input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: LongT5Config input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None decoder_input_ids: Array = None decoder_attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxLongT5Model\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = FlaxLongT5Model.from_pretrained(\"google/long-t5-local-base\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"np\"\n... ).input_ids\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"np\").input_ids\n\n>>> # forward pass\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( input_ids: Array attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxLongT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = FlaxLongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n( decoder_input_ids encoder_outputs encoder_attention_mask: Optional = None decoder_attention_mask: Optional = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxLongT5ForConditionalGeneration\n>>> import jax.numpy as jnp\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = FlaxLongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> logits = outputs.logits\n```", "```py\n( config: LongT5Config input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None decoder_input_ids: Array = None decoder_attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxLongT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = FlaxLongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n\n>>> ARTICLE_TO_SUMMARIZE = \"summarize: My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], return_tensors=\"np\")\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(inputs[\"input_ids\"]).sequences\n>>> print(tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False))\n```", "```py\n( input_ids: Array attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxLongT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = FlaxLongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n( decoder_input_ids encoder_outputs encoder_attention_mask: Optional = None decoder_attention_mask: Optional = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxLongT5ForConditionalGeneration\n>>> import jax.numpy as jnp\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = FlaxLongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n\n>>> text = \"summarize: My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> logits = outputs.logits\n```"]