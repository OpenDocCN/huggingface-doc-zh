["```py\n>>> import evaluate\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n\n>>> dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"validation\")\n>>> model = (\n...     LongT5ForConditionalGeneration.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n...     .to(\"cuda\")\n...     .half()\n... )\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n\n>>> def generate_answers(batch):\n...     inputs_dict = tokenizer(\n...         batch[\"article\"], max_length=16384, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n...     )\n...     input_ids = inputs_dict.input_ids.to(\"cuda\")\n...     attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n...     output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=512, num_beams=2)\n...     batch[\"predicted_abstract\"] = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n...     return batch\n\n>>> result = dataset.map(generate_answer, batched=True, batch_size=2)\n>>> rouge = evaluate.load(\"rouge\")\n>>> rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"])\n```", "```py\n>>> from transformers import AutoTokenizer, LongT5Model\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-local-base\")\n>>> model = LongT5Model.from_pretrained(\"google/long-t5-local-base\")\n\n>>> # Let's try a very long encoder input.\n>>> input_ids = tokenizer(\n...     100 * \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n\n>>> # forward pass\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n>>> model = LongT5ForConditionalGeneration.from_pretrained(\n...     \"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\"\n... )\n\n>>> # Let's try a very long input.\n>>> inputs = tokenizer(100 * \"studies have shown that owning a dog is good for you \", return_tensors=\"pt\")\n>>> input_ids = inputs.input_ids\n\n>>> outputs = model.generate(input_ids)\n>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\nabstractthe aim of this article is to provide an overview of the literature on the role of dog\n```", "```py\n>>> from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-local-base\")\n>>> model = LongT5EncoderModel.from_pretrained(\"google/long-t5-local-base\")\n>>> input_ids = tokenizer(\n...     100 * \"Studies have been shown that owning a dog is good for you \", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> outputs = model(input_ids=input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxLongT5Model\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = FlaxLongT5Model.from_pretrained(\"google/long-t5-local-base\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"np\"\n... ).input_ids\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"np\").input_ids\n\n>>> # forward pass\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxLongT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = FlaxLongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxLongT5ForConditionalGeneration\n>>> import jax.numpy as jnp\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = FlaxLongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxLongT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = FlaxLongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n\n>>> ARTICLE_TO_SUMMARIZE = \"summarize: My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], return_tensors=\"np\")\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(inputs[\"input_ids\"]).sequences\n>>> print(tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False))\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxLongT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = FlaxLongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxLongT5ForConditionalGeneration\n>>> import jax.numpy as jnp\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n>>> model = FlaxLongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n\n>>> text = \"summarize: My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> logits = outputs.logits\n```"]