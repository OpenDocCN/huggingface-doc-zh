# （可选）深度强化学习中的好奇心是什么？

> 原始文本：[https://huggingface.co/learn/deep-rl-course/unit5/curiosity](https://huggingface.co/learn/deep-rl-course/unit5/curiosity)

这是关于好奇心的（可选）介绍。如果您想了解更多信息，可以阅读两篇更深入探讨数学细节的文章：

+   [通过下一个状态预测进行好奇心驱动学习](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa)

+   [随机网络蒸馏：对好奇心驱动学习的新看法](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938)

## 现代强化学习中的两个主要问题

要理解好奇心是什么，我们首先需要了解强化学习中的两个主要问题：

首先是“稀疏奖励问题”：即大多数奖励不包含信息，因此被设为零。

请记住，强化学习基于“奖励假设”，即每个目标都可以描述为奖励的最大化。因此，奖励作为强化学习代理的反馈；如果它们没有收到任何奖励，它们对哪种行为是适当的（或不适当）的知识就无法改变。

![好奇心](../Images/542512c1c81fdf970fe9cb37254df345.png)

来源：感谢奖励，我们的代理知道这个状态下的这个动作是好的

例如，在[Vizdoom](https://vizdoom.cs.put.edu.pl/)中，基于游戏Doom“DoomMyWayHome”的一组环境中，只有在代理找到护甲时才会受到奖励。然而，护甲离您的起点很远，因此大部分奖励将为零。因此，如果我们的代理没有收到有用的反馈（密集奖励），它将需要更长时间才能学习到最佳策略，并且可能会花费时间在找不到目标的情况下转圈。

![好奇心](../Images/e3753d61bd4ca091ba22d9ba1b36cd3f.png)

第二个大问题是外部奖励函数是手工制作的；在每个环境中，人类必须实现一个奖励函数。但是我们如何在大型和复杂的环境中扩展它呢？

## 那么好奇心是什么？

解决这些问题的方法是开发一个内在于代理的奖励函数，即由代理自己生成。代理将充当自学习者，因为它将是学生和自己的反馈大师。

这种内在奖励机制被称为好奇心，因为这种奖励推动代理探索新颖/陌生的状态。为了实现这一点，我们的代理将在探索新轨迹时获得高奖励。

这种奖励受到人类行为的启发。我们自然地有探索环境和发现新事物的内在欲望。

有不同的方法来计算这种内在奖励。经典方法（通过下一个状态预测的好奇心）是计算好奇心为我们的代理在给定当前状态和采取的行动的情况下预测下一个状态的错误。

![好奇心](../Images/6b030efa6065b6f29760b37ae29749d3.png)

因为好奇心的想法是鼓励我们的代理执行减少其行为后果预测不确定性的行动（在代理花费较少时间或在具有复杂动态的区域中，不确定性会更高）。

如果代理在这些状态上花费了很多时间，它将擅长预测下一个状态（低好奇心）。另一方面，如果它处于一个新的、未探索的状态，它将很难预测接下来的状态（高好奇心）。

![好奇心](../Images/513912902e6673ccaf455689bc8325ab.png)

使用好奇心将推动我们的代理倾向于具有高预测误差的转换（这将在代理在那些花费较少时间的区域或具有复杂动态的区域中更高），因此更好地探索我们的环境。

还有其他的好奇心计算方法。ML-Agents使用一种更先进的方法，称为通过随机网络蒸馏实现好奇心。这超出了本教程的范围，但如果你感兴趣，我写了一篇详细解释的文章。
