- en: AudioLDM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AudioLDM
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/audioldm](https://huggingface.co/docs/diffusers/api/pipelines/audioldm)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/audioldm](https://huggingface.co/docs/diffusers/api/pipelines/audioldm)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'AudioLDM was proposed in [AudioLDM: Text-to-Audio Generation with Latent Diffusion
    Models](https://huggingface.co/papers/2301.12503) by Haohe Liu et al. Inspired
    by [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview),
    AudioLDM is a text-to-audio *latent diffusion model (LDM)* that learns continuous
    audio representations from [CLAP](https://huggingface.co/docs/transformers/main/model_doc/clap)
    latents. AudioLDM takes a text prompt as input and predicts the corresponding
    audio. It can generate text-conditional sound effects, human speech and music.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 'Haohe Liu等人在[AudioLDM: Text-to-Audio Generation with Latent Diffusion Models](https://huggingface.co/papers/2301.12503)中提出了AudioLDM。受到[Stable
    Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview)的启发，AudioLDM是一个文本到音频的*潜在扩散模型（LDM）*，从[CLAP](https://huggingface.co/docs/transformers/main/model_doc/clap)潜在中学习连续音频表示。AudioLDM以文本提示作为输入，并预测相应的音频。它可以生成文本条件的音效、人类语音和音乐。'
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要是：
- en: '*Text-to-audio (TTA) system has recently gained attention for its ability to
    synthesize general audio based on text descriptions. However, previous studies
    in TTA have limited generation quality with high computational costs. In this
    study, we propose AudioLDM, a TTA system that is built on a latent space to learn
    the continuous audio representations from contrastive language-audio pretraining
    (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio
    embedding while providing text embedding as a condition during sampling. By learning
    the latent representations of audio signals and their compositions without modeling
    the cross-modal relationship, AudioLDM is advantageous in both generation quality
    and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM
    achieves state-of-the-art TTA performance measured by both objective and subjective
    metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that
    enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot
    fashion. Our implementation and demos are available at [this https URL](https://audioldm.github.io/).*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*最近，文本到音频（TTA）系统因其根据文本描述合成一般音频的能力而受到关注。然而，以往的TTA研究在高计算成本下限制了生成质量。在本研究中，我们提出了AudioLDM，这是一个建立在潜在空间上的TTA系统，从对比语言-音频预训练（CLAP）潜在中学习连续音频表示。预训练的CLAP模型使我们能够训练具有音频嵌入的LDM，并在采样过程中提供文本嵌入作为条件。通过学习音频信号及其组成的潜在表示，而不对跨模态关系进行建模，AudioLDM在生成质量和计算效率方面具有优势。在单个GPU上训练的AudioLDM在客观和主观指标（例如，frechet距离）方面实现了最先进的TTA性能。此外，AudioLDM是第一个以零样式进行各种文本引导音频操作（例如，风格转移）的TTA系统。我们的实现和演示可在[此https
    URL](https://audioldm.github.io/)上找到。*'
- en: The original codebase can be found at [haoheliu/AudioLDM](https://github.com/haoheliu/AudioLDM).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 原始代码库可在[haoheliu/AudioLDM](https://github.com/haoheliu/AudioLDM)找到。
- en: Tips
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示
- en: 'When constructing a prompt, keep in mind:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建提示时，请记住：
- en: Descriptive prompt inputs work best; you can use adjectives to describe the
    sound (for example, “high quality” or “clear”) and make the prompt context specific
    (for example, “water stream in a forest” instead of “stream”).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述性提示输入效果最佳；您可以使用形容词描述声音（例如，“高质量”或“清晰”），并使提示具体化（例如，“森林中的水流”而不是“流水”）。
- en: It’s best to use general terms like “cat” or “dog” instead of specific names
    or abstract objects the model may not be familiar with.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最好使用一般术语，如“猫”或“狗”，而不是特定名称或模型可能不熟悉的抽象对象。
- en: 'During inference:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 推理过程中：
- en: The *quality* of the predicted audio sample can be controlled by the `num_inference_steps`
    argument; higher steps give higher quality audio at the expense of slower inference.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测音频样本的*质量*可以通过`num_inference_steps`参数进行控制；更高的步骤会提供更高质量的音频，但推理速度会变慢。
- en: The *length* of the predicted audio sample can be controlled by varying the
    `audio_length_in_s` argument.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测音频样本的*长度*可以通过变化`audio_length_in_s`参数进行控制。
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 确保查看调度器[指南](../../using-diffusers/schedulers)以了解如何探索调度器速度和质量之间的权衡，并查看[跨管道重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分，以了解如何有效地将相同组件加载到多个管道中。
- en: AudioLDMPipeline
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AudioLDMPipeline
- en: '### `class diffusers.AudioLDMPipeline`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.AudioLDMPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm/pipeline_audioldm.py#L52)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm/pipeline_audioldm.py#L52)'
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) model to encode and decode images to and from
    latent representations.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — 变分自动编码器（VAE）模型，用于对图像进行编码和解码，并转换为潜在表示。'
- en: '`text_encoder` ([ClapTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModelWithProjection))
    — Frozen text-encoder (`ClapTextModelWithProjection`, specifically the [laion/clap-htsat-unfused](https://huggingface.co/laion/clap-htsat-unfused)
    variant.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` ([ClapTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModelWithProjection))
    — 冻结的文本编码器（`ClapTextModelWithProjection`，具体是[laion/clap-htsat-unfused](https://huggingface.co/laion/clap-htsat-unfused)变体。'
- en: '`tokenizer` (`PreTrainedTokenizer`) — A [RobertaTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)
    to tokenize text.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`PreTrainedTokenizer`) — 用于对文本进行标记化的[RobertaTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)。'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — A `UNet2DConditionModel` to denoise the encoded audio latents.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — 用于去噪编码音频潜变量的 `UNet2DConditionModel`。'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded audio
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — 用于与 `unet` 结合使用以去噪编码音频潜变量的调度程序。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。'
- en: '`vocoder` ([SpeechT5HifiGan](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5HifiGan))
    — Vocoder of class `SpeechT5HifiGan`.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocoder` ([SpeechT5HifiGan](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5HifiGan))
    — 类 `SpeechT5HifiGan` 的声码器。'
- en: Pipeline for text-to-audio generation using AudioLDM.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 用于使用 AudioLDM 进行文本到音频生成的管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。
- en: '#### `__call__`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm/pipeline_audioldm.py#L367)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm/pipeline_audioldm.py#L367)'
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    audio generation. If not defined, you need to pass `prompt_embeds`.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` or `List[str]`, *optional*) — 用于引导音频生成的提示或提示。如果未定义，则需要传递 `prompt_embeds`。'
- en: '`audio_length_in_s` (`int`, *optional*, defaults to 5.12) — The length of the
    generated audio sample in seconds.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`audio_length_in_s` (`int`, *optional*, defaults to 5.12) — 生成音频样本的长度（秒）。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 10) — The number of denoising
    steps. More denoising steps usually lead to a higher quality audio at the expense
    of slower inference.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *optional*, defaults to 10) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的音频，但会降低推理速度。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 2.5) — A higher guidance
    scale value encourages the model to generate audio that is closely linked to the
    text `prompt` at the expense of lower sound quality. Guidance scale is enabled
    when `guidance_scale > 1`.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *optional*, defaults to 2.5) — 更高的引导比例值鼓励模型生成与文本
    `prompt` 密切相关的音频，但会降低声音质量。当 `guidance_scale > 1` 时启用引导比例。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in audio generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` or `List[str]`, *optional*) — 用于指导音频生成中不包括什么的提示或提示。如果未定义，则需要传递
    `negative_prompt_embeds`。在不使用引导时（`guidance_scale < 1`）会被忽略。'
- en: '`num_waveforms_per_prompt` (`int`, *optional*, defaults to 1) — The number
    of waveforms to generate per prompt.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_waveforms_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的波形数量。'
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, *optional*, defaults to 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数
    eta (η)。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中被忽略。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for image generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中采样的预生成嘈杂潜变量，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机
    `generator` 进行采样生成潜变量张量。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，文本嵌入将从
    `prompt` 输入参数生成。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds`
    将从 `negative_prompt` 输入参数生成。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)
    instead of a plain tuple.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回[AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)而不是普通元组。'
- en: '`callback` (`Callable`, *optional*) — A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *可选*) — 一个在推断过程中每`callback_steps`步调用一次的函数。该函数接受以下参数：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *可选*, 默认为1) — 调用`callback`函数的频率。如果未指定，则在每个步骤中调用回调。'
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *可选*) — 一个kwargs字典，如果指定，则传递给`AttentionProcessor`，如[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的那样。'
- en: '`output_type` (`str`, *optional*, defaults to `"np"`) — The output format of
    the generated image. Choose between `"np"` to return a NumPy `np.ndarray` or `"pt"`
    to return a PyTorch `torch.Tensor` object.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *可选*, 默认为`"np"`) — 生成图像的输出格式。选择在`"np"`之间返回一个NumPy `np.ndarray`或`"pt"`返回一个PyTorch
    `torch.Tensor`对象。'
- en: Returns
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)
    or `tuple`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)
    或 `tuple`'
- en: If `return_dict` is `True`, [AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated audio.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`return_dict`为`True`，则返回[AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)，否则返回一个`tuple`，其中第一个元素是生成的音频的列表。
- en: The call function to the pipeline for generation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: 'Examples:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `disable_vae_slicing`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm/pipeline_audioldm.py#L108)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm/pipeline_audioldm.py#L108)'
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片VAE解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到在一个步骤中计算解码。
- en: '#### `enable_vae_slicing`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`enable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm/pipeline_audioldm.py#L100)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm/pipeline_audioldm.py#L100)'
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片VAE解码。当启用此选项时，VAE将分割输入张量以在几个步骤中计算解码。这对于节省一些内存并允许更大的批量大小非常有用。
- en: AudioPipelineOutput
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AudioPipelineOutput
- en: '### `class diffusers.AudioPipelineOutput`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.AudioPipelineOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L130)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L130)'
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`audios` (`np.ndarray`) — List of denoised audio samples of a NumPy array of
    shape `(batch_size, num_channels, sample_rate)`.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`audios` (`np.ndarray`) — 一个形状为`(batch_size, num_channels, sample_rate)`的NumPy数组的去噪音频样本列表。'
- en: Output class for audio pipelines.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 音频管道的输出类。
