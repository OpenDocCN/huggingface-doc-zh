- en: Custom Diffusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/training/custom_diffusion](https://huggingface.co/docs/diffusers/training/custom_diffusion)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: '[Custom Diffusion](https://huggingface.co/papers/2212.04488) is a training
    technique for personalizing image generation models. Like Textual Inversion, DreamBooth,
    and LoRA, Custom Diffusion only requires a few (~4-5) example images. This technique
    works by only training weights in the cross-attention layers, and it uses a special
    word to represent the newly learned concept. Custom Diffusion is unique because
    it can also learn multiple concepts at the same time.'
  prefs: []
  type: TYPE_NORMAL
- en: If youâ€™re training on a GPU with limited vRAM, you should try enabling xFormers
    with `--enable_xformers_memory_efficient_attention` for faster training with lower
    vRAM requirements (16GB). To save even more memory, add `--set_grads_to_none`
    in the training argument to set the gradients to `None` instead of zero (this
    option can cause some issues, so if you experience any, try removing this parameter).
  prefs: []
  type: TYPE_NORMAL
- en: This guide will explore the [train_custom_diffusion.py](https://github.com/huggingface/diffusers/blob/main/examples/custom_diffusion/train_custom_diffusion.py)
    script to help you become more familiar with it, and how you can adapt it for
    your own use-case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before running the script, make sure you install the library from source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Navigate to the example folder with the training script and install the required
    dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ðŸ¤— Accelerate is a library for helping you train on multiple GPUs/TPUs or with
    mixed-precision. Itâ€™ll automatically configure your training setup based on your
    hardware and environment. Take a look at the ðŸ¤— Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour)
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize an ðŸ¤— Accelerate environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To setup a default ðŸ¤— Accelerate environment without choosing any configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Or if your environment doesnâ€™t support an interactive shell, like a notebook,
    you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, if you want to train a model on your own dataset, take a look at the
    [Create a dataset for training](create_dataset) guide to learn how to create a
    dataset that works with the training script.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections highlight parts of the training script that are important
    for understanding how to modify it, but it doesnâ€™t cover every aspect of the script
    in detail. If youâ€™re interested in learning more, feel free to read through the
    [script](https://github.com/huggingface/diffusers/blob/main/examples/custom_diffusion/train_custom_diffusion.py)
    and let us know if you have any questions or concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Script parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training script contains all the parameters to help you customize your training
    run. These are found in the [`parse_args()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/custom_diffusion/train_custom_diffusion.py#L319)
    function. The function comes with default values, but you can also set your own
    values in the training command if youâ€™d like.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to change the resolution of the input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Many of the basic parameters are described in the [DreamBooth](dreambooth#script-parameters)
    training guide, so this guide focuses on the parameters unique to Custom Diffusion:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--freeze_model`: freezes the key and value parameters in the cross-attention
    layer; the default is `crossattn_kv`, but you can set it to `crossattn` to train
    all the parameters in the cross-attention layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--concepts_list`: to learn multiple concepts, provide a path to a JSON file
    containing the concepts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--modifier_token`: a special word used to represent the learned concept'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--initializer_token`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prior preservation loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prior preservation loss is a method that uses a modelâ€™s own generated samples
    to help it learn how to generate more diverse images. Because these generated
    sample images belong to the same class as the images you provided, they help the
    model retain what it has learned about the class and how it can use what it already
    knows about the class to make new compositions.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the parameters for prior preservation loss are described in the [DreamBooth](dreambooth#prior-preservation-loss)
    training guide.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Custom Diffusion includes training the target images with a small set of real
    images to prevent overfitting. As you can imagine, this can be easy to do when
    youâ€™re only training on a few images! Download 200 real images with `clip_retrieval`.
    The `class_prompt` should be the same category as the target images. These images
    are stored in `class_data_dir`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To enable regularization, add the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--with_prior_preservation`: whether to use prior preservation loss'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--prior_loss_weight`: controls the influence of the prior preservation loss
    on the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--real_prior`: whether to use a small set of real images to prevent overfitting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Training script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A lot of the code in the Custom Diffusion training script is similar to the
    [DreamBooth](dreambooth#training-script) script. This guide instead focuses on
    the code that is relevant to Custom Diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Custom Diffusion training script has two dataset classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[`CustomDiffusionDataset`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/custom_diffusion/train_custom_diffusion.py#L165):
    preprocesses the images, class images, and prompts for training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`PromptDataset`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/custom_diffusion/train_custom_diffusion.py#L148):
    prepares the prompts for generating class images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, the `modifier_token` is [added to the tokenizer](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/custom_diffusion/train_custom_diffusion.py#L811),
    converted to token ids, and the token embeddings are resized to account for the
    new `modifier_token`. Then the `modifier_token` embeddings are initialized with
    the embeddings of the `initializer_token`. All parameters in the text encoder
    are frozen, except for the token embeddings since this is what the model is trying
    to learn to associate with the concepts.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now youâ€™ll need to add the [Custom Diffusion weights](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/custom_diffusion/train_custom_diffusion.py#L911C3-L911C3)
    to the attention layers. This is a really important step for getting the shape
    and size of the attention weights correct, and for setting the appropriate number
    of attention processors in each UNet block.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The [optimizer](https://github.com/huggingface/diffusers/blob/84cd9e8d01adb47f046b1ee449fc76a0c32dc4e2/examples/custom_diffusion/train_custom_diffusion.py#L982)
    is initialized to update the cross-attention layer parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the [training loop](https://github.com/huggingface/diffusers/blob/84cd9e8d01adb47f046b1ee449fc76a0c32dc4e2/examples/custom_diffusion/train_custom_diffusion.py#L1048),
    it is important to only update the embeddings for the concept youâ€™re trying to
    learn. This means setting the gradients of all the other token embeddings to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Launch the script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once youâ€™ve made all your changes or youâ€™re okay with the default configuration,
    youâ€™re ready to launch the training script! ðŸš€
  prefs: []
  type: TYPE_NORMAL
- en: In this guide, youâ€™ll download and use these example [cat images](https://www.cs.cmu.edu/~custom-diffusion/assets/data.zip).
    You can also create and use your own dataset if you want (see the [Create a dataset
    for training](create_dataset) guide).
  prefs: []
  type: TYPE_NORMAL
- en: Set the environment variable `MODEL_NAME` to a model id on the Hub or a path
    to a local model, `INSTANCE_DIR` to the path where you just downloaded the cat
    images to, and `OUTPUT_DIR` to where you want to save the model. Youâ€™ll use `<new1>`
    as the special word to tie the newly learned embeddings to. The script creates
    and saves model checkpoints and a pytorch_custom_diffusion_weights.bin file to
    your repository.
  prefs: []
  type: TYPE_NORMAL
- en: To monitor training progress with Weights and Biases, add the `--report_to=wandb`
    parameter to the training command and specify a validation prompt with `--validation_prompt`.
    This is useful for debugging and saving intermediate results.
  prefs: []
  type: TYPE_NORMAL
- en: 'If youâ€™re training on human faces, the Custom Diffusion team has found the
    following parameters to work well:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--learning_rate=5e-6`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--max_train_steps` can be anywhere between 1000 and 2000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--freeze_model=crossattn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use at least 15-20 images to train with
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: single conceptmultiple concepts
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Once training is finished, you can use your new Custom Diffusion model for inference.
  prefs: []
  type: TYPE_NORMAL
- en: single conceptmultiple concepts
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Next steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Congratulations on training a model with Custom Diffusion! ðŸŽ‰ To learn more:'
  prefs: []
  type: TYPE_NORMAL
- en: Read the [Multi-Concept Customization of Text-to-Image Diffusion](https://www.cs.cmu.edu/~custom-diffusion/)
    blog post to learn more details about the experimental results from the Custom
    Diffusion team.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
