["```py\n( vocab_size = 250112 d_model = 512 d_kv = 64 d_ff = 1024 num_layers = 8 num_decoder_layers = None num_heads = 6 relative_attention_num_buckets = 32 relative_attention_max_distance = 128 dropout_rate = 0.1 layer_norm_epsilon = 1e-06 initializer_factor = 1.0 feed_forward_proj = 'gated-gelu' is_encoder_decoder = True use_cache = True tokenizer_class = 'T5Tokenizer' tie_word_embeddings = False pad_token_id = 0 eos_token_id = 1 decoder_start_token_id = 0 classifier_dropout = 0.0 **kwargs )\n```", "```py\n( vocab_file eos_token = '</s>' unk_token = '<unk>' pad_token = '<pad>' extra_ids = 100 additional_special_tokens = None sp_model_kwargs: Optional = None legacy = None **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( tokens )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( text: TextInput add_special_tokens = False **kwargs )\n```", "```py\n( vocab_file = None tokenizer_file = None eos_token = '</s>' unk_token = '<unk>' pad_token = '<pad>' extra_ids = 100 additional_special_tokens = None **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( config: MT5Config )\n```", "```py\n>>> from transformers import MT5Model, AutoTokenizer\n\n>>> model = MT5Model.from_pretrained(\"google/mt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n>>> article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n>>> summary = \"Weiter Verhandlung in Syrien.\"\n>>> inputs = tokenizer(article, return_tensors=\"pt\")\n>>> labels = tokenizer(text_target=summary, return_tensors=\"pt\")\n\n>>> outputs = model(input_ids=inputs[\"input_ids\"], decoder_input_ids=labels[\"input_ids\"])\n>>> hidden_states = outputs.last_hidden_state\n```", "```py\n( )\n```", "```py\n# On a 4 GPU machine with mt5-xl:\nmodel = MT5ForConditionalGeneration.from_pretrained(\"Mt5-xl\")\ndevice_map = {\n    0: [0, 1, 2],\n    1: [3, 4, 5, 6, 7, 8, 9],\n    2: [10, 11, 12, 13, 14, 15, 16],\n    3: [17, 18, 19, 20, 21, 22, 23],\n}\nmodel.parallelize(device_map)  # Splits the model across several devices\nmodel.deparallelize()  # Put the model back on cpu and cleans memory by calling torch.cuda.empty_cache()\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, MT5Model\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mt5-small\")\n>>> model = MT5Model.from_pretrained(\"mt5-small\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n\n>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for MT5Model.\n>>> # This is not needed for torch's MT5ForConditionalGeneration as it does this internally using labels arg.\n>>> decoder_input_ids = model._shift_right(decoder_input_ids)\n\n>>> # forward pass\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( device_map = None )\n```", "```py\n# Here is an example of a device map on a machine with 4 GPUs using mt5-xl, which has a total of 24 attention modules:\nmodel = MT5ForConditionalGeneration.from_pretrained(\"mt5-xl\")\ndevice_map = {\n    0: [0, 1, 2],\n    1: [3, 4, 5, 6, 7, 8, 9],\n    2: [10, 11, 12, 13, 14, 15, 16],\n    3: [17, 18, 19, 20, 21, 22, 23],\n}\nmodel.parallelize(device_map)\n```", "```py\n( config: MT5Config )\n```", "```py\n>>> from transformers import MT5ForConditionalGeneration, AutoTokenizer\n\n>>> model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n>>> article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n>>> summary = \"Weiter Verhandlung in Syrien.\"\n>>> inputs = tokenizer(article, text_target=summary, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> loss = outputs.loss\n```", "```py\n( )\n```", "```py\n# On a 4 GPU machine with mt5-xl:\nmodel = MT5ForConditionalGeneration.from_pretrained(\"Mt5-xl\")\ndevice_map = {\n    0: [0, 1, 2],\n    1: [3, 4, 5, 6, 7, 8, 9],\n    2: [10, 11, 12, 13, 14, 15, 16],\n    3: [17, 18, 19, 20, 21, 22, 23],\n}\nmodel.parallelize(device_map)  # Splits the model across several devices\nmodel.deparallelize()  # Put the model back on cpu and cleans memory by calling torch.cuda.empty_cache()\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, MT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mt5-small\")\n>>> model = MT5ForConditionalGeneration.from_pretrained(\"mt5-small\")\n\n>>> # training\n>>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n>>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n>>> outputs = model(input_ids=input_ids, labels=labels)\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n\n>>> # inference\n>>> input_ids = tokenizer(\n...     \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> outputs = model.generate(input_ids)\n>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n>>> # studies have shown that owning a dog is good for you.\n```", "```py\n( device_map = None )\n```", "```py\n# Here is an example of a device map on a machine with 4 GPUs using mt5-xl, which has a total of 24 attention modules:\nmodel = MT5ForConditionalGeneration.from_pretrained(\"mt5-xl\")\ndevice_map = {\n    0: [0, 1, 2],\n    1: [3, 4, 5, 6, 7, 8, 9],\n    2: [10, 11, 12, 13, 14, 15, 16],\n    3: [17, 18, 19, 20, 21, 22, 23],\n}\nmodel.parallelize(device_map)\n```", "```py\n( config: MT5Config )\n```", "```py\n>>> from transformers import MT5EncoderModel, AutoTokenizer\n\n>>> model = MT5EncoderModel.from_pretrained(\"google/mt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n>>> article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n>>> input_ids = tokenizer(article, return_tensors=\"pt\").input_ids\n>>> outputs = model(input_ids)\n>>> hidden_state = outputs.last_hidden_state\n```", "```py\n( )\n```", "```py\n# On a 4 GPU machine with mt5-xl:\nmodel = MT5ForConditionalGeneration.from_pretrained(\"Mt5-xl\")\ndevice_map = {\n    0: [0, 1, 2],\n    1: [3, 4, 5, 6, 7, 8, 9],\n    2: [10, 11, 12, 13, 14, 15, 16],\n    3: [17, 18, 19, 20, 21, 22, 23],\n}\nmodel.parallelize(device_map)  # Splits the model across several devices\nmodel.deparallelize()  # Put the model back on cpu and cleans memory by calling torch.cuda.empty_cache()\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, MT5EncoderModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mt5-small\")\n>>> model = MT5EncoderModel.from_pretrained(\"mt5-small\")\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> outputs = model(input_ids=input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( device_map = None )\n```", "```py\n# Here is an example of a device map on a machine with 4 GPUs using mt5-xl, which has a total of 24 attention modules:\nmodel = MT5ForConditionalGeneration.from_pretrained(\"mt5-xl\")\ndevice_map = {\n    0: [0, 1, 2],\n    1: [3, 4, 5, 6, 7, 8, 9],\n    2: [10, 11, 12, 13, 14, 15, 16],\n    3: [17, 18, 19, 20, 21, 22, 23],\n}\nmodel.parallelize(device_map)\n```", "```py\n( config: MT5Config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n( config: MT5Config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None start_positions: Optional = None end_positions: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```", "```py\n( *args **kwargs )\n```", "```py\n>>> from transformers import TFMT5Model, AutoTokenizer\n\n>>> model = TFMT5Model.from_pretrained(\"google/mt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n>>> article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n>>> summary = \"Weiter Verhandlung in Syrien.\"\n>>> inputs = tokenizer(article, return_tensors=\"tf\")\n>>> labels = tokenizer(text_target=summary, return_tensors=\"tf\")\n\n>>> outputs = model(input_ids=inputs[\"input_ids\"], decoder_input_ids=labels[\"input_ids\"])\n>>> hidden_states = outputs.last_hidden_state\n```", "```py\n( *args **kwargs )\n```", "```py\n>>> from transformers import TFMT5ForConditionalGeneration, AutoTokenizer\n\n>>> model = TFMT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n>>> article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n>>> summary = \"Weiter Verhandlung in Syrien.\"\n>>> inputs = tokenizer(article, text_target=summary, return_tensors=\"tf\")\n\n>>> outputs = model(**inputs)\n>>> loss = outputs.loss\n```", "```py\n( *args **kwargs )\n```", "```py\n>>> from transformers import TFMT5EncoderModel, AutoTokenizer\n\n>>> model = TFMT5EncoderModel.from_pretrained(\"google/mt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n>>> article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n>>> input_ids = tokenizer(article, return_tensors=\"tf\").input_ids\n>>> outputs = model(input_ids)\n>>> hidden_state = outputs.last_hidden_state\n```", "```py\n( config: T5Config input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )\n```", "```py\n>>> from transformers import FlaxMT5Model, AutoTokenizer\n\n>>> model = FlaxMT5Model.from_pretrained(\"google/mt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n\n>>> article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n>>> summary = \"Weiter Verhandlung in Syrien.\"\n>>> inputs = tokenizer(article, return_tensors=\"np\")\n\n>>> decoder_input_ids = tokenizer(text_target=summary, return_tensors=\"np\").input_ids\n\n>>> outputs = model(input_ids=inputs[\"input_ids\"], decoder_input_ids=decoder_input_ids)\n>>> hidden_states = outputs.last_hidden_state\n```", "```py\n( config: T5Config input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )\n```", "```py\n>>> from transformers import FlaxMT5ForConditionalGeneration, AutoTokenizer\n\n>>> model = FlaxMT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n\n>>> article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n>>> summary = \"Weiter Verhandlung in Syrien.\"\n>>> inputs = tokenizer(article, return_tensors=\"np\")\n\n>>> decoder_input_ids = tokenizer(text_target=summary, return_tensors=\"np\").input_ids\n\n>>> outputs = model(**inputs, decoder_input_ids=decoder_input_ids)\n>>> logits = outputs.logits\n```", "```py\n( config: T5Config input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )\n```", "```py\n>>> from transformers import FlaxT5EncoderModel, AutoTokenizer\n\n>>> model = FlaxT5EncoderModel.from_pretrained(\"google/mt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n\n>>> article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n>>> summary = \"Weiter Verhandlung in Syrien.\"\n>>> inputs = tokenizer(article, return_tensors=\"np\")\n\n>>> decoder_input_ids = tokenizer(text_target=summary, return_tensors=\"np\").input_ids\n\n>>> outputs = model(input_ids=inputs[\"input_ids\"])\n>>> hidden_states = outputs.last_hidden_state\n```"]