# Whisper

> åŸæ–‡ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/whisper`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/whisper)

## æ¦‚è¿°

Whisper æ¨¡å‹ç”± Alec Radfordã€Jong Wook Kimã€Tao Xuã€Greg Brockmanã€Christine McLeaveyã€Ilya Sutskever åœ¨[é€šè¿‡å¤§è§„æ¨¡å¼±ç›‘ç£å®ç°ç¨³å¥è¯­éŸ³è¯†åˆ«](https://cdn.openai.com/papers/whisper.pdf)ä¸­æå‡ºã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*æˆ‘ä»¬ç ”ç©¶äº†ç®€å•è®­ç»ƒä»¥é¢„æµ‹äº’è”ç½‘ä¸Šå¤§é‡éŸ³é¢‘è½¬å½•çš„è¯­éŸ³å¤„ç†ç³»ç»Ÿçš„èƒ½åŠ›ã€‚å½“æ‰©å±•åˆ° 680,000 å°æ—¶çš„å¤šè¯­è¨€å’Œå¤šä»»åŠ¡ç›‘ç£æ—¶ï¼Œå¾—åˆ°çš„æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå¹¶ä¸”é€šå¸¸ä¸å…ˆå‰çš„å®Œå…¨ç›‘ç£ç»“æœç«äº‰ï¼Œä½†åœ¨é›¶æ¬¡è¿ç§»è®¾ç½®ä¸­æ— éœ€ä»»ä½•å¾®è°ƒã€‚ä¸äººç±»ç›¸æ¯”ï¼Œæ¨¡å‹æ¥è¿‘å…¶å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬å‘å¸ƒäº†æ¨¡å‹å’Œæ¨ç†ä»£ç ï¼Œä»¥ä½œä¸ºè¿›ä¸€æ­¥ç ”ç©¶ç¨³å¥è¯­éŸ³å¤„ç†çš„åŸºç¡€ã€‚*

æ­¤æ¨¡å‹ç”±[Arthur Zucker](https://huggingface.co/ArthurZ)è´¡çŒ®ã€‚æ­¤æ¨¡å‹çš„ Tensorflow ç‰ˆæœ¬ç”±[amyeroberts](https://huggingface.co/amyeroberts)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯åœ¨[æ­¤å¤„](https://github.com/openai/whisper)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   è¯¥æ¨¡å‹é€šå¸¸æ— éœ€ä»»ä½•å¾®è°ƒå³å¯è¡¨ç°è‰¯å¥½ã€‚

+   è¯¥æ¶æ„éµå¾ªç»å…¸çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œè¿™æ„å‘³ç€å®ƒä¾èµ–äº generate()å‡½æ•°è¿›è¡Œæ¨ç†ã€‚

+   ç›®å‰ä»…å®ç°äº†çŸ­å½¢å¼çš„æ¨ç†ï¼Œå³éŸ³é¢‘è¢«é¢„åˆ†æ®µä¸º<=30 ç§’çš„ç‰‡æ®µã€‚é•¿å½¢å¼ï¼ˆåŒ…æ‹¬æ—¶é—´æˆ³ï¼‰å°†åœ¨æœªæ¥çš„ç‰ˆæœ¬ä¸­å®ç°ã€‚

+   å¯ä»¥ä½¿ç”¨ WhisperProcessor æ¥å‡†å¤‡éŸ³é¢‘ä»¥ä¾›æ¨¡å‹ä½¿ç”¨ï¼Œå¹¶å°†é¢„æµ‹çš„ ID è§£ç å›æ–‡æœ¬ã€‚

+   è¦è½¬æ¢æ¨¡å‹å’Œå¤„ç†å™¨ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ï¼š

```py
python src/transformers/models/whisper/convert_openai_to_hf.py --checkpoint_path "" --pytorch_dump_folder_path "Arthur/whisper-3" --convert_preprocessor True
```

è„šæœ¬å°†è‡ªåŠ¨ä» OpenAI æ£€æŸ¥ç‚¹ç¡®å®šæ‰€æœ‰å¿…è¦çš„å‚æ•°ã€‚éœ€è¦å®‰è£…`tiktoken`åº“ä»¥æ‰§è¡Œå°† OpenAI åˆ†è¯å™¨è½¬æ¢ä¸º`tokenizers`ç‰ˆæœ¬çš„è½¬æ¢ã€‚

## æ¨ç†

ä»¥ä¸‹æ˜¯ä½¿ç”¨é¢„è®­ç»ƒçš„ Whisper æ¨¡å‹è½¬å½•éŸ³é¢‘æ ·æœ¬çš„é€æ­¥æŒ‡å—ï¼š

```py
>>> from datasets import load_dataset
>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration

>>> # Select an audio file and read it:
>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> audio_sample = ds[0]["audio"]
>>> waveform = audio_sample["array"]
>>> sampling_rate = audio_sample["sampling_rate"]

>>> # Load the Whisper model in Hugging Face format:
>>> processor = WhisperProcessor.from_pretrained("openai/whisper-tiny.en")
>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-tiny.en")

>>> # Use the model and processor to transcribe the audio:
>>> input_features = processor(
...     waveform, sampling_rate=sampling_rate, return_tensors="pt"
... ).input_features

>>> # Generate token ids
>>> predicted_ids = model.generate(input_features)

>>> # Decode token ids to text
>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)

>>> transcription[0]
' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'
```

## èµ„æº

å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ Whisperã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æäº¤æ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æ ¸ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

+   ä¸€ä¸ªåŒ…å«è„šæœ¬çš„åˆ†æ”¯ï¼Œç”¨äº[å°† Hugging Face æ ¼å¼çš„ Whisper æ¨¡å‹è½¬æ¢ä¸º OpenAI æ ¼å¼](https://github.com/zuazo-forks/transformers/blob/convert_hf_to_openai/src/transformers/models/whisper/convert_hf_to_openai.py)ã€‚ğŸŒ ä½¿ç”¨ç¤ºä¾‹ï¼š

```py
pip install -U openai-whisper
python convert_hf_to_openai.py \
    --checkpoint openai/whisper-tiny \
    --whisper_dump_path whisper-tiny-openai.pt
```

## WhisperConfig

### `class transformers.WhisperConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/configuration_whisper.py#L62)

```py
( vocab_size = 51865 num_mel_bins = 80 encoder_layers = 4 encoder_attention_heads = 6 decoder_layers = 4 decoder_attention_heads = 6 decoder_ffn_dim = 1536 encoder_ffn_dim = 1536 encoder_layerdrop = 0.0 decoder_layerdrop = 0.0 decoder_start_token_id = 50257 use_cache = True is_encoder_decoder = True activation_function = 'gelu' d_model = 384 dropout = 0.0 attention_dropout = 0.0 activation_dropout = 0.0 init_std = 0.02 scale_embedding = False max_source_positions = 1500 max_target_positions = 448 pad_token_id = 50256 bos_token_id = 50256 eos_token_id = 50256 suppress_tokens = None begin_suppress_tokens = [220, 50256] use_weighted_layer_sum = False classifier_proj_size = 256 apply_spec_augment = False mask_time_prob = 0.05 mask_time_length = 10 mask_time_min_masks = 2 mask_feature_prob = 0.0 mask_feature_length = 10 mask_feature_min_masks = 0 median_filter_width = 7 **kwargs )
```

å‚æ•°

+   `vocab_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 51865ï¼‰â€” Whisper æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨ WhisperModel æ—¶å¯ä»¥ç”±`decoder_input_ids`è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚

+   `num_mel_bins`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 80ï¼‰â€” æ¯ä¸ªè¾“å…¥ç‰¹å¾ä¸­ä½¿ç”¨çš„ mel ç‰¹å¾æ•°é‡ã€‚åº”ä¸`WhisperProcessor`ç±»ä¸­ä½¿ç”¨çš„å€¼å¯¹åº”ã€‚

+   `encoder_layers`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 4ï¼‰â€” ç¼–ç å™¨å±‚æ•°ã€‚

+   `decoder_layers`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 4ï¼‰â€” è§£ç å™¨å±‚æ•°ã€‚

+   `encoder_attention_heads`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 6ï¼‰â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `decoder_attention_heads` (`int`, *optional*, é»˜è®¤ä¸º 6) â€” Transformer è§£ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `encoder_ffn_dim` (`int`, *optional*, é»˜è®¤ä¸º 1536) â€” ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `decoder_ffn_dim` (`int`, *optional*, é»˜è®¤ä¸º 1536) â€” è§£ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `encoder_layerdrop` (`float`, *optional*, é»˜è®¤ä¸º 0.0) â€” ç¼–ç å™¨çš„ LayerDrop æ¦‚ç‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… LayerDrop paper)ã€‚

+   `decoder_layerdrop` (`float`, *optional*, é»˜è®¤ä¸º 0.0) â€” è§£ç å™¨çš„ LayerDrop æ¦‚ç‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… LayerDrop paper)ã€‚

+   `decoder_start_token_id`ï¼ˆ`int`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º 50257ï¼‰--å¯¹åº”äº`â€œ<|startoftranscript|>â€`æ ‡è®°ï¼Œå½“æ²¡æœ‰å‘`generate`å‡½æ•°æä¾›`decoder_input_ids`æ—¶ï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨è¯¥æ ‡è®°ã€‚å®ƒç”¨äºæ ¹æ®ä»»åŠ¡æŒ‡å¯¼æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ã€‚

+   `use_cache` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚

+   `is_encoder_decoder` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ¨¡å‹æ˜¯å¦ç”¨ä½œç¼–ç å™¨/è§£ç å™¨ã€‚

+   `activation_function` (`str`, *optional*, é»˜è®¤ä¸º`"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"silu"`å’Œ`"gelu_new"`ã€‚

+   `d_model` (`int`, *optional*, é»˜è®¤ä¸º 384) â€” å±‚çš„ç»´åº¦ã€‚

+   `dropout` (`float`, *optional*, é»˜è®¤ä¸º 0.1) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å±‚ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚

+   `attention_dropout` (`float`, *optional*, é»˜è®¤ä¸º 0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¼ƒæ¯”ç‡ã€‚

+   `activation_dropout` (`float`, *optional*, é»˜è®¤ä¸º 0.0) â€” å…¨è¿æ¥å±‚å†…éƒ¨æ¿€æ´»çš„ä¸¢å¼ƒæ¯”ç‡ã€‚

+   `init_std` (`float`, *optional*, é»˜è®¤ä¸º 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `scale_embedding` (`bool`, *optional*, é»˜è®¤ä¸º False) â€” é€šè¿‡é™¤ä»¥ sqrt(d_model)æ¥ç¼©æ”¾åµŒå…¥ã€‚

+   `max_source_positions` (`int`, *optional*, é»˜è®¤ä¸º 1500) â€” è¯¥æ¨¡å‹å¯èƒ½ç”¨äºçš„å¯¹æ•°æ¢…å°”æ»¤æ³¢å™¨ç»„ç‰¹å¾çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚

+   `max_target_positions` (`int`, *optional*, é»˜è®¤ä¸º 448) â€” è¯¥æ¨¡å‹å¯èƒ½ç”¨äºçš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸å°†å…¶è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚ 512ã€1024 æˆ– 2048ï¼‰ã€‚

+   `pad_token_id` (`int`, *optional*, é»˜è®¤ä¸º 50256) â€” å¡«å……æ ‡è®° idã€‚

+   `bos_token_id` (`int`, *optional*, é»˜è®¤ä¸º 50256) â€” æµçš„å¼€å§‹æ ‡è®° idã€‚

+   `eos_token_id` (`int`, *optional*, é»˜è®¤ä¸º 50256) â€” æµçš„ç»“æŸæ ‡è®° idã€‚

+   `suppress_tokens` (`List[int]`, *optional*) â€” åŒ…å«å°†åœ¨`generate`å‡½æ•°ä¸­ç”±å¯¹æ•°å¤„ç†å™¨ä½¿ç”¨çš„éè¯­éŸ³æ ‡è®°çš„åˆ—è¡¨ã€‚NON_SPEECH_TOKENS å’Œ NON_SPEECH_TOKENS_MULTI åˆ†åˆ«å¯¹åº”äº`english-only`å’Œ`multilingual`æ¨¡å‹ã€‚

+   `begin_suppress_tokens` (`List[int]`, *optional*, é»˜è®¤ä¸º`[220,50256]`) â€” åŒ…å«åœ¨é‡‡æ ·è¿‡ç¨‹å¼€å§‹æ—¶å°†è¢«æŠ‘åˆ¶çš„æ ‡è®°çš„åˆ—è¡¨ã€‚åˆå§‹åŒ–ä¸º`" "`ï¼ˆ`blank_token_id`ï¼‰å’Œ`eos_token_id`çš„æ ‡è®°ã€‚

+   `use_weighted_layer_sum` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦ä½¿ç”¨å¸¦æœ‰å­¦ä¹ æƒé‡çš„å±‚è¾“å‡ºçš„åŠ æƒå¹³å‡å€¼ã€‚ä»…åœ¨ä½¿ç”¨ WhisperForAudioClassification çš„å®ä¾‹æ—¶ç›¸å…³ã€‚

+   `classifier_proj_size` (`int`, *optional*, é»˜è®¤ä¸º 256) â€” åˆ†ç±»å‰çš„æŠ•å½±ç»´åº¦ï¼Œç”¨äºæ ‡è®°å‡å€¼æ± åŒ–ã€‚ä»…åœ¨ä½¿ç”¨ WhisperForAudioClassification çš„å®ä¾‹æ—¶ç›¸å…³ã€‚

+   `apply_spec_augment` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦å°†*SpecAugment*æ•°æ®å¢å¼ºåº”ç”¨äºç‰¹å¾ç¼–ç å™¨çš„è¾“å‡ºã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779)ã€‚

+   `mask_time_prob` (`float`, *optional*, defaults to 0.05) â€” æ²¿æ—¶é—´è½´çš„æ‰€æœ‰ç‰¹å¾å‘é‡çš„ç™¾åˆ†æ¯”ï¼ˆä»‹äº 0 å’Œ 1 ä¹‹é—´ï¼‰å°†è¢«æ©ç›–ã€‚æ©ç›–è¿‡ç¨‹åœ¨è½´ä¸Šç”Ÿæˆ`mask_time_prob*len(time_axis)/mask_time_length`ä¸ªç‹¬ç«‹çš„æ©ç ã€‚å¦‚æœä»æ¯ä¸ªç‰¹å¾å‘é‡è¢«é€‰æ‹©ä¸ºè¦æ©ç›–çš„å‘é‡è·¨åº¦çš„èµ·å§‹çš„æ¦‚ç‡æ¨ç†ï¼Œ*mask_time_prob*åº”ä¸º`prob_vector_start*mask_time_length`ã€‚è¯·æ³¨æ„ï¼Œé‡å å¯èƒ½ä¼šé™ä½æ©ç›–å‘é‡çš„å®é™…ç™¾åˆ†æ¯”ã€‚åªæœ‰åœ¨`apply_spec_augment == True`æ—¶æ‰ç›¸å…³ã€‚

+   `mask_time_length` (`int`, *optional*, defaults to 10) â€” æ²¿æ—¶é—´è½´çš„å‘é‡è·¨åº¦é•¿åº¦ã€‚

+   `mask_time_min_masks` (`int`, *optional*, defaults to 2), â€” æ²¿æ—¶é—´è½´ç”Ÿæˆçš„é•¿åº¦ä¸º`mask_feature_length`çš„æ©ç çš„æœ€å°æ•°é‡ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ï¼Œä¸`mask_feature_prob`æ— å…³ã€‚åªæœ‰åœ¨â€mask_time_prob*len(time_axis)/mask_time_length < mask_time_min_masksâ€æ—¶æ‰ç›¸å…³ã€‚

+   `mask_feature_prob` (`float`, *optional*, defaults to 0.0) â€” æ²¿ç‰¹å¾è½´çš„æ‰€æœ‰ç‰¹å¾å‘é‡çš„ç™¾åˆ†æ¯”ï¼ˆä»‹äº 0 å’Œ 1 ä¹‹é—´ï¼‰å°†è¢«æ©ç›–ã€‚æ©ç›–è¿‡ç¨‹åœ¨è½´ä¸Šç”Ÿæˆ`mask_feature_prob*len(feature_axis)/mask_time_length`ä¸ªç‹¬ç«‹çš„æ©ç ã€‚å¦‚æœä»æ¯ä¸ªç‰¹å¾å‘é‡è¢«é€‰æ‹©ä¸ºè¦æ©ç›–çš„å‘é‡è·¨åº¦çš„èµ·å§‹çš„æ¦‚ç‡æ¨ç†ï¼Œ*mask_feature_prob*åº”ä¸º`prob_vector_start*mask_feature_length`ã€‚è¯·æ³¨æ„ï¼Œé‡å å¯èƒ½ä¼šé™ä½æ©ç›–å‘é‡çš„å®é™…ç™¾åˆ†æ¯”ã€‚åªæœ‰åœ¨`apply_spec_augment`ä¸º True æ—¶æ‰ç›¸å…³ã€‚

+   `mask_feature_length` (`int`, *optional*, defaults to 10) â€” æ²¿ç‰¹å¾è½´çš„å‘é‡è·¨åº¦é•¿åº¦ã€‚

+   `mask_feature_min_masks` (`int`, *optional*, defaults to 0), â€” æ²¿ç‰¹å¾è½´ç”Ÿæˆçš„é•¿åº¦ä¸º`mask_feature_length`çš„æ©ç çš„æœ€å°æ•°é‡ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ï¼Œä¸`mask_feature_prob`æ— å…³ã€‚åªæœ‰åœ¨`mask_feature_prob*len(feature_axis)/mask_feature_length < mask_feature_min_masks`æ—¶æ‰ç›¸å…³ã€‚

+   `median_filter_width` (`int`, *optional*, defaults to 7) â€” ç”¨äºåœ¨è®¡ç®—æ ‡è®°æ—¶é—´æˆ³æ—¶å¹³æ»‘äº¤å‰æ³¨æ„åŠ›è¾“å‡ºçš„ä¸­å€¼æ»¤æ³¢å™¨çš„å®½åº¦ã€‚åº”ä¸ºå¥‡æ•°ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ WhisperModel é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ– Whisper æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº Whisper [openai/whisper-tiny](https://huggingface.co/openai/whisper-tiny)æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import WhisperConfig, WhisperModel

>>> # Initializing a Whisper tiny style configuration
>>> configuration = WhisperConfig()

>>> # Initializing a model (with random weights) from the tiny style configuration
>>> model = WhisperModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## WhisperTokenizer

### `class transformers.WhisperTokenizer`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L217)

```py
( vocab_file merges_file normalizer_file = None errors = 'replace' unk_token = '<|endoftext|>' bos_token = '<|endoftext|>' eos_token = '<|endoftext|>' pad_token = None add_prefix_space = False language = None task = None predict_timestamps = False **kwargs )
```

å‚æ•°

+   `vocab_file` (`str`) â€” è¯æ±‡æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `merges_file` (`str`) â€” åˆå¹¶æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `normalizer_file` (`str`, *optional*) â€” æ­£åˆ™åŒ–å™¨æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `errors`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º``"replace"``ï¼‰--å°†å­—èŠ‚è§£ç ä¸º UTF-8 æ—¶è¦éµå¾ªçš„ç¤ºä¾‹ã€‚è¯·å‚é˜…[bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)äº†è§£æ›´å¤šä¿¡æ¯ã€‚

+   `unk_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º``"<|endoftext|>"``ï¼‰--æœªçŸ¥ä»¤ç‰Œã€‚è¯æ±‡è¡¨ä¸­æ²¡æœ‰çš„ä»¤ç‰Œæ— æ³•è½¬æ¢ä¸º IDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºè¯¥ä»¤ç‰Œã€‚

+   `bos_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º``"<|endoftext|>"``ï¼‰--åºåˆ—æ ‡è®°çš„å¼€å¤´ã€‚`decoder_start_token_id`ç”¨äºåœ¨ç”Ÿæˆæ—¶å°†ç¬¬ä¸€ä¸ªä»¤ç‰Œè®¾ç½®ä¸º``"<|startoftranscript|>"``ã€‚

+   `eos_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º``"<|endoftext|>"``ï¼‰--åºåˆ—ç»“æŸæ ‡è®°ã€‚

+   `pad_token` (`str`, *optional*) â€” ç”¨äºå¡«å……çš„æ ‡è®°ï¼Œä¾‹å¦‚åœ¨æ‰¹å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—æ—¶ã€‚

+   `add_prefix_space` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨è¾“å…¥å‰æ·»åŠ ä¸€ä¸ªåˆå§‹ç©ºæ ¼ã€‚è¿™å…è®¸å°†å‰å¯¼å•è¯è§†ä¸ºä»»ä½•å…¶ä»–å•è¯ã€‚

+   `language`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰--è½¬å½•æ–‡æœ¬çš„è¯­è¨€ã€‚å¯¹äºå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘ä»»åŠ¡ï¼Œç›¸åº”çš„è¯­è¨€ id æ ‡è®°è¢«é™„åŠ åˆ°åºåˆ—çš„å¼€å¤´ï¼Œä¾‹å¦‚å¯¹äºè¥¿ç­ç‰™è¯­ï¼Œæ ‡è®°``"<|es|>"``è¢«é™„åŠ åˆ°é¡ºåºçš„å¼€å¤´ã€‚è¿™åªèƒ½ç”¨äºå¤šè¯­è¨€å¾®è°ƒã€‚

+   `task`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰--è¦é™„åŠ åœ¨åºåˆ—å¼€å¤´çš„ä»»åŠ¡æ ‡è¯†ç¬¦ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚è¿™åº”ç”¨äºå¤šè¯­è¨€å¾®è°ƒï¼Œ`"transcribe"`ç”¨äºè¯­éŸ³è¯†åˆ«ï¼Œ`"translate"`ç”¨äºè¯­éŸ³ç¿»è¯‘ã€‚

+   predict_timestamps`ï¼ˆ`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`False`ï¼‰--æ˜¯å¦çœç•¥åºåˆ—å¼€å¤´çš„`<|notimestamps|>`æ ‡è®°ã€‚

æ„å»ºä¸€ä¸ª Whisper tokenizerã€‚

è¿™ä¸ªåˆ†è¯å™¨ç»§æ‰¿è‡ª PreTrainedTokenizerï¼Œå…¶ä¸­åŒ…å«ä¸€äº›ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒè¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

#### `set_prefix_tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L389)

```py
( language: str = None task: str = None predict_timestamps: bool = None )
```

å‚æ•°

+   `language`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `None`ï¼‰â€” è½¬å½•æ–‡æœ¬çš„è¯­è¨€ã€‚

+   `task`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `None`ï¼‰â€” ä»»åŠ¡æ ‡è¯†ç¬¦ï¼Œé™„åŠ åœ¨åºåˆ—å¼€å¤´ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚

+   `predict_timestamps`ï¼ˆ`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`None`ï¼‰--æ˜¯å¦çœç•¥åºåˆ—å¼€å¤´çš„`<|notimestamps|>`æ ‡è®°ã€‚

è¦†ç›–é™„åŠ åˆ°æ ‡ç­¾åºåˆ—å¼€å¤´çš„å‰ç¼€æ ‡è®°ã€‚æ­¤æ–¹æ³•å¯ä»¥ç‹¬ç«‹ä½¿ç”¨ä»¥

åœ¨å¾®è°ƒæ—¶æ ¹æ®éœ€è¦æ›´æ–°å‰ç¼€æ ‡è®°ã€‚ç¤ºä¾‹ï¼š

```py
>>> # instantiate the tokenizer and set the prefix token to Spanish
>>> tokenizer = WhisperTokenizer.from_pretrained("openai/whisper-tiny", language="spanish")
>>> # now switch the prefix token from Spanish to French
>>> tokenizer.set_prefix_tokens(language="french")
```

#### `build_inputs_with_special_tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L448)

```py
( token_ids_0 token_ids_1 = None )
```

é€šè¿‡é™„åŠ  eos_token_id ä»åºåˆ—æ„å»ºæ¨¡å‹è¾“å…¥ã€‚

#### `get_special_tokens_mask`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L456)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” ID åˆ—è¡¨ã€‚

+   `token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” åºåˆ—å¯¹çš„å¯é€‰ç¬¬äºŒä¸ª ID åˆ—è¡¨ã€‚

+   `already_has_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`ï¼‰â€” æ ‡è®°åˆ—è¡¨æ˜¯å¦å·²ç»æ ¼å¼åŒ–ä¸ºæ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°ã€‚

è¿”å›

`List[int]`

ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼ŒèŒƒå›´ä¸º [0, 1]ï¼š1 è¡¨ç¤ºç‰¹æ®Šæ ‡è®°ï¼Œ0 è¡¨ç¤ºåºåˆ—æ ‡è®°ã€‚

ä»æ²¡æœ‰æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„æ ‡è®°åˆ—è¡¨ä¸­æ£€ç´¢åºåˆ— IDã€‚åœ¨ä½¿ç”¨åˆ†è¯å™¨çš„ `prepare_for_model` æ–¹æ³•æ·»åŠ ç‰¹æ®Šæ ‡è®°æ—¶è°ƒç”¨æ­¤æ–¹æ³•ã€‚

#### `create_token_type_ids_from_sequences`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” ç¬¬ä¸€ä¸ªæ ‡è®°åŒ–åºåˆ—ã€‚

+   `token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” ç¬¬äºŒä¸ªæ ‡è®°åŒ–åºåˆ—ã€‚

è¿”å›

`List[int]`

æ ‡è®°ç±»å‹ IDã€‚

åˆ›å»ºä¸ä¼ é€’çš„åºåˆ—å¯¹åº”çš„æ ‡è®°ç±»å‹ IDã€‚ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

å¦‚æœæ¨¡å‹æœ‰ä¸€ç§ç‰¹æ®Šçš„æ„å»ºæ–¹å¼ï¼Œåˆ™åº”åœ¨å­ç±»ä¸­é‡å†™å®ƒã€‚

#### `save_vocabulary`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L763)

```py
( save_directory: str filename_prefix: Optional = None )
```

#### `batch_decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)

```py
( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) â†’ export const metadata = 'undefined';List[str]
```

å‚æ•°

+   `sequences`ï¼ˆ`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`ï¼‰â€” æ ‡è®°åŒ–è¾“å…¥ ID çš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨ `__call__` æ–¹æ³•è·å–ã€‚

+   `skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç æ—¶åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚

+   `clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦æ¸…ç†åˆ†è¯ç©ºæ ¼ã€‚å¦‚æœä¸º `None`ï¼Œå°†é»˜è®¤ä¸º `self.clean_up_tokenization_spaces`ã€‚

+   `kwargs`ï¼ˆé¢å¤–çš„å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰â€” å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚

è¿”å›

`List[str]`

è§£ç çš„å¥å­åˆ—è¡¨ã€‚

é€šè¿‡è°ƒç”¨è§£ç å°†æ ‡è®° ID çš„åˆ—è¡¨åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ã€‚

#### `decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L639)

```py
( token_ids skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None output_offsets: bool = False time_precision: float = 0.02 decode_with_timestamps: bool = False normalize: bool = False basic_normalize: bool = False remove_diacritics: bool = False **kwargs ) â†’ export const metadata = 'undefined';str
```

å‚æ•°

+   `token_ids`ï¼ˆ`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`ï¼‰â€” æ ‡è®°åŒ–è¾“å…¥ ID çš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨ `__call__` æ–¹æ³•è·å–ã€‚

+   `skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç æ—¶åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚

+   `clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦æ¸…ç†åˆ†è¯ç©ºæ ¼ã€‚å¦‚æœä¸º `None`ï¼Œå°†é»˜è®¤ä¸º `self.clean_up_tokenization_spaces`ï¼ˆåœ¨ `tokenizer_config` ä¸­å¯ç”¨ï¼‰ã€‚

+   `output_offsets` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¾“å‡ºæ ‡è®°çš„åç§»é‡ã€‚åªæœ‰åœ¨æ¨¡å‹é¢„æµ‹æ—¶é—´æˆ³æ—¶æ‰åº”è®¾ç½®æ­¤é€‰é¡¹ã€‚

+   `time_precision` (`float`, `optional`, é»˜è®¤ä¸º 0.02) â€” ä»æ ‡è®°è½¬æ¢ä¸ºæ—¶é—´çš„æ—¶é—´æ¯”ç‡ã€‚

+   `decode_with_timestamps` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨åŸå§‹æ–‡æœ¬ä¸­åŒ…å«æ—¶é—´æˆ³è¿›è¡Œè§£ç ã€‚

+   `normalize` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦å¯¹è§£ç åçš„æ–‡æœ¬åº”ç”¨è‹±æ–‡æ–‡æœ¬è§„èŒƒåŒ–ã€‚ä»…å½“ç›®æ ‡æ–‡æœ¬ä¸ºè‹±æ–‡æ—¶é€‚ç”¨ã€‚å¦åˆ™ï¼Œåº”åº”ç”¨åŸºæœ¬æ–‡æœ¬è§„èŒƒåŒ–ã€‚

+   `basic_normalize` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦å¯¹è§£ç åçš„æ–‡æœ¬åº”ç”¨åŸºæœ¬æ–‡æœ¬è§„èŒƒåŒ–ã€‚é€‚ç”¨äºå¤šè¯­è¨€ç›®æ ‡æ–‡æœ¬ã€‚

+   `remove_diacritics` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” åœ¨åº”ç”¨åŸºæœ¬æ–‡æœ¬è§„èŒƒåŒ–æ—¶æ˜¯å¦åˆ é™¤å˜éŸ³ç¬¦å·ã€‚åˆ é™¤å˜éŸ³ç¬¦å·å¯èƒ½ä¼šç ´åè§£ç åæ–‡æœ¬ä¸­çš„ä¿¡æ¯ï¼Œå› æ­¤åº”è°¨æ…ä½¿ç”¨ã€‚

+   `kwargs`ï¼ˆå…¶ä»–å…³é”®å­—å‚æ•°ï¼Œ*optional*ï¼‰ â€” å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚

è¿”å›å€¼

`str`

è§£ç åçš„å¥å­ã€‚

å°† id åºåˆ—è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨ tokenizer å’Œè¯æ±‡è¡¨ï¼Œå¯é€‰æ‹©åˆ é™¤ç‰¹æ®Šæ ‡è®°å¹¶æ¸…é™¤æ ‡è®°åŒ–ç©ºæ ¼ã€‚

ç±»ä¼¼äºæ‰§è¡Œ `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`ã€‚

## WhisperTokenizerFast

### `class transformers.WhisperTokenizerFast`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper_fast.py#L90)

```py
( vocab_file = None merges_file = None normalizer_file = None tokenizer_file = None unk_token = '<|endoftext|>' bos_token = '<|endoftext|>' eos_token = '<|endoftext|>' add_prefix_space = False language = None task = None predict_timestamps = False **kwargs )
```

å‚æ•°

+   `vocab_file` (`str`, *optional*) â€” è¯æ±‡æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `merges_file` (`str`, *optional*) â€” åˆå¹¶æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `normalizer_file` (`str`, *optional*) â€” æ­£è§„åŒ–æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `tokenizer_file` (`str`, *optional*) â€” åŒ…å«åŠ è½½ tokenizer æ‰€éœ€æ‰€æœ‰å†…å®¹çš„ [tokenizers](https://github.com/huggingface/tokenizers) æ–‡ä»¶çš„è·¯å¾„ï¼ˆé€šå¸¸å…·æœ‰ .json æ‰©å±•åï¼‰ã€‚

+   `unk_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º``"<|endoftext|>"``ï¼‰--æœªçŸ¥ä»¤ç‰Œã€‚è¯æ±‡è¡¨ä¸­æ²¡æœ‰çš„ä»¤ç‰Œæ— æ³•è½¬æ¢ä¸º IDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºè¯¥ä»¤ç‰Œã€‚

+   `bos_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º``"<|endoftext|>"``ï¼‰--åºåˆ—æ ‡è®°çš„å¼€å¤´ã€‚`decoder_start_token_id`ç”¨äºåœ¨ç”Ÿæˆæ—¶å°†ç¬¬ä¸€ä¸ªä»¤ç‰Œè®¾ç½®ä¸º``"<|startoftranscript|>"``ã€‚

+   `eos_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º``"<|endoftext|>"``ï¼‰--åºåˆ—ç»“æŸæ ‡è®°ã€‚

+   `add_prefix_space` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨è¾“å…¥å‰æ·»åŠ ä¸€ä¸ªåˆå§‹ç©ºæ ¼ã€‚è¿™æ ·å¯ä»¥å°†å‰å¯¼å•è¯è§†ä¸ºä»»ä½•å…¶ä»–å•è¯ã€‚ï¼ˆWhisper tokenizer é€šè¿‡å‰é¢çš„ç©ºæ ¼æ£€æµ‹å•è¯çš„å¼€å¤´ï¼‰ã€‚

+   `language`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰--è½¬å½•æ–‡æœ¬çš„è¯­è¨€ã€‚å¯¹äºå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘ä»»åŠ¡ï¼Œç›¸åº”çš„è¯­è¨€ id æ ‡è®°è¢«é™„åŠ åˆ°åºåˆ—çš„å¼€å¤´ï¼Œä¾‹å¦‚å¯¹äºè¥¿ç­ç‰™è¯­ï¼Œæ ‡è®°`"<|es|>"`è¢«é™„åŠ åˆ°é¡ºåºçš„å¼€å¤´ã€‚è¿™åªèƒ½ç”¨äºå¤šè¯­è¨€å¾®è°ƒã€‚

+   `task`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰--è¦é™„åŠ åœ¨åºåˆ—å¼€å¤´çš„ä»»åŠ¡æ ‡è¯†ç¬¦ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚è¿™åº”ç”¨äºå¤šè¯­è¨€å¾®è°ƒï¼Œâ€œè½¬å½•â€ç”¨äºè¯­éŸ³è¯†åˆ«ï¼Œâ€œç¿»è¯‘â€ç”¨äºè¯­éŸ³ç¿»è¯‘ã€‚

+   `predict_timestamps`ï¼ˆ`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`False`ï¼‰--æ˜¯å¦çœç•¥åºåˆ—å¼€å¤´çš„`<|notimestamps|>`æ ‡è®°ã€‚

æ„å»ºä¸€ä¸ªâ€œå¿«é€Ÿâ€çš„ Whisper tokenizerï¼ˆç”± HuggingFace çš„ *tokenizers* åº“æ”¯æŒï¼‰ã€‚

æ­¤ tokenizer ç»§æ‰¿è‡ª PreTrainedTokenizerFastï¼Œå…¶ä¸­åŒ…å«å¤§å¤šæ•°ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

#### `set_prefix_tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper_fast.py#L462)

```py
( language: str = None task: str = None predict_timestamps: bool = None )
```

å‚æ•°

+   `language` (`str`, *optional*, é»˜è®¤ä¸º `None`) â€” è½¬å½•æ–‡æœ¬çš„è¯­è¨€ã€‚

+   `task` (`str`, *optional*, é»˜è®¤ä¸º `None`) â€” è¦é™„åŠ åˆ°åºåˆ—å¼€å¤´çš„ä»»åŠ¡æ ‡è¯†ç¬¦ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚

+   `predict_timestamps`ï¼ˆ`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`None`ï¼‰--æ˜¯å¦çœç•¥åºåˆ—å¼€å¤´çš„`<|notimestamps|>`æ ‡è®°ã€‚

è¦†ç›–é™„åŠ åˆ°æ ‡ç­¾åºåˆ—å¼€å¤´çš„å‰ç¼€æ ‡è®°ã€‚æ­¤æ–¹æ³•å¯å•ç‹¬ä½¿ç”¨ä»¥

æ ¹æ®éœ€è¦æ›´æ–°å‰ç¼€æ ‡è®°è¿›è¡Œå¾®è°ƒã€‚ç¤ºä¾‹ï¼š

```py
>>> # instantiate the tokenizer and set the prefix token to Spanish
>>> tokenizer = WhisperTokenizerFast.from_pretrained("openai/whisper-tiny", language="spanish")
>>> # now switch the prefix token from Spanish to French
>>> tokenizer.set_prefix_tokens(language="french")
```

#### `build_inputs_with_special_tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper_fast.py#L536)

```py
( token_ids_0 token_ids_1 = None )
```

é€šè¿‡é™„åŠ  eos_token_id ä»åºåˆ—æ„å»ºæ¨¡å‹è¾“å…¥ã€‚

#### `get_special_tokens_mask`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper_fast.py#L544)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” ID åˆ—è¡¨ã€‚

+   `token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” åºåˆ—å¯¹çš„ç¬¬äºŒä¸ª ID åˆ—è¡¨ã€‚

+   `already_has_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ ‡è®°åˆ—è¡¨æ˜¯å¦å·²ç»ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ ¼å¼åŒ–ä¸ºæ¨¡å‹ã€‚

è¿”å›

`List[int]`

æ•´æ•°åˆ—è¡¨åœ¨èŒƒå›´[0, 1]å†…ï¼š1 è¡¨ç¤ºç‰¹æ®Šæ ‡è®°ï¼Œ0 è¡¨ç¤ºåºåˆ—æ ‡è®°ã€‚

ä»æ²¡æœ‰æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„æ ‡è®°åˆ—è¡¨ä¸­æ£€ç´¢åºåˆ— IDã€‚å½“ä½¿ç”¨ tokenizer çš„`prepare_for_model`æ–¹æ³•æ·»åŠ ç‰¹æ®Šæ ‡è®°æ—¶ï¼Œå°†è°ƒç”¨æ­¤æ–¹æ³•ã€‚

#### `create_token_type_ids_from_sequences`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” ç¬¬ä¸€ä¸ªæ ‡è®°åŒ–åºåˆ—ã€‚

+   `token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” ç¬¬äºŒä¸ªæ ‡è®°åŒ–åºåˆ—ã€‚

è¿”å›

`List[int]`

æ ‡è®°ç±»å‹ IDã€‚

åˆ›å»ºä¸ä¼ é€’çš„åºåˆ—å¯¹åº”çš„æ ‡è®°ç±»å‹ IDã€‚ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ

å¦‚æœæ¨¡å‹æœ‰ç‰¹æ®Šæ„å»ºæ–¹å¼ï¼Œåˆ™åº”åœ¨å­ç±»ä¸­é‡å†™ã€‚

#### `save_vocabulary`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper_fast.py#L447)

```py
( save_directory: str filename_prefix: Optional = None )
```

#### `batch_decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)

```py
( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) â†’ export const metadata = 'undefined';List[str]
```

å‚æ•°

+   `sequences`ï¼ˆ`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`ï¼‰â€” æ ‡è®°åŒ–è¾“å…¥ ID çš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚

+   `skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚

+   `clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦æ¸…é™¤æ ‡è®°ç©ºæ ¼ã€‚å¦‚æœä¸º`None`ï¼Œå°†é»˜è®¤ä¸º`self.clean_up_tokenization_spaces`ã€‚

+   `kwargs`ï¼ˆå…¶ä»–å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰â€” å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šè§£ç æ–¹æ³•ã€‚

è¿”å›

`List[str]`

è§£ç å¥å­çš„åˆ—è¡¨ã€‚

é€šè¿‡è°ƒç”¨è§£ç å°†æ ‡è®° ID åˆ—è¡¨çš„åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ã€‚

#### `decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper_fast.py#L338)

```py
( token_ids skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None output_offsets: bool = False time_precision: float = 0.02 decode_with_timestamps: bool = False normalize: bool = False basic_normalize: bool = False remove_diacritics: bool = False **kwargs ) â†’ export const metadata = 'undefined';str
```

å‚æ•°

+   `token_ids`ï¼ˆ`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`ï¼‰â€” æ ‡è®°åŒ–è¾“å…¥ ID çš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚

+   `skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚

+   `clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦æ¸…é™¤æ ‡è®°ç©ºæ ¼ã€‚å¦‚æœä¸º`None`ï¼Œå°†é»˜è®¤ä¸º`self.clean_up_tokenization_spaces`ï¼ˆåœ¨`tokenizer_config`ä¸­å¯ç”¨ï¼‰ã€‚

+   `output_offsets`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¾“å‡ºæ ‡è®°çš„åç§»é‡ã€‚åªæœ‰åœ¨æ¨¡å‹é¢„æµ‹æ—¶é—´æˆ³æ—¶æ‰åº”è®¾ç½®æ­¤é€‰é¡¹ã€‚

+   `time_precision`ï¼ˆ`float`ï¼Œ`å¯é€‰`ï¼Œé»˜è®¤ä¸º 0.02ï¼‰â€” ä»æ ‡è®°åˆ°æ—¶é—´çš„è½¬æ¢æ—¶é—´æ¯”ç‡ã€‚

+   `decode_with_timestamps`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨åŸå§‹æ–‡æœ¬ä¸­åŒ…å«æ—¶é—´æˆ³è¿›è¡Œè§£ç ã€‚

+   `normalize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦å¯¹è§£ç æ–‡æœ¬åº”ç”¨è‹±æ–‡æ–‡æœ¬è§„èŒƒåŒ–ã€‚ä»…åœ¨ç›®æ ‡æ–‡æœ¬ä¸ºè‹±æ–‡æ—¶é€‚ç”¨ã€‚å¦åˆ™ï¼Œåº”åº”ç”¨åŸºæœ¬æ–‡æœ¬è§„èŒƒåŒ–ã€‚

+   `basic_normalize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦å¯¹è§£ç æ–‡æœ¬åº”ç”¨åŸºæœ¬æ–‡æœ¬è§„èŒƒåŒ–ã€‚é€‚ç”¨äºå¤šè¯­è¨€ç›®æ ‡æ–‡æœ¬ã€‚

+   `remove_diacritics`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨åº”ç”¨åŸºæœ¬æ–‡æœ¬è§„èŒƒåŒ–æ—¶åˆ é™¤å˜éŸ³ç¬¦å·ã€‚åˆ é™¤å˜éŸ³ç¬¦å·å¯èƒ½ä¼šç ´åè§£ç æ–‡æœ¬ä¸­çš„ä¿¡æ¯ï¼Œå› æ­¤åº”è°¨æ…ä½¿ç”¨ã€‚

+   `kwargs`ï¼ˆé¢å¤–çš„å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰- å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚

è¿”å›

`str`

è§£ç åçš„å¥å­ã€‚

å°†å­—ç¬¦ä¸²ä¸­çš„ ids åºåˆ—è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨ tokenizer å’Œè¯æ±‡è¡¨ï¼Œå¯é€‰æ‹©åˆ é™¤ç‰¹æ®Šæ ‡è®°å¹¶æ¸…ç†æ ‡è®°åŒ–ç©ºæ ¼ã€‚

ç±»ä¼¼äºæ‰§è¡Œ`self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`ã€‚

## WhisperFeatureExtractor

### `class transformers.WhisperFeatureExtractor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/feature_extraction_whisper.py#L35)

```py
( feature_size = 80 sampling_rate = 16000 hop_length = 160 chunk_length = 30 n_fft = 400 padding_value = 0.0 return_attention_mask = False **kwargs )
```

å‚æ•°

+   `feature_size`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º 80ï¼‰- æå–ç‰¹å¾çš„ç‰¹å¾ç»´åº¦ã€‚

+   `sampling_rate`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º 16000ï¼‰- éŸ³é¢‘æ–‡ä»¶åº”æ•°å­—åŒ–çš„é‡‡æ ·ç‡ï¼Œä»¥èµ«å…¹ï¼ˆHzï¼‰è¡¨ç¤ºã€‚

+   `hop_length`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º 160ï¼‰- ç”¨äºè·å–æ¢…å°”é¢‘ç‡ç³»æ•°çš„ STFT çš„é‡å çª—å£çš„é•¿åº¦ã€‚

+   `chunk_length`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º 30ï¼‰- ç”¨äºä¿®å‰ªå’Œå¡«å……è¾ƒé•¿æˆ–è¾ƒçŸ­éŸ³é¢‘åºåˆ—çš„`sampling_rate`æ ·æœ¬çš„æœ€å¤§å—æ•°ã€‚

+   `n_fft`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º 400ï¼‰- å‚…ç«‹å¶å˜æ¢çš„å¤§å°ã€‚

+   `padding_value`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0.0ï¼‰- ç”¨äºå¡«å……éŸ³é¢‘çš„å¡«å……å€¼ã€‚åº”å¯¹åº”äºé™éŸ³ã€‚

æ„å»ºä¸€ä¸ª Whisper ç‰¹å¾æå–å™¨ã€‚

è¯¥ç‰¹å¾æå–å™¨ç»§æ‰¿è‡ª SequenceFeatureExtractorï¼Œå…¶ä¸­åŒ…å«å¤§éƒ¨åˆ†ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

è¯¥ç±»ä½¿ç”¨è‡ªå®šä¹‰çš„ numpy å®ç°ä»åŸå§‹è¯­éŸ³ä¸­æå– mel æ»¤æ³¢å™¨ç»„ç‰¹å¾ï¼Œè¯¥å®ç°åº”ä¸ pytorch çš„`torch.stft`ç­‰æ•ˆã€‚

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/feature_extraction_whisper.py#L157)

```py
( raw_speech: Union truncation: bool = True pad_to_multiple_of: Optional = None return_tensors: Union = None return_attention_mask: Optional = None padding: Optional = 'max_length' max_length: Optional = None sampling_rate: Optional = None do_normalize: Optional = None **kwargs )
```

å‚æ•°

+   `raw_speech`ï¼ˆ`np.ndarray`ï¼Œ`List[float]`ï¼Œ`List[np.ndarray]`ï¼Œ`List[List[float]]`ï¼‰- è¦å¡«å……çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯ numpy æ•°ç»„ï¼Œæµ®ç‚¹å€¼åˆ—è¡¨ï¼Œnumpy æ•°ç»„åˆ—è¡¨æˆ–æµ®ç‚¹å€¼åˆ—è¡¨çš„åˆ—è¡¨ã€‚å¿…é¡»æ˜¯å•å£°é“éŸ³é¢‘ï¼Œä¸æ˜¯ç«‹ä½“å£°ï¼Œå³æ¯ä¸ªæ—¶é—´æ­¥é•¿ä¸€ä¸ªæµ®ç‚¹æ•°ã€‚

+   `truncation`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰- æ¿€æ´»æˆªæ–­ä»¥å°†è¾“å…¥åºåˆ—æˆªæ–­ä¸º*max_length*ä»¥ä¸Šçš„é•¿åº¦ä¸º*max_length*ã€‚

+   `pad_to_multiple_of`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º Noneï¼‰- å¦‚æœè®¾ç½®ï¼Œå°†å¡«å……åºåˆ—åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚

    è¿™å¯¹äºå¯ç”¨ NVIDIA ç¡¬ä»¶ä¸Šçš„ Tensor Cores ç‰¹åˆ«æœ‰ç”¨ï¼Œå…¶è®¡ç®—èƒ½åŠ›ä¸º`>= 7.5`ï¼ˆVoltaï¼‰ï¼Œæˆ–è€…å¯¹äºå—ç›Šäºåºåˆ—é•¿åº¦ä¸º 128 çš„ TPUsã€‚

+   `return_attention_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç ã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®š feature_extractor çš„é»˜è®¤è®¾ç½®è¿”å›æ³¨æ„åŠ›æ©ç ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

    å¯¹äº Whisper æ¨¡å‹ï¼Œæ‰¹é‡æ¨ç†æ—¶åº”å§‹ç»ˆä¼ é€’`attention_mask`ï¼Œä»¥é¿å…ç»†å¾®çš„é”™è¯¯ã€‚

+   `return_tensors`ï¼ˆ`str`æˆ– TensorTypeï¼Œ*å¯é€‰*ï¼‰- å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯ Python æ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š

    +   `'tf'`ï¼šè¿”å› TensorFlow `tf.constant`å¯¹è±¡ã€‚

    +   `'pt'`ï¼šè¿”å› PyTorch `torch.Tensor`å¯¹è±¡ã€‚

    +   `'np'`ï¼šè¿”å› Numpy `np.ndarray`å¯¹è±¡ã€‚

+   `sampling_rate`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰- `raw_speech`è¾“å…¥é‡‡æ ·çš„é‡‡æ ·ç‡ã€‚å¼ºçƒˆå»ºè®®åœ¨å‰å‘è°ƒç”¨æ—¶ä¼ é€’`sampling_rate`ï¼Œä»¥é˜²æ­¢é™é»˜é”™è¯¯å¹¶å…è®¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æµæ°´çº¿ã€‚

+   `padding_value`ï¼ˆ`float`ï¼Œé»˜è®¤ä¸º 0.0ï¼‰- ç”¨äºå¡«å……å¡«å……å€¼/å‘é‡çš„å€¼ã€‚

+   `do_normalize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦å¯¹è¾“å…¥è¿›è¡Œé›¶å‡å€¼å•ä½æ–¹å·®å½’ä¸€åŒ–ã€‚å½’ä¸€åŒ–å¯ä»¥å¸®åŠ©æ˜¾è‘—æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

ç”¨äºå¯¹ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—è¿›è¡Œç‰¹å¾åŒ–å’Œå‡†å¤‡æ¨¡å‹çš„ä¸»è¦æ–¹æ³•ã€‚å¦‚æœå¯ç”¨ï¼Œå®ç°ä½¿ç”¨ PyTorch è¿›è¡Œ STFT è®¡ç®—ï¼Œå¦åˆ™ä½¿ç”¨è¾ƒæ…¢çš„åŸºäº NumPy çš„æ–¹æ³•ã€‚

## WhisperProcessor

### `class transformers.WhisperProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/processing_whisper.py#L23)

```py
( feature_extractor tokenizer )
```

å‚æ•°

+   `feature_extractor`ï¼ˆ`WhisperFeatureExtractor`ï¼‰â€” WhisperFeatureExtractor çš„ä¸€ä¸ªå®ä¾‹ã€‚ç‰¹å¾æå–å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

+   `tokenizer`ï¼ˆ`WhisperTokenizer`ï¼‰â€” WhisperTokenizer çš„ä¸€ä¸ªå®ä¾‹ã€‚åˆ†è¯å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

æ„å»ºä¸€ä¸ª Whisper å¤„ç†å™¨ï¼Œå°† Whisper ç‰¹å¾æå–å™¨å’Œ Whisper åˆ†è¯å™¨åŒ…è£…æˆä¸€ä¸ªå•ä¸€å¤„ç†å™¨ã€‚

WhisperProcessor æä¾›äº† WhisperFeatureExtractor å’Œ WhisperTokenizer çš„æ‰€æœ‰åŠŸèƒ½ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…**call**()å’Œ decode()ã€‚

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/processing_whisper.py#L49)

```py
( *args **kwargs )
```

å°†`audio`å‚æ•°è½¬å‘åˆ° WhisperFeatureExtractor çš„**call**()ï¼Œå°†`text`å‚æ•°è½¬å‘åˆ°**call**()ã€‚è¯·å‚é˜…ä¸Šè¿°ä¸¤ç§æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `from_pretrained`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L406)

```py
( pretrained_model_name_or_path: Union cache_dir: Union = None force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' **kwargs )
```

å‚æ•°

+   `pretrained_model_name_or_path`ï¼ˆ`str`æˆ–`os.PathLike`ï¼‰â€” è¿™å¯ä»¥æ˜¯ï¼š

    +   ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œé¢„è®­ç»ƒç‰¹å¾æå–å™¨çš„*æ¨¡å‹ ID*ï¼Œæ‰˜ç®¡åœ¨ huggingface.co ä¸Šçš„æ¨¡å‹å­˜å‚¨åº“å†…ã€‚æœ‰æ•ˆçš„æ¨¡å‹ ID å¯ä»¥ä½äºæ ¹çº§åˆ«ï¼Œå¦‚`bert-base-uncased`ï¼Œæˆ–è€…å‘½åç©ºé—´ä¸‹çš„ç”¨æˆ·æˆ–ç»„ç»‡åç§°ï¼Œå¦‚`dbmdz/bert-base-german-cased`ã€‚

    +   ä¸€ä¸ªåŒ…å«ä½¿ç”¨ save_pretrained()æ–¹æ³•ä¿å­˜çš„ç‰¹å¾æå–å™¨æ–‡ä»¶çš„*ç›®å½•*è·¯å¾„ï¼Œä¾‹å¦‚ï¼Œ`./my_model_directory/`ã€‚

    +   ä¿å­˜çš„ç‰¹å¾æå–å™¨ JSON *æ–‡ä»¶*çš„è·¯å¾„æˆ– URLï¼Œä¾‹å¦‚ï¼Œ`./my_model_directory/preprocessor_config.json`ã€‚**kwargs â€” ä¼ é€’ç»™ from_pretrained()å’Œ`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`çš„å…¶ä»–å…³é”®å­—å‚æ•°ã€‚

å®ä¾‹åŒ–ä¸é¢„è®­ç»ƒæ¨¡å‹ç›¸å…³è”çš„å¤„ç†å™¨ã€‚

è¿™ä¸ªç±»æ–¹æ³•åªæ˜¯è°ƒç”¨ç‰¹å¾æå–å™¨çš„ from_pretrained()ã€å›¾åƒå¤„ç†å™¨ ImageProcessingMixin å’Œåˆ†è¯å™¨`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`æ–¹æ³•ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…ä¸Šè¿°æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

#### `save_pretrained`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L167)

```py
( save_directory push_to_hub: bool = False **kwargs )
```

å‚æ•°

+   `save_directory`ï¼ˆ`str`æˆ–`os.PathLike`ï¼‰â€”è¦ä¿å­˜ç‰¹å¾æå–å™¨ JSON æ–‡ä»¶å’Œåˆ†è¯å™¨æ–‡ä»¶çš„ç›®å½•ï¼ˆå¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™å°†åˆ›å»ºè¯¥ç›®å½•ï¼‰ã€‚

+   `push_to_hub`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”ä¿å­˜æ¨¡å‹åæ˜¯å¦å°†å…¶æ¨é€åˆ° Hugging Face æ¨¡å‹ä¸­å¿ƒã€‚æ‚¨å¯ä»¥ä½¿ç”¨`repo_id`æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“ï¼ˆå°†é»˜è®¤ä¸ºæ‚¨çš„å‘½åç©ºé—´ä¸­çš„`save_directory`åç§°ï¼‰ã€‚

+   `kwargs`ï¼ˆ`Dict[str, Any]`ï¼Œ*å¯é€‰*ï¼‰â€”ä¼ é€’ç»™ push_to_hub()æ–¹æ³•çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚

å°†æ­¤å¤„ç†å™¨çš„å±æ€§ï¼ˆç‰¹å¾æå–å™¨ã€åˆ†è¯å™¨ç­‰ï¼‰ä¿å­˜åœ¨æŒ‡å®šç›®å½•ä¸­ï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨ from_pretrained()æ–¹æ³•é‡æ–°åŠ è½½å®ƒã€‚

æ­¤ç±»æ–¹æ³•åªæ˜¯è°ƒç”¨ save_pretrained()å’Œ save_pretrained()ã€‚è¯·å‚è€ƒä¸Šè¿°æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `batch_decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/processing_whisper.py#L83)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ° WhisperTokenizer çš„ batch_decode()ã€‚è¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/processing_whisper.py#L90)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ° WhisperTokenizer çš„ decode()ã€‚è¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

PytorchHide Pytorch å†…å®¹

## WhisperModel

### `class transformers.WhisperModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1488)

```py
( config: WhisperConfig )
```

å‚æ•°

+   `config`ï¼ˆWhisperConfigï¼‰â€”å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„ Whisper æ¨¡å‹è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹ä¹Ÿæ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æ‰€æœ‰ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„äº‹é¡¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1563)

```py
( input_features: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None decoder_inputs_embeds: Optional = None decoder_position_ids: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_features`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰- ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„æµ®ç‚¹å€¼ mel ç‰¹å¾ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚è¦å‡†å¤‡å¥½æ•°ç»„ä¸º`input_features`ï¼Œåº”ä½¿ç”¨ AutoFeatureExtractor æ¥æå– mel ç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚è¯·å‚é˜…`call`()

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œ*SpecAugment*æ•°æ®å¢å¼ºçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºæ ‡è®°æœªè¢«`æ©ç `ï¼Œ

    +   0 è¡¨ç¤ºæ ‡è®°è¢«`æ©ç `ã€‚

    æ³¨æ„åŠ›æ©ç æ˜¯ä»€ä¹ˆï¼Ÿ

+   `decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ WhisperTokenizer æ¥è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    è§£ç å™¨è¾“å…¥ ID æ˜¯ä»€ä¹ˆï¼Ÿ

    Whisper ä½¿ç”¨`decoder_start_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚

+   `decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥`decoder_input_ids`ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

    å¦‚æœæ‚¨æƒ³è¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œæ‚¨åº”è¯¥é˜…è¯»`modeling_whisper._prepare_decoder_attention_mask`å¹¶æ ¹æ®æ‚¨çš„éœ€æ±‚è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[BART è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨ 1ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(encoder_layers, encoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºå°†ç¼–ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`æ©ç `ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`æ©ç `ã€‚

+   `decoder_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºå°†è§£ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`æ©ç `ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`æ©ç `ã€‚

+   `cross_attn_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºå°†äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«`æ©ç `ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«`æ©ç `ã€‚

+   `encoder_outputs`ï¼ˆ`tuple(tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼‰- å…ƒç»„åŒ…æ‹¬(`last_hidden_state`ï¼Œ*å¯é€‰*ï¼š`hidden_states`ï¼Œ*å¯é€‰*ï¼š`attentions`) `last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›) â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, 1)`çš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„è¾“å…¥ï¼‰è€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `decoder_inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, target_sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œå¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ª`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨å¸Œæœ›æ›´å¤šåœ°æ§åˆ¶å¦‚ä½•å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `use_cache` (`bool`, *å¯é€‰*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_outputs.Seq2SeqModelOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.Seq2SeqModelOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆWhisperConfigï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—è¾“å‡ºã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™è¾“å‡ºå½¢çŠ¶ä¸º`(batch_size, 1, hidden_size)`çš„åºåˆ—çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›) â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    è§£ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    ç¼–ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

WhisperModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from transformers import AutoFeatureExtractor, WhisperModel
>>> from datasets import load_dataset

>>> model = WhisperModel.from_pretrained("openai/whisper-base")
>>> feature_extractor = AutoFeatureExtractor.from_pretrained("openai/whisper-base")
>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> inputs = feature_extractor(ds[0]["audio"]["array"], return_tensors="pt")
>>> input_features = inputs.input_features
>>> decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id
>>> last_hidden_state = model(input_features, decoder_input_ids=decoder_input_ids).last_hidden_state
>>> list(last_hidden_state.shape)
[1, 2, 512]
```

#### `_mask_input_features`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1520)

```py
( input_features: FloatTensor attention_mask: Optional = None )
```

æ ¹æ®[SpecAugment](https://arxiv.org/abs/1904.08779)æ²¿æ—¶é—´è½´å’Œ/æˆ–ç‰¹å¾è½´æ©ç›–æå–çš„ç‰¹å¾ã€‚

## WhisperForConditionalGeneration

### `class transformers.WhisperForConditionalGeneration`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1658)

```py
( config: WhisperConfig )
```

å‚æ•°

+   `config` (WhisperConfig) â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´çš„ Whisper æ¨¡å‹ã€‚å¯ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1696)

```py
( input_features: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None decoder_inputs_embeds: Optional = None decoder_position_ids: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_features`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰- ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„æµ®ç‚¹å€¼ mel ç‰¹å¾ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°ç±»å‹ä¸º`List[float]`æˆ–`numpy.ndarray`çš„æ•°ç»„ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ`input_features`ï¼Œåº”ä½¿ç”¨ AutoFeatureExtractor æ¥æå– mel ç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚å‚è§`call()`

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œ*SpecAugment*æ•°æ®å¢å¼ºçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`èŒƒå›´å†…ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç›–`,

    +   0 è¡¨ç¤º`è¢«æ©ç›–`çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ WhisperTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥ IDï¼Ÿ

    Whisper ä½¿ç”¨`decoder_start_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯èƒ½åªéœ€è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¯·å‚è§`past_key_values`ï¼‰ã€‚

+   `decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥`decoder_input_ids`ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

    å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”é˜…è¯»`modeling_whisper._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[BART è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨ 1ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(encoder_layers, encoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºåœ¨ç¼–ç å™¨ä¸­ä½¿æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`èŒƒå›´å†…ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç›–`,

    +   0 è¡¨ç¤ºå¤´éƒ¨`è¢«æ©ç›–`ã€‚

+   `decoder_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºåœ¨è§£ç å™¨ä¸­ä½¿æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`èŒƒå›´å†…ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç›–`,

    +   0 è¡¨ç¤ºå¤´éƒ¨`è¢«æ©ç›–`ã€‚

+   `cross_attn_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºä½¿äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`èŒƒå›´å†…ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç›–`,

    +   0 è¡¨ç¤ºå¤´éƒ¨`è¢«æ©ç›–`ã€‚

+   `encoder_outputs`ï¼ˆ`tuple(tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼‰â€” å…ƒç»„åŒ…å«ï¼ˆ`last_hidden_state`ï¼Œ*å¯é€‰*ï¼š`hidden_states`ï¼Œ*å¯é€‰*ï¼š`attentions`ï¼‰`last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰â€” å…ƒç»„ç”±é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`ç»„æˆï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ï¼Œä»¥åŠ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©åªè¾“å…¥æœ€åä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, 1)`çš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„è¾“å…¥ï¼‰ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `decoder_inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œå¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ª`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹å†…éƒ¨çš„åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”è¯¥åœ¨`[0, ..., config.vocab_size]`èŒƒå›´å†…ï¼Œæˆ–è€…ä¸º-100ï¼ˆå‚è§`input_ids`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç›–ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰`[0, ..., config.vocab_size]`æ ‡ç­¾çš„æ ‡è®°ã€‚

è¿”å›

transformers.modeling_outputs.Seq2SeqLMOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.Seq2SeqLMOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆWhisperConfigï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›ï¼‰â€” è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`torch.FloatTensor`ï¼‰â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›) â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ä»¥ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºå’Œæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    è§£ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºå’Œæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    ç¼–ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

WhisperForConditionalGeneration çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ æ’­çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åçš„é¢„å¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from transformers import AutoProcessor, WhisperForConditionalGeneration
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("openai/whisper-tiny.en")
>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-tiny.en")

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")

>>> inputs = processor(ds[0]["audio"]["array"], return_tensors="pt")
>>> input_features = inputs.input_features

>>> generated_ids = model.generate(inputs=input_features)

>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> transcription
' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'
```

#### `generate`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/generation_whisper.py#L250)

```py
( input_features: Optional = None generation_config: Optional = None logits_processor: Optional = None stopping_criteria: Optional = None prefix_allowed_tokens_fn: Optional = None synced_gpus: bool = False return_timestamps: Optional = None task: Optional = None language: Optional = None is_multilingual: Optional = None prompt_ids: Optional = None condition_on_prev_tokens: Optional = None temperature: Union = None compression_ratio_threshold: Optional = None logprob_threshold: Optional = None no_speech_threshold: Optional = None num_segment_frames: Optional = None attention_mask: Optional = None time_precision: float = 0.02 return_token_timestamps: Optional = None return_segments: bool = False return_dict_in_generate: Optional = None **kwargs ) â†’ export const metadata = 'undefined';ModelOutput or torch.LongTensor or Dict[str, Any]
```

å‚æ•°

+   `input_features`ï¼ˆ`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰â€” æµ®ç‚¹å€¼çš„å¯¹æ•°æ¢…å°”ç‰¹å¾ï¼Œä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œ*ä¾‹å¦‚*é€šè¿‡ soundfile åº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ`input_features`ï¼Œåº”ä½¿ç”¨ AutoFeatureExtractor æ¥æå–æ¢…å°”ç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è§`call`()ã€‚

+   `generation_config`ï¼ˆ`~generation.GenerationConfig`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨ä½œç”Ÿæˆè°ƒç”¨çš„åŸºæœ¬å‚æ•°åŒ–çš„ç”Ÿæˆé…ç½®ã€‚ä¼ é€’ç»™ç”Ÿæˆçš„`**kwargs`ä¸`generation_config`çš„å±æ€§åŒ¹é…å°†è¦†ç›–å®ƒä»¬ã€‚å¦‚æœæœªæä¾›`generation_config`ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ï¼Œå…¶åŠ è½½ä¼˜å…ˆçº§å¦‚ä¸‹ï¼š1ï¼‰ä»`generation_config.json`æ¨¡å‹æ–‡ä»¶ä¸­ï¼Œå¦‚æœå­˜åœ¨ï¼›2ï¼‰ä»æ¨¡å‹é…ç½®ä¸­ã€‚è¯·æ³¨æ„ï¼ŒæœªæŒ‡å®šçš„å‚æ•°å°†ç»§æ‰¿ GenerationConfig çš„é»˜è®¤å€¼ï¼Œåº”æ£€æŸ¥å…¶æ–‡æ¡£ä»¥å‚æ•°åŒ–ç”Ÿæˆã€‚

+   `logits_processor`ï¼ˆ`LogitsProcessorList`ï¼Œ*å¯é€‰*ï¼‰â€” è‡ªå®šä¹‰å¯¹æ•°å¤„ç†å™¨ï¼Œè¡¥å……ç”±å‚æ•°å’Œç”Ÿæˆé…ç½®æ„å»ºçš„é»˜è®¤å¯¹æ•°å¤„ç†å™¨ã€‚å¦‚æœä¼ é€’çš„å¯¹æ•°å¤„ç†å™¨å·²ç»ä½¿ç”¨å‚æ•°æˆ–ç”Ÿæˆé…ç½®åˆ›å»ºï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ã€‚æ­¤åŠŸèƒ½é€‚ç”¨äºé«˜çº§ç”¨æˆ·ã€‚

+   `stopping_criteria`ï¼ˆ`StoppingCriteriaList`ï¼Œ*å¯é€‰*ï¼‰â€” è‡ªå®šä¹‰åœæ­¢æ ‡å‡†ï¼Œè¡¥å……ç”±å‚æ•°å’Œç”Ÿæˆé…ç½®æ„å»ºçš„é»˜è®¤åœæ­¢æ ‡å‡†ã€‚å¦‚æœä¼ é€’çš„åœæ­¢æ ‡å‡†å·²ç»ä½¿ç”¨å‚æ•°æˆ–ç”Ÿæˆé…ç½®åˆ›å»ºï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ã€‚æ­¤åŠŸèƒ½é€‚ç”¨äºé«˜çº§ç”¨æˆ·ã€‚

+   `prefix_allowed_tokens_fn`ï¼ˆ`Callable[[int, torch.Tensor], List[int]]`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœæä¾›ï¼Œæ­¤å‡½æ•°å°†åœ¨æ¯ä¸ªæ­¥éª¤å°†æŸæœç´¢é™åˆ¶ä¸ºä»…å…è®¸çš„æ ‡è®°ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä¸åº”ç”¨çº¦æŸã€‚æ­¤å‡½æ•°æ¥å— 2 ä¸ªå‚æ•°ï¼šæ‰¹æ¬¡ ID`batch_id`å’Œ`input_ids`ã€‚å®ƒå¿…é¡»è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸‹ä¸€ä»£æ­¥éª¤çš„å…è®¸æ ‡è®°ï¼Œæ¡ä»¶æ˜¯æ‰¹æ¬¡ ID`batch_id`å’Œå…ˆå‰ç”Ÿæˆçš„æ ‡è®°`inputs_ids`ã€‚æ­¤å‚æ•°å¯¹äºå—å‰ç¼€çº¦æŸçš„ç”Ÿæˆå¾ˆæœ‰ç”¨ï¼Œå¦‚[è‡ªå›å½’å®ä½“æ£€ç´¢](https://arxiv.org/abs/2010.00904)ä¸­æ‰€è¿°ã€‚

+   `synced_gpus`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦ç»§ç»­è¿è¡Œ while å¾ªç¯ç›´åˆ° max_lengthï¼ˆå¯¹äº ZeRO é˜¶æ®µ 3 æ˜¯å¿…éœ€çš„ï¼‰

+   `return_timestamps`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ–‡æœ¬çš„æ—¶é—´æˆ³ã€‚è¿™å°†å¯ç”¨`WhisperTimestampsLogitsProcessor`ã€‚

+   `task`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºç”Ÿæˆçš„ä»»åŠ¡ï¼Œå¯ä»¥æ˜¯â€œtranslateâ€æˆ–â€œtranscribeâ€ã€‚`model.config.forced_decoder_ids`å°†ç›¸åº”æ›´æ–°ã€‚

+   `language`ï¼ˆ`str`ï¼Œ*optional*ï¼‰--ç”¨äºç”Ÿæˆçš„è¯­è¨€æ ‡è®°ï¼Œå¯ä»¥æ˜¯`<|en|>`ã€`en`æˆ–`english`å½¢å¼ã€‚æ‚¨å¯ä»¥åœ¨`model.generation_config.lang_to_id`å­—å…¸ä¸­æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„è¯­è¨€æ ‡è®°ã€‚

+   `is_multilingual`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨¡å‹æ˜¯å¦æ˜¯å¤šè¯­è¨€çš„ã€‚

+   `prompt_ids`ï¼ˆ`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” é€šè¿‡å°†æ–‡æœ¬ä¼ é€’ç»™`get_prompt_ids()`åˆ›å»ºçš„ä»¤ç‰Œ ID çš„ç§©-1 å¼ é‡ï¼Œä½œä¸ºæ¯ä¸ªå—çš„æç¤ºæä¾›ã€‚è¿™å¯ç”¨äºä¸ºè½¬å½•æä¾›æˆ–â€œæç¤ºå·¥ç¨‹â€ä¸Šä¸‹æ–‡ï¼Œä¾‹å¦‚è‡ªå®šä¹‰è¯æ±‡æˆ–ä¸“æœ‰åè¯ï¼Œä»¥ä½¿å…¶æ›´æœ‰å¯èƒ½æ­£ç¡®é¢„æµ‹è¿™äº›å•è¯ã€‚å®ƒä¸èƒ½ä¸`decoder_start_token_id`ç»“åˆä½¿ç”¨ï¼Œå› ä¸ºå®ƒä¼šè¦†ç›–æ­¤å€¼ã€‚

+   `condition_on_prev_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” ä»…é€‚ç”¨äºé•¿ç¯‡è½¬å½•ã€‚æ˜¯å¦å°†æ¯ä¸ªç‰‡æ®µçš„ç”Ÿæˆæ¡ä»¶è®¾ç½®ä¸ºå‰ä¸€ä¸ªç‰‡æ®µã€‚å¦‚[Whisper è®ºæ–‡](https://cdn.openai.com/papers/whisper.pdf)æ‰€ç¤ºï¼Œè¿™å¯ä»¥å¸®åŠ©æé«˜æ€§èƒ½ã€‚

+   `temperature` (`float` æˆ– `float` åˆ—è¡¨ï¼Œ*å¯é€‰*) â€” ç”¨äºç”Ÿæˆçš„æ¸©åº¦ã€‚ä¼ é€’å•ä¸ª `float` å€¼å¹¶ä¸” `do_sample=True` ä¼šæ¿€æ´»ä½¿ç”¨é‡‡æ ·è¿›è¡Œç”Ÿæˆã€‚å¯¹äºé•¿ç¯‡è½¬å½•ï¼Œå¯ä»¥é€šè¿‡ä¼ é€’ä¸€ç»„æµ®ç‚¹å€¼ï¼ˆä¾‹å¦‚ (0.0, 0.2, 0.4, 0.6, 0.8, 1.0)ï¼‰æ¥æ¿€æ´»æ¸©åº¦å›é€€ã€‚æ­£å¦‚[Whisper è®ºæ–‡](https://cdn.openai.com/papers/whisper.pdf)æ‰€ç¤ºï¼Œè¿™å¯ä»¥å¸®åŠ©æé«˜æ€§èƒ½ã€‚

+   `compression_ratio_threshold` (`float`, *å¯é€‰*) â€” ä»…é€‚ç”¨äºé•¿ç¯‡è½¬å½•ã€‚å¦‚æœå®šä¹‰äº†ï¼Œå°†è®¡ç®—æ¯ä¸ªç‰‡æ®µçš„ zlib å‹ç¼©ç‡ã€‚å¦‚æœä¸€ä¸ªç‰‡æ®µçš„å‹ç¼©ç‡é«˜äº `compression_ratio_threshold`ï¼Œåˆ™æ¿€æ´»æ¸©åº¦å›é€€ï¼šç”Ÿæˆçš„ç‰‡æ®µè¢«ä¸¢å¼ƒï¼Œä½¿ç”¨æ›´é«˜çš„æ¸©åº¦é‡å¤ç”Ÿæˆã€‚è¿™ä¸ªç‰¹æ€§èƒŒåçš„ç›´è§‰æ˜¯ï¼Œå…·æœ‰éå¸¸é«˜å‹ç¼©ç‡çš„ç‰‡æ®µå­˜åœ¨å¤§é‡é‡å¤ã€‚é€šè¿‡å¢åŠ æ¸©åº¦æ³¨å…¥æ›´å¤šéšæœºæ€§å¯ä»¥å‡å°‘ä¸éœ€è¦çš„é‡å¤ã€‚å¦‚æœå®šä¹‰äº† `compression_ratio_threshold`ï¼Œè¯·ç¡®ä¿ `temperature` æ˜¯ä¸€ä¸ªå€¼åˆ—è¡¨ã€‚`compression_ratio_threshold` çš„å¸¸è§å€¼ä¸º 1.35ã€‚æ­£å¦‚[Whisper è®ºæ–‡](https://cdn.openai.com/papers/whisper.pdf)æ‰€ç¤ºï¼Œè¿™å¯ä»¥å¸®åŠ©æé«˜æ€§èƒ½ã€‚

+   `logprob_threshold` (`float`, *å¯é€‰*) â€” ä»…é€‚ç”¨äºé•¿ç¯‡è½¬å½•ã€‚å¦‚æœå®šä¹‰äº†ï¼Œå°†è®¡ç®—æ¯ä¸ªç‰‡æ®µçš„å¹³å‡å¯¹æ•°æ¦‚ç‡ã€‚å¦‚æœç»™å®šç‰‡æ®µçš„å¯¹æ•°æ¦‚ç‡ä½äº `logprob_threshold`ï¼Œåˆ™æ¿€æ´»æ¸©åº¦å›é€€ï¼šç”Ÿæˆçš„ç‰‡æ®µè¢«ä¸¢å¼ƒï¼Œä½¿ç”¨æ›´é«˜çš„æ¸©åº¦é‡å¤ç”Ÿæˆã€‚è¿™ä¸ªç‰¹æ€§èƒŒåçš„ç›´è§‰æ˜¯ï¼Œä½å¯¹æ•°æ¦‚ç‡çš„ç‰‡æ®µå¯ä»¥é€šè¿‡å¢åŠ æ¸©åº¦æ³¨å…¥æ›´å¤šéšæœºæ€§æ¥æ”¹å–„ã€‚å¦‚æœå®šä¹‰äº† `logprob_threshold`ï¼Œè¯·ç¡®ä¿ `temperature` æ˜¯ä¸€ä¸ªå€¼åˆ—è¡¨ã€‚`logprob_threshold` çš„å¸¸è§å€¼ä¸º -1.0ã€‚æ­£å¦‚[Whisper è®ºæ–‡](https://cdn.openai.com/papers/whisper.pdf)æ‰€ç¤ºï¼Œè¿™å¯ä»¥å¸®åŠ©æé«˜æ€§èƒ½ã€‚

+   `no_speech_threshold` (`float`, *å¯é€‰*) â€” ä»…é€‚ç”¨äºé•¿ç¯‡è½¬å½•ã€‚å¦‚æœå®šä¹‰äº†ï¼Œâ€œæ— è¯­éŸ³â€æ ‡è®°ä¸ `logprob_threshold` ç»“åˆä½¿ç”¨æ¥ç¡®å®šä¸€ä¸ªç‰‡æ®µæ˜¯å¦åªåŒ…å«é™éŸ³ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå°†è·³è¿‡è¯¥ç‰‡æ®µçš„è½¬å½•ã€‚æ­£å¦‚[Whisper è®ºæ–‡](https://cdn.openai.com/papers/whisper.pdf)æ‰€ç¤ºï¼Œè¿™å¯ä»¥å¸®åŠ©æé«˜æ€§èƒ½ã€‚

+   `num_segment_frames` (`int`, *å¯é€‰*) â€” å•ä¸ªç‰‡æ®µåŒ…å«çš„å¸§æ•°ã€‚å¦‚æœæœªå®šä¹‰ï¼Œ`num_segment_frames` é»˜è®¤ä¸ºæ¨¡å‹çš„æ­¥å¹…ä¹˜ä»¥æœ€å¤§è¾“å…¥é•¿åº¦ã€‚

+   `attention_mask` (`torch.Tensor`, *å¯é€‰*) â€” åœ¨ä½¿ç”¨æ‰¹é‡å¤§å° > 1 è¿›è¡Œé•¿ç¯‡è½¬å½•æ—¶éœ€è¦ä¼ é€’ `attention_mask`ã€‚

+   `time_precision` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0.02) â€” è¾“å‡ºæ ‡è®°çš„æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰ã€‚*ä¾‹å¦‚*ï¼Œ0.02 è¡¨ç¤ºç”Ÿæˆçš„æ ‡è®°å¹³å‡å æ® 20 æ¯«ç§’ã€‚

+   `return_token_timestamps` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ–‡æœ¬çš„æ ‡è®°çº§æ—¶é—´æˆ³ã€‚å¯ä»¥ä¸ `return_timestamps` é€‰é¡¹ä¸€èµ·ä½¿ç”¨ã€‚è¦è·å¾—å•è¯çº§æ—¶é—´æˆ³ï¼Œè¯·ä½¿ç”¨åˆ†è¯å™¨å°†æ ‡è®°åˆ†ç»„æˆå•è¯ã€‚

+   `return_segments` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦é¢å¤–è¿”å›æ‰€æœ‰ç‰‡æ®µçš„åˆ—è¡¨ã€‚è¯·æ³¨æ„ï¼Œåªæœ‰åœ¨è¿›è¡Œé•¿ç¯‡è½¬å½•æ—¶æ‰èƒ½å¯ç”¨æ­¤é€‰é¡¹ã€‚

+   `return_dict_in_generate` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸ä»…ä»…è¿”å›ç”Ÿæˆçš„æ ‡è®°ã€‚è¯·æ³¨æ„ï¼Œåœ¨è¿›è¡Œé•¿ç¯‡è½¬å½•æ—¶ï¼Œåªæœ‰åœ¨è®¾ç½® `return_segments` ä¸º True æ—¶æ‰èƒ½å¯ç”¨ `return_dict_in_generate`ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªç‰‡æ®µçš„ç”Ÿæˆè¾“å‡ºå°†æ·»åŠ åˆ°æ¯ä¸ªç‰‡æ®µä¸­ã€‚

+   `kwargs`ï¼ˆ`Dict[str, Any]`ï¼Œ*å¯é€‰*ï¼‰-`generate_config`çš„ç‰¹å®šäºç‰¹å®šæ¨¡å‹çš„å‚æ•°åŒ–å’Œ/æˆ–å…¶ä»–æ¨¡å‹ç‰¹å®š kwargsï¼Œå°†è½¬å‘åˆ°æ¨¡å‹çš„`forward`å‡½æ•°ã€‚å¦‚æœæ¨¡å‹æ˜¯ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œåˆ™ä¸åº”ä»¥å‰ç¼€å½¢å¼æŒ‡å®šç¼–ç å™¨ç‰¹å®š kwargsï¼Œè€Œåº”ä»¥*decoder_*ä¸ºå‰ç¼€æŒ‡å®šè§£ç å™¨ç‰¹å®š kwargsã€‚

è¿”å›

ModelOutput æˆ–`torch.LongTensor`æˆ–`Dict[str, Any]`

ä¸€ä¸ª ModelOutputï¼ˆå¦‚æœ`return_dict_in_generate=True`æˆ–å½“`config.return_dict_in_generate=True`æ—¶ï¼‰æˆ–ä¸€ä¸ª`torch.FloatTensor`æˆ–ä¸€ä¸ªæ®µçš„å­—å…¸ï¼Œå½“`return_segments=True`æ—¶ã€‚

å¦‚æœä¼ å…¥çš„è¾“å…¥> 30 ç§’/ > 3000 mel è¾“å…¥ç‰¹å¾ï¼Œå¹¶ä¸”`return_segments=True`ï¼Œåˆ™è¿”å›ä¸€ä¸ªç”Ÿæˆçš„åºåˆ— id å­—å…¸ï¼Œç§°ä¸º`sequences`ï¼Œä»¥åŠæ¯ä¸ªç”Ÿæˆæ®µçš„åˆ—è¡¨ã€‚

å¦åˆ™ï¼Œå¦‚æœä¼ å…¥çš„è¾“å…¥<= 30 ç§’/ >= 3000 mel è¾“å…¥ç‰¹å¾ï¼Œåˆ™å¯èƒ½çš„ ModelOutput ç±»å‹ä¸ºï¼š

+   GenerateEncoderDecoderOutputï¼Œ

+   GenerateBeamEncoderDecoderOutput

å¦åˆ™ï¼Œä»…è¿”å›ç”Ÿæˆçš„è¾“å‡ºåºåˆ— idã€‚

å°†å¯¹æ•° mel è¾“å…¥ç‰¹å¾è½¬å½•æˆ–ç¿»è¯‘ä¸ºè‡ªå›å½’ç”Ÿæˆçš„ä»¤ç‰Œ id åºåˆ—ã€‚

å¤§å¤šæ•°ç”Ÿæˆæ§åˆ¶å‚æ•°éƒ½åœ¨`generation_config`ä¸­è®¾ç½®ï¼Œå¦‚æœæœªä¼ é€’ï¼Œåˆ™å°†è®¾ç½®ä¸ºæ¨¡å‹çš„é»˜è®¤ç”Ÿæˆé…ç½®ã€‚æ‚¨å¯ä»¥é€šè¿‡å°†ç›¸åº”çš„å‚æ•°ä¼ é€’ç»™ generate()æ¥è¦†ç›–ä»»ä½•`generation_config`ï¼Œä¾‹å¦‚`.generate(inputs, num_beams=4, do_sample=True)`ã€‚

æœ‰å…³ç”Ÿæˆç­–ç•¥å’Œä»£ç ç¤ºä¾‹çš„æ¦‚è¿°ï¼Œè¯·æŸ¥çœ‹ä»¥ä¸‹æŒ‡å—ã€‚

ç¤ºä¾‹ï¼š

+   *è¯¦ç»†è½¬å½•*ï¼šè¦è½¬å½•æˆ–ç¿»è¯‘è¶…è¿‡ 30 ç§’çš„éŸ³é¢‘ï¼Œè¯·å¤„ç†éŸ³é¢‘æ–‡ä»¶è€Œä¸æˆªæ–­ï¼Œå¹¶ä¸€æ¬¡ä¼ é€’æ‰€æœ‰ mel ç‰¹å¾ä»¥ç”Ÿæˆã€‚

```py
>>> import torch
>>> from transformers import AutoProcessor, WhisperForConditionalGeneration
>>> from datasets import load_dataset, Audio

>>> processor = AutoProcessor.from_pretrained("openai/whisper-tiny.en")
>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-tiny.en")
>>> model.cuda()

>>> # load audios > 30 seconds
>>> ds = load_dataset("distil-whisper/meanwhile", "default")["test"]
>>> # resample to 16kHz
>>> ds = ds.cast_column("audio", Audio(sampling_rate=16000))
>>> # take first 8 audios and retrieve array
>>> audio = ds[:8]["audio"]
>>> audio = [x["array"] for x in audio]

>>> # make sure to NOT truncate the input audio, to return the `attention_mask` and to pad to the longest audio
>>> inputs = processor(audio, return_tensors="pt", truncation=False, padding="longest", return_attention_mask=True, sampling_rate=16_000)
>>> inputs = inputs.to("cuda", torch.float32)

>>> # transcribe audio to ids
>>> generated_ids = model.generate(**inputs)

>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> transcription[0]
' Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories developing the central headline pawns, definitely maneuvering an oso topical night to F6, fainting a classic Sicilian, nade door variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a fisher's shows in Lip Nitsky attack that culminates in the elegant lethal slow-played, all-passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I. CHEERING AND APPLAUSE Sometimes I startle away, cubside down in the monkey bars of a condemned playground on a super fun site. Get all hept up on goofballs. Rummage that were discarded tag bag of defective toys. Yank out a fist bowl of disembodied doll limbs, toss them on a stained kid's place mat from a defunct dennies. set up a table inside a rusty cargo container down by the Wharf and challenged toothless drifters to the godless bughouse blitz of tournament that is my segment. Meanwhile!'
```

+   *ç®€åŒ–è½¬å½•*ï¼šå¦‚æœä¼ å…¥çš„ mel è¾“å…¥ç‰¹å¾< 30 ç§’ï¼Œåˆ™æ•´ä¸ªéŸ³é¢‘å°†é€šè¿‡ä¸€æ¬¡è°ƒç”¨ç”Ÿæˆè¿›è¡Œè½¬å½•ã€‚

```py
>>> import torch
>>> from transformers import AutoProcessor, WhisperForConditionalGeneration
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("openai/whisper-tiny.en")
>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-tiny.en")

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")

>>> inputs = processor(ds[0]["audio"]["array"], return_tensors="pt")
>>> input_features = inputs.input_features

>>> generated_ids = model.generate(inputs=input_features)

>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> transcription
' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'
```

## WhisperForCausalLM

### `class transformers.WhisperForCausalLM`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1865)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆWhisperConfigï¼‰-æ¨¡å‹é…ç½®ç±»ï¼Œå…·æœ‰æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

Whisper è§£ç å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´ï¼ˆçº¿æ€§å±‚ï¼Œå…¶æƒé‡ä¸è¾“å…¥åµŒå…¥ç»‘å®šï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª PreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼Œè°ƒæ•´è¾“å…¥åµŒå…¥å¤§å°ï¼Œä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1903)

```py
( input_ids: LongTensor = None attention_mask: Optional = None encoder_outputs: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ PreTrainedTokenizer.encode() å’Œ PreTrainedTokenizer.`call`()ã€‚ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ

+   `attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨ `[0, 1]` ä¸­ï¼š

    +   1 è¡¨ç¤ºæ ‡è®°æ˜¯ `not masked`ï¼Œ

    +   0 è¡¨ç¤ºæ ‡è®°æ˜¯ `masked`ã€‚ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `encoder_outputs` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚å¦‚æœæ¨¡å‹é…ç½®ä¸ºè§£ç å™¨ï¼Œåˆ™åœ¨äº¤å‰æ³¨æ„åŠ›ä¸­ä½¿ç”¨ã€‚

+   `head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*) â€” ç”¨äºä½¿æ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨ `[0, 1]` ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢« `masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢« `masked`ã€‚

+   `cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*) â€” ç”¨äºä½¿äº¤å‰æ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨ `[0, 1]` ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢« `masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢« `masked`ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’ `use_cache=True` æˆ– `config.use_cache=True` æ—¶è¿”å›) â€” é•¿åº¦ä¸º `config.n_layers` çš„ `tuple(torch.FloatTensor)` çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, embed_size_per_head)` çš„å¼ é‡å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)` çš„å¼ é‡ã€‚å½“æ¨¡å‹ç”¨ä½œåºåˆ—åˆ°åºåˆ—æ¨¡å‹ä¸­çš„è§£ç å™¨æ—¶ï¼Œåªæœ‰è¿™ä¸¤ä¸ªé¢å¤–çš„å¼ é‡æ˜¯å¿…éœ€çš„ã€‚åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§ `past_key_values` è¾“å…¥ï¼‰ã€‚å¦‚æœä½¿ç”¨äº† `past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©åªè¾“å…¥æœ€åä¸€ä¸ªå½¢çŠ¶ä¸º `(batch_size, 1)` çš„ `decoder_input_ids`ï¼ˆè¿™äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™è¯¥æ¨¡å‹çš„æ ‡è®°ï¼‰è€Œä¸æ˜¯æ‰€æœ‰å½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ `decoder_input_ids`ã€‚

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’ `input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°† `input_ids` ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚

+   `labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” ç”¨äºè®¡ç®—æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”è¯¥åœ¨ `[0, ..., config.vocab_size]` æˆ– -100ï¼ˆå‚è§ `input_ids` æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º `-100` çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰æ ‡ç­¾åœ¨ `[0, ..., config.vocab_size]` ä¸­çš„æ ‡è®°ã€‚

+   `use_cache` (`bool`, *optional*) â€” å¦‚æœè®¾ç½®ä¸º `True`ï¼Œåˆ™ä¼šè¿”å› `past_key_values` é”®å€¼çŠ¶æ€ï¼Œå¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§ `past_key_values`ï¼‰ã€‚

    +   1 è¡¨ç¤ºæ ‡è®°æ˜¯ `not masked`ï¼Œ

    +   0 è¡¨ç¤ºæ ‡è®°æ˜¯ `masked`ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„ `attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šçš„å…ƒç»„ã€‚

è¿”å›å€¼

transformers.modeling_outputs.CausalLMOutputWithCrossAttentions æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.CausalLMOutputWithCrossAttentions æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆWhisperConfigï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `æŸå¤±` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›) â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚

+   `logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    æ³¨æ„åŠ› softmax åçš„è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    æ³¨æ„åŠ› softmax åçš„äº¤å‰æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰ â€” é•¿åº¦ä¸º`config.n_layers`çš„`torch.FloatTensor`å…ƒç»„çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›å±‚çš„ç¼“å­˜é”®ã€å€¼çŠ¶æ€ï¼Œå¦‚æœæ¨¡å‹ç”¨äºç¼–ç å™¨-è§£ç å™¨è®¾ç½®ï¼Œåˆ™ç›¸å…³ã€‚ä»…åœ¨`config.is_decoder = True`æ—¶ç›¸å…³ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆæŸ¥çœ‹`past_key_values`è¾“å…¥ï¼‰ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import WhisperForCausalLM, WhisperForConditionalGeneration, WhisperProcessor
>>> import torch
>>> from datasets import load_dataset

>>> processor = WhisperProcessor.from_pretrained("openai/whisper-large-v2")
>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v2")

>>> assistant_model = WhisperForCausalLM.from_pretrained("distil-whisper/distil-large-v2")

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> sample = ds[0]["audio"]
>>> input_features = processor(
...     sample["array"], sampling_rate=sample["sampling_rate"], return_tensors="pt"
... ).input_features

>>> predicted_ids = model.generate(input_features, assistant_model=assistant_model)

>>> # decode token ids to text
>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
>>> transcription
' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'
```

## WhisperForAudioClassification

### `class transformers.WhisperForAudioClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L2085)

```py
( config )
```

å‚æ•°

+   `input_features` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`) â€” ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„æµ®ç‚¹å€¼ mel ç‰¹å¾ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ`input_features`ï¼Œåº”ä½¿ç”¨ AutoFeatureExtractor æ¥æå– mel ç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚è¯·å‚è§`call`()

+   `head_mask` (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(encoder_layers, encoder_attention_heads)`ï¼Œ*optional*) â€” ç”¨äºä½¿ç¼–ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” å…ƒç»„åŒ…æ‹¬(`last_hidden_state`ï¼Œ*optional*: `hidden_states`ï¼Œ*optional*: `attentions`) `last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*)æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

å¸¦æœ‰é¡¶éƒ¨åºåˆ—åˆ†ç±»å¤´éƒ¨ï¼ˆåœ¨æ±‡èšè¾“å‡ºä¸Šçš„çº¿æ€§å±‚ï¼‰çš„ Whisper ç¼–ç å™¨æ¨¡å‹ï¼Œç”¨äºç±»ä¼¼ SUPERB å…³é”®è¯è¯†åˆ«çš„ä»»åŠ¡ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L2119)

```py
( input_features: Optional = None head_mask: Optional = None encoder_outputs: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_features` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`) â€” ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„æµ®ç‚¹å€¼ mel ç‰¹å¾ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ`input_features`ï¼Œåº”ä½¿ç”¨ AutoFeatureExtractor æ¥æå– mel ç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚è¯·å‚è§`call`()

+   `head_mask` (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(encoder_layers, encoder_attention_heads)`ï¼Œ*optional*) â€” ç”¨äºä½¿ç¼–ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” å…ƒç»„åŒ…æ‹¬(`last_hidden_state`ï¼Œ*optional*: `hidden_states`ï¼Œ*optional*: `attentions`) `last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*)æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨ `[0, ..., config.num_labels - 1]` ä¸­ã€‚å¦‚æœ `config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ `config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

transformers.modeling_outputs.SequenceClassifierOutput æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.SequenceClassifierOutput æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆWhisperConfigï¼‰å’Œè¾“å…¥ã€‚

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾› `labels` æ—¶è¿”å›) â€” åˆ†ç±»ï¼ˆæˆ–å›å½’ï¼Œå¦‚æœ config.num_labels==1ï¼‰æŸå¤±ã€‚

+   `logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€” åˆ†ç±»ï¼ˆæˆ–å›å½’ï¼Œå¦‚æœ config.num_labels==1ï¼‰å¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’ `output_hidden_states=True` æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’ `output_attentions=True` æˆ–å½“ `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ã€‚

    æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ› SoftMax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

WhisperForAudioClassification çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from transformers import AutoFeatureExtractor, WhisperForAudioClassification
>>> from datasets import load_dataset

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("sanchit-gandhi/whisper-medium-fleurs-lang-id")
>>> model = WhisperForAudioClassification.from_pretrained("sanchit-gandhi/whisper-medium-fleurs-lang-id")

>>> ds = load_dataset("google/fleurs", "all", split="validation", streaming=True)
>>> sample = next(iter(ds))

>>> inputs = feature_extractor(
...     sample["audio"]["array"], sampling_rate=sample["audio"]["sampling_rate"], return_tensors="pt"
... )
>>> input_features = inputs.input_features

>>> with torch.no_grad():
...     logits = model(input_features).logits

>>> predicted_class_ids = torch.argmax(logits).item()
>>> predicted_label = model.config.id2label[predicted_class_ids]
>>> predicted_label
'Afrikaans'
```

TensorFlowHide TensorFlow å†…å®¹

## TFWhisperModel

### `class transformers.TFWhisperModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_tf_whisper.py#L1230)

```py
( config: WhisperConfig **kwargs )
```

å‚æ•°

+   `config` (WhisperConfig) â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained() æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„ Whisper æ¨¡å‹è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª TFPreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_tf_whisper.py#L1258)

```py
( input_features: TFModelInputType | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None decoder_position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None decoder_head_mask: np.ndarray | tf.Tensor | None = None cross_attn_head_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None decoder_inputs_embeds: Optional[Tuple[Union[np.ndarray, tf.Tensor]]] = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqModelOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `input_features`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`çš„`tf.Tensor`ï¼‰- ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„ fbank ç‰¹å¾çš„æµ®ç‚¹å€¼ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ`input_features`ï¼Œåº”ä½¿ç”¨ AutoFeatureExtractor æ¥æå– fbank ç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸º`tf.Tensor`ç±»å‹çš„å¼ é‡ã€‚è¯·å‚è§`call`()

+   `decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨`SpeechToTextTokenizer`è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯ decoder input IDsï¼Ÿ

    SpeechToText ä½¿ç”¨`eos_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œå¯é€‰æ‹©åªè¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥`decoder_input_ids`ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœè’™ç‰ˆä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

    å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œè¯·é˜…è¯»`modeling_whisper._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨ 1ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(encoder_layers, encoder_attention_heads)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºåœ¨ç¼–ç å™¨ä¸­ä½¿é€‰å®šæ³¨æ„åŠ›æ¨¡å—çš„å¤´éƒ¨å¤±æ•ˆçš„è’™ç‰ˆã€‚è’™ç‰ˆå€¼é€‰å®šåœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚

+   `decoder_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºåœ¨è§£ç å™¨ä¸­ä½¿é€‰å®šæ³¨æ„åŠ›æ¨¡å—çš„å¤´éƒ¨å¤±æ•ˆçš„è’™ç‰ˆã€‚è’™ç‰ˆå€¼é€‰å®šåœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚

+   `cross_attn_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºä½¿äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„è’™ç‰ˆã€‚è’™ç‰ˆå€¼é€‰å®šåœ¨`[0, 1]`ä¸­ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚

+   `encoder_outputs`ï¼ˆ`tuple(tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼‰- å…ƒç»„åŒ…å«(`last_hidden_state`ï¼Œ*å¯é€‰*ï¼š`hidden_states`ï¼Œ*å¯é€‰*ï¼š`attentions`) `last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `past_key_values`ï¼ˆ`tuple(tuple(tf.Tensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰- é•¿åº¦ä¸º`config.n_layers`çš„`tuple(tf.Tensor)`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`çš„å¼ é‡ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `decoder_inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length, hidden_size)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

`transformers.modeling_tf_outputs.TFSeq2SeqModelOutput`æˆ–`tuple(tf.Tensor)`

ä¸€ä¸ª transformers.modeling_tf_outputs.TFSeq2SeqModelOutput æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆWhisperConfigï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚

+   `last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`ï¼‰â€” æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™ä»…è¾“å‡ºå½¢çŠ¶ä¸º`(batch_size, 1, hidden_size)`åºåˆ—çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ã€‚

+   `past_key_values`ï¼ˆ`List[tf.Tensor]`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰â€” é•¿åº¦ä¸º`config.n_layers`çš„`tf.Tensor`åˆ—è¡¨ï¼Œæ¯ä¸ªå¼ é‡çš„å½¢çŠ¶ä¸º`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`ã€‚

    åŒ…å«è§£ç å™¨çš„é¢„è®¡ç®—éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `decoder_hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    è§£ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    ç¼–ç å™¨åœ¨æ¯ä¸€å±‚çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

TFWhisperModel çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import tensorflow as tf
>>> from transformers import TFWhisperModel, AutoFeatureExtractor
>>> from datasets import load_dataset

>>> model = TFWhisperModel.from_pretrained("openai/whisper-base")
>>> feature_extractor = AutoFeatureExtractor.from_pretrained("openai/whisper-base")
>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> inputs = feature_extractor(ds[0]["audio"]["array"], return_tensors="tf")
>>> input_features = inputs.input_features
>>> decoder_input_ids = tf.convert_to_tensor([[1, 1]]) * model.config.decoder_start_token_id
>>> last_hidden_state = model(input_features, decoder_input_ids=decoder_input_ids).last_hidden_state
>>> list(last_hidden_state.shape)
[1, 2, 512]
```

## TFWhisperForConditionalGeneration

### `class transformers.TFWhisperForConditionalGeneration`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_tf_whisper.py#L1346)

```py
( config: WhisperConfig **kwargs )
```

å‚æ•°

+   `config`ï¼ˆWhisperConfigï¼‰â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´çš„ Whisper æ¨¡å‹ã€‚å¯ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€‚è¯¥æ¨¡å‹ç»§æ‰¿è‡ª TFPreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¯¥æ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_tf_whisper.py#L1381)

```py
( input_features: TFModelInputType | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None decoder_position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None decoder_head_mask: np.ndarray | tf.Tensor | None = None cross_attn_head_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None decoder_inputs_embeds: Optional[Tuple[Union[np.ndarray, tf.Tensor]]] = None labels: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqLMOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `input_features`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`çš„`tf.Tensor`ï¼‰â€” ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„ fbank ç‰¹å¾çš„æµ®ç‚¹å€¼ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œ*ä¾‹å¦‚*é€šè¿‡ soundfile åº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚è¦å‡†å¤‡å¥½æ•°ç»„ä¸º`input_features`ï¼Œåº”ä½¿ç”¨ AutoFeatureExtractor æ¥æå– fbank ç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸º`tf.Tensor`ç±»å‹çš„å¼ é‡ã€‚å‚è§`call`()

+   `decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨`SpeechToTextTokenizer`è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚

    ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥ IDï¼Ÿ

    SpeechToText ä½¿ç”¨`eos_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™å¯èƒ½åªéœ€è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚

+   `decoder_attention_mask` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`, *optional*) â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥`decoder_input_ids`ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

    å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”é˜…è¯»`modeling_whisper._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨ 1ã€‚

+   `head_mask` (`tf.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*) â€” ç”¨äºä½¿ç¼–ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰æ‹©å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚

+   `decoder_head_mask` (`tf.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*) â€” ç”¨äºåœ¨è§£ç å™¨ä¸­ä½¿é€‰æ‹©çš„æ³¨æ„åŠ›æ¨¡å—çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`èŒƒå›´å†…ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚

+   `cross_attn_head_mask` (`tf.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*) â€” ç”¨äºä½¿äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰æ‹©å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`èŒƒå›´å†…ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚

+   `encoder_outputs` (`tuple(tuple(tf.Tensor)`, *optional*) â€” å…ƒç»„åŒ…æ‹¬ï¼ˆ`last_hidden_state`ï¼Œ*å¯é€‰*ï¼š`hidden_states`ï¼Œ*å¯é€‰*ï¼š`attentions`ï¼‰`last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `past_key_values` (`tuple(tuple(tf.Tensor))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›) â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(tf.Tensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¿™äº›æœªå°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`çš„å¼ é‡ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`decoder_input_ids`ã€‚

+   `decoder_inputs_embeds` (`tf.Tensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™å¯èƒ½åªéœ€è¾“å…¥æœ€åçš„`decoder_inputs_embeds`ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒï¼Œä»¥ä¾¿å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `use_cache` (`bool`, *optional*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè®¡ç®—è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”è¯¥åœ¨`[0, ..., config.vocab_size]`èŒƒå›´å†…ï¼Œæˆ–è€…ä¸º-100ï¼ˆå‚è§`input_ids`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰`[0, ..., config.vocab_size]`æ ‡ç­¾çš„æ ‡è®°ã€‚

è¿”å›

transformers.modeling_tf_outputs.TFSeq2SeqLMOutput æˆ–`tuple(tf.Tensor)`

ä¸€ä¸ª transformers.modeling_tf_outputs.TFSeq2SeqLMOutput æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆWhisperConfigï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(n,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼Œå…¶ä¸­ n æ˜¯æœªå±è”½æ ‡ç­¾çš„æ•°é‡ï¼‰- è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`tf.Tensor`ï¼‰- è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `past_key_values`ï¼ˆ`List[tf.Tensor]`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰- é•¿åº¦ä¸º`config.n_layers`çš„`tf.Tensor`åˆ—è¡¨ï¼Œæ¯ä¸ªå¼ é‡å½¢çŠ¶ä¸º`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`ã€‚

    åŒ…å«è§£ç å™¨çš„é¢„å…ˆè®¡ç®—éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚

+   `decoder_hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    è§£ç å™¨æ¯å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡ã€‚

+   `cross_attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡ã€‚

+   `encoder_last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    ç¼–ç å™¨æ¯å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions` (`tuple(tf.Tensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    ç¼–ç å™¨çš„æ³¨æ„æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

TFWhisperForConditionalGeneration çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import tensorflow as tf
>>> from transformers import AutoProcessor, TFWhisperForConditionalGeneration
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("openai/whisper-tiny.en")
>>> model = TFWhisperForConditionalGeneration.from_pretrained("openai/whisper-tiny.en")

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")

>>> inputs = processor(ds[0]["audio"]["array"], return_tensors="tf")
>>> input_features = inputs.input_features

>>> generated_ids = model.generate(input_features=input_features)

>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> transcription
' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'
```

JAXHide JAX content

## FlaxWhisperModel

### `class transformers.FlaxWhisperModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1185)

```py
( config: WhisperConfig input_shape: Tuple = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )
```

å‚æ•°

+   `config` (WhisperConfig) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

+   `dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) â€” è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯`jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨ GPU ä¸Šï¼‰å’Œ`jax.numpy.bfloat16`ï¼ˆåœ¨ TPU ä¸Šï¼‰ä¹‹ä¸€ã€‚è¿™å¯ä»¥ç”¨äºåœ¨ GPU æˆ– TPU ä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šäº†`dtype`ï¼Œåˆ™æ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„`dtype`æ‰§è¡Œã€‚**è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„ dtypeï¼Œä¸å½±å“æ¨¡å‹å‚æ•°çš„ dtypeã€‚**å¦‚æœæ‚¨å¸Œæœ›æ›´æ”¹æ¨¡å‹å‚æ•°çš„ dtypeï¼Œè¯·å‚é˜… to_fp16()å’Œ to_bf16()ã€‚

è£¸ Whisper æ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª FlaxPreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚æ­¤æ¨¡å‹è¿˜æ˜¯ Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ Flax æ¨¡å—ï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥äº†è§£æ‰€æœ‰ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„äº‹é¡¹ã€‚æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒå›ºæœ‰çš„ JAX åŠŸèƒ½ï¼Œä¾‹å¦‚ï¼š

+   [å³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1134)

```py
( input_features: Array decoder_input_ids: Array attention_mask: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_features`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`çš„`numpy.ndarray`ï¼‰- ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„æµ®ç‚¹å€¼ mel ç‰¹å¾ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°ç±»å‹ä¸º`List[float]`æˆ–`numpy.ndarray`çš„æ•°ç»„ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ`input_features`ï¼Œåº”ä½¿ç”¨ WhisperFeatureExtractor æ¥æå–ç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸ºç±»å‹ä¸º`numpy.ndarray`çš„å¼ é‡ã€‚å‚è§`call()`

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰- Whisper ä¸æ”¯æŒå¯¹`input_features`è¿›è¡Œæ©ç ï¼Œæ­¤å‚æ•°ä¿ç•™ä»¥ç¡®ä¿å…¼å®¹æ€§ï¼Œä½†ä¸ä¼šä½¿ç”¨ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¾“å…¥å¯¹æ•° mel é¢‘è°±å›¾ä¸­çš„é™éŸ³å°†è¢«å¿½ç•¥ã€‚

+   `decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰- è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥ä½¿ç”¨ WhisperTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call()`ã€‚ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥ IDï¼Ÿ Whisper ä½¿ç”¨`decoder_start_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚

+   `decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰- é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¼ é‡ï¼Œå¿½ç•¥`decoder_input_ids`ä¸­çš„å¡«å……æ ‡è®°ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨ 1ã€‚

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰- Whisper åœ¨ç¼–ç å™¨ä¸­ä¸ä½¿ç”¨`position_ids`ï¼Œå› ä¸º`input_features`å§‹ç»ˆå…·æœ‰ç›¸åŒçš„å¤§å°ä¸”ä¸ä½¿ç”¨æ©ç ï¼Œä½†æ­¤å‚æ•°ä¿ç•™ä»¥ç¡®ä¿å…¼å®¹æ€§ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¾“å…¥å¯¹æ•° mel é¢‘è°±å›¾ä¸­çš„é™éŸ³å°†è¢«å¿½ç•¥ã€‚

+   `decoder_position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰- æ¯ä¸ªè§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.max_position_embeddings - 1]`ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput æˆ–`tuple(torch.FloatTensor)`

transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆWhisperConfigï¼‰å’Œè¾“å…¥ã€‚

+   `last_hidden_state` (`å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`) â€” æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™åªè¾“å‡ºå½¢çŠ¶ä¸º`(batch_size, 1, hidden_size)`çš„åºåˆ—çš„æœ€åéšè—çŠ¶æ€ã€‚

+   `past_key_values` (`tuple(tuple(jnp.ndarray))`, *å¯é€‰çš„*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›) â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(jnp.ndarray)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `decoder_hidden_states` (`tuple(jnp.ndarray)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    è§£ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºæ—¶çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions` (`tuple(jnp.ndarray)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(jnp.ndarray)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state` (`å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`, *å¯é€‰çš„*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states` (`tuple(jnp.ndarray)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    ç¼–ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºæ—¶çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions` (`tuple(jnp.ndarray)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

`FlaxWhisperPreTrainedModel`çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹:

```py
>>> from transformers import AutoTokenizer, FlaxWhisperModel

>>> tokenizer = AutoTokenizer.from_pretrained("openai/whisper-tiny")
>>> model = FlaxWhisperModel.from_pretrained("openai/whisper-tiny")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## FlaxWhisperForConditionalGeneration

### `class transformers.FlaxWhisperForConditionalGeneration`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1267)

```py
( config: WhisperConfig input_shape: Tuple = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )
```

å‚æ•°

+   `config` (WhisperConfig) â€” æ¨¡å‹çš„æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

+   `dtype` (`jax.numpy.dtype`, *å¯é€‰*, é»˜è®¤ä¸º `jax.numpy.float32`) â€” è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯`jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨ GPU ä¸Šï¼‰å’Œ`jax.numpy.bfloat16`ï¼ˆåœ¨ TPU ä¸Šï¼‰ä¹‹ä¸€ã€‚è¿™å¯ä»¥ç”¨äºåœ¨ GPU æˆ– TPU ä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šäº†`dtype`ï¼Œåˆ™æ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„`dtype`æ‰§è¡Œã€‚**è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„æ•°æ®ç±»å‹ï¼Œä¸å½±å“æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ã€‚**å¦‚æœè¦æ›´æ”¹æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œè¯·å‚é˜… to_fp16()å’Œ to_bf16()ã€‚

å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´çš„ Whisper æ¨¡å‹ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª FlaxPreTrainedModelã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚æ­¤æ¨¡å‹è¿˜æ˜¯ Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ Flax æ¨¡å—ï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥äº†è§£ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒå†…åœ¨çš„ JAX åŠŸèƒ½ï¼Œä¾‹å¦‚ï¼š

+   [å³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1134)

```py
( input_features: Array decoder_input_ids: Array attention_mask: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_features` (`numpy.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`) â€” ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„æµ®ç‚¹å€¼æ¢…å°”ç‰¹å¾ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚è¦å‡†å¤‡æ•°ç»„ä¸º`input_features`ï¼Œåº”ä½¿ç”¨ WhisperFeatureExtractor æ¥æå–ç‰¹å¾ã€å¡«å……å’Œè½¬æ¢ä¸º`numpy.ndarray`ç±»å‹çš„å¼ é‡ã€‚å‚è§`call()`ã€‚

+   `attention_mask` (`numpy.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” Whisper ä¸æ”¯æŒ`input_features`çš„æ©ç ï¼Œæ­¤å‚æ•°ä¿ç•™ä»¥ç¡®ä¿å…¼å®¹æ€§ï¼Œä½†ä¸ä¼šä½¿ç”¨ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¾“å…¥å¯¹æ•°æ¢…å°”é¢‘è°±ä¸­çš„é™éŸ³å°†è¢«å¿½ç•¥ã€‚

+   `decoder_input_ids` (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*) â€” è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥ä½¿ç”¨ WhisperTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode()å’Œ PreTrainedTokenizer.`call`()ã€‚Whisper ä½¿ç”¨`decoder_start_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚

+   `decoder_attention_mask` (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*) â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¼ é‡ï¼Œå¿½ç•¥`decoder_input_ids`ä¸­çš„å¡«å……æ ‡è®°ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨ 1ã€‚

+   `position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*) â€” Whisper åœ¨ç¼–ç å™¨ä¸­ä¸ä½¿ç”¨`position_ids`ï¼Œå› ä¸º`input_features`å§‹ç»ˆå…·æœ‰ç›¸åŒçš„å¤§å°å¹¶ä¸”ä¸ä½¿ç”¨æ©ç ï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ä¿ç•™äº†è¿™ä¸ªå‚æ•°ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¾“å…¥å¯¹æ•°æ¢…å°”é¢‘è°±ä¸­çš„é™éŸ³å°†è¢«å¿½ç•¥ã€‚

+   `decoder_position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*) â€” è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.max_position_embeddings - 1]`ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput æˆ–`tuple(torch.FloatTensor)`

åŒ…å«æ ¹æ®é…ç½®ï¼ˆWhisperConfigï¼‰å’Œè¾“å…¥è€Œç»„æˆçš„å„ç§å…ƒç´ çš„ transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰ã€‚

+   `logits` (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `past_key_values` (`tuple(tuple(jnp.ndarray))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›) â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(jnp.ndarray)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„å…·æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚

+   `decoder_hidden_states` (`tuple(jnp.ndarray)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    è§£ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions` (`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states` (`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    ç¼–ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

`FlaxWhisperPreTrainedModel`çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

è½¬å½•ç¤ºä¾‹ï¼š

```py
>>> from transformers import WhisperProcessor, FlaxWhisperForConditionalGeneration
>>> from datasets import load_dataset

>>> processor = WhisperProcessor.from_pretrained("openai/whisper-tiny.en")
>>> model = FlaxWhisperForConditionalGeneration.from_pretrained("openai/whisper-tiny.en", from_pt=True)
>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> inputs = processor(ds[0]["audio"]["array"], return_tensors="np")
>>> input_features = inputs.input_features
>>> generated_ids = model.generate(input_ids=input_features)
>>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> transcription
' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'
```

## FlaxWhisperForAudioClassification

### `class transformers.FlaxWhisperForAudioClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1597)

```py
( config: WhisperConfig input_shape: Tuple = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )
```

å‚æ•°

+   `config`ï¼ˆWhisperConfigï¼‰ â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

+   `dtype` (`jax.numpy.dtype`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`jax.numpy.float32`) â€” è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯`jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨ GPU ä¸Šï¼‰å’Œ`jax.numpy.bfloat16`ï¼ˆåœ¨ TPU ä¸Šï¼‰ä¸­çš„ä¸€ç§ã€‚è¿™å¯ä»¥ç”¨äºåœ¨ GPU æˆ– TPU ä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šäº†`dtype`ï¼Œåˆ™æ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„`dtype`æ‰§è¡Œã€‚**è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„æ•°æ®ç±»å‹ï¼Œä¸å½±å“æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ã€‚**å¦‚æœæ‚¨å¸Œæœ›æ›´æ”¹æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œè¯·å‚é˜… to_fp16()å’Œ to_bf16()ã€‚

å¸¦æœ‰é¡¶éƒ¨éŸ³é¢‘åˆ†ç±»å¤´çš„ Whisper æ¨¡å‹ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª FlaxPreTrainedModelã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚æ­¤æ¨¡å‹è¿˜æ˜¯ Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ Flax æ¨¡å—ï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒå†…åœ¨çš„ JAX åŠŸèƒ½ï¼Œä¾‹å¦‚ï¼š

+   [å³æ—¶ (JIT) ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1625)

```py
( input_features: Array attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None **kwargs ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_features` (`numpy.ndarray` of shape `(batch_size, feature_size, sequence_length)`) â€” ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„æµ®ç‚¹å€¼ mel ç‰¹å¾ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°† `.flac` æˆ– `.wav` éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°ç±»å‹ä¸º `List[float]` æˆ– `numpy.ndarray` çš„æ•°ç»„ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ (`pip install soundfile`)ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ `input_features`ï¼Œåº”ä½¿ç”¨ WhisperFeatureExtractor æ¥æå–ç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸ºç±»å‹ä¸º `numpy.ndarray` çš„å¼ é‡ã€‚å‚è§ `call`()

+   `attention_mask` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *å¯é€‰*) â€” Whisper ä¸æ”¯æŒå¯¹ `input_features` è¿›è¡Œæ©ç ï¼Œæ­¤å‚æ•°ä¿ç•™äº†å…¼å®¹æ€§ï¼Œä½†ä¸ä¼šä½¿ç”¨ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¾“å…¥å¯¹æ•° mel é¢‘è°±å›¾ä¸­çš„é™éŸ³ä¼šè¢«å¿½ç•¥ã€‚

+   `decoder_input_ids` (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`, *å¯é€‰*) â€” è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥ä½¿ç”¨ WhisperTokenizer è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… PreTrainedTokenizer.encode() å’Œ PreTrainedTokenizer.`call`()ã€‚ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥ IDï¼Ÿ Whisper ä½¿ç”¨ `decoder_start_token_id` ä½œä¸º `decoder_input_ids` ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚

+   `decoder_attention_mask` (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`, *å¯é€‰*) â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥ `decoder_input_ids` ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚é»˜è®¤æƒ…å†µä¸‹è¿˜å°†ä½¿ç”¨å› æœæ©ç ã€‚å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§ [è®ºæ–‡](https://arxiv.org/abs/1910.13461) ä¸­çš„å›¾è¡¨ 1ã€‚

+   `position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *å¯é€‰*) â€” Whisper åœ¨ç¼–ç å™¨ä¸­ä¸ä½¿ç”¨ `position_ids`ï¼Œå› ä¸º `input_features` æ€»æ˜¯ç›¸åŒå¤§å°ä¸”ä¸ä½¿ç”¨æ©ç ï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ä¿ç•™äº†æ­¤å‚æ•°ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¾“å…¥å¯¹æ•° mel é¢‘è°±å›¾ä¸­çš„é™éŸ³ä¼šè¢«å¿½ç•¥ã€‚

+   `decoder_position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *å¯é€‰*) â€” æ¯ä¸ªè§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º `[0, config.max_position_embeddings - 1]`ã€‚

+   `output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

è¿”å›

transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆWhisperConfigï¼‰å’Œè¾“å…¥ã€‚

+   `logits` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`) â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`, *å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

FlaxWhisperForAudioClassification çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

è½¬å½•ç¤ºä¾‹ï¼š

```py
>>> import jax.numpy as jnp
>>> from transformers import AutoFeatureExtractor, FlaxWhisperForAudioClassification
>>> from datasets import load_dataset

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("sanchit-gandhi/whisper-medium-fleurs-lang-id")
>>> model = FlaxWhisperForAudioClassification.from_pretrained(
...     "sanchit-gandhi/whisper-medium-fleurs-lang-id", from_pt=True
... )
>>> ds = load_dataset("google/fleurs", "all", split="validation", streaming=True)

>>> sample = next(iter(ds))

>>> inputs = feature_extractor(
...     sample["audio"]["array"], sampling_rate=sample["audio"]["sampling_rate"], return_tensors="np"
... )
>>> input_features = inputs.input_features

>>> logits = model(input_features).logits

>>> predicted_class_ids = jnp.argmax(logits).item()
>>> predicted_label = model.config.id2label[predicted_class_ids]
>>> predicted_label
'af_za'
```
