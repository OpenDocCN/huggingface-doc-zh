- en: Hands-on
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit3/hands-on](https://huggingface.co/learn/deep-rl-course/unit3/hands-on)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[![Ask
    a Question](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit3/unit3.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve studied the theory behind Deep Q-Learning, **you’re ready to
    train your Deep Q-Learning agent to play Atari Games**. We’ll start with Space
    Invaders, but you’ll be able to use any Atari game you want 🔥
  prefs: []
  type: TYPE_NORMAL
- en: '![Environments](../Images/bf441b005cda192d0dc86eb42475aeb3.png)'
  prefs: []
  type: TYPE_IMG
- en: We’re using the [RL-Baselines-3 Zoo integration](https://github.com/DLR-RM/rl-baselines3-zoo),
    a vanilla version of Deep Q-Learning with no extensions such as Double-DQN, Dueling-DQN,
    or Prioritized Experience Replay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, **if you want to learn to implement Deep Q-Learning by yourself after
    this hands-on**, you definitely should look at the CleanRL implementation: [https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py)'
  prefs: []
  type: TYPE_NORMAL
- en: To validate this hands-on for the certification process, you need to push your
    trained model to the Hub and **get a result of >= 200**.
  prefs: []
  type: TYPE_NORMAL
- en: To find your result, go to the leaderboard and find your model, **the result
    = mean_reward - std of reward**
  prefs: []
  type: TYPE_NORMAL
- en: '**If you don’t find your model, go to the bottom of the page and click on the
    refresh button.**'
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the certification process, check this section 👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  prefs: []
  type: TYPE_NORMAL
- en: And you can check your progress here 👉 [https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)
  prefs: []
  type: TYPE_NORMAL
- en: '**To start the hands-on click on Open In Colab button** 👇 :'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit3/unit3.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unit 3: Deep Q-Learning with Atari Games 👾 using RL Baselines3 Zoo'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Unit 3 Thumbnail](../Images/e8420d1d9f22aa4095ae8b961c412a91.png)'
  prefs: []
  type: TYPE_IMG
- en: In this hands-on, **you’ll train a Deep Q-Learning agent** playing Space Invaders
    using [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo), a training
    framework based on [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)
    that provides scripts for training, evaluating agents, tuning hyperparameters,
    plotting results and recording videos.
  prefs: []
  type: TYPE_NORMAL
- en: We’re using the [RL-Baselines-3 Zoo integration, a vanilla version of Deep Q-Learning](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)
    with no extensions such as Double-DQN, Dueling-DQN, and Prioritized Experience
    Replay.
  prefs: []
  type: TYPE_NORMAL
- en: '🎮 Environments:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[SpacesInvadersNoFrameskip-v4](https://gymnasium.farama.org/environments/atari/space_invaders/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can see the difference between Space Invaders versions here 👉 [https://gymnasium.farama.org/environments/atari/space_invaders/#variants](https://gymnasium.farama.org/environments/atari/space_invaders/#variants)
  prefs: []
  type: TYPE_NORMAL
- en: '📚 RL-Library:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[RL-Baselines3-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objectives of this hands-on 🏆
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the end of the hands-on, you will:'
  prefs: []
  type: TYPE_NORMAL
- en: Be able to understand deeper **how RL Baselines3 Zoo works**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to **push your trained agent and the code to the Hub** with a nice video
    replay and an evaluation score 🔥.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prerequisites 🏗️
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before diving into the hands-on, you need to:'
  prefs: []
  type: TYPE_NORMAL
- en: 🔲 📚 **[Study Deep Q-Learning by reading Unit 3](https://huggingface.co/deep-rl-course/unit3/introduction)**
    🤗
  prefs: []
  type: TYPE_NORMAL
- en: We’re constantly trying to improve our tutorials, so **if you find some issues
    in this hands-on**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s train a Deep Q-Learning agent playing Atari’ Space Invaders 👾 and upload
    it to the Hub.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We strongly recommend students **to use Google Colab for the hands-on exercises
    instead of running them on their personal computers**.
  prefs: []
  type: TYPE_NORMAL
- en: By using Google Colab, **you can focus on learning and experimenting without
    worrying about the technical aspects of setting up your environments**.
  prefs: []
  type: TYPE_NORMAL
- en: To validate this hands-on for the certification process, you need to push your
    trained model to the Hub and **get a result of >= 200**.
  prefs: []
  type: TYPE_NORMAL
- en: To find your result, go to the leaderboard and find your model, **the result
    = mean_reward - std of reward**
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the certification process, check this section 👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  prefs: []
  type: TYPE_NORMAL
- en: Set the GPU 💪
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To **accelerate the agent’s training, we’ll use a GPU**. To do that, go to `Runtime
    > Change Runtime type`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![GPU Step 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
  prefs: []
  type: TYPE_IMG
- en: '`Hardware Accelerator > GPU`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![GPU Step 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
  prefs: []
  type: TYPE_IMG
- en: Install RL-Baselines3 Zoo and its dependencies 📚
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you see `ERROR: pip''s dependency resolver does not currently take into
    account all the packages that are installed.` **this is normal and it’s not a
    critical error** there’s a conflict of version. But the packages we need are installed.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: IF AND ONLY IF THE VERSION ABOVE DOES NOT EXIST ANYMORE. UNCOMMENT AND INSTALL
    THE ONE BELOW
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: To be able to use Atari games in Gymnasium we need to install atari package.
    And accept-rom-license to download the rom files (games files).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Create a virtual display 🔽
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During the hands-on, we’ll need to generate a replay video. To do so, if you
    train it on a headless machine, **we need to have a virtual screen to be able
    to render the environment** (and thus record the frames).
  prefs: []
  type: TYPE_NORMAL
- en: Hence the following cell will install the librairies and create and run a virtual
    screen 🖥
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Train our Deep Q-Learning Agent to Play Space Invaders 👾
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To train an agent with RL-Baselines3-Zoo, we just need to do two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a hyperparameter config file that will contain our training hyperparameters
    called `dqn.yml`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is a template example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we see that:'
  prefs: []
  type: TYPE_NORMAL
- en: We use the `Atari Wrapper` that preprocess the input (Frame reduction ,grayscale,
    stack 4 frames)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use `CnnPolicy`, since we use Convolutional layers to process the frames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We train it for 10 million `n_timesteps`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory (Experience Replay) size is 100000, aka the amount of experience steps
    you saved to train again your agent with.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '💡 My advice is to **reduce the training timesteps to 1M,** which will take
    about 90 minutes on a P100\. `!nvidia-smi` will tell you what GPU you’re using.
    At 10 million steps, this will take about 9 hours. I recommend running this on
    your local computer (or somewhere else). Just click on: `File>Download`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of hyperparameters optimization, my advice is to focus on these 3
    hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`learning_rate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`buffer_size (Experience Memory size)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As a good practice, you need to **check the documentation to understand what
    each hyperparameters does**: [https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters)'
  prefs: []
  type: TYPE_NORMAL
- en: We start the training and save the models on `logs` folder 📁
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the algorithm after `--algo`, where we save the model after `-f` and
    where the hyperparameter config is after `-c`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let’s evaluate our agent 👀
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RL-Baselines3-Zoo provides `enjoy.py`, a python script to evaluate our agent.
    In most RL libraries, we call the evaluation script `enjoy.py`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s evaluate it for 5000 timesteps 🔥
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Publish our trained model on the Hub 🚀
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we saw we got good results after the training, we can publish our trained
    model on the hub 🤗 with one line of code.
  prefs: []
  type: TYPE_NORMAL
- en: '![Space Invaders model](../Images/8ae15c5c6ac1a3242637770ca390f7e1.png)'
  prefs: []
  type: TYPE_IMG
- en: By using `rl_zoo3.push_to_hub` **you evaluate, record a replay, generate a model
    card of your agent and push it to the hub**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This way:'
  prefs: []
  type: TYPE_NORMAL
- en: You can **showcase our work** 🔥
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can **visualize your agent playing** 👀
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can **share with the community an agent that others can use** 💾
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can **access a leaderboard 🏆 to see how well your agent is performing compared
    to your classmates** 👉 [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To be able to share your model with the community there are three more steps
    to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: 1️⃣ (If it’s not already done) create an account to HF ➡ [https://huggingface.co/join](https://huggingface.co/join)
  prefs: []
  type: TYPE_NORMAL
- en: 2️⃣ Sign in and then, you need to store your authentication token from the Hugging
    Face website.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **with write role**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  prefs: []
  type: TYPE_IMG
- en: Copy the token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the cell below and past the token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If you don’t want to use a Google Colab or a Jupyter Notebook, you need to
    use this command instead: `huggingface-cli login`'
  prefs: []
  type: TYPE_NORMAL
- en: 3️⃣ We’re now ready to push our trained agent to the 🤗 Hub 🔥
  prefs: []
  type: TYPE_NORMAL
- en: Let’s run push_to_hub.py file to upload our trained agent to the Hub.
  prefs: []
  type: TYPE_NORMAL
- en: '`--repo-name` : The name of the repo'
  prefs: []
  type: TYPE_NORMAL
- en: '`-orga`: Your Hugging Face username'
  prefs: []
  type: TYPE_NORMAL
- en: '`-f`: Where the trained model folder is (in our case `logs`)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Select Id](../Images/6f04c3f40368af928c98d0979b5abbe5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '###.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Congrats 🥳 you’ve just trained and uploaded your first Deep Q-Learning agent
    using RL-Baselines-3 Zoo. The script above should have displayed a link to a model
    repository such as [https://huggingface.co/ThomasSimonini/dqn-SpaceInvadersNoFrameskip-v4](https://huggingface.co/ThomasSimonini/dqn-SpaceInvadersNoFrameskip-v4).
    When you go to this link, you can:'
  prefs: []
  type: TYPE_NORMAL
- en: See a **video preview of your agent** at the right.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click “Files and versions” to see all the files in the repository.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click “Use in stable-baselines3” to get a code snippet that shows how to load
    the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model card (`README.md` file) which gives a description of the model and the
    hyperparameters you used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the hood, the Hub uses git-based repositories (don’t worry if you don’t
    know what git is), which means you can update the model with new versions as you
    experiment and improve your agent.
  prefs: []
  type: TYPE_NORMAL
- en: '**Compare the results of your agents with your classmates** using the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    🏆'
  prefs: []
  type: TYPE_NORMAL
- en: Load a powerful trained model 🔥
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Stable-Baselines3 team uploaded **more than 150 trained Deep Reinforcement
    Learning agents on the Hub**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find them here: 👉 [https://huggingface.co/sb3](https://huggingface.co/sb3)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Asteroids: [https://huggingface.co/sb3/dqn-AsteroidsNoFrameskip-v4](https://huggingface.co/sb3/dqn-AsteroidsNoFrameskip-v4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beam Rider: [https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4](https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Breakout: [https://huggingface.co/sb3/dqn-BreakoutNoFrameskip-v4](https://huggingface.co/sb3/dqn-BreakoutNoFrameskip-v4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Road Runner: [https://huggingface.co/sb3/dqn-RoadRunnerNoFrameskip-v4](https://huggingface.co/sb3/dqn-RoadRunnerNoFrameskip-v4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s load an agent playing Beam Rider: [https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4](https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4)'
  prefs: []
  type: TYPE_NORMAL
- en: We download the model using `rl_zoo3.load_from_hub`, and place it in a new folder
    that we can call `rl_trained`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Let’s evaluate if for 5000 timesteps
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Why not trying to train your own **Deep Q-Learning Agent playing BeamRiderNoFrameskip-v4?
    🏆.**
  prefs: []
  type: TYPE_NORMAL
- en: If you want to try, check [https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4#hyperparameters](https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4#hyperparameters)
    **in the model card, you have the hyperparameters of the trained agent.**
  prefs: []
  type: TYPE_NORMAL
- en: But finding hyperparameters can be a daunting task. Fortunately, we’ll see in
    the next Unit, how we can **use Optuna for optimizing the Hyperparameters 🔥.**
  prefs: []
  type: TYPE_NORMAL
- en: Some additional challenges 🏆
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best way to learn **is to try things by your own**!
  prefs: []
  type: TYPE_NORMAL
- en: In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    you will find your agents. Can you get to the top?
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a list of environments you can try to train your agent with:'
  prefs: []
  type: TYPE_NORMAL
- en: BeamRiderNoFrameskip-v4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BreakoutNoFrameskip-v4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EnduroNoFrameskip-v4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PongNoFrameskip-v4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, **if you want to learn to implement Deep Q-Learning by yourself**, you
    definitely should look at CleanRL implementation: [https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Environments](../Images/bf441b005cda192d0dc86eb42475aeb3.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Congrats on finishing this chapter!
  prefs: []
  type: TYPE_NORMAL
- en: If you’re still feel confused with all these elements…it’s totally normal! **This
    was the same for me and for all people who studied RL.**
  prefs: []
  type: TYPE_NORMAL
- en: Take time to really **grasp the material before continuing and try the additional
    challenges**. It’s important to master these elements and having a solid foundations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next unit, **we’re going to learn about [Optuna](https://optuna.org/)**.
    One of the most critical task in Deep Reinforcement Learning is to find a good
    set of training hyperparameters. And Optuna is a library that helps you to automate
    the search.
  prefs: []
  type: TYPE_NORMAL
- en: This is a course built with you 👷🏿‍♀️
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, we want to improve and update the course iteratively with your feedback.
    If you have some, please fill this form 👉 [https://forms.gle/3HgA7bEHwAmmLfwh9](https://forms.gle/3HgA7bEHwAmmLfwh9)
  prefs: []
  type: TYPE_NORMAL
- en: We’re constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues).
  prefs: []
  type: TYPE_NORMAL
- en: See you on Bonus unit 2! 🔥
  prefs: []
  type: TYPE_NORMAL
- en: Keep Learning, Stay Awesome 🤗
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
