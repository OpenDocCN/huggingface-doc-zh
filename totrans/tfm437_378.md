# X-CLIP

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xclip](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xclip)

## 概述

X-CLIP模型是由Bolin Ni、Houwen Peng、Minghao Chen、Songyang Zhang、Gaofeng Meng、Jianlong Fu、Shiming Xiang、Haibin Ling在[扩展语言-图像预训练模型以用于通用视频识别](https://arxiv.org/abs/2208.02816)中提出的。X-CLIP是对视频的[CLIP](clip)的最小扩展。该模型包括文本编码器、跨帧视觉编码器、多帧集成Transformer和视频特定提示生成器。

论文摘要如下：

*对比语言-图像预训练在从网络规模数据中学习视觉-文本联合表示方面取得了巨大成功，展示了在各种图像任务中出色的“零样本”泛化能力。然而，如何有效地将这种新的语言-图像预训练方法扩展到视频领域仍然是一个悬而未决的问题。在这项工作中，我们提出了一种简单而有效的方法，将预训练的语言-图像模型直接适应于视频识别，而不是从头开始预训练一个新模型。更具体地说，为了捕捉沿时间维度的帧之间的长距离依赖关系，我们提出了一个跨帧注意机制，明确地在帧之间交换信息。这种模块轻量级且可以无缝地插入预训练的语言-图像模型中。此外，我们提出了一个视频特定的提示方案，利用视频内容信息生成具有区分性的文本提示。大量实验证明我们的方法是有效的，并且可以推广到不同的视频识别场景。特别是，在完全监督的设置下，我们的方法在Kinectics-400上实现了87.1%的top-1准确率，与Swin-L和ViViT-H相比，FLOPs减少了12倍。在零样本实验中，我们的方法在两种流行协议下的top-1准确率方面超过了当前最先进的方法+7.6%和+14.9%。在少样本场景下，我们的方法在标记数据极为有限时，比以前最佳方法提高了+32.1%和+23.1%。*

提示：

+   X-CLIP的使用与[CLIP](clip)相同。

![drawing](../Images/1794371ecfc5bc26b439719f0a1fce20.png) X-CLIP架构。摘自[原始论文。](https://arxiv.org/abs/2208.02816)

该模型由[nielsr](https://huggingface.co/nielsr)贡献。原始代码可以在[这里](https://github.com/microsoft/VideoX/tree/master/X-CLIP)找到。

## 资源

以下是官方Hugging Face和社区（由🌎表示）资源列表，可帮助您开始使用X-CLIP。

+   X-CLIP的演示笔记本可以在[这里](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/X-CLIP)找到。

如果您有兴趣提交资源以包含在此处，请随时提交拉取请求，我们将对其进行审查！资源应该理想地展示一些新内容，而不是重复现有资源。

## XCLIP处理器

### `class transformers.XCLIPProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/x_clip/processing_x_clip.py#L25)

```py
( image_processor = None tokenizer = None **kwargs )
```

参数

+   `image_processor`（[VideoMAEImageProcessor](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEImageProcessor)，*可选*）— 图像处理器是必需的输入。

+   `tokenizer`（[CLIPTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizerFast)，*可选*）— Tokenizer是必需的输入。

构建一个X-CLIP处理器，将VideoMAE图像处理器和CLIP tokenizer包装成一个单一处理器。

[XCLIPProcessor](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPProcessor) 提供了 [VideoMAEImageProcessor](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEImageProcessor) 和 [CLIPTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizerFast) 的所有功能。查看 `__call__()` 和 [decode()](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPProcessor.decode) 获取更多信息。

#### `batch_decode`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/x_clip/processing_x_clip.py#L116)

```py
( *args **kwargs )
```

此方法将所有参数转发到 CLIPTokenizerFast 的 [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。请参考此方法的文档字符串以获取更多信息。

#### `decode`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/x_clip/processing_x_clip.py#L123)

```py
( *args **kwargs )
```

此方法将所有参数转发到 CLIPTokenizerFast 的 [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。请参考此方法的文档字符串以获取更多信息。

## XCLIPConfig

### `class transformers.XCLIPConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/x_clip/configuration_x_clip.py#L265)

```py
( text_config = None vision_config = None projection_dim = 512 prompt_layers = 2 prompt_alpha = 0.1 prompt_hidden_act = 'quick_gelu' prompt_num_attention_heads = 8 prompt_attention_dropout = 0.0 prompt_projection_dropout = 0.0 logit_scale_init_value = 2.6592 **kwargs )
```

参数

+   `text_config` (`dict`, *optional*) — 用于初始化 [XCLIPTextConfig](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPTextConfig) 的配置选项字典。

+   `vision_config` (`dict`, *optional*) — 用于初始化 [XCLIPVisionConfig](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPVisionConfig) 的配置选项字典。

+   `projection_dim` (`int`, *optional*, 默认为 512) — 文本和视觉投影层的维度。

+   `prompt_layers` (`int`, *optional*, 默认为 2) — 视频特定提示生成器中的层数。

+   `prompt_alpha` (`float`, *optional*, 默认为 0.1) — 视频特定提示生成器中使用的 Alpha 值。

+   `prompt_hidden_act` (`str` 或 `function`, *optional*, 默认为 `"quick_gelu"`) — 视频特定提示生成器中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`, `"relu"`, `"selu"` 和 `"gelu_new"` 以及 `"quick_gelu"`。

+   `prompt_num_attention_heads` (`int`, *optional*, 默认为 8) — 视频特定提示生成器中交叉注意力的注意力头数。

+   `prompt_attention_dropout` (`float`, *optional*, 默认为 0.0) — 视频特定提示生成器中注意力层的丢弃概率。

+   `prompt_projection_dropout` (`float`, *optional*, 默认为 0.0) — 视频特定提示生成器中投影层的丢弃概率。

+   `logit_scale_init_value` (`float`, *optional*, 默认为 2.6592) — *logit_scale* 参数的初始值。默认值与原始 XCLIP 实现相同。

+   `kwargs` (*optional*) — 关键字参数字典。

[XCLIPConfig](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPConfig) 是用来存储 [XCLIPModel](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPModel) 配置的配置类。它用于根据指定的参数实例化 X-CLIP 模型，定义文本模型和视觉模型的配置。使用默认值实例化配置将产生类似于 X-CLIP [microsoft/xclip-base-patch32](https://huggingface.co/microsoft/xclip-base-patch32) 架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

#### `from_text_vision_configs`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/x_clip/configuration_x_clip.py#L408)

```py
( text_config: XCLIPTextConfig vision_config: XCLIPVisionConfig **kwargs ) → export const metadata = 'undefined';XCLIPConfig
```

返回

[XCLIPConfig](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPConfig)

配置对象的一个实例

从xclip文本模型配置和xclip视觉模型配置实例化一个[XCLIPConfig](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPConfig)（或派生类）。

## XCLIPTextConfig

### `class transformers.XCLIPTextConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/x_clip/configuration_x_clip.py#L31)

```py
( vocab_size = 49408 hidden_size = 512 intermediate_size = 2048 num_hidden_layers = 12 num_attention_heads = 8 max_position_embeddings = 77 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 49408) — X-CLIP文本模型的词汇表大小。定义了在调用[XCLIPModel](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPModel)时可以表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, defaults to 512) — 编码器层和池化层的维度。

+   `intermediate_size` (`int`, *optional*, defaults to 2048) — Transformer编码器中“中间”（即前馈）层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer编码器中隐藏层的数量。

+   `num_attention_heads` (`int`, *optional*, defaults to 8) — Transformer编码器中每个注意力层的注意力头数。

+   `max_position_embeddings` (`int`, *optional*, defaults to 77) — 此模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如512、1024或2048）。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"quick_gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"` `"quick_gelu"`。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-5) — 层归一化层使用的epsilon。

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的dropout比率。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `initializer_factor` (`float`, *optional*, defaults to 1) — 用于初始化所有权重矩阵的因子（应保持为1，用于内部初始化测试）。

这是一个配置类，用于存储[XCLIPModel](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPModel)的配置。根据指定的参数实例化一个X-CLIP模型，定义模型架构。使用默认值实例化配置将产生类似于X-CLIP [microsoft/xclip-base-patch32](https://huggingface.co/microsoft/xclip-base-patch32)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import XCLIPTextModel, XCLIPTextConfig

>>> # Initializing a XCLIPTextModel with microsoft/xclip-base-patch32 style configuration
>>> configuration = XCLIPTextConfig()

>>> # Initializing a XCLIPTextConfig from the microsoft/xclip-base-patch32 style configuration
>>> model = XCLIPTextModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## XCLIPVisionConfig

### `class transformers.XCLIPVisionConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/x_clip/configuration_x_clip.py#L138)

```py
( hidden_size = 768 intermediate_size = 3072 num_hidden_layers = 12 num_attention_heads = 12 mit_hidden_size = 512 mit_intermediate_size = 2048 mit_num_hidden_layers = 1 mit_num_attention_heads = 8 num_channels = 3 image_size = 224 patch_size = 32 num_frames = 8 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 drop_path_rate = 0.0 **kwargs )
```

参数

+   `hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化层的维度。

+   `intermediate_size` (`int`, *optional*, 默认为3072) — Transformer 编码器中“中间”（即前馈）层的维度。

+   `num_hidden_layers` (`int`, *optional*, 默认为12) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, 默认为12) — Transformer 编码器中每个注意力层的注意力头数。

+   `mit_hidden_size` (`int`, *optional*, 默认为512) — Multiframe Integration Transformer (MIT) 编码器层的维度。

+   `mit_intermediate_size` (`int`, *optional*, 默认为2048) — Multiframe Integration Transformer (MIT) 中“中间”（即前馈）层的维度。

+   `mit_num_hidden_layers` (`int`, *optional*, 默认为1) — Multiframe Integration Transformer (MIT) 中的隐藏层数量。

+   `mit_num_attention_heads` (`int`, *optional*, 默认为8) — Multiframe Integration Transformer (MIT) 中每个注意力层的注意力头数。

+   `image_size` (`int`, *optional*, 默认为224) — 每个图像的大小（分辨率）。

+   `patch_size` (`int`, *optional*, 默认为32) — 每个补丁的大小（分辨率）。

+   `hidden_act` (`str` 或 `function`, *optional*, 默认为`"quick_gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`, `"relu"`, `"selu"`, `"gelu_new"` 和 `"quick_gelu"`。

+   `layer_norm_eps` (`float`, *optional*, 默认为1e-5) — 层归一化层使用的 epsilon。

+   `attention_dropout` (`float`, *optional*, 默认为0.0) — 注意力概率的丢弃比率。

+   `initializer_range` (`float`, *optional*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `initializer_factor` (`float`, *optional*, 默认为1) — 用于初始化所有权重矩阵的因子（应保持为1，仅用于初始化测试内部使用）。

+   `drop_path_rate` (`float`, *optional*, 默认为0.0) — 随机深度率。

这是用于存储 [XCLIPModel](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPModel) 配置的配置类。它用于根据指定的参数实例化 X-CLIP 模型，定义模型架构。使用默认值实例化配置将产生类似于 X-CLIP [microsoft/xclip-base-patch32](https://huggingface.co/microsoft/xclip-base-patch32) 架构的配置。

配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 的文档以获取更多信息。

示例：

```py
>>> from transformers import XCLIPVisionModel, XCLIPVisionConfig

>>> # Initializing a XCLIPVisionModel with microsoft/xclip-base-patch32 style configuration
>>> configuration = XCLIPVisionConfig()

>>> # Initializing a XCLIPVisionModel model from the microsoft/xclip-base-patch32 style configuration
>>> model = XCLIPVisionModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## XCLIPModel

### `class transformers.XCLIPModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/x_clip/modeling_x_clip.py#L1243)

```py
( config: XCLIPConfig )
```

参数

+   `config` ([XCLIPConfig](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPConfig)) — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

该模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/x_clip/modeling_x_clip.py#L1453)

```py
( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None position_ids: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.x_clip.modeling_x_clip.XCLIPOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    输入ID是什么？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）- 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   位置ID是什么？

    +   对于被“掩盖”的标记，位置ID为0。

    注意掩码是什么？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    位置ID是什么？

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）- 像素值。默认情况下将忽略填充。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `return_loss`（`bool`，*可选*）- 是否返回对比损失。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.models.x_clip.modeling_x_clip.XCLIPOutput`或`tuple(torch.FloatTensor)`

`transformers.models.x_clip.modeling_x_clip.XCLIPOutput`或一组`torch.FloatTensor`（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（`<class 'transformers.models.x_clip.configuration_x_clip.XCLIPConfig'>`）和输入的各种元素。对于被“掩盖”的标记，位置ID为1。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当`return_loss`为`True`时返回）- 视频文本相似性的对比损失。

+   `logits_per_video`（形状为`(video_batch_size, text_batch_size)`的`torch.FloatTensor`）- `video_embeds`和`text_embeds`之间的缩放点积分数。这代表视频文本相似性分数。

+   `logits_per_text`（形状为`(text_batch_size, video_batch_size)`的`torch.FloatTensor`）- `text_embeds`和`video_embeds`之间的缩放点积分数。这代表文本视频相似性分数。

+   “text_embeds”（形状为`(batch_size, output_dim)`的`torch.FloatTensor`）- 通过将[XCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPTextModel)的池化输出应用到投影层获得的文本嵌入。

+   “video_embeds”（形状为`(batch_size, output_dim)`的`torch.FloatTensor`）- 通过将[XCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPVisionModel)的池化输出应用到投影层获得的视频嵌入。

+   `text_model_output`（`BaseModelOutputWithPooling`）- [XCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPTextModel)的输出。

+   `vision_model_output` (`BaseModelOutputWithPooling`) — [XCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPVisionModel)的输出。

+   `mit_output` (`BaseModelOutputWithPooling`) — `XCLIPMultiframeIntegrationTransformer` (MIT的简称)的输出。

[XCLIPModel](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import av
>>> import torch
>>> import numpy as np

>>> from transformers import AutoProcessor, AutoModel
>>> from huggingface_hub import hf_hub_download

>>> np.random.seed(0)

>>> def read_video_pyav(container, indices):
...     '''
...     Decode the video with PyAV decoder.
...     Args:
...         container (`av.container.input.InputContainer`): PyAV container.
...         indices (`List[int]`): List of frame indices to decode.
...     Returns:
...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).
...     '''
...     frames = []
...     container.seek(0)
...     start_index = indices[0]
...     end_index = indices[-1]
...     for i, frame in enumerate(container.decode(video=0)):
...         if i > end_index:
...             break
...         if i >= start_index and i in indices:
...             frames.append(frame)
...     return np.stack([x.to_ndarray(format="rgb24") for x in frames])

>>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):
...     '''
...     Sample a given number of frame indices from the video.
...     Args:
...         clip_len (`int`): Total number of frames to sample.
...         frame_sample_rate (`int`): Sample every n-th frame.
...         seg_len (`int`): Maximum allowed index of sample's last frame.
...     Returns:
...         indices (`List[int]`): List of sampled frame indices
...     '''
...     converted_len = int(clip_len * frame_sample_rate)
...     end_idx = np.random.randint(converted_len, seg_len)
...     start_idx = end_idx - converted_len
...     indices = np.linspace(start_idx, end_idx, num=clip_len)
...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)
...     return indices

>>> # video clip consists of 300 frames (10 seconds at 30 FPS)
>>> file_path = hf_hub_download(
...     repo_id="nielsr/video-demo", filename="eating_spaghetti.mp4", repo_type="dataset"
... )
>>> container = av.open(file_path)

>>> # sample 8 frames
>>> indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)
>>> video = read_video_pyav(container, indices)

>>> processor = AutoProcessor.from_pretrained("microsoft/xclip-base-patch32")
>>> model = AutoModel.from_pretrained("microsoft/xclip-base-patch32")

>>> inputs = processor(
...     text=["playing sports", "eating spaghetti", "go shopping"],
...     videos=list(video),
...     return_tensors="pt",
...     padding=True,
... )

>>> # forward pass
>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> logits_per_video = outputs.logits_per_video  # this is the video-text similarity score
>>> probs = logits_per_video.softmax(dim=1)  # we can take the softmax to get the label probabilities
>>> print(probs)
tensor([[1.9496e-04, 9.9960e-01, 2.0825e-04]])
```

#### `get_text_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/x_clip/modeling_x_clip.py#L1291)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

返回

text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`)

通过将投影层应用于[XCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPTextModel)的汇聚输出获得的文本嵌入。

[XCLIPModel](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("microsoft/xclip-base-patch32")
>>> model = AutoModel.from_pretrained("microsoft/xclip-base-patch32")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
>>> text_features = model.get_text_features(**inputs)
```

#### `get_video_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/x_clip/modeling_x_clip.py#L1338)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';video_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

video_features（形状为`(batch_size, output_dim)`的`torch.FloatTensor`

通过将投影层应用于[XCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPVisionModel)和`XCLIPMultiframeIntegrationTransformer`的汇聚输出获得的视频嵌入。

[XCLIPModel](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPModel)的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import av
>>> import torch
>>> import numpy as np

>>> from transformers import AutoProcessor, AutoModel
>>> from huggingface_hub import hf_hub_download

>>> np.random.seed(0)

>>> def read_video_pyav(container, indices):
...     '''
...     Decode the video with PyAV decoder.
...     Args:
...         container (`av.container.input.InputContainer`): PyAV container.
...         indices (`List[int]`): List of frame indices to decode.
...     Returns:
...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).
...     '''
...     frames = []
...     container.seek(0)
...     start_index = indices[0]
...     end_index = indices[-1]
...     for i, frame in enumerate(container.decode(video=0)):
...         if i > end_index:
...             break
...         if i >= start_index and i in indices:
...             frames.append(frame)
...     return np.stack([x.to_ndarray(format="rgb24") for x in frames])

>>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):
...     '''
...     Sample a given number of frame indices from the video.
...     Args:
...         clip_len (`int`): Total number of frames to sample.
...         frame_sample_rate (`int`): Sample every n-th frame.
...         seg_len (`int`): Maximum allowed index of sample's last frame.
...     Returns:
...         indices (`List[int]`): List of sampled frame indices
...     '''
...     converted_len = int(clip_len * frame_sample_rate)
...     end_idx = np.random.randint(converted_len, seg_len)
...     start_idx = end_idx - converted_len
...     indices = np.linspace(start_idx, end_idx, num=clip_len)
...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)
...     return indices

>>> # video clip consists of 300 frames (10 seconds at 30 FPS)
>>> file_path = hf_hub_download(
...     repo_id="nielsr/video-demo", filename="eating_spaghetti.mp4", repo_type="dataset"
... )
>>> container = av.open(file_path)

>>> # sample 8 frames
>>> indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)
>>> video = read_video_pyav(container, indices)

>>> processor = AutoProcessor.from_pretrained("microsoft/xclip-base-patch32")
>>> model = AutoModel.from_pretrained("microsoft/xclip-base-patch32")

>>> inputs = processor(videos=list(video), return_tensors="pt")

>>> video_features = model.get_video_features(**inputs)
```

## XCLIPTextModel

### `class transformers.XCLIPTextModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/x_clip/modeling_x_clip.py#L791)

```py
( config: XCLIPTextConfig )
```

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/x_clip/modeling_x_clip.py#L806)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`）— 词汇表中输入序列标记的索引。默认情况下，如果提供填充，则将被忽略。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [输入ID是什么？](../术语表#input-ids)

+   `attention_mask`（`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1 用于“未被掩盖”的标记，

    +   0 用于“被掩盖”的标记。

    [注意力掩码是什么？](../术语表#attention-mask)

+   `position_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [位置ID是什么？](../术语表#position-ids)

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`，或者`config.return_dict=False`）包含根据配置（`<class 'transformers.models.x_clip.configuration_x_clip.XCLIPTextConfig'>`）和输入的不同元素。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`） — 模型最后一层的隐藏状态序列的输出。

+   `pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`） — 经过用于辅助预训练任务的层进一步处理后的序列的第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这将返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个+每层输出的一个）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[XCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPTextModel)的前向方法，覆盖`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, XCLIPTextModel

>>> model = XCLIPTextModel.from_pretrained("microsoft/xclip-base-patch32")
>>> tokenizer = AutoTokenizer.from_pretrained("microsoft/xclip-base-patch32")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states
```

## XCLIPVisionModel

### `class transformers.XCLIPVisionModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/x_clip/modeling_x_clip.py#L1000)

```py
( config: XCLIPVisionConfig )
```

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/x_clip/modeling_x_clip.py#L1013)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`） — 像素值。默认情况下将忽略填充，如果您提供的话。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`torch.FloatTensor`元组

一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.x_clip.configuration_x_clip.XCLIPVisionConfig'>`）和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）- 模型最后一层输出的隐藏状态序列。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）- 经过用于辅助预训练任务的层进一步处理后，序列的第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这将返回通过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的输出+每层的输出）。

    模型在每一层输出处的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[XCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/xclip#transformers.XCLIPVisionModel)的前向方法覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> import av
>>> import torch
>>> import numpy as np

>>> from transformers import AutoProcessor, XCLIPVisionModel
>>> from huggingface_hub import hf_hub_download

>>> np.random.seed(0)

>>> def read_video_pyav(container, indices):
...     '''
...     Decode the video with PyAV decoder.
...     Args:
...         container (`av.container.input.InputContainer`): PyAV container.
...         indices (`List[int]`): List of frame indices to decode.
...     Returns:
...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).
...     '''
...     frames = []
...     container.seek(0)
...     start_index = indices[0]
...     end_index = indices[-1]
...     for i, frame in enumerate(container.decode(video=0)):
...         if i > end_index:
...             break
...         if i >= start_index and i in indices:
...             frames.append(frame)
...     return np.stack([x.to_ndarray(format="rgb24") for x in frames])

>>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):
...     '''
...     Sample a given number of frame indices from the video.
...     Args:
...         clip_len (`int`): Total number of frames to sample.
...         frame_sample_rate (`int`): Sample every n-th frame.
...         seg_len (`int`): Maximum allowed index of sample's last frame.
...     Returns:
...         indices (`List[int]`): List of sampled frame indices
...     '''
...     converted_len = int(clip_len * frame_sample_rate)
...     end_idx = np.random.randint(converted_len, seg_len)
...     start_idx = end_idx - converted_len
...     indices = np.linspace(start_idx, end_idx, num=clip_len)
...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)
...     return indices

>>> # video clip consists of 300 frames (10 seconds at 30 FPS)
>>> file_path = hf_hub_download(
...     repo_id="nielsr/video-demo", filename="eating_spaghetti.mp4", repo_type="dataset"
... )
>>> container = av.open(file_path)

>>> # sample 16 frames
>>> indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)
>>> video = read_video_pyav(container, indices)

>>> processor = AutoProcessor.from_pretrained("microsoft/xclip-base-patch32")
>>> model = XCLIPVisionModel.from_pretrained("microsoft/xclip-base-patch32")

>>> pixel_values = processor(videos=list(video), return_tensors="pt").pixel_values

>>> batch_size, num_frames, num_channels, height, width = pixel_values.shape
>>> pixel_values = pixel_values.reshape(-1, num_channels, height, width)

>>> outputs = model(pixel_values)
>>> last_hidden_state = outputs.last_hidden_state
```
