- en: OneFormer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/oneformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/oneformer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The OneFormer model was proposed in [OneFormer: One Transformer to Rule Universal
    Image Segmentation](https://arxiv.org/abs/2211.06220) by Jitesh Jain, Jiachen
    Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi. OneFormer is a universal
    image segmentation framework that can be trained on a single panoptic dataset
    to perform semantic, instance, and panoptic segmentation tasks. OneFormer uses
    a task token to condition the model on the task in focus, making the architecture
    task-guided for training, and task-dynamic for inference.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8cd6226185941da2d5d5f4ca249f7bd2.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
- en: 'The abstract from the paper is the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '*Universal Image Segmentation is not a new concept. Past attempts to unify
    image segmentation in the last decades include scene parsing, panoptic segmentation,
    and, more recently, new panoptic architectures. However, such panoptic architectures
    do not truly unify image segmentation because they need to be trained individually
    on the semantic, instance, or panoptic segmentation to achieve the best performance.
    Ideally, a truly universal framework should be trained only once and achieve SOTA
    performance across all three image segmentation tasks. To that end, we propose
    OneFormer, a universal image segmentation framework that unifies segmentation
    with a multi-task train-once design. We first propose a task-conditioned joint
    training strategy that enables training on ground truths of each domain (semantic,
    instance, and panoptic segmentation) within a single multi-task training process.
    Secondly, we introduce a task token to condition our model on the task at hand,
    making our model task-dynamic to support multi-task training and inference. Thirdly,
    we propose using a query-text contrastive loss during training to establish better
    inter-task and inter-class distinctions. Notably, our single OneFormer model outperforms
    specialized Mask2Former models across all three segmentation tasks on ADE20k,
    CityScapes, and COCO, despite the latter being trained on each of the three tasks
    individually with three times the resources. With new ConvNeXt and DiNAT backbones,
    we observe even more performance improvement. We believe OneFormer is a significant
    step towards making image segmentation more universal and accessible.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: The figure below illustrates the architecture of OneFormer. Taken from the [original
    paper](https://arxiv.org/abs/2211.06220).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8617928e627dd67ce070d58bb21d63e1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: This model was contributed by [Jitesh Jain](https://huggingface.co/praeclarumjj3).
    The original code can be found [here](https://github.com/SHI-Labs/OneFormer).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OneFormer requires two inputs during inference: *image* and *task token*.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During training, OneFormer only uses panoptic annotations.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to train the model in a distributed environment across multiple
    nodes, then one should update the `get_num_masks` function inside in the `OneFormerLoss`
    class of `modeling_oneformer.py`. When training on multiple nodes, this should
    be set to the average number of target masks across all nodes, as can be seen
    in the original implementation [here](https://github.com/SHI-Labs/OneFormer/blob/33ebb56ed34f970a30ae103e786c0cb64c653d9a/oneformer/modeling/criterion.py#L287).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One can use [OneFormerProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor)
    to prepare input images and task inputs for the model and optional targets for
    the model. `OneformerProcessor` wraps [OneFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor)
    and [CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)
    into a single instance to both prepare the images and encode the task inputs.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To get the final segmentation, depending on the task, you can call [post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor.post_process_semantic_segmentation)
    or [post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation)
    or [post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation).
    All three tasks can be solved using [OneFormerForUniversalSegmentation](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation)
    output, panoptic segmentation accepts an optional `label_ids_to_fuse` argument
    to fuse instances of the target object/s (e.g. sky) together.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¦è·å¾—æœ€ç»ˆçš„åˆ†å‰²ç»“æœï¼Œå¯ä»¥è°ƒç”¨[post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor.post_process_semantic_segmentation)æˆ–[post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation)æˆ–[post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation)ã€‚è¿™ä¸‰ä¸ªä»»åŠ¡éƒ½å¯ä»¥ä½¿ç”¨[OneFormerForUniversalSegmentation](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation)çš„è¾“å‡ºæ¥è§£å†³ï¼Œå…¨æ™¯åˆ†å‰²æ¥å—ä¸€ä¸ªå¯é€‰çš„`label_ids_to_fuse`å‚æ•°ï¼Œç”¨äºå°†ç›®æ ‡å¯¹è±¡ï¼ˆä¾‹å¦‚å¤©ç©ºï¼‰çš„å®ä¾‹èåˆåœ¨ä¸€èµ·ã€‚
- en: Resources
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with OneFormer.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä»½å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨OneFormerã€‚
- en: Demo notebooks regarding inference + fine-tuning on custom data can be found
    [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/OneFormer).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…³äºæ¨æ–­+åœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒçš„æ¼”ç¤ºç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/OneFormer)æ‰¾åˆ°ã€‚
- en: If youâ€™re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we will review it. The resource should ideally
    demonstrate something new instead of duplicating an existing resource.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æäº¤æ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æŸ¥ã€‚èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°çš„ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: OneFormer specific outputs
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OneFormerç‰¹å®šè¾“å‡º
- en: '### `class transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L803)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L803)'
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the encoder model at the output of
    each stage.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” `torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels,
    height, width)`ã€‚ç¼–ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: '`pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the pixel decoder model at the output
    of each stage.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” `torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels,
    height, width)`ã€‚åƒç´ è§£ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: '`transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*,
    returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the transformer decoder at the output
    of each stage.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*,
    å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” `torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º
    + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ã€‚transformerè§£ç å™¨åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: '`transformer_decoder_object_queries` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, hidden_dim)`) â€” Output object queries from the last layer in the
    transformer decoder.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_object_queries` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,
    num_queries, hidden_dim)`) â€” æ¥è‡ªtransformerè§£ç å™¨æœ€åä¸€å±‚çš„è¾“å‡ºå¯¹è±¡æŸ¥è¯¢ã€‚'
- en: '`transformer_decoder_contrastive_queries` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, hidden_dim)`) â€” Contrastive queries from the transformer decoder.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_contrastive_queries` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,
    num_queries, hidden_dim)`) â€” æ¥è‡ªtransformerè§£ç å™¨çš„å¯¹æ¯”æŸ¥è¯¢ã€‚'
- en: '`transformer_decoder_mask_predictions` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, height, width)`) â€” Mask Predictions from the last layer in the transformer
    decoder.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_mask_predictions` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,
    num_queries, height, width)`) â€” æ¥è‡ªtransformerè§£ç å™¨æœ€åä¸€å±‚çš„æ©ç é¢„æµ‹ã€‚'
- en: '`transformer_decoder_class_predictions` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, num_classes+1)`) â€” Class Predictions from the last layer in the transformer
    decoder.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_class_predictions` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,
    num_queries, num_classes+1)`) â€” æ¥è‡ªtransformerè§£ç å™¨æœ€åä¸€å±‚çš„ç±»åˆ«é¢„æµ‹ã€‚'
- en: '`transformer_decoder_auxiliary_predictions` (Tuple of Dict of `str, torch.FloatTensor`,
    *optional*) â€” Tuple of class and mask predictions from each layer of the transformer
    decoder.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_auxiliary_predictions`ï¼ˆ`str, torch.FloatTensor`å­—å…¸çš„å…ƒç»„ï¼Œ*å¯é€‰*ï¼‰
    â€” æ¥è‡ªtransformerè§£ç å™¨æ¯ä¸€å±‚çš„ç±»åˆ«å’Œæ©ç é¢„æµ‹çš„å…ƒç»„ã€‚'
- en: '`text_queries` (`torch.FloatTensor`, *optional* of shape `(batch_size, num_queries,
    hidden_dim)`) â€” Text queries derived from the input text list used for calculating
    contrastive loss during training.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_queries` (`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`)
    â€” ä»ç”¨äºè®¡ç®—å¯¹æ¯”æŸå¤±çš„è¾“å…¥æ–‡æœ¬åˆ—è¡¨æ´¾ç”Ÿçš„æ–‡æœ¬æŸ¥è¯¢ã€‚'
- en: '`task_token` (`torch.FloatTensor` of shape `(batch_size, hidden_dim)`) â€” 1D
    task token to condition the queries.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_token`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, hidden_dim)`çš„`torch.FloatTensor`ï¼‰â€”ç”¨äºæ¡ä»¶æŸ¥è¯¢çš„ä¸€ç»´ä»»åŠ¡ä»¤ç‰Œã€‚'
- en: '`attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size,
    num_heads, sequence_length, sequence_length)`. Self and Cross Attentions weights
    from transformer decoder.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size,
    num_heads, sequence_length, sequence_length)`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚transformerè§£ç å™¨çš„è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æƒé‡ã€‚'
- en: Class for outputs of [OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel).
    This class returns all the needed hidden states to compute the logits.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äº[OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel)è¾“å‡ºçš„ç±»ã€‚æ­¤ç±»è¿”å›è®¡ç®—logitsæ‰€éœ€çš„æ‰€æœ‰éšè—çŠ¶æ€ã€‚
- en: '### `class transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L853)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L853)'
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`loss` (`torch.Tensor`, *optional*) â€” The computed loss, returned when labels
    are present.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆ`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”å½“å­˜åœ¨æ ‡ç­¾æ—¶è¿”å›è®¡ç®—çš„æŸå¤±ã€‚'
- en: '`class_queries_logits` (`torch.FloatTensor`) â€” A tensor of shape `(batch_size,
    num_queries, num_labels + 1)` representing the proposed classes for each query.
    Note the `+ 1` is needed because we incorporate the null class.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_queries_logits`ï¼ˆ`torch.FloatTensor`ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size, num_queries, num_labels
    + 1)`çš„å¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæŸ¥è¯¢çš„å»ºè®®ç±»åˆ«ã€‚è¯·æ³¨æ„ï¼Œéœ€è¦`+ 1`ï¼Œå› ä¸ºæˆ‘ä»¬åŒ…å«äº†ç©ºç±»ã€‚'
- en: '`masks_queries_logits` (`torch.FloatTensor`) â€” A tensor of shape `(batch_size,
    num_queries, height, width)` representing the proposed masks for each query.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`masks_queries_logits`ï¼ˆ`torch.FloatTensor`ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size, num_queries, height,
    width)`çš„å¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæŸ¥è¯¢çš„å»ºè®®æ©ç ã€‚'
- en: '`auxiliary_predictions` (List of Dict of `str, torch.FloatTensor`, *optional*)
    â€” List of class and mask predictions from each layer of the transformer decoder.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_predictions`ï¼ˆ`strï¼Œtorch.FloatTensor`å­—å…¸åˆ—è¡¨ï¼Œ*å¯é€‰*ï¼‰â€”æ¥è‡ªtransformerè§£ç å™¨æ¯ä¸€å±‚çš„ç±»åˆ«å’Œæ©ç é¢„æµ‹çš„åˆ—è¡¨ã€‚'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the encoder model at the output of
    each stage.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size,
    num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º+ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚ç¼–ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: '`pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the pixel decoder model at the output
    of each stage.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size,
    num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º+ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚åƒç´ è§£ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: '`transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*,
    returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the transformer decoder at the output
    of each stage.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size,
    sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º+ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚transformerè§£ç å™¨åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: '`transformer_decoder_object_queries` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, hidden_dim)`) â€” Output object queries from the last layer in the
    transformer decoder.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_object_queries`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`çš„`torch.FloatTensor`ï¼‰â€”æ¥è‡ªtransformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„è¾“å‡ºå¯¹è±¡æŸ¥è¯¢ã€‚'
- en: '`transformer_decoder_contrastive_queries` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, hidden_dim)`) â€” Contrastive queries from the transformer decoder.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_contrastive_queries`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`çš„`torch.FloatTensor`ï¼‰â€”æ¥è‡ªtransformerè§£ç å™¨çš„å¯¹æ¯”æŸ¥è¯¢ã€‚'
- en: '`transformer_decoder_mask_predictions` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, height, width)`) â€” Mask Predictions from the last layer in the transformer
    decoder.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_mask_predictions`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, height,
    width)`çš„`torch.FloatTensor`ï¼‰â€”æ¥è‡ªtransformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„æ©ç é¢„æµ‹ã€‚'
- en: '`transformer_decoder_class_predictions` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, num_classes+1)`) â€” Class Predictions from the last layer in the transformer
    decoder.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_class_predictions`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, num_classes+1)`çš„`torch.FloatTensor`ï¼‰â€”æ¥è‡ªtransformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„ç±»åˆ«é¢„æµ‹ã€‚'
- en: '`transformer_decoder_auxiliary_predictions` (List of Dict of `str, torch.FloatTensor`,
    *optional*) â€” List of class and mask predictions from each layer of the transformer
    decoder.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_auxiliary_predictions`ï¼ˆ`strï¼Œtorch.FloatTensor`å­—å…¸åˆ—è¡¨ï¼Œ*å¯é€‰*ï¼‰â€”æ¥è‡ªtransformerè§£ç å™¨æ¯ä¸€å±‚çš„ç±»åˆ«å’Œæ©ç é¢„æµ‹çš„åˆ—è¡¨ã€‚'
- en: '`text_queries` (`torch.FloatTensor`, *optional* of shape `(batch_size, num_queries,
    hidden_dim)`) â€” Text queries derived from the input text list used for calculating
    contrastive loss during training.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_queries`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`ï¼‰â€”ä»ç”¨äºè®­ç»ƒæœŸé—´è®¡ç®—å¯¹æ¯”æŸå¤±çš„è¾“å…¥æ–‡æœ¬åˆ—è¡¨æ´¾ç”Ÿçš„æ–‡æœ¬æŸ¥è¯¢ã€‚'
- en: '`task_token` (`torch.FloatTensor` of shape `(batch_size, hidden_dim)`) â€” 1D
    task token to condition the queries.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_token`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, hidden_dim)`çš„`torch.FloatTensor`ï¼‰â€”ç”¨äºæ¡ä»¶æŸ¥è¯¢çš„ä¸€ç»´ä»»åŠ¡ä»¤ç‰Œã€‚'
- en: '`attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size,
    num_heads, sequence_length, sequence_length)`. Self and Cross Attentions weights
    from transformer decoder.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tuple(torch.FloatTensor)`å…ƒç»„ã€‚æ¥è‡ªtransformerè§£ç å™¨çš„è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æƒé‡ã€‚'
- en: Class for outputs of `OneFormerForUniversalSegmentationOutput`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äº`OneFormerForUniversalSegmentationOutput`çš„è¾“å‡ºç±»ã€‚
- en: This output can be directly passed to [post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_semantic_segmentation)
    or [post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation)
    or [post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation)
    depending on the task. Please, see [`~OneFormerImageProcessor] for details regarding
    usage.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤è¾“å‡ºå¯ä»¥ç›´æ¥ä¼ é€’ç»™[post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_semantic_segmentation)æˆ–[post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation)æˆ–[post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation)ï¼Œå…·ä½“å–å†³äºä»»åŠ¡ã€‚è¯·å‚é˜…[`~OneFormerImageProcessor]ä»¥è·å–æœ‰å…³ç”¨æ³•çš„è¯¦ç»†ä¿¡æ¯ã€‚
- en: OneFormerConfig
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OneFormerConfig
- en: '### `class transformers.OneFormerConfig`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OneFormerConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/configuration_oneformer.py#L33)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/configuration_oneformer.py#L33)'
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`backbone_config` (`PretrainedConfig`, *optional*, defaults to `SwinConfig`)
    â€” The configuration of the backbone model.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backbone_config` (`PretrainedConfig`, *optional*, defaults to `SwinConfig`)
    â€” ä¸»å¹²æ¨¡å‹çš„é…ç½®ã€‚'
- en: '`ignore_value` (`int`, *optional*, defaults to 255) â€” Values to be ignored
    in GT label while calculating loss.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_value` (`int`, *optional*, defaults to 255) â€” åœ¨è®¡ç®—æŸå¤±æ—¶è¦å¿½ç•¥çš„GTæ ‡ç­¾ä¸­çš„å€¼ã€‚'
- en: '`num_queries` (`int`, *optional*, defaults to 150) â€” Number of object queries.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_queries` (`int`, *optional*, defaults to 150) â€” å¯¹è±¡æŸ¥è¯¢çš„æ•°é‡ã€‚'
- en: '`no_object_weight` (`float`, *optional*, defaults to 0.1) â€” Weight for no-object
    class predictions.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no_object_weight` (`float`, *optional*, defaults to 0.1) â€” æ— å¯¹è±¡ç±»é¢„æµ‹çš„æƒé‡ã€‚'
- en: '`class_weight` (`float`, *optional*, defaults to 2.0) â€” Weight for Classification
    CE loss.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_weight` (`float`, *optional*, defaults to 2.0) â€” åˆ†ç±»CEæŸå¤±çš„æƒé‡ã€‚'
- en: '`mask_weight` (`float`, *optional*, defaults to 5.0) â€” Weight for binary CE
    loss.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_weight` (`float`, *optional*, defaults to 5.0) â€” äºŒå…ƒCEæŸå¤±çš„æƒé‡ã€‚'
- en: '`dice_weight` (`float`, *optional*, defaults to 5.0) â€” Weight for dice loss.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dice_weight` (`float`, *optional*, defaults to 5.0) â€” DiceæŸå¤±çš„æƒé‡ã€‚'
- en: '`contrastive_weight` (`float`, *optional*, defaults to 0.5) â€” Weight for contrastive
    loss.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`contrastive_weight` (`float`, *optional*, defaults to 0.5) â€” å¯¹æ¯”æŸå¤±çš„æƒé‡ã€‚'
- en: '`contrastive_temperature` (`float`, *optional*, defaults to 0.07) â€” Initial
    value for scaling the contrastive logits.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`contrastive_temperature` (`float`, *optional*, defaults to 0.07) â€” ç”¨äºç¼©æ”¾å¯¹æ¯”å¯¹æ•°çš„åˆå§‹å€¼ã€‚'
- en: '`train_num_points` (`int`, *optional*, defaults to 12544) â€” Number of points
    to sample while calculating losses on mask predictions.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_num_points` (`int`, *optional*, defaults to 12544) â€” åœ¨è®¡ç®—æ©ç é¢„æµ‹æŸå¤±æ—¶è¦é‡‡æ ·çš„ç‚¹æ•°ã€‚'
- en: '`oversample_ratio` (`float`, *optional*, defaults to 3.0) â€” Ratio to decide
    how many points to oversample.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`oversample_ratio` (`float`, *optional*, defaults to 3.0) â€” å†³å®šè¿‡é‡‡æ ·å¤šå°‘ç‚¹çš„æ¯”ç‡ã€‚'
- en: '`importance_sample_ratio` (`float`, *optional*, defaults to 0.75) â€” Ratio of
    points that are sampled via importance sampling.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`importance_sample_ratio` (`float`, *optional*, defaults to 0.75) â€” é€šè¿‡é‡è¦æ€§é‡‡æ ·æŠ½æ ·çš„ç‚¹çš„æ¯”ç‡ã€‚'
- en: '`init_std` (`float`, *optional*, defaults to 0.02) â€” Standard deviation for
    normal intialization.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_std` (`float`, *optional*, defaults to 0.02) â€” æ­£æ€åˆå§‹åŒ–çš„æ ‡å‡†å·®ã€‚'
- en: '`init_xavier_std` (`float`, *optional*, defaults to 1.0) â€” Standard deviation
    for xavier uniform initialization.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_xavier_std` (`float`, *optional*, defaults to 1.0) â€” ç”¨äºxavierå‡åŒ€åˆå§‹åŒ–çš„æ ‡å‡†å·®ã€‚'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) â€” Epsilon for layer
    normalization.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) â€” å±‚å½’ä¸€åŒ–çš„epsilonã€‚'
- en: '`is_training` (`bool`, *optional*, defaults to `False`) â€” Whether to run in
    training or inference mode.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_training` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨è®­ç»ƒæˆ–æ¨ç†æ¨¡å¼ä¸‹è¿è¡Œã€‚'
- en: '`use_auxiliary_loss` (`bool`, *optional*, defaults to `True`) â€” Whether to
    calculate loss using intermediate predictions from transformer decoder.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_auxiliary_loss` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦ä½¿ç”¨transformerè§£ç å™¨çš„ä¸­é—´é¢„æµ‹è®¡ç®—æŸå¤±ã€‚'
- en: '`output_auxiliary_logits` (`bool`, *optional*, defaults to `True`) â€” Whether
    to return intermediate predictions from transformer decoder.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_auxiliary_logits` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦ä»transformerè§£ç å™¨è¿”å›ä¸­é—´é¢„æµ‹ã€‚'
- en: '`strides` (`list`, *optional*, defaults to `[4, 8, 16, 32]`) â€” List containing
    the strides for feature maps in the encoder.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strides` (`list`, *optional*, defaults to `[4, 8, 16, 32]`) â€” åŒ…å«ç¼–ç å™¨ä¸­ç‰¹å¾å›¾çš„æ­¥å¹…çš„åˆ—è¡¨ã€‚'
- en: '`task_seq_len` (`int`, *optional*, defaults to 77) â€” Sequence length for tokenizing
    text list input.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_seq_len` (`int`, *optional*, defaults to 77) â€” ç”¨äºå¯¹æ–‡æœ¬åˆ—è¡¨è¾“å…¥è¿›è¡Œåˆ†è¯çš„åºåˆ—é•¿åº¦ã€‚'
- en: '`text_encoder_width` (`int`, *optional*, defaults to 256) â€” Hidden size for
    text encoder.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_width` (`int`, *optional*, defaults to 256) â€” æ–‡æœ¬ç¼–ç å™¨çš„éšè—å¤§å°ã€‚'
- en: '`text_encoder_context_length` (`int`, *optional*, defaults to 77) â€” Input sequence
    length for text encoder.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_context_length` (`int`, *optional*, defaults to 77) â€” æ–‡æœ¬ç¼–ç å™¨çš„è¾“å…¥åºåˆ—é•¿åº¦ã€‚'
- en: '`text_encoder_num_layers` (`int`, *optional*, defaults to 6) â€” Number of layers
    for transformer in text encoder.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_num_layers` (`int`, *optional*, defaults to 6) â€” æ–‡æœ¬ç¼–ç å™¨ä¸­transformerçš„å±‚æ•°ã€‚'
- en: '`text_encoder_vocab_size` (`int`, *optional*, defaults to 49408) â€” Vocabulary
    size for tokenizer.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_vocab_size` (`int`, *optional*, defaults to 49408) â€” åˆ†è¯å™¨çš„è¯æ±‡é‡ã€‚'
- en: '`text_encoder_proj_layers` (`int`, *optional*, defaults to 2) â€” Number of layers
    in MLP for project text queries.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_proj_layers` (`int`, *optional*, defaults to 2) â€” ç”¨äºé¡¹ç›®æ–‡æœ¬æŸ¥è¯¢çš„MLPä¸­çš„å±‚æ•°ã€‚'
- en: '`text_encoder_n_ctx` (`int`, *optional*, defaults to 16) â€” Number of learnable
    text context queries.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_n_ctx` (`int`, *optional*, é»˜è®¤ä¸º 16) â€” å¯å­¦ä¹ æ–‡æœ¬ä¸Šä¸‹æ–‡æŸ¥è¯¢çš„æ•°é‡ã€‚'
- en: '`conv_dim` (`int`, *optional*, defaults to 256) â€” Feature map dimension to
    map outputs from the backbone.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_dim` (`int`, *optional*, é»˜è®¤ä¸º 256) â€” ä»éª¨å¹²ç½‘ç»œæ˜ å°„è¾“å‡ºçš„ç‰¹å¾å›¾ç»´åº¦ã€‚'
- en: '`mask_dim` (`int`, *optional*, defaults to 256) â€” Dimension for feature maps
    in pixel decoder.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_dim` (`int`, *optional*, é»˜è®¤ä¸º 256) â€” åƒç´ è§£ç å™¨ä¸­ç‰¹å¾å›¾çš„ç»´åº¦ã€‚'
- en: '`hidden_dim` (`int`, *optional*, defaults to 256) â€” Dimension for hidden states
    in transformer decoder.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dim` (`int`, *optional*, é»˜è®¤ä¸º 256) â€” å˜å‹å™¨è§£ç å™¨ä¸­éšè—çŠ¶æ€çš„ç»´åº¦ã€‚'
- en: '`encoder_feedforward_dim` (`int`, *optional*, defaults to 1024) â€” Dimension
    for FFN layer in pixel decoder.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_feedforward_dim` (`int`, *optional*, é»˜è®¤ä¸º 1024) â€” åƒç´ è§£ç å™¨ä¸­FFNå±‚çš„ç»´åº¦ã€‚'
- en: '`norm` (`str`, *optional*, defaults to `"GN"`) â€” Type of normalization.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm` (`str`, *optional*, é»˜è®¤ä¸º `"GN"`) â€” å½’ä¸€åŒ–ç±»å‹ã€‚'
- en: '`encoder_layers` (`int`, *optional*, defaults to 6) â€” Number of layers in pixel
    decoder.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layers` (`int`, *optional*, é»˜è®¤ä¸º 6) â€” åƒç´ è§£ç å™¨ä¸­çš„å±‚æ•°ã€‚'
- en: '`decoder_layers` (`int`, *optional*, defaults to 10) â€” Number of layers in
    transformer decoder.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layers` (`int`, *optional*, é»˜è®¤ä¸º 10) â€” å˜å‹å™¨è§£ç å™¨ä¸­çš„å±‚æ•°ã€‚'
- en: '`use_task_norm` (`bool`, *optional*, defaults to `True`) â€” Whether to normalize
    the task token.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_task_norm` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å¯¹ä»»åŠ¡ä»¤ç‰Œè¿›è¡Œå½’ä¸€åŒ–ã€‚'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 8) â€” Number of attention
    heads in transformer layers in the pixel and transformer decoders.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, é»˜è®¤ä¸º 8) â€” åƒç´ å’Œå˜å‹å™¨è§£ç å™¨ä¸­çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) â€” Dropout probability for
    pixel and transformer decoders.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, é»˜è®¤ä¸º 0.1) â€” åƒç´ å’Œå˜å‹å™¨è§£ç å™¨çš„ä¸¢å¤±æ¦‚ç‡ã€‚'
- en: '`dim_feedforward` (`int`, *optional*, defaults to 2048) â€” Dimension for FFN
    layer in transformer decoder.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim_feedforward` (`int`, *optional*, é»˜è®¤ä¸º 2048) â€” å˜å‹å™¨è§£ç å™¨ä¸­FFNå±‚çš„ç»´åº¦ã€‚'
- en: '`pre_norm` (`bool`, *optional*, defaults to `False`) â€” Whether to normalize
    hidden states before attention layers in transformer decoder.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pre_norm` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨å˜å‹å™¨è§£ç å™¨ä¸­çš„æ³¨æ„åŠ›å±‚ä¹‹å‰å¯¹éšè—çŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–ã€‚'
- en: '`enforce_input_proj` (`bool`, *optional*, defaults to `False`) â€” Whether to
    project hidden states in transformer decoder.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enforce_input_proj` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨å˜å‹å™¨è§£ç å™¨ä¸­æŠ•å½±éšè—çŠ¶æ€ã€‚'
- en: '`query_dec_layers` (`int`, *optional*, defaults to 2) â€” Number of layers in
    query transformer.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query_dec_layers` (`int`, *optional*, é»˜è®¤ä¸º 2) â€” æŸ¥è¯¢å˜å‹å™¨ä¸­çš„å±‚æ•°ã€‚'
- en: '`common_stride` (`int`, *optional*, defaults to 4) â€” Common stride used for
    features in pixel decoder.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`common_stride` (`int`, *optional*, é»˜è®¤ä¸º 4) â€” ç”¨äºåƒç´ è§£ç å™¨ä¸­ç‰¹å¾çš„å¸¸ç”¨æ­¥å¹…ã€‚'
- en: This is the configuration class to store the configuration of a [OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel).
    It is used to instantiate a OneFormer model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the OneFormer [shi-labs/oneformer_ade20k_swin_tiny](https://huggingface.co/shi-labs/oneformer_ade20k_swin_tiny)
    architecture trained on [ADE20k-150](https://huggingface.co/datasets/scene_parse_150).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨ [OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel)
    é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª OneFormer æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºåœ¨ [ADE20k-150](https://huggingface.co/datasets/scene_parse_150)
    ä¸Šè®­ç»ƒçš„ OneFormer [shi-labs/oneformer_ade20k_swin_tiny](https://huggingface.co/shi-labs/oneformer_ade20k_swin_tiny)
    æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    å¹¶å¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Examples:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: OneFormerImageProcessor
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OneFormerImageProcessor
- en: '### `class transformers.OneFormerImageProcessor`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OneFormerImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L368)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L368)'
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) â€” Whether to resize the
    input to a certain `size`.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†è¾“å…¥è°ƒæ•´å¤§å°åˆ°ç‰¹å®šçš„ `size`ã€‚'
- en: '`size` (`int`, *optional*, defaults to 800) â€” Resize the input to the given
    size. Only has an effect if `do_resize` is set to `True`. If size is a sequence
    like `(width, height)`, output size will be matched to this. If size is an int,
    smaller edge of the image will be matched to this number. i.e, if `height > width`,
    then image will be rescaled to `(size * height / width, size)`.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`int`, *optional*, é»˜è®¤ä¸º 800) â€” å°†è¾“å…¥è°ƒæ•´å¤§å°åˆ°ç»™å®šå¤§å°ã€‚ä»…åœ¨ `do_resize` è®¾ç½®ä¸º `True`
    æ—¶æœ‰æ•ˆã€‚å¦‚æœ size æ˜¯ä¸€ä¸ªç±»ä¼¼ `(width, height)` çš„åºåˆ—ï¼Œåˆ™è¾“å‡ºå¤§å°å°†åŒ¹é…åˆ°è¿™ä¸ªã€‚å¦‚æœ size æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œå›¾åƒçš„è¾ƒå°è¾¹å°†åŒ¹é…åˆ°è¿™ä¸ªæ•°å­—ã€‚å³ï¼Œå¦‚æœ
    `height > width`ï¼Œåˆ™å›¾åƒå°†é‡æ–°ç¼©æ”¾ä¸º `(size * height / width, size)`ã€‚'
- en: '`resample` (`int`, *optional*, defaults to `Resampling.BILINEAR`) â€” An optional
    resampling filter. This can be one of `PIL.Image.Resampling.NEAREST`, `PIL.Image.Resampling.BOX`,
    `PIL.Image.Resampling.BILINEAR`, `PIL.Image.Resampling.HAMMING`, `PIL.Image.Resampling.BICUBIC`
    or `PIL.Image.Resampling.LANCZOS`. Only has an effect if `do_resize` is set to
    `True`.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`int`, *optional*, é»˜è®¤ä¸º `Resampling.BILINEAR`) â€” å¯é€‰çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥æ˜¯ `PIL.Image.Resampling.NEAREST`,
    `PIL.Image.Resampling.BOX`, `PIL.Image.Resampling.BILINEAR`, `PIL.Image.Resampling.HAMMING`,
    `PIL.Image.Resampling.BICUBIC` æˆ– `PIL.Image.Resampling.LANCZOS` ä¸­çš„ä¸€ä¸ªã€‚ä»…åœ¨ `do_resize`
    è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚'
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) â€” Whether to rescale
    the input to a certain `scale`.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†è¾“å…¥é‡æ–°ç¼©æ”¾åˆ°ç‰¹å®šçš„ `scale`ã€‚'
- en: '`rescale_factor` (`float`, *optional*, defaults to `1/ 255`) â€” Rescale the
    input by the given factor. Only has an effect if `do_rescale` is set to `True`.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`, *optional*, é»˜è®¤ä¸º `1/ 255`) â€” é€šè¿‡ç»™å®šå› å­é‡æ–°ç¼©æ”¾è¾“å…¥ã€‚ä»…åœ¨ `do_rescale`
    è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    normalize the input with mean and standard deviation.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦å¯¹è¾“å…¥è¿›è¡Œå‡å€¼å’Œæ ‡å‡†å·®å½’ä¸€åŒ–ã€‚'
- en: '`image_mean` (`int`, *optional*, defaults to `[0.485, 0.456, 0.406]`) â€” The
    sequence of means for each channel, to be used when normalizing images. Defaults
    to the ImageNet mean.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`[0.485, 0.456, 0.406]`ï¼‰â€” æ¯ä¸ªé€šé“çš„å‡å€¼åºåˆ—ï¼Œåœ¨è§„èŒƒåŒ–å›¾åƒæ—¶ä½¿ç”¨ã€‚é»˜è®¤ä¸ºImageNetå‡å€¼ã€‚'
- en: '`image_std` (`int`, *optional*, defaults to `[0.229, 0.224, 0.225]`) â€” The
    sequence of standard deviations for each channel, to be used when normalizing
    images. Defaults to the ImageNet std.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`[0.229, 0.224, 0.225]`ï¼‰â€” æ¯ä¸ªé€šé“çš„æ ‡å‡†å·®åºåˆ—ï¼Œåœ¨è§„èŒƒåŒ–å›¾åƒæ—¶ä½¿ç”¨ã€‚é»˜è®¤ä¸ºImageNetæ ‡å‡†å·®ã€‚'
- en: '`ignore_index` (`int`, *optional*) â€” Label to be assigned to background pixels
    in segmentation maps. If provided, segmentation map pixels denoted with 0 (background)
    will be replaced with `ignore_index`.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” åˆ†å‰²åœ°å›¾ä¸­è¦åˆ†é…ç»™èƒŒæ™¯åƒç´ çš„æ ‡ç­¾ã€‚å¦‚æœæä¾›ï¼Œç”¨0ï¼ˆèƒŒæ™¯ï¼‰è¡¨ç¤ºçš„åˆ†å‰²åœ°å›¾åƒç´ å°†è¢«æ›¿æ¢ä¸º`ignore_index`ã€‚'
- en: '`do_reduce_labels` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to decrement all label values of segmentation maps by 1\. Usually used for datasets
    where 0 is used for background, and background itself is not included in all classes
    of a dataset (e.g. ADE20k). The background label will be replaced by `ignore_index`.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_reduce_labels`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦å°†æ‰€æœ‰åˆ†å‰²åœ°å›¾çš„æ ‡ç­¾å€¼å‡1ã€‚é€šå¸¸ç”¨äºæ•°æ®é›†ä¸­ä½¿ç”¨0è¡¨ç¤ºèƒŒæ™¯ï¼Œå¹¶ä¸”èƒŒæ™¯æœ¬èº«ä¸åŒ…å«åœ¨æ•°æ®é›†çš„æ‰€æœ‰ç±»ä¸­çš„æƒ…å†µï¼ˆä¾‹å¦‚ADE20kï¼‰ã€‚èƒŒæ™¯æ ‡ç­¾å°†è¢«æ›¿æ¢ä¸º`ignore_index`ã€‚'
- en: '`repo_path` (`str`, *optional*, defaults to `"shi-labs/oneformer_demo"`) â€”
    Path to hub repo or local directory containing the JSON file with class information
    for the dataset. If unset, will look for `class_info_file` in the current working
    directory.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repo_path`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"shi-labs/oneformer_demo"`ï¼‰â€” åŒ…å«æ•°æ®é›†ç±»ä¿¡æ¯çš„JSONæ–‡ä»¶çš„hubå­˜å‚¨åº“æˆ–æœ¬åœ°ç›®å½•çš„è·¯å¾„ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†åœ¨å½“å‰å·¥ä½œç›®å½•ä¸­æŸ¥æ‰¾`class_info_file`ã€‚'
- en: '`class_info_file` (`str`, *optional*) â€” JSON file containing class information
    for the dataset. See `shi-labs/oneformer_demo/cityscapes_panoptic.json` for an
    example.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_info_file`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” åŒ…å«æ•°æ®é›†ç±»ä¿¡æ¯çš„JSONæ–‡ä»¶ã€‚æŸ¥çœ‹`shi-labs/oneformer_demo/cityscapes_panoptic.json`ä»¥è·å–ç¤ºä¾‹ã€‚'
- en: '`num_text` (`int`, *optional*) â€” Number of text entries in the text input list.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_text`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” æ–‡æœ¬è¾“å…¥åˆ—è¡¨ä¸­çš„æ–‡æœ¬æ¡ç›®æ•°ã€‚'
- en: Constructs a OneFormer image processor. The image processor can be used to prepare
    image(s), task input(s) and optional text inputs and targets for the model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªOneFormerå›¾åƒå¤„ç†å™¨ã€‚è¯¥å›¾åƒå¤„ç†å™¨å¯ç”¨äºä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒã€ä»»åŠ¡è¾“å…¥ä»¥åŠå¯é€‰çš„æ–‡æœ¬è¾“å…¥å’Œç›®æ ‡ã€‚
- en: This image processor inherits from `BaseImageProcessor` which contains most
    of the main methods. Users should refer to this superclass for more information
    regarding those methods.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå›¾åƒå¤„ç†å™¨ç»§æ‰¿è‡ª`BaseImageProcessor`ï¼Œå…¶ä¸­åŒ…å«å¤§éƒ¨åˆ†ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒè¿™ä¸ªè¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `preprocess`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L656)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L656)'
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#### `encode_inputs`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_inputs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L954)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L954)'
- en: '[PRE6]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values_list` (`List[ImageInput]`) â€” List of images (pixel values) to
    be padded. Each image should be a tensor of shape `(channels, height, width)`.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values_list`ï¼ˆ`List[ImageInput]`ï¼‰â€” è¦å¡«å……çš„å›¾åƒï¼ˆåƒç´ å€¼ï¼‰åˆ—è¡¨ã€‚æ¯ä¸ªå›¾åƒåº”è¯¥æ˜¯å½¢çŠ¶ä¸º`(channels,
    height, width)`çš„å¼ é‡ã€‚'
- en: '`task_inputs` (`List[str]`) â€” List of task values.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_inputs`ï¼ˆ`List[str]`ï¼‰â€” ä»»åŠ¡å€¼åˆ—è¡¨ã€‚'
- en: '`segmentation_maps` (`ImageInput`, *optional*) â€” The corresponding semantic
    segmentation maps with the pixel-wise annotations.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation_maps`ï¼ˆ`ImageInput`ï¼Œ*å¯é€‰*ï¼‰â€” å…·æœ‰åƒç´ çº§æ³¨é‡Šçš„ç›¸åº”è¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚'
- en: '(`bool`, *optional*, defaults to `True`): Whether or not to pad images up to
    the largest image in a batch and create a pixel mask.'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰ï¼šæ˜¯å¦å°†å›¾åƒå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€å¤§çš„å›¾åƒå¹¶åˆ›å»ºåƒç´ æ©ç ã€‚
- en: 'If left to the default, will return a pixel mask that is:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†è¿”å›ä¸€ä¸ªåƒç´ æ©ç ï¼Œå³ï¼š
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºçœŸå®åƒç´ ï¼ˆå³`æœªæ©ç `ï¼‰ä¸º1ï¼Œ
- en: 0 for pixels that are padding (i.e. `masked`).
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºå¡«å……åƒç´ ï¼ˆå³`æ©ç `ï¼‰ä¸º0ã€‚
- en: '`instance_id_to_semantic_id` (`List[Dict[int, int]]` or `Dict[int, int]`, *optional*)
    â€” A mapping between object instance ids and class ids. If passed, `segmentation_maps`
    is treated as an instance segmentation map where each pixel represents an instance
    id. Can be provided as a single dictionary with a global/dataset-level mapping
    or as a list of dictionaries (one per image), to map instance ids in each image
    separately.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`instance_id_to_semantic_id`ï¼ˆ`List[Dict[int, int]]`æˆ–`Dict[int, int]`ï¼Œ*å¯é€‰*ï¼‰â€”
    å¯¹è±¡å®ä¾‹IDå’Œç±»IDä¹‹é—´çš„æ˜ å°„ã€‚å¦‚æœä¼ é€’ï¼Œ`segmentation_maps`å°†è¢«è§†ä¸ºå®ä¾‹åˆ†å‰²åœ°å›¾ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ è¡¨ç¤ºä¸€ä¸ªå®ä¾‹IDã€‚å¯ä»¥æä¾›ä¸ºå•ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«å…¨å±€/æ•°æ®é›†çº§åˆ«çš„æ˜ å°„ï¼Œæˆ–ä½œä¸ºå­—å…¸åˆ—è¡¨ï¼ˆæ¯ä¸ªå›¾åƒä¸€ä¸ªï¼‰ï¼Œä»¥åˆ†åˆ«æ˜ å°„æ¯ä¸ªå›¾åƒä¸­çš„å®ä¾‹IDã€‚'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors instead of NumPy arrays. If set to `''pt''`,
    return PyTorch `torch.Tensor` objects.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`ï¼ˆ`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)ï¼Œ*å¯é€‰*ï¼‰â€”
    å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯NumPyæ•°ç»„ã€‚å¦‚æœè®¾ç½®ä¸º`''pt''`ï¼Œåˆ™è¿”å›PyTorch `torch.Tensor`å¯¹è±¡ã€‚'
- en: '`input_data_format` (`str` or `ChannelDimension`, *optional*) â€” The channel
    dimension format of the input image. If not provided, it will be inferred from
    the input image.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format`ï¼ˆ`str`æˆ–`ChannelDimension`ï¼Œ*å¯é€‰*ï¼‰â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­ã€‚'
- en: Returns
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)'
- en: 'A [BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)
    with the following fields:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…·æœ‰ä»¥ä¸‹å­—æ®µçš„[BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)ï¼š
- en: '`pixel_values` â€” Pixel values to be fed to a model.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` â€” è¦é¦ˆé€ç»™æ¨¡å‹çš„åƒç´ å€¼ã€‚'
- en: '`pixel_mask` â€” Pixel mask to be fed to a model (when `=True` or if `pixel_mask`
    is in `self.model_input_names`).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask` â€” è¦é¦ˆé€ç»™æ¨¡å‹çš„åƒç´ æ©æ¨¡ï¼ˆå½“ `=True` æˆ– `pixel_mask` åœ¨ `self.model_input_names`
    ä¸­æ—¶ï¼‰ã€‚'
- en: '`mask_labels` â€” Optional list of mask labels of shape `(labels, height, width)`
    to be fed to a model (when `annotations` are provided).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_labels` â€” å½¢çŠ¶ä¸º `(labels, height, width)` çš„å¯é€‰æ©æ¨¡æ ‡ç­¾åˆ—è¡¨ï¼Œè¦é¦ˆé€ç»™æ¨¡å‹ï¼ˆå½“æä¾› `annotations`
    æ—¶ï¼‰ã€‚'
- en: '`class_labels` â€” Optional list of class labels of shape `(labels)` to be fed
    to a model (when `annotations` are provided). They identify the labels of `mask_labels`,
    e.g. the label of `mask_labels[i][j]` if `class_labels[i][j]`.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_labels` â€” å½¢çŠ¶ä¸º `(labels)` çš„å¯é€‰ç±»æ ‡ç­¾åˆ—è¡¨ï¼Œè¦é¦ˆé€ç»™æ¨¡å‹ï¼ˆå½“æä¾› `annotations` æ—¶ï¼‰ã€‚å®ƒä»¬æ ‡è¯† `mask_labels`
    çš„æ ‡ç­¾ï¼Œä¾‹å¦‚ï¼Œå¦‚æœ `class_labels[i][j]` çš„æ ‡ç­¾ä¸º `mask_labels[i][j]`ã€‚'
- en: '`text_inputs` â€” Optional list of text string entries to be fed to a model (when
    `annotations` are provided). They identify the binary masks present in the image.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_inputs` â€” è¦é¦ˆé€ç»™æ¨¡å‹çš„å¯é€‰æ–‡æœ¬å­—ç¬¦ä¸²æ¡ç›®åˆ—è¡¨ï¼ˆå½“æä¾› `annotations` æ—¶ï¼‰ã€‚å®ƒä»¬æ ‡è¯†å›¾åƒä¸­å­˜åœ¨çš„äºŒè¿›åˆ¶æ©æ¨¡ã€‚'
- en: Pad images up to the largest image in a batch and create a corresponding `pixel_mask`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å›¾åƒå¡«å……åˆ°æ‰¹å¤„ç†ä¸­æœ€å¤§çš„å›¾åƒï¼Œå¹¶åˆ›å»ºç›¸åº”çš„ `pixel_mask`ã€‚
- en: OneFormer addresses semantic segmentation with a mask classification paradigm,
    thus input segmentation maps will be converted to lists of binary masks and their
    respective labels. Letâ€™s see an example, assuming `segmentation_maps = [[2,6,7,9]]`,
    the output will contain `mask_labels = [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]`
    (four binary masks) and `class_labels = [2,6,7,9]`, the labels for each mask.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: OneFormer ä½¿ç”¨æ©æ¨¡åˆ†ç±»èŒƒå¼æ¥å¤„ç†è¯­ä¹‰åˆ†å‰²ï¼Œå› æ­¤è¾“å…¥åˆ†å‰²åœ°å›¾å°†è¢«è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ©æ¨¡åˆ—è¡¨åŠå…¶ç›¸åº”çš„æ ‡ç­¾ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼Œå‡è®¾ `segmentation_maps
    = [[2,6,7,9]]`ï¼Œè¾“å‡ºå°†åŒ…å« `mask_labels = [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]`ï¼ˆå››ä¸ªäºŒè¿›åˆ¶æ©æ¨¡ï¼‰å’Œ
    `class_labels = [2,6,7,9]`ï¼Œæ¯ä¸ªæ©æ¨¡çš„æ ‡ç­¾ã€‚
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_semantic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1089)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1089)'
- en: '[PRE7]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` ([MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation))
    â€” Raw outputs of the model.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` ([MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation))
    â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚'
- en: '`target_sizes` (`List[Tuple[int, int]]`, *optional*) â€” List of length (batch_size),
    where each list item (`Tuple[int, int]]`) corresponds to the requested final size
    (height, width) of each prediction. If left to None, predictions will not be resized.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`List[Tuple[int, int]]`, *å¯é€‰*) â€” é•¿åº¦ä¸º (batch_size) çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªåˆ—è¡¨é¡¹
    (`Tuple[int, int]]`) å¯¹åº”äºæ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ã€å®½åº¦ï¼‰ã€‚å¦‚æœä¿æŒä¸º Noneï¼Œåˆ™ä¸ä¼šè°ƒæ•´é¢„æµ‹å¤§å°ã€‚'
- en: Returns
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[torch.Tensor]`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[torch.Tensor]`'
- en: A list of length `batch_size`, where each item is a semantic segmentation map
    of shape (height, width) corresponding to the target_sizes entry (if `target_sizes`
    is specified). Each entry of each `torch.Tensor` correspond to a semantic class
    id.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé•¿åº¦ä¸º `batch_size` çš„åˆ—è¡¨ï¼Œæ¯ä¸ªé¡¹æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º (height, width) çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äºç›®æ ‡å¤§å°æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šäº† `target_sizes`ï¼‰ã€‚æ¯ä¸ª
    `torch.Tensor` çš„æ¯ä¸ªæ¡ç›®å¯¹åº”äºä¸€ä¸ªè¯­ä¹‰ç±»åˆ« idã€‚
- en: Converts the output of [MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: å°† [MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation)
    çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒ PyTorchã€‚
- en: '#### `post_process_instance_segmentation`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_instance_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1139)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1139)'
- en: '[PRE8]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` (`OneFormerForUniversalSegmentationOutput`) â€” The outputs from `OneFormerForUniversalSegmentationOutput`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` (`OneFormerForUniversalSegmentationOutput`) â€” ä» `OneFormerForUniversalSegmentationOutput`
    å¾—åˆ°çš„è¾“å‡ºã€‚'
- en: '`task_type` (`str`, *optional)*, defaults to â€œinstanceâ€) â€” The post processing
    depends on the task token input. If the `task_type` is â€œpanopticâ€, we need to
    ignore the stuff predictions.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_type` (`str`, *å¯é€‰*)ï¼Œé»˜è®¤ä¸ºâ€œinstanceâ€) â€” åå¤„ç†å–å†³äºä»»åŠ¡ä»¤ç‰Œè¾“å…¥ã€‚å¦‚æœ `task_type` æ˜¯â€œpanopticâ€ï¼Œæˆ‘ä»¬éœ€è¦å¿½ç•¥æ‚é¡¹é¢„æµ‹ã€‚'
- en: '`is_demo` (`bool`, *optional)*, defaults to `True`) â€” Whether the model is
    in demo mode. If true, use threshold to predict final masks.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_demo` (`bool`, *å¯é€‰*)ï¼Œé»˜è®¤ä¸º `True`) â€” æ¨¡å‹æ˜¯å¦å¤„äºæ¼”ç¤ºæ¨¡å¼ã€‚å¦‚æœä¸ºçœŸï¼Œåˆ™ä½¿ç”¨é˜ˆå€¼é¢„æµ‹æœ€ç»ˆæ©æ¨¡ã€‚'
- en: '`threshold` (`float`, *optional*, defaults to 0.5) â€” The probability score
    threshold to keep predicted instance masks.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.5) â€” ä¿ç•™é¢„æµ‹å®ä¾‹æ©æ¨¡çš„æ¦‚ç‡åˆ†æ•°é˜ˆå€¼ã€‚'
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) â€” Threshold to use
    when turning the predicted masks into binary values.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.5) â€” åœ¨å°†é¢„æµ‹çš„æ©æ¨¡è½¬æ¢ä¸ºäºŒè¿›åˆ¶å€¼æ—¶ä½¿ç”¨çš„é˜ˆå€¼ã€‚'
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) â€” The
    overlap mask area threshold to merge or discard small disconnected parts within
    each binary instance mask.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overlap_mask_area_threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.8) â€” åˆå¹¶æˆ–ä¸¢å¼ƒæ¯ä¸ªäºŒè¿›åˆ¶å®ä¾‹æ©æ¨¡ä¸­çš„å°æ–­å¼€éƒ¨åˆ†çš„é‡å æ©æ¨¡åŒºåŸŸé˜ˆå€¼ã€‚'
- en: '`target_sizes` (`List[Tuple]`, *optional*) â€” List of length (batch_size), where
    each list item (`Tuple[int, int]]`) corresponds to the requested final size (height,
    width) of each prediction in batch. If left to None, predictions will not be resized.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`List[Tuple]`, *å¯é€‰*) â€” é•¿åº¦ä¸º (batch_size) çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªåˆ—è¡¨é¡¹ (`Tuple[int,
    int]]`) å¯¹åº”äºæ‰¹å¤„ç†ä¸­æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ã€å®½åº¦ï¼‰ã€‚å¦‚æœä¿æŒä¸º Noneï¼Œåˆ™ä¸ä¼šè°ƒæ•´é¢„æµ‹å¤§å°ã€‚'
- en: '`return_coco_annotation` (`bool`, *optional)*, defaults to `False`) â€” Whether
    to return predictions in COCO format.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_coco_annotation` (`bool`, *å¯é€‰*)ï¼Œé»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä»¥ COCO æ ¼å¼è¿”å›é¢„æµ‹ã€‚'
- en: Returns
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[Dict]`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Dict]`'
- en: 'A list of dictionaries, one per image, each dictionary containing two keys:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå›¾åƒä¸€ä¸ªå­—å…¸ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä¸¤ä¸ªé”®ï¼š
- en: '`segmentation` â€” a tensor of shape `(height, width)` where each pixel represents
    a `segment_id`, set to `None` if no mask if found above `threshold`. If `target_sizes`
    is specified, segmentation is resized to the corresponding `target_sizes` entry.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation` â€” å½¢çŠ¶ä¸º`(height, width)`çš„å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ ä»£è¡¨ä¸€ä¸ª`segment_id`ï¼Œå¦‚æœæœªæ‰¾åˆ°é«˜äº`threshold`çš„æ©æ¨¡ï¼Œåˆ™è®¾ç½®ä¸º`None`ã€‚å¦‚æœæŒ‡å®šäº†`target_sizes`ï¼Œåˆ™å°†åˆ†å‰²è°ƒæ•´ä¸ºç›¸åº”çš„`target_sizes`æ¡ç›®ã€‚'
- en: '`segments_info` â€” A dictionary that contains additional information on each
    segment.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segments_info` â€” åŒ…å«æ¯ä¸ªæ®µçš„å…¶ä»–ä¿¡æ¯çš„å­—å…¸ã€‚'
- en: '`id` â€” an integer representing the `segment_id`.'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id` â€” ä»£è¡¨`segment_id`çš„æ•´æ•°ã€‚'
- en: '`label_id` â€” An integer representing the label / semantic class id corresponding
    to `segment_id`.'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_id` â€” ä»£è¡¨ä¸`segment_id`å¯¹åº”çš„æ ‡ç­¾/è¯­ä¹‰ç±»åˆ«IDçš„æ•´æ•°ã€‚'
- en: '`was_fused` â€” a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False`
    otherwise. Multiple instances of the same class / label were fused and assigned
    a single `segment_id`.'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`was_fused` â€” ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œå¦‚æœ`label_id`åœ¨`label_ids_to_fuse`ä¸­ï¼Œåˆ™ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`ã€‚ç›¸åŒç±»åˆ«/æ ‡ç­¾çš„å¤šä¸ªå®ä¾‹è¢«èåˆå¹¶åˆ†é…ä¸€ä¸ªå•ç‹¬çš„`segment_id`ã€‚'
- en: '`score` â€” Prediction score of segment with `segment_id`.'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` â€” å…·æœ‰`segment_id`çš„æ®µçš„é¢„æµ‹åˆ†æ•°ã€‚'
- en: Converts the output of `OneFormerForUniversalSegmentationOutput` into image
    instance segmentation predictions. Only supports PyTorch.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: å°†`OneFormerForUniversalSegmentationOutput`çš„è¾“å‡ºè½¬æ¢ä¸ºå›¾åƒå®ä¾‹åˆ†å‰²é¢„æµ‹ã€‚ä»…æ”¯æŒPyTorchã€‚
- en: '#### `post_process_panoptic_segmentation`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_panoptic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1259)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1259)'
- en: '[PRE9]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` (`MaskFormerForInstanceSegmentationOutput`) â€” The outputs from [MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs`ï¼ˆ`MaskFormerForInstanceSegmentationOutput`ï¼‰â€” æ¥è‡ª[MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation)çš„è¾“å‡ºã€‚'
- en: '`threshold` (`float`, *optional*, defaults to 0.5) â€” The probability score
    threshold to keep predicted instance masks.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.5ï¼‰â€” ä¿ç•™é¢„æµ‹å®ä¾‹æ©æ¨¡çš„æ¦‚ç‡åˆ†æ•°é˜ˆå€¼ã€‚'
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) â€” Threshold to use
    when turning the predicted masks into binary values.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_threshold`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.5ï¼‰â€” åœ¨å°†é¢„æµ‹çš„æ©æ¨¡è½¬æ¢ä¸ºäºŒè¿›åˆ¶å€¼æ—¶ä½¿ç”¨çš„é˜ˆå€¼ã€‚'
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) â€” The
    overlap mask area threshold to merge or discard small disconnected parts within
    each binary instance mask.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overlap_mask_area_threshold`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.8ï¼‰â€” ç”¨äºåˆå¹¶æˆ–ä¸¢å¼ƒæ¯ä¸ªäºŒè¿›åˆ¶å®ä¾‹æ©æ¨¡ä¸­çš„å°æ–­å¼€éƒ¨åˆ†çš„é‡å æ©æ¨¡åŒºåŸŸé˜ˆå€¼ã€‚'
- en: '`label_ids_to_fuse` (`Set[int]`, *optional*) â€” The labels in this state will
    have all their instances be fused together. For instance we could say there can
    only be one sky in an image, but several persons, so the label ID for sky would
    be in that set, but not the one for person.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_ids_to_fuse`ï¼ˆ`Set[int]`ï¼Œ*å¯é€‰*ï¼‰â€” æ­¤çŠ¶æ€ä¸­çš„æ ‡ç­¾å°†ä½¿å…¶æ‰€æœ‰å®ä¾‹è¢«èåˆåœ¨ä¸€èµ·ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è¯´å›¾åƒä¸­åªèƒ½æœ‰ä¸€ä¸ªå¤©ç©ºï¼Œä½†å¯ä»¥æœ‰å‡ ä¸ªäººï¼Œå› æ­¤å¤©ç©ºçš„æ ‡ç­¾IDå°†åœ¨è¯¥é›†åˆä¸­ï¼Œä½†äººçš„æ ‡ç­¾IDä¸åœ¨å…¶ä¸­ã€‚'
- en: '`target_sizes` (`List[Tuple]`, *optional*) â€” List of length (batch_size), where
    each list item (`Tuple[int, int]]`) corresponds to the requested final size (height,
    width) of each prediction in batch. If left to None, predictions will not be resized.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes`ï¼ˆ`List[Tuple]`ï¼Œ*å¯é€‰*ï¼‰â€” é•¿åº¦ä¸ºï¼ˆbatch_sizeï¼‰çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªåˆ—è¡¨é¡¹ï¼ˆ`Tuple[int,
    int]`ï¼‰å¯¹åº”äºæ‰¹å¤„ç†ä¸­æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚çš„æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰ã€‚å¦‚æœä¿æŒä¸ºNoneï¼Œåˆ™ä¸ä¼šè°ƒæ•´é¢„æµ‹å¤§å°ã€‚'
- en: Returns
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›å€¼
- en: '`List[Dict]`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Dict]`'
- en: 'A list of dictionaries, one per image, each dictionary containing two keys:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå›¾åƒä¸€ä¸ªï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä¸¤ä¸ªé”®ï¼š
- en: '`segmentation` â€” a tensor of shape `(height, width)` where each pixel represents
    a `segment_id`, set to `None` if no mask if found above `threshold`. If `target_sizes`
    is specified, segmentation is resized to the corresponding `target_sizes` entry.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation` â€” å½¢çŠ¶ä¸º`(height, width)`çš„å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ ä»£è¡¨ä¸€ä¸ª`segment_id`ï¼Œå¦‚æœæœªæ‰¾åˆ°é«˜äº`threshold`çš„æ©æ¨¡ï¼Œåˆ™è®¾ç½®ä¸º`None`ã€‚å¦‚æœæŒ‡å®šäº†`target_sizes`ï¼Œåˆ™å°†åˆ†å‰²è°ƒæ•´ä¸ºç›¸åº”çš„`target_sizes`æ¡ç›®ã€‚'
- en: '`segments_info` â€” A dictionary that contains additional information on each
    segment.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segments_info` â€” åŒ…å«æ¯ä¸ªæ®µçš„å…¶ä»–ä¿¡æ¯çš„å­—å…¸ã€‚'
- en: '`id` â€” an integer representing the `segment_id`.'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id` â€” ä»£è¡¨`segment_id`çš„æ•´æ•°ã€‚'
- en: '`label_id` â€” An integer representing the label / semantic class id corresponding
    to `segment_id`.'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_id` â€” ä»£è¡¨ä¸`segment_id`å¯¹åº”çš„æ ‡ç­¾/è¯­ä¹‰ç±»åˆ«IDçš„æ•´æ•°ã€‚'
- en: '`was_fused` â€” a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False`
    otherwise. Multiple instances of the same class / label were fused and assigned
    a single `segment_id`.'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`was_fused` â€” ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œå¦‚æœ`label_id`åœ¨`label_ids_to_fuse`ä¸­ï¼Œåˆ™ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`ã€‚ç›¸åŒç±»åˆ«/æ ‡ç­¾çš„å¤šä¸ªå®ä¾‹è¢«èåˆå¹¶åˆ†é…ä¸€ä¸ªå•ç‹¬çš„`segment_id`ã€‚'
- en: '`score` â€” Prediction score of segment with `segment_id`.'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` â€” å…·æœ‰`segment_id`çš„æ®µçš„é¢„æµ‹åˆ†æ•°ã€‚'
- en: Converts the output of `MaskFormerForInstanceSegmentationOutput` into image
    panoptic segmentation predictions. Only supports PyTorch.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: å°†`MaskFormerForInstanceSegmentationOutput`çš„è¾“å‡ºè½¬æ¢ä¸ºå›¾åƒå…¨æ™¯åˆ†å‰²é¢„æµ‹ã€‚ä»…æ”¯æŒPyTorchã€‚
- en: OneFormerProcessor
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OneFormerProcessor
- en: '### `class transformers.OneFormerProcessor`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OneFormerProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L29)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L29)'
- en: '[PRE10]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`image_processor` ([OneFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor))
    â€” The image processor is a required input.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor`ï¼ˆ[OneFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor)ï¼‰â€”
    å›¾åƒå¤„ç†å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: '`tokenizer` ([`CLIPTokenizer`, `CLIPTokenizerFast`]) â€” The tokenizer is a required
    input.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`ï¼ˆ[`CLIPTokenizer`ï¼Œ`CLIPTokenizerFast`]ï¼‰â€” åˆ†è¯å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: '`max_seq_len` (`int`, *optional*, defaults to 77)) â€” Sequence length for input
    text list.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_seq_len`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º77ï¼‰â€” è¾“å…¥æ–‡æœ¬åˆ—è¡¨çš„åºåˆ—é•¿åº¦ã€‚'
- en: '`task_seq_len` (`int`, *optional*, defaults to 77) â€” Sequence length for input
    task token.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_seq_len`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º77ï¼‰â€” è¾“å…¥ä»»åŠ¡ä»¤ç‰Œçš„åºåˆ—é•¿åº¦ã€‚'
- en: Constructs an OneFormer processor which wraps [OneFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor)
    and [CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)/[CLIPTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizerFast)
    into a single processor that inherits both the image processor and tokenizer functionalities.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªOneFormerå¤„ç†å™¨ï¼Œå°†[OneFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor)å’Œ[CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)/[CLIPTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizerFast)åŒ…è£…æˆä¸€ä¸ªå•ä¸€å¤„ç†å™¨ï¼Œç»§æ‰¿äº†å›¾åƒå¤„ç†å™¨å’Œæ ‡è®°åŒ–å™¨çš„åŠŸèƒ½ã€‚
- en: '#### `encode_inputs`'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_inputs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L146)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L146)'
- en: '[PRE11]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This method forwards all its arguments to [OneFormerImageProcessor.encode_inputs()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.encode_inputs)
    and then tokenizes the task_inputs. Please refer to the docstring of this method
    for more information.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°[OneFormerImageProcessor.encode_inputs()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.encode_inputs)ï¼Œç„¶åå¯¹ä»»åŠ¡è¾“å…¥è¿›è¡Œæ ‡è®°åŒ–ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚
- en: '#### `post_process_instance_segmentation`'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_instance_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L193)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L193)'
- en: '[PRE12]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This method forwards all its arguments to [OneFormerImageProcessor.post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation).
    Please refer to the docstring of this method for more information.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°[OneFormerImageProcessor.post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚
- en: '#### `post_process_panoptic_segmentation`'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_panoptic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L200)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L200)'
- en: '[PRE13]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This method forwards all its arguments to [OneFormerImageProcessor.post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation).
    Please refer to the docstring of this method for more information.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°[OneFormerImageProcessor.post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_semantic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L186)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L186)'
- en: '[PRE14]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This method forwards all its arguments to [OneFormerImageProcessor.post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_semantic_segmentation).
    Please refer to the docstring of this method for more information.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°[OneFormerImageProcessor.post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_semantic_segmentation)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚
- en: OneFormerModel
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OneFormerModel
- en: '### `class transformers.OneFormerModel`'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OneFormerModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L2898)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L2898)'
- en: '[PRE15]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig)ï¼‰-
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare OneFormer Model outputting raw hidden-states without any specific head
    on top. This model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„OneFormeræ¨¡å‹ï¼Œåœ¨é¡¶éƒ¨æ²¡æœ‰ä»»ä½•ç‰¹å®šçš„å¤´éƒ¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ã€‚æ­¤æ¨¡å‹æ˜¯PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L2919)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L2919)'
- en: '[PRE16]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [OneFormerProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor).
    See `OneFormerProcessor.__call__()` for details.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰-
    åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[OneFormerProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor)è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`OneFormerProcessor.__call__()`ã€‚'
- en: '`task_inputs` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” Task inputs. Task inputs can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `OneFormerProcessor.__call__()` for details.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_inputs`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰- ä»»åŠ¡è¾“å…¥ã€‚ä»»åŠ¡è¾“å…¥å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`OneFormerProcessor.__call__()`ã€‚'
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    â€” Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……åƒç´ å€¼ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚åœ¨`[0,
    1]`ä¸­é€‰æ‹©çš„æ©ç å€¼ï¼š'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºçœŸå®åƒç´ ä¸º1ï¼ˆå³`not masked`ï¼‰ï¼Œ
- en: 0 for pixels that are padding (i.e. `masked`).
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºå¡«å……çš„åƒç´ ä¸º0ï¼ˆå³`masked`ï¼‰ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of Detrâ€™s decoder attention layers.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›Detrè§£ç å™¨æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a `~OneFormerModelOutput`
    instead of a plain tuple.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›`~OneFormerModelOutput`è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig))
    and inputs.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput)æˆ–`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the encoder model at the output of
    each stage.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º+ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚ç¼–ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: '`pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the pixel decoder model at the output
    of each stage.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º+ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚åƒç´ è§£ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: '`transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*,
    returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the transformer decoder at the output
    of each stage.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º+ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚transformerè§£ç å™¨åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: '`transformer_decoder_object_queries` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, hidden_dim)`) Output object queries from the last layer in the transformer
    decoder.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_object_queries`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`çš„`torch.FloatTensor`ï¼‰-
    transformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„è¾“å‡ºå¯¹è±¡æŸ¥è¯¢ã€‚'
- en: '`transformer_decoder_contrastive_queries` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, hidden_dim)`) Contrastive queries from the transformer decoder.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_contrastive_queries`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`çš„`torch.FloatTensor`ï¼‰-
    æ¥è‡ªtransformerè§£ç å™¨çš„å¯¹æ¯”æŸ¥è¯¢ã€‚'
- en: '`transformer_decoder_mask_predictions` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, height, width)`) Mask Predictions from the last layer in the transformer
    decoder.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_mask_predictions`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, height,
    width)`çš„`torch.FloatTensor`ï¼‰- transformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„æ©ç é¢„æµ‹ã€‚'
- en: '`transformer_decoder_class_predictions` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, num_classes+1)`) â€” Class Predictions from the last layer in the transformer
    decoder.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_class_predictions`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, num_classes+1)`çš„`torch.FloatTensor`ï¼‰-
    transformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„ç±»åˆ«é¢„æµ‹ã€‚'
- en: '`transformer_decoder_auxiliary_predictions` (Tuple of Dict of `str, torch.FloatTensor`,
    *optional*) â€” Tuple of class and mask predictions from each layer of the transformer
    decoder.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_auxiliary_predictions`ï¼ˆ`strï¼Œtorch.FloatTensor`å­—å…¸çš„å…ƒç»„ï¼Œ*å¯é€‰*ï¼‰-
    æ¥è‡ªtransformerè§£ç å™¨æ¯ä¸€å±‚çš„ç±»åˆ«å’Œæ©ç é¢„æµ‹çš„å…ƒç»„ã€‚'
- en: '`text_queries` (`torch.FloatTensor`, *optional* of shape `(batch_size, num_queries,
    hidden_dim)`) Text queries derived from the input text list used for calculating
    contrastive loss during training.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_queries`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`ï¼‰-
    ä»ç”¨äºè®­ç»ƒæœŸé—´è®¡ç®—å¯¹æ¯”æŸå¤±çš„è¾“å…¥æ–‡æœ¬åˆ—è¡¨æ´¾ç”Ÿçš„æ–‡æœ¬æŸ¥è¯¢ã€‚'
- en: '`task_token` (`torch.FloatTensor` of shape `(batch_size, hidden_dim)`) 1D task
    token to condition the queries.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_token`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, hidden_dim)`çš„`torch.FloatTensor`ï¼‰- ç”¨äºæ¡ä»¶æŸ¥è¯¢çš„ä¸€ç»´ä»»åŠ¡ä»¤ç‰Œã€‚'
- en: '`attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size,
    num_heads, sequence_length, sequence_length)`. Self and Cross Attentions weights
    from transformer decoder.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tuple(torch.FloatTensor)`å…ƒç»„ã€‚æ¥è‡ªå˜å‹å™¨è§£ç å™¨çš„è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æƒé‡ã€‚'
- en: '`OneFormerModelOutput`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`OneFormerModelOutput`'
- en: The [OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel)
    forward method, overrides the `__call__` special method.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE17]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: OneFormerForUniversalSegmentation
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OneFormerForUniversalSegmentation
- en: '### `class transformers.OneFormerForUniversalSegmentation`'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OneFormerForUniversalSegmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L3027)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L3027)'
- en: '[PRE18]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: OneFormer Model for instance, semantic and panoptic image segmentation. This
    model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: OneFormeræ¨¡å‹ä¾‹å¦‚ï¼Œè¯­ä¹‰å’Œå…¨æ™¯å›¾åƒåˆ†å‰²ã€‚æ­¤æ¨¡å‹æ˜¯PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L3098)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L3098)'
- en: '[PRE19]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [OneFormerProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor).
    See `OneFormerProcessor.__call__()` for details.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€”
    åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[OneFormerProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor)è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`OneFormerProcessor.__call__()`ã€‚'
- en: '`task_inputs` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    â€” Task inputs. Task inputs can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `OneFormerProcessor.__call__()` for details.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_inputs`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰â€” ä»»åŠ¡è¾“å…¥ã€‚ä»»åŠ¡è¾“å…¥å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`OneFormerProcessor.__call__()`ã€‚'
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    â€” Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……åƒç´ å€¼ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0,
    1]`ä¸­ï¼š'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºçœŸå®åƒç´ ï¼ˆå³`not masked`ï¼‰çš„åƒç´ ä¸º1ï¼Œ
- en: 0 for pixels that are padding (i.e. `masked`).
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºå¡«å……åƒç´ ï¼ˆå³`masked`ï¼‰çš„åƒç´ ä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æ©ç æ˜¯ä»€ä¹ˆï¼Ÿ
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of Detrâ€™s decoder attention layers.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›Detrè§£ç å™¨æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a `~OneFormerModelOutput`
    instead of a plain tuple.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›`~OneFormerModelOutput`è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`text_inputs` (`List[torch.Tensor]`, *optional*) â€” Tensor fof shape `(num_queries,
    sequence_length)` to be fed to a model'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_inputs`ï¼ˆ`List[torch.Tensor]`ï¼Œ*å¯é€‰*ï¼‰â€” å½¢çŠ¶ä¸º`(num_queries, sequence_length)`çš„å¼ é‡ï¼Œå°†è¢«é¦ˆé€åˆ°æ¨¡å‹'
- en: '`mask_labels` (`List[torch.Tensor]`, *optional*) â€” List of mask labels of shape
    `(num_labels, height, width)` to be fed to a model'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_labels`ï¼ˆ`List[torch.Tensor]`ï¼Œ*å¯é€‰*ï¼‰â€” å½¢çŠ¶ä¸º`(num_labels, height, width)`çš„æ©ç æ ‡ç­¾åˆ—è¡¨ï¼Œå°†è¢«é¦ˆé€åˆ°æ¨¡å‹'
- en: '`class_labels` (`List[torch.LongTensor]`, *optional*) â€” list of target class
    labels of shape `(num_labels, height, width)` to be fed to a model. They identify
    the labels of `mask_labels`, e.g. the label of `mask_labels[i][j]` if `class_labels[i][j]`.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_labels`ï¼ˆ`List[torch.LongTensor]`ï¼Œ*å¯é€‰*ï¼‰â€” å½¢çŠ¶ä¸º`(num_labels, height, width)`çš„ç›®æ ‡ç±»æ ‡ç­¾åˆ—è¡¨ï¼Œå°†è¢«é¦ˆé€åˆ°æ¨¡å‹ã€‚å®ƒä»¬æ ‡è¯†`mask_labels`çš„æ ‡ç­¾ï¼Œä¾‹å¦‚ï¼Œå¦‚æœ`class_labels[i][j]`çš„æ ‡ç­¾æ˜¯`mask_labels[i][j]`ã€‚'
- en: Returns
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig))
    and inputs.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚'
- en: '`loss` (`torch.Tensor`, *optional*) â€” The computed loss, returned when labels
    are present.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆ`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”è®¡ç®—å¾—åˆ°çš„æŸå¤±ï¼Œåœ¨å­˜åœ¨æ ‡ç­¾æ—¶è¿”å›ã€‚'
- en: '`class_queries_logits` (`torch.FloatTensor`) â€” A tensor of shape `(batch_size,
    num_queries, num_labels + 1)` representing the proposed classes for each query.
    Note the `+ 1` is needed because we incorporate the null class.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_queries_logits`ï¼ˆ`torch.FloatTensor`ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size, num_queries, num_labels
    + 1)`çš„å¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæŸ¥è¯¢çš„æè®®ç±»åˆ«ã€‚è¯·æ³¨æ„ï¼Œéœ€è¦`+ 1`ï¼Œå› ä¸ºæˆ‘ä»¬åŒ…å«äº†ç©ºç±»ã€‚'
- en: '`masks_queries_logits` (`torch.FloatTensor`) â€” A tensor of shape `(batch_size,
    num_queries, height, width)` representing the proposed masks for each query.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`masks_queries_logits`ï¼ˆ`torch.FloatTensor`ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size, num_queries, height,
    width)`çš„å¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæŸ¥è¯¢çš„æè®®æ©ç ã€‚'
- en: '`auxiliary_predictions` (List of Dict of `str, torch.FloatTensor`, *optional*)
    â€” List of class and mask predictions from each layer of the transformer decoder.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_predictions`ï¼ˆ`str`ï¼Œ`torch.FloatTensor`å­—å…¸çš„åˆ—è¡¨ï¼Œ*å¯é€‰*ï¼‰â€”æ¥è‡ªtransformerè§£ç å™¨æ¯ä¸€å±‚çš„ç±»åˆ«å’Œæ©ç é¢„æµ‹çš„åˆ—è¡¨ã€‚'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the encoder model at the output of
    each stage.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size,
    num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚ç¼–ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: '`pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the pixel decoder model at the output
    of each stage.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size,
    num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚åƒç´ è§£ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: '`transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*,
    returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the transformer decoder at the output
    of each stage.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size,
    sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚transformerè§£ç å™¨åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚'
- en: '`transformer_decoder_object_queries` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, hidden_dim)`) Output object queries from the last layer in the transformer
    decoder.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_object_queries`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries,
    hidden_dim)`ï¼‰â€”transformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„è¾“å‡ºå¯¹è±¡æŸ¥è¯¢ã€‚'
- en: '`transformer_decoder_contrastive_queries` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, hidden_dim)`) Contrastive queries from the transformer decoder.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_contrastive_queries`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,
    num_queries, hidden_dim)`ï¼‰â€”transformerè§£ç å™¨ä¸­çš„å¯¹æ¯”æŸ¥è¯¢ã€‚'
- en: '`transformer_decoder_mask_predictions` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, height, width)`) Mask Predictions from the last layer in the transformer
    decoder.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_mask_predictions`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,
    num_queries, height, width)`ï¼‰â€”transformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„æ©ç é¢„æµ‹ã€‚'
- en: '`transformer_decoder_class_predictions` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, num_classes+1)`) â€” Class Predictions from the last layer in the transformer
    decoder.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_class_predictions`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,
    num_queries, num_classes+1)`ï¼‰â€”transformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„ç±»åˆ«é¢„æµ‹ã€‚'
- en: '`transformer_decoder_auxiliary_predictions` (List of Dict of `str, torch.FloatTensor`,
    *optional*) â€” List of class and mask predictions from each layer of the transformer
    decoder.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_auxiliary_predictions`ï¼ˆ`str`ï¼Œ`torch.FloatTensor`å­—å…¸çš„åˆ—è¡¨ï¼Œ*å¯é€‰*ï¼‰â€”æ¥è‡ªtransformerè§£ç å™¨æ¯ä¸€å±‚çš„ç±»åˆ«å’Œæ©ç é¢„æµ‹çš„åˆ—è¡¨ã€‚'
- en: '`text_queries` (`torch.FloatTensor`, *optional* of shape `(batch_size, num_queries,
    hidden_dim)`) Text queries derived from the input text list used for calculating
    contrastive loss during training.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_queries`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`ï¼‰â€”ä»ç”¨äºè®­ç»ƒæœŸé—´è®¡ç®—å¯¹æ¯”æŸå¤±çš„è¾“å…¥æ–‡æœ¬åˆ—è¡¨æ´¾ç”Ÿçš„æ–‡æœ¬æŸ¥è¯¢ã€‚'
- en: '`task_token` (`torch.FloatTensor` of shape `(batch_size, hidden_dim)`) 1D task
    token to condition the queries.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_token`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, hidden_dim)`ï¼‰â€”ç”¨äºæ¡ä»¶æŸ¥è¯¢çš„ä¸€ç»´ä»»åŠ¡ä»¤ç‰Œã€‚'
- en: '`attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size,
    num_heads, sequence_length, sequence_length)`. Self and Cross Attentions weights
    from transformer decoder.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tuple(torch.FloatTensor))`, *å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚æ¥è‡ªå˜å‹å™¨è§£ç å™¨çš„è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æƒé‡ã€‚'
- en: '`OneFormerUniversalSegmentationOutput`'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '`OneFormerUniversalSegmentationOutput`'
- en: The [OneFormerForUniversalSegmentation](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation)
    forward method, overrides the `__call__` special method.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[OneFormerForUniversalSegmentation](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: 'Universal segmentation example:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: é€šç”¨åˆ†å‰²ç¤ºä¾‹ï¼š
- en: '[PRE20]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
