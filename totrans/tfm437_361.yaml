- en: OneFormer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/oneformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/oneformer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The OneFormer model was proposed in [OneFormer: One Transformer to Rule Universal
    Image Segmentation](https://arxiv.org/abs/2211.06220) by Jitesh Jain, Jiachen
    Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi. OneFormer is a universal
    image segmentation framework that can be trained on a single panoptic dataset
    to perform semantic, instance, and panoptic segmentation tasks. OneFormer uses
    a task token to condition the model on the task in focus, making the architecture
    task-guided for training, and task-dynamic for inference.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8cd6226185941da2d5d5f4ca249f7bd2.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
- en: 'The abstract from the paper is the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '*Universal Image Segmentation is not a new concept. Past attempts to unify
    image segmentation in the last decades include scene parsing, panoptic segmentation,
    and, more recently, new panoptic architectures. However, such panoptic architectures
    do not truly unify image segmentation because they need to be trained individually
    on the semantic, instance, or panoptic segmentation to achieve the best performance.
    Ideally, a truly universal framework should be trained only once and achieve SOTA
    performance across all three image segmentation tasks. To that end, we propose
    OneFormer, a universal image segmentation framework that unifies segmentation
    with a multi-task train-once design. We first propose a task-conditioned joint
    training strategy that enables training on ground truths of each domain (semantic,
    instance, and panoptic segmentation) within a single multi-task training process.
    Secondly, we introduce a task token to condition our model on the task at hand,
    making our model task-dynamic to support multi-task training and inference. Thirdly,
    we propose using a query-text contrastive loss during training to establish better
    inter-task and inter-class distinctions. Notably, our single OneFormer model outperforms
    specialized Mask2Former models across all three segmentation tasks on ADE20k,
    CityScapes, and COCO, despite the latter being trained on each of the three tasks
    individually with three times the resources. With new ConvNeXt and DiNAT backbones,
    we observe even more performance improvement. We believe OneFormer is a significant
    step towards making image segmentation more universal and accessible.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: The figure below illustrates the architecture of OneFormer. Taken from the [original
    paper](https://arxiv.org/abs/2211.06220).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8617928e627dd67ce070d58bb21d63e1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: This model was contributed by [Jitesh Jain](https://huggingface.co/praeclarumjj3).
    The original code can be found [here](https://github.com/SHI-Labs/OneFormer).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OneFormer requires two inputs during inference: *image* and *task token*.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During training, OneFormer only uses panoptic annotations.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to train the model in a distributed environment across multiple
    nodes, then one should update the `get_num_masks` function inside in the `OneFormerLoss`
    class of `modeling_oneformer.py`. When training on multiple nodes, this should
    be set to the average number of target masks across all nodes, as can be seen
    in the original implementation [here](https://github.com/SHI-Labs/OneFormer/blob/33ebb56ed34f970a30ae103e786c0cb64c653d9a/oneformer/modeling/criterion.py#L287).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One can use [OneFormerProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor)
    to prepare input images and task inputs for the model and optional targets for
    the model. `OneformerProcessor` wraps [OneFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor)
    and [CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)
    into a single instance to both prepare the images and encode the task inputs.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To get the final segmentation, depending on the task, you can call [post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor.post_process_semantic_segmentation)
    or [post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation)
    or [post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation).
    All three tasks can be solved using [OneFormerForUniversalSegmentation](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation)
    output, panoptic segmentation accepts an optional `label_ids_to_fuse` argument
    to fuse instances of the target object/s (e.g. sky) together.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要获得最终的分割结果，可以调用[post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor.post_process_semantic_segmentation)或[post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation)或[post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation)。这三个任务都可以使用[OneFormerForUniversalSegmentation](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation)的输出来解决，全景分割接受一个可选的`label_ids_to_fuse`参数，用于将目标对象（例如天空）的实例融合在一起。
- en: Resources
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with OneFormer.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一份官方Hugging Face和社区（由🌎表示）资源列表，可帮助您开始使用OneFormer。
- en: Demo notebooks regarding inference + fine-tuning on custom data can be found
    [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/OneFormer).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于推断+在自定义数据上进行微调的演示笔记本可以在[这里](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/OneFormer)找到。
- en: If you’re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we will review it. The resource should ideally
    demonstrate something new instead of duplicating an existing resource.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣提交资源以包含在此处，请随时提交拉取请求，我们将对其进行审查。资源应该展示一些新的东西，而不是重复现有资源。
- en: OneFormer specific outputs
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OneFormer特定输出
- en: '### `class transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L803)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L803)'
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the encoder model at the output of
    each stage.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — `torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个阶段的输出），形状为`(batch_size, num_channels,
    height, width)`。编码器模型在每个阶段输出的隐藏状态（也称为特征图）。'
- en: '`pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the pixel decoder model at the output
    of each stage.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — `torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个阶段的输出），形状为`(batch_size, num_channels,
    height, width)`。像素解码器模型在每个阶段输出的隐藏状态（也称为特征图）。'
- en: '`transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*,
    returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the transformer decoder at the output
    of each stage.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*,
    当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — `torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个阶段的输出），形状为`(batch_size, sequence_length, hidden_size)`。transformer解码器在每个阶段输出的隐藏状态（也称为特征图）。'
- en: '`transformer_decoder_object_queries` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, hidden_dim)`) — Output object queries from the last layer in the
    transformer decoder.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_object_queries` (`torch.FloatTensor`，形状为`(batch_size,
    num_queries, hidden_dim)`) — 来自transformer解码器最后一层的输出对象查询。'
- en: '`transformer_decoder_contrastive_queries` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, hidden_dim)`) — Contrastive queries from the transformer decoder.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_contrastive_queries` (`torch.FloatTensor`，形状为`(batch_size,
    num_queries, hidden_dim)`) — 来自transformer解码器的对比查询。'
- en: '`transformer_decoder_mask_predictions` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, height, width)`) — Mask Predictions from the last layer in the transformer
    decoder.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_mask_predictions` (`torch.FloatTensor`，形状为`(batch_size,
    num_queries, height, width)`) — 来自transformer解码器最后一层的掩码预测。'
- en: '`transformer_decoder_class_predictions` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, num_classes+1)`) — Class Predictions from the last layer in the transformer
    decoder.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_class_predictions` (`torch.FloatTensor`，形状为`(batch_size,
    num_queries, num_classes+1)`) — 来自transformer解码器最后一层的类别预测。'
- en: '`transformer_decoder_auxiliary_predictions` (Tuple of Dict of `str, torch.FloatTensor`,
    *optional*) — Tuple of class and mask predictions from each layer of the transformer
    decoder.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_auxiliary_predictions`（`str, torch.FloatTensor`字典的元组，*可选*）
    — 来自transformer解码器每一层的类别和掩码预测的元组。'
- en: '`text_queries` (`torch.FloatTensor`, *optional* of shape `(batch_size, num_queries,
    hidden_dim)`) — Text queries derived from the input text list used for calculating
    contrastive loss during training.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_queries` (`torch.FloatTensor`，*可选*，形状为`(batch_size, num_queries, hidden_dim)`)
    — 从用于计算对比损失的输入文本列表派生的文本查询。'
- en: '`task_token` (`torch.FloatTensor` of shape `(batch_size, hidden_dim)`) — 1D
    task token to condition the queries.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_token`（形状为`(batch_size, hidden_dim)`的`torch.FloatTensor`）—用于条件查询的一维任务令牌。'
- en: '`attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size,
    num_heads, sequence_length, sequence_length)`. Self and Cross Attentions weights
    from transformer decoder.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）—形状为`(batch_size,
    num_heads, sequence_length, sequence_length)`的`tuple(torch.FloatTensor)`元组（每层一个）。transformer解码器的自注意力和交叉注意力权重。'
- en: Class for outputs of [OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel).
    This class returns all the needed hidden states to compute the logits.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 用于[OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel)输出的类。此类返回计算logits所需的所有隐藏状态。
- en: '### `class transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L853)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L853)'
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.Tensor`, *optional*) — The computed loss, returned when labels
    are present.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（`torch.Tensor`，*可选*）—当存在标签时返回计算的损失。'
- en: '`class_queries_logits` (`torch.FloatTensor`) — A tensor of shape `(batch_size,
    num_queries, num_labels + 1)` representing the proposed classes for each query.
    Note the `+ 1` is needed because we incorporate the null class.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_queries_logits`（`torch.FloatTensor`）—形状为`(batch_size, num_queries, num_labels
    + 1)`的张量，表示每个查询的建议类别。请注意，需要`+ 1`，因为我们包含了空类。'
- en: '`masks_queries_logits` (`torch.FloatTensor`) — A tensor of shape `(batch_size,
    num_queries, height, width)` representing the proposed masks for each query.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`masks_queries_logits`（`torch.FloatTensor`）—形状为`(batch_size, num_queries, height,
    width)`的张量，表示每个查询的建议掩码。'
- en: '`auxiliary_predictions` (List of Dict of `str, torch.FloatTensor`, *optional*)
    — List of class and mask predictions from each layer of the transformer decoder.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_predictions`（`str，torch.FloatTensor`字典列表，*可选*）—来自transformer解码器每一层的类别和掩码预测的列表。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the encoder model at the output of
    each stage.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—形状为`(batch_size,
    num_channels, height, width)`的`torch.FloatTensor`元组（一个用于嵌入的输出+一个用于每个阶段的输出）。编码器模型在每个阶段输出的隐藏状态（也称为特征图）。'
- en: '`pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the pixel decoder model at the output
    of each stage.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—形状为`(batch_size,
    num_channels, height, width)`的`torch.FloatTensor`元组（一个用于嵌入的输出+一个用于每个阶段的输出）。像素解码器模型在每个阶段输出的隐藏状态（也称为特征图）。'
- en: '`transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*,
    returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the transformer decoder at the output
    of each stage.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—形状为`(batch_size,
    sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出+一个用于每个阶段的输出）。transformer解码器在每个阶段输出的隐藏状态（也称为特征图）。'
- en: '`transformer_decoder_object_queries` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, hidden_dim)`) — Output object queries from the last layer in the
    transformer decoder.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_object_queries`（形状为`(batch_size, num_queries, hidden_dim)`的`torch.FloatTensor`）—来自transformer解码器中最后一层的输出对象查询。'
- en: '`transformer_decoder_contrastive_queries` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, hidden_dim)`) — Contrastive queries from the transformer decoder.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_contrastive_queries`（形状为`(batch_size, num_queries, hidden_dim)`的`torch.FloatTensor`）—来自transformer解码器的对比查询。'
- en: '`transformer_decoder_mask_predictions` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, height, width)`) — Mask Predictions from the last layer in the transformer
    decoder.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_mask_predictions`（形状为`(batch_size, num_queries, height,
    width)`的`torch.FloatTensor`）—来自transformer解码器中最后一层的掩码预测。'
- en: '`transformer_decoder_class_predictions` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, num_classes+1)`) — Class Predictions from the last layer in the transformer
    decoder.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_class_predictions`（形状为`(batch_size, num_queries, num_classes+1)`的`torch.FloatTensor`）—来自transformer解码器中最后一层的类别预测。'
- en: '`transformer_decoder_auxiliary_predictions` (List of Dict of `str, torch.FloatTensor`,
    *optional*) — List of class and mask predictions from each layer of the transformer
    decoder.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_auxiliary_predictions`（`str，torch.FloatTensor`字典列表，*可选*）—来自transformer解码器每一层的类别和掩码预测的列表。'
- en: '`text_queries` (`torch.FloatTensor`, *optional* of shape `(batch_size, num_queries,
    hidden_dim)`) — Text queries derived from the input text list used for calculating
    contrastive loss during training.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_queries`（`torch.FloatTensor`，*可选*，形状为`(batch_size, num_queries, hidden_dim)`）—从用于训练期间计算对比损失的输入文本列表派生的文本查询。'
- en: '`task_token` (`torch.FloatTensor` of shape `(batch_size, hidden_dim)`) — 1D
    task token to condition the queries.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_token`（形状为`(batch_size, hidden_dim)`的`torch.FloatTensor`）—用于条件查询的一维任务令牌。'
- en: '`attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size,
    num_heads, sequence_length, sequence_length)`. Self and Cross Attentions weights
    from transformer decoder.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tuple(torch.FloatTensor)`元组。来自transformer解码器的自注意力和交叉注意力权重。'
- en: Class for outputs of `OneFormerForUniversalSegmentationOutput`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 用于`OneFormerForUniversalSegmentationOutput`的输出类。
- en: This output can be directly passed to [post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_semantic_segmentation)
    or [post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation)
    or [post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation)
    depending on the task. Please, see [`~OneFormerImageProcessor] for details regarding
    usage.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出可以直接传递给[post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_semantic_segmentation)或[post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation)或[post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation)，具体取决于任务。请参阅[`~OneFormerImageProcessor]以获取有关用法的详细信息。
- en: OneFormerConfig
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OneFormerConfig
- en: '### `class transformers.OneFormerConfig`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OneFormerConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/configuration_oneformer.py#L33)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/configuration_oneformer.py#L33)'
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`backbone_config` (`PretrainedConfig`, *optional*, defaults to `SwinConfig`)
    — The configuration of the backbone model.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backbone_config` (`PretrainedConfig`, *optional*, defaults to `SwinConfig`)
    — 主干模型的配置。'
- en: '`ignore_value` (`int`, *optional*, defaults to 255) — Values to be ignored
    in GT label while calculating loss.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_value` (`int`, *optional*, defaults to 255) — 在计算损失时要忽略的GT标签中的值。'
- en: '`num_queries` (`int`, *optional*, defaults to 150) — Number of object queries.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_queries` (`int`, *optional*, defaults to 150) — 对象查询的数量。'
- en: '`no_object_weight` (`float`, *optional*, defaults to 0.1) — Weight for no-object
    class predictions.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no_object_weight` (`float`, *optional*, defaults to 0.1) — 无对象类预测的权重。'
- en: '`class_weight` (`float`, *optional*, defaults to 2.0) — Weight for Classification
    CE loss.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_weight` (`float`, *optional*, defaults to 2.0) — 分类CE损失的权重。'
- en: '`mask_weight` (`float`, *optional*, defaults to 5.0) — Weight for binary CE
    loss.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_weight` (`float`, *optional*, defaults to 5.0) — 二元CE损失的权重。'
- en: '`dice_weight` (`float`, *optional*, defaults to 5.0) — Weight for dice loss.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dice_weight` (`float`, *optional*, defaults to 5.0) — Dice损失的权重。'
- en: '`contrastive_weight` (`float`, *optional*, defaults to 0.5) — Weight for contrastive
    loss.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`contrastive_weight` (`float`, *optional*, defaults to 0.5) — 对比损失的权重。'
- en: '`contrastive_temperature` (`float`, *optional*, defaults to 0.07) — Initial
    value for scaling the contrastive logits.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`contrastive_temperature` (`float`, *optional*, defaults to 0.07) — 用于缩放对比对数的初始值。'
- en: '`train_num_points` (`int`, *optional*, defaults to 12544) — Number of points
    to sample while calculating losses on mask predictions.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_num_points` (`int`, *optional*, defaults to 12544) — 在计算掩码预测损失时要采样的点数。'
- en: '`oversample_ratio` (`float`, *optional*, defaults to 3.0) — Ratio to decide
    how many points to oversample.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`oversample_ratio` (`float`, *optional*, defaults to 3.0) — 决定过采样多少点的比率。'
- en: '`importance_sample_ratio` (`float`, *optional*, defaults to 0.75) — Ratio of
    points that are sampled via importance sampling.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`importance_sample_ratio` (`float`, *optional*, defaults to 0.75) — 通过重要性采样抽样的点的比率。'
- en: '`init_std` (`float`, *optional*, defaults to 0.02) — Standard deviation for
    normal intialization.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_std` (`float`, *optional*, defaults to 0.02) — 正态初始化的标准差。'
- en: '`init_xavier_std` (`float`, *optional*, defaults to 1.0) — Standard deviation
    for xavier uniform initialization.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_xavier_std` (`float`, *optional*, defaults to 1.0) — 用于xavier均匀初始化的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — Epsilon for layer
    normalization.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — 层归一化的epsilon。'
- en: '`is_training` (`bool`, *optional*, defaults to `False`) — Whether to run in
    training or inference mode.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_training` (`bool`, *optional*, defaults to `False`) — 是否在训练或推理模式下运行。'
- en: '`use_auxiliary_loss` (`bool`, *optional*, defaults to `True`) — Whether to
    calculate loss using intermediate predictions from transformer decoder.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_auxiliary_loss` (`bool`, *optional*, defaults to `True`) — 是否使用transformer解码器的中间预测计算损失。'
- en: '`output_auxiliary_logits` (`bool`, *optional*, defaults to `True`) — Whether
    to return intermediate predictions from transformer decoder.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_auxiliary_logits` (`bool`, *optional*, defaults to `True`) — 是否从transformer解码器返回中间预测。'
- en: '`strides` (`list`, *optional*, defaults to `[4, 8, 16, 32]`) — List containing
    the strides for feature maps in the encoder.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strides` (`list`, *optional*, defaults to `[4, 8, 16, 32]`) — 包含编码器中特征图的步幅的列表。'
- en: '`task_seq_len` (`int`, *optional*, defaults to 77) — Sequence length for tokenizing
    text list input.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_seq_len` (`int`, *optional*, defaults to 77) — 用于对文本列表输入进行分词的序列长度。'
- en: '`text_encoder_width` (`int`, *optional*, defaults to 256) — Hidden size for
    text encoder.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_width` (`int`, *optional*, defaults to 256) — 文本编码器的隐藏大小。'
- en: '`text_encoder_context_length` (`int`, *optional*, defaults to 77) — Input sequence
    length for text encoder.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_context_length` (`int`, *optional*, defaults to 77) — 文本编码器的输入序列长度。'
- en: '`text_encoder_num_layers` (`int`, *optional*, defaults to 6) — Number of layers
    for transformer in text encoder.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_num_layers` (`int`, *optional*, defaults to 6) — 文本编码器中transformer的层数。'
- en: '`text_encoder_vocab_size` (`int`, *optional*, defaults to 49408) — Vocabulary
    size for tokenizer.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_vocab_size` (`int`, *optional*, defaults to 49408) — 分词器的词汇量。'
- en: '`text_encoder_proj_layers` (`int`, *optional*, defaults to 2) — Number of layers
    in MLP for project text queries.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_proj_layers` (`int`, *optional*, defaults to 2) — 用于项目文本查询的MLP中的层数。'
- en: '`text_encoder_n_ctx` (`int`, *optional*, defaults to 16) — Number of learnable
    text context queries.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_n_ctx` (`int`, *optional*, 默认为 16) — 可学习文本上下文查询的数量。'
- en: '`conv_dim` (`int`, *optional*, defaults to 256) — Feature map dimension to
    map outputs from the backbone.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_dim` (`int`, *optional*, 默认为 256) — 从骨干网络映射输出的特征图维度。'
- en: '`mask_dim` (`int`, *optional*, defaults to 256) — Dimension for feature maps
    in pixel decoder.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_dim` (`int`, *optional*, 默认为 256) — 像素解码器中特征图的维度。'
- en: '`hidden_dim` (`int`, *optional*, defaults to 256) — Dimension for hidden states
    in transformer decoder.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dim` (`int`, *optional*, 默认为 256) — 变压器解码器中隐藏状态的维度。'
- en: '`encoder_feedforward_dim` (`int`, *optional*, defaults to 1024) — Dimension
    for FFN layer in pixel decoder.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_feedforward_dim` (`int`, *optional*, 默认为 1024) — 像素解码器中FFN层的维度。'
- en: '`norm` (`str`, *optional*, defaults to `"GN"`) — Type of normalization.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm` (`str`, *optional*, 默认为 `"GN"`) — 归一化类型。'
- en: '`encoder_layers` (`int`, *optional*, defaults to 6) — Number of layers in pixel
    decoder.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layers` (`int`, *optional*, 默认为 6) — 像素解码器中的层数。'
- en: '`decoder_layers` (`int`, *optional*, defaults to 10) — Number of layers in
    transformer decoder.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layers` (`int`, *optional*, 默认为 10) — 变压器解码器中的层数。'
- en: '`use_task_norm` (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the task token.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_task_norm` (`bool`, *optional*, 默认为 `True`) — 是否对任务令牌进行归一化。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 8) — Number of attention
    heads in transformer layers in the pixel and transformer decoders.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, 默认为 8) — 像素和变压器解码器中的注意力头数。'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — Dropout probability for
    pixel and transformer decoders.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, 默认为 0.1) — 像素和变压器解码器的丢失概率。'
- en: '`dim_feedforward` (`int`, *optional*, defaults to 2048) — Dimension for FFN
    layer in transformer decoder.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim_feedforward` (`int`, *optional*, 默认为 2048) — 变压器解码器中FFN层的维度。'
- en: '`pre_norm` (`bool`, *optional*, defaults to `False`) — Whether to normalize
    hidden states before attention layers in transformer decoder.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pre_norm` (`bool`, *optional*, 默认为 `False`) — 是否在变压器解码器中的注意力层之前对隐藏状态进行归一化。'
- en: '`enforce_input_proj` (`bool`, *optional*, defaults to `False`) — Whether to
    project hidden states in transformer decoder.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enforce_input_proj` (`bool`, *optional*, 默认为 `False`) — 是否在变压器解码器中投影隐藏状态。'
- en: '`query_dec_layers` (`int`, *optional*, defaults to 2) — Number of layers in
    query transformer.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query_dec_layers` (`int`, *optional*, 默认为 2) — 查询变压器中的层数。'
- en: '`common_stride` (`int`, *optional*, defaults to 4) — Common stride used for
    features in pixel decoder.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`common_stride` (`int`, *optional*, 默认为 4) — 用于像素解码器中特征的常用步幅。'
- en: This is the configuration class to store the configuration of a [OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel).
    It is used to instantiate a OneFormer model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the OneFormer [shi-labs/oneformer_ade20k_swin_tiny](https://huggingface.co/shi-labs/oneformer_ade20k_swin_tiny)
    architecture trained on [ADE20k-150](https://huggingface.co/datasets/scene_parse_150).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储 [OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel)
    配置的配置类。它用于根据指定的参数实例化一个 OneFormer 模型，定义模型架构。使用默认值实例化配置将产生类似于在 [ADE20k-150](https://huggingface.co/datasets/scene_parse_150)
    上训练的 OneFormer [shi-labs/oneformer_ade20k_swin_tiny](https://huggingface.co/shi-labs/oneformer_ade20k_swin_tiny)
    架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    并可用于控制模型输出。阅读来自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    的文档以获取更多信息。
- en: 'Examples:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: OneFormerImageProcessor
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OneFormerImageProcessor
- en: '### `class transformers.OneFormerImageProcessor`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OneFormerImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L368)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L368)'
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Whether to resize the
    input to a certain `size`.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *optional*, 默认为 `True`) — 是否将输入调整大小到特定的 `size`。'
- en: '`size` (`int`, *optional*, defaults to 800) — Resize the input to the given
    size. Only has an effect if `do_resize` is set to `True`. If size is a sequence
    like `(width, height)`, output size will be matched to this. If size is an int,
    smaller edge of the image will be matched to this number. i.e, if `height > width`,
    then image will be rescaled to `(size * height / width, size)`.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`int`, *optional*, 默认为 800) — 将输入调整大小到给定大小。仅在 `do_resize` 设置为 `True`
    时有效。如果 size 是一个类似 `(width, height)` 的序列，则输出大小将匹配到这个。如果 size 是一个整数，图像的较小边将匹配到这个数字。即，如果
    `height > width`，则图像将重新缩放为 `(size * height / width, size)`。'
- en: '`resample` (`int`, *optional*, defaults to `Resampling.BILINEAR`) — An optional
    resampling filter. This can be one of `PIL.Image.Resampling.NEAREST`, `PIL.Image.Resampling.BOX`,
    `PIL.Image.Resampling.BILINEAR`, `PIL.Image.Resampling.HAMMING`, `PIL.Image.Resampling.BICUBIC`
    or `PIL.Image.Resampling.LANCZOS`. Only has an effect if `do_resize` is set to
    `True`.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`int`, *optional*, 默认为 `Resampling.BILINEAR`) — 可选的重采样滤波器。可以是 `PIL.Image.Resampling.NEAREST`,
    `PIL.Image.Resampling.BOX`, `PIL.Image.Resampling.BILINEAR`, `PIL.Image.Resampling.HAMMING`,
    `PIL.Image.Resampling.BICUBIC` 或 `PIL.Image.Resampling.LANCZOS` 中的一个。仅在 `do_resize`
    设置为 `True` 时有效。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) — Whether to rescale
    the input to a certain `scale`.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *optional*, 默认为 `True`) — 是否将输入重新缩放到特定的 `scale`。'
- en: '`rescale_factor` (`float`, *optional*, defaults to `1/ 255`) — Rescale the
    input by the given factor. Only has an effect if `do_rescale` is set to `True`.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`, *optional*, 默认为 `1/ 255`) — 通过给定因子重新缩放输入。仅在 `do_rescale`
    设置为 `True` 时有效。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) — Whether or not to
    normalize the input with mean and standard deviation.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize`（`bool`，*可选*，默认为`True`）— 是否对输入进行均值和标准差归一化。'
- en: '`image_mean` (`int`, *optional*, defaults to `[0.485, 0.456, 0.406]`) — The
    sequence of means for each channel, to be used when normalizing images. Defaults
    to the ImageNet mean.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean`（`int`，*可选*，默认为`[0.485, 0.456, 0.406]`）— 每个通道的均值序列，在规范化图像时使用。默认为ImageNet均值。'
- en: '`image_std` (`int`, *optional*, defaults to `[0.229, 0.224, 0.225]`) — The
    sequence of standard deviations for each channel, to be used when normalizing
    images. Defaults to the ImageNet std.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std`（`int`，*可选*，默认为`[0.229, 0.224, 0.225]`）— 每个通道的标准差序列，在规范化图像时使用。默认为ImageNet标准差。'
- en: '`ignore_index` (`int`, *optional*) — Label to be assigned to background pixels
    in segmentation maps. If provided, segmentation map pixels denoted with 0 (background)
    will be replaced with `ignore_index`.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ignore_index`（`int`，*可选*）— 分割地图中要分配给背景像素的标签。如果提供，用0（背景）表示的分割地图像素将被替换为`ignore_index`。'
- en: '`do_reduce_labels` (`bool`, *optional*, defaults to `False`) — Whether or not
    to decrement all label values of segmentation maps by 1\. Usually used for datasets
    where 0 is used for background, and background itself is not included in all classes
    of a dataset (e.g. ADE20k). The background label will be replaced by `ignore_index`.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_reduce_labels`（`bool`，*可选*，默认为`False`）— 是否将所有分割地图的标签值减1。通常用于数据集中使用0表示背景，并且背景本身不包含在数据集的所有类中的情况（例如ADE20k）。背景标签将被替换为`ignore_index`。'
- en: '`repo_path` (`str`, *optional*, defaults to `"shi-labs/oneformer_demo"`) —
    Path to hub repo or local directory containing the JSON file with class information
    for the dataset. If unset, will look for `class_info_file` in the current working
    directory.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repo_path`（`str`，*可选*，默认为`"shi-labs/oneformer_demo"`）— 包含数据集类信息的JSON文件的hub存储库或本地目录的路径。如果未设置，将在当前工作目录中查找`class_info_file`。'
- en: '`class_info_file` (`str`, *optional*) — JSON file containing class information
    for the dataset. See `shi-labs/oneformer_demo/cityscapes_panoptic.json` for an
    example.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_info_file`（`str`，*可选*）— 包含数据集类信息的JSON文件。查看`shi-labs/oneformer_demo/cityscapes_panoptic.json`以获取示例。'
- en: '`num_text` (`int`, *optional*) — Number of text entries in the text input list.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_text`（`int`，*可选*）— 文本输入列表中的文本条目数。'
- en: Constructs a OneFormer image processor. The image processor can be used to prepare
    image(s), task input(s) and optional text inputs and targets for the model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个OneFormer图像处理器。该图像处理器可用于为模型准备图像、任务输入以及可选的文本输入和目标。
- en: This image processor inherits from `BaseImageProcessor` which contains most
    of the main methods. Users should refer to this superclass for more information
    regarding those methods.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图像处理器继承自`BaseImageProcessor`，其中包含大部分主要方法。用户应参考这个超类以获取有关这些方法的更多信息。
- en: '#### `preprocess`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L656)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L656)'
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#### `encode_inputs`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_inputs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L954)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L954)'
- en: '[PRE6]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values_list` (`List[ImageInput]`) — List of images (pixel values) to
    be padded. Each image should be a tensor of shape `(channels, height, width)`.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values_list`（`List[ImageInput]`）— 要填充的图像（像素值）列表。每个图像应该是形状为`(channels,
    height, width)`的张量。'
- en: '`task_inputs` (`List[str]`) — List of task values.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_inputs`（`List[str]`）— 任务值列表。'
- en: '`segmentation_maps` (`ImageInput`, *optional*) — The corresponding semantic
    segmentation maps with the pixel-wise annotations.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation_maps`（`ImageInput`，*可选*）— 具有像素级注释的相应语义分割地图。'
- en: '(`bool`, *optional*, defaults to `True`): Whether or not to pad images up to
    the largest image in a batch and create a pixel mask.'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: （`bool`，*可选*，默认为`True`）：是否将图像填充到批次中最大的图像并创建像素掩码。
- en: 'If left to the default, will return a pixel mask that is:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果保持默认设置，将返回一个像素掩码，即：
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于真实像素（即`未掩码`）为1，
- en: 0 for pixels that are padding (i.e. `masked`).
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于填充像素（即`掩码`）为0。
- en: '`instance_id_to_semantic_id` (`List[Dict[int, int]]` or `Dict[int, int]`, *optional*)
    — A mapping between object instance ids and class ids. If passed, `segmentation_maps`
    is treated as an instance segmentation map where each pixel represents an instance
    id. Can be provided as a single dictionary with a global/dataset-level mapping
    or as a list of dictionaries (one per image), to map instance ids in each image
    separately.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`instance_id_to_semantic_id`（`List[Dict[int, int]]`或`Dict[int, int]`，*可选*）—
    对象实例ID和类ID之间的映射。如果传递，`segmentation_maps`将被视为实例分割地图，其中每个像素表示一个实例ID。可以提供为单个字典，其中包含全局/数据集级别的映射，或作为字典列表（每个图像一个），以分别映射每个图像中的实例ID。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of NumPy arrays. If set to `''pt''`,
    return PyTorch `torch.Tensor` objects.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`（`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*）—
    如果设置，将返回张量而不是NumPy数组。如果设置为`''pt''`，则返回PyTorch `torch.Tensor`对象。'
- en: '`input_data_format` (`str` or `ChannelDimension`, *optional*) — The channel
    dimension format of the input image. If not provided, it will be inferred from
    the input image.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format`（`str`或`ChannelDimension`，*可选*）— 输入图像的通道维度格式。如果未提供，将从输入图像中推断。'
- en: Returns
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)'
- en: 'A [BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)
    with the following fields:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有以下字段的[BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)：
- en: '`pixel_values` — Pixel values to be fed to a model.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` — 要馈送给模型的像素值。'
- en: '`pixel_mask` — Pixel mask to be fed to a model (when `=True` or if `pixel_mask`
    is in `self.model_input_names`).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask` — 要馈送给模型的像素掩模（当 `=True` 或 `pixel_mask` 在 `self.model_input_names`
    中时）。'
- en: '`mask_labels` — Optional list of mask labels of shape `(labels, height, width)`
    to be fed to a model (when `annotations` are provided).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_labels` — 形状为 `(labels, height, width)` 的可选掩模标签列表，要馈送给模型（当提供 `annotations`
    时）。'
- en: '`class_labels` — Optional list of class labels of shape `(labels)` to be fed
    to a model (when `annotations` are provided). They identify the labels of `mask_labels`,
    e.g. the label of `mask_labels[i][j]` if `class_labels[i][j]`.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_labels` — 形状为 `(labels)` 的可选类标签列表，要馈送给模型（当提供 `annotations` 时）。它们标识 `mask_labels`
    的标签，例如，如果 `class_labels[i][j]` 的标签为 `mask_labels[i][j]`。'
- en: '`text_inputs` — Optional list of text string entries to be fed to a model (when
    `annotations` are provided). They identify the binary masks present in the image.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_inputs` — 要馈送给模型的可选文本字符串条目列表（当提供 `annotations` 时）。它们标识图像中存在的二进制掩模。'
- en: Pad images up to the largest image in a batch and create a corresponding `pixel_mask`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像填充到批处理中最大的图像，并创建相应的 `pixel_mask`。
- en: OneFormer addresses semantic segmentation with a mask classification paradigm,
    thus input segmentation maps will be converted to lists of binary masks and their
    respective labels. Let’s see an example, assuming `segmentation_maps = [[2,6,7,9]]`,
    the output will contain `mask_labels = [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]`
    (four binary masks) and `class_labels = [2,6,7,9]`, the labels for each mask.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: OneFormer 使用掩模分类范式来处理语义分割，因此输入分割地图将被转换为二进制掩模列表及其相应的标签。让我们看一个例子，假设 `segmentation_maps
    = [[2,6,7,9]]`，输出将包含 `mask_labels = [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]`（四个二进制掩模）和
    `class_labels = [2,6,7,9]`，每个掩模的标签。
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_semantic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1089)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1089)'
- en: '[PRE7]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` ([MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation))
    — Raw outputs of the model.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` ([MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation))
    — 模型的原始输出。'
- en: '`target_sizes` (`List[Tuple[int, int]]`, *optional*) — List of length (batch_size),
    where each list item (`Tuple[int, int]]`) corresponds to the requested final size
    (height, width) of each prediction. If left to None, predictions will not be resized.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`List[Tuple[int, int]]`, *可选*) — 长度为 (batch_size) 的列表，其中每个列表项
    (`Tuple[int, int]]`) 对应于每个预测的请求最终大小（高度、宽度）。如果保持为 None，则不会调整预测大小。'
- en: Returns
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[torch.Tensor]`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[torch.Tensor]`'
- en: A list of length `batch_size`, where each item is a semantic segmentation map
    of shape (height, width) corresponding to the target_sizes entry (if `target_sizes`
    is specified). Each entry of each `torch.Tensor` correspond to a semantic class
    id.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一个长度为 `batch_size` 的列表，每个项是一个形状为 (height, width) 的语义分割地图，对应于目标大小条目（如果指定了 `target_sizes`）。每个
    `torch.Tensor` 的每个条目对应于一个语义类别 id。
- en: Converts the output of [MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 将 [MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation)
    的输出转换为语义分割地图。仅支持 PyTorch。
- en: '#### `post_process_instance_segmentation`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_instance_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1139)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1139)'
- en: '[PRE8]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` (`OneFormerForUniversalSegmentationOutput`) — The outputs from `OneFormerForUniversalSegmentationOutput`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs` (`OneFormerForUniversalSegmentationOutput`) — 从 `OneFormerForUniversalSegmentationOutput`
    得到的输出。'
- en: '`task_type` (`str`, *optional)*, defaults to “instance”) — The post processing
    depends on the task token input. If the `task_type` is “panoptic”, we need to
    ignore the stuff predictions.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_type` (`str`, *可选*)，默认为“instance”) — 后处理取决于任务令牌输入。如果 `task_type` 是“panoptic”，我们需要忽略杂项预测。'
- en: '`is_demo` (`bool`, *optional)*, defaults to `True`) — Whether the model is
    in demo mode. If true, use threshold to predict final masks.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_demo` (`bool`, *可选*)，默认为 `True`) — 模型是否处于演示模式。如果为真，则使用阈值预测最终掩模。'
- en: '`threshold` (`float`, *optional*, defaults to 0.5) — The probability score
    threshold to keep predicted instance masks.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold` (`float`, *可选*, 默认为 0.5) — 保留预测实例掩模的概率分数阈值。'
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) — Threshold to use
    when turning the predicted masks into binary values.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_threshold` (`float`, *可选*, 默认为 0.5) — 在将预测的掩模转换为二进制值时使用的阈值。'
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) — The
    overlap mask area threshold to merge or discard small disconnected parts within
    each binary instance mask.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overlap_mask_area_threshold` (`float`, *可选*, 默认为 0.8) — 合并或丢弃每个二进制实例掩模中的小断开部分的重叠掩模区域阈值。'
- en: '`target_sizes` (`List[Tuple]`, *optional*) — List of length (batch_size), where
    each list item (`Tuple[int, int]]`) corresponds to the requested final size (height,
    width) of each prediction in batch. If left to None, predictions will not be resized.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`List[Tuple]`, *可选*) — 长度为 (batch_size) 的列表，其中每个列表项 (`Tuple[int,
    int]]`) 对应于批处理中每个预测的请求最终大小（高度、宽度）。如果保持为 None，则不会调整预测大小。'
- en: '`return_coco_annotation` (`bool`, *optional)*, defaults to `False`) — Whether
    to return predictions in COCO format.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_coco_annotation` (`bool`, *可选*)，默认为 `False`) — 是否以 COCO 格式返回预测。'
- en: Returns
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[Dict]`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Dict]`'
- en: 'A list of dictionaries, one per image, each dictionary containing two keys:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一个字典列表，每个图像一个字典，每个字典包含两个键：
- en: '`segmentation` — a tensor of shape `(height, width)` where each pixel represents
    a `segment_id`, set to `None` if no mask if found above `threshold`. If `target_sizes`
    is specified, segmentation is resized to the corresponding `target_sizes` entry.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation` — 形状为`(height, width)`的张量，其中每个像素代表一个`segment_id`，如果未找到高于`threshold`的掩模，则设置为`None`。如果指定了`target_sizes`，则将分割调整为相应的`target_sizes`条目。'
- en: '`segments_info` — A dictionary that contains additional information on each
    segment.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segments_info` — 包含每个段的其他信息的字典。'
- en: '`id` — an integer representing the `segment_id`.'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id` — 代表`segment_id`的整数。'
- en: '`label_id` — An integer representing the label / semantic class id corresponding
    to `segment_id`.'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_id` — 代表与`segment_id`对应的标签/语义类别ID的整数。'
- en: '`was_fused` — a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False`
    otherwise. Multiple instances of the same class / label were fused and assigned
    a single `segment_id`.'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`was_fused` — 一个布尔值，如果`label_id`在`label_ids_to_fuse`中，则为`True`，否则为`False`。相同类别/标签的多个实例被融合并分配一个单独的`segment_id`。'
- en: '`score` — Prediction score of segment with `segment_id`.'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` — 具有`segment_id`的段的预测分数。'
- en: Converts the output of `OneFormerForUniversalSegmentationOutput` into image
    instance segmentation predictions. Only supports PyTorch.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 将`OneFormerForUniversalSegmentationOutput`的输出转换为图像实例分割预测。仅支持PyTorch。
- en: '#### `post_process_panoptic_segmentation`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_panoptic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1259)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1259)'
- en: '[PRE9]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` (`MaskFormerForInstanceSegmentationOutput`) — The outputs from [MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs`（`MaskFormerForInstanceSegmentationOutput`）— 来自[MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation)的输出。'
- en: '`threshold` (`float`, *optional*, defaults to 0.5) — The probability score
    threshold to keep predicted instance masks.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`threshold`（`float`，*可选*，默认为0.5）— 保留预测实例掩模的概率分数阈值。'
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) — Threshold to use
    when turning the predicted masks into binary values.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_threshold`（`float`，*可选*，默认为0.5）— 在将预测的掩模转换为二进制值时使用的阈值。'
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) — The
    overlap mask area threshold to merge or discard small disconnected parts within
    each binary instance mask.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overlap_mask_area_threshold`（`float`，*可选*，默认为0.8）— 用于合并或丢弃每个二进制实例掩模中的小断开部分的重叠掩模区域阈值。'
- en: '`label_ids_to_fuse` (`Set[int]`, *optional*) — The labels in this state will
    have all their instances be fused together. For instance we could say there can
    only be one sky in an image, but several persons, so the label ID for sky would
    be in that set, but not the one for person.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_ids_to_fuse`（`Set[int]`，*可选*）— 此状态中的标签将使其所有实例被融合在一起。例如，我们可以说图像中只能有一个天空，但可以有几个人，因此天空的标签ID将在该集合中，但人的标签ID不在其中。'
- en: '`target_sizes` (`List[Tuple]`, *optional*) — List of length (batch_size), where
    each list item (`Tuple[int, int]]`) corresponds to the requested final size (height,
    width) of each prediction in batch. If left to None, predictions will not be resized.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes`（`List[Tuple]`，*可选*）— 长度为（batch_size）的列表，其中每个列表项（`Tuple[int,
    int]`）对应于批处理中每个预测的请求的最终大小（高度，宽度）。如果保持为None，则不会调整预测大小。'
- en: Returns
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '`List[Dict]`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Dict]`'
- en: 'A list of dictionaries, one per image, each dictionary containing two keys:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 一个字典列表，每个图像一个，每个字典包含两个键：
- en: '`segmentation` — a tensor of shape `(height, width)` where each pixel represents
    a `segment_id`, set to `None` if no mask if found above `threshold`. If `target_sizes`
    is specified, segmentation is resized to the corresponding `target_sizes` entry.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation` — 形状为`(height, width)`的张量，其中每个像素代表一个`segment_id`，如果未找到高于`threshold`的掩模，则设置为`None`。如果指定了`target_sizes`，则将分割调整为相应的`target_sizes`条目。'
- en: '`segments_info` — A dictionary that contains additional information on each
    segment.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segments_info` — 包含每个段的其他信息的字典。'
- en: '`id` — an integer representing the `segment_id`.'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id` — 代表`segment_id`的整数。'
- en: '`label_id` — An integer representing the label / semantic class id corresponding
    to `segment_id`.'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_id` — 代表与`segment_id`对应的标签/语义类别ID的整数。'
- en: '`was_fused` — a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False`
    otherwise. Multiple instances of the same class / label were fused and assigned
    a single `segment_id`.'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`was_fused` — 一个布尔值，如果`label_id`在`label_ids_to_fuse`中，则为`True`，否则为`False`。相同类别/标签的多个实例被融合并分配一个单独的`segment_id`。'
- en: '`score` — Prediction score of segment with `segment_id`.'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score` — 具有`segment_id`的段的预测分数。'
- en: Converts the output of `MaskFormerForInstanceSegmentationOutput` into image
    panoptic segmentation predictions. Only supports PyTorch.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 将`MaskFormerForInstanceSegmentationOutput`的输出转换为图像全景分割预测。仅支持PyTorch。
- en: OneFormerProcessor
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OneFormerProcessor
- en: '### `class transformers.OneFormerProcessor`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OneFormerProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L29)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L29)'
- en: '[PRE10]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image_processor` ([OneFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor))
    — The image processor is a required input.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor`（[OneFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor)）—
    图像处理器是必需的输入。'
- en: '`tokenizer` ([`CLIPTokenizer`, `CLIPTokenizerFast`]) — The tokenizer is a required
    input.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[`CLIPTokenizer`，`CLIPTokenizerFast`]）— 分词器是必需的输入。'
- en: '`max_seq_len` (`int`, *optional*, defaults to 77)) — Sequence length for input
    text list.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_seq_len`（`int`，*可选*，默认为77）— 输入文本列表的序列长度。'
- en: '`task_seq_len` (`int`, *optional*, defaults to 77) — Sequence length for input
    task token.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_seq_len`（`int`，*可选*，默认为77）— 输入任务令牌的序列长度。'
- en: Constructs an OneFormer processor which wraps [OneFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor)
    and [CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)/[CLIPTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizerFast)
    into a single processor that inherits both the image processor and tokenizer functionalities.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个OneFormer处理器，将[OneFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor)和[CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)/[CLIPTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizerFast)包装成一个单一处理器，继承了图像处理器和标记化器的功能。
- en: '#### `encode_inputs`'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_inputs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L146)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L146)'
- en: '[PRE11]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This method forwards all its arguments to [OneFormerImageProcessor.encode_inputs()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.encode_inputs)
    and then tokenizes the task_inputs. Please refer to the docstring of this method
    for more information.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将其所有参数转发到[OneFormerImageProcessor.encode_inputs()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.encode_inputs)，然后对任务输入进行标记化。有关更多信息，请参阅此方法的文档字符串。
- en: '#### `post_process_instance_segmentation`'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_instance_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L193)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L193)'
- en: '[PRE12]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This method forwards all its arguments to [OneFormerImageProcessor.post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation).
    Please refer to the docstring of this method for more information.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将其所有参数转发到[OneFormerImageProcessor.post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation)。有关更多信息，请参阅此方法的文档字符串。
- en: '#### `post_process_panoptic_segmentation`'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_panoptic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L200)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L200)'
- en: '[PRE13]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This method forwards all its arguments to [OneFormerImageProcessor.post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation).
    Please refer to the docstring of this method for more information.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将其所有参数转发到[OneFormerImageProcessor.post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation)。有关更多信息，请参阅此方法的文档字符串。
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_semantic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L186)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L186)'
- en: '[PRE14]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This method forwards all its arguments to [OneFormerImageProcessor.post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_semantic_segmentation).
    Please refer to the docstring of this method for more information.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将其所有参数转发到[OneFormerImageProcessor.post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_semantic_segmentation)。有关更多信息，请参阅此方法的文档字符串。
- en: OneFormerModel
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OneFormerModel
- en: '### `class transformers.OneFormerModel`'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OneFormerModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L2898)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L2898)'
- en: '[PRE15]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare OneFormer Model outputting raw hidden-states without any specific head
    on top. This model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的OneFormer模型，在顶部没有任何特定的头部输出原始隐藏状态。此模型是PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L2919)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L2919)'
- en: '[PRE16]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [OneFormerProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor).
    See `OneFormerProcessor.__call__()` for details.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）-
    像素值。像素值可以使用[OneFormerProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor)获得。有关详细信息，请参阅`OneFormerProcessor.__call__()`。'
- en: '`task_inputs` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Task inputs. Task inputs can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `OneFormerProcessor.__call__()` for details.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_inputs`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）- 任务输入。任务输入可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获得。有关详细信息，请参阅`OneFormerProcessor.__call__()`。'
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）- 用于避免在填充像素值上执行注意力的掩码。在`[0,
    1]`中选择的掩码值：'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于真实像素为1（即`not masked`），
- en: 0 for pixels that are padding (i.e. `masked`).
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于填充的像素为0（即`masked`）。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of Detr’s decoder attention layers.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回Detr解码器注意力层的注意力张量。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `~OneFormerModelOutput`
    instead of a plain tuple.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回`~OneFormerModelOutput`而不是普通元组。'
- en: Returns
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig))
    and inputs.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput)或`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig)）和输入的各种元素。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the encoder model at the output of
    each stage.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（一个用于嵌入的输出+一个用于每个阶段的输出）。编码器模型在每个阶段输出的隐藏状态（也称为特征图）。'
- en: '`pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the pixel decoder model at the output
    of each stage.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（一个用于嵌入的输出+一个用于每个阶段的输出）。像素解码器模型在每个阶段输出的隐藏状态（也称为特征图）。'
- en: '`transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*,
    returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the transformer decoder at the output
    of each stage.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出+一个用于每个阶段的输出）。transformer解码器在每个阶段输出的隐藏状态（也称为特征图）。'
- en: '`transformer_decoder_object_queries` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, hidden_dim)`) Output object queries from the last layer in the transformer
    decoder.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_object_queries`（形状为`(batch_size, num_queries, hidden_dim)`的`torch.FloatTensor`）-
    transformer解码器中最后一层的输出对象查询。'
- en: '`transformer_decoder_contrastive_queries` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, hidden_dim)`) Contrastive queries from the transformer decoder.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_contrastive_queries`（形状为`(batch_size, num_queries, hidden_dim)`的`torch.FloatTensor`）-
    来自transformer解码器的对比查询。'
- en: '`transformer_decoder_mask_predictions` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, height, width)`) Mask Predictions from the last layer in the transformer
    decoder.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_mask_predictions`（形状为`(batch_size, num_queries, height,
    width)`的`torch.FloatTensor`）- transformer解码器中最后一层的掩码预测。'
- en: '`transformer_decoder_class_predictions` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, num_classes+1)`) — Class Predictions from the last layer in the transformer
    decoder.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_class_predictions`（形状为`(batch_size, num_queries, num_classes+1)`的`torch.FloatTensor`）-
    transformer解码器中最后一层的类别预测。'
- en: '`transformer_decoder_auxiliary_predictions` (Tuple of Dict of `str, torch.FloatTensor`,
    *optional*) — Tuple of class and mask predictions from each layer of the transformer
    decoder.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_auxiliary_predictions`（`str，torch.FloatTensor`字典的元组，*可选*）-
    来自transformer解码器每一层的类别和掩码预测的元组。'
- en: '`text_queries` (`torch.FloatTensor`, *optional* of shape `(batch_size, num_queries,
    hidden_dim)`) Text queries derived from the input text list used for calculating
    contrastive loss during training.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_queries`（`torch.FloatTensor`，*可选*，形状为`(batch_size, num_queries, hidden_dim)`）-
    从用于训练期间计算对比损失的输入文本列表派生的文本查询。'
- en: '`task_token` (`torch.FloatTensor` of shape `(batch_size, hidden_dim)`) 1D task
    token to condition the queries.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_token`（形状为`(batch_size, hidden_dim)`的`torch.FloatTensor`）- 用于条件查询的一维任务令牌。'
- en: '`attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size,
    num_heads, sequence_length, sequence_length)`. Self and Cross Attentions weights
    from transformer decoder.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tuple(torch.FloatTensor)`元组。来自变压器解码器的自注意力和交叉注意力权重。'
- en: '`OneFormerModelOutput`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`OneFormerModelOutput`'
- en: The [OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel)
    forward method, overrides the `__call__` special method.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE17]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: OneFormerForUniversalSegmentation
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OneFormerForUniversalSegmentation
- en: '### `class transformers.OneFormerForUniversalSegmentation`'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.OneFormerForUniversalSegmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L3027)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L3027)'
- en: '[PRE18]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: OneFormer Model for instance, semantic and panoptic image segmentation. This
    model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: OneFormer模型例如，语义和全景图像分割。此模型是PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L3098)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L3098)'
- en: '[PRE19]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [OneFormerProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor).
    See `OneFormerProcessor.__call__()` for details.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—
    像素值。像素值可以使用[OneFormerProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor)获取。有关详细信息，请参阅`OneFormerProcessor.__call__()`。'
- en: '`task_inputs` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Task inputs. Task inputs can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See `OneFormerProcessor.__call__()` for details.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_inputs`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）— 任务输入。任务输入可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅`OneFormerProcessor.__call__()`。'
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_mask`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）— 用于避免在填充像素值上执行注意力的掩码。掩码值选在`[0,
    1]`中：'
- en: 1 for pixels that are real (i.e. `not masked`),
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于真实像素（即`not masked`）的像素为1，
- en: 0 for pixels that are padding (i.e. `masked`).
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于填充像素（即`masked`）的像素为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力掩码是什么？
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of Detr’s decoder attention layers.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回Detr解码器注意力层的注意力张量。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `~OneFormerModelOutput`
    instead of a plain tuple.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回`~OneFormerModelOutput`而不是普通元组。'
- en: '`text_inputs` (`List[torch.Tensor]`, *optional*) — Tensor fof shape `(num_queries,
    sequence_length)` to be fed to a model'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_inputs`（`List[torch.Tensor]`，*可选*）— 形状为`(num_queries, sequence_length)`的张量，将被馈送到模型'
- en: '`mask_labels` (`List[torch.Tensor]`, *optional*) — List of mask labels of shape
    `(num_labels, height, width)` to be fed to a model'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_labels`（`List[torch.Tensor]`，*可选*）— 形状为`(num_labels, height, width)`的掩码标签列表，将被馈送到模型'
- en: '`class_labels` (`List[torch.LongTensor]`, *optional*) — list of target class
    labels of shape `(num_labels, height, width)` to be fed to a model. They identify
    the labels of `mask_labels`, e.g. the label of `mask_labels[i][j]` if `class_labels[i][j]`.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_labels`（`List[torch.LongTensor]`，*可选*）— 形状为`(num_labels, height, width)`的目标类标签列表，将被馈送到模型。它们标识`mask_labels`的标签，例如，如果`class_labels[i][j]`的标签是`mask_labels[i][j]`。'
- en: Returns
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig))
    and inputs.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig)）和输入的各种元素。'
- en: '`loss` (`torch.Tensor`, *optional*) — The computed loss, returned when labels
    are present.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（`torch.Tensor`，*可选*）—计算得到的损失，在存在标签时返回。'
- en: '`class_queries_logits` (`torch.FloatTensor`) — A tensor of shape `(batch_size,
    num_queries, num_labels + 1)` representing the proposed classes for each query.
    Note the `+ 1` is needed because we incorporate the null class.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_queries_logits`（`torch.FloatTensor`）—形状为`(batch_size, num_queries, num_labels
    + 1)`的张量，表示每个查询的提议类别。请注意，需要`+ 1`，因为我们包含了空类。'
- en: '`masks_queries_logits` (`torch.FloatTensor`) — A tensor of shape `(batch_size,
    num_queries, height, width)` representing the proposed masks for each query.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`masks_queries_logits`（`torch.FloatTensor`）—形状为`(batch_size, num_queries, height,
    width)`的张量，表示每个查询的提议掩码。'
- en: '`auxiliary_predictions` (List of Dict of `str, torch.FloatTensor`, *optional*)
    — List of class and mask predictions from each layer of the transformer decoder.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_predictions`（`str`，`torch.FloatTensor`字典的列表，*可选*）—来自transformer解码器每一层的类别和掩码预测的列表。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the encoder model at the output of
    each stage.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—形状为`(batch_size,
    num_channels, height, width)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个阶段的输出）。编码器模型在每个阶段输出的隐藏状态（也称为特征图）。'
- en: '`pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the pixel decoder model at the output
    of each stage.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—形状为`(batch_size,
    num_channels, height, width)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个阶段的输出）。像素解码器模型在每个阶段输出的隐藏状态（也称为特征图）。'
- en: '`transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*,
    returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the transformer decoder at the output
    of each stage.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—形状为`(batch_size,
    sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个阶段的输出）。transformer解码器在每个阶段输出的隐藏状态（也称为特征图）。'
- en: '`transformer_decoder_object_queries` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, hidden_dim)`) Output object queries from the last layer in the transformer
    decoder.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_object_queries`（`torch.FloatTensor`，形状为`(batch_size, num_queries,
    hidden_dim)`）—transformer解码器中最后一层的输出对象查询。'
- en: '`transformer_decoder_contrastive_queries` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, hidden_dim)`) Contrastive queries from the transformer decoder.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_contrastive_queries`（`torch.FloatTensor`，形状为`(batch_size,
    num_queries, hidden_dim)`）—transformer解码器中的对比查询。'
- en: '`transformer_decoder_mask_predictions` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, height, width)`) Mask Predictions from the last layer in the transformer
    decoder.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_mask_predictions`（`torch.FloatTensor`，形状为`(batch_size,
    num_queries, height, width)`）—transformer解码器中最后一层的掩码预测。'
- en: '`transformer_decoder_class_predictions` (`torch.FloatTensor` of shape `(batch_size,
    num_queries, num_classes+1)`) — Class Predictions from the last layer in the transformer
    decoder.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_class_predictions`（`torch.FloatTensor`，形状为`(batch_size,
    num_queries, num_classes+1)`）—transformer解码器中最后一层的类别预测。'
- en: '`transformer_decoder_auxiliary_predictions` (List of Dict of `str, torch.FloatTensor`,
    *optional*) — List of class and mask predictions from each layer of the transformer
    decoder.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_decoder_auxiliary_predictions`（`str`，`torch.FloatTensor`字典的列表，*可选*）—来自transformer解码器每一层的类别和掩码预测的列表。'
- en: '`text_queries` (`torch.FloatTensor`, *optional* of shape `(batch_size, num_queries,
    hidden_dim)`) Text queries derived from the input text list used for calculating
    contrastive loss during training.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_queries`（`torch.FloatTensor`，*可选*，形状为`(batch_size, num_queries, hidden_dim)`）—从用于训练期间计算对比损失的输入文本列表派生的文本查询。'
- en: '`task_token` (`torch.FloatTensor` of shape `(batch_size, hidden_dim)`) 1D task
    token to condition the queries.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task_token`（`torch.FloatTensor`，形状为`(batch_size, hidden_dim)`）—用于条件查询的一维任务令牌。'
- en: '`attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size,
    num_heads, sequence_length, sequence_length)`. Self and Cross Attentions weights
    from transformer decoder.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tuple(torch.FloatTensor))`, *可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tuple(torch.FloatTensor)`元组（每层一个）。来自变压器解码器的自注意力和交叉注意力权重。'
- en: '`OneFormerUniversalSegmentationOutput`'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '`OneFormerUniversalSegmentationOutput`'
- en: The [OneFormerForUniversalSegmentation](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation)
    forward method, overrides the `__call__` special method.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[OneFormerForUniversalSegmentation](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: 'Universal segmentation example:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 通用分割示例：
- en: '[PRE20]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
