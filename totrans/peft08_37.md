# Llama-Adapter

> 原文：[https://huggingface.co/docs/peft/package_reference/llama_adapter](https://huggingface.co/docs/peft/package_reference/llama_adapter)

[Llama-Adapter](https://hf.co/papers/2303.16199) 是一种专门设计用于将 Llama 转换为遵循指令的模型的 PEFT 方法。Llama 模型被冻结，只学习一组附加到输入指令标记的适应提示。由于随机初始化的模块插入到模型中可能导致模型丢失部分现有知识，Llama-Adapter 使用零初始化的注意力和零门控逐渐向模型添加指令提示。

论文摘要如下：

*我们提出了 LLaMA-Adapter，一种轻量级的适应方法，可以有效地将 LLaMA 微调为一个遵循指令的模型。使用 52K 个自我指导演示，LLaMA-Adapter 仅在冻结的 LLaMA 7B 模型上引入了 1.2M 个可学习参数，并且在 8 个 A100 GPU 上进行微调不到一小时。具体来说，我们采用一组可学习的适应提示，并将它们附加到更高的变换器层的输入文本标记中。然后，提出了一种零初始化的注意力机制和零门控，它可以自适应地将新的指令提示注入到 LLaMA 中，同时有效地保留其预训练知识。通过高效的训练，LLaMA-Adapter 生成了高质量的响应，与完全微调的 7B 参数的 Alpaca 相媲美。此外，我们的方法可以简单地扩展到多模态输入，例如图像，用于基于图像的 LLaMA，在 ScienceQA 上实现了更强大的推理能力。我们在 [https://github.com/ZrrSkywalker/LLaMA-Adapter](https://github.com/ZrrSkywalker/LLaMA-Adapter) 上发布了我们的代码*。

## AdaptionPromptConfig

### `class peft.AdaptionPromptConfig`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adaption_prompt/config.py#L24)

```py
( peft_type: Union = None auto_mapping: Optional = None base_model_name_or_path: Optional = None revision: Optional = None task_type: Union = None inference_mode: bool = False target_modules: str = None adapter_len: int = None adapter_layers: int = None )
```

存储 [AdaptionPromptModel](/docs/peft/v0.8.2/en/package_reference/llama_adapter#peft.AdaptionPromptModel) 的配置。

## AdaptionPromptModel

### `class peft.AdaptionPromptModel`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adaption_prompt/model.py#L26)

```py
( model configs: Dict adapter_name: str )
```

实现如 [https://arxiv.org/pdf/2303.16199.pdf](https://arxiv.org/pdf/2303.16199.pdf) 中描述的适应提示。

顶部的 L 注意力模块被替换为 AdaptedAttention 模块，这些模块包装了原始模块，但插入了带有门控的可训练提示（用于零初始化）。

关于多适配器模式的注释：

+   我们通过保持一个由适配器名称索引的 AdaptedAttention 模块字典来存储不同适配器的状态。

+   每次切换适配器时，我们会从模型中移除当前活动适配器的模块，将其存储在字典中，并用新适配器的模块替换它们。

+   为避免重复和潜在的不一致状态，当前活动的适配器始终从字典中移除。

+   禁用适配器也会导致模块从模型中移除。

#### `add_adapter`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adaption_prompt/model.py#L61)

```py
( adapter_name: str config: AdaptionPromptConfig )
```

使用给定名称和配置添加一个适配器。

#### `disable_adapter_layers`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adaption_prompt/model.py#L115)

```py
( )
```

通过交换出 AdaptedAttention 模块来禁用适配器层。

#### `enable_adapter_layers`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adaption_prompt/model.py#L110)

```py
( )
```

通过交换缓存的 AdaptedAttention 模块启用适配器层。

#### `set_adapter`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adaption_prompt/model.py#L97)

```py
( adapter_name: str )
```

设置模型使用给定名称的适配器。
