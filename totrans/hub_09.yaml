- en: Repository limitations and recommendations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/hub/repositories-recommendations](https://huggingface.co/docs/hub/repositories-recommendations)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: There are some limitations to be aware of when dealing with a large amount of
    data in your repo. Given the time it takes to stream the data, getting an upload/push
    to fail at the end of the process or encountering a degraded experience, be it
    on hf.co or when working locally, can be very annoying.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Recommendations
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We gathered a list of tips and recommendations for structuring your repo. If
    you are looking for more practical tips, check out [this guide](https://huggingface.co/docs/huggingface_hub/main/en/guides/upload#tips-and-tricks-for-large-uploads)
    on how to upload large amount of data using the Python library.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '| Characteristic | Recommended | Tips |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
- en: '| Repo size | - | contact us for large repos (TBs of data) |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
- en: '| Files per repo | <100k | merge data into fewer files |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
- en: '| Entries per folder | <10k | use subdirectories in repo |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
- en: '| File size | <5GB | split data into chunked files |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
- en: '| Commit size | <100 files* | upload files in multiple commits |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
- en: '| Commits per repo | - | upload multiple files per commit and/or squash history
    |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
- en: '** Not relevant when using `git` CLI directly*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Please read the next section to understand better those limits and how to deal
    with them.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Explanations
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What are we talking about when we say “large uploads”, and what are their associated
    limitations? Large uploads can be very diverse, from repositories with a few huge
    files (e.g. model weights) to repositories with thousands of small files (e.g.
    an image dataset).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the hood, the Hub uses Git to version the data, which has structural
    implications on what you can do in your repo. If your repo is crossing some of
    the numbers mentioned in the previous section, **we strongly encourage you to
    check out [`git-sizer`](https://github.com/github/git-sizer)**, which has very
    detailed documentation about the different factors that will impact your experience.
    Here is a TL;DR of factors to consider:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '**Repository size**: The total size of the data you’re planning to upload.
    There is no hard limit on a Hub repository size. However, if you plan to upload
    hundreds of GBs or even TBs of data, we would appreciate it if you could let us
    know in advance so we can better help you if you have any questions during the
    process. You can contact us at [datasets@huggingface.co](mailto:datasets@huggingface.co)
    or on [our Discord](http://hf.co/join/discord).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of files**:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For optimal experience, we recommend keeping the total number of files under
    100k. Try merging the data into fewer files if you have more. For example, json
    files can be merged into a single jsonl file, or large datasets can be exported
    as Parquet files or in [WebDataset](https://github.com/webdataset/webdataset)
    format.
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum number of files per folder cannot exceed 10k files per folder. A
    simple solution is to create a repository structure that uses subdirectories.
    For example, a repo with 1k folders from `000/` to `999/`, each containing at
    most 1000 files, is already enough.
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**File size**: In the case of uploading large files (e.g. model weights), we
    strongly recommend splitting them **into chunks of around 5GB each**. There are
    a few reasons for this:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uploading and downloading smaller files is much easier both for you and the
    other users. Connection issues can always happen when streaming data and smaller
    files avoid resuming from the beginning in case of errors.
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Files are served to the users using CloudFront. From our experience, huge files
    are not cached by this service leading to a slower download speed. In all cases
    no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single
    file size.
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of commits**: There is no hard limit for the total number of commits
    on your repo history. However, from our experience, the user experience on the
    Hub starts to degrade after a few thousand commits. We are constantly working
    to improve the service, but one must always remember that a git repository is
    not meant to work as a database with a lot of writes. If your repo’s history gets
    very large, it is always possible to squash all the commits to get a fresh start
    using `huggingface_hub`’s [`super_squash_history`](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/hf_api#huggingface_hub.HfApi.super_squash_history).
    Be aware that this is a non-revertible operation.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提交次数**：在存储库历史记录中，总提交次数没有硬性限制。然而，根据我们的经验，在几千次提交后，Hub上的用户体验开始下降。我们不断努力改进服务，但人们必须始终记住，git存储库并不是用来作为具有大量写入的数据库。如果您的存储库历史记录变得非常庞大，总是可以将所有提交压缩以使用`huggingface_hub`的[`super_squash_history`](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/hf_api#huggingface_hub.HfApi.super_squash_history)重新开始。请注意，这是一个不可逆转的操作。'
- en: '**Number of operations per commit**: Once again, there is no hard limit here.
    When a commit is uploaded on the Hub, each git operation (addition or delete)
    is checked by the server. When a hundred LFS files are committed at once, each
    file is checked individually to ensure it’s been correctly uploaded. When pushing
    data through HTTP, a timeout of 60s is set on the request, meaning that if the
    process takes more time, an error is raised. However, it can happen (in rare cases)
    that even if the timeout is raised client-side, the process is still completed
    server-side. This can be checked manually by browsing the repo on the Hub. To
    prevent this timeout, we recommend adding around 50-100 files per commit.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每次提交的操作数量**：再次强调，这里没有硬性限制。当提交上传到Hub时，每个git操作（添加或删除）都会被服务器检查。当一次提交了一百个LFS文件时，每个文件都会被单独检查以确保已正确上传。通过HTTP推送数据时，请求会设置一个60秒的超时，意味着如果过程花费更多时间，就会出现错误。然而，在极少数情况下，即使客户端端设置了超时，过程仍可能在服务器端完成。可以通过在Hub上浏览存储库来手动检查这一点。为了防止超时，我们建议每次提交添加大约50-100个文件。'
