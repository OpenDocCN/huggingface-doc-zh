- en: Transformers Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/transformers_agents](https://huggingface.co/docs/transformers/v4.37.2/en/transformers_agents)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/373.3106e7bf.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: Transformers Agents is an experimental API which is subject to change at any
    time. Results returned by the agents can vary as the APIs or underlying models
    are prone to change.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers version v4.29.0, building on the concept of *tools* and *agents*.
    You can play with in [this colab](https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj).
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, it provides a natural language API on top of transformers: we define
    a set of curated tools and design an agent to interpret natural language and to
    use these tools. It is extensible by design; we curated some relevant tools, but
    we’ll show you how the system can be extended easily to use any tool developed
    by the community.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with a few examples of what can be achieved with this new API. It
    is particularly powerful when it comes to multimodal tasks, so let’s take it for
    a spin to generate images and read text out loud.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '| **Input** | **Output** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ![](../Images/afaae1dcbdd21f0361c9407fed1b6736.png) | A beaver is swimming
    in the water |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '| **Input** | **Output** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| A beaver is swimming in the water |'
  prefs: []
  type: TYPE_TB
- en: <https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tts_example.wav>
  prefs: []
  type: TYPE_NORMAL
- en: your browser does not support the audio element. |
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '| **Input** | **Output** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ![](../Images/4acde610014bd0de38cbc315a4d4f1ea.png) | ballroom foyer |'
  prefs: []
  type: TYPE_TB
- en: Quickstart
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before being able to use `agent.run`, you will need to instantiate an agent,
    which is a large language model (LLM). We provide support for openAI models as
    well as opensource alternatives from BigCode and OpenAssistant. The openAI models
    perform better (but require you to have an openAI API key, so cannot be used for
    free); Hugging Face is providing free access to endpoints for BigCode and OpenAssistant
    models.
  prefs: []
  type: TYPE_NORMAL
- en: To start with, please install the `agents` extras in order to install all default
    dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To use openAI models, you instantiate an [OpenAiAgent](/docs/transformers/v4.37.2/en/main_classes/agent#transformers.OpenAiAgent)
    after installing the `openai` dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To use BigCode or OpenAssistant, start by logging in to have access to the
    Inference API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Then, instantiate the agent
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This is using the inference API that Hugging Face provides for free at the moment.
    If you have your own inference endpoint for this model (or another one) you can
    replace the URL above with your URL endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: StarCoder and OpenAssistant are free to use and perform admirably well on simple
    tasks. However, the checkpoints don’t hold up when handling more complex prompts.
    If you’re facing such an issue, we recommend trying out the OpenAI model which,
    while sadly not open-source, performs better at this given time.
  prefs: []
  type: TYPE_NORMAL
- en: You’re now good to go! Let’s dive into the two APIs that you now have at your
    disposal.
  prefs: []
  type: TYPE_NORMAL
- en: Single execution (run)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The single execution method is when using the [run()](/docs/transformers/v4.37.2/en/main_classes/agent#transformers.Agent.run)
    method of the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/eabd440c942fd7c70ec75212a9603e8a.png)'
  prefs: []
  type: TYPE_IMG
- en: It automatically selects the tool (or tools) appropriate for the task you want
    to perform and runs them appropriately. It can perform one or several tasks in
    the same instruction (though the more complex your instruction, the more likely
    the agent is to fail).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8b0548b9654c6237a2cca0caa8e7257d.png)'
  prefs: []
  type: TYPE_IMG
- en: Every [run()](/docs/transformers/v4.37.2/en/main_classes/agent#transformers.Agent.run)
    operation is independent, so you can run it several times in a row with different
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Note that your `agent` is just a large-language model, so small variations in
    your prompt might yield completely different results. It’s important to explain
    as clearly as possible the task you want to perform. We go more in-depth on how
    to write good prompts [here](custom_tools#writing-good-user-inputs).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’d like to keep a state across executions or to pass non-text objects
    to the agent, you can do so by specifying variables that you would like the agent
    to use. For example, you could generate the first image of rivers and lakes, and
    ask the model to update that picture to add an island by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be helpful when the model is unable to understand your request and
    mixes tools. An example would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the model could interpret in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Have the `text-to-image` generate a capybara swimming in the sea
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Or, have the `text-to-image` generate capybara, then use the `image-transformation`
    tool to have it swim in the sea
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In case you would like to force the first scenario, you could do so by passing
    it the prompt as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Chat-based execution (chat)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The agent also has a chat-based approach, using the [chat()](/docs/transformers/v4.37.2/en/main_classes/agent#transformers.Agent.chat)
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/eabd440c942fd7c70ec75212a9603e8a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d8cb971b0acf00ef3700538c4c96efdf.png)'
  prefs: []
  type: TYPE_IMG
- en: This is an interesting approach when you want to keep the state across instructions.
    It’s better for experimentation, but will tend to be much better at single instructions
    rather than complex instructions (which the [run()](/docs/transformers/v4.37.2/en/main_classes/agent#transformers.Agent.run)
    method is better at handling).
  prefs: []
  type: TYPE_NORMAL
- en: This method can also take arguments if you would like to pass non-text types
    or specific prompts.
  prefs: []
  type: TYPE_NORMAL
- en: ⚠️ Remote execution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For demonstration purposes and so that it could be used with all setups, we
    had created remote executors for several of the default tools the agent has access
    for the release. These are created using [inference endpoints](https://huggingface.co/inference-endpoints).
  prefs: []
  type: TYPE_NORMAL
- en: We have turned these off for now, but in order to see how to set up remote executors
    tools yourself, we recommend reading the [custom tool guide](./custom_tools).
  prefs: []
  type: TYPE_NORMAL
- en: What’s happening here? What are tools, and what are agents?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/34d293d2550e3aeb2d2b89b2f4784ac6.png)'
  prefs: []
  type: TYPE_IMG
- en: Agents
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The “agent” here is a large language model, and we’re prompting it so that it
    has access to a specific set of tools.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are pretty good at generating small samples of code, so this API takes
    advantage of that by prompting the LLM gives a small sample of code performing
    a task with a set of tools. This prompt is then completed by the task you give
    your agent and the description of the tools you give it. This way it gets access
    to the doc of the tools you are using, especially their expected inputs and outputs,
    and can generate the relevant code.
  prefs: []
  type: TYPE_NORMAL
- en: Tools
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Tools are very simple: they’re a single function, with a name, and a description.
    We then use these tools’ descriptions to prompt the agent. Through the prompt,
    we show the agent how it would leverage tools to perform what was requested in
    the query.'
  prefs: []
  type: TYPE_NORMAL
- en: This is using brand-new tools and not pipelines, because the agent writes better
    code with very atomic tools. Pipelines are more refactored and often combine several
    tasks in one. Tools are meant to be focused on one very simple task only.
  prefs: []
  type: TYPE_NORMAL
- en: Code-execution?!
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This code is then executed with our small Python interpreter on the set of inputs
    passed along with your tools. We hear you screaming “Arbitrary code execution!”
    in the back, but let us explain why that is not the case.
  prefs: []
  type: TYPE_NORMAL
- en: The only functions that can be called are the tools you provided and the print
    function, so you’re already limited in what can be executed. You should be safe
    if it’s limited to Hugging Face tools.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we don’t allow any attribute lookup or imports (which shouldn’t be needed
    anyway for passing along inputs/outputs to a small set of functions) so all the
    most obvious attacks (and you’d need to prompt the LLM to output them anyway)
    shouldn’t be an issue. If you want to be on the super safe side, you can execute
    the run() method with the additional argument return_code=True, in which case
    the agent will just return the code to execute and you can decide whether to do
    it or not.
  prefs: []
  type: TYPE_NORMAL
- en: The execution will stop at any line trying to perform an illegal operation or
    if there is a regular Python error with the code generated by the agent.
  prefs: []
  type: TYPE_NORMAL
- en: A curated set of tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We identify a set of tools that can empower such agents. Here is an updated
    list of the tools we have integrated in `transformers`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document question answering**: given a document (such as a PDF) in image
    format, answer a question on this document ([Donut](./model_doc/donut))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text question answering**: given a long text and a question, answer the question
    in the text ([Flan-T5](./model_doc/flan-t5))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unconditional image captioning**: Caption the image! ([BLIP](./model_doc/blip))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image question answering**: given an image, answer a question on this image
    ([VILT](./model_doc/vilt))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image segmentation**: given an image and a prompt, output the segmentation
    mask of that prompt ([CLIPSeg](./model_doc/clipseg))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech to text**: given an audio recording of a person talking, transcribe
    the speech into text ([Whisper](./model_doc/whisper))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text to speech**: convert text to speech ([SpeechT5](./model_doc/speecht5))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zero-shot text classification**: given a text and a list of labels, identify
    to which label the text corresponds the most ([BART](./model_doc/bart))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text summarization**: summarize a long text in one or a few sentences ([BART](./model_doc/bart))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Translation**: translate the text into a given language ([NLLB](./model_doc/nllb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These tools have an integration in transformers, and can be used manually as
    well, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Custom tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While we identify a curated set of tools, we strongly believe that the main
    value provided by this implementation is the ability to quickly create and share
    custom tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'By pushing the code of a tool to a Hugging Face Space or a model repository,
    you’re then able to leverage the tool directly with the agent. We’ve added a few
    **transformers-agnostic** tools to the [`huggingface-tools` organization](https://huggingface.co/huggingface-tools):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text downloader**: to download a text from a web URL'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text to image**: generate an image according to a prompt, leveraging stable
    diffusion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image transformation**: modify an image given an initial image and a prompt,
    leveraging instruct pix2pix stable diffusion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text to video**: generate a small video according to a prompt, leveraging
    damo-vilab'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The text-to-image tool we have been using since the beginning is a remote tool
    that lives in [*huggingface-tools/text-to-image*](https://huggingface.co/spaces/huggingface-tools/text-to-image)!
    We will continue releasing such tools on this and other organizations, to further
    supercharge this implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The agents have by default access to tools that reside on [`huggingface-tools`](https://huggingface.co/huggingface-tools).
    We explain how to you can write and share your tools as well as leverage any custom
    tool that resides on the Hub in [following guide](custom_tools).
  prefs: []
  type: TYPE_NORMAL
- en: Code generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far we have shown how to use the agents to perform actions for you. However,
    the agent is only generating code that we then execute using a very restricted
    Python interpreter. In case you would like to use the code generated in a different
    setting, the agent can be prompted to return the code, along with tool definition
    and accurate imports.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the following instruction
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: returns the following code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: that you can then modify and execute yourself.
  prefs: []
  type: TYPE_NORMAL
