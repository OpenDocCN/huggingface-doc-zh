- en: Fine-tune BERT for Text Classification on AWS Trainium
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_bert](https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_bert)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/optimum.neuron/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/entry/start.abfe5599.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/scheduler.9039eef2.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/singletons.9144bb03.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/paths.e169ac99.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/entry/app.df8ec0a0.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/index.cdcc3d35.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/nodes/0.a52c6f40.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/nodes/20.23446f28.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/CodeBlock.e3ac94d9.js">
    <link rel="modulepreload" href="/docs/optimum.neuron/main/en/_app/immutable/chunks/Heading.96ce3702.js">
  prefs: []
  type: TYPE_NORMAL
- en: '*There is a notebook version of that tutorial [here](https://github.com/huggingface/optimum-neuron/blob/main/notebooks/text-classification/notebook.ipynb)*.'
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial will help you to get started with [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls)
    and Hugging Face Transformers. It will cover how to set up a Trainium instance
    on AWS, load & fine-tune a transformers model for text-classification
  prefs: []
  type: TYPE_NORMAL
- en: 'You will learn how to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Setup AWS environment](#1-setup-aws-environment)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Load and process the dataset](#2-load-and-process-the-dataset)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Fine-tune BERT using Hugging Face Transformers and Optimum Neuron](#3-fine-tune-bert-using-hugging-face-transformers)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before we can start, make sure you have a [Hugging Face Account](https://huggingface.co/join)
    to save artifacts and experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quick intro: AWS Trainium'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[AWS Trainium (Trn1)](https://aws.amazon.com/de/ec2/instance-types/trn1/) is
    a purpose-built EC2 for deep learning (DL) training workloads. Trainium is the
    successor of [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf1/?nc1=h_ls)
    focused on high-performance training workloads claiming up to 50% cost-to-train
    savings over comparable GPU-based instances.'
  prefs: []
  type: TYPE_NORMAL
- en: Trainium has been optimized for training natural language processing, computer
    vision, and recommender models used. The accelerator supports a wide range of
    data types, including FP32, TF32, BF16, FP16, UINT8, and configurable FP8.
  prefs: []
  type: TYPE_NORMAL
- en: 'The biggest Trainium instance, the `trn1.32xlarge` comes with over 500GB of
    memory, making it easy to fine-tune ~10B parameter models on a single instance.
    Below you will find an overview of the available instance types. More details
    [here](https://aws.amazon.com/de/ec2/instance-types/trn1/#Product_details):'
  prefs: []
  type: TYPE_NORMAL
- en: '| instance size | accelerators | accelerator memory | vCPU | CPU Memory | price
    per hour |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| trn1.2xlarge | 1 | 32 | 8 | 32 | $1.34 |'
  prefs: []
  type: TYPE_TB
- en: '| trn1.32xlarge | 16 | 512 | 128 | 512 | $21.50 |'
  prefs: []
  type: TYPE_TB
- en: '| trn1n.32xlarge (2x bandwidth) | 16 | 512 | 128 | 512 | $24.78 |'
  prefs: []
  type: TYPE_TB
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Now we know what Trainium offers, let‚Äôs get started. üöÄ
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: This tutorial was created on a trn1.2xlarge AWS EC2 Instance.*'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Setup AWS environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we will use the `trn1.2xlarge` instance on AWS with 1 Accelerator,
    including two Neuron Cores and the [Hugging Face Neuron Deep Learning AMI](https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2).
  prefs: []
  type: TYPE_NORMAL
- en: This blog post doesn‚Äôt cover how to create the instance in detail. You can check
    out my previous blog about [‚ÄúSetting up AWS Trainium for Hugging Face Transformers‚Äù](https://www.philschmid.de/setup-aws-trainium),
    which includes a step-by-step guide on setting up the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Once the instance is up and running, we can ssh into it. But instead of developing
    inside a terminal we want to use a `Jupyter` environment, which we can use for
    preparing our dataset and launching the training. For this, we need to add a port
    for forwarding in the `ssh` command, which will tunnel our localhost traffic to
    the Trainium instance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We can now start our **`jupyter`** server.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You should see a familiar **`jupyter`** output with a URL to the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '**`http://localhost:8080/?token=8c1739aff1755bd7958c4cfccc8d08cb5da5234f61f129a9`**'
  prefs: []
  type: TYPE_NORMAL
- en: We can click on it, and a **`jupyter`** environment opens in our local browser.
  prefs: []
  type: TYPE_NORMAL
- en: '![jupyter.webp](../Images/f3e7326719a8cc7f67122b89fb3e1dc1.png)'
  prefs: []
  type: TYPE_IMG
- en: We are going to use the Jupyter environment only for preparing the dataset and
    then `torchrun` for launching our training script on both neuron cores for distributed
    training. Lets create a new notebook and get started.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Load and process the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are training a Text Classification model on the [emotion](https://huggingface.co/datasets/philschmid/emotion)
    dataset to keep the example straightforward. The `emotion` is a dataset of English
    Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and
    surprise.'
  prefs: []
  type: TYPE_NORMAL
- en: We will use the `load_dataset()` method from the [ü§ó Datasets](https://huggingface.co/docs/datasets/index)
    library to load the `emotion`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs check out an example of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We must convert our ‚ÄúNatural Language‚Äù to token IDs to train our model. This
    is done by a Tokenizer, which tokenizes the inputs (including converting the tokens
    to their corresponding IDs in the pre-trained vocabulary). if you want to learn
    more about this, out [chapter 6](https://huggingface.co/course/chapter6/1?fw=pt)
    of the [Hugging Face Course](https://huggingface.co/course/chapter1/1).
  prefs: []
  type: TYPE_NORMAL
- en: Our Neuron Accelerator expects a fixed shape of inputs. We need to truncate
    or pad all samples to the same length.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Fine-tune BERT using Hugging Face Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normally you would use the [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer)
    and [TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments)
    to fine-tune PyTorch-based transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: But together with AWS, we have developed a [NeuronTrainer](https://huggingface.co/docs/optimum-neuron/package_reference/trainer)
    to improve performance, robustness, and safety when training on Trainium or Inferentia2
    instances. The `NeuronTrainer` also comes with a [model cache](https://www.notion.so/Getting-started-with-AWS-Trainium-and-Hugging-Face-Transformers-8428c72556194aed9c393de101229dcf),
    which allows us to use precompiled models and configuration from Hugging Face
    Hub to skip the compilation step, which would be needed at the beginning of training.
    This can reduce the training time by ~3x.
  prefs: []
  type: TYPE_NORMAL
- en: The `NeuronTrainer` is part of the `optimum-neuron` library and can be used
    as a 1-to-1 replacement for the `Trainer`. You only have to adjust the import
    in your training script.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We prepared a simple [train.py](https://github.com/huggingface/optimum-neuron/blob/main/notebooks/text-classification/scripts/train.py)
    training script based on the [‚ÄúGetting started with Pytorch 2.0 and Hugging Face
    Transformers‚Äù](https://www.philschmid.de/getting-started-pytorch-2-0-transformers#3-fine-tune--evaluate-bert-model-with-the-hugging-face-trainer)
    blog post with the `NeuronTrainer`. Below is an excerpt
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can load the training script into our environment using the `wget` command
    or manually copy it into the notebook from [here](https://github.com/huggingface/optimum-neuron/blob/notebooks/text-classification/scripts/train.py).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We will use `torchrun` to launch our training script on both neuron cores for
    distributed training. `torchrun` is a tool that automatically distributes a PyTorch
    model across multiple accelerators. We can pass the number of accelerators as
    `nproc_per_node` arguments alongside our hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We‚Äôll use the following command to launch training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '***Note**: If you see bad, bad accuracy, you might want to deactivate `bf16`
    for now.*'
  prefs: []
  type: TYPE_NORMAL
- en: After 9 minutes the training was completed and achieved an excellent f1 score
    of `0.914`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Last but not least, terminate the EC2 instance to avoid unnecessary charges.
    Looking at the price-performance, our training only cost **`20ct`** (**`1.34$/h
    * 0.15h = 0.20$`**)
  prefs: []
  type: TYPE_NORMAL
