- en: Inpainting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/inpaint](https://huggingface.co/docs/diffusers/using-diffusers/inpaint)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Inpainting replaces or edits specific areas of an image. This makes it a useful
    tool for image restoration like removing defects and artifacts, or even replacing
    an image area with something entirely new. Inpainting relies on a mask to determine
    which regions of an image to fill in; the area to inpaint is represented by white
    pixels and the area to keep is represented by black pixels. The white pixels are
    filled in by the prompt.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'With ğŸ¤— Diffusers, here is how you can do inpainting:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'Load an inpainting checkpoint with the [AutoPipelineForInpainting](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForInpainting)
    class. Thisâ€™ll automatically detect the appropriate pipeline class to load based
    on the checkpoint:'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Youâ€™ll notice throughout the guide, we use [enable_model_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/gligen#diffusers.StableDiffusionGLIGENTextImagePipeline.enable_model_cpu_offload)
    and [enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_xformers_memory_efficient_attention),
    to save memory and increase inference speed. If youâ€™re using PyTorch 2.0, itâ€™s
    not necessary to call [enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_xformers_memory_efficient_attention)
    on your pipeline because itâ€™ll already be using PyTorch 2.0â€™s native [scaled-dot
    product attention](../optimization/torch2.0#scaled-dot-product-attention).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the base and mask images:'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create a prompt to inpaint the image with and pass it to the pipeline with
    the base and mask images:'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/e54493b4e03ab58931aa2b35535a314d.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: base image
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/50397365c167c77e28b112edb22eb879.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: mask image
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53b76ec0efb1b6851d4c8b23347a3f52.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: generated image
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Create a mask image
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this guide, the mask image is provided in all of the code examples
    for convenience. You can inpaint on your own images, but youâ€™ll need to create
    a mask image for it. Use the Space below to easily create a mask image.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Upload a base image to inpaint on and use the sketch tool to draw a mask. Once
    youâ€™re done, click **Run** to generate and download the mask image.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[https://stevhliu-inpaint-mask-maker.hf.space](https://stevhliu-inpaint-mask-maker.hf.space)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Mask blur
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `~VaeImageProcessor.blur` method provides an option for how to blend the
    original image and inpaint area. The amount of blur is determined by the `blur_factor`
    parameter. Increasing the `blur_factor` increases the amount of blur applied to
    the mask edges, softening the transition between the original image and inpaint
    area. A low or zero `blur_factor` preserves the sharper edges of the mask.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: To use this, create a blurred mask with the image processor.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/bd4255378e4be6f2da63ef7b2951ac46.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: mask with no blur
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a2ec7820a1b085328cbab8c7625a7d0.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: mask with blur applied
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Popular models
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Stable Diffusion Inpainting](https://huggingface.co/runwayml/stable-diffusion-inpainting),
    [Stable Diffusion XL (SDXL) Inpainting](https://huggingface.co/diffusers/stable-diffusion-xl-1.0-inpainting-0.1),
    and [Kandinsky 2.2 Inpainting](https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder-inpaint)
    are among the most popular models for inpainting. SDXL typically produces higher
    resolution images than Stable Diffusion v1.5, and Kandinsky 2.2 is also capable
    of generating high-quality images.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion Inpainting
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Stable Diffusion Inpainting is a latent diffusion model finetuned on 512x512
    images on inpainting. It is a good starting point because it is relatively fast
    and generates good quality images. To use this model for inpainting, youâ€™ll need
    to pass a prompt, base and mask image to the pipeline:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Stable Diffusion XL (SDXL) Inpainting
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SDXL is a larger and more powerful version of Stable Diffusion v1.5\. This model
    can follow a two-stage model process (though each model can also be used alone);
    the base model generates an image, and a refiner model takes that image and further
    enhances its details and quality. Take a look at the [SDXL](sdxl) guide for a
    more comprehensive guide on how to use SDXL and configure itâ€™s parameters.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Kandinsky 2.2 Inpainting
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Kandinsky model family is similar to SDXL because it uses two models as
    well; the image prior model creates image embeddings, and the diffusion model
    generates images from them. You can load the image prior and diffusion model separately,
    but the easiest way to use Kandinsky 2.2 is to load it into the [AutoPipelineForInpainting](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForInpainting)
    class which uses the [KandinskyV22InpaintCombinedPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/kandinsky_v22#diffusers.KandinskyV22InpaintCombinedPipeline)
    under the hood.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/e54493b4e03ab58931aa2b35535a314d.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: base image
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09fbce513beddef5b655c0a444a3c86c.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: Stable Diffusion Inpainting
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2489c89cec0d33d1acc14b3011bf03c9.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Stable Diffusion XL Inpainting
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/44c50c30f2c0b8b5adb606ae538dbb54.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: Kandinsky 2.2 Inpainting
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Non-inpaint specific checkpoints
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, this guide has used inpaint specific checkpoints such as [runwayml/stable-diffusion-inpainting](https://huggingface.co/runwayml/stable-diffusion-inpainting).
    But you can also use regular checkpoints like [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).
    Letâ€™s compare the results of the two checkpoints.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: The image on the left is generated from a regular checkpoint, and the image
    on the right is from an inpaint checkpoint. Youâ€™ll immediately notice the image
    on the left is not as clean, and you can still see the outline of the area the
    model is supposed to inpaint. The image on the right is much cleaner and the inpainted
    area appears more natural.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: runwayml/stable-diffusion-v1-5runwayml/stable-diffusion-inpainting
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/5a890789342f9e1a86868211a7896271.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: runwayml/stable-diffusion-v1-5
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aecbb44542e32cda8ce2bb2da3d692e3.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: runwayml/stable-diffusion-inpainting
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: However, for more basic tasks like erasing an object from an image (like the
    rocks in the road for example), a regular checkpoint yields pretty good results.
    There isnâ€™t as noticeable of difference between the regular and inpaint checkpoint.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: runwayml/stable-diffusion-v1-5runwayml/stable-diffusion-inpaint
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/f49798dfa50ce8a7c46fe18a6da38589.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: runwayml/stable-diffusion-v1-5
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c59fac74bc905b0800d717d2ce675c20.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: runwayml/stable-diffusion-inpainting
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: The trade-off of using a non-inpaint specific checkpoint is the overall image
    quality may be lower, but it generally tends to preserve the mask area (that is
    why you can see the mask outline). The inpaint specific checkpoints are intentionally
    trained to generate higher quality inpainted images, and that includes creating
    a more natural transition between the masked and unmasked areas. As a result,
    these checkpoints are more likely to change your unmasked area.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: If preserving the unmasked area is important for your task, you can use the
    `VaeImageProcessor.apply_overlay` method to force the unmasked area of an image
    to remain the same at the expense of some more unnatural transitions between the
    masked and unmasked areas.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Configure pipeline parameters
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image features - like quality and â€œcreativityâ€ - are dependent on pipeline parameters.
    Knowing what these parameters do is important for getting the results you want.
    Letâ€™s take a look at the most important parameters and see how changing them affects
    the output.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Strength
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`strength` is a measure of how much noise is added to the base image, which
    influences how similar the output is to the base image.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ“ˆ a high `strength` value means more noise is added to an image and the denoising
    process takes longer, but youâ€™ll get higher quality images that are more different
    from the base image
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ğŸ“‰ a low `strength` value means less noise is added to an image and the denoising
    process is faster, but the image quality may not be as great and the generated
    image resembles the base image more
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/39f952b5e61ce473b5fc8dab92c18466.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: strength = 0.6
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba1cef420a7d6632e8dad96029906868.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: strength = 0.8
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b42b3e014dc563c0005c55a35f02fba9.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: strength = 1.0
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Guidance scale
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`guidance_scale` affects how aligned the text prompt and generated image are.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ“ˆ a high `guidance_scale` value means the prompt and generated image are closely
    aligned, so the output is a stricter interpretation of the prompt
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ğŸ“‰ a low `guidance_scale` value means the prompt and generated image are more
    loosely aligned, so the output may be more varied from the prompt
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use `strength` and `guidance_scale` together for more control over how
    expressive the model is. For example, a combination high `strength` and `guidance_scale`
    values gives the model the most creative freedom.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/363406b3094071baef896b0e11a247cb.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: guidance_scale = 2.5
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d10a98f98590aa92471ef3aabf758c6.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: guidance_scale = 7.5
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e32bf623e4fd55de7bce1266f480fffb.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: guidance_scale = 12.5
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Negative prompt
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A negative prompt assumes the opposite role of a prompt; it guides the model
    away from generating certain things in an image. This is useful for quickly improving
    image quality and preventing the model from generating things you donâ€™t want.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/a9e6a070339236ad42e15252d8be37a7.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: negative_prompt = "bad architecture, unstable, poor details, blurry"
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Padding mask crop
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A method for increasing the inpainting image quality is to use the [`padding_mask_crop`](https://huggingface.co/docs/diffusers/v0.25.0/en/api/pipelines/stable_diffusion/inpaint#diffusers.StableDiffusionInpaintPipeline.__call__.padding_mask_crop)
    parameter. When enabled, this option crops the masked area with some user-specified
    padding and itâ€™ll also crop the same area from the original image. Both the image
    and mask are upscaled to a higher resolution for inpainting, and then overlaid
    on the original image. This is a quick and easy way to improve image quality without
    using a separate pipeline like [StableDiffusionUpscalePipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Add the `padding_mask_crop` parameter to the pipeline call and set it to the
    desired padding value.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/1770d17f089e4087ecdf844bc2f75ad6.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: default inpaint image
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4ef40443043de12e889be5caf4c9289.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: inpaint image with `padding_mask_crop` enabled
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Chained inpainting pipelines
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[AutoPipelineForInpainting](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForInpainting)
    can be chained with other ğŸ¤— Diffusers pipelines to edit their outputs. This is
    often useful for improving the output quality from your other diffusion pipelines,
    and if youâ€™re using multiple pipelines, it can be more memory-efficient to chain
    them together to keep the outputs in latent space and reuse the same pipeline
    components.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-image-to-inpaint
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Chaining a text-to-image and inpainting pipeline allows you to inpaint the generated
    image, and you donâ€™t have to provide a base image to begin with. This makes it
    convenient to edit your favorite text-to-image outputs without having to generate
    an entirely new image.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with the text-to-image pipeline to create a castle:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Load the mask image of the output from above:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And letâ€™s inpaint the masked area with a waterfall:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/2cc5fc0f54d60ce54294305f4a65f5b2.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: text-to-image
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b685c42746dfa11241d879c1e331fac2.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: inpaint
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Inpaint-to-image-to-image
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can also chain an inpainting pipeline before another pipeline like image-to-image
    or an upscaler to improve the quality.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by inpainting an image:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now letâ€™s pass the image to another inpainting pipeline with SDXLâ€™s refiner
    model to enhance the image details and quality:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: It is important to specify `output_type="latent"` in the pipeline to keep all
    the outputs in latent space to avoid an unnecessary decode-encode step. This only
    works if the chained pipelines are using the same VAE. For example, in the [Text-to-image-to-inpaint](#text-to-image-to-inpaint)
    section, Kandinsky 2.2 uses a different VAE class than the Stable Diffusion model
    so it wonâ€™t work. But if you use Stable Diffusion v1.5 for both pipelines, then
    you can keep everything in latent space because they both use [AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you can pass this image to an image-to-image pipeline to put the finishing
    touches on it. It is more efficient to use the [from_pipe()](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForImage2Image.from_pipe)
    method to reuse the existing pipeline components, and avoid unnecessarily loading
    all the pipeline components into memory again.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/e54493b4e03ab58931aa2b35535a314d.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: initial image
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5fec6ffc8bfc87edf18dd2bfa1e68bb5.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: inpaint
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba2bb2403b87d4b1fb495bae370dc1c4.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: image-to-image
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Image-to-image and inpainting are actually very similar tasks. Image-to-image
    generates a new image that resembles the existing provided image. Inpainting does
    the same thing, but it only transforms the image area defined by the mask and
    the rest of the image is unchanged. You can think of inpainting as a more precise
    tool for making specific changes and image-to-image has a broader scope for making
    more sweeping changes.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Control image generation
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Getting an image to look exactly the way you want is challenging because the
    denoising process is random. While you can control certain aspects of generation
    by configuring parameters like `negative_prompt`, there are better and more efficient
    methods for controlling image generation.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Prompt weighting
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompt weighting provides a quantifiable way to scale the representation of
    concepts in a prompt. You can use it to increase or decrease the magnitude of
    the text embedding vector for each concept in the prompt, which subsequently determines
    how much of each concept is generated. The [Compel](https://github.com/damian0815/compel)
    library offers an intuitive syntax for scaling the prompt weights and generating
    the embeddings. Learn how to create the embeddings in the [Prompt weighting](../using-diffusers/weighted_prompts)
    guide.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Once youâ€™ve generated the embeddings, pass them to the `prompt_embeds` (and
    `negative_prompt_embeds` if youâ€™re using a negative prompt) parameter in the [AutoPipelineForInpainting](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForInpainting).
    The embeddings replace the `prompt` parameter:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ControlNet
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ControlNet models are used with other diffusion models like Stable Diffusion,
    and they provide an even more flexible and accurate way to control how an image
    is generated. A ControlNet accepts an additional conditioning image input that
    guides the diffusion model to preserve the features in it.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, letâ€™s condition an image with a ControlNet pretrained on inpaint
    images:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now generate an image from the base, mask and control images. Youâ€™ll notice
    features of the base image are strongly preserved in the generated image.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You can take this a step further and chain it with an image-to-image pipeline
    to apply a new [style](https://huggingface.co/nitrosocke/elden-ring-diffusion):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥è¿›ä¸€æ­¥å°†å…¶ä¸å›¾åƒåˆ°å›¾åƒç®¡é“é“¾æ¥èµ·æ¥ï¼Œåº”ç”¨ä¸€ä¸ªæ–°çš„[é£æ ¼](https://huggingface.co/nitrosocke/elden-ring-diffusion)ï¼š
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/e54493b4e03ab58931aa2b35535a314d.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e54493b4e03ab58931aa2b35535a314d.png)'
- en: initial image
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹å›¾åƒ
- en: '![](../Images/916b854bd12a25f80f4e9acf9e6601b6.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/916b854bd12a25f80f4e9acf9e6601b6.png)'
- en: ControlNet inpaint
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet inpaint
- en: '![](../Images/a9594334cac8c0320fd63c03f321d2de.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9594334cac8c0320fd63c03f321d2de.png)'
- en: image-to-image
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ°å›¾åƒ
- en: Optimize
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–
- en: It can be difficult and slow to run diffusion models if youâ€™re resource constrained,
    but it doesnâ€™t have to be with a few optimization tricks. One of the biggest (and
    easiest) optimizations you can enable is switching to memory-efficient attention.
    If youâ€™re using PyTorch 2.0, [scaled-dot product attention](../optimization/torch2.0#scaled-dot-product-attention)
    is automatically enabled and you donâ€™t need to do anything else. For non-PyTorch
    2.0 users, you can install and use [xFormers](../optimization/xformers)â€™s implementation
    of memory-efficient attention. Both options reduce memory usage and accelerate
    inference.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œæ‰©æ•£æ¨¡å‹å¯èƒ½ä¼šå¾ˆå›°éš¾å’Œç¼“æ…¢ï¼Œå¦‚æœä½ çš„èµ„æºå—é™ï¼Œä½†é€šè¿‡ä¸€äº›ä¼˜åŒ–æŠ€å·§ï¼Œå°±ä¸å¿…å¦‚æ­¤ã€‚å…¶ä¸­ä¸€ä¸ªæœ€å¤§ï¼ˆä¹Ÿæ˜¯æœ€ç®€å•ï¼‰çš„ä¼˜åŒ–æ˜¯åˆ‡æ¢åˆ°å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ã€‚å¦‚æœä½ æ­£åœ¨ä½¿ç”¨PyTorch
    2.0ï¼Œ[ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›](../optimization/torch2.0#scaled-dot-product-attention)ä¼šè‡ªåŠ¨å¯ç”¨ï¼Œä½ ä¸éœ€è¦åšå…¶ä»–ä»»ä½•äº‹æƒ…ã€‚å¯¹äºéPyTorch
    2.0ç”¨æˆ·ï¼Œä½ å¯ä»¥å®‰è£…å¹¶ä½¿ç”¨[xFormers](../optimization/xformers)çš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›å®ç°ã€‚è¿™ä¸¤ä¸ªé€‰é¡¹éƒ½å¯ä»¥å‡å°‘å†…å­˜ä½¿ç”¨é‡å¹¶åŠ é€Ÿæ¨æ–­ã€‚
- en: 'You can also offload the model to the CPU to save even more memory:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¿˜å¯ä»¥å°†æ¨¡å‹è½¬ç§»åˆ°CPUä»¥èŠ‚çœæ›´å¤šå†…å­˜ï¼š
- en: '[PRE24]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To speed-up your inference code even more, use [`torch_compile`](../optimization/torch2.0#torchcompile).
    You should wrap `torch.compile` around the most intensive component in the pipeline
    which is typically the UNet:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¿›ä¸€æ­¥åŠ å¿«æ¨æ–­ä»£ç çš„é€Ÿåº¦ï¼Œè¯·ä½¿ç”¨[`torch_compile`](../optimization/torch2.0#torchcompile)ã€‚ä½ åº”è¯¥å°†`torch.compile`åŒ…è£…åœ¨ç®¡é“ä¸­æœ€å¯†é›†çš„ç»„ä»¶å‘¨å›´ï¼Œé€šå¸¸æ˜¯UNetï¼š
- en: '[PRE25]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Learn more in the [Reduce memory usage](../optimization/memory) and [Torch 2.0](../optimization/torch2.0)
    guides.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[å‡å°‘å†…å­˜ä½¿ç”¨](../optimization/memory)å’Œ[Torch 2.0](../optimization/torch2.0)æŒ‡å—ä¸­äº†è§£æ›´å¤šã€‚
