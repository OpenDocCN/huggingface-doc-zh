# 故障排除指南

> 原文链接：[https://huggingface.co/docs/accelerate/basic_tutorials/troubleshooting](https://huggingface.co/docs/accelerate/basic_tutorials/troubleshooting)

本指南旨在为您提供导航一些常见问题所需的工具和知识。然而，由于🤗 Accelerate不断发展，用例和设置多种多样，您可能会遇到本指南未涵盖的问题。如果本指南中列出的建议未涵盖您的情况，请参考指南的最后一部分[寻求帮助](#ask-for-help)，了解如何在特定问题上寻求帮助。

## 日志记录

面对错误时，日志记录可以帮助缩小错误来源。在具有多个进程的分布式设置中，日志记录可能是一个挑战，但🤗 Accelerate提供了一个工具，简化了日志记录过程，并确保日志在分布式设置中同步和有效管理。

要解决问题，请使用`accelerate.logging`而不是标准的Python `logging`模块：

```py
- import logging
+ from accelerate.logging import get_logger
- logger = logging.getLogger(__name__)
+ logger = get_logger(__name__)
```

要设置日志级别（`INFO`，`DEBUG`，`WARNING`，`ERROR`，`CRITICAL`），请将其导出为`ACCELERATE_LOG_LEVEL`环境，或将其作为`log_level`传递给`get_logger`：

```py
from accelerate.logging import get_logger

logger = get_logger(__name__, log_level="INFO")
```

默认情况下，日志仅在主进程上调用。要在所有进程上调用它，请传递`main_process_only=False`。如果日志应在所有进程上按顺序调用，还要传递`in_order=True`。

## 挂起代码和超时错误

### 不匹配的张量形状

如果您的代码在分布式设置上挂起了很长时间，一个常见原因是不同设备上张量的形状不匹配。

在以分布式方式运行脚本时，诸如[Accelerator.gather()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather)和[Accelerator.reduce()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.reduce)等函数是必要的，以跨设备获取张量以集体执行操作。这些（以及其他）函数依赖于`torch.distributed`执行`gather`操作，这要求张量在所有进程中具有**完全相同的形状**。当张量形状不匹配时，您将遇到挂起代码，并最终触发超时异常。

如果您怀疑这种情况，请使用Accelerate的操作调试模式立即捕捉问题。

启用Accelerate的操作调试模式的推荐方法是在`accelerate config`设置期间。启用调试模式的替代方法包括：

+   从CLI：

```py
accelerate launch --debug {my_script.py} --arg1 --arg2
```

+   作为环境变量（避免需要`accelerate launch`）：

```py
ACCELERATE_DEBUG_MODE="1" torchrun {my_script.py} --arg1 --arg2
```

+   手动更改`config.yaml`文件：

```py
 compute_environment: LOCAL_MACHINE
+debug: true
```

一旦启用调试模式，您应该获得类似的回溯，指向张量形状不匹配问题：

```py
Traceback (most recent call last):
  File "/home/zach_mueller_huggingface_co/test.py", line 18, in <module>
    main()
  File "/home/zach_mueller_huggingface_co/test.py", line 15, in main
    broadcast_tensor = broadcast(tensor)
  File "/home/zach_mueller_huggingface_co/accelerate/src/accelerate/utils/operations.py", line 303, in wrapper
accelerate.utils.operations.DistributedOperationException:

Cannot apply desired operation due to shape mismatches. All shapes across devices must be valid.

Operation: `accelerate.utils.operations.broadcast`
Input shapes:
  - Process 0: [1, 5]
  - Process 1: [1, 2, 5]
```

### 早停止导致挂起

在分布式训练中进行早停止时，如果每个进程都有特定的停止条件（例如验证损失），可能不会在所有进程之间同步。结果，进程0上可能发生中断，但进程1上可能没有。这将导致代码无限期挂起，直到超时发生。

如果您有早停止条件，请使用`set_breakpoint`和`check_breakpoint`方法确保所有进程正确结束：

```py
# Assume `should_do_breakpoint` is a custom defined function that returns a conditional, 
# and that conditional might be true only on process 1
if should_do_breakpoint(loss):
    accelerator.set_breakpoint()

# Later in the training script when we need to check for the breakpoint
if accelerator.check_breakpoint():
    break
```

### 在Linux低内核版本上挂起

这是一个已知问题。在Linux上，内核版本<5.5，已经报告了挂起进程。为了避免遇到这个问题，我们建议将系统升级到更高版本的内核。

## CUDA内存不足

在运行训练脚本时，最令人沮丧的错误之一是遇到“CUDA内存不足”，因为整个脚本需要重新启动，进度丢失，通常开发人员希望简单地启动他们的脚本并让其运行。

为解决此问题，`Accelerate`提供了一个实用程序`find_executable_batch_size`，它在很大程度上基于[toma](https://github.com/BlackHC/toma)。该实用程序会重试由于OOM（内存不足）条件而失败的代码，并自动降低批量大小。

### find_executable_batch_size

该算法使用指数衰减，在某些训练脚本的每次失败运行后将批量大小减半。要使用它，重构您的训练函数以包含一个包含此包装器的内部函数，并在其中构建您的数据加载器。至少，这可能看起来像4行新代码。

内部函数*必须*将批量大小作为第一个参数传入，但在调用时我们不会传递一个。包装器会为我们处理这个。

还应注意，任何将消耗CUDA内存并传递给`accelerator`的内容**必须**在内部函数中声明，例如模型和优化器。

```py
def training_function(args):
    accelerator = Accelerator()

+   @find_executable_batch_size(starting_batch_size=args.batch_size)
+   def inner_training_loop(batch_size):
+       nonlocal accelerator # Ensure they can be used in our context
+       accelerator.free_memory() # Free all lingering references
        model = get_model()
        model.to(accelerator.device)
        optimizer = get_optimizer()
        train_dataloader, eval_dataloader = get_dataloaders(accelerator, batch_size)
        lr_scheduler = get_scheduler(
            optimizer, 
            num_training_steps=len(train_dataloader)*num_epochs
        )
        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
            model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
        )
        train(model, optimizer, train_dataloader, lr_scheduler)
        validate(model, eval_dataloader)
+   inner_training_loop()
```

要了解更多，请查看此处的文档[here](../package_reference/utilities#accelerate.find_executable_batch_size)。

## 设备设置之间的不可重现结果

如果您已更改设备设置并观察到不同的模型性能，这很可能是因为在从一个设置转移到另一个设置时，您没有更新脚本。在TPU、多GPU和单GPU与Accelerate下，相同批量大小的相同脚本将产生不同的结果。

例如，如果您以前在单GPU上训练，批量大小为16，当转移到两个GPU设置时，您需要将批量大小更改为8，以获得相同的有效批量大小。这是因为在使用Accelerate进行训练时，传递给数据加载器的批量大小是**每个GPU的批量大小**。

为确保您可以在不同设置之间重现结果，请确保使用相同的种子，根据情况调整批量大小，考虑调整学习率。

有关批量大小的更多详细信息和快速参考，请查看[比较不同设备设置之间性能](../concept_guides/performance)指南。

## 不同GPU上的性能问题

如果您的多GPU设置包含不同的GPU，可能会遇到一些限制：

+   GPU之间的显存可能存在不平衡。在这种情况下，显存较小的GPU将限制批量大小或可以加载到GPU上的模型大小。

+   如果您使用具有不同性能配置文件的GPU，性能将由您使用的最慢的GPU驱动，因为其他GPU将不得不等待其完成工作负载。

同一设置中极不同的GPU可能导致性能瓶颈。

## 寻求帮助

如果上述故障排除工具和建议无法帮助您解决问题，请向社区和团队寻求帮助。

### 论坛

在Hugging Face论坛上寻求帮助-在[🤗Accelerate类别](https://discuss.huggingface.co/c/accelerate/18)中发布您的问题。确保写一个描述性的帖子，提供有关您的设置和可重现代码的相关上下文，以最大程度地提高解决问题的可能性！

### Discord

在[Discord](http://hf.co/join/discord)上提问，让团队和社区帮助您。

### GitHub问题

如果您怀疑发现与库相关的错误，请在🤗 Accelerate [GitHub存储库](https://github.com/huggingface/accelerate/issues)上创建一个问题。包括有关错误的上下文和有关您的分布式设置的详细信息，以帮助我们更好地找出问题所在以及如何解决它。
