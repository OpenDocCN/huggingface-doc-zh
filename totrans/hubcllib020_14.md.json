["```py\n>>> from huggingface_hub import create_inference_endpoint\n\n>>> endpoint = create_inference_endpoint(\n...     \"my-endpoint-name\",\n...     repository=\"gpt2\",\n...     framework=\"pytorch\",\n...     task=\"text-generation\",\n...     accelerator=\"cpu\",\n...     vendor=\"aws\",\n...     region=\"us-east-1\",\n...     type=\"protected\",\n...     instance_size=\"medium\",\n...     instance_type=\"c6i\"\n... )\n```", "```py\n>>> endpoint\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='pending', url=None)\n```", "```py\n# Start an Inference Endpoint running Zephyr-7b-beta on TGI\n>>> from huggingface_hub import create_inference_endpoint\n>>> endpoint = create_inference_endpoint(\n...     \"aws-zephyr-7b-beta-0486\",\n...     repository=\"HuggingFaceH4/zephyr-7b-beta\",\n...     framework=\"pytorch\",\n...     task=\"text-generation\",\n...     accelerator=\"gpu\",\n...     vendor=\"aws\",\n...     region=\"us-east-1\",\n...     type=\"protected\",\n...     instance_size=\"medium\",\n...     instance_type=\"g5.2xlarge\",\n...     custom_image={\n...         \"health_route\": \"/health\",\n...         \"env\": {\n...             \"MAX_BATCH_PREFILL_TOKENS\": \"2048\",\n...             \"MAX_INPUT_LENGTH\": \"1024\",\n...             \"MAX_TOTAL_TOKENS\": \"1512\",\n...             \"MODEL_ID\": \"/repository\"\n...         },\n...         \"url\": \"ghcr.io/huggingface/text-generation-inference:1.1.0\",\n...     },\n... )\n```", "```py\n>>> from huggingface_hub import get_inference_endpoint, list_inference_endpoints\n\n# Get one\n>>> get_inference_endpoint(\"my-endpoint-name\")\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='pending', url=None)\n\n# List all endpoints from an organization\n>>> list_inference_endpoints(namespace=\"huggingface\")\n[InferenceEndpoint(name='aws-starchat-beta', namespace='huggingface', repository='HuggingFaceH4/starchat-beta', status='paused', url=None), ...]\n\n# List all endpoints from all organizations the user belongs to\n>>> list_inference_endpoints(namespace=\"*\")\n[InferenceEndpoint(name='aws-starchat-beta', namespace='huggingface', repository='HuggingFaceH4/starchat-beta', status='paused', url=None), ...]\n```", "```py\n>>> endpoint\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='running', url='https://jpj7k2q4j805b727.us-east-1.aws.endpoints.huggingface.cloud')\n```", "```py\n>>> endpoint.fetch()\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='pending', url=None)\n```", "```py\n# Pending endpoint\n>>> endpoint\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='pending', url=None)\n\n# Wait 10s => raises a InferenceEndpointTimeoutError\n>>> endpoint.wait(timeout=10)\n    raise InferenceEndpointTimeoutError(\"Timeout while waiting for Inference Endpoint to be deployed.\")\nhuggingface_hub._inference_endpoints.InferenceEndpointTimeoutError: Timeout while waiting for Inference Endpoint to be deployed.\n\n# Wait more\n>>> endpoint.wait()\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='running', url='https://jpj7k2q4j805b727.us-east-1.aws.endpoints.huggingface.cloud')\n```", "```py\n# Run text_generation task:\n>>> endpoint.client.text_generation(\"I am\")\n' not a fan of the idea of a \"big-budget\" movie. I think it\\'s a'\n\n# Or in an asyncio context:\n>>> await endpoint.async_client.text_generation(\"I am\")\n```", "```py\n>>> endpoint.client\nhuggingface_hub._inference_endpoints.InferenceEndpointError: Cannot create a client for this Inference Endpoint as it is not yet deployed. Please wait for the Inference Endpoint to be deployed using `endpoint.wait()` and try again.\n```", "```py\n# Pause and resume endpoint\n>>> endpoint.pause()\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='paused', url=None)\n>>> endpoint.resume()\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='pending', url=None)\n>>> endpoint.wait().client.text_generation(...)\n...\n\n# Scale to zero\n>>> endpoint.scale_to_zero()\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='scaledToZero', url='https://jpj7k2q4j805b727.us-east-1.aws.endpoints.huggingface.cloud')\n# Endpoint is not 'running' but still has a URL and will restart on first call.\n```", "```py\n# Change target model\n>>> endpoint.update(repository=\"gpt2-large\")\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2-large', status='pending', url=None)\n\n# Update number of replicas\n>>> endpoint.update(min_replica=2, max_replica=6)\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2-large', status='pending', url=None)\n\n# Update to larger instance\n>>> endpoint.update(accelerator=\"cpu\", instance_size=\"large\", instance_type=\"c6i\")\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2-large', status='pending', url=None)\n```", "```py\n>>> import asyncio\n>>> from huggingface_hub import create_inference_endpoint\n\n# Start endpoint + wait until initialized\n>>> endpoint = create_inference_endpoint(name=\"batch-endpoint\",...).wait()\n\n# Run inference\n>>> client = endpoint.client\n>>> results = [client.text_generation(...) for job in jobs]\n\n# Or with asyncio\n>>> async_client = endpoint.async_client\n>>> results = asyncio.gather(*[async_client.text_generation(...) for job in jobs])\n\n# Pause endpoint\n>>> endpoint.pause()\n```", "```py\n>>> import asyncio\n>>> from huggingface_hub import get_inference_endpoint\n\n# Get endpoint + wait until initialized\n>>> endpoint = get_inference_endpoint(\"batch-endpoint\").resume().wait()\n\n# Run inference\n>>> async_client = endpoint.async_client\n>>> results = asyncio.gather(*[async_client.text_generation(...) for job in jobs])\n\n# Pause endpoint\n>>> endpoint.pause()\n```"]