# 数据收集器

> 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/data_collator](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/data_collator)

数据收集器是通过使用数据集元素列表作为输入来形成批次的对象。这些元素与 `train_dataset` 或 `eval_dataset` 的元素类型相同。

为了能够构建批次，数据收集器可能会应用一些处理（如填充）。其中一些（如 [DataCollatorForLanguageModeling](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling)）还会对形成的批次应用一些随机数据增强（如随机掩码）。

使用示例可在[示例脚本](../examples)或[示例笔记本](../notebooks)中找到。

## 默认数据收集器

#### `transformers.default_data_collator`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L74)

```py
( features: List return_tensors = 'pt' )
```

非常简单的数据收集器，只是将类似字典的对象的批次汇集在一起，并对名为潜在键执行特殊处理：

+   `label`: 处理每个对象的单个值（int 或 float）

+   `label_ids`: 处理每个对象的值列表

不执行任何额外的预处理：输入对象的属性名称将用作模型的相应输入。查看 glue 和 ner 的示例，了解它的用途。

## DefaultDataCollator

### `class transformers.DefaultDataCollator`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L99)

```py
( return_tensors: str = 'pt' )
```

参数

+   `return_tensors`（`str`，*可选*，默认为 `"pt"`）— 要返回的张量类型。允许的值为 “np”、`"pt"` 和 “tf”。

非常简单的数据收集器，只是将类似字典的对象的批次汇集在一起，并对名为潜在键执行特殊处理：

+   `label`: 处理每个对象的单个值（int 或 float）

+   `label_ids`: 处理每个对象的值列表

不执行任何额外的预处理：输入对象的属性名称将用作模型的相应输入。查看 glue 和 ner 的示例，了解它的用途。

这是一个对象（像其他数据收集器一样），而不是像 default_data_collator 那样的纯函数。如果需要在初始化时设置 return_tensors 值，这可能会有所帮助。

## DataCollatorWithPadding

### `class transformers.DataCollatorWithPadding`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L236)

```py
( tokenizer: PreTrainedTokenizerBase padding: Union = True max_length: Optional = None pad_to_multiple_of: Optional = None return_tensors: str = 'pt' )
```

参数

+   `tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer) 或 [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)）— 用于对数据进行编码的分词器。

+   `padding`（`bool`，`str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为 `True`）— 选择一种策略来填充返回的序列（根据模型的填充方向和填充索引），包括：

    +   `True` 或 `'longest'`（默认）：填充到批次中最长的序列（如果只提供单个序列，则不填充）。

    +   `'max_length'`：填充到指定的最大长度（使用参数 `max_length`）或填充到模型的最大可接受输入长度（如果未提供该参数）。

    +   `False` 或 `'do_not_pad'`：不填充（即，可以输出具有不同长度序列的批次）。

+   `max_length`（`int`，*可选*）— 返回列表的最大长度，可选填充长度（见上文）。

+   `pad_to_multiple_of`（`int`，*可选*）— 如果设置，将序列填充到提供的值的倍数。

    这对于启用具有计算能力 >= 7.5（Volta）的 NVIDIA 硬件上的 Tensor Cores 的使用特别有用。

+   `return_tensors`（`str`，*可选*，默认为 `"pt"`）— 要返回的张量类型。允许的值为 “np”、`"pt"` 和 “tf”。

数据收集器，将动态填充接收到的输入。

## DataCollatorForTokenClassification

### `class transformers.DataCollatorForTokenClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L288)

```py
( tokenizer: PreTrainedTokenizerBase padding: Union = True max_length: Optional = None pad_to_multiple_of: Optional = None label_pad_token_id: int = -100 return_tensors: str = 'pt' )
```

参数

+   `tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)或[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)）— 用于对数据进行编码的分词器。

+   `padding`（`bool`，`str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`True`）— 选择一种策略来填充返回的序列（根据模型的填充方向和填充索引），包括：

    +   `True`或`'longest'`（默认）：填充到批次中最长的序列（如果只提供单个序列，则不填充）。

    +   `'max_length'`：填充到指定的最大长度（使用参数`max_length`）或模型的最大可接受输入长度（如果未提供该参数）。

    +   `False`或`'do_not_pad'`：无填充（即，可以输出具有不同长度序列的批次）。

+   `max_length`（`int`，*可选*）— 返回列表的最大长度和可选填充长度（见上文）。

+   `pad_to_multiple_of`（`int`，*可选*）— 如果设置，将序列填充到提供的值的倍数。

    这对于启用具有计算能力>= 7.5（Volta）的 NVIDIA 硬件上的 Tensor Cores 特别有用。

+   `label_pad_token_id`（`int`，*可选*，默认为-100）— 填充标签时要使用的 id（-100 将被 PyTorch 损失函数自动忽略）。

+   `return_tensors`（`str`，*可选*，默认为`"pt"`）— 要返回的 Tensor 类型。允许的值为“np”，`"pt"`和“tf”。

数据收集器，将动态填充接收到的输入以及标签。

## DataCollatorForSeq2Seq

### `class transformers.DataCollatorForSeq2Seq`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L542)

```py
( tokenizer: PreTrainedTokenizerBase model: Optional = None padding: Union = True max_length: Optional = None pad_to_multiple_of: Optional = None label_pad_token_id: int = -100 return_tensors: str = 'pt' )
```

参数

+   `tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)或[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)）— 用于对数据进行编码的分词器。

+   `model`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)，*可选*）— 正在训练的模型。如果设置并且具有*prepare_decoder_input_ids_from_labels*，则使用它来准备*decoder_input_ids*

    这在使用*label_smoothing*时很有用，以避免计算损失两次。

+   `padding`（`bool`，`str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`True`）— 选择一种策略来填充返回的序列（根据模型的填充方向和填充索引），包括：

    +   `True`或`'longest'`（默认）：填充到批次中最长的序列（如果只提供单个序列，则不填充）。

    +   `'max_length'`：填充到指定的最大长度（使用参数`max_length`）或模型的最大可接受输入长度（如果未提供该参数）。

    +   `False`或`'do_not_pad'`：无填充（即，可以输出具有不同长度序列的批次）。

+   `max_length`（`int`，*可选*）— 返回列表的最大长度和可选填充长度（见上文）。

+   `pad_to_multiple_of`（`int`，*可选*）— 如果设置，将序列填充到提供的值的倍数。

    这对于启用具有计算能力>= 7.5（Volta）的 NVIDIA 硬件上的 Tensor Cores 特别有用。

+   `label_pad_token_id`（`int`，*可选*，默认为-100）— 填充标签时要使用的 id（-100 将被 PyTorch 损失函数自动忽略）。

+   `return_tensors` (`str`, *optional*, 默认为`"pt"`) — 要返回的张量类型。允许的值为“np”，`"pt"`和“tf”。

数据收集器将动态填充接收到的输入和标签。

## DataCollatorForLanguageModeling

### `class transformers.DataCollatorForLanguageModeling`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L633)

```py
( tokenizer: PreTrainedTokenizerBase mlm: bool = True mlm_probability: float = 0.15 pad_to_multiple_of: Optional = None tf_experimental_compile: bool = False return_tensors: str = 'pt' )
```

参数

+   `tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)或[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)） — 用于对数据进行编码的分词器。

+   `mlm` (`bool`, *optional*, 默认为 `True`) — 是否使用遮罩语言建模。如果设置为 `False`，则标签与输入相同，忽略填充标记（通过将它们设置为-100）。否则，对于未遮罩的标记，标签为-100，对于遮罩的标记，值为要预测的值。

+   `mlm_probability` (`float`, *optional*, 默认为0.15) — 当`mlm`设置为`True`时，在输入中（随机）遮罩标记的概率。

+   `pad_to_multiple_of` (`int`, *optional*) — 如果设置，将序列填充到提供的值的倍数。

+   `return_tensors` (`str`) — 要返回的张量类型。允许的值为“np”，“pt”和“tf”。

用于语言建模的数据收集器。如果它们的长度不相同，则输入将动态填充到批次的最大长度。

为了获得最佳性能，应该将此数据收集器与具有字典或BatchEncoding项目的数据集一起使用，其中包含`"special_tokens_mask"`键，如由[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)或[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)返回的，参数`return_special_tokens_mask=True`。

#### `numpy_mask_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L839)

```py
( inputs: Any special_tokens_mask: Optional = None )
```

为遮罩语言建模准备遮罩标记输入/标签：80% MASK，10% 随机，10% 原始。

#### `tf_mask_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L686)

```py
( inputs: Any vocab_size mask_token_id special_tokens_mask: Optional = None )
```

为遮罩语言建模准备遮罩标记输入/标签：80% MASK，10% 随机，10% 原始。

#### `torch_mask_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L782)

```py
( inputs: Any special_tokens_mask: Optional = None )
```

为遮罩语言建模准备遮罩标记输入/标签：80% MASK，10% 随机，10% 原始。

## DataCollatorForWholeWordMask

### `class transformers.DataCollatorForWholeWordMask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L877)

```py
( tokenizer: PreTrainedTokenizerBase mlm: bool = True mlm_probability: float = 0.15 pad_to_multiple_of: Optional = None tf_experimental_compile: bool = False return_tensors: str = 'pt' )
```

用于遮罩整个单词的语言建模数据收集器。

+   整理张量批次，尊重它们的分词器的pad_token

+   为遮罩语言建模预处理批次

此数据收集器依赖于[BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)对子词分词的实现细节，特别是子词标记以*##*为前缀。对于不遵循此方案的分词器，此数据收集器将产生一个大致等同于`.DataCollatorForLanguageModeling`的输出。

#### `numpy_mask_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1108)

```py
( inputs: Any mask_labels: Any )
```

为遮罩语言建模准备遮罩标记输入/标签：80% MASK，10% 随机，10% 原始。设置'mask_labels'表示我们使用整词遮罩（wwm），我们根据其引用直接遮罩idxs。

#### `tf_mask_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1066)

```py
( inputs: Any mask_labels: Any )
```

为掩码语言建模准备掩码标记输入/标签：80%掩码，10%随机，10%原始。设置'mask_labels'表示我们使用整词掩码（wwm），我们根据其引用直接掩码idxs。

#### `torch_mask_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1026)

```py
( inputs: Any mask_labels: Any )
```

为掩码语言建模准备掩码标记输入/标签：80%掩码，10%随机，10%原始。设置'mask_labels'表示我们使用整词掩码（wwm），我们根据其引用直接掩码idxs。

## DataCollatorForPermutationLanguageModeling

### `class transformers.DataCollatorForPermutationLanguageModeling`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1232)

```py
( tokenizer: PreTrainedTokenizerBase plm_probability: float = 0.16666666666666666 max_span_length: int = 5 return_tensors: str = 'pt' )
```

用于排列语言建模的数据收集器。

+   整理张量批次，尊重其分词器的pad_token

+   使用特定于XLNet的程序对排列语言建模进行预处理

#### `numpy_mask_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1473)

```py
( inputs: Any )
```

特定序列要预测的掩码标记由以下算法确定：

1.  从序列开头开始，通过设置`cur_len = 0`（到目前为止已处理的标记数）。

1.  从区间`[1, max_span_length]`中采样`span_length`（要掩码的标记跨度的长度）

1.  保留长度为`context_length = span_length / plm_probability`的上下文以包围要掩码的跨度

1.  从区间`[cur_len, cur_len + context_length - span_length]`中采样起始点`start_index`并掩码标记`start_index:start_index + span_length`

1.  设置`cur_len = cur_len + context_length`。如果`cur_len < max_len`（即序列中仍有剩余标记要处理），则从步骤1重复。

#### `tf_mask_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1366)

```py
( inputs: Any )
```

特定序列要预测的掩码标记由以下算法确定：

1.  从序列开头开始，通过设置`cur_len = 0`（到目前为止已处理的标记数）。

1.  从区间`[1, max_span_length]`中采样`span_length`（要掩码的标记跨度的长度）

1.  保留长度为`context_length = span_length / plm_probability`的上下文以包围要掩码的跨度

1.  从区间`[cur_len, cur_len + context_length - span_length]`中采样起始点`start_index`并掩码标记`start_index:start_index + span_length`

1.  设置`cur_len = cur_len + context_length`。如果`cur_len < max_len`（即序列中仍有剩余标记要处理），则从步骤1重复。

#### `torch_mask_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/data/data_collator.py#L1267)

```py
( inputs: Any )
```

特定序列要预测的掩码标记由以下算法确定：

1.  从序列开头开始，通过设置`cur_len = 0`（到目前为止已处理的标记数）。

1.  从区间`[1, max_span_length]`中采样`span_length`（要掩码的标记跨度的长度）

1.  保留长度为`context_length = span_length / plm_probability`的上下文以包围要掩码的跨度

1.  从区间`[cur_len, cur_len + context_length - span_length]`中采样起始点`start_index`并掩码标记`start_index:start_index + span_length`

1.  设置`cur_len = cur_len + context_length`。如果`cur_len < max_len`（即序列中仍有剩余标记要处理），则从步骤1重复。
