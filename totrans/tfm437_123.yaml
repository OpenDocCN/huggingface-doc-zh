- en: Quantization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/quantization](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/quantization)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/quantization](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/quantization)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Quantization techniques reduces memory and computational costs by representing
    weights and activations with lower-precision data types like 8-bit integers (int8).
    This enables loading larger models you normally wouldn’t be able to fit into memory,
    and speeding up inference. Transformers supports the AWQ and GPTQ quantization
    algorithms and it supports 8-bit and 4-bit quantization with bitsandbytes.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 量化技术通过使用低精度数据类型（如8位整数（int8））表示权重和激活来减少内存和计算成本。这使得您可以加载通常无法放入内存的更大模型，并加快推理速度。Transformers支持AWQ和GPTQ量化算法，支持8位和4位量化与bitsandbytes。
- en: Learn how to quantize models in the [Quantization](../quantization) guide.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 了解如何在[量化](../quantization)指南中量化模型。
- en: AwqConfig
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AwqConfig
- en: '### `class transformers.AwqConfig`'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.AwqConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L542)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L542)'
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`bits` (`int`, *optional*, defaults to 4) — The number of bits to quantize
    to.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bits` (`int`，*可选*，默认为4) — 要量化为的位数。'
- en: '`group_size` (`int`, *optional*, defaults to 128) — The group size to use for
    quantization. Recommended value is 128 and -1 uses per-column quantization.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`group_size` (`int`，*可选*，默认为128) — 用于量化的组大小。推荐值为128，-1使用每列量化。'
- en: '`zero_point` (`bool`, *optional*, defaults to `True`) — Whether to use zero
    point quantization.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zero_point` (`bool`，*可选*，默认为`True`) — 是否使用零点量化。'
- en: '`version` (`AWQLinearVersion`, *optional*, defaults to `AWQLinearVersion.GEMM`)
    — The version of the quantization algorithm to use. GEMM is better for big batch_size
    (e.g. >= 8) otherwise, GEMV is better (e.g. < 8 )'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`version` (`AWQLinearVersion`，*可选*，默认为`AWQLinearVersion.GEMM`) — 要使用的量化算法版本。对于大批量大小（例如>=
    8），GEMM更好，否则，GEMV更好（例如< 8）'
- en: '`backend` (`AwqBackendPackingMethod`, *optional*, defaults to `AwqBackendPackingMethod.AUTOAWQ`)
    — The quantization backend. Some models might be quantized using `llm-awq` backend.
    This is useful for users that quantize their own models using `llm-awq` library.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backend` (`AwqBackendPackingMethod`，*可选*，默认为`AwqBackendPackingMethod.AUTOAWQ`)
    — 量化后端。一些模型可能使用`llm-awq`后端进行量化。这对于使用`llm-awq`库量化自己的模型的用户很有用。'
- en: '`do_fuse` (`bool`, *optional*, defaults to `False`) — Whether to fuse attention
    and mlp layers together for faster inference'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_fuse` (`bool`，*可选*，默认为`False`) — 是否将注意力和mlp层融合在一起以加快推理速度'
- en: '`fuse_max_seq_len` (`int`, *optional*) — The Maximum sequence length to generate
    when using fusing.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fuse_max_seq_len` (`int`，*可选*) — 使用融合时生成的最大序列长度。'
- en: '`modules_to_fuse` (`dict`, *optional*, default to `None`) — Overwrite the natively
    supported fusing scheme with the one specified by the users.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modules_to_fuse` (`dict`，*可选*，默认为`None`) — 用用户指定的融合方案覆盖原生支持的融合方案。'
- en: '`modules_to_not_convert` (`list`, *optional*, default to `None`) — The list
    of modules to not quantize, useful for quantizing models that explicitly require
    to have some modules left in their original precision (e.g. Whisper encoder, Llava
    encoder, Mixtral gate layers). Note you cannot quantize directly with transformers,
    please refer to `AutoAWQ` documentation for quantizing HF models.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modules_to_not_convert` (`list`，*可选*，默认为`None`) — 不量化的模块列表，对于需要明确保留一些模块在原始精度中的模型进行量化很有用（例如Whisper编码器，Llava编码器，Mixtral门层）。请注意，您不能直接使用transformers进行量化，请参考`AutoAWQ`文档以量化HF模型。'
- en: This is a wrapper class about all possible attributes and features that you
    can play with a model that has been loaded using `auto-awq` library awq quantization
    relying on auto_awq backend.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关于所有可能属性和特性的包装类，您可以使用`auto-awq`库加载的模型进行玩耍，该库依赖于auto_awq后端的awq量化。
- en: '#### `post_init`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_init`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L605)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L605)'
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Safety checker that arguments are correct
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 安全检查器检查参数是否正确
- en: GPTQConfig
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPTQConfig
- en: '### `class transformers.GPTQConfig`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.GPTQConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L328)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L328)'
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`bits` (`int`) — The number of bits to quantize to, supported numbers are (2,
    3, 4, 8).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bits` (`int`) — 要量化为的位数，支持的数字为（2、3、4、8）。'
- en: '`tokenizer` (`str` or `PreTrainedTokenizerBase`, *optional*) — The tokenizer
    used to process the dataset. You can pass either:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`str`或`PreTrainedTokenizerBase`，*可选*) — 用于处理数据集的分词器。您可以传递以下内容：'
- en: A custom tokenizer object.
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义分词器对象。
- en: A string, the *model id* of a predefined tokenizer hosted inside a model repo
    on huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字符串，托管在huggingface.co模型存储库中的预定义分词器的*模型id*。有效的模型id可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。
- en: A path to a *directory* containing vocabulary files required by the tokenizer,
    for instance saved using the [save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained)
    method, e.g., `./my_model_directory/`.
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含分词器所需词汇文件的*目录*路径，例如使用[save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained)方法保存的目录，例如`./my_model_directory/`。
- en: '`dataset` (`Union[List[str]]`, *optional*) — The dataset used for quantization.
    You can provide your own dataset in a list of string or just use the original
    datasets used in GPTQ paper [‘wikitext2’,‘c4’,‘c4-new’,‘ptb’,‘ptb-new’]'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset` (`Union[List[str]]`，*可选*) — 用于量化的数据集。您可以提供自己的数据集列表，或者只使用GPTQ论文中使用的原始数据集[‘wikitext2’，‘c4’，‘c4-new’，‘ptb’，‘ptb-new’]'
- en: '`group_size` (`int`, *optional*, defaults to 128) — The group size to use for
    quantization. Recommended value is 128 and -1 uses per-column quantization.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`group_size` (`int`, *optional*, defaults to 128) — 用于量化的组大小。推荐值为128，-1使用每列量化。'
- en: '`damp_percent` (`float`, *optional*, defaults to 0.1) — The percent of the
    average Hessian diagonal to use for dampening. Recommended value is 0.1.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`damp_percent` (`float`, *optional*, defaults to 0.1) — 用于阻尼的平均Hessian对角线的百分比。推荐值为0.1。'
- en: '`desc_act` (`bool`, *optional*, defaults to `False`) — Whether to quantize
    columns in order of decreasing activation size. Setting it to False can significantly
    speed up inference but the perplexity may become slightly worse. Also known as
    act-order.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`desc_act` (`bool`, *optional*, defaults to `False`) — 是否按照激活大小递减的顺序量化列。将其设置为False可以显著加快推理速度，但困惑度可能会略有下降。也称为act-order。'
- en: '`sym` (`bool`, *optional*, defaults to `True`) — Whether to use symetric quantization.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sym` (`bool`, *optional*, defaults to `True`) — 是否使用对称量化。'
- en: '`true_sequential` (`bool`, *optional*, defaults to `True`) — Whether to perform
    sequential quantization even within a single Transformer block. Instead of quantizing
    the entire block at once, we perform layer-wise quantization. As a result, each
    layer undergoes quantization using inputs that have passed through the previously
    quantized layers.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`true_sequential` (`bool`, *optional*, defaults to `True`) — 是否在单个Transformer块内执行顺序量化。我们执行逐层量化，而不是一次性量化整个块。因此，每个层都经历了使用已通过先前量化的层的输入进行量化的过程。'
- en: '`use_cuda_fp16` (`bool`, *optional*, defaults to `False`) — Whether or not
    to use optimized cuda kernel for fp16 model. Need to have model in fp16.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cuda_fp16` (`bool`, *optional*, defaults to `False`) — 是否使用优化的cuda内核进行fp16模型。需要将模型设置为fp16。'
- en: '`model_seqlen` (`int`, *optional*) — The maximum sequence length that the model
    can take.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_seqlen` (`int`, *optional*) — 模型可以接受的最大序列长度。'
- en: '`block_name_to_quantize` (`str`, *optional*) — The transformers block name
    to quantize. If None, we will infer the block name using common patterns (e.g.
    model.layers)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`block_name_to_quantize` (`str`, *optional*) — 要量化的transformers块名称。如果为None，我们将使用常见模式（例如model.layers）推断块名称。'
- en: '`module_name_preceding_first_block` (`List[str]`, *optional*) — The layers
    that are preceding the first Transformer block.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`module_name_preceding_first_block` (`List[str]`, *optional*) — 在第一个Transformer块之前的层。'
- en: '`batch_size` (`int`, *optional*, defaults to 1) — The batch size used when
    processing the dataset'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` (`int`, *optional*, defaults to 1) — 处理数据集时使用的批量大小'
- en: '`pad_token_id` (`int`, *optional*) — The pad token id. Needed to prepare the
    dataset when `batch_size` > 1.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *optional*) — 填充标记id。当`batch_size` > 1时需要准备数据集。'
- en: '`use_exllama` (`bool`, *optional*) — Whether to use exllama backend. Defaults
    to `True` if unset. Only works with `bits` = 4.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_exllama` (`bool`, *optional*) — 是否使用exllama后端。如果未设置，默认为`True`。仅在`bits`
    = 4时有效。'
- en: '`max_input_length` (`int`, *optional*) — The maximum input length. This is
    needed to initialize a buffer that depends on the maximum expected input length.
    It is specific to the exllama backend with act-order.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_input_length` (`int`, *optional*) — 最大输入长度。这是为了初始化一个依赖于最大预期输入长度的缓冲区而需要的。它是特定于具有act-order的exllama后端。'
- en: '`exllama_config` (`Dict[str, Any]`, *optional*) — The exllama config. You can
    specify the version of the exllama kernel through the `version` key. Defaults
    to `{"version": 1}` if unset.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exllama_config` (`Dict[str, Any]`, *optional*) — exllama配置。您可以通过`version`键指定exllama内核的版本。如果未设置，默认为`{"version":
    1}`。'
- en: '`cache_block_outputs` (`bool`, *optional*, defaults to `True`) — Whether to
    cache block outputs to reuse as inputs for the succeeding block.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache_block_outputs` (`bool`, *optional*, defaults to `True`) — 是否缓存块输出以便作为后续块的输入重复使用。'
- en: '`modules_in_block_to_quantize` (`List[List[str]]`, *optional*) — List of list
    of module names to quantize in the specified block. This argument is useful to
    exclude certain linear modules from being quantized. The block to quantize can
    be specified by setting `block_name_to_quantize`. We will quantize each list sequentially.
    If not set, we will quantize all linear layers. Example: `modules_in_block_to_quantize
    =[["self_attn.k_proj", "self_attn.v_proj", "self_attn.q_proj"], ["self_attn.o_proj"]]`.
    In this example, we will first quantize the q,k,v layers simultaneously since
    they are independent. Then, we will quantize `self_attn.o_proj` layer with the
    q,k,v layers quantized. This way, we will get better results since it reflects
    the real input `self_attn.o_proj` will get when the model is quantized.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modules_in_block_to_quantize` (`List[List[str]]`, *optional*) — 要在指定块中量化的模块名称列表的列表。此参数可用于排除某些线性模块的量化。可以通过设置`block_name_to_quantize`来指定要量化的块。我们将依次量化每个列表。如果未设置，我们将量化所有线性层。示例：`modules_in_block_to_quantize
    =[["self_attn.k_proj", "self_attn.v_proj", "self_attn.q_proj"], ["self_attn.o_proj"]]`。在此示例中，我们将首先同时量化q、k、v层，因为它们是独立的。然后，我们将在量化q、k、v层的基础上量化`self_attn.o_proj`层。这样，我们将获得更好的结果，因为它反映了模型在量化时`self_attn.o_proj`将获得的真实输入。'
- en: This is a wrapper class about all possible attributes and features that you
    can play with a model that has been loaded using `optimum` api for gptq quantization
    relying on auto_gptq backend.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关于使用`optimum` api加载的模型的所有可能属性和特性的包装类，用于依赖于auto_gptq后端的gptq量化。
- en: '#### `from_dict_optimum`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_dict_optimum`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L527)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L527)'
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Get compatible class with optimum gptq config dict
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 获取与最佳gptq配置字典兼容的类
- en: '#### `post_init`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_init`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L444)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L444)'
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Safety checker that arguments are correct
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 确保参数正确的安全检查器
- en: '#### `to_dict_optimum`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_dict_optimum`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L518)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L518)'
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Get compatible dict for optimum gptq config
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 获取最佳gptq配置的兼容字典
- en: BitsAndBytesConfig
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BitsAndBytesConfig
- en: '### `class transformers.BitsAndBytesConfig`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BitsAndBytesConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L150)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L150)'
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`load_in_8bit` (`bool`, *optional*, defaults to `False`) — This flag is used
    to enable 8-bit quantization with LLM.int8().'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_in_8bit` (`bool`, *optional*, 默认为 `False`) — 此标志用于启用LLM.int8()的8位量化。'
- en: '`load_in_4bit` (`bool`, *optional*, defaults to `False`) — This flag is used
    to enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers
    from `bitsandbytes`.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_in_4bit` (`bool`, *optional*, 默认为 `False`) — 此标志用于通过使用`bitsandbytes`中的FP4/NF4层替换线性层来启用4位量化。'
- en: '`llm_int8_threshold` (`float`, *optional*, defaults to 6.0) — This corresponds
    to the outlier threshold for outlier detection as described in `LLM.int8() : 8-bit
    Matrix Multiplication for Transformers at Scale` paper: [https://arxiv.org/abs/2208.07339](https://arxiv.org/abs/2208.07339)
    Any hidden states value that is above this threshold will be considered an outlier
    and the operation on those values will be done in fp16\. Values are usually normally
    distributed, that is, most values are in the range [-3.5, 3.5], but there are
    some exceptional systematic outliers that are very differently distributed for
    large models. These outliers are often in the interval [-60, -6] or [6, 60]. Int8
    quantization works well for values of magnitude ~5, but beyond that, there is
    a significant performance penalty. A good default threshold is 6, but a lower
    threshold might be needed for more unstable models (small models, fine-tuning).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`llm_int8_threshold` (`float`, *optional*, 默认为 6.0) — 这对应于异常值检测中的异常值阈值，如`LLM.int8()：规模化变压器的8位矩阵乘法`论文中所述：[https://arxiv.org/abs/2208.07339](https://arxiv.org/abs/2208.07339)
    超过此阈值的任何隐藏状态值将被视为异常值，并且对这些值的操作将在fp16中进行。值通常服从正态分布，即，大多数值在范围[-3.5, 3.5]内，但对于大型模型，有一些异常系统性异常值，其分布方式非常不同。这些异常值通常在区间[-60,
    -6]或[6, 60]内。对于幅度约为5的值，int8量化效果很好，但超过这个范围，性能会受到显著影响。一个很好的默认阈值是6，但对于更不稳定的模型（小模型，微调），可能需要更低的阈值。'
- en: '`llm_int8_skip_modules` (`List[str]`, *optional*) — An explicit list of the
    modules that we do not want to convert in 8-bit. This is useful for models such
    as Jukebox that has several heads in different places and not necessarily at the
    last position. For example for `CausalLM` models, the last `lm_head` is kept in
    its original `dtype`.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`llm_int8_skip_modules` (`List[str]`, *optional*) — 我们不希望将其转换为8位的模块的显式列表。这对于具有不同位置的几个头部且不一定在最后位置的模型非常有用，例如Jukebox。例如，对于`CausalLM`模型，最后的`lm_head`保留在其原始`dtype`中。'
- en: '`llm_int8_enable_fp32_cpu_offload` (`bool`, *optional*, defaults to `False`)
    — This flag is used for advanced use cases and users that are aware of this feature.
    If you want to split your model in different parts and run some parts in int8
    on GPU and some parts in fp32 on CPU, you can use this flag. This is useful for
    offloading large models such as `google/flan-t5-xxl`. Note that the int8 operations
    will not be run on CPU.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`llm_int8_enable_fp32_cpu_offload` (`bool`, *optional*, 默认为 `False`) — 此标志用于高级用例和了解此功能的用户。如果要将模型分成不同部分，并在GPU上的某些部分中使用int8，在CPU上的某些部分中使用fp32，则可以使用此标志。这对于卸载大型模型（例如`google/flan-t5-xxl`）非常有用。请注意，int8操作将不会在CPU上运行。'
- en: '`llm_int8_has_fp16_weight` (`bool`, *optional*, defaults to `False`) — This
    flag runs LLM.int8() with 16-bit main weights. This is useful for fine-tuning
    as the weights do not have to be converted back and forth for the backward pass.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`llm_int8_has_fp16_weight` (`bool`, *optional*, 默认为 `False`) — 此标志使用16位主权重运行LLM.int8()。这对于微调很有用，因为权重不必在反向传播中来回转换。'
- en: '`bnb_4bit_compute_dtype` (`torch.dtype` or str, *optional*, defaults to `torch.float32`)
    — This sets the computational type which might be different than the input time.
    For example, inputs might be fp32, but computation can be set to bf16 for speedups.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bnb_4bit_compute_dtype` (`torch.dtype`或str, *optional*, 默认为 `torch.float32`)
    — 这将设置可能与输入时间不同的计算类型。例如，输入可能是fp32，但计算可以设置为bf16以加快速度。'
- en: '`bnb_4bit_quant_type` (`str`, *optional*, defaults to `"fp4"`) — This sets
    the quantization data type in the bnb.nn.Linear4Bit layers. Options are FP4 and
    NF4 data types which are specified by `fp4` or `nf4`.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bnb_4bit_quant_type` (`str`, *optional*, 默认为 `"fp4"`) — 这将在bnb.nn.Linear4Bit层中设置量化数据类型。选项是FP4和NF4数据类型，由`fp4`或`nf4`指定。'
- en: '`bnb_4bit_use_double_quant` (`bool`, *optional*, defaults to `False`) — This
    flag is used for nested quantization where the quantization constants from the
    first quantization are quantized again.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bnb_4bit_use_double_quant` (`bool`, *optional*, 默认为 `False`) — 此标志用于嵌套量化，其中第一次量化的量化常数再次量化。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional parameters from which
    to initialize the configuration object.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *optional*) — 从中初始化配置对象的其他参数。'
- en: This is a wrapper class about all possible attributes and features that you
    can play with a model that has been loaded using `bitsandbytes`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关于使用`bitsandbytes`加载的模型可以玩耍的所有可能属性和功能的包装类。
- en: This replaces `load_in_8bit` or `load_in_4bit`therefore both options are mutually
    exclusive.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这取代了`load_in_8bit`或`load_in_4bit`，因此这两个选项是互斥的。
- en: Currently only supports `LLM.int8()`, `FP4`, and `NF4` quantization. If more
    methods are added to `bitsandbytes`, then more arguments will be added to this
    class.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 目前仅支持`LLM.int8()`，`FP4`和`NF4`量化。如果向`bitsandbytes`添加更多方法，则将向此类添加更多参数。
- en: '#### `is_quantizable`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `is_quantizable`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L266)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L266)'
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Returns `True` if the model is quantizable, `False` otherwise.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型可以量化，则返回`True`，否则返回`False`。
- en: '#### `post_init`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_init`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L235)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L235)'
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Safety checker that arguments are correct - also replaces some NoneType arguments
    with their default values.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 安全检查器，检查参数是否正确 - 还将一些NoneType参数替换为它们的默认值。
- en: '#### `quantization_method`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `quantization_method`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L272)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L272)'
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This method returns the quantization method used for the model. If the model
    is not quantizable, it returns `None`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法返回模型使用的量化方法。如果模型不可量化，则返回`None`。
- en: '#### `to_diff_dict`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to_diff_dict`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L300)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L300)'
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Returns
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`Dict[str, Any]`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dict[str, Any]`'
- en: Dictionary of all the attributes that make up this configuration instance,
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 构成此配置实例的所有属性的字典，
- en: Removes all attributes from config which correspond to the default config attributes
    for better readability and serializes to a Python dictionary.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 从配置中删除所有与默认配置属性相对应的属性，以提高可读性并序列化为Python字典。
