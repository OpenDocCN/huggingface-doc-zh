- en: Quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/quantization](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/quantization)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/52.0ccc915c.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: Quantization techniques reduces memory and computational costs by representing
    weights and activations with lower-precision data types like 8-bit integers (int8).
    This enables loading larger models you normally wouldn’t be able to fit into memory,
    and speeding up inference. Transformers supports the AWQ and GPTQ quantization
    algorithms and it supports 8-bit and 4-bit quantization with bitsandbytes.
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to quantize models in the [Quantization](../quantization) guide.
  prefs: []
  type: TYPE_NORMAL
- en: AwqConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.AwqConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L542)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`bits` (`int`, *optional*, defaults to 4) — The number of bits to quantize
    to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`group_size` (`int`, *optional*, defaults to 128) — The group size to use for
    quantization. Recommended value is 128 and -1 uses per-column quantization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zero_point` (`bool`, *optional*, defaults to `True`) — Whether to use zero
    point quantization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`version` (`AWQLinearVersion`, *optional*, defaults to `AWQLinearVersion.GEMM`)
    — The version of the quantization algorithm to use. GEMM is better for big batch_size
    (e.g. >= 8) otherwise, GEMV is better (e.g. < 8 )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`backend` (`AwqBackendPackingMethod`, *optional*, defaults to `AwqBackendPackingMethod.AUTOAWQ`)
    — The quantization backend. Some models might be quantized using `llm-awq` backend.
    This is useful for users that quantize their own models using `llm-awq` library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_fuse` (`bool`, *optional*, defaults to `False`) — Whether to fuse attention
    and mlp layers together for faster inference'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fuse_max_seq_len` (`int`, *optional*) — The Maximum sequence length to generate
    when using fusing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modules_to_fuse` (`dict`, *optional*, default to `None`) — Overwrite the natively
    supported fusing scheme with the one specified by the users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modules_to_not_convert` (`list`, *optional*, default to `None`) — The list
    of modules to not quantize, useful for quantizing models that explicitly require
    to have some modules left in their original precision (e.g. Whisper encoder, Llava
    encoder, Mixtral gate layers). Note you cannot quantize directly with transformers,
    please refer to `AutoAWQ` documentation for quantizing HF models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a wrapper class about all possible attributes and features that you
    can play with a model that has been loaded using `auto-awq` library awq quantization
    relying on auto_awq backend.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `post_init`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L605)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Safety checker that arguments are correct
  prefs: []
  type: TYPE_NORMAL
- en: GPTQConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.GPTQConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L328)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`bits` (`int`) — The number of bits to quantize to, supported numbers are (2,
    3, 4, 8).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`str` or `PreTrainedTokenizerBase`, *optional*) — The tokenizer
    used to process the dataset. You can pass either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A custom tokenizer object.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *model id* of a predefined tokenizer hosted inside a model repo
    on huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* containing vocabulary files required by the tokenizer,
    for instance saved using the [save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained)
    method, e.g., `./my_model_directory/`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataset` (`Union[List[str]]`, *optional*) — The dataset used for quantization.
    You can provide your own dataset in a list of string or just use the original
    datasets used in GPTQ paper [‘wikitext2’,‘c4’,‘c4-new’,‘ptb’,‘ptb-new’]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`group_size` (`int`, *optional*, defaults to 128) — The group size to use for
    quantization. Recommended value is 128 and -1 uses per-column quantization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`damp_percent` (`float`, *optional*, defaults to 0.1) — The percent of the
    average Hessian diagonal to use for dampening. Recommended value is 0.1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`desc_act` (`bool`, *optional*, defaults to `False`) — Whether to quantize
    columns in order of decreasing activation size. Setting it to False can significantly
    speed up inference but the perplexity may become slightly worse. Also known as
    act-order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sym` (`bool`, *optional*, defaults to `True`) — Whether to use symetric quantization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`true_sequential` (`bool`, *optional*, defaults to `True`) — Whether to perform
    sequential quantization even within a single Transformer block. Instead of quantizing
    the entire block at once, we perform layer-wise quantization. As a result, each
    layer undergoes quantization using inputs that have passed through the previously
    quantized layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cuda_fp16` (`bool`, *optional*, defaults to `False`) — Whether or not
    to use optimized cuda kernel for fp16 model. Need to have model in fp16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_seqlen` (`int`, *optional*) — The maximum sequence length that the model
    can take.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`block_name_to_quantize` (`str`, *optional*) — The transformers block name
    to quantize. If None, we will infer the block name using common patterns (e.g.
    model.layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`module_name_preceding_first_block` (`List[str]`, *optional*) — The layers
    that are preceding the first Transformer block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — The batch size used when
    processing the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*) — The pad token id. Needed to prepare the
    dataset when `batch_size` > 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_exllama` (`bool`, *optional*) — Whether to use exllama backend. Defaults
    to `True` if unset. Only works with `bits` = 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_input_length` (`int`, *optional*) — The maximum input length. This is
    needed to initialize a buffer that depends on the maximum expected input length.
    It is specific to the exllama backend with act-order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exllama_config` (`Dict[str, Any]`, *optional*) — The exllama config. You can
    specify the version of the exllama kernel through the `version` key. Defaults
    to `{"version": 1}` if unset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_block_outputs` (`bool`, *optional*, defaults to `True`) — Whether to
    cache block outputs to reuse as inputs for the succeeding block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modules_in_block_to_quantize` (`List[List[str]]`, *optional*) — List of list
    of module names to quantize in the specified block. This argument is useful to
    exclude certain linear modules from being quantized. The block to quantize can
    be specified by setting `block_name_to_quantize`. We will quantize each list sequentially.
    If not set, we will quantize all linear layers. Example: `modules_in_block_to_quantize
    =[["self_attn.k_proj", "self_attn.v_proj", "self_attn.q_proj"], ["self_attn.o_proj"]]`.
    In this example, we will first quantize the q,k,v layers simultaneously since
    they are independent. Then, we will quantize `self_attn.o_proj` layer with the
    q,k,v layers quantized. This way, we will get better results since it reflects
    the real input `self_attn.o_proj` will get when the model is quantized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a wrapper class about all possible attributes and features that you
    can play with a model that has been loaded using `optimum` api for gptq quantization
    relying on auto_gptq backend.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_dict_optimum`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L527)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Get compatible class with optimum gptq config dict
  prefs: []
  type: TYPE_NORMAL
- en: '#### `post_init`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L444)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Safety checker that arguments are correct
  prefs: []
  type: TYPE_NORMAL
- en: '#### `to_dict_optimum`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L518)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Get compatible dict for optimum gptq config
  prefs: []
  type: TYPE_NORMAL
- en: BitsAndBytesConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.BitsAndBytesConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L150)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`load_in_8bit` (`bool`, *optional*, defaults to `False`) — This flag is used
    to enable 8-bit quantization with LLM.int8().'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_in_4bit` (`bool`, *optional*, defaults to `False`) — This flag is used
    to enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers
    from `bitsandbytes`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`llm_int8_threshold` (`float`, *optional*, defaults to 6.0) — This corresponds
    to the outlier threshold for outlier detection as described in `LLM.int8() : 8-bit
    Matrix Multiplication for Transformers at Scale` paper: [https://arxiv.org/abs/2208.07339](https://arxiv.org/abs/2208.07339)
    Any hidden states value that is above this threshold will be considered an outlier
    and the operation on those values will be done in fp16\. Values are usually normally
    distributed, that is, most values are in the range [-3.5, 3.5], but there are
    some exceptional systematic outliers that are very differently distributed for
    large models. These outliers are often in the interval [-60, -6] or [6, 60]. Int8
    quantization works well for values of magnitude ~5, but beyond that, there is
    a significant performance penalty. A good default threshold is 6, but a lower
    threshold might be needed for more unstable models (small models, fine-tuning).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`llm_int8_skip_modules` (`List[str]`, *optional*) — An explicit list of the
    modules that we do not want to convert in 8-bit. This is useful for models such
    as Jukebox that has several heads in different places and not necessarily at the
    last position. For example for `CausalLM` models, the last `lm_head` is kept in
    its original `dtype`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`llm_int8_enable_fp32_cpu_offload` (`bool`, *optional*, defaults to `False`)
    — This flag is used for advanced use cases and users that are aware of this feature.
    If you want to split your model in different parts and run some parts in int8
    on GPU and some parts in fp32 on CPU, you can use this flag. This is useful for
    offloading large models such as `google/flan-t5-xxl`. Note that the int8 operations
    will not be run on CPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`llm_int8_has_fp16_weight` (`bool`, *optional*, defaults to `False`) — This
    flag runs LLM.int8() with 16-bit main weights. This is useful for fine-tuning
    as the weights do not have to be converted back and forth for the backward pass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bnb_4bit_compute_dtype` (`torch.dtype` or str, *optional*, defaults to `torch.float32`)
    — This sets the computational type which might be different than the input time.
    For example, inputs might be fp32, but computation can be set to bf16 for speedups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bnb_4bit_quant_type` (`str`, *optional*, defaults to `"fp4"`) — This sets
    the quantization data type in the bnb.nn.Linear4Bit layers. Options are FP4 and
    NF4 data types which are specified by `fp4` or `nf4`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bnb_4bit_use_double_quant` (`bool`, *optional*, defaults to `False`) — This
    flag is used for nested quantization where the quantization constants from the
    first quantization are quantized again.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional parameters from which
    to initialize the configuration object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a wrapper class about all possible attributes and features that you
    can play with a model that has been loaded using `bitsandbytes`.
  prefs: []
  type: TYPE_NORMAL
- en: This replaces `load_in_8bit` or `load_in_4bit`therefore both options are mutually
    exclusive.
  prefs: []
  type: TYPE_NORMAL
- en: Currently only supports `LLM.int8()`, `FP4`, and `NF4` quantization. If more
    methods are added to `bitsandbytes`, then more arguments will be added to this
    class.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `is_quantizable`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L266)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Returns `True` if the model is quantizable, `False` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `post_init`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L235)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Safety checker that arguments are correct - also replaces some NoneType arguments
    with their default values.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `quantization_method`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L272)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This method returns the quantization method used for the model. If the model
    is not quantizable, it returns `None`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `to_diff_dict`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/quantization_config.py#L300)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict[str, Any]`'
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary of all the attributes that make up this configuration instance,
  prefs: []
  type: TYPE_NORMAL
- en: Removes all attributes from config which correspond to the default config attributes
    for better readability and serializes to a Python dictionary.
  prefs: []
  type: TYPE_NORMAL
