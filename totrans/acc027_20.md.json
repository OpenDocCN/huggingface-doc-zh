["```py\nfrom accelerate import Accelerator\nimport torch\n\naccelerator = Accelerator(project_dir=\"my/save/path\")\n\nmy_scheduler = torch.optim.lr_scheduler.StepLR(my_optimizer, step_size=1, gamma=0.99)\nmy_model, my_optimizer, my_training_dataloader = accelerator.prepare(my_model, my_optimizer, my_training_dataloader)\n\n# Register the LR scheduler\naccelerator.register_for_checkpointing(my_scheduler)\n\n# Save the starting state\naccelerator.save_state()\n\ndevice = accelerator.device\nmy_model.to(device)\n\n# Perform training\nfor epoch in range(num_epochs):\n    for batch in my_training_dataloader:\n        my_optimizer.zero_grad()\n        inputs, targets = batch\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        outputs = my_model(inputs)\n        loss = my_loss_function(outputs, targets)\n        accelerator.backward(loss)\n        my_optimizer.step()\n    my_scheduler.step()\n\n# Restore the previous state\naccelerator.load_state(\"my/save/path/checkpointing/checkpoint_0\")\n```", "```py\nfrom accelerate import Accelerator\n\naccelerator = Accelerator(project_dir=\"my/save/path\")\n\ntrain_dataloader = accelerator.prepare(train_dataloader)\naccelerator.load_state(\"my_state\")\n\n# Assume the checkpoint was saved 100 steps into the epoch\nskipped_dataloader = accelerator.skip_first_batches(train_dataloader, 100)\n\n# After the first iteration, go back to `train_dataloader`\n\n# First epoch\nfor batch in skipped_dataloader:\n    # Do something\n    pass\n\n# Second epoch\nfor batch in train_dataloader:\n    # Do something\n    pass\n```"]