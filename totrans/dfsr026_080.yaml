- en: xFormers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/diffusers/optimization/xformers](https://huggingface.co/docs/diffusers/optimization/xformers)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/131.147a544c.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Tip.230e2334.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/CodeBlock.57fe6e13.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
  prefs: []
  type: TYPE_NORMAL
- en: We recommend [xFormers](https://github.com/facebookresearch/xformers) for both
    inference and training. In our tests, the optimizations performed in the attention
    blocks allow for both faster speed and reduced memory consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install xFormers from `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The xFormers `pip` package requires the latest version of PyTorch. If you need
    to use a previous version of PyTorch, then we recommend [installing xFormers from
    the source](https://github.com/facebookresearch/xformers#installing-xformers).
  prefs: []
  type: TYPE_NORMAL
- en: After xFormers is installed, you can use `enable_xformers_memory_efficient_attention()`
    for faster inference and reduced memory consumption as shown in this [section](memory#memory-efficient-attention).
  prefs: []
  type: TYPE_NORMAL
- en: According to this [issue](https://github.com/huggingface/diffusers/issues/2234#issuecomment-1416931212),
    xFormers `v0.0.16` cannot be used for training (fine-tune or DreamBooth) in some
    GPUs. If you observe this problem, please install a development version as indicated
    in the issue comments.
  prefs: []
  type: TYPE_NORMAL
