- en: xFormers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/optimization/xformers](https://huggingface.co/docs/diffusers/optimization/xformers)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: We recommend [xFormers](https://github.com/facebookresearch/xformers) for both
    inference and training. In our tests, the optimizations performed in the attention
    blocks allow for both faster speed and reduced memory consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install xFormers from `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The xFormers `pip` package requires the latest version of PyTorch. If you need
    to use a previous version of PyTorch, then we recommend [installing xFormers from
    the source](https://github.com/facebookresearch/xformers#installing-xformers).
  prefs: []
  type: TYPE_NORMAL
- en: After xFormers is installed, you can use `enable_xformers_memory_efficient_attention()`
    for faster inference and reduced memory consumption as shown in this [section](memory#memory-efficient-attention).
  prefs: []
  type: TYPE_NORMAL
- en: According to this [issue](https://github.com/huggingface/diffusers/issues/2234#issuecomment-1416931212),
    xFormers `v0.0.16` cannot be used for training (fine-tune or DreamBooth) in some
    GPUs. If you observe this problem, please install a development version as indicated
    in the issue comments.
  prefs: []
  type: TYPE_NORMAL
