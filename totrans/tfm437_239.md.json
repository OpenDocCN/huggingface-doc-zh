["```py\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n>>> import pandas as pd\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/tapex-large-finetuned-wtq\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/tapex-large-finetuned-wtq\")\n\n>>> # prepare table + question\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n>>> table = pd.DataFrame.from_dict(data)\n>>> question = \"how many movies does Leonardo Di Caprio have?\"\n\n>>> encoding = tokenizer(table, question, return_tensors=\"pt\")\n\n>>> # let the model generate an answer autoregressively\n>>> outputs = model.generate(**encoding)\n\n>>> # decode back to text\n>>> predicted_answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n>>> print(predicted_answer)\n53\n```", "```py\n>>> # prepare table + question\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n>>> table = pd.DataFrame.from_dict(data)\n>>> questions = [\n...     \"how many movies does Leonardo Di Caprio have?\",\n...     \"which actor has 69 movies?\",\n...     \"what's the first name of the actor who has 87 movies?\",\n... ]\n>>> encoding = tokenizer(table, questions, padding=True, return_tensors=\"pt\")\n\n>>> # let the model generate an answer autoregressively\n>>> outputs = model.generate(**encoding)\n\n>>> # decode back to text\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n[' 53', ' george clooney', ' brad pitt']\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/tapex-large-finetuned-tabfact\")\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/tapex-large-finetuned-tabfact\")\n\n>>> # prepare table + sentence\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n>>> table = pd.DataFrame.from_dict(data)\n>>> sentence = \"George Clooney has 30 movies\"\n\n>>> encoding = tokenizer(table, sentence, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(**encoding)\n\n>>> # print prediction\n>>> predicted_class_idx = outputs.logits[0].argmax(dim=0).item()\n>>> print(model.config.id2label[predicted_class_idx])\nRefused\n```"]