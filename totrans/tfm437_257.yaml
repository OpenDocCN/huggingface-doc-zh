- en: ConvNeXt V2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ConvNeXt V2
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/convnextv2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/convnextv2)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/convnextv2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/convnextv2)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The ConvNeXt V2 model was proposed in [ConvNeXt V2: Co-designing and Scaling
    ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808) by Sanghyun
    Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining
    Xie. ConvNeXt V2 is a pure convolutional model (ConvNet), inspired by the design
    of Vision Transformers, and a successor of [ConvNeXT](convnext).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'ConvNeXt V2 模型是由 Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen,
    Zhuang Liu, In So Kweon, Saining Xie 在[ConvNeXt V2: Co-designing and Scaling ConvNets
    with Masked Autoencoders](https://arxiv.org/abs/2301.00808)中提出的。ConvNeXt V2 是一个纯卷积模型（ConvNet），受到
    Vision Transformers 设计的启发，是[ConvNeXT](convnext)的后继者。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的摘要如下：
- en: '*Driven by improved architectures and better representation learning frameworks,
    the field of visual recognition has enjoyed rapid modernization and performance
    boost in the early 2020s. For example, modern ConvNets, represented by ConvNeXt,
    have demonstrated strong performance in various scenarios. While these models
    were originally designed for supervised learning with ImageNet labels, they can
    also potentially benefit from self-supervised learning techniques such as masked
    autoencoders (MAE). However, we found that simply combining these two approaches
    leads to subpar performance. In this paper, we propose a fully convolutional masked
    autoencoder framework and a new Global Response Normalization (GRN) layer that
    can be added to the ConvNeXt architecture to enhance inter-channel feature competition.
    This co-design of self-supervised learning techniques and architectural improvement
    results in a new model family called ConvNeXt V2, which significantly improves
    the performance of pure ConvNets on various recognition benchmarks, including
    ImageNet classification, COCO detection, and ADE20K segmentation. We also provide
    pre-trained ConvNeXt V2 models of various sizes, ranging from an efficient 3.7M-parameter
    Atto model with 76.7% top-1 accuracy on ImageNet, to a 650M Huge model that achieves
    a state-of-the-art 88.9% accuracy using only public training data.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*受到改进的架构和更好的表示学习框架的驱动，视觉识别领域在 2020 年代初迅速现代化和性能提升。例如，现代 ConvNets，如 ConvNeXt，已经在各种场景中展示出强大的性能。虽然这些模型最初是为带有
    ImageNet 标签的监督学习而设计的，但它们也有可能从像遮罩自动编码器（MAE）这样的自监督学习技术中受益。然而，我们发现简单地将这两种方法结合起来会导致性能不佳。在本文中，我们提出了一个完全卷积的遮罩自动编码器框架和一个新的全局响应归一化（GRN）层，可以添加到
    ConvNeXt 架构中以增强通道间特征竞争。这种自监督学习技术和架构改进的共同设计导致了一个名为 ConvNeXt V2 的新模型系列，显著提高了纯 ConvNets
    在各种识别基准上的性能，包括 ImageNet 分类、COCO 检测和 ADE20K 分割。我们还提供了各种规模的预训练 ConvNeXt V2 模型，从一个高效的
    3.7M 参数 Atto 模型，在 ImageNet 上达到 76.7% 的 top-1 准确率，到一个 650M 的 Huge 模型，仅使用公共训练数据就实现了最先进的
    88.9% 准确率。*'
- en: '![drawing](../Images/25b30b91eaa46dd2f91e4a036533fc5f.png) ConvNeXt V2 architecture.
    Taken from the [original paper](https://arxiv.org/abs/2301.00808).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![drawing](../Images/25b30b91eaa46dd2f91e4a036533fc5f.png) ConvNeXt V2 架构。摘自[原始论文](https://arxiv.org/abs/2301.00808)。'
- en: This model was contributed by [adirik](https://huggingface.co/adirik). The original
    code can be found [here](https://github.com/facebookresearch/ConvNeXt-V2).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[adirik](https://huggingface.co/adirik)贡献的。原始代码可以在[这里](https://github.com/facebookresearch/ConvNeXt-V2)找到。
- en: Resources
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with ConvNeXt V2.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个官方的 Hugging Face 和社区（由 🌎 表示）资源列表，帮助您开始使用 ConvNeXt V2。
- en: Image Classification
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类
- en: '[ConvNextV2ForImageClassification](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ConvNextV2ForImageClassification](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification)
    受到这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)的支持。'
- en: If you’re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we’ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣提交资源以包含在这里，请随时打开一个 Pull Request，我们将进行审查！资源应该展示出一些新东西，而不是重复现有资源。
- en: ConvNextV2Config
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ConvNextV2Config
- en: '### `class transformers.ConvNextV2Config`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ConvNextV2Config`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/configuration_convnextv2.py#L30)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/configuration_convnextv2.py#L30)'
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`，*可选*，默认为 3) — 输入通道数。'
- en: '`patch_size` (`int`, optional, defaults to 4) — Patch size to use in the patch
    embedding layer.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`，可选，默认为 4) — 在补丁嵌入层中使用的补丁大小。'
- en: '`num_stages` (`int`, optional, defaults to 4) — The number of stages in the
    model.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_stages` (`int`，可选，默认为 4) — 模型中的阶段数。'
- en: '`hidden_sizes` (`List[int]`, *optional*, defaults to `[96, 192, 384, 768]`)
    — Dimensionality (hidden size) at each stage.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_sizes` (`List[int]`，*可选*，默认为 `[96, 192, 384, 768]`) — 每个阶段的维度（隐藏大小）。'
- en: '`depths` (`List[int]`, *optional*, defaults to `[3, 3, 9, 3]`) — Depth (number
    of blocks) for each stage.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depths` (`List[int]`，*可选*，默认为 `[3, 3, 9, 3]`) — 每个阶段的深度（块数）。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in each block. If string,
    `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` 或 `function`，*可选*，默认为 `"gelu"`) — 每个块中的非线性激活函数（函数或字符串）。如果是字符串，支持
    `"gelu"`、`"relu"`、`"selu"` 和 `"gelu_new"`。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, 默认为1e-12) — 层归一化层使用的epsilon。'
- en: '`drop_path_rate` (`float`, *optional*, defaults to 0.0) — The drop rate for
    stochastic depth.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_path_rate` (`float`, *optional*, 默认为0.0) — 随机深度的丢弃率。'
- en: '`out_features` (`List[str]`, *optional*) — If used as backbone, list of features
    to output. Can be any of `"stem"`, `"stage1"`, `"stage2"`, etc. (depending on
    how many stages the model has). If unset and `out_indices` is set, will default
    to the corresponding stages. If unset and `out_indices` is unset, will default
    to the last stage. Must be in the same order as defined in the `stage_names` attribute.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_features` (`List[str]`, *optional*) — 如果用作骨干，要输出的特征列表。可以是`"stem"`、`"stage1"`、`"stage2"`等（取决于模型有多少阶段）。如果未设置且设置了`out_indices`，将默认为相应的阶段。如果未设置且未设置`out_indices`，将默认为最后一个阶段。必须按照`stage_names`属性中定义的顺序。'
- en: '`out_indices` (`List[int]`, *optional*) — If used as backbone, list of indices
    of features to output. Can be any of 0, 1, 2, etc. (depending on how many stages
    the model has). If unset and `out_features` is set, will default to the corresponding
    stages. If unset and `out_features` is unset, will default to the last stage.
    Must be in the same order as defined in the `stage_names` attribute.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_indices` (`List[int]`, *optional*) — 如果用作骨干，要输出的特征索引列表。可以是0、1、2等（取决于模型有多少阶段）。如果未设置且设置了`out_features`，将默认为相应的阶段。如果未设置且未设置`out_features`，将默认为最后一个阶段。必须按照`stage_names`属性中定义的顺序。'
- en: This is the configuration class to store the configuration of a [ConvNextV2Model](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Model).
    It is used to instantiate an ConvNeXTV2 model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the ConvNeXTV2 [facebook/convnextv2-tiny-1k-224](https://huggingface.co/facebook/convnextv2-tiny-1k-224)
    architecture.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[ConvNextV2Model](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Model)配置的配置类。它用于根据指定的参数实例化ConvNeXTV2模型，定义模型架构。使用默认值实例化配置将产生类似于ConvNeXTV2
    [facebook/convnextv2-tiny-1k-224](https://huggingface.co/facebook/convnextv2-tiny-1k-224)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ConvNextV2Model
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ConvNextV2Model
- en: '### `class transformers.ConvNextV2Model`'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ConvNextV2Model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_convnextv2.py#L344)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_convnextv2.py#L344)'
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ConvNextV2Config](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ConvNextV2Config](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Config))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare ConvNextV2 model outputting raw features without any specific head
    on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的ConvNextV2模型输出原始特征，没有任何特定的头部。这个模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。
- en: '#### `forward`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_convnextv2.py#L363)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_convnextv2.py#L363)'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [ConvNextImageProcessor](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextImageProcessor).
    See [ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — 像素值。像素值可以使用[ConvNextImageProcessor](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextImageProcessor)获取。有关详细信息，请参阅[ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention` or
    `tuple(torch.FloatTensor)`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention` 或
    `tuple(torch.FloatTensor)`'
- en: A `transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention` or
    a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ConvNextV2Config](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Config))
    and inputs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[ConvNextV2Config](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Config)）和输入的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—
    模型最后一层的隐藏状态的序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state after a pooling operation on the spatial dimensions.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）— 空间维度上池化操作后的最后一层隐藏状态。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, num_channels, height,
    width)`.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出和每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出处的隐藏状态加上可选的初始嵌入输出。
- en: The [ConvNextV2Model](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[ConvNextV2Model](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Model)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ConvNextV2ForImageClassification
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ConvNextV2ForImageClassification
- en: '### `class transformers.ConvNextV2ForImageClassification`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ConvNextV2ForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_convnextv2.py#L408)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_convnextv2.py#L408)'
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ConvNextV2Config](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ConvNextV2Config](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Config)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: ConvNextV2 Model with an image classification head on top (a linear layer on
    top of the pooled features), e.g. for ImageNet.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ConvNextV2模型在顶部具有图像分类头部（在池化特征之上的线性层），例如用于ImageNet。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_convnextv2.py#L431)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_convnextv2.py#L431)'
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [ConvNextImageProcessor](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextImageProcessor).
    See [ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—
    像素值。像素值可以使用[ConvNextImageProcessor](/docs/transformers/v4.37.2/en/model_doc/convnext#transformers.ConvNextImageProcessor)获取。有关详细信息，请参阅[ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算图像分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or `tuple(torch.FloatTensor)`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ConvNextV2Config](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Config))
    and inputs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（[ConvNextV2Config](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Config)）和输入。'
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *可选*，当提供`labels`时返回) — 分类（如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each stage) of shape `(batch_size, num_channels, height,
    width)`. Hidden-states (also called feature maps) of the model at the output of
    each stage.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每个阶段的输出）。模型在每个阶段输出的隐藏状态（也称为特征图）。'
- en: The [ConvNextV2ForImageClassification](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[ConvNextV2ForImageClassification](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: TFConvNextV2Model
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFConvNextV2Model
- en: '### `class transformers.TFConvNextV2Model`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFConvNextV2Model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L544)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L544)'
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ConvNextV2Config](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ConvNextV2Config](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Config)）
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare ConvNextV2 model outputting raw features without any specific head
    on top. This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的ConvNextV2模型输出原始特征，没有特定的头部。此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以了解与一般使用和行为相关的所有事项。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典在第一个位置参数中。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于这种支持，当使用`model.fit()`等方法时，应该可以“正常工作”
    - 只需传递您支持的任何格式的输入和标签给`model.fit()`！但是，如果您想在Keras方法之外使用第二种格式，比如在使用Keras`Functional`API创建自己的层或模型时，有三种可能性可以用来收集所有输入张量在第一个位置参数中：
- en: 'a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有`pixel_values`的单个张量，没有其他内容：`model(pixel_values)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([pixel_values, attention_mask])` or `model([pixel_values,
    attention_mask, token_type_ids])`'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度可变的列表，其中包含一个或多个输入张量，按照文档字符串中给定的顺序：`model([pixel_values, attention_mask])`或`model([pixel_values,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"pixel_values": pixel_values, "token_type_ids":
    token_type_ids})`'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个包含与文档字符串中给定的输入名称相关联的一个或多个输入张量的字典：`model({"pixel_values": pixel_values, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您不需要担心这些问题，因为您可以像对待任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L553)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L553)'
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]`, `Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) — Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str,
    np.ndarray]`，每个示例必须具有形状`(batch_size, num_channels, height, width)`） — 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*） — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to `True`.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。这个参数可以在急切模式下使用，在图模式下，该值将始终设置为`True`。'
- en: Returns
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention`
    or `tuple(tf.Tensor)`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention`或`tuple(tf.Tensor)`'
- en: A `transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ConvNextV2Config](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Config))
    and inputs.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention`或一个`tf.Tensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含各种元素，这取决于配置（[ConvNextV2Config](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Config)）和输入。
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, num_channels, height,
    width)`) — Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, num_channels, height, width)`的`tf.Tensor`）
    — 模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`tf.Tensor` of shape `(batch_size, hidden_size)`) — Last layer
    hidden-state after a pooling operation on the spatial dimensions.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`tf.Tensor`，形状为`(batch_size, hidden_size)`) — 在空间维度上进行池化操作后的最后一层隐藏状态。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings, if the model has an embedding layer, + one for
    the output of each layer) of shape `(batch_size, num_channels, height, width)`.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, num_channels, height, width)`的`tf.Tensor`元组（如果模型有嵌入层，则为嵌入的输出+每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出处的隐藏状态以及可选的初始嵌入输出。
- en: The [TFConvNextV2Model](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.TFConvNextV2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFConvNextV2Model](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.TFConvNextV2Model)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE10]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: TFConvNextV2ForImageClassification
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFConvNextV2ForImageClassification
- en: '### `class transformers.TFConvNextV2ForImageClassification`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFConvNextV2ForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L602)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L602)'
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ConvNextV2Config](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ConvNextV2Config](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Config)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: ConvNextV2 Model with an image classification head on top (a linear layer on
    top of the pooled features), e.g. for ImageNet.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ConvNextV2模型，顶部带有图像分类头（在池化特征的顶部是一个线性层），例如用于ImageNet。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)的子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有相关信息。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典放在第一个位置参数中。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是，当将输入传递给模型和层时，Keras方法更喜欢这种格式。由于这种支持，当使用`model.fit()`等方法时，应该可以正常工作
    - 只需以`model.fit()`支持的任何格式传递输入和标签即可！但是，如果要在Keras方法之外使用第二种格式，例如在使用Keras`Functional`API创建自己的层或模型时，有三种可能性可用于在第一个位置参数中收集所有输入张量：
- en: 'a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一个张量，其中仅包含`pixel_values`，没有其他内容：`model(pixel_values)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([pixel_values, attention_mask])` or `model([pixel_values,
    attention_mask, token_type_ids])`'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度不定的列表，其中包含一个或多个输入张量，按照文档字符串中给定的顺序：`model([pixel_values, attention_mask])`或`model([pixel_values,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"pixel_values": pixel_values, "token_type_ids":
    token_type_ids})`'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"pixel_values": pixel_values,
    "token_type_ids": token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像对待任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L624)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L624)'
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]`, `Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) — Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str,
    np.ndarray]`，每个示例的形状必须为`(batch_size, num_channels, height, width)`）— 像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参见[ConvNextImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。此参数仅在急切模式下可用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to `True`.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为`True`。'
- en: '`labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*)
    — Labels for computing the image classification/regression loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression
    loss is computed (Mean-Square loss), If `config.num_labels > 1` a classification
    loss is computed (Cross-Entropy).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`tf.Tensor`或`np.ndarray`，形状为`(batch_size,)`，*可选*) — 用于计算图像分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention` or
    `tuple(tf.Tensor)`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention` 或
    `tuple(tf.Tensor)`'
- en: A `transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([ConvNextV2Config](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Config))
    and inputs.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`或一个`tf.Tensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（[ConvNextV2Config](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.ConvNextV2Config)）和输入。
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`tf.Tensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 分类（如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`tf.Tensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings, if the model has an embedding layer, + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the model at the output of each stage.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, num_channels, height, width)`的`tf.Tensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+
    一个用于每个阶段的输出）。模型在每个阶段输出的隐藏状态（也称为特征图）。'
- en: The [TFConvNextV2ForImageClassification](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.TFConvNextV2ForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFConvNextV2ForImageClassification](/docs/transformers/v4.37.2/en/model_doc/convnextv2#transformers.TFConvNextV2ForImageClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE13]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
