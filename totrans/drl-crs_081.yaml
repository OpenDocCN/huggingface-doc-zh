- en: Advantage Actor-Critic (A2C)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit6/advantage-actor-critic](https://huggingface.co/learn/deep-rl-course/unit6/advantage-actor-critic)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Reducing variance with Actor-Critic methods
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The solution to reducing the variance of the Reinforce algorithm and training
    our agent faster and better is to use a combination of Policy-Based and Value-Based
    methods: *the Actor-Critic method*.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: To understand the Actor-Critic, imagine you’re playing a video game. You can
    play with a friend that will provide you with some feedback. You’re the Actor
    and your friend is the Critic.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![Actor Critic](../Images/042bd39f9e4b369ec6cfbc9ae67d3829.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: You don’t know how to play at the beginning, **so you try some actions randomly**.
    The Critic observes your action and **provides feedback**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Learning from this feedback, **you’ll update your policy and be better at playing
    that game.**
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, your friend (Critic) will also update their way to provide
    feedback so it can be better next time.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the idea behind Actor-Critic. We learn two function approximations:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '*A policy* that **controls how our agent acts**:<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">\pi_{\theta}(s)</annotation></semantics></math> πθ​(s)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A value function* to assist the policy update by measuring how good the action
    taken is:<math><semantics><mrow><msub><mover accent="true"><mi>q</mi><mo>^</mo></mover><mi>w</mi></msub><mo
    stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow>
    <annotation encoding="application/x-tex">\hat{q}_{w}(s,a)</annotation></semantics></math>
    q^​w​(s,a)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Actor-Critic Process
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have seen the Actor Critic’s big picture, let’s dive deeper to understand
    how the Actor and Critic improve together during the training.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw, with Actor-Critic methods, there are two function approximations
    (two neural networks):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '*Actor*, a **policy function** parameterized by theta:<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">\pi_{\theta}(s)</annotation></semantics></math> πθ​(s)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Critic*, a **value function** parameterized by w:<math><semantics><mrow><msub><mover
    accent="true"><mi>q</mi><mo>^</mo></mover><mi>w</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo
    separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">\hat{q}_{w}(s,a)</annotation></semantics></math>
    q^​w​(s,a)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see the training process to understand how the Actor and Critic are optimized:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: At each timestep, t, we get the current state<math><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub></mrow>
    <annotation encoding="application/x-tex">S_t</annotation></semantics></math>St​
    from the environment and **pass it as input through our Actor and Critic**.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our Policy takes the state and **outputs an action** <math><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub></mrow>
    <annotation encoding="application/x-tex">A_t</annotation></semantics></math> At​.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Step 1 Actor Critic](../Images/5633968bb872e917c968f590412b0f7b.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: 'The Critic takes that action also as input and, using<math><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub></mrow>
    <annotation encoding="application/x-tex">S_t</annotation></semantics></math>St​
    and<math><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub></mrow> <annotation
    encoding="application/x-tex">A_t</annotation></semantics></math> At​, **computes
    the value of taking that action at that state: the Q-value**.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Step 2 Actor Critic](../Images/2240186d96dc80dba4057389b4c239ab.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: The action<math><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub></mrow> <annotation
    encoding="application/x-tex">A_t</annotation></semantics></math>At​ performed
    in the environment outputs a new state<math><semantics><mrow><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow>
    <annotation encoding="application/x-tex">S_{t+1}</annotation></semantics></math>St+1​
    and a reward<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow>
    <annotation encoding="application/x-tex">R_{t+1}</annotation></semantics></math>
    Rt+1​ .
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在环境中执行的动作<math><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub></mrow> <annotation
    encoding="application/x-tex">A_t</annotation></semantics></math>在t+1时刻产生一个新状态<math><semantics><mrow><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow>
    <annotation encoding="application/x-tex">S_{t+1}</annotation></semantics></math>和一个奖励<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow>
    <annotation encoding="application/x-tex">R_{t+1}</annotation></semantics></math>。
- en: '![Step 3 Actor Critic](../Images/aca327aae99a5abb4493fc07abd2dd69.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![步骤3 Actor Critic](../Images/aca327aae99a5abb4493fc07abd2dd69.png)'
- en: The Actor updates its policy parameters using the Q value.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演员使用Q值更新其策略参数。
- en: '![Step 4 Actor Critic](../Images/cf429723f7f14d366206f1e0c83f52ed.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![步骤4 Actor Critic](../Images/cf429723f7f14d366206f1e0c83f52ed.png)'
- en: Thanks to its updated parameters, the Actor produces the next action to take
    at<math><semantics><mrow><msub><mi>A</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow>
    <annotation encoding="application/x-tex">A_{t+1}</annotation></semantics></math>
    At+1​ given the new state<math><semantics><mrow><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow>
    <annotation encoding="application/x-tex">S_{t+1}</annotation></semantics></math>
    St+1​.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于更新的参数，演员产生下一个要在<math><semantics><mrow><msub><mi>A</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow>
    <annotation encoding="application/x-tex">A_{t+1}</annotation></semantics></math>处采取的动作，给定新状态<math><semantics><mrow><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow>
    <annotation encoding="application/x-tex">S_{t+1}</annotation></semantics></math>。
- en: The Critic then updates its value parameters.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后评论家更新其价值参数。
- en: '![Step 5 Actor Critic](../Images/ec927ac9ae5bbb27566b494db46568aa.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![步骤5 Actor Critic](../Images/ec927ac9ae5bbb27566b494db46568aa.png)'
- en: Adding Advantage in Actor-Critic (A2C)
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Actor-Critic（A2C）中添加优势
- en: We can stabilize learning further by **using the Advantage function as Critic
    instead of the Action value function**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过**使用优势函数作为评论家而不是动作值函数**来进一步稳定学习。
- en: 'The idea is that the Advantage function calculates the relative advantage of
    an action compared to the others possible at a state: **how taking that action
    at a state is better compared to the average value of the state**. It’s subtracting
    the mean value of the state from the state action pair:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是，优势函数计算动作相对于在状态可能的其他动作的优势：**在该状态采取该动作相对于该状态的平均值更好的程度**。它从状态动作对中减去了状态的平均值：
- en: '![Advantage Function](../Images/d178c6579c3c840bd90abfd5839b83af.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![优势函数](../Images/d178c6579c3c840bd90abfd5839b83af.png)'
- en: In other words, this function calculates **the extra reward we get if we take
    this action at that state compared to the mean reward we get at that state**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，该函数计算**如果我们在该状态采取此动作与我们在该状态获得的平均奖励相比，我们获得的额外奖励**。
- en: The extra reward is what’s beyond the expected value of that state.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的奖励是超出该状态预期值的部分。
- en: 'If A(s,a) > 0: our gradient is **pushed in that direction**.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果A（s，a）>0：我们的梯度会**被推向那个方向**。
- en: If A(s,a) < 0 (our action does worse than the average value of that state),
    **our gradient is pushed in the opposite direction**.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果A（s，a）<0（我们的动作比该状态的平均值表现更差），**我们的梯度会被推向相反的方向**。
- en: The problem with implementing this advantage function is that it requires two
    value functions — <math><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo
    separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">Q(s,a)</annotation></semantics></math>Q(s,a) and
    <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow>
    <annotation encoding="application/x-tex">V(s)</annotation></semantics></math>V(s).
    Fortunately, **we can use the TD error as a good estimator of the advantage function.**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这个优势函数的问题在于它需要两个价值函数 — <math><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo
    separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">Q(s,a)</annotation></semantics></math>和 <math><semantics><mrow><mi>V</mi><mo
    stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">V(s)</annotation></semantics></math>。幸运的是，**我们可以使用TD误差作为优势函数的良好估计器**。
- en: '![Advantage Function](../Images/f524503c61319621c198656df4b56427.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![优势函数](../Images/f524503c61319621c198656df4b56427.png)'
