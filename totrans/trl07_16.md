# DPO è®­ç»ƒå¸ˆ

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/trl/dpo_trainer`](https://huggingface.co/docs/trl/dpo_trainer)

TRL æ”¯æŒ DPO è®­ç»ƒå¸ˆï¼Œç”¨äºä»åå¥½æ•°æ®ä¸­è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå¦‚ Rafailov ç­‰äººåœ¨ 2023 å¹´çš„è®ºæ–‡[ç›´æ¥åå¥½ä¼˜åŒ–ï¼šæ‚¨çš„è¯­è¨€æ¨¡å‹æš—ä¸­æ˜¯ä¸€ä¸ªå¥–åŠ±æ¨¡å‹](https://arxiv.org/abs/2305.18290)ä¸­æ‰€è¿°ã€‚æœ‰å…³å®Œæ•´ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹[`examples/scripts/dpo.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py)ã€‚

é¦–å…ˆå§‹ç»ˆæ˜¯è®­ç»ƒæ‚¨çš„ SFT æ¨¡å‹ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬è®­ç»ƒçš„æ•°æ®å¯¹äº DPO ç®—æ³•æ˜¯åˆ†å¸ƒå†…çš„ã€‚

## æœŸæœ›çš„æ•°æ®é›†æ ¼å¼

DPO è®­ç»ƒå¸ˆæœŸæœ›æ•°æ®é›†å…·æœ‰éå¸¸ç‰¹å®šçš„æ ¼å¼ã€‚ç”±äºæ¨¡å‹å°†è¢«è®­ç»ƒç›´æ¥ä¼˜åŒ–å“ªä¸ªå¥å­åœ¨ç»™å®šä¸¤ä¸ªå¥å­çš„æƒ…å†µä¸‹æœ€ç›¸å…³çš„åå¥½ï¼Œæˆ‘ä»¬åœ¨ä¸‹é¢æä¾›äº†ä¸€ä¸ªæ¥è‡ª[`Anthropic/hh-rlhf`](https://huggingface.co/datasets/Anthropic/hh-rlhf)æ•°æ®é›†çš„ç¤ºä¾‹ï¼š

![](img/108b7079a2ffc651b11289080d0cdc7d.png)

å› æ­¤ï¼Œå¦‚æœä½¿ç”¨é»˜è®¤çš„`DPODataCollatorWithPadding`æ•°æ®æ•´ç†å™¨ï¼Œåˆ™æœ€ç»ˆæ•°æ®é›†å¯¹è±¡åº”åŒ…å«è¿™ 3 ä¸ªæ¡ç›®ã€‚è¿™äº›æ¡ç›®åº”å‘½åä¸ºï¼š

+   `prompt`

+   `chosen`

+   `rejected`

ä¾‹å¦‚ï¼š

```py
dpo_dataset_dict = {
    "prompt": [
        "hello",
        "how are you",
        "What is your name?",
        "What is your name?",
        "Which is the best programming language?",
        "Which is the best programming language?",
        "Which is the best programming language?",
    ],
    "chosen": [
        "hi nice to meet you",
        "I am fine",
        "My name is Mary",
        "My name is Mary",
        "Python",
        "Python",
        "Java",
    ],
    "rejected": [
        "leave me alone",
        "I am not fine",
        "Whats it to you?",
        "I dont have a name",
        "Javascript",
        "C++",
        "C++",
    ],
}
```

å…¶ä¸­`prompt`åŒ…å«ä¸Šä¸‹æ–‡è¾“å…¥ï¼Œ`chosen`åŒ…å«ç›¸åº”çš„é€‰æ‹©å“åº”ï¼Œ`rejected`åŒ…å«ç›¸åº”çš„è´Ÿé¢ï¼ˆè¢«æ‹’ç»çš„ï¼‰å“åº”ã€‚å¯ä»¥çœ‹åˆ°ä¸€ä¸ªæç¤ºå¯ä»¥æœ‰å¤šä¸ªå“åº”ï¼Œè¿™åœ¨å­—å…¸å€¼æ•°ç»„ä¸­çš„æ¡ç›®é‡å¤ä¸­å¾—åˆ°ä½“ç°ã€‚

## æœŸæœ›çš„æ¨¡å‹æ ¼å¼

DPO è®­ç»ƒå¸ˆæœŸæœ›ä¸€ä¸ª`AutoModelForCausalLM`æ¨¡å‹ï¼Œè€Œ PPO åˆ™æœŸæœ›`AutoModelForCausalLMWithValueHead`ç”¨äºå€¼å‡½æ•°ã€‚

## ä½¿ç”¨ DPOTrainer

æœ‰å…³è¯¦ç»†ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹`examples/scripts/dpo.py`è„šæœ¬ã€‚åœ¨é«˜å±‚æ¬¡ä¸Šï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨è¦è®­ç»ƒçš„`model`åˆå§‹åŒ–`DPOTrainer`ï¼Œä½¿ç”¨`ref_model`æ¥è®¡ç®—é¦–é€‰å’Œè¢«æ‹’ç»å“åº”çš„éšå¼å¥–åŠ±ï¼Œ`beta`æ˜¯éšå¼å¥–åŠ±çš„è¶…å‚æ•°ï¼Œæ•°æ®é›†åŒ…å«ä¸Šè¿° 3 ä¸ªæ¡ç›®ã€‚è¯·æ³¨æ„ï¼Œ`model`å’Œ`ref_model`éœ€è¦å…·æœ‰ç›¸åŒçš„æ¶æ„ï¼ˆå³ä»…è§£ç å™¨æˆ–ç¼–ç å™¨-è§£ç å™¨ï¼‰ã€‚

```py
 dpo_trainer = DPOTrainer(
    model,
    model_ref,
    args=training_args,
    beta=0.1,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
)
```

ä¹‹åå¯ä»¥è°ƒç”¨ï¼š

```py
dpo_trainer.train()
```

è¯·æ³¨æ„ï¼Œ`beta`æ˜¯ DPO æŸå¤±çš„æ¸©åº¦å‚æ•°ï¼Œé€šå¸¸åœ¨`0.1`åˆ°`0.5`çš„èŒƒå›´å†…ã€‚æˆ‘ä»¬å¿½ç•¥å‚è€ƒæ¨¡å‹ï¼Œå› ä¸º`beta` -> 0ã€‚

## æŸå¤±å‡½æ•°

æ ¹æ®åå¥½æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ® Bradley-Terry æ¨¡å‹æ‹ŸåˆäºŒå…ƒåˆ†ç±»å™¨ï¼Œäº‹å®ä¸Šï¼ŒDPO ä½œè€…æå‡ºä½¿ç”¨é€šè¿‡`logsigmoid`å¯¹å½’ä¸€åŒ–ä¼¼ç„¶è¿›è¡Œ sigmoid æŸå¤±æ‹Ÿåˆé€»è¾‘å›å½’ã€‚

[RSO](https://arxiv.org/abs/2309.06657)çš„ä½œè€…å»ºè®®ä½¿ç”¨æ¥è‡ª[SLiC](https://arxiv.org/abs/2305.10425)è®ºæ–‡çš„å½’ä¸€åŒ–ä¼¼ç„¶çš„é“°é“¾æŸå¤±ã€‚`DPOTrainer`å¯ä»¥é€šè¿‡`loss_type="hinge"`å‚æ•°åˆ‡æ¢åˆ°æ­¤æŸå¤±ï¼Œæ­¤æ—¶`beta`æ˜¯è¾¹é™…çš„å€’æ•°ã€‚

[IPO](https://arxiv.org/abs/2310.12036)çš„ä½œè€…æä¾›äº†å¯¹ DPO ç®—æ³•çš„æ›´æ·±å…¥çš„ç†è®ºç†è§£ï¼Œå¹¶ç¡®å®šäº†è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ›¿ä»£æŸå¤±ï¼Œå¯ä»¥é€šè¿‡`loss_type="ipo"`å‚æ•°ä¼ é€’ç»™è®­ç»ƒå¸ˆã€‚

[cDPO](https://ericmitchell.ai/cdpo.pdf)æ˜¯å¯¹ DPO æŸå¤±çš„è°ƒæ•´ï¼Œå…¶ä¸­æˆ‘ä»¬å‡è®¾åå¥½æ ‡ç­¾å­˜åœ¨ä¸€å®šæ¦‚ç‡çš„å™ªå£°ï¼Œå¯ä»¥é€šè¿‡`label_smoothing`å‚æ•°ï¼ˆä»‹äº 0 å’Œ 0.5 ä¹‹é—´ï¼‰ä¼ é€’ç»™`DPOTrainer`ï¼Œç„¶åä½¿ç”¨ä¿å®ˆçš„ DPO æŸå¤±ã€‚ä½¿ç”¨`loss_type="cdpo"`å‚æ•°æ¥ä½¿ç”¨å®ƒã€‚

[KTO](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf)æŸå¤±è¢«æ¨å¯¼å‡ºæ¥ï¼Œç›´æ¥æœ€å¤§åŒ– LLM ç”Ÿæˆçš„æ•ˆç”¨ï¼Œè€Œä¸æ˜¯åå¥½çš„å¯¹æ•°ä¼¼ç„¶ã€‚å› æ­¤ï¼Œæ•°æ®é›†ä¸ä¸€å®šæ˜¯åå¥½ï¼Œè€Œæ˜¯ç†æƒ³ä¸ä¸ç†æƒ³çš„å®Œæˆã€‚å¯¹äº`DPOTrainer`æ‰€éœ€çš„æˆå¯¹åå¥½æ•°æ®ï¼Œè¯·ä½¿ç”¨`loss_type="kto_pair"`å‚æ•°æ¥åˆ©ç”¨è¿™ç§æŸå¤±ï¼Œè€Œå¯¹äºæœŸæœ›å’Œä¸æœŸæœ›æ•°æ®çš„æ›´ä¸€èˆ¬æƒ…å†µï¼Œè¯·ä½¿ç”¨å°šæœªå®ç°çš„`KTOTrainer`ã€‚

## æ—¥å¿—è®°å½•

åœ¨è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è®°å½•ä»¥ä¸‹å¥–åŠ±æŒ‡æ ‡ï¼š

+   `rewards/chosen`ï¼šç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹å¯¹äºæ‰€é€‰å“åº”çš„å¯¹æ•°æ¦‚ç‡ä¹‹é—´çš„å¹³å‡å·®å¼‚ï¼ŒæŒ‰ beta ç¼©æ”¾

+   `rewards/rejected`ï¼šç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹å¯¹äºè¢«æ‹’ç»å“åº”çš„å¯¹æ•°æ¦‚ç‡ä¹‹é—´çš„å¹³å‡å·®å¼‚ï¼ŒæŒ‰ beta ç¼©æ”¾

+   `rewards/accuracies`ï¼šæ‰€é€‰å¥–åŠ±æ¯”ç›¸åº”è¢«æ‹’ç»å¥–åŠ±é«˜çš„å¹³å‡å€¼

+   `rewards/margins`ï¼šæ‰€é€‰å¥–åŠ±å’Œç›¸åº”è¢«æ‹’ç»å¥–åŠ±ä¹‹é—´çš„å¹³å‡å·®å¼‚

## ä½¿ç”¨ unsloth åŠ é€Ÿ DPO å¾®è°ƒ

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ä¸`SFTTrainer`å®Œå…¨å…¼å®¹çš„[`unsloth`](https://github.com/unslothai/unsloth)åº“æ¥è¿›ä¸€æ­¥åŠ é€Ÿ QLoRA / LoRAï¼ˆé€Ÿåº¦æé«˜ 2 å€ï¼Œå†…å­˜å‡å°‘ 60%ï¼‰ã€‚ç›®å‰ï¼Œ`unsloth`ä»…æ”¯æŒ Llamaï¼ˆYiï¼ŒTinyLlamaï¼ŒQwenï¼ŒDeepseek ç­‰ï¼‰å’Œ Mistral æ¶æ„ã€‚ä»¥ä¸‹æ˜¯ DPO çš„ä¸€äº›åŸºå‡†æµ‹è¯•ï¼š

| GPU | Model | Dataset | ğŸ¤— | ğŸ¤— + Flash Attention 2 | ğŸ¦¥ Unsloth | ğŸ¦¥ VRAM saved |
| --- | --- | --- | --- | --- | --- | --- |
| A100 40G | Zephyr 7b | Ultra Chat | 1x | 1.24x | **1.88x** | -11.6% |
| Tesla T4 | Zephyr 7b | Ultra Chat | 1x | 1.09x | **1.55x** | -18.6% |

æ ¹æ®[å®˜æ–¹æ–‡æ¡£](https://github.com/unslothai/unsloth)å®‰è£…`unsloth`ã€‚å®‰è£…å®Œæˆåï¼Œæ‚¨å¯ä»¥ä»¥éå¸¸ç®€å•çš„æ–¹å¼å°† unsloth æ•´åˆåˆ°æ‚¨çš„å·¥ä½œæµç¨‹ä¸­ï¼›æ‚¨åªéœ€è¦åŠ è½½`FastLanguageModel`ï¼Œè€Œä¸æ˜¯åŠ è½½`AutoModelForCausalLM`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
import torch
from transformers import TrainingArguments
from trl import DPOTrainer
from unsloth import FastLanguageModel

max_seq_length = 2048 # Supports automatic RoPE Scaling, so choose any number.

# Load model
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/zephyr-sft",
    max_seq_length = max_seq_length,
    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
    load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False.
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Dropout = 0 is currently optimized
    bias = "none",    # Bias = "none" is currently optimized
    use_gradient_checkpointing = True,
    random_state = 3407,
)

args = TrainingArguments(output_dir="./output")

dpo_trainer = DPOTrainer(
    model,
    model_ref=None,
    args=training_args,
    beta=0.1,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
)
dpo_trainer.train()
```

ä¿å­˜çš„æ¨¡å‹ä¸ Hugging Face çš„ transformers åº“å®Œå…¨å…¼å®¹ã€‚åœ¨ä»–ä»¬çš„[å®˜æ–¹å­˜å‚¨åº“](https://github.com/unslothai/unsloth)ä¸­äº†è§£æ›´å¤šå…³äº unsloth çš„ä¿¡æ¯ã€‚

## å‚è€ƒæ¨¡å‹åœ¨ PEFT ä¸­çš„è€ƒè™‘

åœ¨ä½¿ç”¨ PEFT æ—¶ï¼Œæ‚¨æœ‰ä¸‰ä¸ªä¸»è¦é€‰é¡¹ï¼ˆä»¥åŠå‡ ä¸ªå˜ä½“ï¼‰æ¥ç¡®å®šå‚è€ƒæ¨¡å‹çš„å·¥ä½œæ–¹å¼ï¼Œå‡è®¾æ‚¨å¸Œæœ›ä½¿ç”¨ DPO è¿›ä¸€æ­¥å¢å¼ºçš„æ¨¡å‹æ˜¯ä½¿ç”¨ï¼ˆQï¼‰LoRA è¿›è¡Œè°ƒæ•´çš„ã€‚

1.  ç®€å•åœ°åˆ›å»ºä¸¤ä¸ªæ¨¡å‹å®ä¾‹ï¼Œæ¯ä¸ªåŠ è½½æ‚¨çš„é€‚é…å™¨ - è¿è¡Œè‰¯å¥½ä½†æ•ˆç‡å¾ˆä½ã€‚

1.  å°†é€‚é…å™¨åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼Œé¡¶éƒ¨åˆ›å»ºå¦ä¸€ä¸ªé€‚é…å™¨ï¼Œç„¶åå°†`model_ref`å‚æ•°ä¿ç•™ä¸ºç©ºï¼Œè¿™æ · DPOTrainer å°†å¸è½½ç”¨äºå‚è€ƒæ¨ç†çš„é€‚é…å™¨ - é«˜æ•ˆï¼Œä½†å­˜åœ¨ä¸‹é¢è®¨è®ºçš„æ½œåœ¨ç¼ºç‚¹ã€‚

1.  ä¸¤æ¬¡åŠ è½½é€‚é…å™¨ï¼Œä½¿ç”¨ä¸åŒçš„åç§°ï¼Œç„¶ååœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨`set_adapter`æ¥åœ¨ DPO é€‚é…å™¨å’Œå‚è€ƒé€‚é…å™¨ä¹‹é—´è¿›è¡Œåˆ‡æ¢ - ä¸ 2 ç›¸æ¯”ç¨å¾®ä¸é‚£ä¹ˆé«˜æ•ˆï¼ˆé€‚é…å™¨å¤§å° VRAM å¼€é”€ï¼‰ï¼Œä½†é¿å…äº†ç¼ºç‚¹ã€‚

### åœ¨ DPO ä¹‹å‰åˆå¹¶ QLoRA çš„ç¼ºç‚¹ï¼ˆæ–¹æ³• 2ï¼‰

å¦‚[Tim Dettmers](https://twitter.com/Tim_Dettmers/status/1694654191325573456)å»ºè®®çš„ï¼Œåˆå¹¶ QLoRA é€‚é…å™¨çš„æœ€ä½³æ–¹æ³•æ˜¯é¦–å…ˆå¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œç„¶ååˆå¹¶é€‚é…å™¨ï¼Œç„¶åè½¬æ¢å› bf16ã€‚ç±»ä¼¼äº[æ­¤è„šæœ¬](https://github.com/jondurbin/qlora/blob/main/qmerge.py)

æ‚¨ä¹Ÿå¯ä»¥åªæ˜¯ä»¥æ ‡å‡†æ–¹å¼åˆå¹¶é€‚é…å™¨ï¼Œè€Œä¸å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œä½†è¿™æ ·ä¼šå¯¼è‡´æ€§èƒ½é™ä½ 1-2%ï¼ˆæ˜¾ç„¶ï¼Œè¿˜ä¼šå‡ºç°æ›´å¤šç©ºå“åº”çš„é—®é¢˜ï¼‰ã€‚

å¦‚æœæ‚¨ä½¿ç”¨æ¨èçš„æ–¹æ³•ï¼Œå³å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œé‚£ä¹ˆç°åœ¨æ‚¨å¤„äºè¿™æ ·ä¸€ç§æƒ…å†µï¼šè¦ä½¿ç”¨ QLoRA è¿›è¡Œ DPOï¼Œæ‚¨å°†éœ€è¦å†æ¬¡å¯¹åˆå¹¶çš„æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œæˆ–è€…ä½¿ç”¨æ€§èƒ½è¾ƒä½çš„æœªé‡åŒ–åˆå¹¶ã€‚

### ä½¿ç”¨é€‰é¡¹ 3 - ä¸¤æ¬¡åŠ è½½é€‚é…å™¨

ä¸ºäº†é¿å…é€‰é¡¹ 2 çš„ç¼ºç‚¹ï¼Œå¯ä»¥å°†å¾®è°ƒçš„é€‚é…å™¨åŠ è½½åˆ°æ¨¡å‹ä¸­ä¸¤æ¬¡ï¼Œä½¿ç”¨ä¸åŒçš„åç§°ï¼Œå¹¶åœ¨ DPOTrainer ä¸­è®¾ç½®æ¨¡å‹/å‚è€ƒé€‚é…å™¨çš„åç§°ï¼Œä»¥ç•¥å¾®å¢åŠ  VRAMã€‚

ä¾‹å¦‚ï¼š

```py
# Load the base model.
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/mixtral-8x7b-v0.1",
    load_in_4bit=True,
    quantization_config=bnb_config,
    attn_implementation="flash_attention_2",
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
model.config.use_cache = False

# Load the adapter.
model = PeftModel.from_pretrained(
    model,
    "/path/to/peft",
    is_trainable=True,
    adapter_name="train",
)
# Load the adapter a second time, with a different name, which will be our reference model.
model.load_adapter("/path/to/peft", adapter_name="reference")

# Initialize the trainer, without a ref_model param.
dpo_trainer = DPOTrainer(
    model,
    ...
    model_adapter_name="train",
    ref_adapter_name="reference",
)
```

## DPOTrainer

### `class trl.DPOTrainer`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L64)

```py
( model: Union = None ref_model: Union = None beta: float = 0.1 label_smoothing: float = 0 loss_type: Literal = 'sigmoid' args: TrainingArguments = None data_collator: Optional = None label_pad_token_id: int = -100 padding_value: int = 0 truncation_mode: str = 'keep_end' train_dataset: Optional = None eval_dataset: Union = None tokenizer: Optional = None model_init: Optional = None callbacks: Optional = None optimizers: Tuple = (None, None) preprocess_logits_for_metrics: Optional = None max_length: Optional = None max_prompt_length: Optional = None max_target_length: Optional = None peft_config: Optional = None is_encoder_decoder: Optional = None disable_dropout: bool = True generate_during_eval: bool = False compute_metrics: Optional = None precompute_ref_log_probs: bool = False model_init_kwargs: Optional = None ref_model_init_kwargs: Optional = None model_adapter_name: str = None ref_adapter_name: str = None )
```

å‚æ•°

+   `model` (`transformers.PreTrainedModel`) â€” è¦è®­ç»ƒçš„æ¨¡å‹ï¼Œæœ€å¥½æ˜¯ `AutoModelForSequenceClassification`ã€‚

+   `ref_model` (`PreTrainedModelWrapper`) â€” Hugging Face è½¬æ¢å™¨æ¨¡å‹ï¼Œå¸¦æœ‰ä¸€ä¸ªéšæ„è¯­è¨€å»ºæ¨¡å¤´ã€‚ç”¨äºéšå¼å¥–åŠ±è®¡ç®—å’ŒæŸå¤±ã€‚å¦‚æœæ²¡æœ‰æä¾›å‚è€ƒæ¨¡å‹ï¼Œè®­ç»ƒå™¨å°†åˆ›å»ºä¸€ä¸ªä¸è¦ä¼˜åŒ–çš„æ¨¡å‹å…·æœ‰ç›¸åŒæ¶æ„çš„å‚è€ƒæ¨¡å‹ã€‚

+   `beta` (`float`ï¼Œé»˜è®¤ä¸º 0.1) â€” DPO æŸå¤±ä¸­çš„ beta å› å­ã€‚è¾ƒé«˜çš„ beta æ„å‘³ç€ä¸åˆå§‹ç­–ç•¥çš„å‘æ•£è¾ƒå°ã€‚å¯¹äº IPO æŸå¤±ï¼Œbeta æ˜¯è®ºæ–‡ä¸­è¡¨ç¤ºçš„æ­£åˆ™åŒ–å‚æ•° tauã€‚

+   `label_smoothing` (`float`ï¼Œé»˜è®¤ä¸º 0) â€” æ¥è‡ª [cDPO](https://ericmitchell.ai/cdpo.pdf) æŠ¥å‘Šçš„é²æ£’ DPO æ ‡ç­¾å¹³æ»‘å‚æ•°ï¼Œåº”è¯¥åœ¨ 0 å’Œ 0.5 ä¹‹é—´ã€‚

+   `loss_type` (`str`ï¼Œé»˜è®¤ä¸º `"sigmoid"`) â€” è¦ä½¿ç”¨çš„ DPO æŸå¤±ç±»å‹ã€‚å¯ä»¥æ˜¯ `"sigmoid"` é»˜è®¤çš„ DPO æŸå¤±ï¼Œæ¥è‡ª [SLiC](https://arxiv.org/abs/2305.10425) è®ºæ–‡çš„ `"hinge"` æŸå¤±ï¼Œæ¥è‡ª [IPO](https://arxiv.org/abs/2310.12036) è®ºæ–‡çš„ `"ipo"`ï¼Œæˆ–æ¥è‡ª HALOs [æŠ¥å‘Š](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf) çš„ `"kto"`ã€‚

+   `args` (`transformers.TrainingArguments`) â€” ç”¨äºè®­ç»ƒçš„å‚æ•°ã€‚

+   `data_collator` (`transformers.DataCollator`) â€” ç”¨äºè®­ç»ƒçš„æ•°æ®æ•´ç†å™¨ã€‚å¦‚æœæœªæŒ‡å®šä¸º Noneï¼Œåˆ™å°†ä½¿ç”¨é»˜è®¤æ•°æ®æ•´ç†å™¨ï¼ˆ`DPODataCollatorWithPadding`ï¼‰ï¼Œè¯¥æ•´ç†å™¨å°†å°†åºåˆ—å¡«å……åˆ°æ‰¹æ¬¡ä¸­åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼Œç»™å®šä¸€ç»„æˆå¯¹åºåˆ—çš„æ•°æ®é›†ã€‚

+   `label_pad_token_id` (`int`ï¼Œé»˜è®¤ä¸º `-100`) â€” æ ‡ç­¾å¡«å……æ ‡è®° idã€‚å¦‚æœè¦ä½¿ç”¨é»˜è®¤æ•°æ®æ•´ç†å™¨ï¼Œåˆ™éœ€è¦æ­¤å‚æ•°ã€‚

+   `padding_value` (`int`ï¼Œé»˜è®¤ä¸º `0`) â€” å¦‚æœä¸æ ‡è®°å™¨çš„ pad_token_id ä¸åŒï¼Œåˆ™ä¸ºå¡«å……å€¼ã€‚

+   `truncation_mode` (`str`ï¼Œé»˜è®¤ä¸º `keep_end`) â€” è¦ä½¿ç”¨çš„æˆªæ–­æ¨¡å¼ï¼Œå¯ä»¥æ˜¯ `keep_end` æˆ– `keep_start`ã€‚å¦‚æœè¦ä½¿ç”¨é»˜è®¤æ•°æ®æ•´ç†å™¨ï¼Œåˆ™éœ€è¦æ­¤å‚æ•°ã€‚

+   `train_dataset` (`datasets.Dataset`) â€” ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ã€‚

+   `eval_dataset` (`datasets.Dataset`) â€” ç”¨äºè¯„ä¼°çš„æ•°æ®é›†ã€‚

+   `tokenizer` (`transformers.PreTrainedTokenizerBase`) â€” ç”¨äºè®­ç»ƒçš„æ ‡è®°å™¨ã€‚å¦‚æœè¦ä½¿ç”¨é»˜è®¤æ•°æ®æ•´ç†å™¨ï¼Œåˆ™éœ€è¦æ­¤å‚æ•°ã€‚

+   `model_init` (`Callable[[], transformers.PreTrainedModel]`) â€” ç”¨äºè®­ç»ƒçš„æ¨¡å‹åˆå§‹åŒ–å™¨ã€‚å¦‚æœæœªæŒ‡å®šä¸º Noneï¼Œåˆ™å°†ä½¿ç”¨é»˜è®¤æ¨¡å‹åˆå§‹åŒ–å™¨ã€‚

+   `callbacks` (`List[transformers.TrainerCallback]`) â€” ç”¨äºè®­ç»ƒçš„å›è°ƒå‡½æ•°ã€‚

+   `optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`) â€” ç”¨äºè®­ç»ƒçš„ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ã€‚

+   `preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`) â€” åœ¨è®¡ç®—æŒ‡æ ‡ä¹‹å‰ç”¨äºé¢„å¤„ç†å¯¹æ•°çš„å‡½æ•°ã€‚

+   `max_length` (`int`ï¼Œé»˜è®¤ä¸º `None`) â€” æ‰¹æ¬¡ä¸­åºåˆ—çš„æœ€å¤§é•¿åº¦ã€‚å¦‚æœè¦ä½¿ç”¨é»˜è®¤æ•°æ®æ•´ç†å™¨ï¼Œåˆ™éœ€è¦æ­¤å‚æ•°ã€‚

+   `max_prompt_length` (`int`ï¼Œé»˜è®¤ä¸º `None`) â€” æç¤ºçš„æœ€å¤§é•¿åº¦ã€‚å¦‚æœè¦ä½¿ç”¨é»˜è®¤æ•°æ®æ•´ç†å™¨ï¼Œåˆ™éœ€è¦æ­¤å‚æ•°ã€‚

+   `max_target_length` (`int`ï¼Œé»˜è®¤ä¸º `None`) â€” ç›®æ ‡çš„æœ€å¤§é•¿åº¦ã€‚å¦‚æœè¦ä½¿ç”¨é»˜è®¤æ•°æ®æ•´ç†å™¨å¹¶ä¸”æ‚¨çš„æ¨¡å‹æ˜¯ç¼–ç å™¨-è§£ç å™¨ï¼Œåˆ™éœ€è¦æ­¤å‚æ•°ã€‚

+   `peft_config` (`Dict`ï¼Œé»˜è®¤ä¸º `None`) â€” ç”¨äºè®­ç»ƒçš„ PEFT é…ç½®ã€‚å¦‚æœä¼ é€’ PEFT é…ç½®ï¼Œæ¨¡å‹å°†è¢«åŒ…è£…åœ¨ PEFT æ¨¡å‹ä¸­ã€‚

+   `is_encoder_decoder` (`Optional[bool]`, `å¯é€‰`, é»˜è®¤ä¸º `None`) â€” å¦‚æœæ²¡æœ‰æä¾›æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“æ¨¡å‹åˆå§‹åŒ–æ˜¯å¦è¿”å›ç¼–ç å™¨-è§£ç å™¨ã€‚

+   `disable_dropout` (`bool`, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨ `model` å’Œ `ref_model` ä¸­ç¦ç”¨ dropoutã€‚

+   `generate_during_eval` (`bool`, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨è¯„ä¼°æ­¥éª¤ä¸­å¯¹ç”Ÿæˆè¿›è¡Œé‡‡æ ·å’Œè®°å½•ã€‚

+   `compute_metrics` (`Callable[[EvalPrediction], Dict]`, *å¯é€‰*) â€” ç”¨äºè®¡ç®—æŒ‡æ ‡çš„å‡½æ•°ã€‚å¿…é¡»æ¥å—ä¸€ä¸ª `EvalPrediction` å¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²åˆ°æŒ‡æ ‡å€¼çš„å­—å…¸ã€‚

+   `precompute_ref_log_probs` (`bool`, é»˜è®¤ä¸º `False`) â€” ç”¨äºé¢„è®¡ç®—å‚è€ƒæ¨¡å‹å¯¹æ•°æ¦‚ç‡å’Œè¯„ä¼°æ•°æ®é›†çš„æ ‡å¿—ã€‚å¦‚æœè¦åœ¨æ²¡æœ‰å‚è€ƒæ¨¡å‹çš„æƒ…å†µä¸‹è®­ç»ƒå¹¶å‡å°‘æ‰€éœ€çš„æ€» GPU å†…å­˜ï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚model_init_kwargs â€” (`Optional[Dict]`, *å¯é€‰*): ä¼ é€’ç»™ä»å­—ç¬¦ä¸²å®ä¾‹åŒ–æ¨¡å‹æ—¶çš„å¯é€‰ kwargs å­—å…¸ ref_model_init_kwargs â€” (`Optional[Dict]`, *å¯é€‰*): ä¼ é€’ç»™ä»å­—ç¬¦ä¸²å®ä¾‹åŒ–å‚è€ƒæ¨¡å‹æ—¶çš„å¯é€‰ kwargs å­—å…¸

+   `model_adapter_name` (`str`, é»˜è®¤ä¸º `None`) â€” ä½¿ç”¨ LoRA æ—¶çš„è®­ç»ƒç›®æ ‡ PEFT é€‚é…å™¨çš„åç§°ï¼Œå½“æœ‰å¤šä¸ªé€‚é…å™¨æ—¶ã€‚

+   `ref_adapter_name` (`str`, é»˜è®¤ä¸º `None`) â€” ä½¿ç”¨ LoRA æ—¶å‚è€ƒ PEFT é€‚é…å™¨çš„åç§°ï¼Œå½“æœ‰å¤šä¸ªé€‚é…å™¨æ—¶ã€‚

åˆå§‹åŒ– DPOTrainerã€‚

#### `build_tokenized_answer`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L523)

```py
( prompt answer )
```

Llama åˆ†è¯å™¨ç¡®å®æ»¡è¶³ `enc(a + b) = enc(a) + enc(b)`ã€‚å®ƒç¡®ä¿ `enc(a + b) = enc(a) + enc(a + b)[len(enc(a)):]`ã€‚å‚è€ƒï¼š[`github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257`](https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257)

#### `compute_reference_log_probs`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L731)

```py
( padded_batch: Dict )
```

è®¡ç®— DPO ç‰¹å®šæ•°æ®é›†çš„å•ä¸ªå¡«å……æ‰¹æ¬¡çš„å‚è€ƒæ¨¡å‹çš„å¯¹æ•°æ¦‚ç‡ã€‚

#### `concatenated_forward`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L936)

```py
( model: Module batch: Dict )
```

åœ¨ç»™å®šçš„è¾“å…¥æ‰¹æ¬¡ä¸Šè¿è¡Œç»™å®šæ¨¡å‹ï¼Œå°†æ‰€é€‰å’Œè¢«æ‹’ç»çš„è¾“å…¥è¿æ¥åœ¨ä¸€èµ·ã€‚

æˆ‘ä»¬è¿™æ ·åšæ˜¯ä¸ºäº†é¿å…è¿›è¡Œä¸¤æ¬¡å‰å‘ä¼ é€’ï¼Œå› ä¸ºå¯¹äº FSDP æ¥è¯´æ›´å¿«ã€‚

#### `concatenated_inputs`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L755)

```py
( batch: Dict is_encoder_decoder: bool = False label_pad_token_id: int = -100 padding_value: int = 0 device: Optional = None )
```

å°†æ‰€é€‰å’Œè¢«æ‹’ç»çš„è¾“å…¥è¿æ¥æˆä¸€ä¸ªå¼ é‡ã€‚

#### `dpo_loss`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L817)

```py
( policy_chosen_logps: FloatTensor policy_rejected_logps: FloatTensor reference_chosen_logps: FloatTensor reference_rejected_logps: FloatTensor reference_free: bool = False ) â†’ export const metadata = 'undefined';A tuple of three tensors
```

è¿”å›

ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªå¼ é‡çš„å…ƒç»„

(æŸå¤±ï¼Œæ‰€é€‰å¥–åŠ±ï¼Œè¢«æ‹’ç»å¥–åŠ±)ã€‚æŸå¤±å¼ é‡åŒ…å«æ‰¹æ¬¡ä¸­æ¯ä¸ªç¤ºä¾‹çš„ DPO æŸå¤±ã€‚æ‰€é€‰å¥–åŠ±å’Œè¢«æ‹’ç»å¥–åŠ±å¼ é‡åˆ†åˆ«åŒ…å«æ‰€é€‰å’Œè¢«æ‹’ç»å“åº”çš„å¥–åŠ±ã€‚

ä¸ºä¸€æ‰¹ç­–ç•¥å’Œå‚è€ƒæ¨¡å‹å¯¹æ•°æ¦‚ç‡è®¡ç®— DPO æŸå¤±ã€‚

#### `evaluation_loop`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1154)

```py
( dataloader: DataLoader description: str prediction_loss_only: Optional = None ignore_keys: Optional = None metric_key_prefix: str = 'eval' )
```

è¦†ç›–å†…ç½®è¯„ä¼°å¾ªç¯ä»¥å­˜å‚¨æ¯ä¸ªæ‰¹æ¬¡çš„æŒ‡æ ‡ã€‚é¢„æµ‹/è¯„ä¼°å¾ªç¯ï¼Œç”± `Trainer.evaluate()` å’Œ `Trainer.predict()` å…±äº«ã€‚

æ— è®ºæ˜¯å¦æœ‰æ ‡ç­¾éƒ½å¯ä»¥ä½¿ç”¨ã€‚

#### `get_batch_logps`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L898)

```py
( logits: FloatTensor labels: LongTensor average_log_prob: bool = False label_pad_token_id: int = -100 is_encoder_decoder: bool = False )
```

è®¡ç®—ç»™å®šæ ‡ç­¾åœ¨ç»™å®šå¯¹æ•°ä¸‹çš„å¯¹æ•°æ¦‚ç‡ã€‚

#### `get_batch_loss_metrics`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L982)

```py
( model batch: Dict train_eval: Literal = 'train' )
```

ä¸ºç»™å®šçš„è¾“å…¥æ‰¹æ¬¡è®¡ç®— DPO æŸå¤±å’Œå…¶ä»–æŒ‡æ ‡ï¼Œç”¨äºè®­ç»ƒæˆ–æµ‹è¯•ã€‚

#### `get_batch_samples`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1064)

```py
( model batch: Dict )
```

ä¸ºç»™å®šçš„è¾“å…¥æ‰¹æ¬¡ä»æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ç”Ÿæˆæ ·æœ¬ã€‚

#### `get_eval_dataloader`

[<æ¥æº>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L471)

```py
( eval_dataset: Optional = None )
```

å‚æ•°

+   `eval_dataset`ï¼ˆ`torch.utils.data.Dataset`ï¼Œ*å¯é€‰*ï¼‰- å¦‚æœæä¾›ï¼Œå°†è¦†ç›–`self.eval_dataset`ã€‚å¦‚æœå®ƒæ˜¯ä¸€ä¸ª[Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)ï¼Œåˆ™ä¸è¢«`model.forward()`æ–¹æ³•æ¥å—çš„åˆ—å°†è¢«è‡ªåŠ¨åˆ é™¤ã€‚å®ƒå¿…é¡»å®ç°`__len__`ã€‚

è¿”å›è¯„ä¼°`~torch.utils.data.DataLoader`ã€‚

transformers.src.transformers.trainer.get_eval_dataloader çš„å­ç±»ï¼Œç”¨äºé¢„è®¡ç®—`ref_log_probs`ã€‚

#### `get_train_dataloader`

[<æ¥æº>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L428)

```py
( )
```

è¿”å›è®­ç»ƒ`~torch.utils.data.DataLoader`ã€‚

transformers.src.transformers.trainer.get_train_dataloader çš„å­ç±»ï¼Œç”¨äºé¢„è®¡ç®—`ref_log_probs`ã€‚

#### `log`

[<æ¥æº>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1204)

```py
( logs: Dict )
```

å‚æ•°

+   `logs`ï¼ˆ`Dict[str, float]`ï¼‰- è¦è®°å½•çš„å€¼ã€‚

åœ¨è§‚å¯Ÿè®­ç»ƒçš„å„ç§å¯¹è±¡ä¸Šè®°å½•`logs`ï¼ŒåŒ…æ‹¬å­˜å‚¨çš„æŒ‡æ ‡ã€‚

#### `null_ref_context`

[<æ¥æº>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L719)

```py
( )
```

ç”¨äºå¤„ç†ç©ºå¼•ç”¨æ¨¡å‹ï¼ˆå³ peft é€‚é…å™¨æ“ä½œï¼‰çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚

#### `tokenize_row`

[<æ¥æº>](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L573)

```py
( feature model: Union = None )
```

å¯¹æ¥è‡ª DPO ç‰¹å®šæ•°æ®é›†çš„å•è¡Œè¿›è¡Œæ ‡è®°åŒ–ã€‚

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰å°†å…¶è½¬æ¢ä¸º PyTorch å¼ é‡ï¼›æˆ‘ä»¬åªæ˜¯å¤„ç†æˆªæ–­ï¼Œä»¥é˜²æç¤º+æ‰€é€‰æˆ–æç¤º+æ‹’ç»å“åº”å¤ªé•¿ã€‚é¦–å…ˆæˆªæ–­æç¤ºï¼›å¦‚æœä»ç„¶å¤ªé•¿ï¼Œæˆ‘ä»¬æˆªæ–­æ‰€é€‰/æ‹’ç»ã€‚

æˆ‘ä»¬è¿˜ä¸ºæ‰€é€‰/æ‹’ç»çš„å“åº”åˆ›å»ºæ ‡ç­¾ï¼Œå…¶é•¿åº¦ç­‰äºæç¤ºå’Œæ‰€é€‰/æ‹’ç»å“åº”çš„é•¿åº¦ä¹‹å’Œï¼Œå¯¹äºæç¤ºæ ‡è®°ä½¿ç”¨ label_pad_token_idã€‚
