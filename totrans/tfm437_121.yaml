- en: Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: The pipelines are a great and easy way to use models for inference. These pipelines
    are objects that abstract most of the complex code from the library, offering
    a simple API dedicated to several tasks, including Named Entity Recognition, Masked
    Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering.
    See the [task summary](../task_summary) for examples of use.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two categories of pipeline abstractions to be aware about:'
  prefs: []
  type: TYPE_NORMAL
- en: The [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    which is the most powerful object encapsulating all other pipelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task-specific pipelines are available for [audio](#audio), [computer vision](#computer-vision),
    [natural language processing](#natural-language-processing), and [multimodal](#multimodal)
    tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pipeline abstraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *pipeline* abstraction is a wrapper around all the other available pipelines.
    It is instantiated as any other pipeline but can provide additional quality of
    life.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simple call on one item:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to use a specific model from the [hub](https://huggingface.co)
    you can ignore the task if the model on the hub already defines it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To call a pipeline on many items, you can call it with a *list*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: To iterate over full datasets it is recommended to use a `dataset` directly.
    This means you don’t need to allocate the whole dataset at once, nor do you need
    to do batching yourself. This should work just as fast as custom loops on GPU.
    If it doesn’t don’t hesitate to create an issue.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For ease of use, a generator is also possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#### `transformers.pipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/__init__.py#L531)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`task` (`str`) — The task defining which pipeline will be returned. Currently
    accepted tasks are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"audio-classification"`: will return a [AudioClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AudioClassificationPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"automatic-speech-recognition"`: will return a [AutomaticSpeechRecognitionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"conversational"`: will return a [ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"depth-estimation"`: will return a [DepthEstimationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.DepthEstimationPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"document-question-answering"`: will return a [DocumentQuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.DocumentQuestionAnsweringPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"feature-extraction"`: will return a [FeatureExtractionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.FeatureExtractionPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"fill-mask"`: will return a [FillMaskPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.FillMaskPipeline):.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"image-classification"`: will return a [ImageClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ImageClassificationPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"image-segmentation"`: will return a [ImageSegmentationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ImageSegmentationPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"image-to-image"`: will return a [ImageToImagePipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ImageToImagePipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"image-to-text"`: will return a [ImageToTextPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ImageToTextPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"mask-generation"`: will return a [MaskGenerationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.MaskGenerationPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"object-detection"`: will return a [ObjectDetectionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ObjectDetectionPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"question-answering"`: will return a [QuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"summarization"`: will return a [SummarizationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.SummarizationPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"table-question-answering"`: will return a [TableQuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TableQuestionAnsweringPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"text2text-generation"`: will return a [Text2TextGenerationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Text2TextGenerationPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"text-classification"` (alias `"sentiment-analysis"` available): will return
    a [TextClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TextClassificationPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"text-generation"`: will return a [TextGenerationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TextGenerationPipeline):.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"text-to-audio"` (alias `"text-to-speech"` available): will return a [TextToAudioPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TextToAudioPipeline):.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"token-classification"` (alias `"ner"` available): will return a [TokenClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TokenClassificationPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"translation"`: will return a [TranslationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TranslationPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"translation_xx_to_yy"`: will return a [TranslationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TranslationPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"video-classification"`: will return a [VideoClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.VideoClassificationPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"visual-question-answering"`: will return a [VisualQuestionAnsweringPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.VisualQuestionAnsweringPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"zero-shot-classification"`: will return a [ZeroShotClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ZeroShotClassificationPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"zero-shot-image-classification"`: will return a [ZeroShotImageClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ZeroShotImageClassificationPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"zero-shot-audio-classification"`: will return a [ZeroShotAudioClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ZeroShotAudioClassificationPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"zero-shot-object-detection"`: will return a [ZeroShotObjectDetectionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ZeroShotObjectDetectionPipeline).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model` (`str` or [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel),
    *optional*) — The model that will be used by the pipeline to make predictions.
    This can be a model identifier or an actual instance of a pretrained model inheriting
    from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    (for PyTorch) or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    (for TensorFlow).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If not provided, the default for the `task` will be loaded.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`config` (`str` or [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
    *optional*) — The configuration that will be used by the pipeline to instantiate
    the model. This can be a model identifier or an actual pretrained model configuration
    inheriting from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If not provided, the default configuration file for the requested model will
    be used. That means that if `model` is given, its default configuration will be
    used. However, if `model` is not supplied, this `task`’s default model’s config
    is used instead.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`tokenizer` (`str` or [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer),
    *optional*) — The tokenizer that will be used by the pipeline to encode data for
    the model. This can be a model identifier or an actual pretrained tokenizer inheriting
    from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If not provided, the default tokenizer for the given `model` will be loaded
    (if it is a string). If `model` is not specified or not a string, then the default
    tokenizer for `config` is loaded (if it is a string). However, if `config` is
    also not given or not a string, then the default tokenizer for the given `task`
    will be loaded.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`feature_extractor` (`str` or `PreTrainedFeatureExtractor`, *optional*) — The
    feature extractor that will be used by the pipeline to encode data for the model.
    This can be a model identifier or an actual pretrained feature extractor inheriting
    from `PreTrainedFeatureExtractor`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extractors are used for non-NLP models, such as Speech or Vision models
    as well as multi-modal models. Multi-modal models will also require a tokenizer
    to be passed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If not provided, the default feature extractor for the given `model` will be
    loaded (if it is a string). If `model` is not specified or not a string, then
    the default feature extractor for `config` is loaded (if it is a string). However,
    if `config` is also not given or not a string, then the default feature extractor
    for the given `task` will be loaded.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*, defaults to `"main"`) — When passing a task
    name or a string model identifier: The specific model version to use. It can be
    a branch name, a tag name, or a commit id, since we use a git-based system for
    storing models and other artifacts on huggingface.co, so `revision` can be any
    identifier allowed by git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_fast` (`bool`, *optional*, defaults to `True`) — Whether or not to use
    a Fast tokenizer if possible (a [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_auth_token` (`str` or *bool*, *optional*) — The token to use as HTTP bearer
    authorization for remote files. If `True`, will use the token generated when running
    `huggingface-cli login` (stored in `~/.huggingface`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int` or `str` or `torch.device`) — Defines the device (*e.g.*, `"cpu"`,
    `"cuda:1"`, `"mps"`, or a GPU ordinal rank like `1`) on which this pipeline will
    be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_map` (`str` or `Dict[str, Union[int, str, torch.device]`, *optional*)
    — Sent directly as `model_kwargs` (just a simpler shortcut). When `accelerate`
    library is present, set `device_map="auto"` to compute the most optimized `device_map`
    automatically (see [here](https://huggingface.co/docs/accelerate/main/en/package_reference/big_modeling#accelerate.cpu_offload)
    for more information).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not use `device_map` AND `device` at the same time as they will conflict
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`torch_dtype` (`str` or `torch.dtype`, *optional*) — Sent directly as `model_kwargs`
    (just a simpler shortcut) to use the available precision for this model (`torch.float16`,
    `torch.bfloat16`, … or `"auto"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trust_remote_code` (`bool`, *optional*, defaults to `False`) — Whether or
    not to allow for custom code defined on the Hub in their own modeling, configuration,
    tokenization or even pipeline files. This option should only be set to `True`
    for repositories you trust and in which you have read the code, as it will execute
    code present on the Hub on your local machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_kwargs` (`Dict[str, Any]`, *optional*) — Additional dictionary of keyword
    arguments passed along to the model’s `from_pretrained(..., **model_kwargs)` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional keyword arguments passed
    along to the specific pipeline init (see the documentation for the corresponding
    pipeline class for possible values).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)'
  prefs: []
  type: TYPE_NORMAL
- en: A suitable pipeline for the task.
  prefs: []
  type: TYPE_NORMAL
- en: Utility factory method to build a [Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline).
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipelines are made of:'
  prefs: []
  type: TYPE_NORMAL
- en: A [tokenizer](tokenizer) in charge of mapping raw textual input to token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [model](model) to make predictions from the inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some (optional) post processing for enhancing model’s output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Pipeline batching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All pipelines can use batching. This will work whenever the pipeline uses its
    streaming ability (so when passing lists or `Dataset` or `generator`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: However, this is not automatically a win for performance. It can be either a
    10x speedup or 5x slowdown depending on hardware, data and the actual model being
    used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example where it’s mostly a speedup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Example where it’s most a slowdown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is a occasional very long sentence compared to the other. In that case,
    the **whole** batch will need to be 400 tokens long, so the whole batch will be
    [64, 400] instead of [64, 4], leading to the high slowdown. Even worse, on bigger
    batches, the program simply crashes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'There are no good (general) solutions for this problem, and your mileage may
    vary depending on your use cases. Rule of thumb:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For users, a rule of thumb is:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Measure performance on your load, with your hardware. Measure, measure, and
    keep measuring. Real numbers are the only way to go.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are latency constrained (live product doing inference), don’t batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are using CPU, don’t batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you are using throughput (you want to run your model on a bunch of static
    data), on GPU, then:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have no clue about the size of the sequence_length (“natural” data),
    by default don’t batch, measure and try tentatively to add it, add OOM checks
    to recover when it will fail (and it will at some point if you don’t control the
    sequence_length.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If your sequence_length is super regular, then batching is more likely to be
    VERY interesting, measure and push it until you get OOMs.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The larger the GPU the more likely batching is going to be more interesting
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As soon as you enable batching, make sure you can handle OOMs nicely.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline chunk batching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`zero-shot-classification` and `question-answering` are slightly specific in
    the sense, that a single input might yield multiple forward pass of a model. Under
    normal circumstances, this would yield issues with `batch_size` argument.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to circumvent this issue, both of these pipelines are a bit specific,
    they are `ChunkPipeline` instead of regular `Pipeline`. In short:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This should be very transparent to your code because the pipelines are used
    in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: This is a simplified view, since the pipeline can handle automatically the batch
    to ! Meaning you don’t have to care about how many forward passes you inputs are
    actually going to trigger, you can optimize the `batch_size` independently of
    the inputs. The caveats from the previous section still apply.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline custom code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you want to override a specific pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t hesitate to create an issue for your task at hand, the goal of the pipeline
    is to be easy to use and support most cases, so `transformers` could maybe support
    your use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to try simply you can:'
  prefs: []
  type: TYPE_NORMAL
- en: Subclass your pipeline of choice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: That should enable you to do all the custom code you want.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Implementing a new pipeline](../add_new_pipeline)'
  prefs: []
  type: TYPE_NORMAL
- en: Audio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pipelines available for audio tasks include the following.
  prefs: []
  type: TYPE_NORMAL
- en: AudioClassificationPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.AudioClassificationPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/audio_classification.py#L66)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio classification pipeline using any `AutoModelForAudioClassification`. This
    pipeline predicts the class of a raw waveform or an audio file. In case of an
    audio file, ffmpeg should be installed to support multiple audio formats.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"audio-classification"`.'
  prefs: []
  type: TYPE_NORMAL
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=audio-classification).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/audio_classification.py#L103)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`inputs` (`np.ndarray` or `bytes` or `str` or `dict`) — The inputs is either
    :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`str` that is the filename of the audio file, the file will be read at the
    correct sampling rate to get the waveform using *ffmpeg*. This requires *ffmpeg*
    to be installed on the system.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bytes` it is supposed to be the content of an audio file and is interpreted
    by *ffmpeg* in the same way.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: (`np.ndarray` of shape (n, ) of type `np.float32` or `np.float64`) Raw audio
    at the correct sampling rate (no further check will be done)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dict` form can be used to pass raw audio sampled at arbitrary `sampling_rate`
    and let this pipeline do the resampling. The dict must be either be in the format
    `{"sampling_rate": int, "raw": np.array}`, or `{"sampling_rate": int, "array":
    np.array}`, where the key `"raw"` or `"array"` is used to denote the raw audio
    waveform.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_k` (`int`, *optional*, defaults to None) — The number of top labels that
    will be returned by the pipeline. If the provided number is `None` or higher than
    the number of labels available in the model configuration, it will default to
    the number of labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A list of `dict` with the following keys
  prefs: []
  type: TYPE_NORMAL
- en: '`label` (`str`) — The label predicted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score` (`float`) — The corresponding probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classify the sequence(s) given as inputs. See the [AutomaticSpeechRecognitionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)
    documentation for more information.
  prefs: []
  type: TYPE_NORMAL
- en: AutomaticSpeechRecognitionPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.AutomaticSpeechRecognitionPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/automatic_speech_recognition.py#L134)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature_extractor` ([SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor))
    — The feature extractor that will be used by the pipeline to encode waveform for
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder` (`pyctcdecode.BeamSearchDecoderCTC`, *optional*) — [PyCTCDecode’s
    BeamSearchDecoderCTC](https://github.com/kensho-technologies/pyctcdecode/blob/2fd33dc37c4111417e08d89ccd23d28e9b308d19/pyctcdecode/decoder.py#L180)
    can be passed for language model boosted decoding. See [Wav2Vec2ProcessorWithLM](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM)
    for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunk_length_s` (`float`, *optional*, defaults to 0) — The input length for
    in each chunk. If `chunk_length_s = 0` then chunking is disabled (default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on how to effectively use `chunk_length_s`, please have
    a look at the [ASR chunking blog post](https://huggingface.co/blog/asr-chunking).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`stride_length_s` (`float`, *optional*, defaults to `chunk_length_s / 6`) —
    The length of stride on the left and right of each chunk. Used only with `chunk_length_s
    > 0`. This enables the model to *see* more context and infer letters better than
    without this context but the pipeline discards the stride bits at the end to make
    the final reconstitution as perfect as possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on how to effectively use `stride_length_s`, please have
    a look at the [ASR chunking blog post](https://huggingface.co/blog/asr-chunking).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed. If no framework
    is specified, will default to the one currently installed. If no framework is
    specified and both frameworks are installed, will default to the framework of
    the `model`, or to PyTorch if no model is provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (Union[`int`, `torch.device`], *optional*) — Device ordinal for CPU/GPU
    supports. Setting this to `None` will leverage CPU, a positive will run the model
    on the associated CUDA device id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch_dtype` (Union[`int`, `torch.dtype`], *optional*) — The data-type (dtype)
    of the computation. Setting this to `None` will use float32 precision. Set to
    `torch.float16` or `torch.bfloat16` to use half-precision in the respective dtypes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline that aims at extracting spoken text contained within some audio.
  prefs: []
  type: TYPE_NORMAL
- en: The input can be either a raw waveform or a audio file. In case of the audio
    file, ffmpeg should be installed for to support multiple audio formats
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/automatic_speech_recognition.py#L229)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`inputs` (`np.ndarray` or `bytes` or `str` or `dict`) — The inputs is either
    :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`str` that is either the filename of a local audio file, or a public URL address
    to download the audio file. The file will be read at the correct sampling rate
    to get the waveform using *ffmpeg*. This requires *ffmpeg* to be installed on
    the system.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bytes` it is supposed to be the content of an audio file and is interpreted
    by *ffmpeg* in the same way.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: (`np.ndarray` of shape (n, ) of type `np.float32` or `np.float64`) Raw audio
    at the correct sampling rate (no further check will be done)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dict` form can be used to pass raw audio sampled at arbitrary `sampling_rate`
    and let this pipeline do the resampling. The dict must be in the format `{"sampling_rate":
    int, "raw": np.array}` with optionally a `"stride": (left: int, right: int)` than
    can ask the pipeline to treat the first `left` samples and last `right` samples
    to be ignored in decoding (but used at inference to provide more context to the
    model). Only use `stride` with CTC models.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_timestamps` (*optional*, `str` or `bool`) — Only available for pure
    CTC models (Wav2Vec2, HuBERT, etc) and the Whisper model. Not available for other
    sequence-to-sequence models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For CTC models, timestamps can take one of two formats:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`"char"`: the pipeline will return timestamps along the text for every character
    in the text. For instance, if you get `[{"text": "h", "timestamp": (0.5, 0.6)},
    {"text": "i", "timestamp": (0.7, 0.9)}]`, then it means the model predicts that
    the letter “h” was spoken after `0.5` and before `0.6` seconds.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"word"`: the pipeline will return timestamps along the text for every word
    in the text. For instance, if you get `[{"text": "hi ", "timestamp": (0.5, 0.9)},
    {"text": "there", "timestamp": (1.0, 1.5)}]`, then it means the model predicts
    that the word “hi” was spoken after `0.5` and before `0.9` seconds.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the Whisper model, timestamps can take one of two formats:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`"word"`: same as above for word-level CTC timestamps. Word-level timestamps
    are predicted through the *dynamic-time warping (DTW)* algorithm, an approximation
    to word-level timestamps by inspecting the cross-attention weights.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True`: the pipeline will return timestamps along the text for *segments* of
    words in the text. For instance, if you get `[{"text": " Hi there!", "timestamp":
    (0.5, 1.5)}]`, then it means the model predicts that the segment “Hi there!” was
    spoken after `0.5` and before `1.5` seconds. Note that a segment of text refers
    to a sequence of one or more words, rather than individual words as with word-level
    timestamps.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generate_kwargs` (`dict`, *optional*) — The dictionary of ad-hoc parametrization
    of `generate_config` to be used for the generation call. For a complete overview
    of generate, check the [following guide](https://huggingface.co/docs/transformers/en/main_classes/text_generation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_new_tokens` (`int`, *optional*) — The maximum numbers of tokens to generate,
    ignoring the number of tokens in the prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A dictionary with the following keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`text` (`str`): The recognized text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunks` (*optional(, `List[Dict]`) When using `return_timestamps`, the `chunks`
    will become a list containing all the various text chunks identified by the model,*
    e.g.* `[{"text": "hi ", "timestamp": (0.5, 0.9)}, {"text": "there", "timestamp":
    (1.0, 1.5)}]`. The original full text can roughly be recovered by doing `"".join(chunk["text"]
    for chunk in output["chunks"])`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transcribe the audio sequence(s) given as inputs to text. See the [AutomaticSpeechRecognitionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)
    documentation for more information.
  prefs: []
  type: TYPE_NORMAL
- en: TextToAudioPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.TextToAudioPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_to_audio.py#L27)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Text-to-audio generation pipeline using any `AutoModelForTextToWaveform` or
    `AutoModelForTextToSpectrogram`. This pipeline generates an audio file from an
    input text and optional other conditional inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: You can specify parameters passed to the model by using `TextToAudioPipeline.__call__.forward_params`
    or `TextToAudioPipeline.__call__.generate_kwargs`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifiers: `"text-to-speech"` or `"text-to-audio"`.'
  prefs: []
  type: TYPE_NORMAL
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=text-to-speech).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_to_audio.py#L160)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text_inputs` (`str` or `List[str]`) — The text(s) to generate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forward_params` (`dict`, *optional*) — Parameters passed to the model generation/forward
    method. `forward_params` are always passed to the underlying model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generate_kwargs` (`dict`, *optional*) — The dictionary of ad-hoc parametrization
    of `generate_config` to be used for the generation call. For a complete overview
    of generate, check the [following guide](https://huggingface.co/docs/transformers/en/main_classes/text_generation).
    `generate_kwargs` are only passed to the underlying model if the latter is a generative
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A `dict` or a list of `dict`
  prefs: []
  type: TYPE_NORMAL
- en: 'The dictionaries have two keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`audio` (`np.ndarray` of shape `(nb_channels, audio_length)`) — The generated
    audio waveform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`) — The sampling rate of the generated audio waveform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates speech/audio from the inputs. See the [TextToAudioPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.TextToAudioPipeline)
    documentation for more information.
  prefs: []
  type: TYPE_NORMAL
- en: ZeroShotAudioClassificationPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.ZeroShotAudioClassificationPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_audio_classification.py#L32)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero shot audio classification pipeline using `ClapModel`. This pipeline predicts
    the class of an audio when you provide an audio and a set of `candidate_labels`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
    This audio classification pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"zero-shot-audio-classification"`. See the
    list of available models on [huggingface.co/models](https://huggingface.co/models?filter=zero-shot-audio-classification).'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_audio_classification.py#L64)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`audios` (`str`, `List[str]`, `np.array` or `List[np.array]`) — The pipeline
    handles three types of inputs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a http link pointing to an audio
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a local path to an audio
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An audio loaded in numpy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`candidate_labels` (`List[str]`) — The candidate labels for this audio'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hypothesis_template` (`str`, *optional*, defaults to `"This is a sound of
    {}"`) — The sentence used in cunjunction with *candidate_labels* to attempt the
    audio classification by replacing the placeholder with the candidate_labels. Then
    likelihood is estimated by using logits_per_audio'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign labels to the audio(s) passed as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pipelines available for computer vision tasks include the following.
  prefs: []
  type: TYPE_NORMAL
- en: DepthEstimationPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.DepthEstimationPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/depth_estimation.py#L22)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depth estimation pipeline using any `AutoModelForDepthEstimation`. This pipeline
    predicts the depth of an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This depth estimation pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"depth-estimation"`.'
  prefs: []
  type: TYPE_NORMAL
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=depth-estimation).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/depth_estimation.py#L53)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — The pipeline
    handles three types of images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a http link pointing to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a local path to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An image loaded in PIL directly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pipeline accepts either a single image or a batch of images, which must
    then be passed as a string. Images in a batch must all be in the same format:
    all as http links, all as local paths, or all as PIL images.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`top_k` (`int`, *optional*, defaults to 5) — The number of top labels that
    will be returned by the pipeline. If the provided number is higher than the number
    of labels available in the model configuration, it will default to the number
    of labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign labels to the image(s) passed as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: ImageClassificationPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.ImageClassificationPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_classification.py#L50)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function_to_apply` (`str`, *optional*, defaults to `"default"`) — The function
    to apply to the model outputs in order to retrieve the scores. Accepts four different
    values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"default"`: if the model has a single label, will apply the sigmoid function
    on the output. If the model has several labels, will apply the softmax function
    on the output.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"sigmoid"`: Applies the sigmoid function on the output.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"softmax"`: Applies the softmax function on the output.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"none"`: Does not apply any function on the output.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Image classification pipeline using any `AutoModelForImageClassification`. This
    pipeline predicts the class of an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This image classification pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"image-classification"`.'
  prefs: []
  type: TYPE_NORMAL
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=image-classification).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_classification.py#L111)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — The pipeline
    handles three types of images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a http link pointing to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a local path to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An image loaded in PIL directly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pipeline accepts either a single image or a batch of images, which must
    then be passed as a string. Images in a batch must all be in the same format:
    all as http links, all as local paths, or all as PIL images.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`function_to_apply` (`str`, *optional*, defaults to `"default"`) — The function
    to apply to the model outputs in order to retrieve the scores. Accepts four different
    values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If this argument is not specified, then it will apply the following functions
    according to the number of labels:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If the model has a single label, will apply the sigmoid function on the output.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model has several labels, will apply the softmax function on the output.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Possible values are:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`"sigmoid"`: Applies the sigmoid function on the output.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"softmax"`: Applies the softmax function on the output.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"none"`: Does not apply any function on the output.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_k` (`int`, *optional*, defaults to 5) — The number of top labels that
    will be returned by the pipeline. If the provided number is higher than the number
    of labels available in the model configuration, it will default to the number
    of labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign labels to the image(s) passed as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: ImageSegmentationPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.ImageSegmentationPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_segmentation.py#L30)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image segmentation pipeline using any `AutoModelForXXXSegmentation`. This pipeline
    predicts masks of objects and their classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This image segmentation pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"image-segmentation"`.'
  prefs: []
  type: TYPE_NORMAL
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=image-segmentation).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_segmentation.py#L97)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — The pipeline
    handles three types of images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing an HTTP(S) link pointing to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a local path to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An image loaded in PIL directly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pipeline accepts either a single image or a batch of images. Images in
    a batch must all be in the same format: all as HTTP(S) links, all as local paths,
    or all as PIL images.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`subtask` (`str`, *optional*) — Segmentation task to be performed, choose [`semantic`,
    `instance` and `panoptic`] depending on model capabilities. If not set, the pipeline
    will attempt tp resolve in the following order: `panoptic`, `instance`, `semantic`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threshold` (`float`, *optional*, defaults to 0.9) — Probability threshold
    to filter out predicted masks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) — Threshold to use
    when turning the predicted masks into binary values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.5) — Mask
    overlap threshold to eliminate small, disconnected segments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform segmentation (detect masks & classes) in the image(s) passed as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: ImageToImagePipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.ImageToImagePipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_to_image.py#L39)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image to Image pipeline using any `AutoModelForImageToImage`. This pipeline
    generates an image based on a previous image input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This image to image pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"image-to-image"`.'
  prefs: []
  type: TYPE_NORMAL
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=image-to-image).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_to_image.py#L87)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — The pipeline
    handles three types of images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a http link pointing to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a local path to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An image loaded in PIL directly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pipeline accepts either a single image or a batch of images, which must
    then be passed as a string. Images in a batch must all be in the same format:
    all as http links, all as local paths, or all as PIL images.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is used and the
    call may block forever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transform the image(s) passed as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: ObjectDetectionPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.ObjectDetectionPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/object_detection.py#L26)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection pipeline using any `AutoModelForObjectDetection`. This pipeline
    predicts bounding boxes of objects and their classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This object detection pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"object-detection"`.'
  prefs: []
  type: TYPE_NORMAL
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=object-detection).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/object_detection.py#L72)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — The pipeline
    handles three types of images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing an HTTP(S) link pointing to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a local path to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An image loaded in PIL directly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pipeline accepts either a single image or a batch of images. Images in
    a batch must all be in the same format: all as HTTP(S) links, all as local paths,
    or all as PIL images.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`threshold` (`float`, *optional*, defaults to 0.9) — The probability necessary
    to make a prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detect objects (bounding boxes & classes) in the image(s) passed as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: VideoClassificationPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.VideoClassificationPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/video_classification.py#L21)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video classification pipeline using any `AutoModelForVideoClassification`. This
    pipeline predicts the class of a video.
  prefs: []
  type: TYPE_NORMAL
- en: 'This video classification pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"video-classification"`.'
  prefs: []
  type: TYPE_NORMAL
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=video-classification).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/video_classification.py#L51)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`videos` (`str`, `List[str]`) — The pipeline handles three types of videos:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a http link pointing to a video
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a local path to a video
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pipeline accepts either a single video or a batch of videos, which must
    then be passed as a string. Videos in a batch must all be in the same format:
    all as http links or all as local paths.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`top_k` (`int`, *optional*, defaults to 5) — The number of top labels that
    will be returned by the pipeline. If the provided number is higher than the number
    of labels available in the model configuration, it will default to the number
    of labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_frames` (`int`, *optional*, defaults to `self.model.config.num_frames`)
    — The number of frames sampled from the video to run the classification on. If
    not provided, will default to the number of frames specified in the model configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frame_sampling_rate` (`int`, *optional*, defaults to 1) — The sampling rate
    used to select frames from the video. If not provided, will default to 1, i.e.
    every frame will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign labels to the video(s) passed as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: ZeroShotImageClassificationPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.ZeroShotImageClassificationPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_image_classification.py#L32)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero shot image classification pipeline using `CLIPModel`. This pipeline predicts
    the class of an image when you provide an image and a set of `candidate_labels`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This image classification pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"zero-shot-image-classification"`.'
  prefs: []
  type: TYPE_NORMAL
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=zero-shot-image-classification).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_image_classification.py#L76)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — The pipeline
    handles three types of images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a http link pointing to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a local path to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An image loaded in PIL directly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`candidate_labels` (`List[str]`) — The candidate labels for this image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hypothesis_template` (`str`, *optional*, defaults to `"This is a photo of
    {}"`) — The sentence used in cunjunction with *candidate_labels* to attempt the
    image classification by replacing the placeholder with the candidate_labels. Then
    likelihood is estimated by using logits_per_image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign labels to the image(s) passed as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: ZeroShotObjectDetectionPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.ZeroShotObjectDetectionPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_object_detection.py#L22)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero shot object detection pipeline using `OwlViTForObjectDetection`. This pipeline
    predicts bounding boxes of objects when you provide an image and a set of `candidate_labels`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This object detection pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"zero-shot-object-detection"`.'
  prefs: []
  type: TYPE_NORMAL
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=zero-shot-object-detection).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_object_detection.py#L65)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`image` (`str`, `PIL.Image` or `List[Dict[str, Any]]`) — The pipeline handles
    three types of images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing an http url pointing to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a local path to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An image loaded in PIL directly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can use this parameter to send directly a list of images, or a dataset
    or a generator like so:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Detect objects (bounding boxes & classes) in the image(s) passed as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Natural Language Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pipelines available for natural language processing tasks include the following.
  prefs: []
  type: TYPE_NORMAL
- en: ConversationalPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.Conversation`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L18)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`messages` (Union[str, List[Dict[str, str]]], *optional*) — The initial messages
    to start the conversation, either a string, or a list of dicts containing “role”
    and “content” keys. If a string is passed, it is interpreted as a single message
    with the “user” role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conversation_id` (`uuid.UUID`, *optional*) — Unique identifier for the conversation.
    If not provided, a random UUID4 id will be assigned to the conversation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utility class containing a conversation and its history. This class is meant
    to be used as an input to the [ConversationalPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ConversationalPipeline).
    The conversation contains several utility functions to manage the addition of
    new user inputs and generated model responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '#### `add_user_input`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L89)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Add a user input to the conversation for the next round. This is a legacy method
    that assumes that inputs must alternate user/assistant/user/assistant, and so
    will not add multiple user messages in succession. We recommend just using `add_message`
    with role “user” instead.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `append_response`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L110)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: This is a legacy method. We recommend just using `add_message` with an appropriate
    role instead.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `mark_processed`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L116)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: This is a legacy method, as the Conversation no longer distinguishes between
    processed and unprocessed user input. We set a counter here to keep behaviour
    mostly backward-compatible, but in general you should just read the messages directly
    when writing new code.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.ConversationalPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L194)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_length_for_response` (`int`, *optional*, defaults to 32) — The minimum
    length (in number of tokens) for a response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minimum_tokens` (`int`, *optional*, defaults to 10) — The minimum length of
    tokens to leave for a response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-turn conversational pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This conversational pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"conversational"`.'
  prefs: []
  type: TYPE_NORMAL
- en: This pipeline can be used with any model that has a [chat template](https://huggingface.co/docs/transformers/chat_templating)
    set.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/conversational.py#L262)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`conversations` (a [Conversation](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Conversation)
    or a list of [Conversation](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Conversation))
    — Conversation to generate responses for. Inputs can also be passed as a list
    of dictionaries with `role` and `content` keys - in this case, they will be converted
    to `Conversation` objects automatically. Multiple conversations in either format
    may be passed as a list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `False`) —
    Whether or not to clean up the potential extra spaces in the text output. generate_kwargs
    — Additional keyword arguments to pass along to the generate method of the model
    (see the generate method corresponding to your framework [here](./model#generative-models)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[Conversation](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Conversation)
    or a list of [Conversation](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Conversation)'
  prefs: []
  type: TYPE_NORMAL
- en: Conversation(s) with updated generated responses for those containing a new
    user input.
  prefs: []
  type: TYPE_NORMAL
- en: Generate responses for the conversation(s) given as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: FillMaskPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.FillMaskPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/fill_mask.py#L22)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_k` (`int`, defaults to 5) — The number of predictions to return.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`targets` (`str` or `List[str]`, *optional*) — When passed, the model will
    limit the scores to the passed targets instead of looking up in the whole vocab.
    If the provided targets are not in the model vocab, they will be tokenized and
    the first resulting token will be used (with a warning, and that might be slower).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Masked language modeling prediction pipeline using any `ModelWithLMHead`. See
    the [masked language modeling examples](../task_summary#masked-language-modeling)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This mask filling pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"fill-mask"`.'
  prefs: []
  type: TYPE_NORMAL
- en: The models that this pipeline can use are models that have been trained with
    a masked language modeling objective, which includes the bi-directional models
    in the library. See the up-to-date list of available models on [huggingface.co/models](https://huggingface.co/models?filter=fill-mask).
  prefs: []
  type: TYPE_NORMAL
- en: 'This pipeline only works for inputs with exactly one token masked. Experimental:
    We added support for multiple masks. The returned values are raw model output,
    and correspond to disjoint probabilities where one might expect joint probabilities
    (See [discussion](https://github.com/huggingface/transformers/pull/10222)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This pipeline now supports tokenizer_kwargs. For example try:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/fill_mask.py#L248)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`args` (`str` or `List[str]`) — One or several texts (or one list of prompts)
    with masked tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`targets` (`str` or `List[str]`, *optional*) — When passed, the model will
    limit the scores to the passed targets instead of looking up in the whole vocab.
    If the provided targets are not in the model vocab, they will be tokenized and
    the first resulting token will be used (with a warning, and that might be slower).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_k` (`int`, *optional*) — When passed, overrides the number of predictions
    to return.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A list or a list of list of `dict`
  prefs: []
  type: TYPE_NORMAL
- en: 'Each result comes as list of dictionaries with the following keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sequence` (`str`) — The corresponding input with the mask token prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score` (`float`) — The corresponding probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`int`) — The predicted token id (to replace the masked one).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_str` (`str`) — The predicted token (to replace the masked one).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fill the masked token in the text(s) given as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: QuestionAnsweringPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.QuestionAnsweringPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/question_answering.py#L224)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question Answering pipeline using any `ModelForQuestionAnswering`. See the [question
    answering examples](../task_summary#question-answering) for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This question answering pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"question-answering"`.'
  prefs: []
  type: TYPE_NORMAL
- en: The models that this pipeline can use are models that have been fine-tuned on
    a question answering task. See the up-to-date list of available models on [huggingface.co/models](https://huggingface.co/models?filter=question-answering).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/question_answering.py#L343)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`args` (`SquadExample` or a list of `SquadExample`) — One or several `SquadExample`
    containing the question and context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`X` (`SquadExample` or a list of `SquadExample`, *optional*) — One or several
    `SquadExample` containing the question and context (will be treated the same way
    as if passed as the first positional argument).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data` (`SquadExample` or a list of `SquadExample`, *optional*) — One or several
    `SquadExample` containing the question and context (will be treated the same way
    as if passed as the first positional argument).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question` (`str` or `List[str]`) — One or several question(s) (must be used
    in conjunction with the `context` argument).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context` (`str` or `List[str]`) — One or several context(s) associated with
    the question(s) (must be used in conjunction with the `question` argument).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topk` (`int`, *optional*, defaults to 1) — The number of answers to return
    (will be chosen by order of likelihood). Note that we return less than topk answers
    if there are not enough options available within the context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_stride` (`int`, *optional*, defaults to 128) — If the context is too long
    to fit with the question for the model, it will be split in several chunks with
    some overlap. This argument controls the size of that overlap.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_answer_len` (`int`, *optional*, defaults to 15) — The maximum length of
    predicted answers (e.g., only answers with a shorter length are considered).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_seq_len` (`int`, *optional*, defaults to 384) — The maximum length of
    the total sentence (context + question) in tokens of each chunk passed to the
    model. The context will be split in several chunks (using `doc_stride` as overlap)
    if needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_question_len` (`int`, *optional*, defaults to 64) — The maximum length
    of the question after tokenization. It will be truncated if needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`handle_impossible_answer` (`bool`, *optional*, defaults to `False`) — Whether
    or not we accept impossible as an answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`align_to_words` (`bool`, *optional*, defaults to `True`) — Attempts to align
    the answer to real words. Improves quality on space separated langages. Might
    hurt on non-space-separated languages (like Japanese or Chinese)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A `dict` or a list of `dict`
  prefs: []
  type: TYPE_NORMAL
- en: 'Each result comes as a dictionary with the following keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`score` (`float`) — The probability associated to the answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start` (`int`) — The character start index of the answer (in the tokenized
    version of the input).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end` (`int`) — The character end index of the answer (in the tokenized version
    of the input).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`answer` (`str`) — The answer to the question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answer the question(s) given as inputs by using the context(s).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `create_sample`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/question_answering.py#L278)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`question` (`str` or `List[str]`) — The question(s) asked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context` (`str` or `List[str]`) — The context(s) in which we will look for
    the answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: One or a list of `SquadExample`
  prefs: []
  type: TYPE_NORMAL
- en: The corresponding `SquadExample` grouping question and context.
  prefs: []
  type: TYPE_NORMAL
- en: QuestionAnsweringPipeline leverages the `SquadExample` internally. This helper
    method encapsulate all the logic for converting question(s) and context(s) to
    `SquadExample`.
  prefs: []
  type: TYPE_NORMAL
- en: We currently support extractive question answering.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `span_to_answer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/question_answering.py#L630)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text` (`str`) — The actual context to extract the answer from.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start` (`int`) — The answer starting token index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end` (`int`) — The answer end token index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary like `{‘answer’
  prefs: []
  type: TYPE_NORMAL
- en: 'str, ‘start’: int, ‘end’: int}`'
  prefs: []
  type: TYPE_NORMAL
- en: When decoding from token probabilities, this method maps token indexes to actual
    word in the initial context.
  prefs: []
  type: TYPE_NORMAL
- en: SummarizationPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.SummarizationPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L216)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarize news articles and other documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'This summarizing pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"summarization"`.'
  prefs: []
  type: TYPE_NORMAL
- en: The models that this pipeline can use are models that have been fine-tuned on
    a summarization task, which is currently, ’*bart-large-cnn*’, ’*t5-small*’, ’*t5-base*’,
    ’*t5-large*’, ’*t5-3b*’, ’*t5-11b*’. See the up-to-date list of available models
    on [huggingface.co/models](https://huggingface.co/models?filter=summarization).
    For a list of available parameters, see the [following documentation](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L245)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`documents` (*str* or `List[str]`) — One or several articles (or one list of
    articles) to summarize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_text` (`bool`, *optional*, defaults to `True`) — Whether or not to
    include the decoded texts in the outputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`bool`, *optional*, defaults to `False`) — Whether or not
    to include the tensors of predictions (as token indices) in the outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `False`) —
    Whether or not to clean up the potential extra spaces in the text output. generate_kwargs
    — Additional keyword arguments to pass along to the generate method of the model
    (see the generate method corresponding to your framework [here](./model#generative-models)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A list or a list of list of `dict`
  prefs: []
  type: TYPE_NORMAL
- en: 'Each result comes as a dictionary with the following keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`summary_text` (`str`, present when `return_text=True`) — The summary of the
    corresponding input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summary_token_ids` (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`)
    — The token ids of the summary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarize the text(s) given as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: TableQuestionAnsweringPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.TableQuestionAnsweringPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/table_question_answering.py#L87)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Table Question Answering pipeline using a `ModelForTableQuestionAnswering`.
    This pipeline is only available in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This tabular question answering pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"table-question-answering"`.'
  prefs: []
  type: TYPE_NORMAL
- en: The models that this pipeline can use are models that have been fine-tuned on
    a tabular question answering task. See the up-to-date list of available models
    on [huggingface.co/models](https://huggingface.co/models?filter=table-question-answering).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/table_question_answering.py#L270)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`table` (`pd.DataFrame` or `Dict`) — Pandas DataFrame or dictionary that will
    be converted to a DataFrame containing all the table values. See above for an
    example of dictionary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query` (`str` or `List[str]`) — Query or list of queries that will be sent
    to the model alongside the table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sequential` (`bool`, *optional*, defaults to `False`) — Whether to do inference
    sequentially or as a batch. Batching is faster, but models like SQA require the
    inference to be done sequentially to extract relations within sequences, given
    their conversational nature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`bool`, `str` or `TapasTruncationStrategy`, *optional*, defaults
    to `False`) — Activates and controls truncation. Accepts the following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''drop_rows_to_fit''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate row by row, removing rows
    from the table.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A dictionary or a list of dictionaries containing results
  prefs: []
  type: TYPE_NORMAL
- en: 'Each result is a dictionary with the following keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`answer` (`str`) — The answer of the query given the table. If there is an
    aggregator, the answer will be preceded by `AGGREGATOR >`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`coordinates` (`List[Tuple[int, int]]`) — Coordinates of the cells of the answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cells` (`List[str]`) — List of strings made up of the answer cell values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aggregator` (`str`) — If the model has an aggregator, this returns the aggregator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Answers queries according to a table. The pipeline accepts several types of
    inputs which are detailed below:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pipeline(table, query)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline(table, [query])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline(table=table, query=query)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline(table=table, query=[query])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline({"table": table, "query": query})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline({"table": table, "query": [query]})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline([{"table": table, "query": query}, {"table": table, "query": query}])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `table` argument should be a dict or a DataFrame built from that dict,
    containing the whole table:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'This dictionary can be passed in as such, or can be converted to a pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: TextClassificationPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.TextClassificationPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_classification.py#L34)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_all_scores` (`bool`, *optional*, defaults to `False`) — Whether to
    return all prediction scores or just the one of the predicted class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function_to_apply` (`str`, *optional*, defaults to `"default"`) — The function
    to apply to the model outputs in order to retrieve the scores. Accepts four different
    values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"default"`: if the model has a single label, will apply the sigmoid function
    on the output. If the model has several labels, will apply the softmax function
    on the output.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"sigmoid"`: Applies the sigmoid function on the output.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"softmax"`: Applies the softmax function on the output.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"none"`: Does not apply any function on the output.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification pipeline using any `ModelForSequenceClassification`. See
    the [sequence classification examples](../task_summary#sequence-classification)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This text classification pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"sentiment-analysis"` (for classifying sequences
    according to positive or negative sentiments).'
  prefs: []
  type: TYPE_NORMAL
- en: If multiple classification labels are available (`model.config.num_labels >=
    2`), the pipeline will run a softmax over the results. If there is a single label,
    the pipeline will run a sigmoid over the result.
  prefs: []
  type: TYPE_NORMAL
- en: The models that this pipeline can use are models that have been fine-tuned on
    a sequence classification task. See the up-to-date list of available models on
    [huggingface.co/models](https://huggingface.co/models?filter=text-classification).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_classification.py#L122)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`args` (`str` or `List[str]` or `Dict[str]`, or `List[Dict[str]]`) — One or
    several texts to classify. In order to use text pairs for your classification,
    you can send a dictionary containing `{"text", "text_pair"}` keys, or a list of
    those.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_k` (`int`, *optional*, defaults to `1`) — How many results to return.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`function_to_apply` (`str`, *optional*, defaults to `"default"`) — The function
    to apply to the model outputs in order to retrieve the scores. Accepts four different
    values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If this argument is not specified, then it will apply the following functions
    according to the number of labels:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If the model has a single label, will apply the sigmoid function on the output.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model has several labels, will apply the softmax function on the output.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Possible values are:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`"sigmoid"`: Applies the sigmoid function on the output.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"softmax"`: Applies the softmax function on the output.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"none"`: Does not apply any function on the output.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A list or a list of list of `dict`
  prefs: []
  type: TYPE_NORMAL
- en: 'Each result comes as list of dictionaries with the following keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`label` (`str`) — The label predicted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score` (`float`) — The corresponding probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `top_k` is used, one such dictionary is returned per label.
  prefs: []
  type: TYPE_NORMAL
- en: Classify the text(s) given as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: TextGenerationPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.TextGenerationPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_generation.py#L23)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language generation pipeline using any `ModelWithLMHead`. This pipeline predicts
    the words that will follow a specified text prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial).
    You can pass text generation parameters to this pipeline to control stopping criteria,
    decoding strategy, and more. Learn more about text generation parameters in [Text
    generation strategies](../generation_strategies) and [Text generation](text_generation).
  prefs: []
  type: TYPE_NORMAL
- en: 'This language generation pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"text-generation"`.'
  prefs: []
  type: TYPE_NORMAL
- en: The models that this pipeline can use are models that have been trained with
    an autoregressive language modeling objective, which includes the uni-directional
    models in the library (e.g. gpt2). See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=text-generation).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text_generation.py#L178)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`args` (`str` or `List[str]`) — One or several prompts (or one list of prompts)
    to complete.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the tensors of predictions (as token indices) in the outputs. If set
    to `True`, the decoded text is not returned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_text` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return the decoded texts in the outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_full_text` (`bool`, *optional*, defaults to `True`) — If set to `False`
    only added text is returned, otherwise the full text is returned. Only meaningful
    if *return_text* is set to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `False`) —
    Whether or not to clean up the potential extra spaces in the text output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefix` (`str`, *optional*) — Prefix added to prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`handle_long_generation` (`str`, *optional*) — By default, this pipelines does
    not handle long generation (ones that exceed in one form or the other the model
    maximum length). There is no perfect way to adress this (more info :[https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227](https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227)).
    This provides common strategies to work around that problem depending on your
    use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`None` : default strategy where nothing in particular happens'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"hole"`: Truncates left of input, and leaves a gap wide enough to let generation
    happen (might truncate a lot of the prompt and not suitable when generation exceed
    the model capacity)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: generate_kwargs — Additional keyword arguments to pass along to the generate
    method of the model (see the generate method corresponding to your framework [here](./model#generative-models)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A list or a list of list of `dict`
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns one of the following dictionaries (cannot return a combination of both
    `generated_text` and `generated_token_ids`):'
  prefs: []
  type: TYPE_NORMAL
- en: '`generated_text` (`str`, present when `return_text=True`) — The generated text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generated_token_ids` (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`)
    — The token ids of the generated text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complete the prompt(s) given as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Text2TextGenerationPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.Text2TextGenerationPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L25)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for text to text generation using seq2seq models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial).
    You can pass text generation parameters to this pipeline to control stopping criteria,
    decoding strategy, and more. Learn more about text generation parameters in [Text
    generation strategies](../generation_strategies) and [Text generation](text_generation).
  prefs: []
  type: TYPE_NORMAL
- en: 'This Text2TextGenerationPipeline pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"text2text-generation"`.'
  prefs: []
  type: TYPE_NORMAL
- en: The models that this pipeline can use are models that have been fine-tuned on
    a translation task. See the up-to-date list of available models on [huggingface.co/models](https://huggingface.co/models?filter=text2text-generation).
    For a list of available parameters, see the [following documentation](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L138)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`args` (`str` or `List[str]`) — Input text for the encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`bool`, *optional*, defaults to `False`) — Whether or not
    to include the tensors of predictions (as token indices) in the outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_text` (`bool`, *optional*, defaults to `True`) — Whether or not to
    include the decoded texts in the outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `False`) —
    Whether or not to clean up the potential extra spaces in the text output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`TruncationStrategy`, *optional*, defaults to `TruncationStrategy.DO_NOT_TRUNCATE`)
    — The truncation strategy for the tokenization within the pipeline. `TruncationStrategy.DO_NOT_TRUNCATE`
    (default) will never truncate, but it is sometimes desirable to truncate the input
    to fit the model’s max_length instead of throwing an error down the line. generate_kwargs
    — Additional keyword arguments to pass along to the generate method of the model
    (see the generate method corresponding to your framework [here](./model#generative-models)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A list or a list of list of `dict`
  prefs: []
  type: TYPE_NORMAL
- en: 'Each result comes as a dictionary with the following keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`generated_text` (`str`, present when `return_text=True`) — The generated text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generated_token_ids` (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`)
    — The token ids of the generated text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate the output text(s) using text(s) given as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `check_inputs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L111)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: Checks whether there might be something wrong with given input with regard to
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: TokenClassificationPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.TokenClassificationPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L61)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ignore_labels` (`List[str]`, defaults to `["O"]`) — A list of labels to ignore.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`grouped_entities` (`bool`, *optional*, defaults to `False`) — DEPRECATED,
    use `aggregation_strategy` instead. Whether or not to group the tokens corresponding
    to the same entity together in the predictions or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stride` (`int`, *optional*) — If stride is provided, the pipeline is applied
    on all the text. The text is split into chunks of size model_max_length. Works
    only with fast tokenizers and `aggregation_strategy` different from `NONE`. The
    value of this argument defines the number of overlapping tokens between chunks.
    In other words, the model will shift forward by `tokenizer.model_max_length -
    stride` tokens each step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aggregation_strategy` (`str`, *optional*, defaults to `"none"`) — The strategy
    to fuse (or not) tokens based on the model prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“none” : Will simply not do any aggregation and simply return raw results from
    the model'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '“simple” : Will attempt to group entities following the default schema. (A,
    B-TAG), (B, I-TAG), (C, I-TAG), (D, B-TAG2) (E, B-TAG2) will end up being [{“word”:
    ABC, “entity”: “TAG”}, {“word”: “D”, “entity”: “TAG2”}, {“word”: “E”, “entity”:
    “TAG2”}] Notice that two consecutive B tags will end up as different entities.
    On word based languages, we might end up splitting words undesirably : Imagine
    Microsoft being tagged as [{“word”: “Micro”, “entity”: “ENTERPRISE”}, {“word”:
    “soft”, “entity”: “NAME”}]. Look for FIRST, MAX, AVERAGE for ways to mitigate
    that and disambiguate words (on languages that support that meaning, which is
    basically tokens separated by a space). These mitigations will only work on real
    words, “New york” might still be tagged with two different entities.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '“first” : (works only on word based models) Will use the `SIMPLE` strategy
    except that words, cannot end up with different tags. Words will simply use the
    tag of the first token of the word when there is ambiguity.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '“average” : (works only on word based models) Will use the `SIMPLE` strategy
    except that words, cannot end up with different tags. scores will be averaged
    first across tokens, and then the maximum label is applied.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '“max” : (works only on word based models) Will use the `SIMPLE` strategy except
    that words, cannot end up with different tags. Word entity will simply be the
    token with the maximum score.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Named Entity Recognition pipeline using any `ModelForTokenClassification`. See
    the [named entity recognition examples](../task_summary#named-entity-recognition)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This token recognition pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"ner"` (for predicting the classes of tokens
    in a sequence: person, organisation, location or miscellaneous).'
  prefs: []
  type: TYPE_NORMAL
- en: The models that this pipeline can use are models that have been fine-tuned on
    a token classification task. See the up-to-date list of available models on [huggingface.co/models](https://huggingface.co/models?filter=token-classification).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L219)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`inputs` (`str` or `List[str]`) — One or several texts (or one list of texts)
    for token classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A list or a list of list of `dict`
  prefs: []
  type: TYPE_NORMAL
- en: 'Each result comes as a list of dictionaries (one for each token in the corresponding
    input, or each entity if this pipeline was instantiated with an aggregation_strategy)
    with the following keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`word` (`str`) — The token/word classified. This is obtained by decoding the
    selected tokens. If you want to have the exact string in the original sentence,
    use `start` and `end`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score` (`float`) — The corresponding probability for `entity`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`entity` (`str`) — The entity predicted for that token/word (it is named *entity_group*
    when *aggregation_strategy* is not `"none"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index` (`int`, only present when `aggregation_strategy="none"`) — The index
    of the corresponding token in the sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start` (`int`, *optional*) — The index of the start of the corresponding entity
    in the sentence. Only exists if the offsets are available within the tokenizer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end` (`int`, *optional*) — The index of the end of the corresponding entity
    in the sentence. Only exists if the offsets are available within the tokenizer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classify each token of the text(s) given as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `aggregate_words`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L470)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Override tokens from a given word that disagree to force agreement on word boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: micro|soft| com|pany| B-ENT I-NAME I-ENT I-ENT will be rewritten with
    first strategy as microsoft| company| B-ENT I-ENT'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `gather_pre_entities`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L356)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Fuse various numpy arrays into dicts with all the information needed for aggregation
  prefs: []
  type: TYPE_NORMAL
- en: '#### `group_entities`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L533)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`entities` (`dict`) — The entities predicted by the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find and group together the adjacent tokens with the same entity predicted.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `group_sub_entities`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/token_classification.py#L498)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`entities` (`dict`) — The entities predicted by the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Group together the adjacent tokens with the same entity predicted.
  prefs: []
  type: TYPE_NORMAL
- en: TranslationPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.TranslationPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L286)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translates from one language to another.
  prefs: []
  type: TYPE_NORMAL
- en: 'This translation pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"translation_xx_to_yy"`.'
  prefs: []
  type: TYPE_NORMAL
- en: The models that this pipeline can use are models that have been fine-tuned on
    a translation task. See the up-to-date list of available models on [huggingface.co/models](https://huggingface.co/models?filter=translation).
    For a list of available parameters, see the [following documentation](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/text2text_generation.py#L341)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`args` (`str` or `List[str]`) — Texts to be translated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`bool`, *optional*, defaults to `False`) — Whether or not
    to include the tensors of predictions (as token indices) in the outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_text` (`bool`, *optional*, defaults to `True`) — Whether or not to
    include the decoded texts in the outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `False`) —
    Whether or not to clean up the potential extra spaces in the text output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`src_lang` (`str`, *optional*) — The language of the input. Might be required
    for multilingual models. Will not have any effect for single pair translation
    models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tgt_lang` (`str`, *optional*) — The language of the desired output. Might
    be required for multilingual models. Will not have any effect for single pair
    translation models generate_kwargs — Additional keyword arguments to pass along
    to the generate method of the model (see the generate method corresponding to
    your framework [here](./model#generative-models)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A list or a list of list of `dict`
  prefs: []
  type: TYPE_NORMAL
- en: 'Each result comes as a dictionary with the following keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`translation_text` (`str`, present when `return_text=True`) — The translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`translation_token_ids` (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`)
    — The token ids of the translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translate the text(s) given as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: ZeroShotClassificationPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.ZeroShotClassificationPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_classification.py#L46)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLI-based zero-shot classification pipeline using a `ModelForSequenceClassification`
    trained on NLI (natural language inference) tasks. Equivalent of `text-classification`
    pipelines, but these models don’t require a hardcoded number of potential classes,
    they can be chosen at runtime. It usually means it’s slower but it is **much**
    more flexible.
  prefs: []
  type: TYPE_NORMAL
- en: Any combination of sequences and labels can be passed and each combination will
    be posed as a premise/hypothesis pair and passed to the pretrained model. Then,
    the logit for *entailment* is taken as the logit for the candidate label being
    valid. Any NLI model can be used, but the id of the *entailment* label must be
    included in the model config’s :attr:*~transformers.PretrainedConfig.label2id*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This NLI pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"zero-shot-classification"`.'
  prefs: []
  type: TYPE_NORMAL
- en: The models that this pipeline can use are models that have been fine-tuned on
    an NLI task. See the up-to-date list of available models on [huggingface.co/models](https://huggingface.co/models?search=nli).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/zero_shot_classification.py#L163)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`str` or `List[str]`) — The sequence(s) to classify, will be truncated
    if the model input is too large.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`candidate_labels` (`str` or `List[str]`) — The set of possible class labels
    to classify each sequence into. Can be a single label, a string of comma-separated
    labels, or a list of labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hypothesis_template` (`str`, *optional*, defaults to `"This example is {}."`)
    — The template used to turn each label into an NLI-style hypothesis. This template
    must include a {} or similar syntax for the candidate label to be inserted into
    the template. For example, the default template is `"This example is {}."` With
    the candidate label `"sports"`, this would be fed into the model like `"<cls>
    sequence to classify <sep> This example is sports . <sep>"`. The default template
    works well in many cases, but it may be worthwhile to experiment with different
    templates depending on the task setting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multi_label` (`bool`, *optional*, defaults to `False`) — Whether or not multiple
    candidate labels can be true. If `False`, the scores are normalized such that
    the sum of the label likelihoods for each sequence is 1\. If `True`, the labels
    are considered independent and probabilities are normalized for each candidate
    by doing a softmax of the entailment score vs. the contradiction score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A `dict` or a list of `dict`
  prefs: []
  type: TYPE_NORMAL
- en: 'Each result comes as a dictionary with the following keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sequence` (`str`) — The sequence for which this is the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`List[str]`) — The labels sorted by order of likelihood.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`List[float]`) — The probabilities for each of the labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classify the sequence(s) given as inputs. See the [ZeroShotClassificationPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.ZeroShotClassificationPipeline)
    documentation for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pipelines available for multimodal tasks include the following.
  prefs: []
  type: TYPE_NORMAL
- en: DocumentQuestionAnsweringPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.DocumentQuestionAnsweringPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/document_question_answering.py#L101)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document Question Answering pipeline using any `AutoModelForDocumentQuestionAnswering`.
    The inputs/outputs are similar to the (extractive) question answering pipeline;
    however, the pipeline takes an image (and optional OCR’d words/boxes) as input
    instead of text context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This document question answering pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"document-question-answering"`.'
  prefs: []
  type: TYPE_NORMAL
- en: The models that this pipeline can use are models that have been fine-tuned on
    a document question answering task. See the up-to-date list of available models
    on [huggingface.co/models](https://huggingface.co/models?filter=document-question-answering).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/document_question_answering.py#L194)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`image` (`str` or `PIL.Image`) — The pipeline handles three types of images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a http link pointing to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a local path to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An image loaded in PIL directly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The pipeline accepts either a single image or a batch of images. If given a
    single image, it can be broadcasted to multiple questions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`question` (`str`) — A question to ask of the document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`word_boxes` (`List[str, Tuple[float, float, float, float]]`, *optional*) —
    A list of words and bounding boxes (normalized 0->1000). If you provide this optional
    input, then the pipeline will use these words and boxes instead of running OCR
    on the image to derive them for models that need them (e.g. LayoutLM). This allows
    you to reuse OCR’d results across many invocations of the pipeline without having
    to re-run it each time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_k` (`int`, *optional*, defaults to 1) — The number of answers to return
    (will be chosen by order of likelihood). Note that we return less than top_k answers
    if there are not enough options available within the context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_stride` (`int`, *optional*, defaults to 128) — If the words in the document
    are too long to fit with the question for the model, it will be split in several
    chunks with some overlap. This argument controls the size of that overlap.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_answer_len` (`int`, *optional*, defaults to 15) — The maximum length of
    predicted answers (e.g., only answers with a shorter length are considered).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_seq_len` (`int`, *optional*, defaults to 384) — The maximum length of
    the total sentence (context + question) in tokens of each chunk passed to the
    model. The context will be split in several chunks (using `doc_stride` as overlap)
    if needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_question_len` (`int`, *optional*, defaults to 64) — The maximum length
    of the question after tokenization. It will be truncated if needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`handle_impossible_answer` (`bool`, *optional*, defaults to `False`) — Whether
    or not we accept impossible as an answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lang` (`str`, *optional*) — Language to use while running OCR. Defaults to
    english.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tesseract_config` (`str`, *optional*) — Additional flags to pass to tesseract
    while running OCR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A `dict` or a list of `dict`
  prefs: []
  type: TYPE_NORMAL
- en: 'Each result comes as a dictionary with the following keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`score` (`float`) — The probability associated to the answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start` (`int`) — The start word index of the answer (in the OCR’d version
    of the input or provided `word_boxes`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end` (`int`) — The end word index of the answer (in the OCR’d version of the
    input or provided `word_boxes`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`answer` (`str`) — The answer to the question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`words` (`list[int]`) — The index of each word/box pair that is in the answer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answer the question(s) given as inputs by using the document(s). A document
    is defined as an image and an optional list of (word, box) tuples which represent
    the text in the document. If the `word_boxes` are not provided, it will use the
    Tesseract OCR engine (if available) to extract the words and boxes automatically
    for LayoutLM-like models which require them as input. For Donut, no OCR is run.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can invoke the pipeline several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pipeline(image=image, question=question)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline(image=image, question=question, word_boxes=word_boxes)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline([{"image": image, "question": question}])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline([{"image": image, "question": question, "word_boxes": word_boxes}])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FeatureExtractionPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.FeatureExtractionPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/feature_extraction.py#L7)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_tensors` (`bool`, *optional*) — If `True`, returns a tensor according
    to the specified framework, otherwise returns a list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenize_kwargs` (`dict`, *optional*) — Additional dictionary of keyword arguments
    passed along to the tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction pipeline using no model head. This pipeline extracts the
    hidden states from the base transformer, which can be used as features in downstream
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This feature extraction pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the task identifier: `"feature-extraction"`.'
  prefs: []
  type: TYPE_NORMAL
- en: All models may be used for this pipeline. See a list of all models, including
    community-contributed models on [huggingface.co/models](https://huggingface.co/models).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/feature_extraction.py#L96)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`args` (`str` or `List[str]`) — One or several texts (or one list of texts)
    to get the features of.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A nested list of `float`
  prefs: []
  type: TYPE_NORMAL
- en: The features computed by the model.
  prefs: []
  type: TYPE_NORMAL
- en: Extract the features of the input(s).
  prefs: []
  type: TYPE_NORMAL
- en: ImageToTextPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.ImageToTextPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_to_text.py#L30)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image To Text pipeline using a `AutoModelForVision2Seq`. This pipeline predicts
    a caption for a given image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This image to text pipeline can currently be loaded from pipeline() using the
    following task identifier: “image-to-text”.'
  prefs: []
  type: TYPE_NORMAL
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?pipeline_tag=image-to-text).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/image_to_text.py#L83)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — The pipeline
    handles three types of images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a HTTP(s) link pointing to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a local path to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An image loaded in PIL directly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The pipeline accepts either a single image or a batch of images.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`max_new_tokens` (`int`, *optional*) — The amount of maximum tokens to generate.
    By default it will use `generate` default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generate_kwargs` (`Dict`, *optional*) — Pass it to send all of these arguments
    directly to `generate` allowing full control of this function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A list or a list of list of `dict`
  prefs: []
  type: TYPE_NORMAL
- en: 'Each result comes as a dictionary with the following key:'
  prefs: []
  type: TYPE_NORMAL
- en: '`generated_text` (`str`) — The generated text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign labels to the image(s) passed as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: MaskGenerationPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.MaskGenerationPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/mask_generation.py#L22)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature_extractor` ([SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor))
    — The feature extractor that will be used by the pipeline to encode the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`points_per_batch` (*optional*, int, default to 64) — Sets the number of points
    run simultaneously by the model. Higher numbers may be faster but use more GPU
    memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_bboxes_mask` (`bool`, *optional*, default to `False`) — Whether or
    not to output the bounding box predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_rle_masks` (`bool`, *optional*, default to `False`) — Whether or not
    to output the masks in `RLE` format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic mask generation for images using `SamForMaskGeneration`. This pipeline
    predicts binary masks for an image, given an image. It is a `ChunkPipeline` because
    you can seperate the points in a mini-batch in order to avoid OOM issues. Use
    the `points_per_batch` argument to control the number of points that will be processed
    at the same time. Default is `64`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline works in 3 steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`preprocess`: A grid of 1024 points evenly separated is generated along with
    bounding boxes and point labels. For more details on how the points and bounding
    boxes are created, check the `_generate_crop_boxes` function. The image is also
    preprocessed using the `image_processor`. This function `yields` a minibatch of
    `points_per_batch`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`forward`: feeds the outputs of `preprocess` to the model. The image embedding
    is computed only once. Calls both `self.model.get_image_embeddings` and makes
    sure that the gradients are not computed, and the tensors and models are on the
    same device.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`postprocess`: The most important part of the automatic mask generation happens
    here. Three steps are induced:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'image_processor.postprocess_masks (run on each minibatch loop): takes in the
    raw output masks, resizes them according to the image size, and transforms there
    to binary masks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'image_processor.filter_masks (on each minibatch loop): uses both `pred_iou_thresh`
    and `stability_scores`. Also applies a variety of filters based on non maximum
    suppression to remove bad masks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: image_processor.postprocess_masks_for_amg applies the NSM on the mask to only
    keep relevant ones.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This segmentation pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifier: `"mask-generation"`.'
  prefs: []
  type: TYPE_NORMAL
- en: See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=mask-generation).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/mask_generation.py#L135)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`inputs` (`np.ndarray` or `bytes` or `str` or `dict`) — Image or list of images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.0) — Threshold to use
    when turning the predicted masks into binary values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pred_iou_thresh` (`float`, *optional*, defaults to 0.88) — A filtering threshold
    in `[0,1]` applied on the model’s predicted mask quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stability_score_thresh` (`float`, *optional*, defaults to 0.95) — A filtering
    threshold in `[0,1]`, using the stability of the mask under changes to the cutoff
    used to binarize the model’s mask predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stability_score_offset` (`int`, *optional*, defaults to 1) — The amount to
    shift the cutoff when calculated the stability score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crops_nms_thresh` (`float`, *optional*, defaults to 0.7) — The box IoU cutoff
    used by non-maximal suppression to filter duplicate masks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crops_n_layers` (`int`, *optional*, defaults to 0) — If `crops_n_layers>0`,
    mask prediction will be run again on crops of the image. Sets the number of layers
    to run, where each layer has 2**i_layer number of image crops.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crop_overlap_ratio` (`float`, *optional*, defaults to `512 / 1500`) — Sets
    the degree to which crops overlap. In the first crop layer, crops will overlap
    by this fraction of the image length. Later layers with more crops scale down
    this overlap.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crop_n_points_downscale_factor` (`int`, *optional*, defaults to `1`) — The
    number of points-per-side sampled in layer n is scaled down by crop_n_points_downscale_factor**n.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A dictionary with the following keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mask` (`PIL.Image`) — A binary mask of the detected object as a PIL Image
    of shape `(width, height)` of the original image. Returns a mask filled with zeros
    if no object is found.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score` (*optional* `float`) — Optionally, when the model is capable of estimating
    a confidence of the “object” described by the label and the mask.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates binary segmentation masks
  prefs: []
  type: TYPE_NORMAL
- en: VisualQuestionAnsweringPipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.VisualQuestionAnsweringPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/visual_question_answering.py#L18)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual Question Answering pipeline using a `AutoModelForVisualQuestionAnswering`.
    This pipeline is currently only available in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)
  prefs: []
  type: TYPE_NORMAL
- en: 'This visual question answering pipeline can currently be loaded from [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)
    using the following task identifiers: `"visual-question-answering", "vqa"`.'
  prefs: []
  type: TYPE_NORMAL
- en: The models that this pipeline can use are models that have been fine-tuned on
    a visual question answering task. See the up-to-date list of available models
    on [huggingface.co/models](https://huggingface.co/models?filter=visual-question-answering).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/visual_question_answering.py#L70)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`image` (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`) — The pipeline
    handles three types of images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a http link pointing to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A string containing a local path to an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An image loaded in PIL directly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The pipeline accepts either a single image or a batch of images. If given a
    single image, it can be broadcasted to multiple questions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`question` (`str`, `List[str]`) — The question(s) asked. If given a single
    question, it can be broadcasted to multiple images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_k` (`int`, *optional*, defaults to 5) — The number of top labels that
    will be returned by the pipeline. If the provided number is higher than the number
    of labels available in the model configuration, it will default to the number
    of labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (`float`, *optional*, defaults to None) — The maximum time in seconds
    to wait for fetching images from the web. If None, no timeout is set and the call
    may block forever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A dictionary or a list of dictionaries containing the result. The dictionaries
    contain the following keys
  prefs: []
  type: TYPE_NORMAL
- en: '`label` (`str`) — The label identified by the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score` (`int`) — The score attributed by the model for that label.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Answers open-ended questions about images. The pipeline accepts several types
    of inputs which are detailed below:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pipeline(image=image, question=question)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline({"image": image, "question": question})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline([{"image": image, "question": question}])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline([{"image": image, "question": question}, {"image": image, "question":
    question}])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parent class: Pipeline'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.Pipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L749)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    or [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — The model that will be used by the pipeline to make predictions. This needs
    to be a model inheriting from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    for PyTorch and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    for TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that will be used by the pipeline to encode data for the model.
    This object inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelcard` (`str` or `ModelCard`, *optional*) — Model card attributed to the
    model for this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework` (`str`, *optional*) — The framework to use, either `"pt"` for PyTorch
    or `"tf"` for TensorFlow. The specified framework must be installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no framework is specified, will default to the one currently installed. If
    no framework is specified and both frameworks are installed, will default to the
    framework of the `model`, or to PyTorch if no model is provided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`task` (`str`, defaults to `""`) — A task-identifier for the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers` (`int`, *optional*, defaults to 8) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number
    of workers to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional*, defaults to 1) — When the pipeline will use
    *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
    the batch to use, for inference this is not always beneficial, please read [Batching
    with pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args_parser` ([ArgumentHandler](/docs/transformers/v4.37.2/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler),
    *optional*) — Reference to the object in charge of parsing supplied pipeline parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`int`, *optional*, defaults to -1) — Device ordinal for CPU/GPU supports.
    Setting this to -1 will leverage CPU, a positive will run the model on the associated
    CUDA device id. You can pass native `torch.device` or a `str` too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binary_output` (`bool`, *optional*, defaults to `False`) — Flag indicating
    if the output the pipeline should happen in a binary format (i.e., pickle) or
    as raw text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Pipeline class is the class from which all pipelines inherit. Refer to this
    class for methods shared across different pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Base class implementing pipelined operations. Pipeline workflow is defined
    as a sequence of the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Input -> Tokenization -> Model Inference -> Post-Processing (task dependent)
    -> Output
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline supports running on CPU or GPU through the device argument (see below).
  prefs: []
  type: TYPE_NORMAL
- en: Some pipeline, like for instance [FeatureExtractionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.FeatureExtractionPipeline)
    (`'feature-extraction'`) output large tensor object as nested-lists. In order
    to avoid dumping such large structure as textual data we provide the `binary_output`
    constructor argument. If set to `True`, the output will be stored in the pickle
    format.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `check_model_type`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L984)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`supported_models` (`List[str]` or `dict`) — The list of models supported by
    the pipeline, or a dictionary with model class values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check if the model class is in supported by the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `device_placement`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L923)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: Context Manager allowing tensor allocation on the user-specified device in framework
    agnostic way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: '#### `ensure_tensor_on_device`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L950)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`inputs` (keyword arguments that should be `torch.Tensor`, the rest is ignored)
    — The tensors to place on `self.device`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Recursive` on lists **only**. —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict[str, torch.Tensor]`'
  prefs: []
  type: TYPE_NORMAL
- en: The same as `inputs` but on the proper device.
  prefs: []
  type: TYPE_NORMAL
- en: Ensure PyTorch tensors are on the specified device.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `postprocess`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L1047)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: Postprocess will receive the raw outputs of the `_forward` method, generally
    tensors, and reformat them into something more friendly. Generally it will output
    a list or a dict or results (containing just strings and numbers).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `predict`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L917)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: Scikit / Keras interface to transformers’ pipelines. This method will forward
    to **call**().
  prefs: []
  type: TYPE_NORMAL
- en: '#### `preprocess`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L1026)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: Preprocess will take the `input_` of a specific pipeline and return a dictionary
    of everything necessary for `_forward` to run properly. It should contain at least
    one tensor, but might have arbitrary other items.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L861)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`save_directory` (`str`) — A path to the directory where to saved. It will
    be created if it doesn’t exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safe_serialization` (`str`) — Whether to save the model using `safetensors`
    or the traditional way for PyTorch or Tensorflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the pipeline’s model and tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `transform`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/pipelines/base.py#L911)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: Scikit / Keras interface to transformers’ pipelines. This method will forward
    to **call**().
  prefs: []
  type: TYPE_NORMAL
