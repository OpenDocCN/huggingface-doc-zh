["```py\npip install spacy ftfy==4.4.3\npython -m spacy download en\n```", "```py\n>>> from transformers import OpenAIGPTConfig, OpenAIGPTModel\n\n>>> # Initializing a GPT configuration\n>>> configuration = OpenAIGPTConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = OpenAIGPTModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoTokenizer, OpenAIGPTModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = OpenAIGPTModel.from_pretrained(\"openai-gpt\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, OpenAIGPTLMHeadModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = OpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, OpenAIGPTDoubleHeadsModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = OpenAIGPTDoubleHeadsModel.from_pretrained(\"openai-gpt\")\n>>> tokenizer.add_special_tokens(\n...     {\"cls_token\": \"[CLS]\"}\n... )  # Add a [CLS] to the vocabulary (we should train it also!)\n>>> model.resize_token_embeddings(len(tokenizer))\n\n>>> choices = [\"Hello, my dog is cute [CLS]\", \"Hello, my cat is cute [CLS]\"]\n>>> input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\n>>> mc_token_ids = torch.tensor([input_ids.size(-1) - 1, input_ids.size(-1) - 1]).unsqueeze(0)  # Batch size 1\n\n>>> outputs = model(input_ids, mc_token_ids=mc_token_ids)\n>>> lm_logits = outputs.logits\n>>> mc_logits = outputs.mc_logits\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, OpenAIGPTForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = OpenAIGPTForSequenceClassification.from_pretrained(\"openai-gpt\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = OpenAIGPTForSequenceClassification.from_pretrained(\"openai-gpt\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, OpenAIGPTForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = OpenAIGPTForSequenceClassification.from_pretrained(\"openai-gpt\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = OpenAIGPTForSequenceClassification.from_pretrained(\n...     \"openai-gpt\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> from transformers import AutoTokenizer, TFOpenAIGPTModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = TFOpenAIGPTModel.from_pretrained(\"openai-gpt\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, TFOpenAIGPTLMHeadModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n>>> logits = outputs.logits\n```", "```py\n>>> import tensorflow as tf\n>>> from transformers import AutoTokenizer, TFOpenAIGPTDoubleHeadsModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = TFOpenAIGPTDoubleHeadsModel.from_pretrained(\"openai-gpt\")\n\n>>> # Add a [CLS] to the vocabulary (we should train it also!)\n>>> tokenizer.add_special_tokens({\"cls_token\": \"[CLS]\"})\n>>> model.resize_token_embeddings(len(tokenizer))  # Update the model embeddings with the new vocabulary size\n>>> print(tokenizer.cls_token_id, len(tokenizer))  # The newly token the last token of the vocabulary\n\n>>> choices = [\"Hello, my dog is cute [CLS]\", \"Hello, my cat is cute [CLS]\"]\n>>> encoding = tokenizer(choices, return_tensors=\"tf\")\n>>> inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}\n>>> inputs[\"mc_token_ids\"] = tf.constant(\n...     [inputs[\"input_ids\"].shape[-1] - 1, inputs[\"input_ids\"].shape[-1] - 1]\n... )[\n...     None, :\n... ]  # Batch size 1\n>>> outputs = model(inputs)\n>>> lm_prediction_scores, mc_prediction_scores = outputs[:2]\n```", "```py\n>>> from transformers import AutoTokenizer, TFOpenAIGPTForSequenceClassification\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = TFOpenAIGPTForSequenceClassification.from_pretrained(\"openai-gpt\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n\n>>> logits = model(**inputs).logits\n\n>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n```", "```py\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = TFOpenAIGPTForSequenceClassification.from_pretrained(\"openai-gpt\", num_labels=num_labels)\n\n>>> labels = tf.constant(1)\n>>> loss = model(**inputs, labels=labels).loss\n```"]