["```py\npip install spacy ftfy==4.4.3\npython -m spacy download en\n```", "```py\n( vocab_size = 40478 n_positions = 512 n_embd = 768 n_layer = 12 n_head = 12 afn = 'gelu' resid_pdrop = 0.1 embd_pdrop = 0.1 attn_pdrop = 0.1 layer_norm_epsilon = 1e-05 initializer_range = 0.02 summary_type = 'cls_index' summary_use_proj = True summary_activation = None summary_proj_to_labels = True summary_first_dropout = 0.1 **kwargs )\n```", "```py\n>>> from transformers import OpenAIGPTConfig, OpenAIGPTModel\n\n>>> # Initializing a GPT configuration\n>>> configuration = OpenAIGPTConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = OpenAIGPTModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file merges_file unk_token = '<unk>' **kwargs )\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( vocab_file = None merges_file = None tokenizer_file = None unk_token = '<unk>' **kwargs )\n```", "```py\n( loss: Optional = None mc_loss: Optional = None logits: FloatTensor = None mc_logits: FloatTensor = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( logits: tf.Tensor = None mc_logits: tf.Tensor = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, OpenAIGPTModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = OpenAIGPTModel.from_pretrained(\"openai-gpt\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, OpenAIGPTLMHeadModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = OpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None mc_token_ids: Optional = None labels: Optional = None mc_labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, OpenAIGPTDoubleHeadsModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = OpenAIGPTDoubleHeadsModel.from_pretrained(\"openai-gpt\")\n>>> tokenizer.add_special_tokens(\n...     {\"cls_token\": \"[CLS]\"}\n... )  # Add a [CLS] to the vocabulary (we should train it also!)\n>>> model.resize_token_embeddings(len(tokenizer))\n\n>>> choices = [\"Hello, my dog is cute [CLS]\", \"Hello, my cat is cute [CLS]\"]\n>>> input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\n>>> mc_token_ids = torch.tensor([input_ids.size(-1) - 1, input_ids.size(-1) - 1]).unsqueeze(0)  # Batch size 1\n\n>>> outputs = model(input_ids, mc_token_ids=mc_token_ids)\n>>> lm_logits = outputs.logits\n>>> mc_logits = outputs.mc_logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, OpenAIGPTForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = OpenAIGPTForSequenceClassification.from_pretrained(\"openai-gpt\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = OpenAIGPTForSequenceClassification.from_pretrained(\"openai-gpt\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, OpenAIGPTForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = OpenAIGPTForSequenceClassification.from_pretrained(\"openai-gpt\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = OpenAIGPTForSequenceClassification.from_pretrained(\n...     \"openai-gpt\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFOpenAIGPTModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = TFOpenAIGPTModel.from_pretrained(\"openai-gpt\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFCausalLMOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFOpenAIGPTLMHeadModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n>>> logits = outputs.logits\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None mc_token_ids: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> import tensorflow as tf\n>>> from transformers import AutoTokenizer, TFOpenAIGPTDoubleHeadsModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = TFOpenAIGPTDoubleHeadsModel.from_pretrained(\"openai-gpt\")\n\n>>> # Add a [CLS] to the vocabulary (we should train it also!)\n>>> tokenizer.add_special_tokens({\"cls_token\": \"[CLS]\"})\n>>> model.resize_token_embeddings(len(tokenizer))  # Update the model embeddings with the new vocabulary size\n>>> print(tokenizer.cls_token_id, len(tokenizer))  # The newly token the last token of the vocabulary\n\n>>> choices = [\"Hello, my dog is cute [CLS]\", \"Hello, my cat is cute [CLS]\"]\n>>> encoding = tokenizer(choices, return_tensors=\"tf\")\n>>> inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}\n>>> inputs[\"mc_token_ids\"] = tf.constant(\n...     [inputs[\"input_ids\"].shape[-1] - 1, inputs[\"input_ids\"].shape[-1] - 1]\n... )[\n...     None, :\n... ]  # Batch size 1\n>>> outputs = model(inputs)\n>>> lm_prediction_scores, mc_prediction_scores = outputs[:2]\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSequenceClassifierOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFOpenAIGPTForSequenceClassification\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n>>> model = TFOpenAIGPTForSequenceClassification.from_pretrained(\"openai-gpt\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n\n>>> logits = model(**inputs).logits\n\n>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n```", "```py\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = TFOpenAIGPTForSequenceClassification.from_pretrained(\"openai-gpt\", num_labels=num_labels)\n\n>>> labels = tf.constant(1)\n>>> loss = model(**inputs, labels=labels).loss\n```"]