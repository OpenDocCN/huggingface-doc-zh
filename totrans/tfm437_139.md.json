["```py\n( vocab_size = 50358 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu_new' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 4096 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 use_cache = True pad_token_id = 0 bos_token_id = 1 eos_token_id = 2 sep_token_id = 66 attention_type = 'block_sparse' use_bias = True rescale_embeddings = False block_size = 64 num_random_blocks = 3 classifier_dropout = None **kwargs )\n```", "```py\n>>> from transformers import BigBirdConfig, BigBirdModel\n\n>>> # Initializing a BigBird google/bigbird-roberta-base style configuration\n>>> configuration = BigBirdConfig()\n\n>>> # Initializing a model (with random weights) from the google/bigbird-roberta-base style configuration\n>>> model = BigBirdModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file unk_token = '<unk>' bos_token = '<s>' eos_token = '</s>' pad_token = '<pad>' sep_token = '[SEP]' mask_token = '[MASK]' cls_token = '[CLS]' sp_model_kwargs: Optional = None **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( vocab_file = None tokenizer_file = None unk_token = '<unk>' bos_token = '<s>' eos_token = '</s>' pad_token = '<pad>' sep_token = '[SEP]' mask_token = '[MASK]' cls_token = '[CLS]' **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence |\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( loss: Optional = None prediction_logits: FloatTensor = None seq_relationship_logits: FloatTensor = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( config add_pooling_layer = True )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, BigBirdModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n>>> model = BigBirdModel.from_pretrained(\"google/bigbird-roberta-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None next_sentence_label: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.big_bird.modeling_big_bird.BigBirdForPreTrainingOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, BigBirdForPreTraining\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n>>> model = BigBirdForPreTraining.from_pretrained(\"google/bigbird-roberta-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> prediction_logits = outputs.prediction_logits\n>>> seq_relationship_logits = outputs.seq_relationship_logits\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, BigBirdForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n>>> model = BigBirdForCausalLM.from_pretrained(\"google/bigbird-roberta-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.MaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, BigBirdForMaskedLM\n>>> from datasets import load_dataset\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n>>> model = BigBirdForMaskedLM.from_pretrained(\"google/bigbird-roberta-base\")\n>>> squad_ds = load_dataset(\"squad_v2\", split=\"train\")\n>>> # select random long article\n>>> LONG_ARTICLE_TARGET = squad_ds[81514][\"context\"]\n>>> # select random sentence\n>>> LONG_ARTICLE_TARGET[332:398]\n'the highest values are very close to the theoretical maximum value'\n\n>>> # add mask_token\n>>> LONG_ARTICLE_TO_MASK = LONG_ARTICLE_TARGET.replace(\"maximum\", \"[MASK]\")\n>>> inputs = tokenizer(LONG_ARTICLE_TO_MASK, return_tensors=\"pt\")\n>>> # long article input\n>>> list(inputs[\"input_ids\"].shape)\n[1, 919]\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n>>> # retrieve index of [MASK]\n>>> mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n>>> predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n>>> tokenizer.decode(predicted_token_id)\n'maximum'\n```", "```py\n>>> labels = tokenizer(LONG_ARTICLE_TARGET, return_tensors=\"pt\")[\"input_ids\"]\n>>> labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n>>> outputs = model(**inputs, labels=labels)\n>>> round(outputs.loss.item(), 2)\n1.99\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, BigBirdForSequenceClassification\n>>> from datasets import load_dataset\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"l-yohai/bigbird-roberta-base-mnli\")\n>>> model = BigBirdForSequenceClassification.from_pretrained(\"l-yohai/bigbird-roberta-base-mnli\")\n>>> squad_ds = load_dataset(\"squad_v2\", split=\"train\")\n>>> LONG_ARTICLE = squad_ds[81514][\"context\"]\n>>> inputs = tokenizer(LONG_ARTICLE, return_tensors=\"pt\")\n>>> # long input article\n>>> list(inputs[\"input_ids\"].shape)\n[1, 919]\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n>>> predicted_class_id = logits.argmax().item()\n>>> model.config.id2label[predicted_class_id]\n'LABEL_0'\n```", "```py\n>>> num_labels = len(model.config.id2label)\n>>> model = BigBirdForSequenceClassification.from_pretrained(\n...     \"l-yohai/bigbird-roberta-base-mnli\", num_labels=num_labels\n... )\n>>> labels = torch.tensor(1)\n>>> loss = model(**inputs, labels=labels).loss\n>>> round(loss.item(), 2)\n1.13\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.MultipleChoiceModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, BigBirdForMultipleChoice\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n>>> model = BigBirdForMultipleChoice.from_pretrained(\"google/bigbird-roberta-base\")\n\n>>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n>>> choice0 = \"It is eaten with a fork and a knife.\"\n>>> choice1 = \"It is eaten while held in the hand.\"\n>>> labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n\n>>> encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"pt\", padding=True)\n>>> outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1\n\n>>> # the linear classifier still needs to be trained\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, BigBirdForTokenClassification\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n>>> model = BigBirdForTokenClassification.from_pretrained(\"google/bigbird-roberta-base\")\n\n>>> inputs = tokenizer(\n...     \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n... )\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_token_class_ids = logits.argmax(-1)\n\n>>> # Note that tokens are classified rather then input words which means that\n>>> # there might be more predicted token classes than words.\n>>> # Multiple token classes might account for the same word\n>>> predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n\n>>> labels = predicted_token_class_ids\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config add_pooling_layer = False )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None question_lengths: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.big_bird.modeling_big_bird.BigBirdForQuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, BigBirdForQuestionAnswering\n>>> from datasets import load_dataset\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n>>> model = BigBirdForQuestionAnswering.from_pretrained(\"google/bigbird-roberta-base\")\n>>> squad_ds = load_dataset(\"squad_v2\", split=\"train\")\n>>> # select random article and question\n>>> LONG_ARTICLE = squad_ds[81514][\"context\"]\n>>> QUESTION = squad_ds[81514][\"question\"]\n>>> QUESTION\n'During daytime how high can the temperatures reach?'\n\n>>> inputs = tokenizer(QUESTION, LONG_ARTICLE, return_tensors=\"pt\")\n>>> # long article and question input\n>>> list(inputs[\"input_ids\"].shape)\n[1, 929]\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n>>> predict_answer_token_ids = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n>>> predict_answer_token = tokenizer.decode(predict_answer_token_ids)\n```", "```py\n>>> target_start_index, target_end_index = torch.tensor([130]), torch.tensor([132])\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = outputs.loss\n```", "```py\n( config: BigBirdConfig input_shape: Optional = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )\n```", "```py\n( input_ids attention_mask = None token_type_ids = None position_ids = None head_mask = None encoder_hidden_states = None encoder_attention_mask = None params: dict = None dropout_rng: Optional = None indices_rng: Optional = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None past_key_values: dict = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBigBirdModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n>>> model = FlaxBigBirdModel.from_pretrained(\"google/bigbird-roberta-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: BigBirdConfig input_shape: Optional = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )\n```", "```py\n( input_ids attention_mask = None token_type_ids = None position_ids = None head_mask = None encoder_hidden_states = None encoder_attention_mask = None params: dict = None dropout_rng: Optional = None indices_rng: Optional = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None past_key_values: dict = None ) \u2192 export const metadata = 'undefined';transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForPreTrainingOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBigBirdForPreTraining\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n>>> model = FlaxBigBirdForPreTraining.from_pretrained(\"google/bigbird-roberta-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"np\")\n>>> outputs = model(**inputs)\n\n>>> prediction_logits = outputs.prediction_logits\n>>> seq_relationship_logits = outputs.seq_relationship_logits\n```", "```py\n( config: BigBirdConfig input_shape: Optional = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )\n```", "```py\n( input_ids attention_mask = None token_type_ids = None position_ids = None head_mask = None encoder_hidden_states = None encoder_attention_mask = None params: dict = None dropout_rng: Optional = None indices_rng: Optional = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None past_key_values: dict = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBigBirdForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n>>> model = FlaxBigBirdForCausalLM.from_pretrained(\"google/bigbird-roberta-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"np\")\n>>> outputs = model(**inputs)\n\n>>> # retrieve logts for next token\n>>> next_token_logits = outputs.logits[:, -1]\n```", "```py\n( config: BigBirdConfig input_shape: Optional = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )\n```", "```py\n( input_ids attention_mask = None token_type_ids = None position_ids = None head_mask = None encoder_hidden_states = None encoder_attention_mask = None params: dict = None dropout_rng: Optional = None indices_rng: Optional = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None past_key_values: dict = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxMaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBigBirdForMaskedLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n>>> model = FlaxBigBirdForMaskedLM.from_pretrained(\"google/bigbird-roberta-base\")\n\n>>> inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"jax\")\n\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n```", "```py\n( config: BigBirdConfig input_shape: Optional = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )\n```", "```py\n( input_ids attention_mask = None token_type_ids = None position_ids = None head_mask = None encoder_hidden_states = None encoder_attention_mask = None params: dict = None dropout_rng: Optional = None indices_rng: Optional = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None past_key_values: dict = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBigBirdForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n>>> model = FlaxBigBirdForSequenceClassification.from_pretrained(\"google/bigbird-roberta-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n```", "```py\n( config: BigBirdConfig input_shape: Optional = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids attention_mask = None token_type_ids = None position_ids = None head_mask = None encoder_hidden_states = None encoder_attention_mask = None params: dict = None dropout_rng: Optional = None indices_rng: Optional = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None past_key_values: dict = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBigBirdForMultipleChoice\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n>>> model = FlaxBigBirdForMultipleChoice.from_pretrained(\"google/bigbird-roberta-base\")\n\n>>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n>>> choice0 = \"It is eaten with a fork and a knife.\"\n>>> choice1 = \"It is eaten while held in the hand.\"\n\n>>> encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"jax\", padding=True)\n>>> outputs = model(**{k: v[None, :] for k, v in encoding.items()})\n\n>>> logits = outputs.logits\n```", "```py\n( config: BigBirdConfig input_shape: Optional = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )\n```", "```py\n( input_ids attention_mask = None token_type_ids = None position_ids = None head_mask = None encoder_hidden_states = None encoder_attention_mask = None params: dict = None dropout_rng: Optional = None indices_rng: Optional = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None past_key_values: dict = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxTokenClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBigBirdForTokenClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n>>> model = FlaxBigBirdForTokenClassification.from_pretrained(\"google/bigbird-roberta-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n```", "```py\n( config: BigBirdConfig input_shape: Optional = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True gradient_checkpointing: bool = False **kwargs )\n```", "```py\n( input_ids attention_mask = None token_type_ids = None position_ids = None head_mask = None question_lengths = None params: dict = None dropout_rng: Optional = None indices_rng: Optional = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.big_bird.modeling_flax_big_bird.FlaxBigBirdForQuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBigBirdForQuestionAnswering\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n>>> model = FlaxBigBirdForQuestionAnswering.from_pretrained(\"google/bigbird-roberta-base\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n>>> inputs = tokenizer(question, text, return_tensors=\"jax\")\n\n>>> outputs = model(**inputs)\n>>> start_scores = outputs.start_logits\n>>> end_scores = outputs.end_logits\n```"]