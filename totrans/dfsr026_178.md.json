["```py\nimport torch\nfrom diffusers import TextToVideoZeroPipeline\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = TextToVideoZeroPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\nprompt = \"A panda is playing guitar on times square\"\nresult = pipe(prompt=prompt).images\nresult = [(r * 255).astype(\"uint8\") for r in result]\nimageio.mimsave(\"video.mp4\", result, fps=4)\n```", "```py\nimport torch\nfrom diffusers import TextToVideoZeroPipeline\nimport numpy as np\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = TextToVideoZeroPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\nseed = 0\nvideo_length = 24  #24 \u00f7 4fps = 6 seconds\nchunk_size = 8\nprompt = \"A panda is playing guitar on times square\"\n\n# Generate the video chunk-by-chunk\nresult = []\nchunk_ids = np.arange(0, video_length, chunk_size - 1)\ngenerator = torch.Generator(device=\"cuda\")\nfor i in range(len(chunk_ids)):\n    print(f\"Processing chunk {i + 1} / {len(chunk_ids)}\")\n    ch_start = chunk_ids[i]\n    ch_end = video_length if i == len(chunk_ids) - 1 else chunk_ids[i + 1]\n    # Attach the first frame for Cross Frame Attention\n    frame_ids = [0] + list(range(ch_start, ch_end))\n    # Fix the seed for the temporal consistency\n    generator.manual_seed(seed)\n    output = pipe(prompt=prompt, video_length=len(frame_ids), generator=generator, frame_ids=frame_ids)\n    result.append(output.images[1:])\n\n# Concatenate chunks and save\nresult = np.concatenate(result)\nresult = [(r * 255).astype(\"uint8\") for r in result]\nimageio.mimsave(\"video.mp4\", result, fps=4)\n```", "```py\nimport torch\nfrom diffusers import TextToVideoZeroSDXLPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\npipe = TextToVideoZeroSDXLPipeline.from_pretrained(\n    model_id, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n).to(\"cuda\")\n```", "```py\n    from huggingface_hub import hf_hub_download\n\n    filename = \"__assets__/poses_skeleton_gifs/dance1_corr.mp4\"\n    repo_id = \"PAIR/Text2Video-Zero\"\n    video_path = hf_hub_download(repo_type=\"space\", repo_id=repo_id, filename=filename)\n    ```", "```py\n    from PIL import Image\n    import imageio\n\n    reader = imageio.get_reader(video_path, \"ffmpeg\")\n    frame_count = 8\n    pose_images = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]\n    ```", "```py\n    import torch\n    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n    from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n\n    model_id = \"runwayml/stable-diffusion-v1-5\"\n    controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-openpose\", torch_dtype=torch.float16)\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        model_id, controlnet=controlnet, torch_dtype=torch.float16\n    ).to(\"cuda\")\n\n    # Set the attention processor\n    pipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n    pipe.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n\n    # fix latents for all frames\n    latents = torch.randn((1, 4, 64, 64), device=\"cuda\", dtype=torch.float16).repeat(len(pose_images), 1, 1, 1)\n\n    prompt = \"Darth Vader dancing in a desert\"\n    result = pipe(prompt=[prompt] * len(pose_images), image=pose_images, latents=latents).images\n    imageio.mimsave(\"video.mp4\", result, fps=4)\n    ```", "```py\n    import torch\n    from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel\n    from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n\n    controlnet_model_id = 'thibaud/controlnet-openpose-sdxl-1.0'\n    model_id = 'stabilityai/stable-diffusion-xl-base-1.0'\n\n    controlnet = ControlNetModel.from_pretrained(controlnet_model_id, torch_dtype=torch.float16)\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \tmodel_id, controlnet=controlnet, torch_dtype=torch.float16\n    ).to('cuda')\n\n    # Set the attention processor\n    pipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n    pipe.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n\n    # fix latents for all frames\n    latents = torch.randn((1, 4, 128, 128), device=\"cuda\", dtype=torch.float16).repeat(len(pose_images), 1, 1, 1)\n\n    prompt = \"Darth Vader dancing in a desert\"\n    result = pipe(prompt=[prompt] * len(pose_images), image=pose_images, latents=latents).images\n    imageio.mimsave(\"video.mp4\", result, fps=4)\n    ```", "```py\n    from huggingface_hub import hf_hub_download\n\n    filename = \"__assets__/pix2pix video/camel.mp4\"\n    repo_id = \"PAIR/Text2Video-Zero\"\n    video_path = hf_hub_download(repo_type=\"space\", repo_id=repo_id, filename=filename)\n    ```", "```py\n    from PIL import Image\n    import imageio\n\n    reader = imageio.get_reader(video_path, \"ffmpeg\")\n    frame_count = 8\n    video = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]\n    ```", "```py\n    import torch\n    from diffusers import StableDiffusionInstructPix2PixPipeline\n    from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n\n    model_id = \"timbrooks/instruct-pix2pix\"\n    pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n    pipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=3))\n\n    prompt = \"make it Van Gogh Starry Night style\"\n    result = pipe(prompt=[prompt] * len(video), image=video).images\n    imageio.mimsave(\"edited_video.mp4\", result, fps=4)\n    ```", "```py\n    from huggingface_hub import hf_hub_download\n\n    filename = \"__assets__/canny_videos_mp4/girl_turning.mp4\"\n    repo_id = \"PAIR/Text2Video-Zero\"\n    video_path = hf_hub_download(repo_type=\"space\", repo_id=repo_id, filename=filename)\n    ```", "```py\n    from PIL import Image\n    import imageio\n\n    reader = imageio.get_reader(video_path, \"ffmpeg\")\n    frame_count = 8\n    canny_edges = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]\n    ```", "```py\n    import torch\n    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n    from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n\n    # set model id to custom model\n    model_id = \"PAIR/text2video-zero-controlnet-canny-avatar\"\n    controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        model_id, controlnet=controlnet, torch_dtype=torch.float16\n    ).to(\"cuda\")\n\n    # Set the attention processor\n    pipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n    pipe.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n\n    # fix latents for all frames\n    latents = torch.randn((1, 4, 64, 64), device=\"cuda\", dtype=torch.float16).repeat(len(canny_edges), 1, 1, 1)\n\n    prompt = \"oil painting of a beautiful girl avatar style\"\n    result = pipe(prompt=[prompt] * len(canny_edges), image=canny_edges, latents=latents).images\n    imageio.mimsave(\"video.mp4\", result, fps=4)\n    ```"]