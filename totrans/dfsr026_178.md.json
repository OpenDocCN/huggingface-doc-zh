["```py\nimport torch\nfrom diffusers import TextToVideoZeroPipeline\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = TextToVideoZeroPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\nprompt = \"A panda is playing guitar on times square\"\nresult = pipe(prompt=prompt).images\nresult = [(r * 255).astype(\"uint8\") for r in result]\nimageio.mimsave(\"video.mp4\", result, fps=4)\n```", "```py\nimport torch\nfrom diffusers import TextToVideoZeroPipeline\nimport numpy as np\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = TextToVideoZeroPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\nseed = 0\nvideo_length = 24  #24 \u00f7 4fps = 6 seconds\nchunk_size = 8\nprompt = \"A panda is playing guitar on times square\"\n\n# Generate the video chunk-by-chunk\nresult = []\nchunk_ids = np.arange(0, video_length, chunk_size - 1)\ngenerator = torch.Generator(device=\"cuda\")\nfor i in range(len(chunk_ids)):\n    print(f\"Processing chunk {i + 1} / {len(chunk_ids)}\")\n    ch_start = chunk_ids[i]\n    ch_end = video_length if i == len(chunk_ids) - 1 else chunk_ids[i + 1]\n    # Attach the first frame for Cross Frame Attention\n    frame_ids = [0] + list(range(ch_start, ch_end))\n    # Fix the seed for the temporal consistency\n    generator.manual_seed(seed)\n    output = pipe(prompt=prompt, video_length=len(frame_ids), generator=generator, frame_ids=frame_ids)\n    result.append(output.images[1:])\n\n# Concatenate chunks and save\nresult = np.concatenate(result)\nresult = [(r * 255).astype(\"uint8\") for r in result]\nimageio.mimsave(\"video.mp4\", result, fps=4)\n```", "```py\nimport torch\nfrom diffusers import TextToVideoZeroSDXLPipeline\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\npipe = TextToVideoZeroSDXLPipeline.from_pretrained(\n    model_id, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n).to(\"cuda\")\n```", "```py\n    from huggingface_hub import hf_hub_download\n\n    filename = \"__assets__/poses_skeleton_gifs/dance1_corr.mp4\"\n    repo_id = \"PAIR/Text2Video-Zero\"\n    video_path = hf_hub_download(repo_type=\"space\", repo_id=repo_id, filename=filename)\n    ```", "```py\n    from PIL import Image\n    import imageio\n\n    reader = imageio.get_reader(video_path, \"ffmpeg\")\n    frame_count = 8\n    pose_images = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]\n    ```", "```py\n    import torch\n    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n    from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n\n    model_id = \"runwayml/stable-diffusion-v1-5\"\n    controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-openpose\", torch_dtype=torch.float16)\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        model_id, controlnet=controlnet, torch_dtype=torch.float16\n    ).to(\"cuda\")\n\n    # Set the attention processor\n    pipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n    pipe.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n\n    # fix latents for all frames\n    latents = torch.randn((1, 4, 64, 64), device=\"cuda\", dtype=torch.float16).repeat(len(pose_images), 1, 1, 1)\n\n    prompt = \"Darth Vader dancing in a desert\"\n    result = pipe(prompt=[prompt] * len(pose_images), image=pose_images, latents=latents).images\n    imageio.mimsave(\"video.mp4\", result, fps=4)\n    ```", "```py\n    import torch\n    from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel\n    from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n\n    controlnet_model_id = 'thibaud/controlnet-openpose-sdxl-1.0'\n    model_id = 'stabilityai/stable-diffusion-xl-base-1.0'\n\n    controlnet = ControlNetModel.from_pretrained(controlnet_model_id, torch_dtype=torch.float16)\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \tmodel_id, controlnet=controlnet, torch_dtype=torch.float16\n    ).to('cuda')\n\n    # Set the attention processor\n    pipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n    pipe.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n\n    # fix latents for all frames\n    latents = torch.randn((1, 4, 128, 128), device=\"cuda\", dtype=torch.float16).repeat(len(pose_images), 1, 1, 1)\n\n    prompt = \"Darth Vader dancing in a desert\"\n    result = pipe(prompt=[prompt] * len(pose_images), image=pose_images, latents=latents).images\n    imageio.mimsave(\"video.mp4\", result, fps=4)\n    ```", "```py\n    from huggingface_hub import hf_hub_download\n\n    filename = \"__assets__/pix2pix video/camel.mp4\"\n    repo_id = \"PAIR/Text2Video-Zero\"\n    video_path = hf_hub_download(repo_type=\"space\", repo_id=repo_id, filename=filename)\n    ```", "```py\n    from PIL import Image\n    import imageio\n\n    reader = imageio.get_reader(video_path, \"ffmpeg\")\n    frame_count = 8\n    video = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]\n    ```", "```py\n    import torch\n    from diffusers import StableDiffusionInstructPix2PixPipeline\n    from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n\n    model_id = \"timbrooks/instruct-pix2pix\"\n    pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n    pipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=3))\n\n    prompt = \"make it Van Gogh Starry Night style\"\n    result = pipe(prompt=[prompt] * len(video), image=video).images\n    imageio.mimsave(\"edited_video.mp4\", result, fps=4)\n    ```", "```py\n    from huggingface_hub import hf_hub_download\n\n    filename = \"__assets__/canny_videos_mp4/girl_turning.mp4\"\n    repo_id = \"PAIR/Text2Video-Zero\"\n    video_path = hf_hub_download(repo_type=\"space\", repo_id=repo_id, filename=filename)\n    ```", "```py\n    from PIL import Image\n    import imageio\n\n    reader = imageio.get_reader(video_path, \"ffmpeg\")\n    frame_count = 8\n    canny_edges = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]\n    ```", "```py\n    import torch\n    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n    from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n\n    # set model id to custom model\n    model_id = \"PAIR/text2video-zero-controlnet-canny-avatar\"\n    controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        model_id, controlnet=controlnet, torch_dtype=torch.float16\n    ).to(\"cuda\")\n\n    # Set the attention processor\n    pipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n    pipe.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n\n    # fix latents for all frames\n    latents = torch.randn((1, 4, 64, 64), device=\"cuda\", dtype=torch.float16).repeat(len(canny_edges), 1, 1, 1)\n\n    prompt = \"oil painting of a beautiful girl avatar style\"\n    result = pipe(prompt=[prompt] * len(canny_edges), image=canny_edges, latents=latents).images\n    imageio.mimsave(\"video.mp4\", result, fps=4)\n    ```", "```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor requires_safety_checker: bool = True )\n```", "```py\n( prompt: Union video_length: Optional = 8 height: Optional = None width: Optional = None num_inference_steps: int = 50 guidance_scale: float = 7.5 negative_prompt: Union = None num_videos_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None motion_field_strength_x: float = 12 motion_field_strength_y: float = 12 output_type: Optional = 'tensor' return_dict: bool = True callback: Optional = None callback_steps: Optional = 1 t0: int = 44 t1: int = 47 frame_ids: Optional = None ) \u2192 export const metadata = 'undefined';TextToVideoPipelineOutput\n```", "```py\n( latents timesteps prompt_embeds guidance_scale callback callback_steps num_warmup_steps extra_step_kwargs cross_attention_kwargs = None ) \u2192 export const metadata = 'undefined';latents\n```", "```py\n( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```", "```py\n( x_t0 t0 t1 generator ) \u2192 export const metadata = 'undefined';x_t1\n```", "```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel text_encoder_2: CLIPTextModelWithProjection tokenizer: CLIPTokenizer tokenizer_2: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers image_encoder: CLIPVisionModelWithProjection = None feature_extractor: CLIPImageProcessor = None force_zeros_for_empty_prompt: bool = True add_watermarker: Optional = None )\n```", "```py\n( prompt: Union prompt_2: Union = None video_length: Optional = 8 height: Optional = None width: Optional = None num_inference_steps: int = 50 denoising_end: Optional = None guidance_scale: float = 7.5 negative_prompt: Union = None negative_prompt_2: Union = None num_videos_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None frame_ids: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None latents: Optional = None motion_field_strength_x: float = 12 motion_field_strength_y: float = 12 output_type: Optional = 'tensor' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None guidance_rescale: float = 0.0 original_size: Optional = None crops_coords_top_left: Tuple = (0, 0) target_size: Optional = None t0: int = 44 t1: int = 47 )\n```", "```py\n( latents timesteps prompt_embeds guidance_scale callback callback_steps num_warmup_steps extra_step_kwargs add_text_embeds add_time_ids cross_attention_kwargs = None guidance_rescale: float = 0.0 ) \u2192 export const metadata = 'undefined';latents\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( prompt: str prompt_2: Optional = None device: Optional = None num_images_per_prompt: int = 1 do_classifier_free_guidance: bool = True negative_prompt: Optional = None negative_prompt_2: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```", "```py\n( x_t0 t0 t1 generator ) \u2192 export const metadata = 'undefined';x_t1\n```", "```py\n( images: Union nsfw_content_detected: Optional )\n```"]