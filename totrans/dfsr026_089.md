# 金属性能着色器（MPS）

> 原始文本：[https://huggingface.co/docs/diffusers/optimization/mps](https://huggingface.co/docs/diffusers/optimization/mps)

🤗 Diffusers兼容使用PyTorch [`mps`](https://pytorch.org/docs/stable/notes/mps.html)设备的苹果硅（M1/M2芯片），该设备使用Metal框架来利用MacOS设备上的GPU。您需要：

+   具有苹果硅（M1/M2）硬件的macOS计算机

+   macOS 12.6或更高版本（建议使用13.0或更高版本）

+   Python的arm64版本

+   [PyTorch 2.0](https://pytorch.org/get-started/locally/)（建议）或1.13（`mps`支持的最低版本）

`mps`后端使用PyTorch的`.to()`接口将稳定扩散管道移动到您的M1或M2设备上：

```py
from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
pipe = pipe.to("mps")

# Recommended if your computer has < 64 GB of RAM
pipe.enable_attention_slicing()

prompt = "a photo of an astronaut riding a horse on mars"
image = pipe(prompt).images[0]
image
```

在批处理中生成多个提示可能会[崩溃](https://github.com/huggingface/diffusers/issues/363)或无法可靠工作。我们认为这与PyTorch中的[`mps`](https://github.com/pytorch/pytorch/issues/84039)后端有关。在调查此问题时，您应该迭代而不是批处理。

如果您使用**PyTorch 1.13**，您需要通过额外的一次通过“prime”管道。这是一个临时解决方法，用于解决第一次推理通过产生与后续推理通过略有不同结果的问题。您只需要执行此步骤一次，仅经过一次推理步骤后，您可以丢弃结果。

```py
  from diffusers import DiffusionPipeline

  pipe = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5").to("mps")
  pipe.enable_attention_slicing()

  prompt = "a photo of an astronaut riding a horse on mars"
  # First-time "warmup" pass if PyTorch version is 1.13
+ _ = pipe(prompt, num_inference_steps=1)

  # Results match those from the CPU device after the warmup pass.
  image = pipe(prompt).images[0]
```

## 故障排除

M1/M2的性能对内存压力非常敏感。当发生这种情况时，系统会自动进行交换，如果需要的话，这会显著降低性能。

为防止这种情况发生，我们建议*注意力切片*以减少推理过程中的内存压力并防止交换。如果您的计算机系统RAM少于64GB，或者生成的图像分辨率大于512×512像素的非标准分辨率，则这一点尤为重要。在您的管道上调用[enable_attention_slicing()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_attention_slicing)函数：

```py
from diffusers import DiffusionPipeline
import torch

pipeline = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True).to("mps")
pipeline.enable_attention_slicing()
```

注意力切片将昂贵的注意力操作分为多个步骤，而不是一次性完成。在没有通用内存的计算机上，这通常可以提高约20%的性能，但我们观察到在大多数苹果硅计算机上有更好的性能，除非您有64GB或更多的RAM。
