# PyTorch 2.0

> 原始文本：[https://huggingface.co/docs/diffusers/optimization/torch2.0](https://huggingface.co/docs/diffusers/optimization/torch2.0)

🤗 Diffusers支持来自[PyTorch 2.0](https://pytorch.org/get-started/pytorch-2.0/)的最新优化，其中包括：

1.  一种内存高效的注意力实现，无需任何额外的依赖项，如xFormers。

1.  [`torch.compile`](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)，一种即时（JIT）编译器，用于在编译单个模型时提供额外的性能提升。

这两种优化都需要PyTorch 2.0或更高版本以及🤗 Diffusers > 0.13.0。

```py
pip install --upgrade torch diffusers
```

## 缩放点积注意力

[`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)（SDPA）是一种经过优化且内存高效的注意力机制（类似于xFormers），根据模型输入和GPU类型自动启用几种其他优化。如果您正在使用PyTorch 2.0和最新版本的🤗 Diffusers，则默认情况下启用SDPA，因此您无需向代码中添加任何内容。

但是，如果您想要显式启用它，可以设置[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)以使用[AttnProcessor2_0](/docs/diffusers/v0.26.3/en/api/attnprocessor#diffusers.models.attention_processor.AttnProcessor2_0)： 

```py
  import torch
  from diffusers import DiffusionPipeline
+ from diffusers.models.attention_processor import AttnProcessor2_0

  pipe = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
+ pipe.unet.set_attn_processor(AttnProcessor2_0())

  prompt = "a photo of an astronaut riding a horse on mars"
  image = pipe(prompt).images[0]
```

SDPA应该与`xFormers`一样快速和内存高效；有关更多详细信息，请查看[基准测试](#benchmark)。

在某些情况下 - 例如使管道更具确定性或将其转换为其他格式 - 使用原始注意力处理器[AttnProcessor](/docs/diffusers/v0.26.3/en/api/attnprocessor#diffusers.models.attention_processor.AttnProcessor)可能会有所帮助。要恢复到[AttnProcessor](/docs/diffusers/v0.26.3/en/api/attnprocessor#diffusers.models.attention_processor.AttnProcessor)，请在管道上调用[set_default_attn_processor()](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel.set_default_attn_processor)函数：

```py
  import torch
  from diffusers import DiffusionPipeline

  pipe = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
+ pipe.unet.set_default_attn_processor()

  prompt = "a photo of an astronaut riding a horse on mars"
  image = pipe(prompt).images[0]
```

## torch.compile

`torch.compile`函数通常可以为您的PyTorch代码提供额外的加速。在🤗 Diffusers中，通常最好使用`torch.compile`包装UNet，因为它在管道中执行大部分繁重的工作。

```py
from diffusers import DiffusionPipeline
import torch

pipe = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead", fullgraph=True)
images = pipe(prompt, num_inference_steps=steps, num_images_per_prompt=batch_size).images[0]
```

根据GPU类型，`torch.compile`可以在SDPA的基础上提供**5-300倍**的*额外加速*！如果您使用的是更近期的GPU架构，如Ampere（A100，3090），Ada（4090）和Hopper（H100），`torch.compile`能够从这些GPU中挤出更多性能。

编译需要一些时间才能完成，因此最适合您准备好管道后多次执行相同类型的推理操作的情况。例如，在不同的图像尺寸上调用编译后的管道会再次触发编译，这可能很昂贵。

有关`torch.compile`的更多信息和不同选项，请参考[`torch_compile`](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)教程。

## 基准测试

我们对PyTorch 2.0的高效注意力实现和`torch.compile`在不同GPU和批量大小上进行了全面的基准测试，针对我们最常用的五个管道。该代码在🤗 Diffusers v0.17.0.dev0上进行了基准测试，以优化`torch.compile`的使用（有关更多详细信息，请参见[此处](https://github.com/huggingface/diffusers/pull/3313)）。

展开下面的下拉菜单以找到用于基准测试每个管道的代码：

<details>### 稳定的扩散文本到图像

```py
from diffusers import DiffusionPipeline
import torch

path = "runwayml/stable-diffusion-v1-5"

run_compile = True  # Set True / False

pipe = DiffusionPipeline.from_pretrained(path, torch_dtype=torch.float16, use_safetensors=True)
pipe = pipe.to("cuda")
pipe.unet.to(memory_format=torch.channels_last)

if run_compile:
    print("Run torch compile")
    pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead", fullgraph=True)

prompt = "ghibli style, a fantasy landscape with castles"

for _ in range(3):
    images = pipe(prompt=prompt).images
```

### 稳定的扩散图像到图像

```py
from diffusers import StableDiffusionImg2ImgPipeline
from diffusers.utils import load_image
import torch

url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"

init_image = load_image(url)
init_image = init_image.resize((512, 512))

path = "runwayml/stable-diffusion-v1-5"

run_compile = True  # Set True / False

pipe = StableDiffusionImg2ImgPipeline.from_pretrained(path, torch_dtype=torch.float16, use_safetensors=True)
pipe = pipe.to("cuda")
pipe.unet.to(memory_format=torch.channels_last)

if run_compile:
    print("Run torch compile")
    pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead", fullgraph=True)

prompt = "ghibli style, a fantasy landscape with castles"

for _ in range(3):
    image = pipe(prompt=prompt, image=init_image).images[0]
```

### 稳定的扩散修复

```py
from diffusers import StableDiffusionInpaintPipeline
from diffusers.utils import load_image
import torch

img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"

init_image = load_image(img_url).resize((512, 512))
mask_image = load_image(mask_url).resize((512, 512))

path = "runwayml/stable-diffusion-inpainting"

run_compile = True  # Set True / False

pipe = StableDiffusionInpaintPipeline.from_pretrained(path, torch_dtype=torch.float16, use_safetensors=True)
pipe = pipe.to("cuda")
pipe.unet.to(memory_format=torch.channels_last)

if run_compile:
    print("Run torch compile")
    pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead", fullgraph=True)

prompt = "ghibli style, a fantasy landscape with castles"

for _ in range(3):
    image = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]
```

### ControlNet

```py
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
from diffusers.utils import load_image
import torch

url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"

init_image = load_image(url)
init_image = init_image.resize((512, 512))

path = "runwayml/stable-diffusion-v1-5"

run_compile = True  # Set True / False
controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    path, controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True
)

pipe = pipe.to("cuda")
pipe.unet.to(memory_format=torch.channels_last)
pipe.controlnet.to(memory_format=torch.channels_last)

if run_compile:
    print("Run torch compile")
    pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead", fullgraph=True)
    pipe.controlnet = torch.compile(pipe.controlnet, mode="reduce-overhead", fullgraph=True)

prompt = "ghibli style, a fantasy landscape with castles"

for _ in range(3):
    image = pipe(prompt=prompt, image=init_image).images[0]
```

### DeepFloyd IF文本到图像+放大

```py
from diffusers import DiffusionPipeline
import torch

run_compile = True  # Set True / False

pipe_1 = DiffusionPipeline.from_pretrained("DeepFloyd/IF-I-M-v1.0", variant="fp16", text_encoder=None, torch_dtype=torch.float16, use_safetensors=True)
pipe_1.to("cuda")
pipe_2 = DiffusionPipeline.from_pretrained("DeepFloyd/IF-II-M-v1.0", variant="fp16", text_encoder=None, torch_dtype=torch.float16, use_safetensors=True)
pipe_2.to("cuda")
pipe_3 = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-x4-upscaler", torch_dtype=torch.float16, use_safetensors=True)
pipe_3.to("cuda")

pipe_1.unet.to(memory_format=torch.channels_last)
pipe_2.unet.to(memory_format=torch.channels_last)
pipe_3.unet.to(memory_format=torch.channels_last)

if run_compile:
    pipe_1.unet = torch.compile(pipe_1.unet, mode="reduce-overhead", fullgraph=True)
    pipe_2.unet = torch.compile(pipe_2.unet, mode="reduce-overhead", fullgraph=True)
    pipe_3.unet = torch.compile(pipe_3.unet, mode="reduce-overhead", fullgraph=True)

prompt = "the blue hulk"

prompt_embeds = torch.randn((1, 2, 4096), dtype=torch.float16)
neg_prompt_embeds = torch.randn((1, 2, 4096), dtype=torch.float16)

for _ in range(3):
    image_1 = pipe_1(prompt_embeds=prompt_embeds, negative_prompt_embeds=neg_prompt_embeds, output_type="pt").images
    image_2 = pipe_2(image=image_1, prompt_embeds=prompt_embeds, negative_prompt_embeds=neg_prompt_embeds, output_type="pt").images
    image_3 = pipe_3(prompt=prompt, image=image_1, noise_level=100).images
```</details>

下面的图表突出显示了在五个GPU系列中，使用PyTorch 2.0和启用`torch.compile`的[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)的相对加速情况。以下图表的基准是*每秒迭代次数*。

![t2i_speedup](../Images/b9e5a83589de9942d685c07d4cf11ecd.png)

为了让您更好地了解这种加速对其他流水线的影响，考虑以下关于A100使用PyTorch 2.0和`torch.compile`的图表：

![a100_numbers](../Images/f5f080ae9f99b4a72d2909f42bde4477.png)

在以下表格中，我们报告了我们的发现，以*每秒迭代次数*表示。

### A100（批量大小：1）

| **流水线** | **torch 2.0 - 无编译** | **torch nightly - 无编译** | **torch 2.0 - 编译** | **torch nightly - 编译** |
| :-: | :-: | :-: | :-: | :-: |
| SD - 文本到图像 | 21.66 | 23.13 | 44.03 | 49.74 |
| SD - 图像到图像 | 21.81 | 22.40 | 43.92 | 46.32 |
| SD - 补全 | 22.24 | 23.23 | 43.76 | 49.25 |
| SD - 控制网络 | 15.02 | 15.82 | 32.13 | 36.08 |

| IF | 20.21 / 13.84 /

24.00 | 20.12 / 13.70 /

24.03 | ❌ | 97.34 / 27.23 /

111.66 |

| SDXL - 文本到图像 | 8.64 | 9.9 | - | - |
| --- | --- | --- | --- | --- |

### A100（批量大小：4）

| **流水线** | **torch 2.0 - 无编译** | **torch nightly - 无编译** | **torch 2.0 - 编译** | **torch nightly - 编译** |
| :-: | :-: | :-: | :-: | :-: |
| SD - 文本到图像 | 11.6 | 13.12 | 14.62 | 17.27 |
| SD - 图像到图像 | 11.47 | 13.06 | 14.66 | 17.25 |
| SD - 补全 | 11.67 | 13.31 | 14.88 | 17.48 |
| SD - 控制网络 | 8.28 | 9.38 | 10.51 | 12.41 |
| IF | 25.02 | 18.04 | ❌ | 48.47 |
| SDXL - 文本到图像 | 2.44 | 2.74 | - | - |

### A100（批量大小：16）

| **流水线** | **torch 2.0 - 无编译** | **torch nightly - 无编译** | **torch 2.0 - 编译** | **torch nightly - 编译** |
| :-: | :-: | :-: | :-: | :-: |
| SD - 文本到图像 | 3.04 | 3.6 | 3.83 | 4.68 |
| SD - 图像到图像 | 2.98 | 3.58 | 3.83 | 4.67 |
| SD - 补全 | 3.04 | 3.66 | 3.9 | 4.76 |
| SD - 控制网络 | 2.15 | 2.58 | 2.74 | 3.35 |
| IF | 8.78 | 9.82 | ❌ | 16.77 |
| SDXL - 文本到图像 | 0.64 | 0.72 | - | - |

### V100（批量大小：1）

| **流水线** | **torch 2.0 - 无编译** | **torch nightly - 无编译** | **torch 2.0 - 编译** | **torch nightly - 编译** |
| :-: | :-: | :-: | :-: | :-: |
| SD - 文本到图像 | 18.99 | 19.14 | 20.95 | 22.17 |
| SD - 图像到图像 | 18.56 | 19.18 | 20.95 | 22.11 |
| SD - 补全 | 19.14 | 19.06 | 21.08 | 22.20 |
| SD - 控制网络 | 13.48 | 13.93 | 15.18 | 15.88 |

| IF | 20.01 / 9.08 /

23.34 | 19.79 / 8.98 /

24.10 | ❌ | 55.75 / 11.57 /

57.67 |

### V100（批量大小：4）

| **流水线** | **torch 2.0 - 无编译** | **torch nightly - 无编译** | **torch 2.0 - 编译** | **torch nightly - 编译** |
| :-: | :-: | :-: | :-: | :-: |
| SD - 文本到图像 | 5.96 | 5.89 | 6.83 | 6.86 |
| SD - 图像到图像 | 5.90 | 5.91 | 6.81 | 6.82 |
| SD - 补全 | 5.99 | 6.03 | 6.93 | 6.95 |
| SD - 控制网络 | 4.26 | 4.29 | 4.92 | 4.93 |
| IF | 15.41 | 14.76 | ❌ | 22.95 |

### V100（批量大小：16）

| **流水线** | **torch 2.0 - 无编译** | **torch nightly - 无编译** | **torch 2.0 - 编译** | **torch nightly - 编译** |
| :-: | :-: | :-: | :-: | :-: |
| SD - 文本到图像 | 1.66 | 1.66 | 1.92 | 1.90 |
| SD - 图像到图像 | 1.65 | 1.65 | 1.91 | 1.89 |
| SD - 补全 | 1.69 | 1.69 | 1.95 | 1.93 |
| SD - 控制网络 | 1.19 | 1.19 | 热身后OOM | 1.36 |
| IF | 5.43 | 5.29 | ❌ | 7.06 |

### T4（批量大小：1）

| **流水线** | **torch 2.0 - 无编译** | **torch nightly - 无编译** | **torch 2.0 - 编译** | **torch nightly - 编译** |
| :-: | :-: | :-: | :-: | :-: |
| SD - 文本到图像 | 6.9 | 6.95 | 7.3 | 7.56 |
| SD - 图像到图像 | 6.84 | 6.99 | 7.04 | 7.55 |
| SD - 补全 | 6.91 | 6.7 | 7.01 | 7.37 |
| SD - 控制网络 | 4.89 | 4.86 | 5.35 | 5.48 |

| IF | 17.42 / 2.47 /

18.52 | 16.96 / 2.45 /

18.69 | ❌ | 24.63 / 2.47 /

23.39 |

| SDXL - 文本到图像 | 1.15 | 1.16 | - | - |
| --- | --- | --- | --- | --- |

### T4（批量大小：4）

| **Pipeline** | **torch 2.0 - no compile** | **torch nightly - no compile** | **torch 2.0 - compile** | **torch nightly - compile** |
| :-: | :-: | :-: | :-: | :-: |
| SD - txt2img | 1.79 | 1.79 | 2.03 | 1.99 |
| SD - img2img | 1.77 | 1.77 | 2.05 | 2.04 |
| SD - inpaint | 1.81 | 1.82 | 2.09 | 2.09 |
| SD - controlnet | 1.34 | 1.27 | 1.47 | 1.46 |
| IF | 5.79 | 5.61 | ❌ | 7.39 |
| SDXL - txt2img | 0.288 | 0.289 | - | - |

### T4 (batch size: 16)

| **Pipeline** | **torch 2.0 - no compile** | **torch nightly - no compile** | **torch 2.0 - compile** | **torch nightly - compile** |
| :-: | :-: | :-: | :-: | :-: |
| SD - txt2img | 2.34s | 2.30s | OOM after 2nd iteration | 1.99s |
| SD - img2img | 2.35s | 2.31s | OOM after warmup | 2.00s |
| SD - inpaint | 2.30s | 2.26s | OOM after 2nd iteration | 1.95s |
| SD - controlnet | OOM after 2nd iteration | OOM after 2nd iteration | OOM after warmup | OOM after warmup |
| IF * | 1.44 | 1.44 | ❌ | 1.94 |
| SDXL - txt2img | OOM | OOM | - | - |

### RTX 3090 (batch size: 1)

| **Pipeline** | **torch 2.0 - no compile** | **torch nightly - no compile** | **torch 2.0 - compile** | **torch nightly - compile** |
| :-: | :-: | :-: | :-: | :-: |
| SD - txt2img | 22.56 | 22.84 | 23.84 | 25.69 |
| SD - img2img | 22.25 | 22.61 | 24.1 | 25.83 |
| SD - inpaint | 22.22 | 22.54 | 24.26 | 26.02 |
| SD - controlnet | 16.03 | 16.33 | 17.38 | 18.56 |

| IF | 27.08 / 9.07 /

31.23 | 26.75 / 8.92 /

31.47 | ❌ | 68.08 / 11.16 /

65.29 |

### RTX 3090 (batch size: 4)

| **Pipeline** | **torch 2.0 - no compile** | **torch nightly - no compile** | **torch 2.0 - compile** | **torch nightly - compile** |
| :-: | :-: | :-: | :-: | :-: |
| SD - txt2img | 6.46 | 6.35 | 7.29 | 7.3 |
| SD - img2img | 6.33 | 6.27 | 7.31 | 7.26 |
| SD - inpaint | 6.47 | 6.4 | 7.44 | 7.39 |
| SD - controlnet | 4.59 | 4.54 | 5.27 | 5.26 |
| IF | 16.81 | 16.62 | ❌ | 21.57 |

### RTX 3090 (batch size: 16)

| **Pipeline** | **torch 2.0 - no compile** | **torch nightly - no compile** | **torch 2.0 - compile** | **torch nightly - compile** |
| :-: | :-: | :-: | :-: | :-: |
| SD - txt2img | 1.7 | 1.69 | 1.93 | 1.91 |
| SD - img2img | 1.68 | 1.67 | 1.93 | 1.9 |
| SD - inpaint | 1.72 | 1.71 | 1.97 | 1.94 |
| SD - controlnet | 1.23 | 1.22 | 1.4 | 1.38 |
| IF | 5.01 | 5.00 | ❌ | 6.33 |

### RTX 4090 (batch size: 1)

| **Pipeline** | **torch 2.0 - no compile** | **torch nightly - no compile** | **torch 2.0 - compile** | **torch nightly - compile** |
| :-: | :-: | :-: | :-: | :-: |
| SD - txt2img | 40.5 | 41.89 | 44.65 | 49.81 |
| SD - img2img | 40.39 | 41.95 | 44.46 | 49.8 |
| SD - inpaint | 40.51 | 41.88 | 44.58 | 49.72 |
| SD - controlnet | 29.27 | 30.29 | 32.26 | 36.03 |

| IF | 69.71 / 18.78 /

85.49 | 69.13 / 18.80 /

85.56 | ❌ | 124.60 / 26.37 /

138.79 |

| SDXL - txt2img | 6.8 | 8.18 | - | - |
| --- | --- | --- | --- | --- |

### RTX 4090 (batch size: 4)

| **Pipeline** | **torch 2.0 - no compile** | **torch nightly - no compile** | **torch 2.0 - compile** | **torch nightly - compile** |
| :-: | :-: | :-: | :-: | :-: |
| SD - txt2img | 12.62 | 12.84 | 15.32 | 15.59 |
| SD - img2img | 12.61 | 12,.79 | 15.35 | 15.66 |
| SD - inpaint | 12.65 | 12.81 | 15.3 | 15.58 |
| SD - controlnet | 9.1 | 9.25 | 11.03 | 11.22 |
| IF | 31.88 | 31.14 | ❌ | 43.92 |
| SDXL - txt2img | 2.19 | 2.35 | - | - |

### RTX 4090 (batch size: 16)

| **Pipeline** | **torch 2.0 - no compile** | **torch nightly - no compile** | **torch 2.0 - compile** | **torch nightly - compile** |
| :-: | :-: | :-: | :-: | :-: |
| SD - txt2img | 3.17 | 3.2 | 3.84 | 3.85 |
| SD - img2img | 3.16 | 3.2 | 3.84 | 3.85 |
| SD - inpaint | 3.17 | 3.2 | 3.85 | 3.85 |
| SD - controlnet | 2.23 | 2.3 | 2.7 | 2.75 |
| IF | 9.26 | 9.2 | ❌ | 13.31 |
| SDXL - txt2img | 0.52 | 0.53 | - | - |

## Notes

+   关注此[PR](https://github.com/huggingface/diffusers/pull/3313)以获取有关进行基准测试所使用环境的更多详细信息。

+   对于DeepFloyd IF管道，其中批量大小> 1，我们仅在第一个IF管道中用于文本到图像生成的批量大小> 1，并且不用于放大。这意味着两个放大管道接收到批量大小为1。

*感谢PyTorch团队的[Horace He](https://github.com/Chillee)在改进我们对Diffusers中`torch.compile()`支持方面的支持。*
