["```py\ninput_ids = tokenizer.encode(\"This is a sentence from the training data\", return_tensors=\"pt\")\nloss = model(input_ids, labels=input_ids)[0]\n```", "```py\n( attention_head_size = 64 attn_layers = ['local', 'lsh', 'local', 'lsh', 'local', 'lsh'] axial_norm_std = 1.0 axial_pos_embds = True axial_pos_shape = [64, 64] axial_pos_embds_dim = [64, 192] chunk_size_lm_head = 0 eos_token_id = 2 feed_forward_size = 512 hash_seed = None hidden_act = 'relu' hidden_dropout_prob = 0.05 hidden_size = 256 initializer_range = 0.02 is_decoder = False layer_norm_eps = 1e-12 local_num_chunks_before = 1 local_num_chunks_after = 0 local_attention_probs_dropout_prob = 0.05 local_attn_chunk_length = 64 lsh_attn_chunk_length = 64 lsh_attention_probs_dropout_prob = 0.0 lsh_num_chunks_before = 1 lsh_num_chunks_after = 0 max_position_embeddings = 4096 num_attention_heads = 12 num_buckets = None num_hashes = 1 pad_token_id = 0 vocab_size = 320 tie_word_embeddings = False use_cache = True classifier_dropout = None **kwargs )\n```", "```py\n>>> from transformers import ReformerConfig, ReformerModel\n\n>>> # Initializing a Reformer configuration\n>>> configuration = ReformerConfig()\n\n>>> # Initializing a Reformer model (with random weights)\n>>> model = ReformerModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file eos_token = '</s>' unk_token = '<unk>' additional_special_tokens = [] sp_model_kwargs: Optional = None **kwargs )\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( vocab_file = None tokenizer_file = None eos_token = '</s>' unk_token = '<unk>' additional_special_tokens = [] **kwargs )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None num_hashes: Optional = None past_buckets_states: Optional = None use_cache: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.reformer.modeling_reformer.ReformerModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, ReformerModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/reformer-crime-and-punishment\")\n>>> model = ReformerModel.from_pretrained(\"google/reformer-crime-and-punishment\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None position_ids: Optional = None attention_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None num_hashes: Optional = None past_buckets_states: Optional = None use_cache: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, ReformerModelWithLMHead\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/reformer-crime-and-punishment\")\n>>> model = ReformerModelWithLMHead.from_pretrained(\"google/reformer-crime-and-punishment\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None position_ids: Optional = None attention_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None num_hashes: Optional = None labels: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.MaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, ReformerForMaskedLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-reformer\")\n>>> model = ReformerForMaskedLM.from_pretrained(\"hf-internal-testing/tiny-random-reformer\")\n\n>>> # add mask_token\n>>> tokenizer.add_special_tokens({\"mask_token\": \"[MASK]\"})\n>>> inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n\n>>> # resize model's embedding matrix\n>>> model.resize_token_embeddings(new_num_tokens=model.config.vocab_size + 1)\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> # retrieve index of [MASK]\n>>> mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n\n>>> predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n>>> predicted_token = tokenizer.decode(predicted_token_id)\n```", "```py\n>>> labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n>>> # mask labels of non-[MASK] tokens\n>>> labels = torch.where(\n...     inputs.input_ids == tokenizer.mask_token_id, labels[:, : inputs[\"input_ids\"].shape[-1]], -100\n... )\n\n>>> outputs = model(**inputs, labels=labels)\n>>> loss = round(outputs.loss.item(), 2)\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None position_ids: Optional = None attention_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None num_hashes: Optional = None labels: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, ReformerForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/reformer-crime-and-punishment\")\n>>> model = ReformerForSequenceClassification.from_pretrained(\"google/reformer-crime-and-punishment\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n>>> label = model.config.id2label[predicted_class_id]\n```", "```py\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = ReformerForSequenceClassification.from_pretrained(\n...     \"google/reformer-crime-and-punishment\", num_labels=num_labels\n... )\n\n>>> labels = torch.tensor(1)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None position_ids: Optional = None attention_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None num_hashes: Optional = None start_positions: Optional = None end_positions: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.QuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, ReformerForQuestionAnswering\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/reformer-crime-and-punishment\")\n>>> model = ReformerForQuestionAnswering.from_pretrained(\"google/reformer-crime-and-punishment\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n\n>>> # target is \"nice puppet\"\n>>> target_start_index = torch.tensor([14])\n>>> target_end_index = torch.tensor([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = outputs.loss\n```"]