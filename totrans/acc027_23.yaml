- en: Low Precision Training Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/low_precision_training](https://huggingface.co/docs/accelerate/usage_guides/low_precision_training)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ¤— Accelerate provides integrations to train on lower precision methods using
    specified supported hardware through the `TransformersEngine` and `MS-AMP` packages.
    This documentation will help guide you through what hardware is supported, how
    to configure your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to leverage the low precision methods, and what you can expect when training.
  prefs: []
  type: TYPE_NORMAL
- en: What training on FP8 means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To explore more of the nitty-gritty in training in FP8 with PyTorch and ðŸ¤— Accelerate,
    check out the [concept_guide](../concept_guides/low_precision_training.md) on
    why this can be difficult. But essentially rather than training in BF16, some
    (or all) aspects of training a model can be performed using 8 bits instead of
    16\. The challenge is doing so without degrading final performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is only enabled on specific NVIDIA hardware, namely:'
  prefs: []
  type: TYPE_NORMAL
- en: Anything after the 3000 series consumer graphics cards (such as the 4090)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopper-based GPU architectures (such as the `H100` and `H200`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What this will result in is some gain in the memory used (as weâ€™ve cut the needed
    memory in half for some parts of training) and an increase in throughput *should*
    be seen as well for larger models that can replace certain layers with FP8-enabled
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the Accelerator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Currently two different backends for FP8 are supported (`TransformersEngine`
    and `MS-AMP`), each with different capabilities and configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use either, the same core API is used. Just pass `mixed_precision="fp8"`
    to either the [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator),
    during `accelerate config` when prompted about mixed precision, or as part of
    your `config.yaml` file in the `mixed_precision` key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, if `MS-AMP` is available in your environment, ðŸ¤— Accelerate will
    automatically utilize it as a backend. To specify it yourself (and customize other
    parts of the FP8 mixed precision setup), you can utilize the [utils.FP8RecipeKwargs](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.FP8RecipeKwargs):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Configuring MS-AMP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Of the two, `MS-AMP` is traditionally the easier one to configure as there
    is only a single argument: the optimization level.'
  prefs: []
  type: TYPE_NORMAL
- en: Currently two levels of optimization are supported in the ðŸ¤— Accelerate integration,
    `"O1"` and `"O2"` (using the letter â€˜oâ€™, not zero).
  prefs: []
  type: TYPE_NORMAL
- en: '`"O1"` will cast the weight gradients and `all_reduce` communications to happen
    in 8-bit, while the rest are done in 16 bit. This reduces the general GPU memory
    usage and speeds up communication bandwidths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"O2"` will also cast first-order optimizer states into 8 bit, while the second
    order states are in FP16\. (Currently just the `Adam` optimizer is supported).
    This tries itâ€™s best to minimize final accuracy degradation and will save the
    highest potential memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To specify an optimization level, pass it to the `FP8KwargsHandler` by setting
    the `optimization_level` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Configuring TransformersEngine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TransformersEngine has much more available for customizing how and what FP8
    calculations are performed. A full list of supported arguments and what they mean
    are available in [NVIDIAâ€™s documentation](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html),
    however they are restated as part of `FP8KwargsHandler`â€™s docstring for your convience.
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ¤— Accelerate tries to set sensible defaults, but exploring and tweaking the
    various parameters yourself can lead to better performance potentially.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use it, specify `backend="te"` and modify any of the arguments you want
    as part of your kwarg handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Futher Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To learn more about training in FP8 please check out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Our concept guide](../concept_guides/low_precision_training.md) detailing
    into more about both TransformersEngine and MS-AMP'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The `transformers-engine` documentation](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The `MS-AMP` documentation](https://azure.github.io/MS-AMP/docs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
