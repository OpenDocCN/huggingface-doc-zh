- en: Low Precision Training Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½ç²¾åº¦è®­ç»ƒæ–¹æ³•
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/low_precision_training](https://huggingface.co/docs/accelerate/usage_guides/low_precision_training)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/accelerate/usage_guides/low_precision_training](https://huggingface.co/docs/accelerate/usage_guides/low_precision_training)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ¤— Accelerate provides integrations to train on lower precision methods using
    specified supported hardware through the `TransformersEngine` and `MS-AMP` packages.
    This documentation will help guide you through what hardware is supported, how
    to configure your [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    to leverage the low precision methods, and what you can expect when training.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Accelerateæä¾›äº†é›†æˆï¼Œå¯ä»¥ä½¿ç”¨æŒ‡å®šæ”¯æŒçš„ç¡¬ä»¶é€šè¿‡`TransformersEngine`å’Œ`MS-AMP`è½¯ä»¶åŒ…è¿›è¡Œä½ç²¾åº¦æ–¹æ³•çš„è®­ç»ƒã€‚æœ¬æ–‡æ¡£å°†å¸®åŠ©æ‚¨äº†è§£æ”¯æŒçš„ç¡¬ä»¶æ˜¯ä»€ä¹ˆï¼Œå¦‚ä½•é…ç½®æ‚¨çš„[åŠ é€Ÿå™¨](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)ä»¥åˆ©ç”¨ä½ç²¾åº¦æ–¹æ³•ï¼Œä»¥åŠåœ¨è®­ç»ƒæ—¶å¯ä»¥æœŸæœ›ä»€ä¹ˆã€‚
- en: What training on FP8 means
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨FP8ä¸Šè¿›è¡Œè®­ç»ƒæ„å‘³ç€ä»€ä¹ˆ
- en: To explore more of the nitty-gritty in training in FP8 with PyTorch and ğŸ¤— Accelerate,
    check out the [concept_guide](../concept_guides/low_precision_training.md) on
    why this can be difficult. But essentially rather than training in BF16, some
    (or all) aspects of training a model can be performed using 8 bits instead of
    16\. The challenge is doing so without degrading final performance.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ·±å…¥äº†è§£ä½¿ç”¨PyTorchå’ŒğŸ¤— Accelerateåœ¨FP8ä¸­è¿›è¡Œè®­ç»ƒçš„ç»†èŠ‚ï¼Œè¯·æŸ¥çœ‹[æ¦‚å¿µæŒ‡å—](../concept_guides/low_precision_training.md)ï¼Œäº†è§£ä¸ºä»€ä¹ˆè¿™å¯èƒ½ä¼šå¾ˆå›°éš¾ã€‚ä½†åŸºæœ¬ä¸Šï¼Œä¸åœ¨BF16ä¸­è®­ç»ƒä¸åŒï¼Œè®­ç»ƒæ¨¡å‹çš„æŸäº›ï¼ˆæˆ–å…¨éƒ¨ï¼‰æ–¹é¢å¯ä»¥ä½¿ç”¨8ä½è€Œä¸æ˜¯16ä½æ¥æ‰§è¡Œã€‚æŒ‘æˆ˜åœ¨äºåœ¨ä¸é™ä½æœ€ç»ˆæ€§èƒ½çš„æƒ…å†µä¸‹è¿™æ ·åšã€‚
- en: 'This is only enabled on specific NVIDIA hardware, namely:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä»…åœ¨ç‰¹å®šçš„NVIDIAç¡¬ä»¶ä¸Šå¯ç”¨ï¼Œå³ï¼š
- en: Anything after the 3000 series consumer graphics cards (such as the 4090)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3000ç³»åˆ—æ¶ˆè´¹çº§æ˜¾å¡ä¹‹åçš„ä»»ä½•ç¡¬ä»¶ï¼ˆä¾‹å¦‚4090ï¼‰
- en: Hopper-based GPU architectures (such as the `H100` and `H200`)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸºäºHopperçš„GPUæ¶æ„ï¼ˆä¾‹å¦‚`H100`å’Œ`H200`ï¼‰
- en: What this will result in is some gain in the memory used (as weâ€™ve cut the needed
    memory in half for some parts of training) and an increase in throughput *should*
    be seen as well for larger models that can replace certain layers with FP8-enabled
    ones.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†å¯¼è‡´ä¸€äº›å†…å­˜ä½¿ç”¨ä¸Šçš„å¢ç›Šï¼ˆå› ä¸ºæˆ‘ä»¬å·²ç»å°†æŸäº›è®­ç»ƒéƒ¨åˆ†æ‰€éœ€çš„å†…å­˜å‡å°‘äº†ä¸€åŠï¼‰ï¼Œå¹¶ä¸”å¯¹äºå¯ä»¥ç”¨FP8å¯ç”¨çš„æŸäº›å±‚æ›¿æ¢çš„è¾ƒå¤§æ¨¡å‹ï¼Œä¹Ÿåº”è¯¥ä¼šçœ‹åˆ°ååé‡çš„å¢åŠ ã€‚
- en: Configuring the Accelerator
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é…ç½®åŠ é€Ÿå™¨
- en: Currently two different backends for FP8 are supported (`TransformersEngine`
    and `MS-AMP`), each with different capabilities and configurations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰æ”¯æŒä¸¤ç§ä¸åŒçš„FP8åç«¯ï¼ˆ`TransformersEngine`å’Œ`MS-AMP`ï¼‰ï¼Œæ¯ä¸ªéƒ½å…·æœ‰ä¸åŒçš„åŠŸèƒ½å’Œé…ç½®ã€‚
- en: 'To use either, the same core API is used. Just pass `mixed_precision="fp8"`
    to either the [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator),
    during `accelerate config` when prompted about mixed precision, or as part of
    your `config.yaml` file in the `mixed_precision` key:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨ä»»ä½•ä¸€ä¸ªï¼Œéƒ½ä½¿ç”¨ç›¸åŒçš„æ ¸å¿ƒAPIã€‚åªéœ€å°†`mixed_precision="fp8"`ä¼ é€’ç»™[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)ä¹‹ä¸€ï¼Œåœ¨`accelerate
    config`æœŸé—´ï¼Œå½“è¯¢é—®æ··åˆç²¾åº¦æ—¶ï¼Œæˆ–ä½œä¸º`config.yaml`æ–‡ä»¶ä¸­çš„`mixed_precision`é”®çš„ä¸€éƒ¨åˆ†ï¼š
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'By default, if `MS-AMP` is available in your environment, ğŸ¤— Accelerate will
    automatically utilize it as a backend. To specify it yourself (and customize other
    parts of the FP8 mixed precision setup), you can utilize the [utils.FP8RecipeKwargs](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.FP8RecipeKwargs):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæ‚¨çš„ç¯å¢ƒä¸­æœ‰`MS-AMP`ï¼ŒğŸ¤— Accelerateå°†è‡ªåŠ¨å°†å…¶ç”¨ä½œåç«¯ã€‚è¦è‡ªè¡ŒæŒ‡å®šå®ƒï¼ˆå¹¶è‡ªå®šä¹‰FP8æ··åˆç²¾åº¦è®¾ç½®çš„å…¶ä»–éƒ¨åˆ†ï¼‰ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[utils.FP8RecipeKwargs](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.FP8RecipeKwargs)ï¼š
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Configuring MS-AMP
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é…ç½®MS-AMP
- en: 'Of the two, `MS-AMP` is traditionally the easier one to configure as there
    is only a single argument: the optimization level.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸¤è€…ä¸­ï¼Œ`MS-AMP`ä¼ ç»Ÿä¸Šæ›´å®¹æ˜“é…ç½®ï¼Œå› ä¸ºåªæœ‰ä¸€ä¸ªå‚æ•°ï¼šä¼˜åŒ–çº§åˆ«ã€‚
- en: Currently two levels of optimization are supported in the ğŸ¤— Accelerate integration,
    `"O1"` and `"O2"` (using the letter â€˜oâ€™, not zero).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ğŸ¤— Accelerateé›†æˆæ”¯æŒä¸¤ä¸ªä¼˜åŒ–çº§åˆ«ï¼Œ`"O1"`å’Œ`"O2"`ï¼ˆä½¿ç”¨å­—æ¯â€˜oâ€™ï¼Œè€Œä¸æ˜¯é›¶ï¼‰ã€‚
- en: '`"O1"` will cast the weight gradients and `all_reduce` communications to happen
    in 8-bit, while the rest are done in 16 bit. This reduces the general GPU memory
    usage and speeds up communication bandwidths.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"O1"`å°†å°†æƒé‡æ¢¯åº¦å’Œ`all_reduce`é€šä¿¡è½¬æ¢ä¸º8ä½ï¼Œè€Œå…¶ä½™éƒ¨åˆ†å°†ä»¥16ä½å®Œæˆã€‚è¿™å‡å°‘äº†ä¸€èˆ¬GPUå†…å­˜ä½¿ç”¨é‡å¹¶åŠ å¿«äº†é€šä¿¡å¸¦å®½ã€‚'
- en: '`"O2"` will also cast first-order optimizer states into 8 bit, while the second
    order states are in FP16\. (Currently just the `Adam` optimizer is supported).
    This tries itâ€™s best to minimize final accuracy degradation and will save the
    highest potential memory.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"O2"`è¿˜å°†ä¸€é˜¶ä¼˜åŒ–å™¨çŠ¶æ€è½¬æ¢ä¸º8ä½ï¼Œè€ŒäºŒé˜¶çŠ¶æ€ä¸ºFP16ï¼ˆç›®å‰ä»…æ”¯æŒ`Adam`ä¼˜åŒ–å™¨ï¼‰ã€‚è¿™å°½æœ€å¤§åŠªåŠ›æœ€å°åŒ–æœ€ç»ˆå‡†ç¡®æ€§çš„é™çº§ï¼Œå¹¶å°†èŠ‚çœæœ€é«˜æ½œåœ¨å†…å­˜ã€‚'
- en: 'To specify an optimization level, pass it to the `FP8KwargsHandler` by setting
    the `optimization_level` argument:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æŒ‡å®šä¼˜åŒ–çº§åˆ«ï¼Œè¯·å°†å…¶ä¼ é€’ç»™`FP8KwargsHandler`ï¼Œè®¾ç½®`optimization_level`å‚æ•°ï¼š
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Configuring TransformersEngine
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é…ç½®TransformersEngine
- en: TransformersEngine has much more available for customizing how and what FP8
    calculations are performed. A full list of supported arguments and what they mean
    are available in [NVIDIAâ€™s documentation](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html),
    however they are restated as part of `FP8KwargsHandler`â€™s docstring for your convience.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: TransformersEngineæœ‰æ›´å¤šå¯ç”¨äºè‡ªå®šä¹‰å¦‚ä½•ä»¥åŠä½•æ—¶æ‰§è¡ŒFP8è®¡ç®—çš„å†…å®¹ã€‚æ”¯æŒçš„å‚æ•°åˆ—è¡¨å’Œå®ƒä»¬çš„å«ä¹‰åœ¨[NVIDIAæ–‡æ¡£](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html)ä¸­éƒ½æœ‰ï¼Œä½†æ˜¯å®ƒä»¬ä¹Ÿä½œä¸º`FP8KwargsHandler`çš„docstringçš„ä¸€éƒ¨åˆ†é‡ç”³ï¼Œä»¥æ–¹ä¾¿æ‚¨æŸ¥çœ‹ã€‚
- en: ğŸ¤— Accelerate tries to set sensible defaults, but exploring and tweaking the
    various parameters yourself can lead to better performance potentially.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Accelerateå°è¯•è®¾ç½®åˆç†çš„é»˜è®¤å€¼ï¼Œä½†æ˜¯è‡ªå·±æ¢ç´¢å’Œè°ƒæ•´å„ç§å‚æ•°å¯èƒ½ä¼šå¯¼è‡´æ›´å¥½çš„æ€§èƒ½ã€‚
- en: 'To use it, specify `backend="te"` and modify any of the arguments you want
    as part of your kwarg handler:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨å®ƒï¼Œè¯·æŒ‡å®š`backend="te"`å¹¶ä¿®æ”¹æ‚¨æƒ³è¦ä½œä¸ºkwargå¤„ç†ç¨‹åºçš„ä»»ä½•å‚æ•°ï¼š
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Futher Reading
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»
- en: 'To learn more about training in FP8 please check out the following resources:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤šå…³äºåœ¨FP8ä¸­è®­ç»ƒçš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä»¥ä¸‹èµ„æºï¼š
- en: '[Our concept guide](../concept_guides/low_precision_training.md) detailing
    into more about both TransformersEngine and MS-AMP'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æˆ‘ä»¬çš„æ¦‚å¿µæŒ‡å—](../concept_guides/low_precision_training.md)è¯¦ç»†ä»‹ç»äº†TransformersEngineå’ŒMS-AMPã€‚'
- en: '[The `transformers-engine` documentation](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[transformers-engineæ–‡æ¡£](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html)'
- en: '[The `MS-AMP` documentation](https://azure.github.io/MS-AMP/docs/)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MS-AMPæ–‡æ¡£](https://azure.github.io/MS-AMP/docs/)'
