- en: Create a custom architecture
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建自定义架构
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/create_a_model](https://huggingface.co/docs/transformers/v4.37.2/en/create_a_model)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '原文链接: [https://huggingface.co/docs/transformers/v4.37.2/en/create_a_model](https://huggingface.co/docs/transformers/v4.37.2/en/create_a_model)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'An [`AutoClass`](model_doc/auto) automatically infers the model architecture
    and downloads pretrained configuration and weights. Generally, we recommend using
    an `AutoClass` to produce checkpoint-agnostic code. But users who want more control
    over specific model parameters can create a custom 🤗 Transformers model from just
    a few base classes. This could be particularly useful for anyone who is interested
    in studying, training or experimenting with a 🤗 Transformers model. In this guide,
    dive deeper into creating a custom model without an `AutoClass`. Learn how to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[`AutoClass`](model_doc/auto)会自动推断模型架构并下载预训练配置和权重。通常，我们建议使用`AutoClass`生成与检查点无关的代码。但是，希望对特定模型参数有更多控制的用户可以从几个基类创建一个自定义🤗
    Transformers模型。这对于任何对研究、训练或实验🤗 Transformers模型感兴趣的人特别有用。在本指南中，深入了解如何创建一个自定义模型而不使用`AutoClass`。学习如何：'
- en: Load and customize a model configuration.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载并自定义模型配置。
- en: Create a model architecture.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建模型架构。
- en: Create a slow and fast tokenizer for text.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为文本创建慢速和快速分词器。
- en: Create an image processor for vision tasks.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为视觉任务创建图像处理器。
- en: Create a feature extractor for audio tasks.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为音频任务创建特征提取器。
- en: Create a processor for multimodal tasks.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为多模态任务创建处理器。
- en: Configuration
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置
- en: A [configuration](main_classes/configuration) refers to a model’s specific attributes.
    Each model configuration has different attributes; for instance, all NLP models
    have the `hidden_size`, `num_attention_heads`, `num_hidden_layers` and `vocab_size`
    attributes in common. These attributes specify the number of attention heads or
    hidden layers to construct a model with.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[configuration](main_classes/configuration)指的是模型的特定属性。每个模型配置都有不同的属性；例如，所有NLP模型都共有`hidden_size`、`num_attention_heads`、`num_hidden_layers`和`vocab_size`属性。这些属性指定了构建模型所需的注意力头或隐藏层的数量。'
- en: 'Get a closer look at [DistilBERT](model_doc/distilbert) by accessing [DistilBertConfig](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertConfig)
    to inspect it’s attributes:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问[DistilBertConfig](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertConfig)来查看其属性，进一步了解[DistilBERT](model_doc/distilbert)：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[DistilBertConfig](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertConfig)
    displays all the default attributes used to build a base [DistilBertModel](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertModel).
    All attributes are customizable, creating space for experimentation. For example,
    you can customize a default model to:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[DistilBertConfig](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertConfig)显示了用于构建基本[DistilBertModel](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertModel)的所有默认属性。所有属性都是可定制的，为实验创造了空间。例如，您可以自定义一个默认模型：'
- en: Try a different activation function with the `activation` parameter.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`activation`参数尝试不同的激活函数。
- en: Use a higher dropout ratio for the attention probabilities with the `attention_dropout`
    parameter.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`attention_dropout`参数为注意力概率设置更高的dropout比率。
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Pretrained model attributes can be modified in the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)
    function:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型属性可以在[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)函数中修改：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once you are satisfied with your model configuration, you can save it with
    [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained).
    Your configuration file is stored as a JSON file in the specified save directory:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您满意您的模型配置，您可以用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained)保存它。您的配置文件将以JSON文件的形式存储在指定的保存目录中：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To reuse the configuration file, load it with [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要重用配置文件，请使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)加载它：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can also save your configuration file as a dictionary or even just the difference
    between your custom configuration attributes and the default configuration attributes!
    See the [configuration](main_classes/configuration) documentation for more details.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将配置文件保存为字典，甚至只保存自定义配置属性与默认配置属性之间的差异！查看[configuration](main_classes/configuration)文档以获取更多详细信息。
- en: Model
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: The next step is to create a [model](main_classes/models). The model - also
    loosely referred to as the architecture - defines what each layer is doing and
    what operations are happening. Attributes like `num_hidden_layers` from the configuration
    are used to define the architecture. Every model shares the base class [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    and a few common methods like resizing input embeddings and pruning self-attention
    heads. In addition, all models are also either a [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html),
    [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    or [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)
    subclass. This means models are compatible with each of their respective framework’s
    usage.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个[model](main_classes/models)。模型 - 也宽泛地称为架构 - 定义了每个层正在做什么以及正在发生的操作。像`num_hidden_layers`这样的配置属性用于定义架构。每个模型都共享基类[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)和一些常见方法，如调整输入嵌入和修剪自注意力头。此外，所有模型也是[`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)、[`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)或[`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)的子类。这意味着模型与各自框架的使用是兼容的。
- en: PytorchHide Pytorch content
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏Pytorch内容
- en: 'Load your custom configuration attributes into the model:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This creates a model with random values instead of pretrained weights. You won’t
    be able to use this model for anything useful yet until you train it. Training
    is a costly and time-consuming process. It is generally better to use a pretrained
    model to obtain better results faster, while using only a fraction of the resources
    required for training.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a pretrained model with [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When you load pretrained weights, the default model configuration is automatically
    loaded if the model is provided by 🤗 Transformers. However, you can still replace
    - some or all of - the default model configuration attributes with your own if
    you’d like:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: TensorFlowHide TensorFlow content
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Load your custom configuration attributes into the model:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This creates a model with random values instead of pretrained weights. You won’t
    be able to use this model for anything useful yet until you train it. Training
    is a costly and time-consuming process. It is generally better to use a pretrained
    model to obtain better results faster, while using only a fraction of the resources
    required for training.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a pretrained model with [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When you load pretrained weights, the default model configuration is automatically
    loaded if the model is provided by 🤗 Transformers. However, you can still replace
    - some or all of - the default model configuration attributes with your own if
    you’d like:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Model heads
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, you have a base DistilBERT model which outputs the *hidden states*.
    The hidden states are passed as inputs to a model head to produce the final output.
    🤗 Transformers provides a different model head for each task as long as a model
    supports the task (i.e., you can’t use DistilBERT for a sequence-to-sequence task
    like translation).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: For example, [DistilBertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)
    is a base DistilBERT model with a sequence classification head. The sequence classification
    head is a linear layer on top of the pooled outputs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Easily reuse this checkpoint for another task by switching to a different model
    head. For a question answering task, you would use the [DistilBertForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering)
    model head. The question answering head is similar to the sequence classification
    head except it is a linear layer on top of the hidden states output.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: TensorFlowHide TensorFlow content
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: For example, [TFDistilBertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification)
    is a base DistilBERT model with a sequence classification head. The sequence classification
    head is a linear layer on top of the pooled outputs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Easily reuse this checkpoint for another task by switching to a different model
    head. For a question answering task, you would use the [TFDistilBertForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering)
    model head. The question answering head is similar to the sequence classification
    head except it is a linear layer on top of the hidden states output.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Tokenizer
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The last base class you need before using a model for textual data is a [tokenizer](main_classes/tokenizer)
    to convert raw text to tensors. There are two types of tokenizers you can use
    with 🤗 Transformers:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer):
    a Python implementation of a tokenizer.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast):
    a tokenizer from our Rust-based [🤗 Tokenizer](https://huggingface.co/docs/tokenizers/python/latest/)
    library. This tokenizer type is significantly faster - especially during batch
    tokenization - due to its Rust implementation. The fast tokenizer also offers
    additional methods like *offset mapping* which maps tokens to their original words
    or characters.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both tokenizers support common methods such as encoding and decoding, adding
    new tokens, and managing special tokens.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Not every model supports a fast tokenizer. Take a look at this [table](index#supported-frameworks)
    to check if a model has fast tokenizer support.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'If you trained your own tokenizer, you can create one from your *vocabulary*
    file:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It is important to remember the vocabulary from a custom tokenizer will be
    different from the vocabulary generated by a pretrained model’s tokenizer. You
    need to use a pretrained model’s vocabulary if you are using a pretrained model,
    otherwise the inputs won’t make sense. Create a tokenizer with a pretrained model’s
    vocabulary with the [DistilBertTokenizer](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertTokenizer)
    class:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Create a fast tokenizer with the [DistilBertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertTokenizerFast)
    class:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: By default, [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    will try to load a fast tokenizer. You can disable this behavior by setting `use_fast=False`
    in `from_pretrained`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Image Processor
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An image processor processes vision inputs. It inherits from the base [ImageProcessingMixin](/docs/transformers/v4.37.2/en/internal/image_processing_utils#transformers.ImageProcessingMixin)
    class.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'To use, create an image processor associated with the model you’re using. For
    example, create a default [ViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor)
    if you are using [ViT](model_doc/vit) for image classification:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If you aren’t looking for any customization, just use the `from_pretrained`
    method to load a model’s default image processor parameters.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Modify any of the [ViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor)
    parameters to create your custom image processor:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Feature Extractor
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A feature extractor processes audio inputs. It inherits from the base [FeatureExtractionMixin](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin)
    class, and may also inherit from the [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    class for processing audio inputs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'To use, create a feature extractor associated with the model you’re using.
    For example, create a default [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)
    if you are using [Wav2Vec2](model_doc/wav2vec2) for audio classification:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If you aren’t looking for any customization, just use the `from_pretrained`
    method to load a model’s default feature extractor parameters.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'Modify any of the [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)
    parameters to create your custom feature extractor:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Processor
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For models that support multimodal tasks, 🤗 Transformers offers a processor
    class that conveniently wraps processing classes such as a feature extractor and
    a tokenizer into a single object. For example, let’s use the [Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)
    for an automatic speech recognition task (ASR). ASR transcribes audio to text,
    so you will need a feature extractor and a tokenizer.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a feature extractor to handle the audio inputs:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Create a tokenizer to handle the text inputs:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个分词器来处理文本输入：
- en: '[PRE23]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Combine the feature extractor and tokenizer in [Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 将特征提取器和分词器组合在[Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)中：
- en: '[PRE24]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: With two basic classes - configuration and model - and an additional preprocessing
    class (tokenizer, image processor, feature extractor, or processor), you can create
    any of the models supported by 🤗 Transformers. Each of these base classes are
    configurable, allowing you to use the specific attributes you want. You can easily
    setup a model for training or modify an existing pretrained model to fine-tune.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通过两个基本类 - 配置和模型 - 以及额外的预处理类（分词器、图像处理器、特征提取器或处理器），您可以创建🤗 Transformers支持的任何模型。这些基本类都是可配置的，允许您使用您想要的特定属性。您可以轻松设置一个用于训练的模型或修改现有的预训练模型进行微调。
