- en: Create a custom architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/create_a_model](https://huggingface.co/docs/transformers/v4.37.2/en/create_a_model)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/14.cf64dd14.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Markdown.fef84341.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/stores.c16bc1a5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'An [`AutoClass`](model_doc/auto) automatically infers the model architecture
    and downloads pretrained configuration and weights. Generally, we recommend using
    an `AutoClass` to produce checkpoint-agnostic code. But users who want more control
    over specific model parameters can create a custom ðŸ¤— Transformers model from just
    a few base classes. This could be particularly useful for anyone who is interested
    in studying, training or experimenting with a ðŸ¤— Transformers model. In this guide,
    dive deeper into creating a custom model without an `AutoClass`. Learn how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Load and customize a model configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a model architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a slow and fast tokenizer for text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create an image processor for vision tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a feature extractor for audio tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a processor for multimodal tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A [configuration](main_classes/configuration) refers to a modelâ€™s specific attributes.
    Each model configuration has different attributes; for instance, all NLP models
    have the `hidden_size`, `num_attention_heads`, `num_hidden_layers` and `vocab_size`
    attributes in common. These attributes specify the number of attention heads or
    hidden layers to construct a model with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Get a closer look at [DistilBERT](model_doc/distilbert) by accessing [DistilBertConfig](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertConfig)
    to inspect itâ€™s attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[DistilBertConfig](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertConfig)
    displays all the default attributes used to build a base [DistilBertModel](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertModel).
    All attributes are customizable, creating space for experimentation. For example,
    you can customize a default model to:'
  prefs: []
  type: TYPE_NORMAL
- en: Try a different activation function with the `activation` parameter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a higher dropout ratio for the attention probabilities with the `attention_dropout`
    parameter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Pretrained model attributes can be modified in the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you are satisfied with your model configuration, you can save it with
    [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained).
    Your configuration file is stored as a JSON file in the specified save directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To reuse the configuration file, load it with [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can also save your configuration file as a dictionary or even just the difference
    between your custom configuration attributes and the default configuration attributes!
    See the [configuration](main_classes/configuration) documentation for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to create a [model](main_classes/models). The model - also
    loosely referred to as the architecture - defines what each layer is doing and
    what operations are happening. Attributes like `num_hidden_layers` from the configuration
    are used to define the architecture. Every model shares the base class [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    and a few common methods like resizing input embeddings and pruning self-attention
    heads. In addition, all models are also either a [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html),
    [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    or [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)
    subclass. This means models are compatible with each of their respective frameworkâ€™s
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: 'Load your custom configuration attributes into the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This creates a model with random values instead of pretrained weights. You wonâ€™t
    be able to use this model for anything useful yet until you train it. Training
    is a costly and time-consuming process. It is generally better to use a pretrained
    model to obtain better results faster, while using only a fraction of the resources
    required for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a pretrained model with [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When you load pretrained weights, the default model configuration is automatically
    loaded if the model is provided by ðŸ¤— Transformers. However, you can still replace
    - some or all of - the default model configuration attributes with your own if
    youâ€™d like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: 'Load your custom configuration attributes into the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This creates a model with random values instead of pretrained weights. You wonâ€™t
    be able to use this model for anything useful yet until you train it. Training
    is a costly and time-consuming process. It is generally better to use a pretrained
    model to obtain better results faster, while using only a fraction of the resources
    required for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a pretrained model with [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When you load pretrained weights, the default model configuration is automatically
    loaded if the model is provided by ðŸ¤— Transformers. However, you can still replace
    - some or all of - the default model configuration attributes with your own if
    youâ€™d like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Model heads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, you have a base DistilBERT model which outputs the *hidden states*.
    The hidden states are passed as inputs to a model head to produce the final output.
    ðŸ¤— Transformers provides a different model head for each task as long as a model
    supports the task (i.e., you canâ€™t use DistilBERT for a sequence-to-sequence task
    like translation).
  prefs: []
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: For example, [DistilBertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)
    is a base DistilBERT model with a sequence classification head. The sequence classification
    head is a linear layer on top of the pooled outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Easily reuse this checkpoint for another task by switching to a different model
    head. For a question answering task, you would use the [DistilBertForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering)
    model head. The question answering head is similar to the sequence classification
    head except it is a linear layer on top of the hidden states output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: For example, [TFDistilBertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification)
    is a base DistilBERT model with a sequence classification head. The sequence classification
    head is a linear layer on top of the pooled outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Easily reuse this checkpoint for another task by switching to a different model
    head. For a question answering task, you would use the [TFDistilBertForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering)
    model head. The question answering head is similar to the sequence classification
    head except it is a linear layer on top of the hidden states output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The last base class you need before using a model for textual data is a [tokenizer](main_classes/tokenizer)
    to convert raw text to tensors. There are two types of tokenizers you can use
    with ðŸ¤— Transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer):
    a Python implementation of a tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast):
    a tokenizer from our Rust-based [ðŸ¤— Tokenizer](https://huggingface.co/docs/tokenizers/python/latest/)
    library. This tokenizer type is significantly faster - especially during batch
    tokenization - due to its Rust implementation. The fast tokenizer also offers
    additional methods like *offset mapping* which maps tokens to their original words
    or characters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both tokenizers support common methods such as encoding and decoding, adding
    new tokens, and managing special tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Not every model supports a fast tokenizer. Take a look at this [table](index#supported-frameworks)
    to check if a model has fast tokenizer support.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you trained your own tokenizer, you can create one from your *vocabulary*
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to remember the vocabulary from a custom tokenizer will be
    different from the vocabulary generated by a pretrained modelâ€™s tokenizer. You
    need to use a pretrained modelâ€™s vocabulary if you are using a pretrained model,
    otherwise the inputs wonâ€™t make sense. Create a tokenizer with a pretrained modelâ€™s
    vocabulary with the [DistilBertTokenizer](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertTokenizer)
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a fast tokenizer with the [DistilBertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertTokenizerFast)
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: By default, [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    will try to load a fast tokenizer. You can disable this behavior by setting `use_fast=False`
    in `from_pretrained`.
  prefs: []
  type: TYPE_NORMAL
- en: Image Processor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An image processor processes vision inputs. It inherits from the base [ImageProcessingMixin](/docs/transformers/v4.37.2/en/internal/image_processing_utils#transformers.ImageProcessingMixin)
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use, create an image processor associated with the model youâ€™re using. For
    example, create a default [ViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor)
    if you are using [ViT](model_doc/vit) for image classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: If you arenâ€™t looking for any customization, just use the `from_pretrained`
    method to load a modelâ€™s default image processor parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modify any of the [ViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor)
    parameters to create your custom image processor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Feature Extractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A feature extractor processes audio inputs. It inherits from the base [FeatureExtractionMixin](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin)
    class, and may also inherit from the [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    class for processing audio inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use, create a feature extractor associated with the model youâ€™re using.
    For example, create a default [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)
    if you are using [Wav2Vec2](model_doc/wav2vec2) for audio classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: If you arenâ€™t looking for any customization, just use the `from_pretrained`
    method to load a modelâ€™s default feature extractor parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modify any of the [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)
    parameters to create your custom feature extractor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Processor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For models that support multimodal tasks, ðŸ¤— Transformers offers a processor
    class that conveniently wraps processing classes such as a feature extractor and
    a tokenizer into a single object. For example, letâ€™s use the [Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)
    for an automatic speech recognition task (ASR). ASR transcribes audio to text,
    so you will need a feature extractor and a tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a feature extractor to handle the audio inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a tokenizer to handle the text inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Combine the feature extractor and tokenizer in [Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: With two basic classes - configuration and model - and an additional preprocessing
    class (tokenizer, image processor, feature extractor, or processor), you can create
    any of the models supported by ðŸ¤— Transformers. Each of these base classes are
    configurable, allowing you to use the specific attributes you want. You can easily
    setup a model for training or modify an existing pretrained model to fine-tune.
  prefs: []
  type: TYPE_NORMAL
