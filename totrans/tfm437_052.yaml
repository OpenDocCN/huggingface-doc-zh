- en: Create a custom architecture
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ›å»ºè‡ªå®šä¹‰æ¶æ„
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/create_a_model](https://huggingface.co/docs/transformers/v4.37.2/en/create_a_model)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'åŸæ–‡é“¾æ¥: [https://huggingface.co/docs/transformers/v4.37.2/en/create_a_model](https://huggingface.co/docs/transformers/v4.37.2/en/create_a_model)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'An [`AutoClass`](model_doc/auto) automatically infers the model architecture
    and downloads pretrained configuration and weights. Generally, we recommend using
    an `AutoClass` to produce checkpoint-agnostic code. But users who want more control
    over specific model parameters can create a custom ğŸ¤— Transformers model from just
    a few base classes. This could be particularly useful for anyone who is interested
    in studying, training or experimenting with a ğŸ¤— Transformers model. In this guide,
    dive deeper into creating a custom model without an `AutoClass`. Learn how to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[`AutoClass`](model_doc/auto)ä¼šè‡ªåŠ¨æ¨æ–­æ¨¡å‹æ¶æ„å¹¶ä¸‹è½½é¢„è®­ç»ƒé…ç½®å’Œæƒé‡ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨`AutoClass`ç”Ÿæˆä¸æ£€æŸ¥ç‚¹æ— å…³çš„ä»£ç ã€‚ä½†æ˜¯ï¼Œå¸Œæœ›å¯¹ç‰¹å®šæ¨¡å‹å‚æ•°æœ‰æ›´å¤šæ§åˆ¶çš„ç”¨æˆ·å¯ä»¥ä»å‡ ä¸ªåŸºç±»åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰ğŸ¤—
    Transformersæ¨¡å‹ã€‚è¿™å¯¹äºä»»ä½•å¯¹ç ”ç©¶ã€è®­ç»ƒæˆ–å®éªŒğŸ¤— Transformersæ¨¡å‹æ„Ÿå…´è¶£çš„äººç‰¹åˆ«æœ‰ç”¨ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ·±å…¥äº†è§£å¦‚ä½•åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰æ¨¡å‹è€Œä¸ä½¿ç”¨`AutoClass`ã€‚å­¦ä¹ å¦‚ä½•ï¼š'
- en: Load and customize a model configuration.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ è½½å¹¶è‡ªå®šä¹‰æ¨¡å‹é…ç½®ã€‚
- en: Create a model architecture.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ›å»ºæ¨¡å‹æ¶æ„ã€‚
- en: Create a slow and fast tokenizer for text.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºæ–‡æœ¬åˆ›å»ºæ…¢é€Ÿå’Œå¿«é€Ÿåˆ†è¯å™¨ã€‚
- en: Create an image processor for vision tasks.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºè§†è§‰ä»»åŠ¡åˆ›å»ºå›¾åƒå¤„ç†å™¨ã€‚
- en: Create a feature extractor for audio tasks.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºéŸ³é¢‘ä»»åŠ¡åˆ›å»ºç‰¹å¾æå–å™¨ã€‚
- en: Create a processor for multimodal tasks.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºå¤šæ¨¡æ€ä»»åŠ¡åˆ›å»ºå¤„ç†å™¨ã€‚
- en: Configuration
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é…ç½®
- en: A [configuration](main_classes/configuration) refers to a modelâ€™s specific attributes.
    Each model configuration has different attributes; for instance, all NLP models
    have the `hidden_size`, `num_attention_heads`, `num_hidden_layers` and `vocab_size`
    attributes in common. These attributes specify the number of attention heads or
    hidden layers to construct a model with.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[configuration](main_classes/configuration)æŒ‡çš„æ˜¯æ¨¡å‹çš„ç‰¹å®šå±æ€§ã€‚æ¯ä¸ªæ¨¡å‹é…ç½®éƒ½æœ‰ä¸åŒçš„å±æ€§ï¼›ä¾‹å¦‚ï¼Œæ‰€æœ‰NLPæ¨¡å‹éƒ½å…±æœ‰`hidden_size`ã€`num_attention_heads`ã€`num_hidden_layers`å’Œ`vocab_size`å±æ€§ã€‚è¿™äº›å±æ€§æŒ‡å®šäº†æ„å»ºæ¨¡å‹æ‰€éœ€çš„æ³¨æ„åŠ›å¤´æˆ–éšè—å±‚çš„æ•°é‡ã€‚'
- en: 'Get a closer look at [DistilBERT](model_doc/distilbert) by accessing [DistilBertConfig](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertConfig)
    to inspect itâ€™s attributes:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è®¿é—®[DistilBertConfig](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertConfig)æ¥æŸ¥çœ‹å…¶å±æ€§ï¼Œè¿›ä¸€æ­¥äº†è§£[DistilBERT](model_doc/distilbert)ï¼š
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[DistilBertConfig](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertConfig)
    displays all the default attributes used to build a base [DistilBertModel](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertModel).
    All attributes are customizable, creating space for experimentation. For example,
    you can customize a default model to:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[DistilBertConfig](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertConfig)æ˜¾ç¤ºäº†ç”¨äºæ„å»ºåŸºæœ¬[DistilBertModel](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertModel)çš„æ‰€æœ‰é»˜è®¤å±æ€§ã€‚æ‰€æœ‰å±æ€§éƒ½æ˜¯å¯å®šåˆ¶çš„ï¼Œä¸ºå®éªŒåˆ›é€ äº†ç©ºé—´ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥è‡ªå®šä¹‰ä¸€ä¸ªé»˜è®¤æ¨¡å‹ï¼š'
- en: Try a different activation function with the `activation` parameter.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`activation`å‚æ•°å°è¯•ä¸åŒçš„æ¿€æ´»å‡½æ•°ã€‚
- en: Use a higher dropout ratio for the attention probabilities with the `attention_dropout`
    parameter.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`attention_dropout`å‚æ•°ä¸ºæ³¨æ„åŠ›æ¦‚ç‡è®¾ç½®æ›´é«˜çš„dropoutæ¯”ç‡ã€‚
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Pretrained model attributes can be modified in the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)
    function:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„è®­ç»ƒæ¨¡å‹å±æ€§å¯ä»¥åœ¨[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)å‡½æ•°ä¸­ä¿®æ”¹ï¼š
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once you are satisfied with your model configuration, you can save it with
    [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained).
    Your configuration file is stored as a JSON file in the specified save directory:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨æ»¡æ„æ‚¨çš„æ¨¡å‹é…ç½®ï¼Œæ‚¨å¯ä»¥ç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained)ä¿å­˜å®ƒã€‚æ‚¨çš„é…ç½®æ–‡ä»¶å°†ä»¥JSONæ–‡ä»¶çš„å½¢å¼å­˜å‚¨åœ¨æŒ‡å®šçš„ä¿å­˜ç›®å½•ä¸­ï¼š
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To reuse the configuration file, load it with [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¦é‡ç”¨é…ç½®æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)åŠ è½½å®ƒï¼š
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can also save your configuration file as a dictionary or even just the difference
    between your custom configuration attributes and the default configuration attributes!
    See the [configuration](main_classes/configuration) documentation for more details.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥å°†é…ç½®æ–‡ä»¶ä¿å­˜ä¸ºå­—å…¸ï¼Œç”šè‡³åªä¿å­˜è‡ªå®šä¹‰é…ç½®å±æ€§ä¸é»˜è®¤é…ç½®å±æ€§ä¹‹é—´çš„å·®å¼‚ï¼æŸ¥çœ‹[configuration](main_classes/configuration)æ–‡æ¡£ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
- en: Model
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹
- en: The next step is to create a [model](main_classes/models). The model - also
    loosely referred to as the architecture - defines what each layer is doing and
    what operations are happening. Attributes like `num_hidden_layers` from the configuration
    are used to define the architecture. Every model shares the base class [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    and a few common methods like resizing input embeddings and pruning self-attention
    heads. In addition, all models are also either a [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html),
    [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    or [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)
    subclass. This means models are compatible with each of their respective frameworkâ€™s
    usage.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯åˆ›å»ºä¸€ä¸ª[model](main_classes/models)ã€‚æ¨¡å‹ - ä¹Ÿå®½æ³›åœ°ç§°ä¸ºæ¶æ„ - å®šä¹‰äº†æ¯ä¸ªå±‚æ­£åœ¨åšä»€ä¹ˆä»¥åŠæ­£åœ¨å‘ç”Ÿçš„æ“ä½œã€‚åƒ`num_hidden_layers`è¿™æ ·çš„é…ç½®å±æ€§ç”¨äºå®šä¹‰æ¶æ„ã€‚æ¯ä¸ªæ¨¡å‹éƒ½å…±äº«åŸºç±»[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)å’Œä¸€äº›å¸¸è§æ–¹æ³•ï¼Œå¦‚è°ƒæ•´è¾“å…¥åµŒå…¥å’Œä¿®å‰ªè‡ªæ³¨æ„åŠ›å¤´ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰æ¨¡å‹ä¹Ÿæ˜¯[`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)ã€[`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)æˆ–[`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)çš„å­ç±»ã€‚è¿™æ„å‘³ç€æ¨¡å‹ä¸å„è‡ªæ¡†æ¶çš„ä½¿ç”¨æ˜¯å…¼å®¹çš„ã€‚
- en: PytorchHide Pytorch content
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè—Pytorchå†…å®¹
- en: 'Load your custom configuration attributes into the model:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This creates a model with random values instead of pretrained weights. You wonâ€™t
    be able to use this model for anything useful yet until you train it. Training
    is a costly and time-consuming process. It is generally better to use a pretrained
    model to obtain better results faster, while using only a fraction of the resources
    required for training.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a pretrained model with [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When you load pretrained weights, the default model configuration is automatically
    loaded if the model is provided by ğŸ¤— Transformers. However, you can still replace
    - some or all of - the default model configuration attributes with your own if
    youâ€™d like:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: TensorFlowHide TensorFlow content
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Load your custom configuration attributes into the model:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This creates a model with random values instead of pretrained weights. You wonâ€™t
    be able to use this model for anything useful yet until you train it. Training
    is a costly and time-consuming process. It is generally better to use a pretrained
    model to obtain better results faster, while using only a fraction of the resources
    required for training.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a pretrained model with [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When you load pretrained weights, the default model configuration is automatically
    loaded if the model is provided by ğŸ¤— Transformers. However, you can still replace
    - some or all of - the default model configuration attributes with your own if
    youâ€™d like:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Model heads
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, you have a base DistilBERT model which outputs the *hidden states*.
    The hidden states are passed as inputs to a model head to produce the final output.
    ğŸ¤— Transformers provides a different model head for each task as long as a model
    supports the task (i.e., you canâ€™t use DistilBERT for a sequence-to-sequence task
    like translation).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: For example, [DistilBertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)
    is a base DistilBERT model with a sequence classification head. The sequence classification
    head is a linear layer on top of the pooled outputs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Easily reuse this checkpoint for another task by switching to a different model
    head. For a question answering task, you would use the [DistilBertForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering)
    model head. The question answering head is similar to the sequence classification
    head except it is a linear layer on top of the hidden states output.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: TensorFlowHide TensorFlow content
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: For example, [TFDistilBertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification)
    is a base DistilBERT model with a sequence classification head. The sequence classification
    head is a linear layer on top of the pooled outputs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Easily reuse this checkpoint for another task by switching to a different model
    head. For a question answering task, you would use the [TFDistilBertForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering)
    model head. The question answering head is similar to the sequence classification
    head except it is a linear layer on top of the hidden states output.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Tokenizer
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The last base class you need before using a model for textual data is a [tokenizer](main_classes/tokenizer)
    to convert raw text to tensors. There are two types of tokenizers you can use
    with ğŸ¤— Transformers:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer):
    a Python implementation of a tokenizer.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast):
    a tokenizer from our Rust-based [ğŸ¤— Tokenizer](https://huggingface.co/docs/tokenizers/python/latest/)
    library. This tokenizer type is significantly faster - especially during batch
    tokenization - due to its Rust implementation. The fast tokenizer also offers
    additional methods like *offset mapping* which maps tokens to their original words
    or characters.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both tokenizers support common methods such as encoding and decoding, adding
    new tokens, and managing special tokens.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Not every model supports a fast tokenizer. Take a look at this [table](index#supported-frameworks)
    to check if a model has fast tokenizer support.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'If you trained your own tokenizer, you can create one from your *vocabulary*
    file:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It is important to remember the vocabulary from a custom tokenizer will be
    different from the vocabulary generated by a pretrained modelâ€™s tokenizer. You
    need to use a pretrained modelâ€™s vocabulary if you are using a pretrained model,
    otherwise the inputs wonâ€™t make sense. Create a tokenizer with a pretrained modelâ€™s
    vocabulary with the [DistilBertTokenizer](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertTokenizer)
    class:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Create a fast tokenizer with the [DistilBertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/distilbert#transformers.DistilBertTokenizerFast)
    class:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: By default, [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    will try to load a fast tokenizer. You can disable this behavior by setting `use_fast=False`
    in `from_pretrained`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Image Processor
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An image processor processes vision inputs. It inherits from the base [ImageProcessingMixin](/docs/transformers/v4.37.2/en/internal/image_processing_utils#transformers.ImageProcessingMixin)
    class.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'To use, create an image processor associated with the model youâ€™re using. For
    example, create a default [ViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor)
    if you are using [ViT](model_doc/vit) for image classification:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If you arenâ€™t looking for any customization, just use the `from_pretrained`
    method to load a modelâ€™s default image processor parameters.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Modify any of the [ViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor)
    parameters to create your custom image processor:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Feature Extractor
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A feature extractor processes audio inputs. It inherits from the base [FeatureExtractionMixin](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin)
    class, and may also inherit from the [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    class for processing audio inputs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'To use, create a feature extractor associated with the model youâ€™re using.
    For example, create a default [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)
    if you are using [Wav2Vec2](model_doc/wav2vec2) for audio classification:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If you arenâ€™t looking for any customization, just use the `from_pretrained`
    method to load a modelâ€™s default feature extractor parameters.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'Modify any of the [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)
    parameters to create your custom feature extractor:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Processor
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For models that support multimodal tasks, ğŸ¤— Transformers offers a processor
    class that conveniently wraps processing classes such as a feature extractor and
    a tokenizer into a single object. For example, letâ€™s use the [Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)
    for an automatic speech recognition task (ASR). ASR transcribes audio to text,
    so you will need a feature extractor and a tokenizer.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a feature extractor to handle the audio inputs:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Create a tokenizer to handle the text inputs:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªåˆ†è¯å™¨æ¥å¤„ç†æ–‡æœ¬è¾“å…¥ï¼š
- en: '[PRE23]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Combine the feature extractor and tokenizer in [Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨ç»„åˆåœ¨[Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)ä¸­ï¼š
- en: '[PRE24]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: With two basic classes - configuration and model - and an additional preprocessing
    class (tokenizer, image processor, feature extractor, or processor), you can create
    any of the models supported by ğŸ¤— Transformers. Each of these base classes are
    configurable, allowing you to use the specific attributes you want. You can easily
    setup a model for training or modify an existing pretrained model to fine-tune.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¸¤ä¸ªåŸºæœ¬ç±» - é…ç½®å’Œæ¨¡å‹ - ä»¥åŠé¢å¤–çš„é¢„å¤„ç†ç±»ï¼ˆåˆ†è¯å™¨ã€å›¾åƒå¤„ç†å™¨ã€ç‰¹å¾æå–å™¨æˆ–å¤„ç†å™¨ï¼‰ï¼Œæ‚¨å¯ä»¥åˆ›å»ºğŸ¤— Transformersæ”¯æŒçš„ä»»ä½•æ¨¡å‹ã€‚è¿™äº›åŸºæœ¬ç±»éƒ½æ˜¯å¯é…ç½®çš„ï¼Œå…è®¸æ‚¨ä½¿ç”¨æ‚¨æƒ³è¦çš„ç‰¹å®šå±æ€§ã€‚æ‚¨å¯ä»¥è½»æ¾è®¾ç½®ä¸€ä¸ªç”¨äºè®­ç»ƒçš„æ¨¡å‹æˆ–ä¿®æ”¹ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
