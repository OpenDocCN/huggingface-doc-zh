- en: Deploy models to Amazon SageMaker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型部署到Amazon SageMaker
- en: 'Original text: [https://huggingface.co/docs/sagemaker/inference](https://huggingface.co/docs/sagemaker/inference)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/sagemaker/inference](https://huggingface.co/docs/sagemaker/inference)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploying a 🤗 Transformers models in SageMaker for inference is as easy as:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在SageMaker中部署🤗 Transformers模型进行推理就像这样简单：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This guide will show you how to deploy models with zero-code using the [Inference
    Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit). The
    Inference Toolkit builds on top of the [`pipeline` feature](https://huggingface.co/docs/transformers/main_classes/pipelines)
    from 🤗 Transformers. Learn how to:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南将向您展示如何使用[推理工具包](https://github.com/aws/sagemaker-huggingface-inference-toolkit)零代码部署模型。推理工具包建立在🤗
    Transformers的[`pipeline`功能](https://huggingface.co/docs/transformers/main_classes/pipelines)之上。学习如何：
- en: '[Install and setup the Inference Toolkit](#installation-and-setup).'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[安装和设置推理工具包](#installation-and-setup)。'
- en: '[Deploy a 🤗 Transformers model trained in SageMaker](#deploy-a-transformer-model-trained-in-sagemaker).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在SageMaker中部署经过训练的🤗 Transformers模型](#deploy-a-transformer-model-trained-in-sagemaker)。'
- en: '[Deploy a 🤗 Transformers model from the Hugging Face [model Hub](https://huggingface.co/models)](#deploy-a-model-from-the-hub).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从Hugging Face [模型Hub](https://huggingface.co/models)部署🤗 Transformers模型](#deploy-a-model-from-the-hub)。'
- en: '[Run a Batch Transform Job using 🤗 Transformers and Amazon SageMaker](#run-batch-transform-with-transformers-and-sagemaker).'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用🤗 Transformers和Amazon SageMaker运行批量转换作业](#run-batch-transform-with-transformers-and-sagemaker)。'
- en: '[Create a custom inference module](#user-defined-code-and-modules).'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[创建自定义推理模块](#user-defined-code-and-modules)。'
- en: Installation and setup
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装和设置
- en: Before deploying a 🤗 Transformers model to SageMaker, you need to sign up for
    an AWS account. If you don’t have an AWS account yet, learn more [here](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在将🤗 Transformers模型部署到SageMaker之前，您需要注册AWS账户。如果您还没有AWS账户，请在[此处](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html)了解更多。
- en: 'Once you have an AWS account, get started using one of the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您拥有AWS账户，可以通过以下方式之一开始使用：
- en: '[SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html)'
- en: '[SageMaker notebook instance](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SageMaker笔记本实例](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html)'
- en: Local environment
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地环境
- en: To start training locally, you need to setup an appropriate [IAM role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始本地训练，您需要设置适当的[IAM角色](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)。
- en: Upgrade to the latest `sagemaker` version.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 升级到最新的`sagemaker`版本。
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**SageMaker environment**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**SageMaker环境**'
- en: 'Setup your SageMaker environment as shown below:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 按照下面所示设置您的SageMaker环境：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*Note: The execution role is only available when running a notebook within
    SageMaker. If you run `get_execution_role` in a notebook not on SageMaker, expect
    a `region` error.*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：执行角色仅在SageMaker内运行笔记本时可用。如果在不在SageMaker上运行的笔记本中运行`get_execution_role`，则会出现`region`错误。*'
- en: '**Local environment**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**本地环境**'
- en: 'Setup your local environment as shown below:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 按照下面所示设置您的本地环境：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Deploy a 🤗 Transformers model trained in SageMaker
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在SageMaker中部署经过训练的🤗 Transformers模型
- en: '[https://www.youtube.com/embed/pfBGgSGnYLs](https://www.youtube.com/embed/pfBGgSGnYLs)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/embed/pfBGgSGnYLs](https://www.youtube.com/embed/pfBGgSGnYLs)'
- en: 'There are two ways to deploy your Hugging Face model trained in SageMaker:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以部署在SageMaker中训练的Hugging Face模型：
- en: Deploy it after your training has finished.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练完成后部署它。
- en: Deploy your saved model at a later time from S3 with the `model_data`.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稍后可以使用`model_data`从S3部署保存的模型。
- en: 📓 Open the [notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb)
    for an example of how to deploy a model from S3 to SageMaker for inference.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 📓打开[笔记本](https://github.com/huggingface/notebooks/blob/main/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb)查看如何将模型从S3部署到SageMaker进行推理的示例。
- en: Deploy after training
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练后部署
- en: To deploy your model directly after training, ensure all required files are
    saved in your training script, including the tokenizer and the model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练后直接部署您的模型，请确保所有必需的文件都保存在您的训练脚本中，包括分词器和模型。
- en: If you use the Hugging Face `Trainer`, you can pass your tokenizer as an argument
    to the `Trainer`. It will be automatically saved when you call `trainer.save_model()`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用Hugging Face的`Trainer`，您可以将分词器作为参数传递给`Trainer`。当您调用`trainer.save_model()`时，它将自动保存。
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After you run your request you can delete the endpoint as shown:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 运行请求后，您可以按照以下方式删除端点：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Deploy with model_data
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用model_data部署
- en: If you’ve already trained your model and want to deploy it at a later time,
    use the `model_data` argument to specify the location of your tokenizer and model
    weights.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经训练了模型并希望在以后部署它，请使用`model_data`参数指定您的分词器和模型权重的位置。
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After you run our request, you can delete the endpoint again with:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 运行我们的请求后，您可以使用以下链接再次删除端点：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Create a model artifact for deployment
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为部署创建模型工件
- en: 'For later deployment, you can create a `model.tar.gz` file that contains all
    the required files, such as:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后部署时，可以创建一个包含所有必需文件的`model.tar.gz`文件，例如：
- en: '`pytorch_model.bin`'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch_model.bin`'
- en: '`tf_model.h5`'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf_model.h5`'
- en: '`tokenizer.json`'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer.json`'
- en: '`tokenizer_config.json`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_config.json`'
- en: 'For example, your file should look like this:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您的文件应该如下所示：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create your own `model.tar.gz` from a model from the 🤗 Hub:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从🤗 Hub创建自己的`model.tar.gz`文件：
- en: 'Download a model:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载模型：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Create a `tar` file:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`tar`文件：
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Upload `model.tar.gz` to S3:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`model.tar.gz`上传到S3：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now you can provide the S3 URI to the `model_data` argument to deploy your model
    later.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以提供S3 URI给`model_data`参数，以便稍后部署您的模型。
- en: Deploy a model from the 🤗 Hub
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从🤗 Hub部署模型
- en: '[https://www.youtube.com/embed/l9QZuazbzWM](https://www.youtube.com/embed/l9QZuazbzWM)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/embed/l9QZuazbzWM](https://www.youtube.com/embed/l9QZuazbzWM)'
- en: 'To deploy a model directly from the 🤗 Hub to SageMaker, define two environment
    variables when you create a `HuggingFaceModel`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要直接从🤗 Hub部署模型到SageMaker，创建`HuggingFaceModel`时定义两个环境变量：
- en: '`HF_MODEL_ID` defines the model ID which is automatically loaded from [huggingface.co/models](http://huggingface.co/models)
    when you create a SageMaker endpoint. Access 10,000+ models on he 🤗 Hub through
    this environment variable.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HF_MODEL_ID` 定义了模型 ID，当您创建 SageMaker 端点时，它会自动从[huggingface.co/models](http://huggingface.co/models)加载。通过这个环境变量可以访问
    🤗 Hub 上的 10,000 多个模型。'
- en: '`HF_TASK` defines the task for the 🤗 Transformers `pipeline`. A complete list
    of tasks can be found [here](https://huggingface.co/docs/transformers/main_classes/pipelines).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HF_TASK` 定义了 🤗 Transformers `pipeline` 的任务。完整的任务列表可以在[这里](https://huggingface.co/docs/transformers/main_classes/pipelines)找到。'
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After you run our request, you can delete the endpoint again with:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在您运行我们的请求后，您可以使用以下命令再次删除端点：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 📓 Open the [notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)
    for an example of how to deploy a model from the 🤗 Hub to SageMaker for inference.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 📓 打开[笔记本](https://github.com/huggingface/notebooks/blob/main/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)查看如何将模型从
    🤗 Hub 部署到 SageMaker 进行推断的示例。
- en: Run batch transform with 🤗 Transformers and SageMaker
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 🤗 Transformers 和 SageMaker 运行批量转换
- en: '[https://www.youtube.com/embed/lnTixz0tUBg](https://www.youtube.com/embed/lnTixz0tUBg)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/embed/lnTixz0tUBg](https://www.youtube.com/embed/lnTixz0tUBg)'
- en: After training a model, you can use [SageMaker batch transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)
    to perform inference with the model. Batch transform accepts your inference data
    as an S3 URI and then SageMaker will take care of downloading the data, running
    the prediction, and uploading the results to S3\. For more details about batch
    transform, take a look [here](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型后，您可以使用[SageMaker 批量转换](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)来执行模型推断。批量转换接受您的推断数据作为
    S3 URI，然后 SageMaker 将负责下载数据，运行预测，并将结果上传到 S3。有关批量转换的更多详细信息，请查看[这里](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html)。
- en: ⚠️ The Hugging Face Inference DLC currently only supports `.jsonl` for batch
    transform due to the complex structure of textual data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ⚠️ 目前 Hugging Face 推断 DLC 仅支持 `.jsonl` 用于批量转换，因为文本数据的结构复杂。
- en: '*Note: Make sure your `inputs` fit the `max_length` of the model during preprocessing.*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：确保您的 `inputs` 在预处理期间符合模型的 `max_length`。*'
- en: 'If you trained a model using the Hugging Face Estimator, call the `transformer()`
    method to create a transform job for a model based on the training job (see [here](https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-batch-transform)
    for more details):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用 Hugging Face Estimator 训练了一个模型，请调用 `transformer()` 方法为基于训练作业的模型创建一个转换作业（有关更多详细信息，请参见[这里](https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-batch-transform)）：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If you want to run your batch transform job later or with a model from the
    🤗 Hub, create a `HuggingFaceModel` instance and then call the `transformer()`
    method:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想稍后运行批量转换作业或使用 🤗 Hub 中的模型，创建一个 `HuggingFaceModel` 实例，然后调用 `transformer()`
    方法：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `input.jsonl` looks like this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`input.jsonl` 如下所示：'
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 📓 Open the [notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/12_batch_transform_inference/sagemaker-notebook.ipynb)
    for an example of how to run a batch transform job for inference.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 📓 打开[笔记本](https://github.com/huggingface/notebooks/blob/main/sagemaker/12_batch_transform_inference/sagemaker-notebook.ipynb)查看如何运行用于推断的批量转换作业的示例。
- en: User defined code and modules
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户定义的代码和模块
- en: 'The Hugging Face Inference Toolkit allows the user to override the default
    methods of the `HuggingFaceHandlerService`. You will need to create a folder named
    `code/` with an `inference.py` file in it. See [here](#create-a-model-artifact-for-deployment)
    for more details on how to archive your model artifacts. For example:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 推断工具包允许用户覆盖 `HuggingFaceHandlerService` 的默认方法。您需要创建一个名为 `code/`
    的文件夹，并在其中放置一个 `inference.py` 文件。有关如何归档模型工件的更多详细信息，请参阅[这里](#create-a-model-artifact-for-deployment)。例如：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `inference.py` file contains your custom inference module, and the `requirements.txt`
    file contains additional dependencies that should be added. The custom module
    can override the following methods:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`inference.py` 文件包含您的自定义推断模块，`requirements.txt` 文件包含应添加的附加依赖项。自定义模块可以覆盖以下方法：'
- en: '`model_fn(model_dir)` overrides the default method for loading a model. The
    return value `model` will be used in `predict` for predictions. `predict` receives
    argument the `model_dir`, the path to your unzipped `model.tar.gz`.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_fn(model_dir)` 覆盖了加载模型的默认方法。返回值 `model` 将在 `predict` 中用于预测。`predict`
    接收参数 `model_dir`，即您解压后的 `model.tar.gz` 的路径。'
- en: '`transform_fn(model, data, content_type, accept_type)` overrides the default
    transform function with your custom implementation. You will need to implement
    your own `preprocess`, `predict` and `postprocess` steps in the `transform_fn`.
    This method can’t be combined with `input_fn`, `predict_fn` or `output_fn` mentioned
    below.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transform_fn(model, data, content_type, accept_type)` 覆盖了默认的转换函数，使用您自定义的实现。您需要在
    `transform_fn` 中实现自己的 `preprocess`、`predict` 和 `postprocess` 步骤。此方法不能与下面提到的 `input_fn`、`predict_fn`
    或 `output_fn` 结合使用。'
- en: '`input_fn(input_data, content_type)` overrides the default method for preprocessing.
    The return value `data` will be used in `predict` for predictions. The inputs
    are:'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_fn(input_data, content_type)` 覆盖了预处理的默认方法。返回值 `data` 将在 `predict` 中用于预测。输入是：'
- en: '`input_data` is the raw body of your request.'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data` 是您请求的原始主体。'
- en: '`content_type` is the content type from the request header.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`content_type` 是请求头中的内容类型。'
- en: '`predict_fn(processed_data, model)` overrides the default method for predictions.
    The return value `predictions` will be used in `postprocess`. The input is `processed_data`,
    the result from `preprocess`.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict_fn(processed_data, model)` 覆盖了预测的默认方法。返回值 `predictions` 将在 `postprocess`
    中使用。输入是 `processed_data`，是从 `preprocess` 得到的结果。'
- en: '`output_fn(prediction, accept)` overrides the default method for postprocessing.
    The return value `result` will be the response of your request (e.g.`JSON`). The
    inputs are:'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_fn(prediction, accept)` 覆盖了后处理的默认方法。返回值 `result` 将是您请求的响应（例如 `JSON`）。输入是：'
- en: '`predictions` is the result from `predict`.'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predictions`是来自`predict`的结果。'
- en: '`accept` is the return accept type from the HTTP Request, e.g. `application/json`.'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`accept`是来自HTTP请求的返回接受类型，例如`application/json`。'
- en: 'Here is an example of a custom inference module with `model_fn`, `input_fn`,
    `predict_fn`, and `output_fn`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个带有`model_fn`、`input_fn`、`predict_fn`和`output_fn`的自定义推理模块示例：
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Customize your inference module with only `model_fn` and `transform_fn`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 只使用`model_fn`和`transform_fn`自定义您的推理模块：
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
