["```py\ndef expand_and_normalize_bbox(bboxes, doc_width, doc_height):\n    # here, bboxes are numpy array\n\n    # Normalize bbox -> 0 ~ 1\n    bboxes[:, [0, 2]] = bboxes[:, [0, 2]] / width\n    bboxes[:, [1, 3]] = bboxes[:, [1, 3]] / height\n```", "```py\ndef make_box_first_token_mask(bboxes, words, tokenizer, max_seq_length=512):\n\n    box_first_token_mask = np.zeros(max_seq_length, dtype=np.bool_)\n\n    # encode(tokenize) each word from words (List[str])\n    input_ids_list: List[List[int]] = [tokenizer.encode(e, add_special_tokens=False) for e in words]\n\n    # get the length of each box\n    tokens_length_list: List[int] = [len(l) for l in input_ids_list]\n\n    box_end_token_indices = np.array(list(itertools.accumulate(tokens_length_list)))\n    box_start_token_indices = box_end_token_indices - np.array(tokens_length_list)\n\n    # filter out the indices that are out of max_seq_length\n    box_end_token_indices = box_end_token_indices[box_end_token_indices < max_seq_length - 1]\n    if len(box_start_token_indices) > len(box_end_token_indices):\n        box_start_token_indices = box_start_token_indices[: len(box_end_token_indices)]\n\n    # set box_start_token_indices to True\n    box_first_token_mask[box_start_token_indices] = True\n\n    return box_first_token_mask\n\n```", "```py\n( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 dim_bbox = 8 bbox_scale = 100.0 n_relations = 1 classifier_dropout_prob = 0.1 **kwargs )\n```", "```py\n>>> from transformers import BrosConfig, BrosModel\n\n>>> # Initializing a BROS jinho8345/bros-base-uncased style configuration\n>>> configuration = BrosConfig()\n\n>>> # Initializing a model from the jinho8345/bros-base-uncased style configuration\n>>> model = BrosModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( tokenizer = None **kwargs )\n```", "```py\n( text: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True return_tensors: Union = None **kwargs )\n```", "```py\n( config add_pooling_layer = True )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import BrosProcessor, BrosModel\n\n>>> processor = BrosProcessor.from_pretrained(\"jinho8345/bros-base-uncased\")\n\n>>> model = BrosModel.from_pretrained(\"jinho8345/bros-base-uncased\")\n\n>>> encoding = processor(\"Hello, my dog is cute\", add_special_tokens=False, return_tensors=\"pt\")\n>>> bbox = torch.tensor([[[0, 0, 1, 1]]]).repeat(1, encoding[\"input_ids\"].shape[-1], 1)\n>>> encoding[\"bbox\"] = bbox\n\n>>> outputs = model(**encoding)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None bbox_first_token_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import BrosProcessor, BrosForTokenClassification\n\n>>> processor = BrosProcessor.from_pretrained(\"jinho8345/bros-base-uncased\")\n\n>>> model = BrosForTokenClassification.from_pretrained(\"jinho8345/bros-base-uncased\")\n\n>>> encoding = processor(\"Hello, my dog is cute\", add_special_tokens=False, return_tensors=\"pt\")\n>>> bbox = torch.tensor([[[0, 0, 1, 1]]]).repeat(1, encoding[\"input_ids\"].shape[-1], 1)\n>>> encoding[\"bbox\"] = bbox\n\n>>> outputs = model(**encoding)\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None bbox_first_token_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None initial_token_labels: Optional = None subsequent_token_labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.bros.modeling_bros.BrosSpadeOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import BrosProcessor, BrosSpadeEEForTokenClassification\n\n>>> processor = BrosProcessor.from_pretrained(\"jinho8345/bros-base-uncased\")\n\n>>> model = BrosSpadeEEForTokenClassification.from_pretrained(\"jinho8345/bros-base-uncased\")\n\n>>> encoding = processor(\"Hello, my dog is cute\", add_special_tokens=False, return_tensors=\"pt\")\n>>> bbox = torch.tensor([[[0, 0, 1, 1]]]).repeat(1, encoding[\"input_ids\"].shape[-1], 1)\n>>> encoding[\"bbox\"] = bbox\n\n>>> outputs = model(**encoding)\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None bbox: Optional = None attention_mask: Optional = None bbox_first_token_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import BrosProcessor, BrosSpadeELForTokenClassification\n\n>>> processor = BrosProcessor.from_pretrained(\"jinho8345/bros-base-uncased\")\n\n>>> model = BrosSpadeELForTokenClassification.from_pretrained(\"jinho8345/bros-base-uncased\")\n\n>>> encoding = processor(\"Hello, my dog is cute\", add_special_tokens=False, return_tensors=\"pt\")\n>>> bbox = torch.tensor([[[0, 0, 1, 1]]]).repeat(1, encoding[\"input_ids\"].shape[-1], 1)\n>>> encoding[\"bbox\"] = bbox\n\n>>> outputs = model(**encoding)\n```"]