["```py\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/umt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n\n>>> inputs = tokenizer(\n...     \"A <extra_id_0> walks into a bar and orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\",\n...     return_tensors=\"pt\",\n... )\n>>> outputs = model.generate(**inputs)\n>>> print(tokenizer.batch_decode(outputs))\n['<pad><extra_id_0>nyone who<extra_id_1> drink<extra_id_2> a<extra_id_3> alcohol<extra_id_4> A<extra_id_5> A. This<extra_id_6> I<extra_id_7><extra_id_52><extra_id_53></s>']\n```", "```py\n>>> from transformers import UMT5Model, AutoTokenizer\n\n>>> model = UMT5Model.from_pretrained(\"google/umt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n>>> noisy_text = \"UN Offizier sagt, dass weiter <extra_id_0> werden muss in Syrien.\"\n>>> label = \"<extra_id_0> verhandelt\"\n>>> inputs = tokenizer(inputs, return_tensors=\"pt\")\n>>> labels = tokenizer(label=label, return_tensors=\"pt\")\n\n>>> outputs = model(input_ids=inputs[\"input_ids\"], decoder_input_ids=labels[\"input_ids\"])\n>>> hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, UMT5Model\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n>>> model = UMT5Model.from_pretrained(\"google/umt5-small\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n\n>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for UMT5Model.\n>>> # This is not needed for torch's UMT5ForConditionalGeneration as it does this internally using labels arg.\n>>> decoder_input_ids = model._shift_right(decoder_input_ids)\n\n>>> # forward pass\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import UMT5ForConditionalGeneration, AutoTokenizer\n\n>>> model = UMT5ForConditionalGeneration.from_pretrained(\"google/umt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n>>> article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n>>> summary = \"Weiter Verhandlung in Syrien.\"\n>>> inputs = tokenizer(article, text_target=summary, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> loss = outputs.loss\n```", "```py\n>>> from transformers import AutoTokenizer, UMT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n>>> model = UMT5ForConditionalGeneration.from_pretrained(\"google/umt5-small\")\n\n>>> # training\n>>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n>>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n>>> outputs = model(input_ids=input_ids, labels=labels)\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n\n>>> # inference\n>>> input_ids = tokenizer(\"Studies have shown that <extra_id_0> good for you\", return_tensors=\"pt\").input_ids\n>>> outputs = model.generate(input_ids)\n>>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n```", "```py\n>>> from transformers import UMT5EncoderModel, AutoTokenizer\n\n>>> model = UMT5EncoderModel.from_pretrained(\"google/umt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n>>> article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n>>> input_ids = tokenizer(article, return_tensors=\"pt\").input_ids\n>>> outputs = model(input_ids)\n>>> hidden_state = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, UMT5EncoderModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n>>> model = UMT5EncoderModel.from_pretrained(\"google/umt5-small\")\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> outputs = model(input_ids=input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```"]