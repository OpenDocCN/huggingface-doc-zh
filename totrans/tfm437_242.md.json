["```py\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/umt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n\n>>> inputs = tokenizer(\n...     \"A <extra_id_0> walks into a bar and orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\",\n...     return_tensors=\"pt\",\n... )\n>>> outputs = model.generate(**inputs)\n>>> print(tokenizer.batch_decode(outputs))\n['<pad><extra_id_0>nyone who<extra_id_1> drink<extra_id_2> a<extra_id_3> alcohol<extra_id_4> A<extra_id_5> A. This<extra_id_6> I<extra_id_7><extra_id_52><extra_id_53></s>']\n```", "```py\n( vocab_size = 250112 d_model = 512 d_kv = 64 d_ff = 1024 num_layers = 8 num_decoder_layers = None num_heads = 6 relative_attention_num_buckets = 32 relative_attention_max_distance = 128 dropout_rate = 0.1 layer_norm_epsilon = 1e-06 initializer_factor = 1.0 feed_forward_proj = 'gated-gelu' is_encoder_decoder = True use_cache = True tokenizer_class = 'T5Tokenizer' tie_word_embeddings = True pad_token_id = 0 eos_token_id = 1 decoder_start_token_id = 0 classifier_dropout = 0.0 **kwargs )\n```", "```py\n( config )\n```", "```py\n>>> from transformers import UMT5Model, AutoTokenizer\n\n>>> model = UMT5Model.from_pretrained(\"google/umt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n>>> noisy_text = \"UN Offizier sagt, dass weiter <extra_id_0> werden muss in Syrien.\"\n>>> label = \"<extra_id_0> verhandelt\"\n>>> inputs = tokenizer(inputs, return_tensors=\"pt\")\n>>> labels = tokenizer(label=label, return_tensors=\"pt\")\n\n>>> outputs = model(input_ids=inputs[\"input_ids\"], decoder_input_ids=labels[\"input_ids\"])\n>>> hidden_states = outputs.last_hidden_state\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, UMT5Model\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n>>> model = UMT5Model.from_pretrained(\"google/umt5-small\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n\n>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for UMT5Model.\n>>> # This is not needed for torch's UMT5ForConditionalGeneration as it does this internally using labels arg.\n>>> decoder_input_ids = model._shift_right(decoder_input_ids)\n\n>>> # forward pass\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n>>> from transformers import UMT5ForConditionalGeneration, AutoTokenizer\n\n>>> model = UMT5ForConditionalGeneration.from_pretrained(\"google/umt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n>>> article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n>>> summary = \"Weiter Verhandlung in Syrien.\"\n>>> inputs = tokenizer(article, text_target=summary, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> loss = outputs.loss\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, UMT5ForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n>>> model = UMT5ForConditionalGeneration.from_pretrained(\"google/umt5-small\")\n\n>>> # training\n>>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n>>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n>>> outputs = model(input_ids=input_ids, labels=labels)\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n\n>>> # inference\n>>> input_ids = tokenizer(\"Studies have shown that <extra_id_0> good for you\", return_tensors=\"pt\").input_ids\n>>> outputs = model.generate(input_ids)\n>>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n```", "```py\n( config )\n```", "```py\n>>> from transformers import UMT5EncoderModel, AutoTokenizer\n\n>>> model = UMT5EncoderModel.from_pretrained(\"google/umt5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n>>> article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n>>> input_ids = tokenizer(article, return_tensors=\"pt\").input_ids\n>>> outputs = model(input_ids)\n>>> hidden_state = outputs.last_hidden_state\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, UMT5EncoderModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n>>> model = UMT5EncoderModel.from_pretrained(\"google/umt5-small\")\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> outputs = model(input_ids=input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: UMT5Config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None start_positions: Optional = None end_positions: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```"]