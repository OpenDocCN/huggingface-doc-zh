- en: Prior Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/diffusers/api/models/prior_transformer](https://huggingface.co/docs/diffusers/api/models/prior_transformer)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/20.00d9900b.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Docstring.93f6f462.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
  prefs: []
  type: TYPE_NORMAL
- en: The Prior Transformer was originally introduced in [Hierarchical Text-Conditional
    Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125)
    by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings;
    image embeddings are predicted through a denoising diffusion process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Contrastive models like CLIP have been shown to learn robust representations
    of images that capture both semantics and style. To leverage these representations
    for image generation, we propose a two-stage model: a prior that generates a CLIP
    image embedding given a text caption, and a decoder that generates an image conditioned
    on the image embedding. We show that explicitly generating image representations
    improves image diversity with minimal loss in photorealism and caption similarity.
    Our decoders conditioned on image representations can also produce variations
    of an image that preserve both its semantics and style, while varying the non-essential
    details absent from the image representation. Moreover, the joint embedding space
    of CLIP enables language-guided image manipulations in a zero-shot fashion. We
    use diffusion models for the decoder and experiment with both autoregressive and
    diffusion models for the prior, finding that the latter are computationally more
    efficient and produce higher-quality samples.*'
  prefs: []
  type: TYPE_NORMAL
- en: PriorTransformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.PriorTransformer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L36)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 32) — The number of heads
    to use for multi-head attention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_head_dim` (`int`, *optional*, defaults to 64) — The number of channels
    in each head.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_layers` (`int`, *optional*, defaults to 20) — The number of layers of
    Transformer blocks to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embedding_dim` (`int`, *optional*, defaults to 768) — The dimension of the
    model input `hidden_states`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_embeddings` (`int`, *optional*, defaults to 77) — The number of embeddings
    of the model input `hidden_states`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`additional_embeddings` (`int`, *optional*, defaults to 4) — The number of
    additional tokens appended to the projected `hidden_states`. The actual length
    of the used `hidden_states` is `num_embeddings + additional_embeddings`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time_embed_act_fn` (`str`, *optional*, defaults to ‘silu’) — The activation
    function to use to create timestep embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`norm_in_type` (`str`, *optional*, defaults to None) — The normalization layer
    to apply on hidden states before passing to Transformer blocks. Set it to `None`
    if normalization is not needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embedding_proj_norm_type` (`str`, *optional*, defaults to None) — The normalization
    layer to apply on the input `proj_embedding`. Set it to `None` if normalization
    is not needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hid_proj_type` (`str`, *optional*, defaults to `linear`) — The projection
    layer to apply on the input `encoder_hidden_states`. Set it to `None` if `encoder_hidden_states`
    is `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`added_emb_type` (`str`, *optional*, defaults to `prd`) — Additional embeddings
    to condition the model. Choose from `prd` or `None`. if choose `prd`, it will
    prepend a token indicating the (quantized) dot product between the text embedding
    and image embedding as proposed in the unclip paper [https://arxiv.org/abs/2204.06125](https://arxiv.org/abs/2204.06125)
    If it is `None`, no additional embeddings will be prepended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time_embed_dim` (`int, *optional*, defaults to None) -- The dimension of timestep
    embeddings. If None, will be set to` num_attention_heads * attention_head_dim`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embedding_proj_dim` (`int`, *optional*, default to None) — The dimension of
    `proj_embedding`. If None, will be set to `embedding_dim`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_embed_dim` (`int`, *optional*, default to None) — The dimension of the
    output. If None, will be set to `embedding_dim`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Prior Transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L245)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_states` (`torch.FloatTensor` of shape `(batch_size, embedding_dim)`)
    — The currently predicted image embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timestep` (`torch.LongTensor`) — Current denoising step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proj_embedding` (`torch.FloatTensor` of shape `(batch_size, embedding_dim)`)
    — Projected embedding vector the denoising process is conditioned on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`torch.FloatTensor` of shape `(batch_size, num_embeddings,
    embedding_dim)`) — Hidden states of the text embeddings the denoising process
    is conditioned on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.BoolTensor` of shape `(batch_size, num_embeddings)`)
    — Text mask for the text embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a `~models.prior_transformer.PriorTransformerOutput` instead of a plain
    tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`~models.prior_transformer.PriorTransformerOutput` or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If return_dict is True, a `~models.prior_transformer.PriorTransformerOutput`
    is returned, otherwise a tuple is returned where the first element is the sample
    tensor.
  prefs: []
  type: TYPE_NORMAL
- en: The [PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer)
    forward method.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_attn_processor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L195)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`processor` (`dict` of `AttentionProcessor` or only `AttentionProcessor`) —
    The instantiated processor class or a dictionary of processor classes that will
    be set as the processor for **all** `Attention` layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `processor` is a dict, the key needs to define the path to the corresponding
    cross attention processor. This is strongly recommended when setting trainable
    attention processors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Sets the attention processor to use to compute attention.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_default_attn_processor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L230)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Disables custom attention processors and sets the default attention implementation.
  prefs: []
  type: TYPE_NORMAL
- en: PriorTransformerOutput
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.transformers.prior_transformer.PriorTransformerOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L23)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`predicted_image_embedding` (`torch.FloatTensor` of shape `(batch_size, embedding_dim)`)
    — The predicted CLIP image embedding conditioned on the CLIP text embedding input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of [PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer).
  prefs: []
  type: TYPE_NORMAL
