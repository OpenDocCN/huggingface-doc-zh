- en: Prior Transformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Prior Transformer
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/models/prior_transformer](https://huggingface.co/docs/diffusers/api/models/prior_transformer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/models/prior_transformer](https://huggingface.co/docs/diffusers/api/models/prior_transformer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The Prior Transformer was originally introduced in [Hierarchical Text-Conditional
    Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125)
    by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings;
    image embeddings are predicted through a denoising diffusion process.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Prior Transformer最初是由Ramesh等人在[具有CLIP潜在特征的分层文本条件图像生成](https://huggingface.co/papers/2204.06125)中引入的。它用于从CLIP文本嵌入中预测CLIP图像嵌入；图像嵌入通过去噪扩散过程进行预测。
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Contrastive models like CLIP have been shown to learn robust representations
    of images that capture both semantics and style. To leverage these representations
    for image generation, we propose a two-stage model: a prior that generates a CLIP
    image embedding given a text caption, and a decoder that generates an image conditioned
    on the image embedding. We show that explicitly generating image representations
    improves image diversity with minimal loss in photorealism and caption similarity.
    Our decoders conditioned on image representations can also produce variations
    of an image that preserve both its semantics and style, while varying the non-essential
    details absent from the image representation. Moreover, the joint embedding space
    of CLIP enables language-guided image manipulations in a zero-shot fashion. We
    use diffusion models for the decoder and experiment with both autoregressive and
    diffusion models for the prior, finding that the latter are computationally more
    efficient and produce higher-quality samples.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*像CLIP这样的对比模型已被证明能够学习捕捉图像的语义和风格的稳健表示。为了利用这些表示来进行图像生成，我们提出了一个两阶段模型：一个先验模型，根据文本标题生成CLIP图像嵌入，以及一个解码器，根据图像嵌入生成图像。我们展示了明确生成图像表示可以提高图像的多样性，同时最小化逼真度和标题相似性的损失。我们的解码器根据图像表示可以产生保留其语义和风格的图像变体，同时变化非必要的细节，这些细节在图像表示中不存在。此外，CLIP的联合嵌入空间使得可以以零样本的方式进行语言引导的图像操作。我们使用扩散模型进行解码器，并尝试先验模型的自回归和扩散模型，发现后者在计算上更有效，并产生更高质量的样本。*'
- en: PriorTransformer
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PriorTransformer
- en: '### `class diffusers.PriorTransformer`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.PriorTransformer`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L36)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L36)'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`num_attention_heads` (`int`, *optional*, defaults to 32) — The number of heads
    to use for multi-head attention.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, 默认为32) — 用于多头注意力的头数。'
- en: '`attention_head_dim` (`int`, *optional*, defaults to 64) — The number of channels
    in each head.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_head_dim` (`int`, *optional*, 默认为64) — 每个头中的通道数。'
- en: '`num_layers` (`int`, *optional*, defaults to 20) — The number of layers of
    Transformer blocks to use.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers` (`int`, *optional*, 默认为20) — 要使用的Transformer块层数。'
- en: '`embedding_dim` (`int`, *optional*, defaults to 768) — The dimension of the
    model input `hidden_states`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding_dim` (`int`, *optional*, 默认为768) — 模型输入`hidden_states`的维度'
- en: '`num_embeddings` (`int`, *optional*, defaults to 77) — The number of embeddings
    of the model input `hidden_states`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_embeddings` (`int`, *optional*, 默认为77) — 模型输入`hidden_states`的嵌入数量'
- en: '`additional_embeddings` (`int`, *optional*, defaults to 4) — The number of
    additional tokens appended to the projected `hidden_states`. The actual length
    of the used `hidden_states` is `num_embeddings + additional_embeddings`.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_embeddings` (`int`, *optional*, 默认为4) — 附加到投影的`hidden_states`的令牌数量。使用的`hidden_states`的实际长度为`num_embeddings
    + additional_embeddings`。'
- en: '`dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    to use.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, 默认为0.0) — 要使用的dropout概率。'
- en: '`time_embed_act_fn` (`str`, *optional*, defaults to ‘silu’) — The activation
    function to use to create timestep embeddings.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_embed_act_fn` (`str`, *optional*, 默认为''silu'') — 用于创建时间步嵌入的激活函数。'
- en: '`norm_in_type` (`str`, *optional*, defaults to None) — The normalization layer
    to apply on hidden states before passing to Transformer blocks. Set it to `None`
    if normalization is not needed.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_in_type` (`str`, *optional*, 默认为None) — 在传递给Transformer块之前对隐藏状态应用的规范化层。如果不需要规范化，请将其设置为`None`。'
- en: '`embedding_proj_norm_type` (`str`, *optional*, defaults to None) — The normalization
    layer to apply on the input `proj_embedding`. Set it to `None` if normalization
    is not needed.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding_proj_norm_type` (`str`, *optional*, 默认为None) — 应用于输入`proj_embedding`的规范化层。如果不需要规范化，请将其设置为`None`。'
- en: '`encoder_hid_proj_type` (`str`, *optional*, defaults to `linear`) — The projection
    layer to apply on the input `encoder_hidden_states`. Set it to `None` if `encoder_hidden_states`
    is `None`.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hid_proj_type` (`str`, *optional*, 默认为`linear`) — 应用于输入`encoder_hidden_states`的投影层。如果`encoder_hidden_states`为`None`，则将其设置为`None`。'
- en: '`added_emb_type` (`str`, *optional*, defaults to `prd`) — Additional embeddings
    to condition the model. Choose from `prd` or `None`. if choose `prd`, it will
    prepend a token indicating the (quantized) dot product between the text embedding
    and image embedding as proposed in the unclip paper [https://arxiv.org/abs/2204.06125](https://arxiv.org/abs/2204.06125)
    If it is `None`, no additional embeddings will be prepended.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`added_emb_type` (`str`, *optional*, 默认为`prd`) — 用于条件模型的附加嵌入。从`prd`或`None`中选择。如果选择`prd`，它将在文本嵌入和图像嵌入之间插入一个表示（量化）点积的标记，如在unclip论文中提出的[https://arxiv.org/abs/2204.06125](https://arxiv.org/abs/2204.06125)。如果是`None`，则不会添加额外的嵌入。'
- en: '`time_embed_dim` (`int, *optional*, defaults to None) -- The dimension of timestep
    embeddings. If None, will be set to` num_attention_heads * attention_head_dim`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_embed_dim` (`int, *optional*, 默认为None) -- 时间步嵌入的维度。如果为None，则将设置为`num_attention_heads
    * attention_head_dim`'
- en: '`embedding_proj_dim` (`int`, *optional*, default to None) — The dimension of
    `proj_embedding`. If None, will be set to `embedding_dim`.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding_proj_dim` (`int`, *optional*, 默认为None) — `proj_embedding`的维度。如果为None，则将设置为`embedding_dim`。'
- en: '`clip_embed_dim` (`int`, *optional*, default to None) — The dimension of the
    output. If None, will be set to `embedding_dim`.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_embed_dim`（`int`，*可选*，默认为None）- 输出的维度。如果为None，则设置为`embedding_dim`。'
- en: A Prior Transformer model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个先验Transformer模型。
- en: '#### `forward`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L245)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[源代码](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L245)'
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_states` (`torch.FloatTensor` of shape `(batch_size, embedding_dim)`)
    — The currently predicted image embeddings.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（形状为`(batch_size, embedding_dim)`的`torch.FloatTensor`）- 当前预测的图像嵌入。'
- en: '`timestep` (`torch.LongTensor`) — Current denoising step.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timestep`（`torch.LongTensor`）- 当前去噪步骤。'
- en: '`proj_embedding` (`torch.FloatTensor` of shape `(batch_size, embedding_dim)`)
    — Projected embedding vector the denoising process is conditioned on.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`proj_embedding`（形状为`(batch_size, embedding_dim)`的`torch.FloatTensor`）- 去噪过程所依赖的投影嵌入向量。'
- en: '`encoder_hidden_states` (`torch.FloatTensor` of shape `(batch_size, num_embeddings,
    embedding_dim)`) — Hidden states of the text embeddings the denoising process
    is conditioned on.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（形状为`(batch_size, num_embeddings, embedding_dim)`的`torch.FloatTensor`）-
    文本嵌入的隐藏状态，去噪过程是基于这些隐藏状态进行的。'
- en: '`attention_mask` (`torch.BoolTensor` of shape `(batch_size, num_embeddings)`)
    — Text mask for the text embeddings.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, num_embeddings)`的`torch.BoolTensor`）- 文本嵌入的文本掩码。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a `~models.prior_transformer.PriorTransformerOutput` instead of a plain
    tuple.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*，默认为`True`）- 是否返回`~models.prior_transformer.PriorTransformerOutput`而不是普通元组。'
- en: Returns
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`~models.prior_transformer.PriorTransformerOutput` or `tuple`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`~models.prior_transformer.PriorTransformerOutput`或`tuple`'
- en: If return_dict is True, a `~models.prior_transformer.PriorTransformerOutput`
    is returned, otherwise a tuple is returned where the first element is the sample
    tensor.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`return_dict`为True，则返回`~models.prior_transformer.PriorTransformerOutput`，否则返回一个元组，其中第一个元素是样本张量。
- en: The [PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer)
    forward method.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[先验Transformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer)的前向方法。'
- en: '#### `set_attn_processor`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_attn_processor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L195)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[源代码](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L195)'
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`processor` (`dict` of `AttentionProcessor` or only `AttentionProcessor`) —
    The instantiated processor class or a dictionary of processor classes that will
    be set as the processor for **all** `Attention` layers.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`processor`（`AttentionProcessor`的字典或仅`AttentionProcessor`）- 实例化的处理器类或将设置为**所有**`Attention`层的处理器的处理器类字典。'
- en: If `processor` is a dict, the key needs to define the path to the corresponding
    cross attention processor. This is strongly recommended when setting trainable
    attention processors.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果`processor`是一个字典，则键需要定义到相应交叉注意力处理器的路径。在设置可训练的注意力处理器时强烈推荐这样做。
- en: Sets the attention processor to use to compute attention.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 设置用于计算注意力的注意力处理器。
- en: '#### `set_default_attn_processor`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_default_attn_processor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L230)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[源代码](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L230)'
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Disables custom attention processors and sets the default attention implementation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用自定义注意力处理器并设置默认的注意力实现。
- en: PriorTransformerOutput
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PriorTransformerOutput
- en: '### `class diffusers.models.transformers.prior_transformer.PriorTransformerOutput`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.transformers.prior_transformer.PriorTransformerOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L23)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[源代码](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L23)'
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`predicted_image_embedding` (`torch.FloatTensor` of shape `(batch_size, embedding_dim)`)
    — The predicted CLIP image embedding conditioned on the CLIP text embedding input.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predicted_image_embedding`（形状为`(batch_size, embedding_dim)`的`torch.FloatTensor`）-
    在CLIP文本嵌入输入条件下预测的CLIP图像嵌入。'
- en: The output of [PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[先验Transformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer)的输出。'
