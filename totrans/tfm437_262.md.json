["```py\n>>> from transformers import DetrForObjectDetection\n\n>>> model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n```", "```py\n>>> from transformers import DetrConfig, DetrForObjectDetection\n\n>>> config = DetrConfig()\n>>> model = DetrForObjectDetection(config)\n```", "```py\n>>> config = DetrConfig(use_pretrained_backbone=False)\n>>> model = DetrForObjectDetection(config)\n```", "```py\n( use_timm_backbone = True backbone_config = None num_channels = 3 num_queries = 100 encoder_layers = 6 encoder_ffn_dim = 2048 encoder_attention_heads = 8 decoder_layers = 6 decoder_ffn_dim = 2048 decoder_attention_heads = 8 encoder_layerdrop = 0.0 decoder_layerdrop = 0.0 is_encoder_decoder = True activation_function = 'relu' d_model = 256 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 init_std = 0.02 init_xavier_std = 1.0 auxiliary_loss = False position_embedding_type = 'sine' backbone = 'resnet50' use_pretrained_backbone = True dilation = False class_cost = 1 bbox_cost = 5 giou_cost = 2 mask_loss_coefficient = 1 dice_loss_coefficient = 1 bbox_loss_coefficient = 5 giou_loss_coefficient = 2 eos_coefficient = 0.1 **kwargs )\n```", "```py\n>>> from transformers import DetrConfig, DetrModel\n\n>>> # Initializing a DETR facebook/detr-resnet-50 style configuration\n>>> configuration = DetrConfig()\n\n>>> # Initializing a model (with random weights) from the facebook/detr-resnet-50 style configuration\n>>> model = DetrModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( backbone_config: PretrainedConfig **kwargs ) \u2192 export const metadata = 'undefined';DetrConfig\n```", "```py\n( format: Union = <AnnotationFormat.COCO_DETECTION: 'coco_detection'> do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_pad: bool = True **kwargs )\n```", "```py\n( images: Union annotations: Union = None return_segmentation_masks: bool = None masks_path: Union = None do_resize: Optional = None size: Optional = None resample = None do_rescale: Optional = None rescale_factor: Union = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None do_pad: Optional = None format: Union = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( outputs threshold: float = 0.5 target_sizes: Union = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( outputs target_sizes: List = None ) \u2192 export const metadata = 'undefined';List[torch.Tensor]\n```", "```py\n( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 target_sizes: Optional = None return_coco_annotation: Optional = False ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 label_ids_to_fuse: Optional = None target_sizes: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( *args **kwargs )\n```", "```py\n( images **kwargs )\n```", "```py\n( outputs threshold: float = 0.5 target_sizes: Union = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( outputs target_sizes: List = None ) \u2192 export const metadata = 'undefined';List[torch.Tensor]\n```", "```py\n( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 target_sizes: Optional = None return_coco_annotation: Optional = False ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 label_ids_to_fuse: Optional = None target_sizes: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( last_hidden_state: FloatTensor = None past_key_values: Optional = None decoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None encoder_last_hidden_state: Optional = None encoder_hidden_states: Optional = None encoder_attentions: Optional = None intermediate_hidden_states: Optional = None )\n```", "```py\n( loss: Optional = None loss_dict: Optional = None logits: FloatTensor = None pred_boxes: FloatTensor = None auxiliary_outputs: Optional = None last_hidden_state: Optional = None decoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None encoder_last_hidden_state: Optional = None encoder_hidden_states: Optional = None encoder_attentions: Optional = None )\n```", "```py\n( loss: Optional = None loss_dict: Optional = None logits: FloatTensor = None pred_boxes: FloatTensor = None pred_masks: FloatTensor = None auxiliary_outputs: Optional = None last_hidden_state: Optional = None decoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None encoder_last_hidden_state: Optional = None encoder_hidden_states: Optional = None encoder_attentions: Optional = None )\n```", "```py\n( config: DetrConfig )\n```", "```py\n( pixel_values: FloatTensor pixel_mask: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.detr.modeling_detr.DetrModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, DetrModel\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n>>> model = DetrModel.from_pretrained(\"facebook/detr-resnet-50\")\n\n>>> # prepare image for the model\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(**inputs)\n\n>>> # the last hidden states are the final query embeddings of the Transformer decoder\n>>> # these are of shape (batch_size, num_queries, hidden_size)\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 100, 256]\n```", "```py\n( config: DetrConfig )\n```", "```py\n( pixel_values: FloatTensor pixel_mask: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.detr.modeling_detr.DetrObjectDetectionOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, DetrForObjectDetection\n>>> import torch\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n>>> model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n>>> target_sizes = torch.tensor([image.size[::-1]])\n>>> results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[\n...     0\n... ]\n\n>>> for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(\n...         f\"Detected {model.config.id2label[label.item()]} with confidence \"\n...         f\"{round(score.item(), 3)} at location {box}\"\n...     )\nDetected remote with confidence 0.998 at location [40.16, 70.81, 175.55, 117.98]\nDetected remote with confidence 0.996 at location [333.24, 72.55, 368.33, 187.66]\nDetected couch with confidence 0.995 at location [-0.02, 1.15, 639.73, 473.76]\nDetected cat with confidence 0.999 at location [13.24, 52.05, 314.02, 470.93]\nDetected cat with confidence 0.999 at location [345.4, 23.85, 640.37, 368.72]\n```", "```py\n( config: DetrConfig )\n```", "```py\n( pixel_values: FloatTensor pixel_mask: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.detr.modeling_detr.DetrSegmentationOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import io\n>>> import requests\n>>> from PIL import Image\n>>> import torch\n>>> import numpy\n\n>>> from transformers import AutoImageProcessor, DetrForSegmentation\n>>> from transformers.image_transforms import rgb_to_id\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n>>> model = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n\n>>> # prepare image for the model\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(**inputs)\n\n>>> # Use the `post_process_panoptic_segmentation` method of the `image_processor` to retrieve post-processed panoptic segmentation maps\n>>> # Segmentation results are returned as a list of dictionaries\n>>> result = image_processor.post_process_panoptic_segmentation(outputs, target_sizes=[(300, 500)])\n\n>>> # A tensor of shape (height, width) where each value denotes a segment id, filled with -1 if no segment is found\n>>> panoptic_seg = result[0][\"segmentation\"]\n>>> # Get prediction score and segment_id to class_id mapping of each segment\n>>> panoptic_segments_info = result[0][\"segments_info\"]\n```"]