- en: Load community pipelines and components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview](https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Community pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Community pipelines are any [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    class that are different from the original implementation as specified in their
    paper (for example, the [StableDiffusionControlNetPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline)
    corresponds to the [Text-to-Image Generation with ControlNet Conditioning](https://arxiv.org/abs/2302.05543)
    paper). They provide additional functionality or extend the original implementation
    of a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: There are many cool community pipelines like [Speech to Image](https://github.com/huggingface/diffusers/tree/main/examples/community#speech-to-image)
    or [Composable Stable Diffusion](https://github.com/huggingface/diffusers/tree/main/examples/community#composable-stable-diffusion),
    and you can find all the official community pipelines [here](https://github.com/huggingface/diffusers/tree/main/examples/community).
  prefs: []
  type: TYPE_NORMAL
- en: 'To load any community pipeline on the Hub, pass the repository id of the community
    pipeline to the `custom_pipeline` argument and the model repository where you‚Äôd
    like to load the pipeline weights and components from. For example, the example
    below loads a dummy pipeline from [`hf-internal-testing/diffusers-dummy-pipeline`](https://huggingface.co/hf-internal-testing/diffusers-dummy-pipeline/blob/main/pipeline.py)
    and the pipeline weights and components from [`google/ddpm-cifar10-32`](https://huggingface.co/google/ddpm-cifar10-32):'
  prefs: []
  type: TYPE_NORMAL
- en: üîí By loading a community pipeline from the Hugging Face Hub, you are trusting
    that the code you are loading is safe. Make sure to inspect the code online before
    loading and running it automatically!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Loading an official community pipeline is similar, but you can mix loading
    weights from an official repository id and pass pipeline components directly.
    The example below loads the community [CLIP Guided Stable Diffusion](https://github.com/huggingface/diffusers/tree/main/examples/community#clip-guided-stable-diffusion)
    pipeline, and you can pass the CLIP model components directly to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For more information about community pipelines, take a look at the [Community
    pipelines](custom_pipeline_examples) guide for how to use them and if you‚Äôre interested
    in adding a community pipeline check out the [How to contribute a community pipeline](contribute_pipeline)
    guide!
  prefs: []
  type: TYPE_NORMAL
- en: Community components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Community components allow users to build pipelines that may have customized
    components that are not a part of Diffusers. If your pipeline has custom components
    that Diffusers doesn‚Äôt already support, you need to provide their implementations
    as Python modules. These customized components could be a VAE, UNet, and scheduler.
    In most cases, the text encoder is imported from the Transformers library. The
    pipeline code itself can also be customized.
  prefs: []
  type: TYPE_NORMAL
- en: This section shows how users should use community components to build a community
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'You‚Äôll use the [showlab/show-1-base](https://huggingface.co/showlab/show-1-base)
    pipeline checkpoint as an example. So, let‚Äôs start loading the components:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import and load the text encoder from Transformers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Load a scheduler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Load an image processor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In steps 4 and 5, the custom [UNet](https://github.com/showlab/Show-1/blob/main/showone/models/unet_3d_condition.py)
    and [pipeline](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/unet/showone_unet_3d_condition.py)
    implementation must match the format shown in their files for this example to
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Now you‚Äôll load a [custom UNet](https://github.com/showlab/Show-1/blob/main/showone/models/unet_3d_condition.py),
    which in this example, has already been implemented in the `showone_unet_3d_condition.py`
    [script](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/unet/showone_unet_3d_condition.py)
    for your convenience. You‚Äôll notice the `UNet3DConditionModel` class name is changed
    to `ShowOneUNet3DConditionModel` because [UNet3DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet3d-cond#diffusers.UNet3DConditionModel)
    already exists in Diffusers. Any components needed for the `ShowOneUNet3DConditionModel`
    class should be placed in the `showone_unet_3d_condition.py` script.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once this is done, you can initialize the UNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Finally, you‚Äôll load the custom pipeline code. For this example, it has already
    been created for you in the `pipeline_t2v_base_pixel.py` [script](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/pipeline_t2v_base_pixel.py).
    This script contains a custom `TextToVideoIFPipeline` class for generating videos
    from text. Just like the custom UNet, any code needed for the custom pipeline
    to work should go in the `pipeline_t2v_base_pixel.py` script.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once everything is in place, you can initialize the `TextToVideoIFPipeline`
    with the `ShowOneUNet3DConditionModel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Push the pipeline to the Hub to share with the community!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After the pipeline is successfully pushed, you need a couple of changes:'
  prefs: []
  type: TYPE_NORMAL
- en: Change the `_class_name` attribute in [`model_index.json`](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/model_index.json#L2)
    to `"pipeline_t2v_base_pixel"` and `"TextToVideoIFPipeline"`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload `showone_unet_3d_condition.py` to the `unet` [directory](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/unet/showone_unet_3d_condition.py).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload `pipeline_t2v_base_pixel.py` to the pipeline base [directory](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/unet/showone_unet_3d_condition.py).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To run inference, simply add the `trust_remote_code` argument while initializing
    the pipeline to handle all the ‚Äúmagic‚Äù behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As an additional reference example, you can refer to the repository structure
    of [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl/),
    that makes use of the `trust_remote_code` feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
