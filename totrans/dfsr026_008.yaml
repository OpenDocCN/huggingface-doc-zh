- en: Understanding pipelines, models and schedulers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解管道、模型和调度器
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/write_own_pipeline](https://huggingface.co/docs/diffusers/using-diffusers/write_own_pipeline)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/diffusers/using-diffusers/write_own_pipeline](https://huggingface.co/docs/diffusers/using-diffusers/write_own_pipeline)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 🧨 Diffusers is designed to be a user-friendly and flexible toolbox for building
    diffusion systems tailored to your use-case. At the core of the toolbox are models
    and schedulers. While the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    bundles these components together for convenience, you can also unbundle the pipeline
    and use the models and schedulers separately to create new diffusion systems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散器旨在成为一个用户友好和灵活的工具箱，用于构建适合您用例的扩散系统。工具箱的核心是模型和调度器。虽然 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    将这些组件捆绑在一起以方便使用，但您也可以解除管道并单独使用模型和调度器来创建新的扩散系统。
- en: In this tutorial, you’ll learn how to use models and schedulers to assemble
    a diffusion system for inference, starting with a basic pipeline and then progressing
    to the Stable Diffusion pipeline.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将学习如何使用模型和调度器来组装一个用于推断的扩散系统，从基本管道开始，然后逐步过渡到稳定扩散管道。
- en: Deconstruct a basic pipeline
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分解基本管道
- en: 'A pipeline is a quick and easy way to run a model for inference, requiring
    no more than four lines of code to generate an image:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 管道是运行模型进行推断的一种快速简便的方法，只需不超过四行代码即可生成图像：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Image of cat created from DDPMPipeline](../Images/14589ab570d5bde87da475dacae4b73e.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![从DDPMPipeline创建的猫图像](../Images/14589ab570d5bde87da475dacae4b73e.png)'
- en: That was super easy, but how did the pipeline do that? Let’s breakdown the pipeline
    and take a look at what’s happening under the hood.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这太容易了，但管道是如何做到的呢？让我们分解管道并看看底层发生了什么。
- en: In the example above, the pipeline contains a [UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel)
    model and a [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler).
    The pipeline denoises an image by taking random noise the size of the desired
    output and passing it through the model several times. At each timestep, the model
    predicts the *noise residual* and the scheduler uses it to predict a less noisy
    image. The pipeline repeats this process until it reaches the end of the specified
    number of inference steps.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，管道包含一个 [UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel)
    模型和一个 [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)。管道通过取与所需输出相同大小的随机噪声，并将其通过模型多次传递来去噪图像。在每个时间步中，模型预测
    *噪声残差*，调度器使用它来预测一个更少嘈杂的图像。管道重复此过程，直到达到指定的推断步数的末尾。
- en: To recreate the pipeline with the model and scheduler separately, let’s write
    our own denoising process.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要分别使用模型和调度器重新创建管道，请编写我们自己的去噪过程。
- en: 'Load the model and scheduler:'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载模型和调度器：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Set the number of timesteps to run the denoising process for:'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置运行去噪过程的时间步数：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Setting the scheduler timesteps creates a tensor with evenly spaced elements
    in it, 50 in this example. Each element corresponds to a timestep at which the
    model denoises an image. When you create the denoising loop later, you’ll iterate
    over this tensor to denoise an image:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置调度器时间步数会创建一个张量，其中包含均匀间隔的元素，例如此示例中的 50 个。每个元素对应于模型去噪图像的时间步。稍后创建去噪循环时，您将遍历此张量以去噪图像：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create some random noise with the same shape as the desired output:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建与所需输出相同形状的一些随机噪声：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now write a loop to iterate over the timesteps. At each timestep, the model
    does a [UNet2DModel.forward()](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel.forward)
    pass and returns the noisy residual. The scheduler’s [step()](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler.step)
    method takes the noisy residual, timestep, and input and it predicts the image
    at the previous timestep. This output becomes the next input to the model in the
    denoising loop, and it’ll repeat until it reaches the end of the `timesteps` array.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在编写一个循环以遍历时间步。在每个时间步中，模型执行 [UNet2DModel.forward()](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel.forward)
    操作并返回嘈杂的残差。调度器的 [step()](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler.step)
    方法获取嘈杂的残差、时间步和输入，并预测前一个时间步的图像。此输出成为去噪循环中模型的下一个输入，并将重复，直到达到 `timesteps` 数组的末尾。
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is the entire denoising process, and you can use this same pattern to write
    any diffusion system.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是整个去噪过程，您可以使用相同的模式编写任何扩散系统。
- en: 'The last step is to convert the denoised output into an image:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是将去噪输出转换为图像：
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the next section, you’ll put your skills to the test and breakdown the more
    complex Stable Diffusion pipeline. The steps are more or less the same. You’ll
    initialize the necessary components, and set the number of timesteps to create
    a `timestep` array. The `timestep` array is used in the denoising loop, and for
    each element in this array, the model predicts a less noisy image. The denoising
    loop iterates over the `timestep`’s, and at each timestep, it outputs a noisy
    residual and the scheduler uses it to predict a less noisy image at the previous
    timestep. This process is repeated until you reach the end of the `timestep` array.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将将您的技能付诸实践，并分解更复杂的稳定扩散管道。步骤多少是相同的。您将初始化必要的组件，并设置时间步数以创建一个 `timestep`
    数组。`timestep` 数组在去噪循环中使用，对于该数组中的每个元素，模型会预测一个更少嘈杂的图像。去噪循环会遍历 `timestep`，在每个时间步中，它会输出一个嘈杂的残差，调度器使用它来预测前一个时间步的更少嘈杂的图像。这个过程会重复，直到达到
    `timestep` 数组的末尾。
- en: Let’s try it out!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试一试！
- en: Deconstruct the Stable Diffusion pipeline
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分解稳定扩散管道
- en: Stable Diffusion is a text-to-image *latent diffusion* model. It is called a
    latent diffusion model because it works with a lower-dimensional representation
    of the image instead of the actual pixel space, which makes it more memory efficient.
    The encoder compresses the image into a smaller representation, and a decoder
    to convert the compressed representation back into an image. For text-to-image
    models, you’ll need a tokenizer and an encoder to generate text embeddings. From
    the previous example, you already know you need a UNet model and a scheduler.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散是一种文本到图像的*潜在扩散*模型。它被称为潜在扩散模型，因为它使用图像的较低维表示而不是实际的像素空间，这使其更节省内存。编码器将图像压缩为较小的表示，解码器将压缩表示转换回图像。对于文本到图像模型，您将需要一个分词器和一个编码器来生成文本嵌入。从前面的示例中，您已经知道需要一个UNet模型和一个调度器。
- en: As you can see, this is already more complex than the DDPM pipeline which only
    contains a UNet model. The Stable Diffusion model has three separate pretrained
    models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，这已经比仅包含UNet模型的DDPM管线更复杂。稳定扩散模型有三个单独的预训练模型。
- en: 💡 Read the [How does Stable Diffusion work?](https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work)
    blog for more details about how the VAE, UNet, and text encoder models work.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 阅读[稳定扩散是如何工作的？](https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work)博客，了解有关VAE、UNet和文本编码器模型如何工作的更多细节。
- en: 'Now that you know what you need for the Stable Diffusion pipeline, load all
    these components with the [from_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.from_pretrained)
    method. You can find them in the pretrained [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    checkpoint, and each component is stored in a separate subfolder:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您知道稳定扩散管线所需的内容，使用[from_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.from_pretrained)方法加载所有这些组件。您可以在预训练的[`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)检查点中找到它们，每个组件都存储在一个单独的子文件夹中：
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Instead of the default [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler),
    exchange it for the [UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)
    to see how easy it is to plug a different scheduler in:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 将默认的[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)替换为[UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)，看看如何轻松地插入不同的调度器：
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To speed up inference, move the models to a GPU since, unlike the scheduler,
    they have trainable weights:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快推理速度，将模型移动到GPU上，因为与调度器不同，它们具有可训练的权重：
- en: '[PRE9]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Create text embeddings
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建文本嵌入
- en: The next step is to tokenize the text to generate embeddings. The text is used
    to condition the UNet model and steer the diffusion process towards something
    that resembles the input prompt.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是对文本进行标记化以生成嵌入。文本用于调节UNet模型并引导扩散过程朝着类似于输入提示的方向发展。
- en: 💡 The `guidance_scale` parameter determines how much weight should be given
    to the prompt when generating an image.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 `guidance_scale`参数确定在生成图像时应给予提示的权重有多大。
- en: Feel free to choose any prompt you like if you want to generate something else!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想生成其他内容，请随意选择任何提示！
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Tokenize the text and generate the embeddings from the prompt:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化文本并从提示生成嵌入：
- en: '[PRE11]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You’ll also need to generate the *unconditional text embeddings* which are
    the embeddings for the padding token. These need to have the same shape (`batch_size`
    and `seq_length`) as the conditional `text_embeddings`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要生成*无条件文本嵌入*，这些是填充令牌的嵌入。这些需要与条件`text_embeddings`具有相同的形状（`batch_size`和`seq_length`）：
- en: '[PRE12]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s concatenate the conditional and unconditional embeddings into a batch
    to avoid doing two forward passes:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将条件和无条件嵌入连接成一个批次，以避免进行两次前向传递：
- en: '[PRE13]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Create random noise
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建随机噪声
- en: Next, generate some initial random noise as a starting point for the diffusion
    process. This is the latent representation of the image, and it’ll be gradually
    denoised. At this point, the `latent` image is smaller than the final image size
    but that’s okay though because the model will transform it into the final 512x512
    image dimensions later.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，生成一些初始随机噪声作为扩散过程的起点。这是图像的潜在表示，它将逐渐去噪。此时，`latent`图像比最终图像尺寸要小，但这没关系，因为模型稍后会将其转换为最终的512x512图像尺寸。
- en: '💡 The height and width are divided by 8 because the `vae` model has 3 down-sampling
    layers. You can check by running the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 高度和宽度除以8，因为`vae`模型有3个下采样层。您可以通过运行以下内容来检查：
- en: '[PRE14]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Denoise the image
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 去噪图像
- en: 'Start by scaling the input with the initial noise distribution, *sigma*, the
    noise scale value, which is required for improved schedulers like [UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过初始噪声分布*sigma*对输入进行缩放，噪声比例值，这对于改进的调度器如[UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)是必需的：
- en: '[PRE16]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The last step is to create the denoising loop that’ll progressively transform
    the pure noise in `latents` to an image described by your prompt. Remember, the
    denoising loop needs to do three things:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是创建去噪循环，逐渐将`latents`中的纯噪声转换为由您的提示描述的图像。记住，去噪循环需要做三件事：
- en: Set the scheduler’s timesteps to use during denoising.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置调度器在去噪过程中要使用的时间步长。
- en: Iterate over the timesteps.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代时间步长。
- en: At each timestep, call the UNet model to predict the noise residual and pass
    it to the scheduler to compute the previous noisy sample.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个时间步骤中，调用UNet模型来预测噪声残差，并将其传递给调度器以计算先前的嘈杂样本。
- en: '[PRE17]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Decode the image
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码图像
- en: 'The final step is to use the `vae` to decode the latent representation into
    an image and get the decoded output with `sample`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用`vae`将潜在表示解码为图像，并使用`sample`获取解码输出：
- en: '[PRE18]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Lastly, convert the image to a `PIL.Image` to see your generated image!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将图像转换为`PIL.Image`以查看生成的图像！
- en: '[PRE19]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/4e7ad063cb2c61e9b5a381072c11e2f2.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e7ad063cb2c61e9b5a381072c11e2f2.png)'
- en: Next steps
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一步
- en: From basic to complex pipelines, you’ve seen that all you really need to write
    your own diffusion system is a denoising loop. The loop should set the scheduler’s
    timesteps, iterate over them, and alternate between calling the UNet model to
    predict the noise residual and passing it to the scheduler to compute the previous
    noisy sample.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从基本到复杂的管道，您已经看到，编写自己的扩散系统所需的只是一个去噪环。该循环应设置调度器的时间步长，对其进行迭代，并在调用 UNet 模型以预测噪声残差并将其传递给调度器以计算先前的嘈杂样本之间进行交替。
- en: 'This is really what 🧨 Diffusers is designed for: to make it intuitive and easy
    to write your own diffusion system using models and schedulers.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实是 🧨 Diffusers 的设计目的：使使用模型和调度器编写自己的扩散系统变得直观和简单。
- en: 'For your next steps, feel free to:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您可以自由地：
- en: Learn how to [build and contribute a pipeline](../using-diffusers/contribute_pipeline)
    to 🧨 Diffusers. We can’t wait and see what you’ll come up with!
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何[构建和贡献管道](../using-diffusers/contribute_pipeline)给 🧨 Diffusers。我们迫不及待地想看看您会想出什么！
- en: Explore [existing pipelines](../api/pipelines/overview) in the library, and
    see if you can deconstruct and build a pipeline from scratch using the models
    and schedulers separately.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在库中探索[现有的管道](../api/pipelines/overview)，看看您是否可以解构并从头开始构建一个管道，分别使用模型和调度器。
