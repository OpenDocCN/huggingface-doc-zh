- en: Understanding pipelines, models and schedulers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/diffusers/using-diffusers/write_own_pipeline](https://huggingface.co/docs/diffusers/using-diffusers/write_own_pipeline)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/193.74e0ea35.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Tip.230e2334.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/CodeBlock.57fe6e13.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/DocNotebookDropdown.5fa27ace.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ§¨ Diffusers is designed to be a user-friendly and flexible toolbox for building
    diffusion systems tailored to your use-case. At the core of the toolbox are models
    and schedulers. While the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    bundles these components together for convenience, you can also unbundle the pipeline
    and use the models and schedulers separately to create new diffusion systems.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, youâ€™ll learn how to use models and schedulers to assemble
    a diffusion system for inference, starting with a basic pipeline and then progressing
    to the Stable Diffusion pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Deconstruct a basic pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A pipeline is a quick and easy way to run a model for inference, requiring
    no more than four lines of code to generate an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![Image of cat created from DDPMPipeline](../Images/14589ab570d5bde87da475dacae4b73e.png)'
  prefs: []
  type: TYPE_IMG
- en: That was super easy, but how did the pipeline do that? Letâ€™s breakdown the pipeline
    and take a look at whatâ€™s happening under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: In the example above, the pipeline contains a [UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel)
    model and a [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler).
    The pipeline denoises an image by taking random noise the size of the desired
    output and passing it through the model several times. At each timestep, the model
    predicts the *noise residual* and the scheduler uses it to predict a less noisy
    image. The pipeline repeats this process until it reaches the end of the specified
    number of inference steps.
  prefs: []
  type: TYPE_NORMAL
- en: To recreate the pipeline with the model and scheduler separately, letâ€™s write
    our own denoising process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the model and scheduler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of timesteps to run the denoising process for:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Setting the scheduler timesteps creates a tensor with evenly spaced elements
    in it, 50 in this example. Each element corresponds to a timestep at which the
    model denoises an image. When you create the denoising loop later, youâ€™ll iterate
    over this tensor to denoise an image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create some random noise with the same shape as the desired output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now write a loop to iterate over the timesteps. At each timestep, the model
    does a [UNet2DModel.forward()](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel.forward)
    pass and returns the noisy residual. The schedulerâ€™s [step()](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler.step)
    method takes the noisy residual, timestep, and input and it predicts the image
    at the previous timestep. This output becomes the next input to the model in the
    denoising loop, and itâ€™ll repeat until it reaches the end of the `timesteps` array.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This is the entire denoising process, and you can use this same pattern to write
    any diffusion system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to convert the denoised output into an image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, youâ€™ll put your skills to the test and breakdown the more
    complex Stable Diffusion pipeline. The steps are more or less the same. Youâ€™ll
    initialize the necessary components, and set the number of timesteps to create
    a `timestep` array. The `timestep` array is used in the denoising loop, and for
    each element in this array, the model predicts a less noisy image. The denoising
    loop iterates over the `timestep`â€™s, and at each timestep, it outputs a noisy
    residual and the scheduler uses it to predict a less noisy image at the previous
    timestep. This process is repeated until you reach the end of the `timestep` array.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s try it out!
  prefs: []
  type: TYPE_NORMAL
- en: Deconstruct the Stable Diffusion pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stable Diffusion is a text-to-image *latent diffusion* model. It is called a
    latent diffusion model because it works with a lower-dimensional representation
    of the image instead of the actual pixel space, which makes it more memory efficient.
    The encoder compresses the image into a smaller representation, and a decoder
    to convert the compressed representation back into an image. For text-to-image
    models, youâ€™ll need a tokenizer and an encoder to generate text embeddings. From
    the previous example, you already know you need a UNet model and a scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this is already more complex than the DDPM pipeline which only
    contains a UNet model. The Stable Diffusion model has three separate pretrained
    models.
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ’¡ Read the [How does Stable Diffusion work?](https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work)
    blog for more details about how the VAE, UNet, and text encoder models work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you know what you need for the Stable Diffusion pipeline, load all
    these components with the [from_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.from_pretrained)
    method. You can find them in the pretrained [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    checkpoint, and each component is stored in a separate subfolder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of the default [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler),
    exchange it for the [UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)
    to see how easy it is to plug a different scheduler in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To speed up inference, move the models to a GPU since, unlike the scheduler,
    they have trainable weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Create text embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next step is to tokenize the text to generate embeddings. The text is used
    to condition the UNet model and steer the diffusion process towards something
    that resembles the input prompt.
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ’¡ The `guidance_scale` parameter determines how much weight should be given
    to the prompt when generating an image.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to choose any prompt you like if you want to generate something else!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Tokenize the text and generate the embeddings from the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Youâ€™ll also need to generate the *unconditional text embeddings* which are
    the embeddings for the padding token. These need to have the same shape (`batch_size`
    and `seq_length`) as the conditional `text_embeddings`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Letâ€™s concatenate the conditional and unconditional embeddings into a batch
    to avoid doing two forward passes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Create random noise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, generate some initial random noise as a starting point for the diffusion
    process. This is the latent representation of the image, and itâ€™ll be gradually
    denoised. At this point, the `latent` image is smaller than the final image size
    but thatâ€™s okay though because the model will transform it into the final 512x512
    image dimensions later.
  prefs: []
  type: TYPE_NORMAL
- en: 'ðŸ’¡ The height and width are divided by 8 because the `vae` model has 3 down-sampling
    layers. You can check by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Denoise the image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Start by scaling the input with the initial noise distribution, *sigma*, the
    noise scale value, which is required for improved schedulers like [UniPCMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step is to create the denoising loop thatâ€™ll progressively transform
    the pure noise in `latents` to an image described by your prompt. Remember, the
    denoising loop needs to do three things:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the schedulerâ€™s timesteps to use during denoising.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate over the timesteps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each timestep, call the UNet model to predict the noise residual and pass
    it to the scheduler to compute the previous noisy sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Decode the image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The final step is to use the `vae` to decode the latent representation into
    an image and get the decoded output with `sample`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, convert the image to a `PIL.Image` to see your generated image!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4e7ad063cb2c61e9b5a381072c11e2f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Next steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From basic to complex pipelines, youâ€™ve seen that all you really need to write
    your own diffusion system is a denoising loop. The loop should set the schedulerâ€™s
    timesteps, iterate over them, and alternate between calling the UNet model to
    predict the noise residual and passing it to the scheduler to compute the previous
    noisy sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is really what ðŸ§¨ Diffusers is designed for: to make it intuitive and easy
    to write your own diffusion system using models and schedulers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For your next steps, feel free to:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to [build and contribute a pipeline](../using-diffusers/contribute_pipeline)
    to ðŸ§¨ Diffusers. We canâ€™t wait and see what youâ€™ll come up with!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore [existing pipelines](../api/pipelines/overview) in the library, and
    see if you can deconstruct and build a pipeline from scratch using the models
    and schedulers separately.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
