- en: Polytropon
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Polytropon
- en: 'Original text: [https://huggingface.co/docs/peft/package_reference/poly](https://huggingface.co/docs/peft/package_reference/poly)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/peft/package_reference/poly](https://huggingface.co/docs/peft/package_reference/poly)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Polytropon](https://hf.co/papers/2202.13914) is a multitask model with a number
    of different LoRA adapters in it’s “inventory”. The model learns the correct combination
    of adapters from the inventory with a routing function to choose the best subset
    of modules for a specific task. PEFT also supports [Multi-Head Adapter Routing
    (MHR)](https://hf.co/papers/2211.03831) for Polytropon which builds on and improves
    the routing function by combining the adapter heads more granularly. The adapter
    heads are separated into disjoint blocks and a different routing function is learned
    for each one, allowing for more expressivity.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Polytropon](https://hf.co/papers/2202.13914) 是一个多任务模型，其中包含多个不同的LoRA适配器在其“库存”中。该模型通过路由函数学习从库存中选择适配器的正确组合，以选择特定任务的最佳模块子集。PEFT还支持[多头适配器路由（MHR）](https://hf.co/papers/2211.03831)用于Polytropon，它在路由函数上进行了改进，通过更细粒度地组合适配器头部。适配器头部被分成不相交的块，并为每个块学习不同的路由函数，从而实现更多的表达能力。'
- en: Combining Modular Skills in Multitask LearningMulti-Head Adapter Routing for
    Cross-Task Generalization
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 将模块化技能结合在多任务学习中跨任务泛化的多头适配器路由
- en: 'The abstract from the paper is:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要为：
- en: '*A modular design encourages neural models to disentangle and recombine different
    facets of knowledge to generalise more systematically to new tasks. In this work,
    we assume that each task is associated with a subset of latent discrete skills
    from a (potentially small) inventory. In turn, skills correspond to parameter-efficient
    (sparse / low-rank) model parameterisations. By jointly learning these and a task-skill
    allocation matrix, the network for each task is instantiated as the average of
    the parameters of active skills. To favour non-trivial soft partitions of skills
    across tasks, we experiment with a series of inductive biases, such as an Indian
    Buffet Process prior and a two-speed learning rate. We evaluate our latent-skill
    model on two main settings: 1) multitask reinforcement learning for grounded instruction
    following on 8 levels of the BabyAI platform; and 2) few-shot adaptation of pre-trained
    text-to-text generative models on CrossFit, a benchmark comprising 160 NLP tasks.
    We find that the modular design of a network significantly increases sample efficiency
    in reinforcement learning and few-shot generalisation in supervised learning,
    compared to baselines with fully shared, task-specific, or conditionally generated
    parameters where knowledge is entangled across tasks. In addition, we show how
    discrete skills help interpretability, as they yield an explicit hierarchy of
    tasks.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*模块化设计鼓励神经模型将知识的不同方面解开并重新组合，以更系统地推广到新任务。在这项工作中，我们假设每个任务与来自（潜在小）库存的潜在离散技能子集相关联。反过来，技能对应于参数高效（稀疏/低秩）的模型参数化。通过共同学习这些和任务-技能分配矩阵，每个任务的网络被实例化为活动技能的参数的平均值。为了支持跨任务的非平凡软技能划分，我们尝试一系列归纳偏差，例如印度自助餐过程先验和双速学习率。我们在两个主要设置上评估我们的潜在技能模型：1）BabyAI平台上8个级别的多任务强化学习，用于基于指令的跟随；2）在CrossFit上对预训练文本生成模型进行少样本适应，这是一个包含160个NLP任务的基准。我们发现，与完全共享、任务特定或有条件生成参数的基线相比，网络的模块化设计显著提高了强化学习中的样本效率和监督学习中的少样本泛化能力，其中知识在任务之间纠缠。此外，我们展示了离散技能如何帮助解释性，因为它们产生了任务的明确层次结构。*'
- en: PolyConfig
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PolyConfig
- en: '### `class peft.PolyConfig`'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.PolyConfig`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/poly/config.py#L22)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/poly/config.py#L22)'
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`r` (`int`) — Attention dimension of each Lora in Poly.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r` (`int`) — Poly中每个LoRA的注意力维度。'
- en: '`target_modules` (`Union[List[str],str]`) — The names of the modules to apply
    Poly to.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_modules` (`Union[List[str],str]`) — 要应用Poly的模块的名称。'
- en: '`modules_to_save` (`List[str]`) — List of modules apart from Poly layers to
    be set as trainable and saved in the final checkpoint.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modules_to_save` (`List[str]`) — 除了Poly层之外要设置为可训练并保存在最终检查点中的模块列表。'
- en: '`init_weights` (bool) — Whether to perform initialization of Poly weights.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_weights` (bool) — 是否对Poly权重进行初始化。'
- en: '`poly_type` (`Literal["poly"]`) — The variant of the Poly module to use. Currently,
    only “poly” is supported.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`poly_type` (`Literal["poly"]`) — 要使用的Poly模块的变体。目前仅支持“poly”。'
- en: '`n_tasks` (`int`) — The number of tasks in a multitasking scenario.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_tasks` (`int`) — 多任务场景中的任务数量。'
- en: '`n_skills` (`int`) — The number of skills (LoRA) in each Poly layer.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_skills` (`int`) — 每个Poly层中的技能（LoRA）数量。'
- en: '`n_splits` (`int`) — The number of splits within each LoRA of a Poly layer.
    A value greater than 1 indicates the use of Multi-Head Routing (MHR).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_splits` (`int`) — Poly层中每个LoRA内的分割数。大于1的值表示使用多头路由（MHR）。'
- en: This is the configuration class to store the configuration of a [PolyModel](/docs/peft/v0.8.2/en/package_reference/poly#peft.PolyModel).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[PolyModel](/docs/peft/v0.8.2/en/package_reference/poly#peft.PolyModel)配置的配置类。
- en: '[Polytropon (Poly)](https://arxiv.org/abs/2202.13914)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Polytropon (Poly)](https://arxiv.org/abs/2202.13914)'
- en: '[Multi-Head Routing (MHR)](https://arxiv.org/abs/2211.03831)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多头路由（MHR）](https://arxiv.org/abs/2211.03831)'
- en: PolyModel
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PolyModel
- en: '### `class peft.PolyModel`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.PolyModel`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/poly/model.py#L33)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/poly/model.py#L33)'
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
