- en: Polytropon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/peft/package_reference/poly](https://huggingface.co/docs/peft/package_reference/poly)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/peft/v0.8.2/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/start.c9bed6ec.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/scheduler.d627b047.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/singletons.95cf6adf.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.a57a1c33.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/paths.5d07c46f.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/app.72c78cae.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.d48c4817.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/0.aa346fde.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/30.89b0ab38.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Docstring.270658d8.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Heading.47e562a9.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/HfOption.84c24d39.js">
  prefs: []
  type: TYPE_NORMAL
- en: '[Polytropon](https://hf.co/papers/2202.13914) is a multitask model with a number
    of different LoRA adapters in it’s “inventory”. The model learns the correct combination
    of adapters from the inventory with a routing function to choose the best subset
    of modules for a specific task. PEFT also supports [Multi-Head Adapter Routing
    (MHR)](https://hf.co/papers/2211.03831) for Polytropon which builds on and improves
    the routing function by combining the adapter heads more granularly. The adapter
    heads are separated into disjoint blocks and a different routing function is learned
    for each one, allowing for more expressivity.'
  prefs: []
  type: TYPE_NORMAL
- en: Combining Modular Skills in Multitask LearningMulti-Head Adapter Routing for
    Cross-Task Generalization
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A modular design encourages neural models to disentangle and recombine different
    facets of knowledge to generalise more systematically to new tasks. In this work,
    we assume that each task is associated with a subset of latent discrete skills
    from a (potentially small) inventory. In turn, skills correspond to parameter-efficient
    (sparse / low-rank) model parameterisations. By jointly learning these and a task-skill
    allocation matrix, the network for each task is instantiated as the average of
    the parameters of active skills. To favour non-trivial soft partitions of skills
    across tasks, we experiment with a series of inductive biases, such as an Indian
    Buffet Process prior and a two-speed learning rate. We evaluate our latent-skill
    model on two main settings: 1) multitask reinforcement learning for grounded instruction
    following on 8 levels of the BabyAI platform; and 2) few-shot adaptation of pre-trained
    text-to-text generative models on CrossFit, a benchmark comprising 160 NLP tasks.
    We find that the modular design of a network significantly increases sample efficiency
    in reinforcement learning and few-shot generalisation in supervised learning,
    compared to baselines with fully shared, task-specific, or conditionally generated
    parameters where knowledge is entangled across tasks. In addition, we show how
    discrete skills help interpretability, as they yield an explicit hierarchy of
    tasks.*'
  prefs: []
  type: TYPE_NORMAL
- en: PolyConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.PolyConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/poly/config.py#L22)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`r` (`int`) — Attention dimension of each Lora in Poly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_modules` (`Union[List[str],str]`) — The names of the modules to apply
    Poly to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modules_to_save` (`List[str]`) — List of modules apart from Poly layers to
    be set as trainable and saved in the final checkpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_weights` (bool) — Whether to perform initialization of Poly weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`poly_type` (`Literal["poly"]`) — The variant of the Poly module to use. Currently,
    only “poly” is supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_tasks` (`int`) — The number of tasks in a multitasking scenario.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_skills` (`int`) — The number of skills (LoRA) in each Poly layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_splits` (`int`) — The number of splits within each LoRA of a Poly layer.
    A value greater than 1 indicates the use of Multi-Head Routing (MHR).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [PolyModel](/docs/peft/v0.8.2/en/package_reference/poly#peft.PolyModel).
  prefs: []
  type: TYPE_NORMAL
- en: '[Polytropon (Poly)](https://arxiv.org/abs/2202.13914)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multi-Head Routing (MHR)](https://arxiv.org/abs/2211.03831)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PolyModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.PolyModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/poly/model.py#L33)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
