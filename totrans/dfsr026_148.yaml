- en: Kandinsky 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/diffusers/api/pipelines/kandinsky3](https://huggingface.co/docs/diffusers/api/pipelines/kandinsky3)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/50.33922e32.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Tip.230e2334.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Docstring.93f6f462.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/CodeBlock.57fe6e13.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/ExampleCodeBlock.658f5cd6.js">
  prefs: []
  type: TYPE_NORMAL
- en: Kandinsky 3 is created by [Vladimir Arkhipkin](https://github.com/oriBetelgeuse),[Anastasia
    Maltseva](https://github.com/NastyaMittseva),[Igor Pavlov](https://github.com/boomb0om),[Andrei
    Filatov](https://github.com/anvilarth),[Arseniy Shakhmatov](https://github.com/cene555),[Andrey
    Kuznetsov](https://github.com/kuznetsoffandrey),[Denis Dimitrov](https://github.com/denndimitrov),
    [Zein Shaheen](https://github.com/zeinsh)
  prefs: []
  type: TYPE_NORMAL
- en: 'The description from it’s Github page:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Kandinsky 3.0 is an open-source text-to-image diffusion model built upon the
    Kandinsky2-x model family. In comparison to its predecessors, enhancements have
    been made to the text understanding and visual quality of the model, achieved
    by increasing the size of the text encoder and Diffusion U-Net models, respectively.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Its architecture includes 3 main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[FLAN-UL2](https://huggingface.co/google/flan-ul2), which is an encoder decoder
    model based on the T5 architecture.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: New U-Net architecture featuring BigGAN-deep blocks doubles depth while maintaining
    the same number of parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sber-MoVQGAN is a decoder proven to have superior results in image restoration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The original codebase can be found at [ai-forever/Kandinsky-3](https://github.com/ai-forever/Kandinsky-3).
  prefs: []
  type: TYPE_NORMAL
- en: Check out the [Kandinsky Community](https://huggingface.co/kandinsky-community)
    organization on the Hub for the official model checkpoints for tasks like text-to-image,
    image-to-image, and inpainting.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to check out the schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Kandinsky3Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.Kandinsky3Pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky3/pipeline_kandinsky3.py#L49)'
  prefs: []
  type: TYPE_NORMAL
- en: '( tokenizer: T5Tokenizer text_encoder: T5EncoderModel unet: Kandinsky3UNet
    scheduler: DDPMScheduler movq: VQModel )'
  prefs: []
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky3/pipeline_kandinsky3.py#L338)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt: Union = None num_inference_steps: int = 25 guidance_scale: float
    = 3.0 negative_prompt: Union = None num_images_per_prompt: Optional = 1 height:
    Optional = 1024 width: Optional = 1024 generator: Union = None prompt_embeds:
    Optional = None negative_prompt_embeds: Optional = None attention_mask: Optional
    = None negative_attention_mask: Optional = None output_type: Optional = ''pil''
    return_dict: bool = True latents = None callback_on_step_end: Optional = None
    callback_on_step_end_tensor_inputs: List = [''latents''] **kwargs ) → [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    the image generation. If not defined, one has to pass `prompt_embeds`. instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_inference_steps** (`int`, *optional*, defaults to 25) — The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**timesteps** (`List[int]`, *optional*) — Custom timesteps to use for the denoising
    process. If not defined, equal spaced `num_inference_steps` timesteps are used.
    Must be in descending order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**guidance_scale** (`float`, *optional*, defaults to 3.0) — Guidance scale
    as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt** (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_images_per_prompt** (`int`, *optional*, defaults to 1) — The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**height** (`int`, *optional*, defaults to self.unet.config.sample_size) —
    The height in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**width** (`int`, *optional*, defaults to self.unet.config.sample_size) — The
    width in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eta** (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) in the DDIM paper: [https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502).
    Only applies to [schedulers.DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    will be ignored for others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** (`torch.Generator` or `List[torch.Generator]`, *optional*) —
    One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_embeds** (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt_embeds** (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor`, *optional*) — Pre-generated attention
    mask. Must provide if passing `prompt_embeds` directly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_attention_mask** (`torch.FloatTensor`, *optional*) — Pre-generated
    negative attention mask. Must provide if passing `negative_prompt_embeds` directly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_type** (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generate image. Choose between [PIL](https://pillow.readthedocs.io/en/stable/):
    `PIL.Image.Image` or `np.array`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a `~pipelines.stable_diffusion.IFPipelineOutput` instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callback** (`Callable`, *optional*) — A function that will be called every
    `callback_steps` steps during inference. The function will be called with the
    following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callback_steps** (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function will be called. If not specified, the callback will be
    called at every step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**clean_caption** (`bool`, *optional*, defaults to `True`) — Whether or not
    to clean the caption before creating embeddings. Requires `beautifulsoup4` and
    `ftfy` to be installed. If the dependencies are not installed, the embeddings
    will be created from the raw prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attention_kwargs** (`dict`, *optional*) — A kwargs dictionary that
    if specified is passed along to the `AttentionProcessor` as defined under `self.processor`
    in [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Function invoked when calling the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#### encode_prompt'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky3/pipeline_kandinsky3.py#L95)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt do_classifier_free_guidance = True num_images_per_prompt = 1 device
    = None negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds:
    Optional = None _cut_context = False attention_mask: Optional = None negative_attention_mask:
    Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`str` or `List[str]`, *optional*) — prompt to be encoded device
    — (`torch.device`, *optional*): torch device to place the resulting embeddings
    on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_images_per_prompt** (`int`, *optional*, defaults to 1) — number of images
    that should be generated per prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_classifier_free_guidance** (`bool`, *optional*, defaults to `True`) —
    whether to use classifier free guidance or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt** (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`.
    instead. If not defined, one has to pass `negative_prompt_embeds`. instead. Ignored
    when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_embeds** (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt_embeds** (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor`, *optional*) — Pre-generated attention
    mask. Must provide if passing `prompt_embeds` directly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_attention_mask** (`torch.FloatTensor`, *optional*) — Pre-generated
    negative attention mask. Must provide if passing `negative_prompt_embeds` directly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encodes the prompt into text encoder hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: Kandinsky3Img2ImgPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.Kandinsky3Img2ImgPipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky3/pipeline_kandinsky3_img2img.py#L62)'
  prefs: []
  type: TYPE_NORMAL
- en: '( tokenizer: T5Tokenizer text_encoder: T5EncoderModel unet: Kandinsky3UNet
    scheduler: DDPMScheduler movq: VQModel )'
  prefs: []
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky3/pipeline_kandinsky3_img2img.py#L412)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt: Union = None image: Union = None strength: float = 0.3 num_inference_steps:
    int = 25 guidance_scale: float = 3.0 negative_prompt: Union = None num_images_per_prompt:
    Optional = 1 generator: Union = None prompt_embeds: Optional = None negative_prompt_embeds:
    Optional = None attention_mask: Optional = None negative_attention_mask: Optional
    = None output_type: Optional = ''pil'' return_dict: bool = True callback_on_step_end:
    Optional = None callback_on_step_end_tensor_inputs: List = [''latents''] **kwargs
    ) → [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    the image generation. If not defined, one has to pass `prompt_embeds`. instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image** (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`,
    `List[PIL.Image.Image]`, or `List[np.ndarray]`) — `Image`, or tensor representing
    an image batch, that will be used as the starting point for the process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**strength** (`float`, *optional*, defaults to 0.8) — Indicates extent to transform
    the reference `image`. Must be between 0 and 1\. `image` is used as a starting
    point and more noise is added the higher the `strength`. The number of denoising
    steps depends on the amount of noise initially added. When `strength` is 1, added
    noise is maximum and the denoising process runs for the full number of iterations
    specified in `num_inference_steps`. A value of 1 essentially ignores `image`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_inference_steps** (`int`, *optional*, defaults to 50) — The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**guidance_scale** (`float`, *optional*, defaults to 3.0) — Guidance scale
    as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt** (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_images_per_prompt** (`int`, *optional*, defaults to 1) — The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generator** (`torch.Generator` or `List[torch.Generator]`, *optional*) —
    One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**prompt_embeds** (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_prompt_embeds** (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor`, *optional*) — Pre-generated attention
    mask. Must provide if passing `prompt_embeds` directly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**negative_attention_mask** (`torch.FloatTensor`, *optional*) — Pre-generated
    negative attention mask. Must provide if passing `negative_prompt_embeds` directly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_type** (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generate image. Choose between [PIL](https://pillow.readthedocs.io/en/stable/):
    `PIL.Image.Image` or `np.array`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a `~pipelines.stable_diffusion.IFPipelineOutput` instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callback_on_step_end** (`Callable`, *optional*) — A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callback_on_step_end_tensor_inputs** (`List`, *optional*) — The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeline
    class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Function invoked when calling the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#### encode_prompt'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky3/pipeline_kandinsky3_img2img.py#L118)'
  prefs: []
  type: TYPE_NORMAL
- en: '( prompt do_classifier_free_guidance = True num_images_per_prompt = 1 device
    = None negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds:
    Optional = None _cut_context = False attention_mask: Optional = None negative_attention_mask:
    Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt** (`str` or `List[str]`, *optional*) — prompt to be encoded'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encodes the prompt into text encoder hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: 'device: (`torch.device`, *optional*): torch device to place the resulting embeddings
    on num_images_per_prompt (`int`, *optional*, defaults to 1): number of images
    that should be generated per prompt do_classifier_free_guidance (`bool`, *optional*,
    defaults to `True`): whether to use classifier free guidance or not negative_prompt
    (`str` or `List[str]`, *optional*): The prompt or prompts not to guide the image
    generation. If not defined, one has to pass `negative_prompt_embeds`. instead.
    If not defined, one has to pass `negative_prompt_embeds`. instead. Ignored when
    not using guidance (i.e., ignored if `guidance_scale` is less than `1`). prompt_embeds
    (`torch.FloatTensor`, *optional*): Pre-generated text embeddings. Can be used
    to easily tweak text inputs, *e.g.* prompt weighting. If not provided, text embeddings
    will be generated from `prompt` input argument. negative_prompt_embeds (`torch.FloatTensor`,
    *optional*): Pre-generated negative text embeddings. Can be used to easily tweak
    text inputs, *e.g.* prompt weighting. If not provided, negative_prompt_embeds
    will be generated from `negative_prompt` input argument. attention_mask (`torch.FloatTensor`,
    *optional*): Pre-generated attention mask. Must provide if passing `prompt_embeds`
    directly. negative_attention_mask (`torch.FloatTensor`, *optional*): Pre-generated
    negative attention mask. Must provide if passing `negative_prompt_embeds` directly.'
  prefs: []
  type: TYPE_NORMAL
