- en: Question answering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/question_answering](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/question_answering)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/354.45f73746.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Youtube.e1129c6f.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/DocNotebookDropdown.3e6b3817.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Markdown.fef84341.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/stores.c16bc1a5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/ajPx5LwJD-I](https://www.youtube-nocookie.com/embed/ajPx5LwJD-I)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question answering tasks return an answer given a question. If youâ€™ve ever
    asked a virtual assistant like Alexa, Siri or Google what the weather is, then
    youâ€™ve used a question answering model before. There are two common types of question
    answering tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extractive: extract the answer from the given context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abstractive: generate an answer from the context that correctly answers the
    question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This guide will show you how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the
    [SQuAD](https://huggingface.co/datasets/squad) dataset for extractive question
    answering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your finetuned model for inference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The task illustrated in this tutorial is supported by the following model architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ALBERT](../model_doc/albert), [BART](../model_doc/bart), [BERT](../model_doc/bert),
    [BigBird](../model_doc/big_bird), [BigBird-Pegasus](../model_doc/bigbird_pegasus),
    [BLOOM](../model_doc/bloom), [CamemBERT](../model_doc/camembert), [CANINE](../model_doc/canine),
    [ConvBERT](../model_doc/convbert), [Data2VecText](../model_doc/data2vec-text),
    [DeBERTa](../model_doc/deberta), [DeBERTa-v2](../model_doc/deberta-v2), [DistilBERT](../model_doc/distilbert),
    [ELECTRA](../model_doc/electra), [ERNIE](../model_doc/ernie), [ErnieM](../model_doc/ernie_m),
    [Falcon](../model_doc/falcon), [FlauBERT](../model_doc/flaubert), [FNet](../model_doc/fnet),
    [Funnel Transformer](../model_doc/funnel), [OpenAI GPT-2](../model_doc/gpt2),
    [GPT Neo](../model_doc/gpt_neo), [GPT NeoX](../model_doc/gpt_neox), [GPT-J](../model_doc/gptj),
    [I-BERT](../model_doc/ibert), [LayoutLMv2](../model_doc/layoutlmv2), [LayoutLMv3](../model_doc/layoutlmv3),
    [LED](../model_doc/led), [LiLT](../model_doc/lilt), [Longformer](../model_doc/longformer),
    [LUKE](../model_doc/luke), [LXMERT](../model_doc/lxmert), [MarkupLM](../model_doc/markuplm),
    [mBART](../model_doc/mbart), [MEGA](../model_doc/mega), [Megatron-BERT](../model_doc/megatron-bert),
    [MobileBERT](../model_doc/mobilebert), [MPNet](../model_doc/mpnet), [MPT](../model_doc/mpt),
    [MRA](../model_doc/mra), [MT5](../model_doc/mt5), [MVP](../model_doc/mvp), [Nezha](../model_doc/nezha),
    [NystrÃ¶mformer](../model_doc/nystromformer), [OPT](../model_doc/opt), [QDQBert](../model_doc/qdqbert),
    [Reformer](../model_doc/reformer), [RemBERT](../model_doc/rembert), [RoBERTa](../model_doc/roberta),
    [RoBERTa-PreLayerNorm](../model_doc/roberta-prelayernorm), [RoCBert](../model_doc/roc_bert),
    [RoFormer](../model_doc/roformer), [Splinter](../model_doc/splinter), [SqueezeBERT](../model_doc/squeezebert),
    [T5](../model_doc/t5), [UMT5](../model_doc/umt5), [XLM](../model_doc/xlm), [XLM-RoBERTa](../model_doc/xlm-roberta),
    [XLM-RoBERTa-XL](../model_doc/xlm-roberta-xl), [XLNet](../model_doc/xlnet), [X-MOD](../model_doc/xmod),
    [YOSO](../model_doc/yoso)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We encourage you to login to your Hugging Face account so you can upload and
    share your model with the community. When prompted, enter your token to login:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Load SQuAD dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start by loading a smaller subset of the SQuAD dataset from the ðŸ¤— Datasets library.
    Thisâ€™ll give you a chance to experiment and make sure everything works before
    spending more time training on the full dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the datasetâ€™s `train` split into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then take a look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'There are several important fields here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`answers`: the starting location of the answer token and the answer text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context`: background information from which the model needs to extract the
    answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question`: the question a model should answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/qgaM0weJHpA](https://www.youtube-nocookie.com/embed/qgaM0weJHpA)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to load a DistilBERT tokenizer to process the `question` and
    `context` fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few preprocessing steps particular to question answering tasks
    you should be aware of:'
  prefs: []
  type: TYPE_NORMAL
- en: Some examples in a dataset may have a very long `context` that exceeds the maximum
    input length of the model. To deal with longer sequences, truncate only the `context`
    by setting `truncation="only_second"`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, map the start and end positions of the answer to the original `context`
    by setting `return_offset_mapping=True`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the mapping in hand, now you can find the start and end tokens of the answer.
    Use the `sequence_ids` method to find which part of the offset corresponds to
    the `question` and which corresponds to the `context`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is how you can create a function to truncate and map the start and end
    tokens of the `answer` to the `context`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets
    [map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)
    function. You can speed up the `map` function by setting `batched=True` to process
    multiple elements of the dataset at once. Remove any columns you donâ€™t need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now create a batch of examples using [DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator).
    Unlike other data collators in ðŸ¤— Transformers, the [DefaultDataCollator](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DefaultDataCollator)
    does not apply any additional preprocessing such as padding.
  prefs: []
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Train
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: If you arenâ€™t familiar with finetuning a model with the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!
  prefs: []
  type: TYPE_NORMAL
- en: 'Youâ€™re ready to start training your model now! Load DistilBERT with [AutoModelForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForQuestionAnswering):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, only three steps remain:'
  prefs: []
  type: TYPE_NORMAL
- en: Define your training hyperparameters in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
    The only required parameter is `output_dir` which specifies where to save your
    model. Youâ€™ll push this model to the Hub by setting `push_to_hub=True` (you need
    to be signed in to Hugging Face to upload your model).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the training arguments to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    along with the model, dataset, tokenizer, and data collator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to finetune your model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Once training is completed, share your model to the Hub with the [push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)
    method so everyone can use your model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: If you arenâ€™t familiar with finetuning a model with Keras, take a look at the
    basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!
  prefs: []
  type: TYPE_NORMAL
- en: 'To finetune a model in TensorFlow, start by setting up an optimizer function,
    learning rate schedule, and some training hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can load DistilBERT with [TFAutoModelForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForQuestionAnswering):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert your datasets to the `tf.data.Dataset` format with [prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The last thing to setup before you start training is to provide a way to push
    your model to the Hub. This can be done by specifying where to push your model
    and tokenizer in the [PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, youâ€™re ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)
    with your training and validation datasets, the number of epochs, and your callback
    to finetune the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Once training is completed, your model is automatically uploaded to the Hub
    so everyone can use it!
  prefs: []
  type: TYPE_NORMAL
- en: For a more in-depth example of how to finetune a model for question answering,
    take a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)
    or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluation for question answering requires a significant amount of postprocessing.
    To avoid taking up too much of your time, this guide skips the evaluation step.
    The [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    still calculates the evaluation loss during training so youâ€™re not completely
    in the dark about your modelâ€™s performance.
  prefs: []
  type: TYPE_NORMAL
- en: If have more time and youâ€™re interested in how to evaluate your model for question
    answering, take a look at the [Question answering](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing)
    chapter from the ðŸ¤— Hugging Face Course!
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Great, now that youâ€™ve finetuned a model, you can use it for inference!
  prefs: []
  type: TYPE_NORMAL
- en: 'Come up with a question and some context youâ€™d like the model to predict:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The simplest way to try out your finetuned model for inference is to use it
    in a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline).
    Instantiate a `pipeline` for question answering with your model, and pass your
    text to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also manually replicate the results of the `pipeline` if youâ€™d like:'
  prefs: []
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: 'Tokenize the text and return PyTorch tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass your inputs to the model and return the `logits`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the highest probability from the model output for the start and end positions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Decode the predicted tokens to get the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: 'Tokenize the text and return TensorFlow tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass your inputs to the model and return the `logits`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the highest probability from the model output for the start and end positions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Decode the predicted tokens to get the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
