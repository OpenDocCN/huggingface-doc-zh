- en: AsymmetricAutoencoderKL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AsymmetricAutoencoderKL
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/models/asymmetricautoencoderkl](https://huggingface.co/docs/diffusers/api/models/asymmetricautoencoderkl)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/models/asymmetricautoencoderkl](https://huggingface.co/docs/diffusers/api/models/asymmetricautoencoderkl)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Improved larger variational autoencoder (VAE) model with KL loss for inpainting
    task: [Designing a Better Asymmetric VQGAN for StableDiffusion](https://arxiv.org/abs/2306.04632)
    by Zixin Zhu, Xuelu Feng, Dongdong Chen, Jianmin Bao, Le Wang, Yinpeng Chen, Lu
    Yuan, Gang Hua.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 改进的更大的变分自动编码器（VAE）模型，具有用于修复任务的KL损失：[为StableDiffusion设计更好的不对称VQGAN](https://arxiv.org/abs/2306.04632)
    作者：Zixin Zhu，Xuelu Feng，Dongdong Chen，Jianmin Bao，Le Wang，Yinpeng Chen，Lu Yuan，Gang
    Hua。
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要：
- en: '*StableDiffusion is a revolutionary text-to-image generator that is causing
    a stir in the world of image generation and editing. Unlike traditional methods
    that learn a diffusion model in pixel space, StableDiffusion learns a diffusion
    model in the latent space via a VQGAN, ensuring both efficiency and quality. It
    not only supports image generation tasks, but also enables image editing for real
    images, such as image inpainting and local editing. However, we have observed
    that the vanilla VQGAN used in StableDiffusion leads to significant information
    loss, causing distortion artifacts even in non-edited image regions. To this end,
    we propose a new asymmetric VQGAN with two simple designs. Firstly, in addition
    to the input from the encoder, the decoder contains a conditional branch that
    incorporates information from task-specific priors, such as the unmasked image
    region in inpainting. Secondly, the decoder is much heavier than the encoder,
    allowing for more detailed recovery while only slightly increasing the total inference
    cost. The training cost of our asymmetric VQGAN is cheap, and we only need to
    retrain a new asymmetric decoder while keeping the vanilla VQGAN encoder and StableDiffusion
    unchanged. Our asymmetric VQGAN can be widely used in StableDiffusion-based inpainting
    and local editing methods. Extensive experiments demonstrate that it can significantly
    improve the inpainting and editing performance, while maintaining the original
    text-to-image capability. The code is available at [https://github.com/buxiangzhiren/Asymmetric_VQGAN](https://github.com/buxiangzhiren/Asymmetric_VQGAN)*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*StableDiffusion是一种革命性的文本到图像生成器，在图像生成和编辑领域引起轰动。与传统方法不同，传统方法在像素空间中学习扩散模型，StableDiffusion通过VQGAN在潜在空间中学习扩散模型，确保了效率和质量。它不仅支持图像生成任务，还能够对真实图像进行编辑，例如图像修复和局部编辑。然而，我们观察到StableDiffusion中使用的普通VQGAN会导致信息严重丢失，甚至在非编辑的图像区域也会出现失真伪影。因此，我们提出了一种新的不对称VQGAN，具有两个简单的设计。首先，除了来自编码器的输入外，解码器还包含一个条件分支，该分支将任务特定的先验信息（例如修复中的未遮罩图像区域）合并到其中。其次，解码器比编码器更重，可以更详细地恢复，同时仅略微增加总推理成本。我们的不对称VQGAN的训练成本低廉，我们只需要重新训练一个新的不对称解码器，同时保持普通VQGAN编码器和StableDiffusion不变。我们的不对称VQGAN可以广泛应用于基于StableDiffusion的修复和局部编辑方法。大量实验证明，它可以显著提高修复和编辑性能，同时保持原始的文本到图像能力。代码可在
    [https://github.com/buxiangzhiren/Asymmetric_VQGAN](https://github.com/buxiangzhiren/Asymmetric_VQGAN)
    找到*'
- en: Evaluation results can be found in section 4.1 of the original paper.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 评估结果可以在原始论文的第4.1节中找到。
- en: Available checkpoints
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可用的检查点
- en: '[https://huggingface.co/cross-attention/asymmetric-autoencoder-kl-x-1-5](https://huggingface.co/cross-attention/asymmetric-autoencoder-kl-x-1-5)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/cross-attention/asymmetric-autoencoder-kl-x-1-5](https://huggingface.co/cross-attention/asymmetric-autoencoder-kl-x-1-5)'
- en: '[https://huggingface.co/cross-attention/asymmetric-autoencoder-kl-x-2](https://huggingface.co/cross-attention/asymmetric-autoencoder-kl-x-2)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/cross-attention/asymmetric-autoencoder-kl-x-2](https://huggingface.co/cross-attention/asymmetric-autoencoder-kl-x-2)'
- en: Example Usage
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例用法
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: AsymmetricAutoencoderKL
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AsymmetricAutoencoderKL
- en: '### `class diffusers.AsymmetricAutoencoderKL`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.AsymmetricAutoencoderKL`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_asym_kl.py#L26)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_asym_kl.py#L26)'
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`in_channels` (int, *optional*, defaults to 3) — Number of channels in the
    input image.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`in_channels` (int, *可选*, 默认为 3) — 输入图像中的通道数。'
- en: '`out_channels` (int, *optional*, defaults to 3) — Number of channels in the
    output.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_channels` (int, *可选*, 默认为 3) — 输出中的通道数。'
- en: '`down_block_types` (`Tuple[str]`, *optional*, defaults to `("DownEncoderBlock2D",)`)
    — Tuple of downsample block types.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`down_block_types` (`Tuple[str]`, *可选*, 默认为 `("DownEncoderBlock2D",)`) — 下采样块类型的元组。'
- en: '`down_block_out_channels` (`Tuple[int]`, *optional*, defaults to `(64,)`) —
    Tuple of down block output channels.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`down_block_out_channels` (`Tuple[int]`, *可选*, 默认为 `(64,)`) — 下采样块输出通道的元组。'
- en: '`layers_per_down_block` (`int`, *optional*, defaults to `1`) — Number layers
    for down block.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layers_per_down_block` (`int`, *可选*, 默认为 `1`) — 下采样块的层数。'
- en: '`up_block_types` (`Tuple[str]`, *optional*, defaults to `("UpDecoderBlock2D",)`)
    — Tuple of upsample block types.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`up_block_types` (`Tuple[str]`, *可选*, 默认为 `("UpDecoderBlock2D",)`) — 上采样块类型的元组。'
- en: '`up_block_out_channels` (`Tuple[int]`, *optional*, defaults to `(64,)`) — Tuple
    of up block output channels.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`up_block_out_channels` (`Tuple[int]`, *可选*, 默认为 `(64,)`) — 上采样块输出通道的元组。'
- en: '`layers_per_up_block` (`int`, *optional*, defaults to `1`) — Number layers
    for up block.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layers_per_up_block` (`int`, *可选*, 默认为 `1`) — 上采样块的层数。'
- en: '`act_fn` (`str`, *optional*, defaults to `"silu"`) — The activation function
    to use.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`act_fn` (`str`, *可选*, 默认为 `"silu"`) — 要使用的激活函数。'
- en: '`latent_channels` (`int`, *optional*, defaults to 4) — Number of channels in
    the latent space.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latent_channels` (`int`, *可选*, 默认为 4) — 潜在空间中的通道数。'
- en: '`sample_size` (`int`, *optional*, defaults to `32`) — Sample input size.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_size` (`int`, *可选*, 默认为 `32`) — 样本输入大小。'
- en: '`norm_num_groups` (`int`, *optional*, defaults to `32`) — Number of groups
    to use for the first normalization layer in ResNet blocks.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_num_groups` (`int`, *可选*, 默认为 `32`) — 在ResNet块中用于第一个归一化层的组数。'
- en: '`scaling_factor` (`float`, *optional*, defaults to 0.18215) — The component-wise
    standard deviation of the trained latent space computed using the first batch
    of the training set. This is used to scale the latent space to have unit variance
    when training the diffusion model. The latents are scaled with the formula `z
    = z * scaling_factor` before being passed to the diffusion model. When decoding,
    the latents are scaled back to the original scale with the formula: `z = 1 / scaling_factor
    * z`. For more details, refer to sections 4.3.2 and D.1 of the [High-Resolution
    Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
    paper.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaling_factor` (`float`，*optional*，默认为 0.18215) — 使用训练集的第一批计算的训练潜在空间的分量标准差。在训练扩散模型时，用于将潜在空间缩放为单位方差。在传递给扩散模型之前，潜在空间使用公式
    `z = z * scaling_factor` 进行缩放。解码时，潜在空间使用公式缩放回原始比例：`z = 1 / scaling_factor * z`。有关更多详细信息，请参考
    [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
    论文的 4.3.2 和 D.1 节。'
- en: Designing a Better Asymmetric VQGAN for StableDiffusion [https://arxiv.org/abs/2306.04632](https://arxiv.org/abs/2306.04632)
    . A VAE model with KL loss for encoding images into latents and decoding latent
    representations into images.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为 StableDiffusion 设计更好的不对称 VQGAN [https://arxiv.org/abs/2306.04632](https://arxiv.org/abs/2306.04632)。一个具有
    KL 损失的 VAE 模型，用于将图像编码为潜在空间，并将潜在表示解码为图像。
- en: This model inherits from [ModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin).
    Check the superclass documentation for it’s generic methods implemented for all
    models (such as downloading or saving).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自 [ModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin)。查看超类文档以获取所有模型实现的通用方法（如下载或保存）。
- en: '#### `forward`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_asym_kl.py#L158)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_asym_kl.py#L158)'
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sample` (`torch.FloatTensor`) — Input sample.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample` (`torch.FloatTensor`) — 输入样本。'
- en: '`mask` (`torch.FloatTensor`, *optional*, defaults to `None`) — Optional inpainting
    mask.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask` (`torch.FloatTensor`，*optional*，默认为 `None`) — 可选的修复遮罩。'
- en: '`sample_posterior` (`bool`, *optional*, defaults to `False`) — Whether to sample
    from the posterior.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_posterior` (`bool`，*optional*，默认为 `False`) — 是否从后验中采样。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a `DecoderOutput` instead of a plain tuple.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, 默认为 `True`) — 是否返回一个 `DecoderOutput` 而不是一个普通的元组。'
- en: AutoencoderKLOutput
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoencoderKLOutput
- en: '### `class diffusers.models.modeling_outputs.AutoencoderKLOutput`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.modeling_outputs.AutoencoderKLOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_outputs.py#L6)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_outputs.py#L6)'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`latent_dist` (`DiagonalGaussianDistribution`) — Encoded outputs of `Encoder`
    represented as the mean and logvar of `DiagonalGaussianDistribution`. `DiagonalGaussianDistribution`
    allows for sampling latents from the distribution.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latent_dist` (`DiagonalGaussianDistribution`) — `Encoder` 编码输出表示为 `DiagonalGaussianDistribution`
    的均值和对数方差。`DiagonalGaussianDistribution` 允许从分布中采样潜在空间。'
- en: Output of AutoencoderKL encoding method.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: AutoencoderKL 编码方法的输出。
- en: DecoderOutput
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DecoderOutput
- en: '### `class diffusers.models.autoencoders.vae.DecoderOutput`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.autoencoders.vae.DecoderOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/vae.py#L33)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/vae.py#L33)'
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sample` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — The decoded output sample from the last layer of the model.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height, width)`)
    — 模型最后一层的解码输出样本。'
- en: Output of decoding method.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 解码方法的输出。
