# 训练器

> 原始文本：[https://huggingface.co/docs/tokenizers/api/trainers](https://huggingface.co/docs/tokenizers/api/trainers)

PythonRustNode

## BpeTrainer

### `class tokenizers.trainers.BpeTrainer`

```py
( )
```

参数

+   `vocab_size` (`int`, *可选*) — 最终词汇表的大小，包括所有标记和字母表。

+   `min_frequency` (`int`, *可选*) — 一对应该合并的最小频率。

+   `show_progress` (`bool`, *可选*) — 在训练时是否显示进度条。

+   `special_tokens` (`List[Union[str, AddedToken]]`, *可选*) — 模型应该知道的特殊标记列表。

+   `limit_alphabet` (`int`, *可选*) — 保留在字母表中的最大不同字符数。

+   `initial_alphabet` (`List[str]`, *可选*) — 包含在初始字母表中的字符列表，即使在训练数据集中没有看到。如果字符串包含多个字符，则仅保留第一个字符。

+   `continuing_subword_prefix` (`str`, *可选*) — 用于每个不是单词开头的子词的前缀。

+   `end_of_word_suffix` (`str`, *可选*) — 用于每个作为单词结尾的子词的后缀。

+   `max_token_length` (`int`, *可选*) — 防止创建超过指定大小的标记。这有助于减少使用高度重复标记（如*======*用于维基百科）污染词汇表。

能够训练BPE模型的训练器

## UnigramTrainer

### `class tokenizers.trainers.UnigramTrainer`

```py
( vocab_size = 8000 show_progress = True special_tokens = [] shrinking_factor = 0.75 unk_token = None max_piece_length = 16 n_sub_iterations = 2 )
```

参数

+   `vocab_size` (`int`) — 最终词汇表的大小，包括所有标记和字母表。

+   `show_progress` (`bool`) — 在训练时是否显示进度条。

+   `special_tokens` (`List[Union[str, AddedToken]]`) — 模型应该知道的特殊标记列表。

+   `initial_alphabet` (`List[str]`) — 包含在初始字母表中的字符列表，即使在训练数据集中没有看到。如果字符串包含多个字符，则仅保留第一个字符。

+   `shrinking_factor` (`float`) — 用于在训练的每个步骤中缩减词汇表的缩减因子。

+   `unk_token` (`str`) — 用于识别词汇表外标记的标记。

+   `max_piece_length` (`int`) — 给定标记的最大长度。

+   `n_sub_iterations` (`int`) — 在修剪词汇表之前执行EM算法的迭代次数。

能够训练Unigram模型的训练器

## WordLevelTrainer

### `class tokenizers.trainers.WordLevelTrainer`

```py
( )
```

参数

+   `vocab_size` (`int`, *可选*) — 最终词汇表的大小，包括所有标记和字母表。

+   `min_frequency` (`int`, *可选*) — 一对应该合并的最小频率。

+   `show_progress` (`bool`, *可选*) — 在训练时是否显示进度条。

+   `special_tokens` (`List[Union[str, AddedToken]]`) — 模型应该知道的特殊标记列表。

能够训练WorldLevel模型的训练器

## WordPieceTrainer

### `class tokenizers.trainers.WordPieceTrainer`

```py
( vocab_size = 30000 min_frequency = 0 show_progress = True special_tokens = [] limit_alphabet = None initial_alphabet = [] continuing_subword_prefix = '##' end_of_word_suffix = None )
```

参数

+   `vocab_size` (`int`, *可选*) — 最终词汇表的大小，包括所有标记和字母表。

+   `min_frequency` (`int`, *可选*) — 一对应该合并的最小频率。

+   `show_progress` (`bool`, *可选*) — 在训练时是否显示进度条。

+   `special_tokens` (`List[Union[str, AddedToken]]`, *可选*) — 模型应该知道的特殊标记列表。

+   `limit_alphabet` (`int`, *可选*) — 保留在字母表中的最大不同字符数。

+   `initial_alphabet` (`List[str]`, *可选*) — 包含在初始字母表中的字符列表，即使在训练数据集中没有看到。如果字符串包含多个字符，则仅保留第一个字符。

+   `continuing_subword_prefix` (`str`, *可选*) — 用于每个不是单词开头的子词的前缀。

+   `end_of_word_suffix` (`str`, *可选*) — 用于每个作为单词结尾的子词的后缀。

能够训练WordPiece模型的训练器
