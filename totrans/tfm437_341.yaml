- en: CLVP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clvp](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clvp)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CLVP (Contrastive Language-Voice Pretrained Transformer) model was proposed
    in [Better speech synthesis through scaling](https://arxiv.org/abs/2305.07243)
    by James Betker.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*In recent years, the field of image generation has been revolutionized by
    the application of autoregressive transformers and DDPMs. These approaches model
    the process of image generation as a step-wise probabilistic processes and leverage
    large amounts of compute and data to learn the image distribution. This methodology
    of improving performance need not be confined to images. This paper describes
    a way to apply advances in the image generative domain to speech synthesis. The
    result is TorToise - an expressive, multi-voice text-to-speech system.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [Susnato Dhar](https://huggingface.co/susnato).
    The original code can be found [here](https://github.com/neonbjb/tortoise-tts).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CLVP is an integral part of the Tortoise TTS model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CLVP can be used to compare different generated speech candidates with the provided
    text, and the best speech tokens are forwarded to the diffusion model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The use of the `ClvpModelForConditionalGeneration.generate()` method is strongly
    recommended for tortoise usage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the CLVP model expects the audio to be sampled at 22.05 kHz contrary
    to other audio models which expects 16 kHz.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Brief Explanation:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [ClvpTokenizer](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpTokenizer)
    tokenizes the text input, and the [ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor)
    extracts the log mel-spectrogram from the desired audio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ClvpConditioningEncoder` takes those text tokens and audio representations
    and converts them into embeddings conditioned on the text and audio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [ClvpForCausalLM](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpForCausalLM)
    uses those embeddings to generate multiple speech candidates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each speech candidate is passed through the speech encoder ([ClvpEncoder](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpEncoder))
    which converts them into a vector representation, and the text encoder ([ClvpEncoder](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpEncoder))
    converts the text tokens into the same latent space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end, we compare each speech vector with the text vector to see which
    speech vector is most similar to the text vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ClvpModelForConditionalGeneration.generate()` compresses all of the logic
    described above into a single method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ClvpConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClvpConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/configuration_clvp.py#L341)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text_config` (`dict`, *optional*) — Dictionary of configuration options used
    to initialize the CLVP text encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_config` (`dict`, *optional*) — Dictionary of configuration options
    used to initialize CLVP speech encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_config` (`dict`, *optional*) — Dictionary of configuration options
    used to initialize [ClvpDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpDecoderConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`projection_dim` (`int`, *optional*, defaults to 768) — Dimentionality of text
    and speech projection layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logit_scale_init_value` (`float`, *optional*, defaults to 2.6592) — The inital
    value of the *logit_scale* paramter. Default is used as per the original CLVP
    implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — A factor for
    initializing all weight matrices (should be kept to 1.0, used internally for initialization
    testing).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (*optional*) — Dictionary of keyword arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig)
    is the configuration class to store the configuration of a [ClvpModelForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpModelForConditionalGeneration).
    It is used to instantiate a CLVP model according to the specified arguments, defining
    the text model, speech model and decoder model configs. Instantiating a configuration
    with the defaults will yield a similar configuration to that of the CLVP [susnato/clvp_dev](https://huggingface.co/susnato/clvp_dev)
    architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_sub_model_configs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/configuration_clvp.py#L428)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text_config` (`ClvpEncoderConfig`) — Text model configuration of type [ClvpEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpEncoderConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_config` (`ClvpEncoderConfig`) — Speech model configuration of type
    [ClvpEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpEncoderConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_config` (`ClvpDecoderConfig`) — Decoder model configuration of type
    [ClvpDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpDecoderConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig)'
  prefs: []
  type: TYPE_NORMAL
- en: An instance of a configuration object
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a [ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig)
    (or a derived class) from CLVP text model configuration, CLVP speech model configuration
    and CLVP decoder model configuration.
  prefs: []
  type: TYPE_NORMAL
- en: ClvpEncoderConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClvpEncoderConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/configuration_clvp.py#L36)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 256) — Vocabulary size of the
    CLVP Encoder model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intermediate_size` (`int`, *optional*, defaults to 1536) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`projection_dim` (`int`, *optional*, defaults to 768) — Dimensionality of the
    projection vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 20) — Number of hidden
    layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` `"quick_gelu"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio for the
    feed-forward layers in `ClvpEncoderMLP`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_rotary_embedding` (`bool`, *optional*, defaults to `True`) — Whether to
    use rotary_embedding or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_attention_bias` (`bool`, *optional*, defaults to `False`) — Whether to
    use bias in Query, Key and Value layers during self attention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summary_type` (`str`, *optional*, defaults to `"mean"`) — What strategy to
    use to get pooler_output from the last_hidden_state. `"last"`, `"first"`, `"mean"`
    and `"cls_index"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — A factor for
    initializing all weight matrices (should be kept to 1.0, used internally for initialization
    testing).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token_id` (`int`, *optional*, defaults to 255) — Beginning of sequence
    token id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`int`, *optional*, defaults to 0) — End of sequence token id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [ClvpEncoder](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpEncoder).
    It is used to instantiate a CLVP text or CLVP speech encoder according to the
    specified arguments. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the encoder of the CLVP [susnato/clvp_dev](https://huggingface.co/susnato/clvp_dev)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ClvpDecoderConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClvpDecoderConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/configuration_clvp.py#L167)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 8194) — Vocabulary size of the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 608) — The maximum
    sequence length of mel tokens that this model might ever be used with. Similar
    to `n_positions` in `GPT2Config`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_text_tokens` (`int`, *optional*, defaults to 404) — The maximum sequence
    length of text tokens that this model might ever be used with. Similar to `n_positions`
    in `GPT2Config`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 1024) — Dimensionality of the
    embeddings and hidden states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 30) — Number of hidden
    layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 16) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_inner` (`int`, *optional*) — Dimensionality of the inner feed-forward layers.
    `None` will set it to 4 times `hidden_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_mel_attn_blocks` (`int`, *optional*, defaults to 6) — Denotes the number
    of self attention layers in `ClvpConditioningEncoder`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation_function` (`str`, *optional*, defaults to `"gelu_new"`) — Activation
    function, to be selected in the list `["relu", "silu", "gelu", "tanh", "gelu_new"]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resid_pdrop` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embd_pdrop` (`float`, *optional*, defaults to 0.1) — The dropout ratio for
    the embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for the attention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_epsilon` (`float`, *optional*, defaults to 1e-05) — The epsilon
    to use in the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summary_type` (`string`, *optional*, defaults to `"cls_index"`) — Argument
    used when doing sequence summary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Has to be one of the following options:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`"last"`: Take the last token hidden state (like XLNet).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"first"`: Take the first token hidden state (like BERT).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"mean"`: Take the mean of all tokens hidden states.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"cls_index"`: Supply a Tensor of classification token position (like GPT/GPT-2).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"attn"`: Not implemented now, use multi-head attention.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summary_use_proj` (`bool`, *optional*, defaults to `True`) — Whether or not
    to add a projection after the vector extraction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summary_activation` (`str`, *optional*) — Pass `"tanh"` for a tanh activation
    to the output, any other value will result in no activation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summary_proj_to_labels` (`bool`, *optional*, defaults to `True`) — Whether
    the projection outputs should have `config.num_labels` or `config.hidden_size`
    classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summary_first_dropout` (`float`, *optional*, defaults to 0.1) — The dropout
    ratio to be used after the projection and activation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token_id` (`int`, *optional*, defaults to 8192) — Beginning of sequence
    token id, used at the start of the generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`int`, *optional*, defaults to 8193) — End of sequence token
    id, used in the method `ClvpModelForConditionalGeneration.fix_speech_decoder_output()`
    to correct decoder outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature_size` (`int`, *optional*, defaults to 80) — The feature dimension
    of the extracted mel features. This value is used in `ClvpConditioningEncoder`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_attention_bias` (`bool`, *optional*, defaults to `True`) — Whether to
    use bias in Query, Key and Value layers during self attention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — A factor for
    initializing all weight matrices (should be kept to 1.0, used internally for initialization
    testing).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_fixing_codes` (`list`, *optional*, defaults to `[83, 45, 45, 248]`)
    — These values are used in the method `fix_speech_decoder_output` to fix decoder
    generated outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [ClvpDecoder](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpDecoder).
    It is used to instantiate a CLVP Decoder Model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Decoder part of the CLVP [susnato/clvp_dev](https://huggingface.co/susnato/clvp_dev)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture is similar to GPT2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ClvpTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClvpTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/tokenization_clvp.py#L91)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`merges_file` (`str`) — Path to the merges file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`errors` (`str`, *optional*, defaults to `"replace"`) — Paradigm to follow
    when decoding bytes to UTF-8\. See [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)
    for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"[UNK]"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The beginning
    of sequence token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token` (`str`, *optional*, defaults to `"[STOP]"`) — The end of sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token` (`str`, *optional*, defaults to `"[STOP]"`) — The pad token of
    the sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) — Whether or not
    to add an initial space to the input. This allows to treat the leading word just
    as any other word. (CLVP tokenizer detect beginning of words by the preceding
    space).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_bos_token` (`bool`, *optional*, defaults to `False`) — Whether to add
    `bos_token` in front of the sequence when add_special_tokens=True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_eos_token` (`bool`, *optional*, defaults to `False`) — Whether to add
    `eos_token` in end of the sequence when add_special_tokens=True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a CLVP tokenizer. Based on byte-level Byte-Pair-Encoding.
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer has been trained to treat spaces like parts of the tokens (a
    bit like sentencepiece) so a word will
  prefs: []
  type: TYPE_NORMAL
- en: 'be encoded differently whether it is at the beginning of the sentence (without
    space) or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can get around that behavior by passing `add_prefix_space=True` when instantiating
    this tokenizer or when you call it on some text, but since the model was not pretrained
    this way, it might yield a decrease in performance.
  prefs: []
  type: TYPE_NORMAL
- en: When used with `is_split_into_words=True`, this tokenizer will add a space before
    each word (even the first one).
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_vocabulary`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/tokenization_clvp.py#L352)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ClvpFeatureExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClvpFeatureExtractor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/feature_extraction_clvp.py#L33)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_size` (`int`, *optional*, defaults to 80) — The feature dimension
    of the extracted features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*, defaults to 22050) — The sampling rate
    at which the audio files should be digitalized expressed in hertz (Hz).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default_audio_length` (`int`, *optional*, defaults to 6) — The default length
    of raw audio in seconds. If `max_length` is not set during `__call__` then it
    will automatically be set to default_audio_length * `self.sampling_rate`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hop_length` (`int`, *optional*, defaults to 256) — Length of the overlaping
    windows for the STFT used to obtain the Mel Frequency coefficients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunk_length` (`int`, *optional*, defaults to 30) — The maximum number of
    chuncks of `sampling_rate` samples used to trim and pad longer or shorter audio
    sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_fft` (`int`, *optional*, defaults to 1024) — Size of the Fourier transform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding_value` (`float`, *optional*, defaults to 0.0) — Padding value used
    to pad the audio. Should correspond to silences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mel_norms` (`list` of length `feature_size`, *optional*) — If `mel_norms`
    is provided then it will be used to normalize the log-mel spectrograms along each
    mel-filter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_attention_mask` (`bool`, *optional*, defaults to `False`) — Whether
    to return the attention mask. If left to the default, it will return the attention
    mask.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Constructs a CLVP feature extractor.
  prefs: []
  type: TYPE_NORMAL
- en: This feature extractor inherits from [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: This class extracts log-mel-spectrogram features from raw speech using a custom
    numpy implementation of the `Short Time Fourier Transform` which should match
    pytorch’s `torch.stft` equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/feature_extraction_clvp.py#L131)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`raw_speech` (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`)
    — The sequence or batch of sequences to be padded. Each sequence can be a numpy
    array, a list of float values, a list of numpy arrays or a list of list of float
    values. Must be mono channel audio, not stereo, i.e. single float per timestep.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*) — The sampling rate at which the `raw_speech`
    input was sampled. It is strongly recommended to pass `sampling_rate` at the forward
    call to prevent silent errors and allow automatic speech recognition pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`bool`, *optional*, default to `True`) — Activates truncation
    to cut input sequences longer than *max_length* to *max_length*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta), or on TPUs which benefit from having
    sequence lengths be a multiple of 128.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_attention_mask` (`bool`, *optional*, defaults to `True`) — Whether
    to return the attention mask. If left to the default, it will return the attention
    mask.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding_value` (`float`, defaults to 0.0) — The value that is used to fill
    the padding values / vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — The maximum input length of the inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ClvpFeatureExtractor` is used to extract various voice specific properties
    such as the pitch and tone of the voice, speaking speed, and even speaking defects
    like a lisp or stuttering from a sample voice or `raw_speech`.'
  prefs: []
  type: TYPE_NORMAL
- en: First the voice is padded or truncated in a way such that it becomes a waveform
    of `self.default_audio_length` seconds long and then the log-mel spectrogram is
    extracted from it.
  prefs: []
  type: TYPE_NORMAL
- en: ClvpProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClvpProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/processing_clvp.py#L24)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_extractor` (`ClvpFeatureExtractor`) — An instance of [ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor).
    The feature extractor is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`ClvpTokenizer`) — An instance of [ClvpTokenizer](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpTokenizer).
    The tokenizer is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a CLVP processor which wraps a CLVP Feature Extractor and a CLVP
    Tokenizer into a single processor.
  prefs: []
  type: TYPE_NORMAL
- en: '[ClvpProcessor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpProcessor)
    offers all the functionalities of [ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor)
    and [ClvpTokenizer](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpTokenizer).
    See the [**call**()](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpProcessor.__call__),
    [decode()](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpProcessor.decode)
    and [batch_decode()](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpProcessor.batch_decode)
    for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/processing_clvp.py#L49)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Forwards the `audio` and `sampling_rate` arguments to [**call**()](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor.__call__)
    and the `text` argument to [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__).
    Please refer to the doctsring of the above two methods for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/processing_clvp.py#L86)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This method forwards all its arguments to ClvpTokenizer’s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `batch_decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/processing_clvp.py#L78)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This method forwards all its arguments to ClvpTokenizer’s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  prefs: []
  type: TYPE_NORMAL
- en: ClvpModelForConditionalGeneration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClvpModelForConditionalGeneration`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1509)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The composite CLVP model with a text encoder, speech encoder and speech decoder
    model.The speech decoder model generates the speech_ids from the text and the
    text encoder and speech encoder workstogether to filter out the best speech_ids.
    This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1737)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of input sequence tokens in the vocabulary. Padding will be ignored
    by default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, feature_size,
    time_dim)`) — Indicates log mel-spectrogram representations for audio returned
    by [ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conditioning_encoder_inputs_embeds` (`torch.FloatTensor`, *optional*) — inputs_embeds
    for `ClvpConditioningEncoder`. Can be used in place of `input_ids`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder_inputs_embeds` (`torch.FloatTensor`, *optional*) — inputs_embeds
    for the text encoder model passed in place of `input_ids`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding text token indices.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_loss` (`bool`, *optional*) — Whether or not to return the contrastive
    loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.clvp.modeling_clvp.ClvpOutput` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.clvp.modeling_clvp.ClvpOutput` or a tuple of `torch.FloatTensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration (`<class 'transformers.models.clvp.configuration_clvp.ClvpConfig'>`)
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss`
    is `True`) — Contrastive loss for speech-text similarity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_ids` (`torch.LongTensor`, *optional*) — speech_ids (or speech candidates)
    generated by the `ClvpForCausalLM` model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_per_speech` (`torch.FloatTensor` of shape `(speech_batch_size, text_batch_size)`)
    — The scaled dot product scores between `speech_embeds` and `text_embeds`. This
    represents the speech-text similarity scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_per_text` (`torch.FloatTensor` of shape `(text_batch_size, speech_batch_size)`)
    — The scaled dot product scores between `text_embeds` and `speech_embeds`. This
    represents the text-speech similarity scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_embeds` (`torch.FloatTensor` of shape `(batch_size, output_dim`) — The
    text embeddings obtained by applying the projection layer to the pooled output
    of the text encoder model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_embeds` (`torch.FloatTensor` of shape `(batch_size, output_dim`) —
    The speech embeddings obtained by applying the projection layer to the pooled
    output of the speech encoder model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_model_output` (`BaseModelOutputWithPooling`) — The pooled output of the
    `last_hidden_state` of the text encoder Model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_model_output` (`BaseModelOutputWithPooling`) — The pooled output of
    the `last_hidden_state` of the speech encoder Model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`torch.FloatTensor`, *optional*) — The hidden states
    of the decoder model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder_hidden_states` (`torch.FloatTensor`, *optional*) — The hidden
    states of the text encoder model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speech_encoder_hidden_states` (`torch.FloatTensor`, *optional*) — The hidden
    states of the speech encoder model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [ClvpModelForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpModelForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1869)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Input text Tokens. Processed from the [ClvpTokenizer](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, feature_size,
    time_dim)`, *optional*) — Indicates log-melspectrogram representations for audio
    returned by [ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding text token indices.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_max_mel_tokens` (`int`, *optional*) — Pads generated speech_ids to
    the specified value. This is to implement the same logic from the official repo,
    link: [https://github.com/neonbjb/tortoise-tts/blob/80f89987a5abda5e2b082618cd74f9c7411141dc/tortoise/api.py#L430](https://github.com/neonbjb/tortoise-tts/blob/80f89987a5abda5e2b082618cd74f9c7411141dc/tortoise/api.py#L430)
    and to make sure the logits are same. This does not affect generation quality
    so please don’t consider using it since it is less efficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of decoder model, text encoder and speech encoder models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`ClvpOutput` or tuple'
  prefs: []
  type: TYPE_NORMAL
- en: A `ClvpOutput` (if `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`)
    or a tuple.
  prefs: []
  type: TYPE_NORMAL
- en: Generate method for `ClvpModelForConditionalGeneration`, this method calls the
    `generate` method of `ClvpForCausalLM` and then uses those generated `speech_ids`
    to process `text_embeds` and `speech_embeds` using `ClvpEncoder`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_text_features`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1583)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`text_encoder_inputs_embeds` (`torch.FloatTensor`, *optional*) — inputs_embeds
    for the text encoder model passed in place of `input_ids`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, output_dim)`'
  prefs: []
  type: TYPE_NORMAL
- en: The text embeddings obtained by applying the projection layer to the pooled
    output of the CLVP Text Model.
  prefs: []
  type: TYPE_NORMAL
- en: This method can be used to extract text_embeds from a text. The text embeddings
    obtained by applying the projection layer to the pooled output of the CLVP text
    encoder model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '#### `get_speech_features`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1640)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`speech_ids` (`torch.LongTensor` of shape `(batch_size, num_speech_ids)`, *optional*)
    — Speech Tokens. Padding will be ignored by default should you provide it. If
    speech_ids are provided then input_ids and input_features will be automatically
    ignored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Input text Tokens. Processed from the [ClvpTokenizer](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpTokenizer).
    If speech_ids is not provided, then input_ids and input_features will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, feature_size,
    time_dim)`, *optional*) — Indicates log-melspectrogram representations for audio
    returned by [ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor).
    If speech_ids is not provided, then input_ids and input_features will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conditioning_encoder_inputs_embeds` (`torch.FloatTensor`, *optional*) — inputs_embeds
    for `ClvpConditioningEncoder`. Can be used in place of `input_ids`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding speech token indices.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generation_config` (`GenerationConfig`, *optional*) — generation config to
    control the generation of speech_ids if they are not provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, output_dim)`'
  prefs: []
  type: TYPE_NORMAL
- en: The speech embeddings obtained by applying the projection layer to the pooled
    output of the CLVP Speech Model.
  prefs: []
  type: TYPE_NORMAL
- en: This method can be used to extract speech_embeds. The speech embeddings are
    obtained by applying the speech model on speech_ids. If speech_ids is not present
    but both input_ids and input_features are given then the decoder model will be
    used to first generate the speech_ids and then applying the speech model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: ClvpForCausalLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClvpForCausalLM`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1280)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CLVP decoder model with a language modelling head on top. This model inherits
    from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1421)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`)
    — Contains precomputed hidden-states (key and values in the attention blocks)
    as computed by the model (see `past_key_values` output below). Can be used to
    speed up sequential decoding. The `input_ids` which have their past given to this
    model should not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `past_key_values` is used, `attention_mask` needs to contain the masking
    strategy that was used for `past_key_values`. In other words, the `attention_mask`
    always has to have the length: `len(past_key_values) + len(input_ids)`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `inputs_embeds` have
    to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for language modeling. Note that the labels **are shifted** inside the
    model, i.e. you can set `labels = input_ids` Indices are selected in `[-100, 0,
    ..., config.vocab_size]` All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [ClvpForCausalLM](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpForCausalLM)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: ClvpModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClvpModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1209)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare Clvp decoder model outputting raw hidden-states without any specific
    head on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1231)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`)
    — Contains precomputed hidden-states (key and values in the attention blocks)
    as computed by the model (see `past_key_values` output below). Can be used to
    speed up sequential decoding. The `input_ids` which have their past given to this
    model should not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `past_key_values` is used, `attention_mask` needs to contain the masking
    strategy that was used for `past_key_values`. In other words, the `attention_mask`
    always has to have the length: `len(past_key_values) + len(input_ids)`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `inputs_embeds` have
    to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [ClvpModel](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: ClvpEncoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClvpEncoder`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L874)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Transformer encoder consisting of `config.num_hidden_layers` self attention
    layers. Each layer is a `ClvpEncoderLayer`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L906)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`,
    *optional*) — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — input embeddings for the model. This bypasses the
    model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor`, *optional*) — Denotes the position ids
    of `input_ids`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ClvpDecoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.ClvpDecoder`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1030)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer
    is a `ClvpDecoderLayer`
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1065)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`)
    — Contains precomputed hidden-states (key and values in the attention blocks)
    as computed by the model (see `past_key_values` output below). Can be used to
    speed up sequential decoding. The `input_ids` which have their past given to this
    model should not be passed as `input_ids` as they have already been computed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `past_key_values` is used, `attention_mask` needs to contain the masking
    strategy that was used for `past_key_values`. In other words, the `attention_mask`
    always has to have the length: `len(past_key_values) + len(input_ids)`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used, optionally only the last `inputs_embeds` have
    to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [ClvpDecoder](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpDecoder)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
