- en: CLVP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CLVP
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clvp](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clvp)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '原始文本: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clvp](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clvp)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The CLVP (Contrastive Language-Voice Pretrained Transformer) model was proposed
    in [Better speech synthesis through scaling](https://arxiv.org/abs/2305.07243)
    by James Betker.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: CLVP（对比语言-声音预训练变压器）模型由James Betker在[通过缩放实现更好的语音合成](https://arxiv.org/abs/2305.07243)中提出。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*In recent years, the field of image generation has been revolutionized by
    the application of autoregressive transformers and DDPMs. These approaches model
    the process of image generation as a step-wise probabilistic processes and leverage
    large amounts of compute and data to learn the image distribution. This methodology
    of improving performance need not be confined to images. This paper describes
    a way to apply advances in the image generative domain to speech synthesis. The
    result is TorToise - an expressive, multi-voice text-to-speech system.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*近年来，图像生成领域已经通过自回归变压器和DDPMs的应用而发生了革命。这些方法将图像生成过程建模为逐步的概率过程，并利用大量计算和数据来学习图像分布。提高性能的这种方法不一定局限于图像。本文描述了一种将图像生成领域的进展应用于语音合成的方法。结果是TorToise
    - 一种富有表现力的、多声音的文本到语音系统。*'
- en: This model was contributed by [Susnato Dhar](https://huggingface.co/susnato).
    The original code can be found [here](https://github.com/neonbjb/tortoise-tts).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[Susnato Dhar](https://huggingface.co/susnato)贡献。原始代码可在[此处](https://github.com/neonbjb/tortoise-tts)找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: CLVP is an integral part of the Tortoise TTS model.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CLVP是Tortoise TTS模型的一个重要部分。
- en: CLVP can be used to compare different generated speech candidates with the provided
    text, and the best speech tokens are forwarded to the diffusion model.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CLVP可用于将不同生成的语音候选与提供的文本进行比较，并将最佳语音标记转发到扩散模型。
- en: The use of the `ClvpModelForConditionalGeneration.generate()` method is strongly
    recommended for tortoise usage.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强烈建议使用`ClvpModelForConditionalGeneration.generate()`方法进行龟速使用。
- en: Note that the CLVP model expects the audio to be sampled at 22.05 kHz contrary
    to other audio models which expects 16 kHz.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，CLVP模型期望音频采样率为22.05 kHz，而其他音频模型期望为16 kHz。
- en: 'Brief Explanation:'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简要说明：
- en: The [ClvpTokenizer](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpTokenizer)
    tokenizes the text input, and the [ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor)
    extracts the log mel-spectrogram from the desired audio.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ClvpTokenizer](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpTokenizer)
    对文本输入进行标记化处理，而[ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor)
    从所需音频中提取对数梅尔频谱图。'
- en: '`ClvpConditioningEncoder` takes those text tokens and audio representations
    and converts them into embeddings conditioned on the text and audio.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ClvpConditioningEncoder` 获取这些文本标记和音频表示，并将它们转换为在文本和音频上进行条件化的嵌入。'
- en: The [ClvpForCausalLM](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpForCausalLM)
    uses those embeddings to generate multiple speech candidates.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ClvpForCausalLM](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpForCausalLM)
    使用这些嵌入来生成多个语音候选。'
- en: Each speech candidate is passed through the speech encoder ([ClvpEncoder](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpEncoder))
    which converts them into a vector representation, and the text encoder ([ClvpEncoder](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpEncoder))
    converts the text tokens into the same latent space.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个语音候选通过语音编码器（[ClvpEncoder](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpEncoder)）传递，将它们转换为矢量表示，文本编码器（[ClvpEncoder](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpEncoder)）将文本标记转换为相同的潜在空间。
- en: At the end, we compare each speech vector with the text vector to see which
    speech vector is most similar to the text vector.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将每个语音向量与文本向量进行比较，以查看哪个语音向量与文本向量最相似。
- en: '`ClvpModelForConditionalGeneration.generate()` compresses all of the logic
    described above into a single method.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ClvpModelForConditionalGeneration.generate()` 将上述所有逻辑压缩为一个方法。'
- en: 'Example :'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ClvpConfig
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ClvpConfig
- en: '### `class transformers.ClvpConfig`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ClvpConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/configuration_clvp.py#L341)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/configuration_clvp.py#L341)'
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text_config` (`dict`, *optional*) — Dictionary of configuration options used
    to initialize the CLVP text encoder.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_config` (`dict`, *可选*) — 用于初始化CLVP文本编码器的配置选项字典。'
- en: '`speech_config` (`dict`, *optional*) — Dictionary of configuration options
    used to initialize CLVP speech encoder.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speech_config` (`dict`, *可选*) — 用于初始化CLVP语音编码器的配置选项字典。'
- en: '`decoder_config` (`dict`, *optional*) — Dictionary of configuration options
    used to initialize [ClvpDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpDecoderConfig).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_config` (`dict`, *可选*) — 用于初始化[ClvpDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpDecoderConfig)的配置选项字典。'
- en: '`projection_dim` (`int`, *optional*, defaults to 768) — Dimentionality of text
    and speech projection layers.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projection_dim` (`int`, *可选*, 默认为768) — 文本和语音投影层的维度。'
- en: '`logit_scale_init_value` (`float`, *optional*, defaults to 2.6592) — The inital
    value of the *logit_scale* paramter. Default is used as per the original CLVP
    implementation.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logit_scale_init_value` (`float`, *可选*, 默认为2.6592) — *logit_scale*参数的初始值。默认值根据原始CLVP实现使用。'
- en: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — A factor for
    initializing all weight matrices (should be kept to 1.0, used internally for initialization
    testing).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_factor` (`float`, *可选*, 默认为1.0) — 用于初始化所有权重矩阵的因子（应保持为1.0，用于内部初始化测试）。'
- en: '`kwargs` (*optional*) — Dictionary of keyword arguments.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*可选*) — 关键字参数字典。'
- en: '[ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig)
    is the configuration class to store the configuration of a [ClvpModelForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpModelForConditionalGeneration).
    It is used to instantiate a CLVP model according to the specified arguments, defining
    the text model, speech model and decoder model configs. Instantiating a configuration
    with the defaults will yield a similar configuration to that of the CLVP [susnato/clvp_dev](https://huggingface.co/susnato/clvp_dev)
    architecture.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig)
    是用于存储 [ClvpModelForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpModelForConditionalGeneration)
    配置的类。它用于根据指定的参数实例化 CLVP 模型，定义文本模型、语音模型和解码器模型配置。使用默认值实例化配置将产生类似于 CLVP [susnato/clvp_dev](https://huggingface.co/susnato/clvp_dev)
    架构的配置。'
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    的文档以获取更多信息。
- en: 'Example:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `from_sub_model_configs`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_sub_model_configs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/configuration_clvp.py#L428)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/configuration_clvp.py#L428)'
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text_config` (`ClvpEncoderConfig`) — Text model configuration of type [ClvpEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpEncoderConfig).'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_config` (`ClvpEncoderConfig`) — 类型为 [ClvpEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpEncoderConfig)
    的文本模型配置。'
- en: '`speech_config` (`ClvpEncoderConfig`) — Speech model configuration of type
    [ClvpEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpEncoderConfig).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speech_config` (`ClvpEncoderConfig`) — 类型为 [ClvpEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpEncoderConfig)
    的语音模型配置。'
- en: '`decoder_config` (`ClvpDecoderConfig`) — Decoder model configuration of type
    [ClvpDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpDecoderConfig).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_config` (`ClvpDecoderConfig`) — 类型为 [ClvpDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpDecoderConfig)
    的解码器模型配置。'
- en: Returns
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig)'
- en: An instance of a configuration object
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象的一个实例
- en: Instantiate a [ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig)
    (or a derived class) from CLVP text model configuration, CLVP speech model configuration
    and CLVP decoder model configuration.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从 CLVP 文本模型配置、CLVP 语音模型配置和 CLVP 解码器模型配置实例化一个 [ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig)（或派生类）。
- en: ClvpEncoderConfig
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ClvpEncoderConfig
- en: '### `class transformers.ClvpEncoderConfig`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ClvpEncoderConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/configuration_clvp.py#L36)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/configuration_clvp.py#L36)'
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 256) — Vocabulary size of the
    CLVP Encoder model.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 256) — CLVP 编码器模型的词汇表大小。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化层的维度。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 1536) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 1536) — Transformer 编码器中“中间”（即前馈）层的维度。'
- en: '`projection_dim` (`int`, *optional*, defaults to 768) — Dimensionality of the
    projection vector.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projection_dim` (`int`, *optional*, defaults to 768) — 投影向量的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 20) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 20) — Transformer 编码器中的隐藏层数。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer 编码器中每个注意力层的注意力头数。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` `"quick_gelu"` are supported.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持
    `"gelu"`、`"relu"`、`"selu"`、`"gelu_new"` 和 `"quick_gelu"`。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — 层归一化层使用的 epsilon。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for the attention probabilities.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力概率的 dropout
    比率。'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio for the
    feed-forward layers in `ClvpEncoderMLP`.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, defaults to 0.1) — `ClvpEncoderMLP` 中前馈层的 dropout
    比率。'
- en: '`use_rotary_embedding` (`bool`, *optional*, defaults to `True`) — Whether to
    use rotary_embedding or not.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_rotary_embedding` (`bool`, *optional*, defaults to `True`) — 是否使用旋转嵌入。'
- en: '`use_attention_bias` (`bool`, *optional*, defaults to `False`) — Whether to
    use bias in Query, Key and Value layers during self attention.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_attention_bias` (`bool`, *optional*, defaults to `False`) — 在自注意力期间是否使用
    Query、Key 和 Value 层中的偏置。'
- en: '`summary_type` (`str`, *optional*, defaults to `"mean"`) — What strategy to
    use to get pooler_output from the last_hidden_state. `"last"`, `"first"`, `"mean"`
    and `"cls_index"` are supported.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_type` (`str`, *optional*, defaults to `"mean"`) — 从 last_hidden_state
    获取 pooler_output 的策略。支持 `"last"`、`"first"`、`"mean"` 和 `"cls_index"`。'
- en: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — A factor for
    initializing all weight matrices (should be kept to 1.0, used internally for initialization
    testing).'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_factor` (`float`, *optional*, 默认为1.0) — 用于初始化所有权重矩阵的因子（应保持为1.0，用于内部初始化测试）。'
- en: '`bos_token_id` (`int`, *optional*, defaults to 255) — Beginning of sequence
    token id.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token_id` (`int`, *optional*, 默认为255) — 序列开始标记id。'
- en: '`eos_token_id` (`int`, *optional*, defaults to 0) — End of sequence token id.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`, *optional*, 默认为0) — 序列结束标记id。'
- en: This is the configuration class to store the configuration of a [ClvpEncoder](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpEncoder).
    It is used to instantiate a CLVP text or CLVP speech encoder according to the
    specified arguments. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the encoder of the CLVP [susnato/clvp_dev](https://huggingface.co/susnato/clvp_dev)
    architecture.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[ClvpEncoder](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpEncoder)配置的配置类。根据指定的参数实例化一个CLVP文本或CLVP语音编码器。使用默认值实例化配置将产生与CLVP
    [susnato/clvp_dev](https://huggingface.co/susnato/clvp_dev)架构的编码器类似的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ClvpDecoderConfig
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ClvpDecoderConfig
- en: '### `class transformers.ClvpDecoderConfig`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ClvpDecoderConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/configuration_clvp.py#L167)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/configuration_clvp.py#L167)'
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 8194) — Vocabulary size of the
    model.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, 默认为8194) — 模型的词汇表大小。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 608) — The maximum
    sequence length of mel tokens that this model might ever be used with. Similar
    to `n_positions` in `GPT2Config`.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, 默认为608) — 此模型可能用于的最大mel标记序列长度。类似于`GPT2Config`中的`n_positions`。'
- en: '`max_text_tokens` (`int`, *optional*, defaults to 404) — The maximum sequence
    length of text tokens that this model might ever be used with. Similar to `n_positions`
    in `GPT2Config`.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_text_tokens` (`int`, *optional*, 默认为404) — 此模型可能用于的文本标记的最大序列长度。类似于`GPT2Config`中的`n_positions`。'
- en: '`hidden_size` (`int`, *optional*, defaults to 1024) — Dimensionality of the
    embeddings and hidden states.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, 默认为1024) — 嵌入和隐藏状态的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 30) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, 默认为30) — Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 16) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, 默认为16) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`n_inner` (`int`, *optional*) — Dimensionality of the inner feed-forward layers.
    `None` will set it to 4 times `hidden_size`.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_inner` (`int`, *optional*) — 内部前馈层的维度。`None`将将其设置为`hidden_size`的4倍。'
- en: '`num_mel_attn_blocks` (`int`, *optional*, defaults to 6) — Denotes the number
    of self attention layers in `ClvpConditioningEncoder`.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_mel_attn_blocks` (`int`, *optional*, 默认为6) — 表示`ClvpConditioningEncoder`中的自注意力层数量。'
- en: '`activation_function` (`str`, *optional*, defaults to `"gelu_new"`) — Activation
    function, to be selected in the list `["relu", "silu", "gelu", "tanh", "gelu_new"]`.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_function` (`str`, *optional*, 默认为`"gelu_new"`) — 激活函数，可在列表`["relu",
    "silu", "gelu", "tanh", "gelu_new"]`中选择。'
- en: '`resid_pdrop` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resid_pdrop` (`float`, *optional*, 默认为0.1) — 嵌入层、编码器和池化器中所有全连接层的丢弃概率。'
- en: '`embd_pdrop` (`float`, *optional*, defaults to 0.1) — The dropout ratio for
    the embeddings.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embd_pdrop` (`float`, *optional*, 默认为0.1) — 嵌入的丢弃比率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for the attention.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, 默认为0.1) — 注意力的丢弃比率。'
- en: '`layer_norm_epsilon` (`float`, *optional*, defaults to 1e-05) — The epsilon
    to use in the layer normalization layers.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_epsilon` (`float`, *optional*, 默认为1e-05) — 在层归一化层中使用的epsilon。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`summary_type` (`string`, *optional*, defaults to `"cls_index"`) — Argument
    used when doing sequence summary.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_type` (`string`, *optional*, 默认为`"cls_index"`) — 在进行序列摘要时使用的参数。'
- en: 'Has to be one of the following options:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 必须是以下选项之一：
- en: '`"last"`: Take the last token hidden state (like XLNet).'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"last"`: 获取最后一个标记的隐藏状态（类似XLNet）。'
- en: '`"first"`: Take the first token hidden state (like BERT).'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"first"`: 获取第一个标记的隐藏状态（类似BERT）。'
- en: '`"mean"`: Take the mean of all tokens hidden states.'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"mean"`: 获取所有标记的隐藏状态的平均值。'
- en: '`"cls_index"`: Supply a Tensor of classification token position (like GPT/GPT-2).'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"cls_index"`: 提供分类标记位置的张量（类似GPT/GPT-2）。'
- en: '`"attn"`: Not implemented now, use multi-head attention.'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"attn"`: 目前未实现，使用多头注意力。'
- en: '`summary_use_proj` (`bool`, *optional*, defaults to `True`) — Whether or not
    to add a projection after the vector extraction.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_use_proj` (`bool`, *optional*, 默认为`True`) — 是否在向量提取后添加投影。'
- en: '`summary_activation` (`str`, *optional*) — Pass `"tanh"` for a tanh activation
    to the output, any other value will result in no activation.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_activation` (`str`, *optional*) — 将`"tanh"`传递给输出以获得tanh激活，任何其他值将导致无激活。'
- en: '`summary_proj_to_labels` (`bool`, *optional*, defaults to `True`) — Whether
    the projection outputs should have `config.num_labels` or `config.hidden_size`
    classes.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_proj_to_labels` (`bool`, *可选*, 默认为 `True`) — 投影输出是否应具有 `config.num_labels`
    或 `config.hidden_size` 类别。'
- en: '`summary_first_dropout` (`float`, *optional*, defaults to 0.1) — The dropout
    ratio to be used after the projection and activation.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary_first_dropout` (`float`, *可选*, 默认为 0.1) — 投影和激活后要使用的丢弃比率。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *可选*, 默认为 `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。'
- en: '`bos_token_id` (`int`, *optional*, defaults to 8192) — Beginning of sequence
    token id, used at the start of the generation.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token_id` (`int`, *可选*, 默认为 8192) — 序列开始标记的 ID，在生成开始时使用。'
- en: '`eos_token_id` (`int`, *optional*, defaults to 8193) — End of sequence token
    id, used in the method `ClvpModelForConditionalGeneration.fix_speech_decoder_output()`
    to correct decoder outputs.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`, *可选*, 默认为 8193) — 序列结束标记的 ID，在方法 `ClvpModelForConditionalGeneration.fix_speech_decoder_output()`
    中用于修正解码器输出。'
- en: '`feature_size` (`int`, *optional*, defaults to 80) — The feature dimension
    of the extracted mel features. This value is used in `ClvpConditioningEncoder`.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_size` (`int`, *可选*, 默认为 80) — 提取的 mel 特征的特征维度。此值在 `ClvpConditioningEncoder`
    中使用。'
- en: '`use_attention_bias` (`bool`, *optional*, defaults to `True`) — Whether to
    use bias in Query, Key and Value layers during self attention.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_attention_bias` (`bool`, *可选*, 默认为 `True`) — 在自注意力中是否使用 Query、Key 和 Value
    层的偏置。'
- en: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — A factor for
    initializing all weight matrices (should be kept to 1.0, used internally for initialization
    testing).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_factor` (`float`, *可选*, 默认为 1.0) — 用于初始化所有权重矩阵的因子（应保持为 1.0，用于内部初始化测试）。'
- en: '`decoder_fixing_codes` (`list`, *optional*, defaults to `[83, 45, 45, 248]`)
    — These values are used in the method `fix_speech_decoder_output` to fix decoder
    generated outputs.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_fixing_codes` (`list`, *可选*, 默认为 `[83, 45, 45, 248]`) — 这些值在方法 `fix_speech_decoder_output`
    中用于修正解码器生成的输出。'
- en: This is the configuration class to store the configuration of a [ClvpDecoder](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpDecoder).
    It is used to instantiate a CLVP Decoder Model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Decoder part of the CLVP [susnato/clvp_dev](https://huggingface.co/susnato/clvp_dev)
    architecture.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储 [ClvpDecoder](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpDecoder)
    的配置。它用于根据指定的参数实例化一个 CLVP 解码器模型，定义模型架构。使用默认值实例化配置将产生与 CLVP [susnato/clvp_dev](https://huggingface.co/susnato/clvp_dev)
    架构的解码器部分类似的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    的文档以获取更多信息。
- en: The architecture is similar to GPT2.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构类似于 GPT2。
- en: 'Example:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ClvpTokenizer
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ClvpTokenizer
- en: '### `class transformers.ClvpTokenizer`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ClvpTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/tokenization_clvp.py#L91)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/tokenization_clvp.py#L91)'
- en: '[PRE8]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇文件的路径。'
- en: '`merges_file` (`str`) — Path to the merges file.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file` (`str`) — 合并文件的路径。'
- en: '`errors` (`str`, *optional*, defaults to `"replace"`) — Paradigm to follow
    when decoding bytes to UTF-8\. See [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)
    for more information.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`errors` (`str`, *可选*, 默认为 `"replace"`) — 解码字节为 UTF-8 时要遵循的范例。更多信息请参考 [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)。'
- en: '`unk_token` (`str`, *optional*, defaults to `"[UNK]"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *可选*, 默认为 `"[UNK]"`) — 未知标记。词汇表中没有的标记无法转换为 ID，而是设置为此标记。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The beginning
    of sequence token.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--序列标记的开头。'
- en: '`eos_token` (`str`, *optional*, defaults to `"[STOP]"`) — The end of sequence
    token.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *可选*, 默认为 `"[STOP]"`) — 序列结束标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"[STOP]"`) — The pad token of
    the sequence.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *可选*, 默认为 `"[STOP]"`) — 序列的填充标记。'
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) — Whether or not
    to add an initial space to the input. This allows to treat the leading word just
    as any other word. (CLVP tokenizer detect beginning of words by the preceding
    space).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_prefix_space` (`bool`, *可选*, 默认为 `False`) — 是否在输入前添加一个初始空格。这允许将开头的单词视为任何其他单词。（CLVP
    分词器通过前导空格检测单词的开头）。'
- en: '`add_bos_token` (`bool`, *optional*, defaults to `False`) — Whether to add
    `bos_token` in front of the sequence when add_special_tokens=True.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_bos_token` (`bool`, *可选*, 默认为 `False`) — 当 `add_special_tokens=True` 时，是否在序列前添加
    `bos_token`。'
- en: '`add_eos_token` (`bool`, *optional*, defaults to `False`) — Whether to add
    `eos_token` in end of the sequence when add_special_tokens=True.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_eos_token` (`bool`, *可选*, 默认为 `False`) — 当 `add_special_tokens=True` 时，是否在序列末尾添加
    `eos_token`。'
- en: Construct a CLVP tokenizer. Based on byte-level Byte-Pair-Encoding.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 CLVP 分词器。基于字节级字节对编码。
- en: This tokenizer has been trained to treat spaces like parts of the tokens (a
    bit like sentencepiece) so a word will
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 该分词器已经训练成将空格视为标记的一部分（有点像 sentencepiece），因此一个单词将会在句子中的不同位置被编码成不同的标记。
- en: 'be encoded differently whether it is at the beginning of the sentence (without
    space) or not:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在句子开头（无空格）或不是时，将以不同方式编码：
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can get around that behavior by passing `add_prefix_space=True` when instantiating
    this tokenizer or when you call it on some text, but since the model was not pretrained
    this way, it might yield a decrease in performance.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在实例化此分词器时或在对某些文本调用时传递 `add_prefix_space=True`，可以避免这种行为，但由于模型不是以这种方式进行预训练的，可能会导致性能下降。
- en: When used with `is_split_into_words=True`, this tokenizer will add a space before
    each word (even the first one).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当与 `is_split_into_words=True` 一起使用时，此分词器将在每个单词之前添加一个空格（甚至是第一个单词）。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `save_vocabulary`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/tokenization_clvp.py#L352)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/tokenization_clvp.py#L352)'
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ClvpFeatureExtractor
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ClvpFeatureExtractor
- en: '### `class transformers.ClvpFeatureExtractor`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ClvpFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/feature_extraction_clvp.py#L33)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/feature_extraction_clvp.py#L33)'
- en: '[PRE11]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`feature_size` (`int`, *optional*, defaults to 80) — The feature dimension
    of the extracted features.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_size`（`int`，*可选*，默认为80）— 提取特征的特征维度。'
- en: '`sampling_rate` (`int`, *optional*, defaults to 22050) — The sampling rate
    at which the audio files should be digitalized expressed in hertz (Hz).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate`（`int`，*可选*，默认为22050）— 音频文件应数字化的采样率，以赫兹（Hz）表示。'
- en: '`default_audio_length` (`int`, *optional*, defaults to 6) — The default length
    of raw audio in seconds. If `max_length` is not set during `__call__` then it
    will automatically be set to default_audio_length * `self.sampling_rate`.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`default_audio_length`（`int`，*可选*，默认为6）— 原始音频的默认长度（以秒为单位）。如果在 `__call__` 中未设置
    `max_length`，则将自动设置为 default_audio_length * `self.sampling_rate`。'
- en: '`hop_length` (`int`, *optional*, defaults to 256) — Length of the overlaping
    windows for the STFT used to obtain the Mel Frequency coefficients.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hop_length`（`int`，*可选*，默认为256）— 用于获取梅尔频率系数的 STFT 中的重叠窗口的长度。'
- en: '`chunk_length` (`int`, *optional*, defaults to 30) — The maximum number of
    chuncks of `sampling_rate` samples used to trim and pad longer or shorter audio
    sequences.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk_length`（`int`，*可选*，默认为30）— 用于修剪和填充较长或较短音频序列的 `sampling_rate` 个样本块的最大数量。'
- en: '`n_fft` (`int`, *optional*, defaults to 1024) — Size of the Fourier transform.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_fft`（`int`，*可选*，默认为1024）— 傅立叶变换的大小。'
- en: '`padding_value` (`float`, *optional*, defaults to 0.0) — Padding value used
    to pad the audio. Should correspond to silences.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_value`（`float`，*可选*，默认为0.0）— 用于填充音频的填充值。应对应于静音。'
- en: '`mel_norms` (`list` of length `feature_size`, *optional*) — If `mel_norms`
    is provided then it will be used to normalize the log-mel spectrograms along each
    mel-filter.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mel_norms`（长度为 `feature_size` 的 `list`，*可选*）— 如果提供了 `mel_norms`，则将用于沿每个梅尔滤波器对数梅尔频谱进行归一化。'
- en: '`return_attention_mask` (`bool`, *optional*, defaults to `False`) — Whether
    to return the attention mask. If left to the default, it will return the attention
    mask.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask`（`bool`，*可选*，默认为 `False`）— 是否返回注意力掩码。如果保持默认设置，将返回注意力掩码。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: Constructs a CLVP feature extractor.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 CLVP 特征提取器。
- en: This feature extractor inherits from [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 此特征提取器继承自 [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: This class extracts log-mel-spectrogram features from raw speech using a custom
    numpy implementation of the `Short Time Fourier Transform` which should match
    pytorch’s `torch.stft` equivalent.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此类使用自定义 numpy 实现的 `Short Time Fourier Transform` 从原始语音中提取对数梅尔频谱特征，该实现应与 pytorch
    的 `torch.stft` 等效。
- en: '#### `__call__`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/feature_extraction_clvp.py#L131)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/feature_extraction_clvp.py#L131)'
- en: '[PRE12]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`raw_speech` (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`)
    — The sequence or batch of sequences to be padded. Each sequence can be a numpy
    array, a list of float values, a list of numpy arrays or a list of list of float
    values. Must be mono channel audio, not stereo, i.e. single float per timestep.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`raw_speech`（`np.ndarray`，`List[float]`，`List[np.ndarray]`，`List[List[float]]`）—
    要填充的序列或序列批次。每个序列可以是一个 numpy 数组，一个浮点值列表，一个 numpy 数组列表或一个浮点值列表的列表。必须是单声道音频，不是立体声，即每个时间步长一个浮点数。'
- en: '`sampling_rate` (`int`, *optional*) — The sampling rate at which the `raw_speech`
    input was sampled. It is strongly recommended to pass `sampling_rate` at the forward
    call to prevent silent errors and allow automatic speech recognition pipeline.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate`（`int`，*可选*）— `raw_speech` 输入的采样率。强烈建议在前向调用时传递 `sampling_rate`，以防止静默错误并允许自动语音识别流水线。'
- en: '`truncation` (`bool`, *optional*, default to `True`) — Activates truncation
    to cut input sequences longer than *max_length* to *max_length*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`（`bool`，*可选*，默认为 `True`）— 激活截断以将输入序列截断为比 *max_length* 更长的输入序列。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`（`int`，*可选*）— 如果设置，将填充序列到提供的值的倍数。'
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta), or on TPUs which benefit from having
    sequence lengths be a multiple of 128.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这对于在具有计算能力 `>= 7.5`（Volta）的 NVIDIA 硬件上启用 Tensor Cores 或在受益于序列长度为 128 的 TPU 上使用特别有用。
- en: '`return_attention_mask` (`bool`, *optional*, defaults to `True`) — Whether
    to return the attention mask. If left to the default, it will return the attention
    mask.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask` (`bool`，*可选*，默认为 `True`) — 是否返回注意力掩码。如果保持默认设置，将返回注意力掩码。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*)
    — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回 TensorFlow `tf.constant` 对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回 PyTorch `torch.Tensor` 对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回 Numpy `np.ndarray` 对象。'
- en: '`padding_value` (`float`, defaults to 0.0) — The value that is used to fill
    the padding values / vectors.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_value` (`float`，默认为 0.0) — 用于填充填充值/向量的值。'
- en: '`max_length` (`int`, *optional*) — The maximum input length of the inputs.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`，*可选*) — 输入的最大长度。'
- en: '`ClvpFeatureExtractor` is used to extract various voice specific properties
    such as the pitch and tone of the voice, speaking speed, and even speaking defects
    like a lisp or stuttering from a sample voice or `raw_speech`.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClvpFeatureExtractor` 用于从样本声音或 `raw_speech` 中提取各种声音特定属性，如声音的音高和音调、说话速度，甚至说话缺陷，如口吃或结巴。'
- en: First the voice is padded or truncated in a way such that it becomes a waveform
    of `self.default_audio_length` seconds long and then the log-mel spectrogram is
    extracted from it.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，声音被填充或截断，使其成为 `self.default_audio_length` 秒长的波形，然后从中提取对数梅尔频谱图。
- en: ClvpProcessor
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ClvpProcessor
- en: '### `class transformers.ClvpProcessor`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ClvpProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/processing_clvp.py#L24)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/processing_clvp.py#L24)'
- en: '[PRE13]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`feature_extractor` (`ClvpFeatureExtractor`) — An instance of [ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor).
    The feature extractor is a required input.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor` (`ClvpFeatureExtractor`) — [ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor)
    的一个实例。特征提取器是必需的输入。'
- en: '`tokenizer` (`ClvpTokenizer`) — An instance of [ClvpTokenizer](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpTokenizer).
    The tokenizer is a required input.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`ClvpTokenizer`) — [ClvpTokenizer](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpTokenizer)
    的一个实例。分词器是必需的输入。'
- en: Constructs a CLVP processor which wraps a CLVP Feature Extractor and a CLVP
    Tokenizer into a single processor.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 CLVP 处理器，将 CLVP 特征提取器和 CLVP 分词器封装成一个单一处理器。
- en: '[ClvpProcessor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpProcessor)
    offers all the functionalities of [ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor)
    and [ClvpTokenizer](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpTokenizer).
    See the [**call**()](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpProcessor.__call__),
    [decode()](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpProcessor.decode)
    and [batch_decode()](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpProcessor.batch_decode)
    for more information.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[ClvpProcessor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpProcessor)
    提供了 [ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor)
    和 [ClvpTokenizer](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpTokenizer)
    的所有功能。查看 [**call**()](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpProcessor.__call__)、[decode()](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpProcessor.decode)
    和 [batch_decode()](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpProcessor.batch_decode)
    获取更多信息。'
- en: '#### `__call__`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/processing_clvp.py#L49)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/processing_clvp.py#L49)'
- en: '[PRE14]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Forwards the `audio` and `sampling_rate` arguments to [**call**()](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor.__call__)
    and the `text` argument to [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__).
    Please refer to the doctsring of the above two methods for more information.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `audio` 和 `sampling_rate` 参数转发到 [**call**()](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor.__call__)，将
    `text` 参数转发到 [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。有关更多信息，请参阅上述两种方法的文档字符串。
- en: '#### `decode`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/processing_clvp.py#L86)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/processing_clvp.py#L86)'
- en: '[PRE15]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This method forwards all its arguments to ClvpTokenizer’s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将其所有参数转发给 ClvpTokenizer 的 [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。有关更多信息，请参阅此方法的文档字符串。
- en: '#### `batch_decode`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/processing_clvp.py#L78)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/processing_clvp.py#L78)'
- en: '[PRE16]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This method forwards all its arguments to ClvpTokenizer’s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将其所有参数转发给 ClvpTokenizer 的 [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。有关更多信息，请参阅此方法的文档字符串。
- en: ClvpModelForConditionalGeneration
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ClvpModelForConditionalGeneration
- en: '### `class transformers.ClvpModelForConditionalGeneration`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ClvpModelForConditionalGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1509)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1509)'
- en: '[PRE17]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The composite CLVP model with a text encoder, speech encoder and speech decoder
    model.The speech decoder model generates the speech_ids from the text and the
    text encoder and speech encoder workstogether to filter out the best speech_ids.
    This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 具有文本编码器、语音编码器和语音解码器模型的复合CLVP模型。语音解码器模型从文本生成语音ID，文本编码器和语音编码器一起工作以过滤出最佳的语音ID。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1737)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1737)'
- en: '[PRE18]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of input sequence tokens in the vocabulary. Padding will be ignored
    by default should you provide it.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 词汇表中输入序列标记的索引。默认情况下将忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, feature_size,
    time_dim)`) — Indicates log mel-spectrogram representations for audio returned
    by [ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor).'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features`（形状为`(batch_size, feature_size, time_dim)`的`torch.FloatTensor`）—
    表示音频返回的log mel-spectrogram表示，由[ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor)返回。'
- en: '`conditioning_encoder_inputs_embeds` (`torch.FloatTensor`, *optional*) — inputs_embeds
    for `ClvpConditioningEncoder`. Can be used in place of `input_ids`.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conditioning_encoder_inputs_embeds`（`torch.FloatTensor`，*可选*）— 用于`ClvpConditioningEncoder`的inputs_embeds。可用于替代`input_ids`。'
- en: '`text_encoder_inputs_embeds` (`torch.FloatTensor`, *optional*) — inputs_embeds
    for the text encoder model passed in place of `input_ids`.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_inputs_embeds`（`torch.FloatTensor`，*可选*）— 用于文本编码器模型的inputs_embeds，代替`input_ids`。'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding text token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 避免对填充文本标记索引执行注意力的掩码。选择的掩码值在`[0,
    1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被“掩码”的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“掩码”的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`return_loss` (`bool`, *optional*) — Whether or not to return the contrastive
    loss.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_loss`（`bool`，*可选*）— 是否返回对比损失。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.clvp.modeling_clvp.ClvpOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.clvp.modeling_clvp.ClvpOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.clvp.modeling_clvp.ClvpOutput` or a tuple of `torch.FloatTensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration (`<class 'transformers.models.clvp.configuration_clvp.ClvpConfig'>`)
    and inputs.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.clvp.modeling_clvp.ClvpOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包括根据配置(`<class
    'transformers.models.clvp.configuration_clvp.ClvpConfig'>`)和输入不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss`
    is `True`) — Contrastive loss for speech-text similarity.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当`return_loss`为`True`时返回) — 语音-文本相似性的对比损失。'
- en: '`speech_ids` (`torch.LongTensor`, *optional*) — speech_ids (or speech candidates)
    generated by the `ClvpForCausalLM` model.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speech_ids` (`torch.LongTensor`，*可选*) — 由`ClvpForCausalLM`模型生成的语音id（或语音候选）。'
- en: '`logits_per_speech` (`torch.FloatTensor` of shape `(speech_batch_size, text_batch_size)`)
    — The scaled dot product scores between `speech_embeds` and `text_embeds`. This
    represents the speech-text similarity scores.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_per_speech` (`torch.FloatTensor`，形状为`(speech_batch_size, text_batch_size)`)
    — `speech_embeds`和`text_embeds`之间的缩放点积分数。这代表了语音-文本相似性分数。'
- en: '`logits_per_text` (`torch.FloatTensor` of shape `(text_batch_size, speech_batch_size)`)
    — The scaled dot product scores between `text_embeds` and `speech_embeds`. This
    represents the text-speech similarity scores.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_per_text` (`torch.FloatTensor`，形状为`(text_batch_size, speech_batch_size)`)
    — `text_embeds`和`speech_embeds`之间的缩放点积分数。这代表了文本-语音相似性分数。'
- en: '`text_embeds` (`torch.FloatTensor` of shape `(batch_size, output_dim`) — The
    text embeddings obtained by applying the projection layer to the pooled output
    of the text encoder model.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_embeds` (`torch.FloatTensor`，形状为`(batch_size, output_dim`) — 通过将文本编码器模型的汇总输出应用到投影层获得的文本嵌入。'
- en: '`speech_embeds` (`torch.FloatTensor` of shape `(batch_size, output_dim`) —
    The speech embeddings obtained by applying the projection layer to the pooled
    output of the speech encoder model.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speech_embeds` (`torch.FloatTensor`，形状为`(batch_size, output_dim`) — 通过将语音编码器模型的汇总输出应用到投影层获得的语音嵌入。'
- en: '`text_model_output` (`BaseModelOutputWithPooling`) — The pooled output of the
    `last_hidden_state` of the text encoder Model.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_model_output` (`BaseModelOutputWithPooling`) — 文本编码器模型的`last_hidden_state`的汇总输出。'
- en: '`speech_model_output` (`BaseModelOutputWithPooling`) — The pooled output of
    the `last_hidden_state` of the speech encoder Model.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speech_model_output` (`BaseModelOutputWithPooling`) — 语音编码器模型的`last_hidden_state`的汇总输出。'
- en: '`decoder_hidden_states` (`torch.FloatTensor`, *optional*) — The hidden states
    of the decoder model.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`torch.FloatTensor`, *可选*) — 解码器模型的隐藏状态。'
- en: '`text_encoder_hidden_states` (`torch.FloatTensor`, *optional*) — The hidden
    states of the text encoder model.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_hidden_states` (`torch.FloatTensor`, *可选*) — 文本编码器模型的隐藏状态。'
- en: '`speech_encoder_hidden_states` (`torch.FloatTensor`, *optional*) — The hidden
    states of the speech encoder model.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speech_encoder_hidden_states` (`torch.FloatTensor`, *可选*) — 语音编码器模型的隐藏状态。'
- en: The [ClvpModelForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpModelForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[ClvpModelForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpModelForConditionalGeneration)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE19]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#### `generate`'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1869)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1869)'
- en: '[PRE20]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Input text Tokens. Processed from the [ClvpTokenizer](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpTokenizer).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*可选*) —
    输入文本标记。从[ClvpTokenizer](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpTokenizer)处理而来。'
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, feature_size,
    time_dim)`, *optional*) — Indicates log-melspectrogram representations for audio
    returned by [ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features` (`torch.FloatTensor`，形状为`(batch_size, feature_size, time_dim)`，*可选*)
    — 表示音频的log-melspectrogram表示，由[ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor)返回。'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding text token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) —
    用于避免在填充文本标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记为1。
- en: 0 for tokens that are `masked`.
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_config` (`~generation.GenerationConfig`, *optional*) — 用作生成调用的基本参数化的生成配置。传递给generate的`**kwargs`匹配`generation_config`的属性将覆盖它们。如果未提供`generation_config`，将使用默认值，其加载优先级如下：1）来自`generation_config.json`模型文件，如果存在；2）来自模型配置。请注意，未指定的参数将继承[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)的默认值，其文档应该被检查以参数化生成。'
- en: '`pad_to_max_mel_tokens` (`int`, *optional*) — Pads generated speech_ids to
    the specified value. This is to implement the same logic from the official repo,
    link: [https://github.com/neonbjb/tortoise-tts/blob/80f89987a5abda5e2b082618cd74f9c7411141dc/tortoise/api.py#L430](https://github.com/neonbjb/tortoise-tts/blob/80f89987a5abda5e2b082618cd74f9c7411141dc/tortoise/api.py#L430)
    and to make sure the logits are same. This does not affect generation quality
    so please don’t consider using it since it is less efficient.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_max_mel_tokens` (`int`, *optional*) — 将生成的speech_ids填充到指定值。这是为了实现与官方repo相同的逻辑，链接：[https://github.com/neonbjb/tortoise-tts/blob/80f89987a5abda5e2b082618cd74f9c7411141dc/tortoise/api.py#L430](https://github.com/neonbjb/tortoise-tts/blob/80f89987a5abda5e2b082618cd74f9c7411141dc/tortoise/api.py#L430)
    并确保对数相同。这不会影响生成质量，因此请不要考虑使用它，因为效率较低。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of decoder model, text encoder and speech encoder models.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回解码器模型、文本编码器和语音编码器模型的隐藏状态。'
- en: Returns
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`ClvpOutput` or tuple'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClvpOutput` 或 元组'
- en: A `ClvpOutput` (if `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`)
    or a tuple.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`ClvpOutput`（如果`return_dict_in_generate=True`或当`config.return_dict_in_generate=True`时）或一个元组。
- en: Generate method for `ClvpModelForConditionalGeneration`, this method calls the
    `generate` method of `ClvpForCausalLM` and then uses those generated `speech_ids`
    to process `text_embeds` and `speech_embeds` using `ClvpEncoder`.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClvpModelForConditionalGeneration`的生成方法，此方法调用`ClvpForCausalLM`的`generate`方法，然后使用生成的`speech_ids`来处理`text_embeds`和`speech_embeds`，使用`ClvpEncoder`。'
- en: '#### `get_text_features`'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_text_features`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1583)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1583)'
- en: '[PRE21]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    词汇表中输入序列标记的索引。默认情况下将忽略填充。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`text_encoder_inputs_embeds` (`torch.FloatTensor`, *optional*) — inputs_embeds
    for the text encoder model passed in place of `input_ids`.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder_inputs_embeds` (`torch.FloatTensor`, *optional*) — 用于文本编码器模型的inputs_embeds，代替`input_ids`。'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`未被掩码`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`被掩码`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: Returns
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, output_dim)`'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor` of shape `(batch_size, output_dim)`'
- en: The text embeddings obtained by applying the projection layer to the pooled
    output of the CLVP Text Model.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将投影层应用于CLVP文本模型的池化输出获得的文本嵌入。
- en: This method can be used to extract text_embeds from a text. The text embeddings
    obtained by applying the projection layer to the pooled output of the CLVP text
    encoder model.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法可用于从文本中提取text_embeds。通过将投影层应用于CLVP文本编码器模型的池化输出获得的文本嵌入。
- en: 'Examples:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE22]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '#### `get_speech_features`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_speech_features`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1640)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1640)'
- en: '[PRE23]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`speech_ids` (`torch.LongTensor` of shape `(batch_size, num_speech_ids)`, *optional*)
    — Speech Tokens. Padding will be ignored by default should you provide it. If
    speech_ids are provided then input_ids and input_features will be automatically
    ignored.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`speech_ids` (`torch.LongTensor` of shape `(batch_size, num_speech_ids)`, *optional*)
    — 语音标记。默认情况下将忽略填充。如果提供了speech_ids，则将自动忽略input_ids和input_features。'
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Input text Tokens. Processed from the [ClvpTokenizer](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpTokenizer).
    If speech_ids is not provided, then input_ids and input_features will be used.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — 输入文本标记。从[ClvpTokenizer](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpTokenizer)处理。如果未提供speech_ids，则将使用input_ids和input_features。'
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, feature_size,
    time_dim)`, *optional*) — Indicates log-melspectrogram representations for audio
    returned by [ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor).
    If speech_ids is not provided, then input_ids and input_features will be used.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features` (`torch.FloatTensor` of shape `(batch_size, feature_size,
    time_dim)`, *optional*) — 指示音频的log-melspectrogram表示，由[ClvpFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpFeatureExtractor)返回。如果未提供speech_ids，则将使用input_ids和input_features。'
- en: '`conditioning_encoder_inputs_embeds` (`torch.FloatTensor`, *optional*) — inputs_embeds
    for `ClvpConditioningEncoder`. Can be used in place of `input_ids`.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conditioning_encoder_inputs_embeds`（`torch.FloatTensor`，*可选*）- 用于`ClvpConditioningEncoder`的inputs_embeds。可以代替`input_ids`使用。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding speech token indices.
    Mask values selected in `[0, 1]`:'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    用于避免在填充语音标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`generation_config` (`GenerationConfig`, *optional*) — generation config to
    control the generation of speech_ids if they are not provided.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_config`（`GenerationConfig`，*可选*）- 用于控制生成语音ID的生成配置，如果它们未提供。'
- en: Returns
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor` of shape `(batch_size, output_dim)`'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`的形状为`(batch_size, output_dim)`'
- en: The speech embeddings obtained by applying the projection layer to the pooled
    output of the CLVP Speech Model.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将投影层应用于CLVP语音模型的汇聚输出获得的语音嵌入。
- en: This method can be used to extract speech_embeds. The speech embeddings are
    obtained by applying the speech model on speech_ids. If speech_ids is not present
    but both input_ids and input_features are given then the decoder model will be
    used to first generate the speech_ids and then applying the speech model.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法可用于提取语音嵌入。通过将语音模型应用于语音ID获得语音嵌入。如果不存在语音ID，但提供了input_ids和input_features，则解码器模型将首先用于生成语音ID，然后应用语音模型。
- en: 'Examples:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE24]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ClvpForCausalLM
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ClvpForCausalLM
- en: '### `class transformers.ClvpForCausalLM`'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ClvpForCausalLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1280)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1280)'
- en: '[PRE25]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The CLVP decoder model with a language modelling head on top. This model inherits
    from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 具有语言建模头部的CLVP解码器模型。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1421)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1421)'
- en: '[PRE26]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`)
    — Contains precomputed hidden-states (key and values in the attention blocks)
    as computed by the model (see `past_key_values` output below). Can be used to
    speed up sequential decoding. The `input_ids` which have their past given to this
    model should not be passed as `input_ids` as they have already been computed.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（长度为`config.n_layers`的`Tuple[Tuple[torch.Tensor]]`）- 包含由模型计算的预计算隐藏状态（注意力块中的键和值），如下面的`past_key_values`输出所示。可用于加速顺序解码。将其过去传递给此模型的`input_ids`不应作为`input_ids`传递，因为它们已经计算过。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。
- en: 'If `past_key_values` is used, `attention_mask` needs to contain the masking
    strategy that was used for `past_key_values`. In other words, the `attention_mask`
    always has to have the length: `len(past_key_values) + len(input_ids)`'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，则`attention_mask`需要包含用于`past_key_values`的掩码策略。换句话说，`attention_mask`的长度始终必须为：`len(past_key_values)
    + len(input_ids)`
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor`，形状为`(batch_size, input_ids_length)`，*optional*)
    — 指示输入的第一部分和第二部分的段标记索引。索引选择在`[0, 1]`中：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*)
    — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*)
    — 用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部是`not masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部是`masked`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*)
    — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵。'
- en: If `past_key_values` is used, optionally only the last `inputs_embeds` have
    to be input (see `past_key_values`).
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用`past_key_values`，则可选择仅输入最后的`inputs_embeds`（参见`past_key_values`）。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for language modeling. Note that the labels **are shifted** inside the
    model, i.e. you can set `labels = input_ids` Indices are selected in `[-100, 0,
    ..., config.vocab_size]` All labels set to `-100` are ignored (masked), the loss
    is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*)
    — 用于语言建模的标签。请注意，模型内部的标签**已移位**，即您可以设置`labels = input_ids`。索引在`[-100, 0, ..., config.vocab_size]`中选择。所有设置为`-100`的标签都被忽略（掩码），损失仅计算标签在`[0,
    ..., config.vocab_size]`中的标签'
- en: The [ClvpForCausalLM](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpForCausalLM)
    forward method, overrides the `__call__` special method.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '[ClvpForCausalLM](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpForCausalLM)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是前者，因为前者负责运行前处理和后处理步骤，而后者会默默地忽略它们。
- en: ClvpModel
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ClvpModel
- en: '### `class transformers.ClvpModel`'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ClvpModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1209)'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1209)'
- en: '[PRE27]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([ClvpConfig](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpConfig))
    — 模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare Clvp decoder model outputting raw hidden-states without any specific
    head on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 裸Clvp解码器模型输出原始隐藏状态，没有特定的头部。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以了解库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1231)'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1231)'
- en: '[PRE28]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`)
    — Contains precomputed hidden-states (key and values in the attention blocks)
    as computed by the model (see `past_key_values` output below). Can be used to
    speed up sequential decoding. The `input_ids` which have their past given to this
    model should not be passed as `input_ids` as they have already been computed.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（长度为`config.n_layers`的`Tuple[Tuple[torch.Tensor]]`）- 包含由模型计算的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码。将其过去的`input_ids`不应作为`input_ids`传递给此模型，因为它们已经计算过。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被`屏蔽`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被`屏蔽`的标记。
- en: 'If `past_key_values` is used, `attention_mask` needs to contain the masking
    strategy that was used for `past_key_values`. In other words, the `attention_mask`
    always has to have the length: `len(past_key_values) + len(input_ids)`'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，则`attention_mask`需要包含用于`past_key_values`的掩码策略。换句话说，`attention_mask`的长度始终为：`len(past_key_values)
    + len(input_ids)`
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`，*可选*）-
    段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`中：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*的标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*的标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    每个输入序列标记的位置嵌入的索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）-
    用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部是`未屏蔽`，
- en: 0 indicates the head is `masked`.
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部是`屏蔽`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    可选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: If `past_key_values` is used, optionally only the last `inputs_embeds` have
    to be input (see `past_key_values`).
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，则可选择仅输入最后的`inputs_embeds`（参见`past_key_values`）。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*）- 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: The [ClvpModel](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpModel)
    forward method, overrides the `__call__` special method.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '[ClvpModel](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: ClvpEncoder
  id: totrans-373
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Clvp编码器
- en: '### `class transformers.ClvpEncoder`'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '### `类transformers.Clvp编码器`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L874)'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L874)'
- en: '[PRE29]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Transformer encoder consisting of `config.num_hidden_layers` self attention
    layers. Each layer is a `ClvpEncoderLayer`.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 由`config.num_hidden_layers`个自注意层组成的Transformer编码器。每一层都是一个`ClvpEncoderLayer`。
- en: '#### `forward`'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L906)'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L906)'
- en: '[PRE30]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`,
    *optional*) — Indices of input sequence tokens in the vocabulary.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`，*可选*）— 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — input embeddings for the model. This bypasses the
    model’s internal embedding lookup matrix.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    模型的输入嵌入。这将绕过模型的内部嵌入查找矩阵。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-387
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`未屏蔽`的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`已屏蔽`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`position_ids` (`torch.LongTensor`, *optional*) — Denotes the position ids
    of `input_ids`.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（`torch.LongTensor`，*可选*）— 表示`input_ids`的位置id。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: ClvpDecoder
  id: totrans-394
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Clvp解码器
- en: '### `class transformers.ClvpDecoder`'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '### `类transformers.Clvp解码器`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1030)'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1030)'
- en: '[PRE31]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer
    is a `ClvpDecoderLayer`
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 由*config.num_hidden_layers*层组成的Transformer解码器。每一层都是一个`ClvpDecoderLayer`
- en: '#### `forward`'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1065)'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clvp/modeling_clvp.py#L1065)'
- en: '[PRE32]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`)
    — Contains precomputed hidden-states (key and values in the attention blocks)
    as computed by the model (see `past_key_values` output below). Can be used to
    speed up sequential decoding. The `input_ids` which have their past given to this
    model should not be passed as `input_ids` as they have already been computed.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（长度为`config.n_layers`的`Tuple[Tuple[torch.Tensor]]`）— 包含模型计算的预计算隐藏状态（注意力块中的键和值），如下面的`past_key_values`输出所示。可用于加速顺序解码。已经计算过其过去的`input_ids`的`input_ids`不应作为`input_ids`传递，因为它们已经被计算过。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）-
    用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被`掩码`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-409
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被`掩码`的标记。
- en: 'If `past_key_values` is used, `attention_mask` needs to contain the masking
    strategy that was used for `past_key_values`. In other words, the `attention_mask`
    always has to have the length: `len(past_key_values) + len(input_ids)`'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，则`attention_mask`需要包含用于`past_key_values`的掩码策略。换句话说，`attention_mask`的长度始终必须为：`len(past_key_values)
    + len(input_ids)`
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`，*可选*）-
    段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`中：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）-
    每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）-
    用于使自注意力模块中选择的头部失效的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 indicates the head is `not masked`,
  id: totrans-419
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被`掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`掩码`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。'
- en: If `past_key_values` is used, optionally only the last `inputs_embeds` have
    to be input (see `past_key_values`).
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，则可选择仅输入最后的`inputs_embeds`（参见`past_key_values`）。
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*）- 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: The [ClvpDecoder](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpDecoder)
    forward method, overrides the `__call__` special method.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '[ClvpDecoder](/docs/transformers/v4.37.2/en/model_doc/clvp#transformers.ClvpDecoder)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
