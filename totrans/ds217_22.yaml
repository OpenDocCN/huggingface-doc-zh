- en: Use with Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸Sparkä¸€èµ·ä½¿ç”¨
- en: 'Original text: [https://huggingface.co/docs/datasets/use_with_spark](https://huggingface.co/docs/datasets/use_with_spark)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/datasets/use_with_spark](https://huggingface.co/docs/datasets/use_with_spark)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This document is a quick introduction to using ğŸ¤— Datasets with Spark, with a
    particular focus on how to load a Spark DataFrame into a [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    object.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æ˜¯å…³äºå¦‚ä½•å°†Spark DataFrameåŠ è½½åˆ°[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)å¯¹è±¡ä¸­çš„ğŸ¤—
    Datasetsçš„å¿«é€Ÿä»‹ç»ã€‚
- en: From there, you have fast access to any element and you can use it as a data
    loader to train models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é‚£é‡Œï¼Œæ‚¨å¯ä»¥å¿«é€Ÿè®¿é—®ä»»ä½•å…ƒç´ ï¼Œå¹¶å°†å…¶ç”¨ä½œæ•°æ®åŠ è½½å™¨æ¥è®­ç»ƒæ¨¡å‹ã€‚
- en: Load from Spark
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»SparkåŠ è½½
- en: A [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    object is a wrapper of an Arrow table, which allows fast reads from arrays in
    the dataset to PyTorch, TensorFlow and JAX tensors. The Arrow table is memory
    mapped from disk, which can load datasets bigger than your available RAM.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)å¯¹è±¡æ˜¯Arrowè¡¨çš„åŒ…è£…å™¨ï¼Œå…è®¸ä»æ•°æ®é›†ä¸­çš„æ•°ç»„å¿«é€Ÿè¯»å–åˆ°PyTorchã€TensorFlowå’ŒJAXå¼ é‡ã€‚Arrowè¡¨æ˜¯ä»ç£ç›˜å†…å­˜æ˜ å°„çš„ï¼Œå¯ä»¥åŠ è½½æ¯”æ‚¨å¯ç”¨RAMæ›´å¤§çš„æ•°æ®é›†ã€‚'
- en: 'You can get a [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    from a Spark DataFrame using `Dataset.from_spark()`:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨`Dataset.from_spark()`ä»Spark DataFrameè·å–[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)ï¼š
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The Spark workers write the dataset on disk in a cache directory as Arrow files,
    and the [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    is loaded from there.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Sparkå·¥ä½œèŠ‚ç‚¹å°†æ•°æ®é›†å†™å…¥ç¼“å­˜ç›®å½•ä¸­çš„Arrowæ–‡ä»¶ä¸­ï¼Œç„¶åä»é‚£é‡ŒåŠ è½½[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)ã€‚
- en: 'Alternatively, you can skip materialization by using `IterableDataset.from_spark()`,
    which returns an [IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨`IterableDataset.from_spark()`æ¥è·³è¿‡å®ä½“åŒ–ï¼Œå®ƒè¿”å›ä¸€ä¸ª[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)ï¼š
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Caching
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¼“å­˜
- en: When using `Dataset.from_spark()`, the resulting [Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)
    is cached; if you call `Dataset.from_spark()` multiple times on the same DataFrame
    it wonâ€™t re-run the Spark job that writes the dataset as Arrow files on disk.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½¿ç”¨`Dataset.from_spark()`æ—¶ï¼Œç”Ÿæˆçš„[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)ä¼šè¢«ç¼“å­˜ï¼›å¦‚æœåœ¨ç›¸åŒçš„DataFrameä¸Šå¤šæ¬¡è°ƒç”¨`Dataset.from_spark()`ï¼Œå®ƒä¸ä¼šé‡æ–°è¿è¡Œå†™å…¥Arrowæ–‡ä»¶çš„Sparkä½œä¸šã€‚
- en: You can set the cache location by passing `cache_dir=` to `Dataset.from_spark()`.
    Make sure to use a disk that is available to both your workers and your current
    machine (the driver).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡å°†`cache_dir=`ä¼ é€’ç»™`Dataset.from_spark()`æ¥è®¾ç½®ç¼“å­˜ä½ç½®ã€‚ç¡®ä¿ä½¿ç”¨ä¸€ä¸ªå¯¹æ‚¨çš„å·¥ä½œèŠ‚ç‚¹å’Œå½“å‰æœºå™¨ï¼ˆé©±åŠ¨ç¨‹åºï¼‰éƒ½å¯ç”¨çš„ç£ç›˜ã€‚
- en: In a different session, a Spark DataFrame doesnâ€™t have the same [semantic hash](https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.DataFrame.semanticHash.html),
    and it will rerun a Spark job and store it in a new cache.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¦ä¸€ä¸ªä¼šè¯ä¸­ï¼ŒSpark DataFrameæ²¡æœ‰ç›¸åŒçš„[è¯­ä¹‰å“ˆå¸Œ](https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.DataFrame.semanticHash.html)ï¼Œå®ƒå°†é‡æ–°è¿è¡Œä¸€ä¸ªSparkä½œä¸šå¹¶å°†å…¶å­˜å‚¨åœ¨æ–°çš„ç¼“å­˜ä¸­ã€‚
- en: Feature types
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç‰¹å¾ç±»å‹
- en: 'If your dataset is made of images, audio data or N-dimensional arrays, you
    can specify the `features=` argument in `Dataset.from_spark()` (or `IterableDataset.from_spark()`):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„æ•°æ®é›†ç”±å›¾åƒã€éŸ³é¢‘æ•°æ®æˆ–Nç»´æ•°ç»„ç»„æˆï¼Œæ‚¨å¯ä»¥åœ¨`Dataset.from_spark()`ï¼ˆæˆ–`IterableDataset.from_spark()`ï¼‰ä¸­æŒ‡å®š`features=`å‚æ•°ï¼š
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can check the [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)
    documentation to know about all the feature types available.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥æŸ¥çœ‹[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)æ–‡æ¡£ï¼Œäº†è§£æ‰€æœ‰å¯ç”¨çš„ç‰¹å¾ç±»å‹ã€‚
