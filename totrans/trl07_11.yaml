- en: Trainer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/trl/trainer](https://huggingface.co/docs/trl/trainer)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: At TRL we support PPO (Proximal Policy Optimisation) with an implementation
    that largely follows the structure introduced in the paper “Fine-Tuning Language
    Models from Human Preferences” by D. Ziegler et al. [[paper](https://arxiv.org/pdf/1909.08593.pdf),
    [code](https://github.com/openai/lm-human-preferences)]. The Trainer and model
    classes are largely inspired from `transformers.Trainer` and `transformers.AutoModel`
    classes and adapted for RL. We also support a `RewardTrainer` that can be used
    to train a reward model.
  prefs: []
  type: TYPE_NORMAL
- en: PPOConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class trl.PPOConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_config.py#L34)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Configuration class for PPOTrainer
  prefs: []
  type: TYPE_NORMAL
- en: PPOTrainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class trl.PPOTrainer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L109)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '*`*config**` (`PPOConfig`) — Configuration object for PPOTrainer. Check the
    documentation of `PPOConfig` for more — details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*model**` (`PreTrainedModelWrapper`) — Model to be optimized, Hugging Face
    transformer model with a value head. — Check the documentation of `PreTrainedModelWrapper`
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*ref_model**` (`PreTrainedModelWrapper`, *optional*) — Reference model to
    be used for KL penalty, Hugging Face — transformer model with a casual language
    modelling head. Check the documentation of `PreTrainedModelWrapper` for more details.
    If no reference model is provided, the trainer will create a reference model with
    the same architecture as the model to be optimized with shared layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*tokenizer**` (`PreTrainedTokenizerBase`) — Tokenizer to be used for encoding
    the — data. Check the documentation of `transformers.PreTrainedTokenizer` and
    `transformers.PreTrainedTokenizerFast` for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*dataset**` (Union[`torch.utils.data.Dataset`, `datasets.Dataset`], *optional*)
    — PyTorch dataset or Hugging — Face dataset. This is used to create a PyTorch
    dataloader. If no dataset is provided, the dataloader must be created outside
    the trainer users needs to design their own dataloader and make sure the batch
    size that is used is the same as the one specified in the configuration object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*optimizer**` (`torch.optim.Optimizer`, *optional*) — Optimizer to be used
    for training. If no optimizer is — provided, the trainer will create an Adam optimizer
    with the learning rate specified in the configuration object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*data_collator**` (DataCollatorForLanguageModeling, *optional*) — Data collator
    to be used for training and — passed along the dataloader'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*num_shared_layers**` (int, *optional*) — Number of layers to be shared between
    the model and the reference — model, if no reference model is passed. If no number
    is provided, all the layers will be shared.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*lr_scheduler**` (`torch.optim.lr_scheduler`, *optional*) — Learning rate
    scheduler to be used for training. —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The PPOTrainer uses Proximal Policy Optimization to optimise language models.
    Note, this trainer is heavily inspired by the original OpenAI learning to summarize
    work here: [https://github.com/openai/summarize-from-feedback](https://github.com/openai/summarize-from-feedback)'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `batched_forward_pass`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L941)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`queries` (`torch.LongTensor`) — List of tensors containing the encoded queries,
    shape (`batch_size`, `query_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`responses` (`torch.LongTensor`) — List of tensors containing the encoded responses,
    shape (`batch_size`, `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_logits` (`bool`, *optional*, defaults to `False`) — Whether to return
    all_logits. Set to `False` if logits are not needed to reduce memory consumption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: (tuple)
  prefs: []
  type: TYPE_NORMAL
- en: 'all_logprobs (`torch.FloatTensor`): Log probabilities of the responses, shape
    (`batch_size`, `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'all_ref_logprobs (`torch.FloatTensor`): Log probabilities of the responses,
    shape (`batch_size`, `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'all_values (`torch.FloatTensor`): Values of the responses, shape (`batch_size`,
    `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate model outputs in multiple batches.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute_rewards`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1078)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor`) — Scores from the reward model, shape (`batch_size`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logprobs` (`torch.FloatTensor`) — Log probabilities of the model, shape (`batch_size`,
    `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ref_logprobs` (`torch.FloatTensor`) — Log probabilities of the reference model,
    shape (`batch_size`, `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Per token rewards, shape (`batch_size`, `response_length`) `torch.FloatTensor`:
    Non score rewards, shape (`batch_size`, `response_length`) `torch.FloatTensor`:
    KL penalty, shape (`batch_size`, `response_length`)'
  prefs: []
  type: TYPE_NORMAL
- en: Compute per token rewards from scores and KL-penalty.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `create_model_card`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1386)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path` (`str`) — The path to save the model card to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_name` (`str`, *optional*) — The name of the model, defaults to `TRL
    Model`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates and saves a model card for a TRL model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `gather_stats`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L897)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`stats` (dict[str, Any]) —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`a` dictionary of stats to be gathered. The stats should contain torch tensors.
    —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`dict[str, Any]`'
  prefs: []
  type: TYPE_NORMAL
- en: A dictionary of stats with the tensors gathered.
  prefs: []
  type: TYPE_NORMAL
- en: Gather stats from all processes. Useful in the context of distributed training.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L431)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`query_tensor` (`torch.LongTensor`) — A tensor of shape (`seq_len`) containing
    query tokens or a list of tensors of shape (`seq_len`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length_sampler` (`Callable`, *optional*) — Callable that returns the number
    of newly generated tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, *optional) — Batch size used for generation, defaults
    to `4`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_prompt` (`bool`, *optional*) — If set to `False` the prompt is not
    returned but only the newly generated tokens, defaults to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generate_ref_response` (`bool`, *optional*) — If set to `True` the reference
    response is also generated, defaults to `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generation_kwargs` (dict[str, Any]) — Keyword arguments for generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.LongTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: A tensor of shape (`batch_size`, `gen_len`) containing response tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Generate response with the model given the query tensor. call the `generate`
    method of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `log_stats`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1313)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`stats` (dict[str, Any]) — A dictionary of training stats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch` (dict[str, Any]) — A dictionary of batch data, this contains the queries
    and responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rewards` (`List[torch.FloatTensor]`) — A tensor of rewards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function that logs all the training stats. Call it at the end of each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `loss`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1160)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`old_logprobs` (`torch.FloatTensor`) — Log probabilities of the model, shape
    (`batch_size`, `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`values` (`torch.FloatTensor`) — Values of the value head, shape (`batch_size`,
    `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rewards` (`torch.FloatTensor`) — Rewards from the reward model, shape (`batch_size`,
    `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor`) — Logits of the model, shape (`batch_size`,
    `response_length`, `vocab_size`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`v_pred` (`torch.FloatTensor`) — Values of the value head, shape (`batch_size`,
    `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logprobs` (`torch.FloatTensor`) — Log probabilities of the model, shape (`batch_size`,
    `response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate policy and value losses.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `prepare_dataloader`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L376)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset` (Union[`torch.utils.data.Dataset`, `datasets.Dataset`]) — PyTorch
    dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset
    will be preprocessed by removing the columns that are not used by the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_collator` (Optional[function]) — Data collator function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.utils.data.DataLoader`'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch dataloader
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the dataloader for training.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `record_step_stats`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1249)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`kl_coef` (`float`) — KL coefficient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data` (`dict`) — Dictionary of training step data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: stats (`dict`)
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary of training step statistics
  prefs: []
  type: TYPE_NORMAL
- en: Record training step statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L617)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`queries` (List`torch.LongTensor`) — List of tensors containing the encoded
    queries of shape (`query_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`responses` (List`torch.LongTensor`) — List of tensors containing the encoded
    responses of shape (`response_length`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (List`torch.FloatTensor`) — List of tensors containing the scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`response_masks` (List`torch.FloatTensor`, *optional*)) — List of tensors containing
    masks of the response tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`dict[str, Any]`'
  prefs: []
  type: TYPE_NORMAL
- en: A summary of the training statistics
  prefs: []
  type: TYPE_NORMAL
- en: Run a PPO optimisation step given a list of queries, model responses, and rewards.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `train_minibatch`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ppo_trainer.py#L1032)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`logprobs` (`torch.FloatTensor`) — Log probabilities of the model, shape [mini_batch_size,
    response_length]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`values` (`torch.FloatTensor`) — Values of the value head, shape [mini_batch_size,
    response_length]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query` (`torch.LongTensor`) — Encoded queries, shape [mini_batch_size, query_length]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`response` (`torch.LongTensor`) — Encoded responses, shape [mini_batch_size,
    response_length]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_input` (`torch.LongTensor`) — Concatenated queries and responses, shape
    [mini_batch_size, query_length+response_length]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: train_stats (dict[str, `torch.Tensor`])
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary of training statistics
  prefs: []
  type: TYPE_NORMAL
- en: Train one PPO minibatch
  prefs: []
  type: TYPE_NORMAL
- en: RewardConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class trl.RewardConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/reward_config.py#L21)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*, defaults to `None`) — The maximum length of
    the sequences in the batch. This argument is required if you want to use the default
    data collator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gradient_checkpointing` (`bool`, *optional*, defaults to `True`) — If True,
    use gradient checkpointing to save memory at the expense of slower backward pass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RewardConfig collects all training arguments related to the [RewardTrainer](/docs/trl/v0.7.10/en/reward_trainer#trl.RewardTrainer)
    class.
  prefs: []
  type: TYPE_NORMAL
- en: Using `HfArgumentParser` we can turn this class into [argparse](https://docs.python.org/3/library/argparse#module-argparse)
    arguments that can be specified on the command line.
  prefs: []
  type: TYPE_NORMAL
- en: RewardTrainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class trl.RewardTrainer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/reward_trainer.py#L36)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The RewardTrainer can be used to train your custom Reward Model. It is a subclass
    of the `transformers.Trainer` class and inherits all of its attributes and methods.
    It is recommended to use an `AutoModelForSequenceClassification` as the reward
    model. The reward model should be trained on a dataset of paired examples, where
    each example is a tuple of two sequences. The reward model should be trained to
    predict which example in the pair is more relevant to the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: The reward trainer expects a very specific format for the dataset. The dataset
    should contain two 4 entries at least if you don’t use the default `RewardDataCollatorWithPadding`
    data collator. The entries should be named
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids_chosen`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask_chosen`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_ids_rejected`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask_rejected`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, you can also pass a `margin` entry to the dataset. This entry should
    contain the margin used to modulate the loss of the reward model as outlined in
    [https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/).
    If you don’t pass a margin, no margin will be used.
  prefs: []
  type: TYPE_NORMAL
- en: SFTTrainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class trl.SFTTrainer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/sft_trainer.py#L54)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (Union[`transformers.PreTrainedModel`, `nn.Module`, `str`]) — The model
    to train, can be a `PreTrainedModel`, a `torch.nn.Module` or a string with the
    model name to load from cache or download. The model can be also converted to
    a `PeftModel` if a `PeftConfig` object is passed to the `peft_config` argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args` (Optional[transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.36.2/en/main_classes/trainer#transformers.TrainingArguments))
    — The arguments to tweak for training. Please refer to the official documentation
    of `transformers.TrainingArguments` for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_collator` (Optional`transformers.DataCollator`) — The data collator to
    use for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_dataset` (Optional[datasets.Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset))
    — The dataset to use for training. We recommend users to use `trl.trainer.ConstantLengthDataset`
    to create their dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eval_dataset` (Optional[Union[`datasets.Dataset`, Dict[`str`, `datasets.Dataset`]]])
    — The dataset to use for evaluation. We recommend users to use `trl.trainer.ConstantLengthDataset`
    to create their dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (Optional[transformers.PreTrainedTokenizer](https://huggingface.co/docs/transformers/v4.36.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer to use for training. If not specified, the tokenizer associated
    to the model will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_init` (`Callable[[], transformers.PreTrainedModel]`) — The model initializer
    to use for training. If None is specified, the default model initializer will
    be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`compute_metrics` (`Callable[[transformers.EvalPrediction], Dict]`, *optional*
    defaults to None) — The function used to compute metrics during evaluation. It
    should return a dictionary mapping metric names to metric values. If not specified,
    only the loss will be computed during evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callbacks` (`List[transformers.TrainerCallback]`) — The callbacks to use for
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`)
    — The optimizer and scheduler to use for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`)
    — The function to use to preprocess the logits before computing the metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` (`Optional[PeftConfig]`) — The PeftConfig object to use to initialize
    the PeftModel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataset_text_field` (`Optional[str]`) — The name of the text field of the
    dataset, in case this is passed by a user, the trainer will automatically create
    a `ConstantLengthDataset` based on the `dataset_text_field` argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`formatting_func` (`Optional[Callable]`) — The formatting function to be used
    for creating the `ConstantLengthDataset`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_seq_length` (`Optional[int]`) — The maximum sequence length to use for
    the `ConstantLengthDataset` and for automatically creating the Dataset. Defaults
    to `512`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`infinite` (`Optional[bool]`) — Whether to use an infinite dataset or not.
    Defaults to `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_of_sequences` (`Optional[int]`) — The number of sequences to use for the
    `ConstantLengthDataset`. Defaults to `1024`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chars_per_token` (`Optional[float]`) — The number of characters per token
    to use for the `ConstantLengthDataset`. Defaults to `3.6`. You can check how this
    is computed in the stack-llama example: [https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53](https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`packing` (`Optional[bool]`) — Used only in case `dataset_text_field` is passed.
    This argument is used by the `ConstantLengthDataset` to pack the sequences of
    the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataset_num_proc` (`Optional[int]`) — The number of workers to use to tokenize
    the data. Only used when `packing=False`. Defaults to None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataset_batch_size` (`int`) — The number of examples to tokenize per batch.
    If batch_size <= 0 or batch_size == None, tokenize the full dataset as a single
    batch. Defaults to 1000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`neftune_noise_alpha` (`Optional[float]`) — If not `None`, this will activate
    NEFTune noise embeddings. This has been proven to drastically improve model performances
    for instruction fine-tuning. Check out the original paper here: [https://arxiv.org/abs/2310.05914](https://arxiv.org/abs/2310.05914)
    and the original code here: [https://github.com/neelsjain/NEFTune](https://github.com/neelsjain/NEFTune)
    model_init_kwargs — (`Optional[Dict]`, *optional*): Dict of Optional kwargs to
    pass when instantiating the model from a string dataset_kwargs — (`Optional[Dict]`,
    *optional*): Dict of Optional kwargs to pass when creating packed or non-packed
    datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Class definition of the Supervised Finetuning Trainer (SFT Trainer). This class
    is a wrapper around the `transformers.Trainer` class and inherits all of its attributes
    and methods. The trainer takes care of properly initializing the PeftModel in
    case a user passes a `PeftConfig` object.
  prefs: []
  type: TYPE_NORMAL
- en: DPOTrainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class trl.DPOTrainer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L64)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`transformers.PreTrainedModel`) — The model to train, preferably an
    `AutoModelForSequenceClassification`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ref_model` (`PreTrainedModelWrapper`) — Hugging Face transformer model with
    a casual language modelling head. Used for implicit reward computation and loss.
    If no reference model is provided, the trainer will create a reference model with
    the same architecture as the model to be optimized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta` (`float`, defaults to 0.1) — The beta factor in DPO loss. Higher beta
    means less divergence from the initial policy. For the IPO loss, beta is the regularization
    parameter denoted by tau in the paper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_smoothing` (`float`, defaults to 0) — The robust DPO label smoothing
    parameter from the [cDPO](https://ericmitchell.ai/cdpo.pdf) report that should
    be between 0 and 0.5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss_type` (`str`, defaults to `"sigmoid"`) — The type of DPO loss to use.
    Either `"sigmoid"` the default DPO loss,`"hinge"` loss from [SLiC](https://arxiv.org/abs/2305.10425)
    paper, `"ipo"` from [IPO](https://arxiv.org/abs/2310.12036) paper, or `"kto"`
    from the HALOs [report](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`args` (`transformers.TrainingArguments`) — The arguments to use for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_collator` (`transformers.DataCollator`) — The data collator to use for
    training. If None is specified, the default data collator (`DPODataCollatorWithPadding`)
    will be used which will pad the sequences to the maximum length of the sequences
    in the batch, given a dataset of paired sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_pad_token_id` (`int`, defaults to `-100`) — The label pad token id.
    This argument is required if you want to use the default data collator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding_value` (`int`, defaults to `0`) — The padding value if it is different
    to the tokenizer’s pad_token_id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation_mode` (`str`, defaults to `keep_end`) — The truncation mode to
    use, either `keep_end` or `keep_start`. This argument is required if you want
    to use the default data collator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_dataset` (`datasets.Dataset`) — The dataset to use for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eval_dataset` (`datasets.Dataset`) — The dataset to use for evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`transformers.PreTrainedTokenizerBase`) — The tokenizer to use
    for training. This argument is required if you want to use the default data collator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_init` (`Callable[[], transformers.PreTrainedModel]`) — The model initializer
    to use for training. If None is specified, the default model initializer will
    be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callbacks` (`List[transformers.TrainerCallback]`) — The callbacks to use for
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimizers` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`)
    — The optimizer and scheduler to use for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`preprocess_logits_for_metrics` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`)
    — The function to use to preprocess the logits before computing the metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, defaults to `None`) — The maximum length of the sequences
    in the batch. This argument is required if you want to use the default data collator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_prompt_length` (`int`, defaults to `None`) — The maximum length of the
    prompt. This argument is required if you want to use the default data collator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_target_length` (`int`, defaults to `None`) — The maximum length of the
    target. This argument is required if you want to use the default data collator
    and your model is an encoder-decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` (`Dict`, defaults to `None`) — The PEFT configuration to use
    for training. If you pass a PEFT configuration, the model will be wrapped in a
    PEFT model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_encoder_decoder` (`Optional[bool]`, `optional`, defaults to `None`) — If
    no model is provided, we need to know if the model_init returns an encoder-decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`disable_dropout` (`bool`, defaults to `True`) — Whether or not to disable
    dropouts in `model` and `ref_model`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generate_during_eval` (`bool`, defaults to `False`) — Whether to sample and
    log generations during evaluation step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`compute_metrics` (`Callable[[EvalPrediction], Dict]`, *optional*) — The function
    to use to compute the metrics. Must take a `EvalPrediction` and return a dictionary
    string to metric values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`precompute_ref_log_probs` (`bool`, defaults to `False`) — Flag to precompute
    reference model log probabilities and evaluation datasets. This is useful if you
    want to train without the reference model and reduce the total GPU memory needed.
    model_init_kwargs — (`Optional[Dict]`, *optional*): Dict of Optional kwargs to
    pass when instantiating the model from a string ref_model_init_kwargs — (`Optional[Dict]`,
    *optional*): Dict of Optional kwargs to pass when instantiating the ref model
    from a string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_adapter_name` (`str`, defaults to `None`) — Name of the train target
    PEFT adapter, when using LoRA with multiple adapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ref_adapter_name` (`str`, defaults to `None`) — Name of the reference PEFT
    adapter, when using LoRA with multiple adapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize DPOTrainer.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `build_tokenized_answer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L523)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Llama tokenizer does satisfy `enc(a + b) = enc(a) + enc(b)`. It does ensure
    `enc(a + b) = enc(a) + enc(a + b)[len(enc(a)):]`. Reference: [https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257](https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257)'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute_reference_log_probs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L731)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Computes log probabilities of the reference model for a single padded batch
    of a DPO specific dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `concatenated_forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L936)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Run the given model on the given batch of inputs, concatenating the chosen and
    rejected inputs together.
  prefs: []
  type: TYPE_NORMAL
- en: We do this to avoid doing two forward passes, because it’s faster for FSDP.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `concatenated_inputs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L755)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Concatenate the chosen and rejected inputs into a single tensor.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `dpo_loss`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L817)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: A tuple of three tensors
  prefs: []
  type: TYPE_NORMAL
- en: (losses, chosen_rewards, rejected_rewards). The losses tensor contains the DPO
    loss for each example in the batch. The chosen_rewards and rejected_rewards tensors
    contain the rewards for the chosen and rejected responses, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Compute the DPO loss for a batch of policy and reference model log probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `evaluation_loop`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1154)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Overriding built-in evaluation loop to store metrics for each batch. Prediction/evaluation
    loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.
  prefs: []
  type: TYPE_NORMAL
- en: Works both with or without labels.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_batch_logps`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L898)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Compute the log probabilities of the given labels under the given logits.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_batch_loss_metrics`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L982)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Compute the DPO loss and other metrics for the given batch of inputs for train
    or test.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_batch_samples`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1064)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Generate samples from the model and reference model for the given batch of inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_eval_dataloader`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L471)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`eval_dataset` (`torch.utils.data.Dataset`, *optional*) — If provided, will
    override `self.eval_dataset`. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset),
    columns not accepted by the `model.forward()` method are automatically removed.
    It must implement `__len__`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns the evaluation `~torch.utils.data.DataLoader`.
  prefs: []
  type: TYPE_NORMAL
- en: Subclass of transformers.src.transformers.trainer.get_eval_dataloader to precompute
    `ref_log_probs`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_train_dataloader`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L428)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Returns the training `~torch.utils.data.DataLoader`.
  prefs: []
  type: TYPE_NORMAL
- en: Subclass of transformers.src.transformers.trainer.get_train_dataloader to precompute
    `ref_log_probs`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `log`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L1204)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`logs` (`Dict[str, float]`) — The values to log.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log `logs` on the various objects watching training, including stored metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `null_ref_context`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L719)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Context manager for handling null reference model (that is, peft adapter manipulation).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `tokenize_row`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/dpo_trainer.py#L573)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Tokenize a single row from a DPO specific dataset.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, we don’t convert to PyTorch tensors yet; we just handle the truncation
    in case the prompt + chosen or prompt + rejected responses is/are too long. First
    we truncate the prompt; if we’re still too long, we truncate the chosen/rejected.
  prefs: []
  type: TYPE_NORMAL
- en: We also create the labels for the chosen/rejected responses, which are of length
    equal to the sum of the length of the prompt and the chosen/rejected response,
    with label_pad_token_id for the prompt tokens.
  prefs: []
  type: TYPE_NORMAL
- en: DDPOConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class trl.DDPOConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ddpo_config.py#L11)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Configuration class for DDPOTrainer
  prefs: []
  type: TYPE_NORMAL
- en: DDPOTrainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class trl.DDPOTrainer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ddpo_trainer.py#L55)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '*`*config**` (`DDPOConfig`) — Configuration object for DDPOTrainer. Check the
    documentation of `PPOConfig` for more — details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*reward_function**` (Callable[[torch.Tensor, Tuple[str], Tuple[Any]], torch.Tensor])
    — Reward function to be used —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*prompt_function**` (Callable[[], Tuple[str, Any]]) — Function to generate
    prompts to guide model —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*sd_pipeline**` (`DDPOStableDiffusionPipeline`) — Stable Diffusion pipeline
    to be used for training. —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*image_samples_hook**` (Optional[Callable[[Any, Any, Any], Any]]) — Hook
    to be called to log images —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The DDPOTrainer uses Deep Diffusion Policy Optimization to optimise diffusion
    models. Note, this trainer is heavily inspired by the work here: [https://github.com/kvablack/ddpo-pytorch](https://github.com/kvablack/ddpo-pytorch)
    As of now only Stable Diffusion based pipelines are supported'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `calculate_loss`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ddpo_trainer.py#L340)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`latents` (torch.Tensor) — The latents sampled from the diffusion model, shape:
    [batch_size, num_channels_latents, height, width]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timesteps` (torch.Tensor) — The timesteps sampled from the diffusion model,
    shape: [batch_size]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`next_latents` (torch.Tensor) — The next latents sampled from the diffusion
    model, shape: [batch_size, num_channels_latents, height, width]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_probs` (torch.Tensor) — The log probabilities of the latents, shape: [batch_size]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`advantages` (torch.Tensor) — The advantages of the latents, shape: [batch_size]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embeds` (torch.Tensor) — The embeddings of the prompts, shape: [2*batch_size
    or batch_size, …] Note: the “or” is because if train_cfg is True, the expectation
    is that negative prompts are concatenated to the embeds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the loss for a batch of an unpacked sample
  prefs: []
  type: TYPE_NORMAL
- en: '#### `create_model_card`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ddpo_trainer.py#L606)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path` (`str`) — The path to save the model card to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_name` (`str`, *optional*) — The name of the model, defaults to `TRL
    DDPO Model`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates and saves a model card for a TRL model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ddpo_trainer.py#L234)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`epoch` (int) — The current epoch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global_step` (int) — The current global step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: global_step (int)
  prefs: []
  type: TYPE_NORMAL
- en: The updated global step.
  prefs: []
  type: TYPE_NORMAL
- en: Perform a single step of training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Side Effects:'
  prefs: []
  type: TYPE_NORMAL
- en: Model weights are updated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logs the statistics to the accelerator trackers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `self.image_samples_callback` is not None, it will be called with the prompt_image_pairs,
    global_step, and the accelerator tracker.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `train`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/ddpo_trainer.py#L596)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Train the model for a given number of epochs
  prefs: []
  type: TYPE_NORMAL
- en: IterativeSFTTrainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class trl.IterativeSFTTrainer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/iterative_sft_trainer.py#L39)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '*`*model**` (`PreTrainedModel`) — Model to be optimized, either an ‘AutoModelForCausalLM’
    or an ‘AutoModelForSeq2SeqLM’. — Check the documentation of `PreTrainedModel`
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*args**` (`transformers.TrainingArguments`) — — The arguments to use for
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*tokenizer**` (`PreTrainedTokenizerBase`) — Tokenizer to be used for encoding
    the — data. Check the documentation of `transformers.PreTrainedTokenizer` and
    `transformers.PreTrainedTokenizerFast` for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*optimizers**` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`)
    — — The optimizer and scheduler to use for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*data_collator**` (Union[DataCollatorForLanguageModeling, DataCollatorForSeq2Seq],
    *optional*) — Data collator to be used for training and — passed along the dataloader.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*eval_dataset**` (`datasets.Dataset`) — The dataset to use for evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*max_length**` (`int`, defaults to `None`) — — The maximum length of the
    input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*truncation_mode**` (`str`, defaults to `keep_end`) — — The truncation mode
    to use, either `keep_end` or `keep_start`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*preprocess_logits_for_metrics**` (`Callable[[torch.Tensor, torch.Tensor],
    torch.Tensor]`) — — The function to use to preprocess the logits before computing
    the metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*compute_metrics**` (`Callable[[EvalPrediction], Dict]`, *optional*) — —
    The function to use to compute the metrics. Must take a `EvalPrediction` and return
    a dictionary string to metric values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*optimize_device_cache` * *(`bool`,* optional*, defaults to `False`) — Optimize
    CUDA cache for slightly more memory-efficient training. —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The IterativeSFTTrainer can be used to finetune models with methods that requires
    some steps between optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/iterative_sft_trainer.py#L229)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (List`torch.LongTensor`) — List of tensors containing the input_ids
    (if not provided, text will be used)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (List`torch.LongTensor`, , *optional*) — List of tensors containing
    the attention_mask'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (List`torch.FloatTensor`, *optional*) — List of tensors containing
    the labels (if set to None, will default to input_ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`texts` (List`str`, *optional*) — List of strings containing the text input
    (if not provided, input_ids will directly be used)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`texts_labels` (List`str`, *optional*) — List of strings containing the text
    labels (if set to None, will default to text)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`dict[str, Any]`'
  prefs: []
  type: TYPE_NORMAL
- en: A summary of the training statistics
  prefs: []
  type: TYPE_NORMAL
- en: Run an optimisation step given a list of input_ids, attention_mask, and labels
    or a list of text and text_labels.
  prefs: []
  type: TYPE_NORMAL
- en: set_seed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '#### `trl.set_seed`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/core.py#L209)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`seed` (`int`) — The seed to set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helper function for reproducible behavior to set the seed in `random`, `numpy`,
    and `torch`.
  prefs: []
  type: TYPE_NORMAL
