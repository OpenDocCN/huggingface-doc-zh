["```py\n( image_config: Dict = None text_config: Dict = None multimodal_config: Dict = None image_codebook_config: Dict = None hidden_size: int = 768 layer_norm_eps: float = 1e-12 projection_dim: int = 768 init_codebook: bool = True logit_scale_init_value: float = 2.6592 initializer_range: float = 0.02 ce_ignore_index: int = -100 mim_weight: float = 1.0 mlm_weight: float = 1.0 global_contrastive_weight: float = 1.0 itm_weight: float = 1.0 mmm_image_weight: float = 1.0 mmm_text_weight: float = 1.0 global_backprop_contrastive: bool = True skip_unmasked_multimodal_encoder: bool = True return_loss: bool = True **kwargs )\n```", "```py\n>>> from transformers import FlavaConfig, FlavaModel, FlavaForPreTraining\n\n>>> # Initializing a FlavaConfig with style configuration\n>>> configuration = FlavaConfig()\n\n>>> # Initializing a FlavaModel and FlavaForPreTraining model (with random weights) from the style configuration\n>>> model = FlavaModel(configuration)\n>>> model_pre = FlavaForPreTraining(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n>>> configuration_pre = model_pre.config\n```", "```py\n( image_config: FlavaImageConfig text_config: FlavaTextConfig multimodal_config: FlavaMultimodalConfig image_codebook_config: FlavaImageCodebookConfig **kwargs ) \u2192 export const metadata = 'undefined';FlavaConfig\n```", "```py\n( vocab_size: int = 30522 type_vocab_size: int = 2 max_position_embeddings: int = 512 position_embedding_type: str = 'absolute' hidden_size: int = 768 num_hidden_layers: int = 12 num_attention_heads: int = 12 intermediate_size: int = 3072 hidden_act: str = 'gelu' hidden_dropout_prob: float = 0.0 attention_probs_dropout_prob: float = 0.0 initializer_range: float = 0.02 layer_norm_eps: float = 1e-12 pad_token_id: int = 0 qkv_bias: bool = True **kwargs )\n```", "```py\n>>> from transformers import FlavaTextConfig, FlavaTextModel\n\n>>> # Initializing a FlavaTextModel with  style configuration\n>>> configuration = FlavaTextConfig()\n\n>>> # Initializing a FlavaTextModel model (with random weights) from the style configuration\n>>> model = FlavaTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( hidden_size: int = 768 num_hidden_layers: int = 12 num_attention_heads: int = 12 intermediate_size: int = 3072 hidden_act: int = 'gelu' hidden_dropout_prob: float = 0.0 attention_probs_dropout_prob: float = 0.0 initializer_range: float = 0.02 layer_norm_eps: float = 1e-12 image_size: int = 224 patch_size: int = 16 num_channels: int = 3 qkv_bias: bool = True mask_token: bool = True vocab_size: int = 8192 **kwargs )\n```", "```py\n>>> from transformers import FlavaImageConfig, FlavaImageModel\n\n>>> # Initializing a FlavaImageModel with  style configuration\n>>> configuration = FlavaImageConfig()\n\n>>> # Initializing a FlavaImageModel model (with random weights) from the style configuration\n>>> model = FlavaImageModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( hidden_size: int = 768 num_hidden_layers: int = 6 num_attention_heads: int = 12 intermediate_size: int = 3072 hidden_act: int = 'gelu' hidden_dropout_prob: int = 0.0 attention_probs_dropout_prob: int = 0.0 initializer_range: float = 0.02 layer_norm_eps: float = 1e-12 qkv_bias: bool = True use_cls_token: bool = True **kwargs )\n```", "```py\n>>> from transformers import FlavaMultimodalConfig, FlavaMultimodalModel\n\n>>> # Initializing a FlavaMultimodalModel with  style configuration\n>>> configuration = FlavaMultimodalConfig()\n\n>>> # Initializing a FlavaMultimodalModel model (with random weights) from the style configuration\n>>> model = FlavaMultimodalModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( num_groups: int = 4 input_channels: int = 3 num_blocks_per_group: int = 2 hidden_size: int = 256 vocab_size: int = 8192 freeze: int = True initializer_range: float = 0.02 **kwargs )\n```", "```py\n( image_processor = None tokenizer = None **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BICUBIC: 3> do_center_crop: bool = True crop_size: Dict = None do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None return_image_mask: bool = False input_size_patches: int = 14 total_mask_patches: int = 75 mask_group_min_patches: int = 16 mask_group_max_patches: Optional = None mask_group_min_aspect_ratio: float = 0.3 mask_group_max_aspect_ratio: Optional = None return_codebook_pixels: bool = False codebook_do_resize: bool = True codebook_size: bool = None codebook_resample: int = <Resampling.LANCZOS: 1> codebook_do_center_crop: bool = True codebook_crop_size: int = None codebook_do_rescale: bool = True codebook_rescale_factor: Union = 0.00392156862745098 codebook_do_map_pixels: bool = True codebook_do_normalize: bool = True codebook_image_mean: Union = None codebook_image_std: Union = None **kwargs )\n```", "```py\n( images: Union do_resize: Optional = None size: Dict = None resample: Resampling = None do_center_crop: Optional = None crop_size: Optional = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None return_image_mask: Optional = None input_size_patches: Optional = None total_mask_patches: Optional = None mask_group_min_patches: Optional = None mask_group_max_patches: Optional = None mask_group_min_aspect_ratio: Optional = None mask_group_max_aspect_ratio: Optional = None return_codebook_pixels: Optional = None codebook_do_resize: Optional = None codebook_size: Optional = None codebook_resample: Optional = None codebook_do_center_crop: Optional = None codebook_crop_size: Optional = None codebook_do_rescale: Optional = None codebook_rescale_factor: Optional = None codebook_do_map_pixels: Optional = None codebook_do_normalize: Optional = None codebook_image_mean: Optional = None codebook_image_std: Optional = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( config: FlavaConfig image_codebook: Optional = None )\n```", "```py\n( input_ids: Optional = None input_ids_masked: Optional = None pixel_values: Optional = None codebook_pixel_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None bool_masked_pos: Optional = None position_ids: Optional = None image_attention_mask: Optional = None skip_unmasked_multimodal_encoder: bool = None mlm_labels: Optional = None mim_labels: Optional = None itm_labels: Optional = None output_attentions: Optional = None output_hidden_states: bool = True return_dict: Optional = None return_loss: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.flava.modeling_flava.FlavaForPreTrainingOutput or tuple(torch.FloatTensor)\n```", "```py\n( config: FlavaConfig )\n```", "```py\n( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None bool_masked_pos: Optional = None position_ids: Optional = None image_attention_mask: Optional = None skip_multimodal_encoder: Optional = None output_attentions: Optional = None output_hidden_states: bool = True return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.flava.modeling_flava.FlavaModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, FlavaModel\n\n>>> model = FlavaModel.from_pretrained(\"facebook/flava-full\")\n>>> processor = AutoProcessor.from_pretrained(\"facebook/flava-full\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(text=[\"a photo of a cat\"], images=image, return_tensors=\"pt\", padding=True)\n\n>>> outputs = model(**inputs)\n\n>>> image_embeddings = outputs.image_embeddings\n>>> text_embeddings = outputs.text_embeddings\n>>> multimodal_embeddings = outputs.multimodal_embeddings\n\n>>> outputs.image_embeddings.shape\ntorch.Size([1, 197, 768])\n\n>>> text_embeddings.shape\ntorch.Size([1, 7, 768])\n\n>>> multimodal_embeddings.shape\ntorch.Size([1, 205, 768])\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( pixel_values: Optional = None bool_masked_pos: Optional = None interpolate_pos_encoding: Optional = None attention_mask: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( config: FlavaImageCodebookConfig **kwargs: Any )\n```", "```py\n( pixel_values: FloatTensor )\n```", "```py\n( pixel_values: Tensor )\n```", "```py\n( pixel_values: Tensor )\n```", "```py\n( config: FlavaTextConfig add_pooling_layer: bool = True )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlavaTextModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/flava-full\")\n>>> model = FlavaTextModel.from_pretrained(\"facebook/flava-full\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: FlavaImageConfig add_pooling_layer: bool = True )\n```", "```py\n( pixel_values: Optional = None bool_masked_pos: Optional = None interpolate_pos_encoding: Optional = None attention_mask: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, FlavaImageModel\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/flava-full\")\n>>> model = FlavaImageModel.from_pretrained(\"facebook/flava-full\")\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 197, 768]\n```", "```py\n( config: FlavaMultimodalConfig add_pooling_layer = True )\n```", "```py\n( hidden_states: Tensor attention_mask: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlavaMultimodalModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/flava-full\")\n>>> model = FlavaMultimodalModel.from_pretrained(\"facebook/flava-full\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```"]