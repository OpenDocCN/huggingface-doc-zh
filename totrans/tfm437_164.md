# ESM

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/esm](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/esm)

## 概述

本页面提供了Meta AI基础人工智能研究团队的Transformer蛋白质语言模型的代码和预训练权重，提供了最先进的ESMFold和ESM-2，以及之前发布的ESM-1b和ESM-1v。Transformer蛋白质语言模型是由Alexander Rives、Joshua Meier、Tom Sercu、Siddharth Goyal、Zeming Lin、Jason Liu、Demi Guo、Myle Ott、C. Lawrence Zitnick、Jerry Ma和Rob Fergus在论文[Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.pnas.org/content/118/15/e2016239118)中引入的。该论文的第一个版本于2019年[预印](https://www.biorxiv.org/content/10.1101/622803v1?versioned=true)。

ESM-2在一系列结构预测任务中表现优异，胜过所有经过测试的单序列蛋白质语言模型，并实现了原子分辨率结构预测。该模型是由Zeming Lin、Halil Akin、Roshan Rao、Brian Hie、Zhongkai Zhu、Wenting Lu、Allan dos Santos Costa、Maryam Fazel-Zarandi、Tom Sercu、Sal Candido和Alexander Rives在论文[Language models of protein sequences at the scale of evolution enable accurate structure prediction](https://doi.org/10.1101/2022.07.20.500902)中发布的。

该论文还介绍了ESMFold。它使用了一个ESM-2干部，带有一个可以以最先进的准确性预测折叠蛋白质结构的头部。与[AlphaFold2](https://www.nature.com/articles/s41586-021-03819-2)不同，它依赖于大型预训练蛋白质语言模型干部的标记嵌入，并且在推断时不执行多序列比对（MSA）步骤，这意味着ESMFold检查点完全是“独立的” - 它们不需要已知蛋白质序列和结构的数据库以及相关的外部查询工具来进行预测，并且因此速度更快。

来自“Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences”的摘要是

*在人工智能领域，通过无监督学习实现的数据规模和模型容量的结合，推动了表示学习和统计生成方面的重大进展。在生命科学领域，预期的测序增长将带来有关自然序列多样性的前所未有的数据。在进化规模上进行蛋白质语言建模是生物学预测和生成人工智能的逻辑步骤。为此，我们使用无监督学习在跨越进化多样性的250亿蛋白质序列中训练了一个深度上下文语言模型，共计860亿个氨基酸。所得模型包含有关生物性质的信息。这些表示仅从序列数据中学习而来。学习到的表示空间具有多尺度组织，反映了从氨基酸的生化性质到蛋白质的远程同源的结构。表示中编码了有关二级和三级结构的信息，并可以通过线性投影进行识别。表示学习产生了能够在一系列应用中泛化的特征，实现了最先进的突变效应和二级结构的监督预测，并改进了长程接触预测的最先进特征。*

来自“Language models of protein sequences at the scale of evolution enable accurate structure prediction”的摘要是

*最近已经证明，大型语言模型在规模上具有新兴的能力，超越简单的模式匹配，进行更高级别的推理，并生成逼真的图像和文本。虽然在蛋白质序列上训练的语言模型已经在较小规模上进行了研究，但对于它们在扩大规模时学习到的生物学知识知之甚少。在这项工作中，我们训练了具有 150 亿参数的模型，这是迄今为止评估的最大蛋白质语言模型。我们发现随着模型的扩大，它们学习到的信息使得能够预测蛋白质的三维结构，分辨率达到单个原子。我们提出了 ESMFold，用于直接从蛋白质的个体序列进行高精度端到端的原子级结构预测。ESMFold 对于低困惑度且被语言模型充分理解的序列具有与 AlphaFold2 和 RoseTTAFold 相似的准确性。ESMFold 推理速度比 AlphaFold2 快一个数量级，使得能够在实际时间范围内探索宏基因组蛋白质的结构空间。*

原始代码可以在 [这里](https://github.com/facebookresearch/esm) 找到，并由 Meta AI 的 Fundamental AI Research 团队开发。ESM-1b、ESM-1v 和 ESM-2 由 [jasonliu](https://huggingface.co/jasonliu) 和 [Matt](https://huggingface.co/Rocketknight1) 贡献给了 HuggingFace。

ESMFold 由 [Matt](https://huggingface.co/Rocketknight1) 和 [Sylvain](https://huggingface.co/sgugger) 贡献给了 HuggingFace，特别感谢 Nikita Smetanin、Roshan Rao 和 Tom Sercu 在整个过程中的帮助！

## 使用提示

+   ESM 模型是使用掩码语言建模（MLM）目标进行训练的。

+   HuggingFace 移植的 ESMFold 使用了 [openfold](https://github.com/aqlaboratory/openfold) 库的部分内容。`openfold` 库使用 Apache License 2.0 许可。

## 资源

+   [文本分类任务指南](../tasks/sequence_classification)

+   [标记分类任务指南](../tasks/token_classification)

+   [掩码语言建模任务指南](../tasks/masked_language_modeling)

## EsmConfig

### `class transformers.EsmConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/configuration_esm.py#L33)

```py
( vocab_size = None mask_token_id = None pad_token_id = None hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 1026 initializer_range = 0.02 layer_norm_eps = 1e-12 position_embedding_type = 'absolute' use_cache = True emb_layer_norm_before = None token_dropout = False is_folding_model = False esmfold_config = None vocab_list = None **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*) — ESM 模型的词汇表大小。定义了在调用 `ESMModel` 时可以表示的不同标记数量。

+   `mask_token_id` (`int`, *optional*) — 词汇表中掩码标记的索引。由于“mask-dropout”缩放技巧，必须在配置中包含此项，该技巧将根据掩码标记的数量来缩放输入。

+   `pad_token_id` (`int`, *optional*) — 词汇表中填充标记的索引。由于 ESM 代码的某些部分使用此标记而不是注意力掩码，因此必须在配置中包含此项。

+   `hidden_size` (`int`, *optional*, 默认为768) — 编码器层和池化器层的维度。

+   `num_hidden_layers` (`int`, *optional*, 默认为12) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, 默认为12) — Transformer 编码器中每个注意力层的注意力头数量。

+   `intermediate_size` (`int`, *optional*, 默认为3072) — Transformer 编码器中“中间”（通常称为前馈）层的维度。

+   `hidden_dropout_prob` (`float`, *optional*, 默认为0.1) — 嵌入层、编码器和池化器中所有全连接层的丢弃概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, 默认为0.1) — 注意力概率的丢弃比率。

+   `max_position_embeddings` (`int`, *optional*, 默认为1026) — 该模型可能会与之一起使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512、1024 或 2048）。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的 epsilon。

+   `position_embedding_type` (`str`, *optional*, defaults to `"absolute"`) — 位置嵌入的类型。选择 `"absolute"`, `"relative_key"`, `"relative_key_query", "rotary"` 中的一个。对于位置嵌入使用 `"absolute"`。有关 `"relative_key"` 的更多信息，请参考 [Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)。有关 `"relative_key_query"` 的更多信息，请参考 [Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)] 中的 *Method 4* (https://arxiv.org/abs/2009.13658)。

+   `is_decoder` (`bool`, *optional*, defaults to `False`) — 模型是否用作解码器。如果为 `False`，则模型用作编码器。

+   `use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。仅在 `config.is_decoder=True` 时相关。

+   `emb_layer_norm_before` (`bool`, *optional*) — 是否在嵌入之后但在网络主干之前应用层归一化。

+   `token_dropout` (`bool`, defaults to `False`) — 启用此选项时，掩码标记将被视为已通过输入丢失删除。

这是用于存储 `ESMModel` 配置的配置类。根据指定的参数实例化一个 ESM 模型，定义模型架构。使用默认值实例化配置将产生类似于 ESM [facebook/esm-1b](https://huggingface.co/facebook/esm-1b) 架构的配置。

配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 的文档以获取更多信息。

示例：

```py
>>> from transformers import EsmModel, EsmConfig

>>> # Initializing a ESM facebook/esm-1b style configuration >>> configuration = EsmConfig()

>>> # Initializing a model from the configuration >>> model = ESMModel(configuration)

>>> # Accessing the model configuration >>> configuration = model.config
```

#### `to_dict`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/configuration_esm.py#L161)

```py
( ) → export const metadata = 'undefined';Dict[str, any]
```

返回

`Dict[str, any]`

包含构成此配置实例的所有属性的字典，

将此实例序列化为 Python 字典。覆盖默认的 [to_dict()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.to_dict)。

## EsmTokenizer

### `class transformers.EsmTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/tokenization_esm.py#L47)

```py
( vocab_file unk_token = '<unk>' cls_token = '<cls>' pad_token = '<pad>' mask_token = '<mask>' eos_token = '<eos>' **kwargs )
```

构造一个 ESM 分词器。

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/tokenization_esm.py#L106)

```py
( token_ids_0: List token_ids_1: Optional = None )
```

#### `get_special_tokens_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/tokenization_esm.py#L120)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';A list of integers in the range [0, 1]
```

参数

+   `token_ids_0` (`List[int]`) — 第一个序列的 id 列表。

+   `token_ids_1` (`List[int]`, *optional*) — 第二个序列的 id 列表。

+   `already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — 标记列表是否已经格式化为模型的特殊标记。

返回

一个范围在 [0, 1] 内的整数列表

1 表示特殊标记，0 表示序列标记。

从没有添加特殊标记的标记列表中检索序列 id。当使用 tokenizer 的 `prepare_for_model` 或 `encode_plus` 方法添加特殊标记时，将调用此方法。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 第一个标记化序列。

+   `token_ids_1`（`List[int]`，*可选*）- 第二个令牌化序列。

返回

`List[int]`

令牌类型ID。

创建与传递的序列相对应的令牌类型ID。[什么是令牌类型ID？](../glossary#token-type-ids)

如果模型有特殊的构建方式，应该在子类中重写。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/tokenization_esm.py#L151)

```py
( save_directory filename_prefix )
```

Pytorch隐藏Pytorch内容

## EsmModel

### `类transformers.EsmModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_esm.py#L767)

```py
( config add_pooling_layer = True )
```

参数

+   `config`（[EsmConfig](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmConfig)）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸ESM模型变压器输出原始隐藏状态，没有特定的头部。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档，了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

该模型可以作为编码器（仅具有自注意力）以及解码器行为，此时在自注意力层之间添加了一层交叉注意力，遵循[Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N. Gomez、Lukasz Kaiser和Illia Polosukhin所描述的架构](https://arxiv.org/abs/1706.03762)。

为了作为解码器行为，模型需要使用配置中设置为`True`的`is_decoder`参数进行初始化。要在Seq2Seq模型中使用，模型需要使用`is_decoder`参数和`add_cross_attention`设置为`True`进行初始化；然后期望将`encoder_hidden_states`作为输入传递。

#### `前进`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_esm.py#L814)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`((batch_size, sequence_length))`的`torch.LongTensor`）- 词汇表中输入序列令牌的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`((batch_size, sequence_length))`的`torch.FloatTensor`，*可选*）- 避免在填充令牌索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   1对于“未屏蔽”的标记，

    +   0对于“屏蔽”的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids`（形状为`((batch_size, sequence_length))`的`torch.LongTensor`，*可选*）- 每个输入序列令牌在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选在`[0, 1]`之间：

    +   1表示头部未被掩盖，

    +   0表示头部被掩盖。

+   `inputs_embeds` (`torch.FloatTensor` of shape `((batch_size, sequence_length), hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `encoder_hidden_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 编码器最后一层的隐藏状态序列。如果模型配置为解码器，则在交叉注意力中使用。

+   `encoder_attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在编码器输入的填充标记索引上执行注意力的掩码。如果模型配置为解码器，则在交叉注意力中使用此掩码。掩码值选在`[0, 1]`之间：

    +   1表示未被掩盖的标记，

    +   0表示被掩盖的标记。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，长度为`config.n_layers`，每个元组有4个形状为`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`的张量） — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。

    如果使用`past_key_values`，用户可以选择仅输入最后一个形状为`(batch_size, 1)`的`decoder_input_ids`（这些没有将其过去的键值状态提供给此模型的输入）而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

返回

[transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions)或`tuple(torch.FloatTensor)`

[transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包含根据配置（[EsmConfig](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmConfig)）和输入的不同元素。

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列的输出。

+   `pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`) — 经过进一步处理的序列第一个标记（分类标记）的最后一层隐藏状态（辅助预训练任务中使用的层）。例如，对于BERT系列模型，这将返回经过线性层和tanh激活函数处理后的分类标记。线性层权重是从预训练期间的下一个句子预测（分类）目标中训练的。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在自注意力头中，注意力softmax之后的注意力权重，用于计算加权平均值。

+   `cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`和`config.add_cross_attention=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）— 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`，还有2个形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块中的键和值，以及在交叉注意力块中，如果`config.is_encoder_decoder=True`，还可以使用`past_key_values`输入）以加速顺序解码。

[EsmModel](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是调用此函数，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, EsmModel
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/esm2_t6_8M_UR50D")
>>> model = EsmModel.from_pretrained("facebook/esm2_t6_8M_UR50D")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## EsmForMaskedLM

### `class transformers.EsmForMaskedLM`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_esm.py#L953)

```py
( config )
```

参数

+   `config`（[EsmConfig](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmConfig)）— 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

在顶部带有`语言建模`头的ESM模型。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_esm.py#L977)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.MaskedLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    什么是输入ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   1表示未被“掩盖”的标记，

    +   0表示被“掩盖”的标记。

    什么是注意力掩码？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置ID？

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）— 用于使自注意力模块的选定头部失效的掩码。选择的掩码值在`[0, 1]`中：

    +   1表示未被“掩盖”的头部，

    +   0表示头部被“掩盖”。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`中（请参阅`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩盖），仅对具有标签在`[0, ..., config.vocab_size]`中的标记计算损失

+   `kwargs`（`Dict[str, any]`，可选，默认为*{}*）— 用于隐藏已弃用的旧参数。

返回

[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含各种元素，具体取决于配置（[EsmConfig](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmConfig)）和输入。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，在提供`labels`时返回）— 掩码语言建模（MLM）损失。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头部的预测分数（SoftMax之前每个词汇标记的分数）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层的输出，则为嵌入的输出+每层的输出）。

    模型在每一层输出处的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[EsmForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmForMaskedLM)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, EsmForMaskedLM
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/esm2_t6_8M_UR50D")
>>> model = EsmForMaskedLM.from_pretrained("facebook/esm2_t6_8M_UR50D")

>>> inputs = tokenizer("The capital of France is <mask>.", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # retrieve index of <mask>
>>> mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

>>> predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)

>>> labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]
>>> # mask labels of non-<mask> tokens
>>> labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

>>> outputs = model(**inputs, labels=labels)
```

## EsmForSequenceClassification

### `class transformers.EsmForSequenceClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_esm.py#L1066)

```py
( config )
```

参数

+   `config` ([EsmConfig](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmConfig)) — 模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

ESM模型变压器，顶部带有序列分类/回归头（池化输出顶部的线性层），例如用于GLUE任务。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_esm.py#L1084)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值为`[0, 1]`：

    +   1表示未被`masked`的标记，

    +   0表示被`masked`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*) — 用于使自注意力模块的选定头部失效的掩码。选择的掩码值为`[0, 1]`：

    +   1表示头部未被`masked`，

    +   0表示头部被`masked`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回值

[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（[EsmConfig](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmConfig)）和输入。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 分类（如果`config.num_labels==1`则为回归）损失。

+   `logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`）— 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出，+ 每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在自注意力头中用于计算加权平均值的注意力softmax之后的注意力权重。

[EsmForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmForSequenceClassification)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传播的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

单标签分类示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, EsmForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/esm2_t6_8M_UR50D")
>>> model = EsmForSequenceClassification.from_pretrained("facebook/esm2_t6_8M_UR50D")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_id = logits.argmax().item()

>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = EsmForSequenceClassification.from_pretrained("facebook/esm2_t6_8M_UR50D", num_labels=num_labels)

>>> labels = torch.tensor([1])
>>> loss = model(**inputs, labels=labels).loss
```

多标签分类示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, EsmForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/esm2_t6_8M_UR50D")
>>> model = EsmForSequenceClassification.from_pretrained("facebook/esm2_t6_8M_UR50D", problem_type="multi_label_classification")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]

>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = EsmForSequenceClassification.from_pretrained(
...     "facebook/esm2_t6_8M_UR50D", num_labels=num_labels, problem_type="multi_label_classification"
... )

>>> labels = torch.sum(
...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1
... ).to(torch.float)
>>> loss = model(**inputs, labels=labels).loss
```

## EsmForTokenClassification

### `class transformers.EsmForTokenClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_esm.py#L1160)

```py
( config )
```

参数

+   `config` ([EsmConfig](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

ESM模型在顶部带有一个标记分类头部（隐藏状态输出的顶部线性层），例如用于命名实体识别（NER）任务。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_esm.py#L1178)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   1 表示未被遮蔽的标记，

    +   0 表示被遮蔽的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。选在范围`[0, config.max_position_embeddings - 1]`内。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*) — 用于使自注意力模块中的特定头部失效的掩码。掩码值选在`[0, 1]`之间：

    +   1 表示头部未被遮蔽，

    +   0 表示头部被遮蔽。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 可选地，您可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 用于计算标记分类损失的标签。索引应在`[0, ..., config.num_labels - 1]`内。

返回

[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（[EsmConfig](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmConfig)）和输入。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）-分类损失。

+   `logits`（形状为`(batch_size, sequence_length, config.num_labels)`的`torch.FloatTensor`）-分类分数（SoftMax之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的输出+每一层的输出）。

    模型在每一层输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在自注意力头中用于计算加权平均值的注意力权重softmax后。

[EsmForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmForTokenClassification)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, EsmForTokenClassification
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/esm2_t6_8M_UR50D")
>>> model = EsmForTokenClassification.from_pretrained("facebook/esm2_t6_8M_UR50D")

>>> inputs = tokenizer(
...     "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="pt"
... )

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_token_class_ids = logits.argmax(-1)

>>> # Note that tokens are classified rather then input words which means that
>>> # there might be more predicted token classes than words.
>>> # Multiple token classes might account for the same word
>>> predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]

>>> labels = predicted_token_class_ids
>>> loss = model(**inputs, labels=labels).loss
```

## EsmForProteinFolding

### `class transformers.EsmForProteinFolding`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_esmfold.py#L2011)

```py
( config )
```

参数

+   `config`（[EsmConfig](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmConfig)）-包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

ESMForProteinFolding是原始ESMFold模型的HuggingFace移植版。它由一个ESM-2“干”部分和一个蛋白质折叠“头”部分组成，尽管与大多数其他输出头部不同，这个“头”部分在大小和运行时间上与模型的其余部分相似！它输出一个包含关于输入蛋白质的预测结构信息的字典。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_esmfold.py#L2084)

```py
( input_ids: Tensor attention_mask: Optional = None position_ids: Optional = None masking_pattern: Optional = None num_recycles: Optional = None ) → export const metadata = 'undefined';transformers.models.esm.modeling_esmfold.EsmForProteinFoldingOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）-词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 避免在填充令牌索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   对于未被`masked`的令牌为1，

    +   对于被`masked`的令牌为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 每个输入序列令牌在位置嵌入中的位置索引。选在范围`[0, config.max_position_embeddings - 1]`内。

    [什么是位置ID？](../glossary#position-ids)

+   `masking_pattern` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 在训练期间要屏蔽的令牌位置，作为一种正则化形式。掩码值选在`[0, 1]`之间。

+   `num_recycles` (`int`，*可选*，默认为`None`) — 重复输入序列的次数。如果为`None`，则默认为`config.num_recycles`。 “Recycling”包括将折叠主干的输出作为输入传递给主干。在训练期间，每个批次的循环次数应该随着每次循环而变化，以确保模型学会在每次循环后输出有效的预测。在推断期间，num_recycles应设置为模型训练的最大值，以获得最大准确性。因此，当此值设置为`None`时，将使用config.max_recycles。

返回

`transformers.models.esm.modeling_esmfold.EsmForProteinFoldingOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.esm.modeling_esmfold.EsmForProteinFoldingOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置(`<class 'transformers.models.esm.configuration_esm.EsmConfig'>`)和输入的不同元素。

+   `frames` (`torch.FloatTensor`) — 输出的框架。

+   `sidechain_frames` (`torch.FloatTensor`) — 输出的侧链框架。

+   `unnormalized_angles` (`torch.FloatTensor`) — 预测的未归一化主链和侧链扭转角度。

+   `angles` (`torch.FloatTensor`) — 预测的主链和侧链扭转角度。

+   `positions` (`torch.FloatTensor`) — 预测的主链和侧链原子位置。

+   `states` (`torch.FloatTensor`) — 蛋白质折叠主干的隐藏状态。

+   `s_s` (`torch.FloatTensor`) — 通过连接ESM-2 LM干每一层的隐藏状态派生的每个残基的嵌入。

+   `s_z` (`torch.FloatTensor`) — 成对残基嵌入。

+   `distogram_logits` (`torch.FloatTensor`) — 用于计算残基距离的distogram的输入logits。

+   `lm_logits` (`torch.FloatTensor`) — ESM-2蛋白质语言模型干输出的logits。

+   `aatype` (`torch.FloatTensor`) — 输入的氨基酸（AlphaFold2索引）。

+   `atom14_atom_exists` (`torch.FloatTensor`) — 每个原子是否存在于atom14表示中。

+   `residx_atom14_to_atom37` (`torch.FloatTensor`) — 在atom14和atom37表示之间的原子映射。

+   `residx_atom37_to_atom14` (`torch.FloatTensor`) — 在atom37和atom14表示之间的原子映射。

+   `atom37_atom_exists` (`torch.FloatTensor`) — 每个原子是否存在于atom37表示中。

+   `residue_index` (`torch.FloatTensor`) — 蛋白质链中每个残基的索引。除非使用内部填充令牌，否则这将是从0到`sequence_length`的整数序列。

+   `lddt_head` (`torch.FloatTensor`) — 用于计算plddt的lddt头部的原始输出。

+   `plddt` (`torch.FloatTensor`) — 每个残基的置信度分数。低置信度区域可能表明模型预测不确定的区域，或者蛋白质结构无序的区域。

+   `ptm_logits` (`torch.FloatTensor`) — 用于计算 ptm 的原始对数。

+   `ptm` (`torch.FloatTensor`) — TM-score 输出，代表模型对整体结构的高级置信度。

+   `aligned_confidence_probs` (`torch.FloatTensor`) — 对齐结构的每个残基的置信度分数。

+   `predicted_aligned_error` (`torch.FloatTensor`) — 模型预测与真实值之间的预测误差。

+   `max_predicted_aligned_error` (`torch.FloatTensor`) — 每个样本的最大预测误差。

[EsmForProteinFolding](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmForProteinFolding) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, EsmForProteinFolding

>>> model = EsmForProteinFolding.from_pretrained("facebook/esmfold_v1")
>>> tokenizer = AutoTokenizer.from_pretrained("facebook/esmfold_v1")
>>> inputs = tokenizer(["MLKNVQVQLV"], return_tensors="pt", add_special_tokens=False)  # A tiny random peptide
>>> outputs = model(**inputs)
>>> folded_positions = outputs.positions
```

TensorFlow 隐藏 TensorFlow 内容

## TFEsmModel

### `class transformers.TFEsmModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_tf_esm.py#L1099)

```py
( config: EsmConfig add_pooling_layer = True *inputs **kwargs )
```

参数

+   `config` ([EsmConfig](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained) 方法以加载模型权重。

裸 ESM 模型变压器输出原始隐藏状态，没有特定的头部。

该模型继承自 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型也是 Keras [Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) 子类。将其用作常规 Keras 模型，并参考 TF/Keras 文档以了解所有与一般使用和行为相关的事项。

#### `call`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_tf_esm.py#L1109)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None encoder_hidden_states: np.ndarray | tf.Tensor | None = None encoder_attention_mask: np.ndarray | tf.Tensor | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions or tuple(tf.Tensor)
```

参数

+   `input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。查看 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__) 以获取详细信息。

    [什么是输入 ID？](../glossary#input-ids)

+   `attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]`：

    +   1 表示 `未被掩盖` 的标记，

    +   0 表示 `被掩盖` 的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选在范围 `[0, config.max_position_embeddings - 1]`。

    [什么是位置 ID？](../glossary#position-ids)

+   `head_mask` (`tf.Tensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值选在 `[0, 1]`：

    +   1 表示头部 `未被掩盖`。

    +   0 表示头部被 `掩盖`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）— 可选地，可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这很有用。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

+   `encoder_hidden_states`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）— 编码器最后一层的隐藏状态序列。如果模型配置为解码器，则在交叉注意力中使用。

+   `encoder_attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）— 避免对编码器输入的填充标记索引执行注意力的掩码。如果模型配置为解码器，则在交叉注意力中使用。掩码值选择在`[0, 1]`中。

    +   对于未被掩码的标记为1，

    +   对于被掩码的标记为0。

+   `past_key_values`（长度为`config.n_layers`的`Tuple[Tuple[tf.Tensor]]`）— 包含注意力块的预计算键和值隐藏状态。可用于加速解码。如果使用`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（那些没有将其过去的键值状态提供给此模型的）的形状为`(batch_size, 1)`而不是所有`decoder_input_ids`的形状为`(batch_size, sequence_length)`。

+   `use_cache`（`bool`，*可选*，默认为`True`）— 如果设置为`True`，将返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。在训练期间设置为`False`，在生成期间设置为`True`。

返回

[transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions)或者`tuple(tf.Tensor)`

一个[transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions)或者一个`tf.Tensor`元组（如果传递了`return_dict=False`或者`config.return_dict=False`时）包括根据配置（[EsmConfig](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmConfig)）和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`）— 模型最后一层的隐藏状态序列。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`tf.Tensor`）— 序列第一个标记（分类标记）的最后一层隐藏状态，经过线性层和Tanh激活函数进一步处理。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

    此输出通常*不是*输入的语义内容的良好摘要，您通常最好对整个输入序列的隐藏状态进行平均或池化。

+   `past_key_values`（`List[tf.Tensor]`，*可选*，当传递`use_cache=True`或者`config.use_cache=True`时返回）— 长度为`config.n_layers`的`tf.Tensor`列表，每个张量的形状为`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`。

    包含预先计算的隐藏状态（注意力块中的键和值），可用于加速顺序解码。

+   `hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出+一个用于每一层的输出）。

    模型在每一层的输出处的隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    解码器的交叉注意力层的注意力权重，在注意力softmax后使用，用于计算交叉注意力头中的加权平均值。

TFEsmModel前向方法，覆盖`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, TFEsmModel
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/esm2_t6_8M_UR50D")
>>> model = TFEsmModel.from_pretrained("facebook/esm2_t6_8M_UR50D")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
>>> outputs = model(inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## TFEsmForMaskedLM

### `class transformers.TFEsmForMaskedLM`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_tf_esm.py#L1181)

```py
( config )
```

参数

+   `config`（EsmConfig）-模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看from_pretrained()方法以加载模型权重。

ESM模型顶部带有`语言建模`头。

这个模型继承自TFPreTrainedModel。查看超类文档以了解库实现的通用方法（如下载或保存，调整输入嵌入，修剪头等）。

这个模型也是Keras Model的子类。将其用作常规Keras模型，并参考TF/Keras文档以了解所有与一般用法和行为相关的事项。

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_tf_esm.py#L1212)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None encoder_hidden_states: np.ndarray | tf.Tensor | None = None encoder_attention_mask: np.ndarray | tf.Tensor | None = None labels: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFMaskedLMOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`）-词汇表中输入序列标记的索引。

    可以使用AutoTokenizer获取索引。查看PreTrainedTokenizer.encode()和PreTrainedTokenizer.`call`()获取详细信息。

    什么是输入ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）-避免在填充标记索引上执行注意力的掩码。选择在`[0, 1]`中的掩码值：

    +   对于未被屏蔽的标记为1，

    +   对于被屏蔽的标记为0。

    注意力掩码是什么？

+   `position_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]` 中选择。

    [什么是位置 ID？](../glossary#position-ids)

+   `head_mask` (`tf.Tensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于将自注意力模块的选定头部置零的掩码。掩码值选定在 `[0, 1]` 之间：

    +   1 表示头部未被 `masked`。

    +   0 表示头部被 `masked`。

+   `inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制权，以便将 `input_ids` 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

+   `labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于计算掩码语言建模损失的标签。索引应在 `[-100, 0, ..., config.vocab_size]` 范围内（参见 `input_ids` 文档字符串）。索引设置为 `-100` 的标记将被忽略（masked），损失仅计算具有标签在 `[0, ..., config.vocab_size]` 范围内的标记。

+   `kwargs` (`Dict[str, any]`, optional, 默认为 *{}*) — 用于隐藏已被弃用的旧参数。

返回

[transformers.modeling_tf_outputs.TFMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput) 或 `tuple(tf.Tensor)`

一个 [transformers.modeling_tf_outputs.TFMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput) 或一个 `tf.Tensor` 元组（如果传递 `return_dict=False` 或者 `config.return_dict=False` 时）包含各种元素，取决于配置（[EsmConfig](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmConfig)）和输入。

+   `loss` (`tf.Tensor` of shape `(n,)`, *optional*, 其中 n 是非掩码标签的数量，在提供 `labels` 时返回) — 掩码语言建模（MLM）损失。

+   `logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头部的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递 `output_hidden_states=True` 或者 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `tf.Tensor` 元组（一个用于嵌入的输出 + 一个用于每层的输出）。

    每层模型的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(tf.Tensor)`, *optional*, 当传递 `output_attentions=True` 或者 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `tf.Tensor` 元组（每层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头部中的加权平均值。

[TFEsmForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.TFEsmForMaskedLM) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传播的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者负责运行前处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, TFEsmForMaskedLM
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/esm2_t6_8M_UR50D")
>>> model = TFEsmForMaskedLM.from_pretrained("facebook/esm2_t6_8M_UR50D")

>>> inputs = tokenizer("The capital of France is <mask>.", return_tensors="tf")
>>> logits = model(**inputs).logits

>>> # retrieve index of <mask>
>>> mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])
>>> selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)

>>> predicted_token_id = tf.math.argmax(selected_logits, axis=-1)
```

```py
>>> labels = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]
>>> # mask labels of non-<mask> tokens
>>> labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

>>> outputs = model(**inputs, labels=labels)
```

## TFEsmForSequenceClassification

### `class transformers.TFEsmForSequenceClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_tf_esm.py#L1345)

```py
( config )
```

参数

+   `config`（[EsmConfig](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

ESM模型变压器，顶部带有序列分类/回归头（顶部的线性层在汇总输出之上），例如用于GLUE任务。

此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型还是Keras [Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规Keras模型，并参考TF/Keras文档以获取有关一般用法和行为的所有事项。

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_tf_esm.py#L1363)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None labels: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSequenceClassifierOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`）— 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    什么是输入ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   对于`未屏蔽`的标记为1，

    +   0表示标记为`屏蔽`。

    什么是注意力掩码？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    什么是位置ID？

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`tf.Tensor`，*可选*）— 用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示头部未被`屏蔽`，

    +   0表示头部被`屏蔽`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为关联向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

+   `labels` (`tf.Tensor`的形状为`(batch_size,)`，*可选*) — 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)或`tuple(tf.Tensor)`

一个[transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[EsmConfig](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmConfig)）和输入的各种元素。

+   `loss` (`tf.Tensor`的形状为`(batch_size, )`，*可选*，在提供`labels`时返回) — 分类（如果config.num_labels==1则为回归）损失。

+   `logits` (`tf.Tensor`的形状为`(batch_size, config.num_labels)`) — 分类（如果config.num_labels==1则为回归）分数（SoftMax之前）。

+   `hidden_states` (`tuple(tf.Tensor)`，*可选*，在传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入输出，一个用于每一层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(tf.Tensor)`，*可选*，在传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    在自注意力头中使用注意力softmax后的注意力权重，用于计算加权平均值。

[TFEsmForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.TFEsmForSequenceClassification)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, TFEsmForSequenceClassification
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/esm2_t6_8M_UR50D")
>>> model = TFEsmForSequenceClassification.from_pretrained("facebook/esm2_t6_8M_UR50D")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")

>>> logits = model(**inputs).logits

>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])
```

```py
>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = TFEsmForSequenceClassification.from_pretrained("facebook/esm2_t6_8M_UR50D", num_labels=num_labels)

>>> labels = tf.constant(1)
>>> loss = model(**inputs, labels=labels).loss
```

## TFEsmForTokenClassification

### `class transformers.TFEsmForTokenClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_tf_esm.py#L1430)

```py
( config )
```

参数

+   `config` ([EsmConfig](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

ESM模型在顶部带有一个标记分类头（隐藏状态输出的线性层），例如用于命名实体识别（NER）任务。

这个模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档，了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个Keras [Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的Keras模型，并参考TF/Keras文档以获取所有与一般用法和行为相关的事项。

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/esm/modeling_tf_esm.py#L1450)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None labels: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFTokenClassifierOutput or tuple(tf.Tensor)
```

参数

+   `input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示未被`掩码`的标记，

    +   0 表示被`掩码`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]` 中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`tf.Tensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示头部未被`掩码`。

    +   0 表示头部被`掩码`。

+   `inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制权来将 `input_ids` 索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通的元组。

+   `labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于计算标记分类损失的标签。索引应在 `[0, ..., config.num_labels - 1]` 中。

返回

[transformers.modeling_tf_outputs.TFTokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput) 或 `tuple(tf.Tensor)`

一个 [transformers.modeling_tf_outputs.TFTokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput) 或一个 `tf.Tensor` 元组（如果传递了 `return_dict=False` 或 `config.return_dict=False`）包含各种元素，这取决于配置（[EsmConfig](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.EsmConfig)）和输入。

+   `loss` (`tf.Tensor` of shape `(n,)`, *optional*, 当提供 `labels` 时返回，其中 n 是未被掩码的标签数) — 分类损失。

+   `logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.num_labels)`) — 分类分数（SoftMax 之前）。

+   `hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `tf.Tensor` 元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态以及初始嵌入输出。

+   `attentions` (`tuple(tf.Tensor)`, *可选的*, 当传递 `output_attentions=True` 或 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `tf.Tensor` 元组（每层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

[TFEsmForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/esm#transformers.TFEsmForTokenClassification) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在这个函数中定义，但应该在此之后调用 `Module` 实例，而不是这个函数，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, TFEsmForTokenClassification
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/esm2_t6_8M_UR50D")
>>> model = TFEsmForTokenClassification.from_pretrained("facebook/esm2_t6_8M_UR50D")

>>> inputs = tokenizer(
...     "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="tf"
... )

>>> logits = model(**inputs).logits
>>> predicted_token_class_ids = tf.math.argmax(logits, axis=-1)

>>> # Note that tokens are classified rather then input words which means that
>>> # there might be more predicted token classes than words.
>>> # Multiple token classes might account for the same word
>>> predicted_tokens_classes = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]
```

```py
>>> labels = predicted_token_class_ids
>>> loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)
```
