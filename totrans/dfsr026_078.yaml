- en: Reduce memory usage
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‡å°‘å†…å­˜ä½¿ç”¨
- en: 'Original text: [https://huggingface.co/docs/diffusers/optimization/memory](https://huggingface.co/docs/diffusers/optimization/memory)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/optimization/memory](https://huggingface.co/docs/diffusers/optimization/memory)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: A barrier to using diffusion models is the large amount of memory required.
    To overcome this challenge, there are several memory-reducing techniques you can
    use to run even some of the largest models on free-tier or consumer GPUs. Some
    of these techniques can even be combined to further reduce memory usage.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„éšœç¢æ˜¯æ‰€éœ€çš„å¤§é‡å†…å­˜ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å‡ ç§å‡å°‘å†…å­˜çš„æŠ€æœ¯ï¼Œç”šè‡³å¯ä»¥åœ¨å…è´¹æˆ–æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œä¸€äº›æœ€å¤§çš„æ¨¡å‹ã€‚æœ‰äº›æŠ€æœ¯ç”šè‡³å¯ä»¥ç»“åˆä½¿ç”¨ä»¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚
- en: In many cases, optimizing for memory or speed leads to improved performance
    in the other, so you should try to optimize for both whenever you can. This guide
    focuses on minimizing memory usage, but you can also learn more about how to [Speed
    up inference](fp16).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œä¼˜åŒ–å†…å­˜æˆ–é€Ÿåº¦ä¼šå¯¼è‡´å¦ä¸€ä¸ªæ–¹é¢çš„æ€§èƒ½æé«˜ï¼Œå› æ­¤æ‚¨åº”è¯¥å°½å¯èƒ½åŒæ—¶ä¼˜åŒ–ä¸¤è€…ã€‚æœ¬æŒ‡å—ä¾§é‡äºæœ€å°åŒ–å†…å­˜ä½¿ç”¨ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥äº†è§£æœ‰å…³å¦‚ä½•[åŠ é€Ÿæ¨æ–­](fp16)çš„æ›´å¤šä¿¡æ¯ã€‚
- en: The results below are obtained from generating a single 512x512 image from the
    prompt a photo of an astronaut riding a horse on mars with 50 DDIM steps on a
    Nvidia Titan RTX, demonstrating the speed-up you can expect as a result of reduced
    memory consumption.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ç»“æœæ˜¯ä»åœ¨Nvidia Titan RTXä¸Šä½¿ç”¨50ä¸ªDDIMæ­¥éª¤ä»ä¸€ä¸ªåœ¨ç«æ˜Ÿä¸Šéª‘é©¬çš„å®‡èˆªå‘˜çš„ç…§ç‰‡ç”Ÿæˆå•ä¸ª512x512å›¾åƒè·å¾—çš„ï¼Œå±•ç¤ºäº†ç”±äºå‡å°‘å†…å­˜æ¶ˆè€—è€Œå¯ä»¥æœŸæœ›çš„åŠ é€Ÿã€‚
- en: '|  | latency | speed-up |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '|  | å»¶è¿Ÿ | åŠ é€Ÿ |'
- en: '| --- | --- | --- |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| original | 9.50s | x1 |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| åŸå§‹ | 9.50ç§’ | x1 |'
- en: '| fp16 | 3.61s | x2.63 |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| fp16 | 3.61ç§’ | x2.63 |'
- en: '| channels last | 3.30s | x2.88 |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| é€šé“æœ€å | 3.30ç§’ | x2.88 |'
- en: '| traced UNet | 3.21s | x2.96 |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| è¿½è¸ªçš„UNet | 3.21ç§’ | x2.96 |'
- en: '| memory-efficient attention | 2.63s | x3.61 |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| å†…å­˜é«˜æ•ˆæ³¨æ„åŠ› | 2.63ç§’ | x3.61 |'
- en: Sliced VAE
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ‡ç‰‡çš„VAE
- en: Sliced VAE enables decoding large batches of images with limited VRAM or batches
    with 32 images or more by decoding the batches of latents one image at a time.
    Youâ€™ll likely want to couple this with [enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.enable_xformers_memory_efficient_attention)
    to reduce memory use further if you have xFormers installed.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ‡ç‰‡çš„VAEé€šè¿‡é€ä¸ªè§£ç æ½œåœ¨çš„æ‰¹é‡å›¾åƒæˆ–åŒ…å«32å¼ æˆ–æ›´å¤šå›¾åƒçš„æ‰¹æ¬¡æ¥è§£ç å¤§æ‰¹é‡å›¾åƒï¼Œä»è€Œä½¿æœ‰é™çš„VRAMæˆ–æ‰¹æ¬¡å¤„ç†æ›´å®¹æ˜“ã€‚å¦‚æœæ‚¨å®‰è£…äº†xFormersï¼Œæ‚¨å¯èƒ½è¿˜æƒ³å°†å…¶ä¸[enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.enable_xformers_memory_efficient_attention)ç»“åˆä½¿ç”¨ä»¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚
- en: 'To use sliced VAE, call [enable_vae_slicing()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline.enable_vae_slicing)
    on your pipeline before inference:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨åˆ‡ç‰‡çš„VAEï¼Œè¯·åœ¨æ¨æ–­ä¹‹å‰åœ¨æ‚¨çš„ç®¡é“ä¸Šè°ƒç”¨[enable_vae_slicing()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline.enable_vae_slicing)ï¼š
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You may see a small performance boost in VAE decoding on multi-image batches,
    and there should be no performance impact on single-image batches.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤šå›¾åƒæ‰¹æ¬¡ä¸Šè¿›è¡ŒVAEè§£ç å¯èƒ½ä¼šå¸¦æ¥è½»å¾®çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¸”å¯¹äºå•å›¾åƒæ‰¹æ¬¡ä¸åº”è¯¥æœ‰æ€§èƒ½å½±å“ã€‚
- en: Tiled VAE
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¹³é“ºçš„VAE
- en: Tiled VAE processing also enables working with large images on limited VRAM
    (for example, generating 4k images on 8GB of VRAM) by splitting the image into
    overlapping tiles, decoding the tiles, and then blending the outputs together
    to compose the final image. You should also used tiled VAE with [enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.enable_xformers_memory_efficient_attention)
    to reduce memory use further if you have xFormers installed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¹³é“ºçš„VAEå¤„ç†è¿˜å¯ä»¥é€šè¿‡å°†å›¾åƒåˆ†æˆé‡å çš„ç“¦ç‰‡ï¼Œè§£ç ç“¦ç‰‡ï¼Œç„¶åå°†è¾“å‡ºæ··åˆåœ¨ä¸€èµ·æ¥ç»„æˆæœ€ç»ˆå›¾åƒï¼Œä»è€Œåœ¨æœ‰é™çš„VRAMä¸Šå¤„ç†å¤§å›¾åƒï¼ˆä¾‹å¦‚ï¼Œåœ¨8GBçš„VRAMä¸Šç”Ÿæˆ4kå›¾åƒï¼‰ã€‚å¦‚æœæ‚¨å®‰è£…äº†xFormersï¼Œè¿˜åº”è¯¥ä½¿ç”¨[enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.enable_xformers_memory_efficient_attention)æ¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚
- en: 'To use tiled VAE processing, call [enable_vae_tiling()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline.enable_vae_tiling)
    on your pipeline before inference:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨å¹³é“ºçš„VAEå¤„ç†ï¼Œè¯·åœ¨æ¨æ–­ä¹‹å‰åœ¨æ‚¨çš„ç®¡é“ä¸Šè°ƒç”¨[enable_vae_tiling()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline.enable_vae_tiling)ï¼š
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The output image has some tile-to-tile tone variation because the tiles are
    decoded separately, but you shouldnâ€™t see any sharp and obvious seams between
    the tiles. Tiling is turned off for images that are 512x512 or smaller.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå›¾åƒå­˜åœ¨ä¸€äº›ç“¦ç‰‡ä¹‹é—´çš„è‰²è°ƒå˜åŒ–ï¼Œå› ä¸ºç“¦ç‰‡æ˜¯åˆ†å¼€è§£ç çš„ï¼Œä½†æ‚¨ä¸åº”è¯¥çœ‹åˆ°ç“¦ç‰‡ä¹‹é—´çš„æ˜æ˜¾ç¼éš™ã€‚å¯¹äºå°ºå¯¸ä¸º512x512æˆ–æ›´å°çš„å›¾åƒï¼Œç“¦ç‰‡åŠŸèƒ½å·²å…³é—­ã€‚
- en: CPU offloading
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CPUå¸è½½
- en: Offloading the weights to the CPU and only loading them on the GPU when performing
    the forward pass can also save memory. Often, this technique can reduce memory
    consumption to less than 3GB.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æƒé‡å¸è½½åˆ°CPUï¼Œå¹¶ä»…åœ¨æ‰§è¡Œå‰å‘ä¼ é€’æ—¶åŠ è½½å®ƒä»¬åˆ°GPUä¸Šä¹Ÿå¯ä»¥èŠ‚çœå†…å­˜ã€‚é€šå¸¸ï¼Œè¿™ç§æŠ€æœ¯å¯ä»¥å°†å†…å­˜æ¶ˆè€—å‡å°‘åˆ°ä¸åˆ°3GBã€‚
- en: 'To perform CPU offloading, call [enable_sequential_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/latent_upscale#diffusers.StableDiffusionLatentUpscalePipeline.enable_sequential_cpu_offload):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¦æ‰§è¡ŒCPUå¸è½½ï¼Œè¯·åœ¨æ¨æ–­ä¹‹å‰åœ¨æ‚¨çš„ç®¡é“ä¸Šè°ƒç”¨[enable_sequential_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/latent_upscale#diffusers.StableDiffusionLatentUpscalePipeline.enable_sequential_cpu_offload):'
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: CPU offloading works on submodules rather than whole models. This is the best
    way to minimize memory consumption, but inference is much slower due to the iterative
    nature of the diffusion process. The UNet component of the pipeline runs several
    times (as many as `num_inference_steps`); each time, the different UNet submodules
    are sequentially onloaded and offloaded as needed, resulting in a large number
    of memory transfers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: CPUå¸è½½é€‚ç”¨äºå­æ¨¡å—è€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹ã€‚è¿™æ˜¯æœ€å°åŒ–å†…å­˜æ¶ˆè€—çš„æœ€ä½³æ–¹æ³•ï¼Œä½†ç”±äºæ‰©æ•£è¿‡ç¨‹çš„è¿­ä»£æ€§è´¨ï¼Œæ¨æ–­é€Ÿåº¦è¦æ…¢å¾—å¤šã€‚ç®¡é“çš„UNetç»„ä»¶è¿è¡Œå¤šæ¬¡ï¼ˆæœ€å¤š`num_inference_steps`æ¬¡ï¼‰ï¼›æ¯æ¬¡ï¼Œä¸åŒçš„UNetå­æ¨¡å—æ ¹æ®éœ€è¦é¡ºåºåŠ è½½å’Œå¸è½½ï¼Œå¯¼è‡´å¤§é‡çš„å†…å­˜ä¼ è¾“ã€‚
- en: Consider using [model offloading](#model-offloading) if you want to optimize
    for speed because it is much faster. The tradeoff is your memory savings wonâ€™t
    be as large.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³è¦ä¼˜åŒ–é€Ÿåº¦ï¼Œè€ƒè™‘ä½¿ç”¨[model offloading](#model-offloading)ï¼Œå› ä¸ºå®ƒé€Ÿåº¦æ›´å¿«ã€‚æƒè¡¡æ˜¯æ‚¨çš„å†…å­˜èŠ‚çœé‡ä¸ä¼šé‚£ä¹ˆå¤§ã€‚
- en: When using [enable_sequential_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/latent_upscale#diffusers.StableDiffusionLatentUpscalePipeline.enable_sequential_cpu_offload),
    donâ€™t move the pipeline to CUDA beforehand or else the gain in memory consumption
    will only be minimal (see this [issue](https://github.com/huggingface/diffusers/issues/1934)
    for more information).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨[enable_sequential_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/latent_upscale#diffusers.StableDiffusionLatentUpscalePipeline.enable_sequential_cpu_offload)æ—¶ï¼Œä¸è¦äº‹å…ˆå°†ç®¡é“ç§»åŠ¨åˆ°CUDAï¼Œå¦åˆ™å†…å­˜æ¶ˆè€—çš„å¢ç›Šå°†åªæ˜¯å¾®ä¸è¶³é“çš„ï¼ˆæœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§æ­¤[é—®é¢˜](https://github.com/huggingface/diffusers/issues/1934)ï¼‰ã€‚
- en: '[enable_sequential_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/latent_upscale#diffusers.StableDiffusionLatentUpscalePipeline.enable_sequential_cpu_offload)
    is a stateful operation that installs hooks on the models.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[enable_sequential_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/latent_upscale#diffusers.StableDiffusionLatentUpscalePipeline.enable_sequential_cpu_offload)æ˜¯ä¸€ä¸ªæœ‰çŠ¶æ€çš„æ“ä½œï¼Œå®ƒåœ¨æ¨¡å‹ä¸Šå®‰è£…é’©å­ã€‚'
- en: Model offloading
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¸è½½
- en: Model offloading requires ğŸ¤— Accelerate version 0.17.0 or higher.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¸è½½éœ€è¦ğŸ¤— Accelerateç‰ˆæœ¬0.17.0æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚
- en: '[Sequential CPU offloading](#cpu-offloading) preserves a lot of memory but
    it makes inference slower because submodules are moved to GPU as needed, and theyâ€™re
    immediately returned to the CPU when a new module runs.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sequential CPU offloading](#cpu-offloading)ä¿ç•™äº†å¤§é‡å†…å­˜ï¼Œä½†ä½¿æ¨ç†é€Ÿåº¦å˜æ…¢ï¼Œå› ä¸ºå­æ¨¡å—åœ¨éœ€è¦æ—¶ç§»åŠ¨åˆ°GPUï¼Œå¹¶åœ¨æ–°æ¨¡å—è¿è¡Œæ—¶ç«‹å³è¿”å›åˆ°CPUã€‚'
- en: Full-model offloading is an alternative that moves whole models to the GPU,
    instead of handling each modelâ€™s constituent *submodules*. There is a negligible
    impact on inference time (compared with moving the pipeline to `cuda`), and it
    still provides some memory savings.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæ•´æ¨¡å‹å¸è½½æ˜¯ä¸€ç§å°†æ•´ä¸ªæ¨¡å‹ç§»åŠ¨åˆ°GPUçš„æ›¿ä»£æ–¹æ³•ï¼Œè€Œä¸æ˜¯å¤„ç†æ¯ä¸ªæ¨¡å‹çš„ç»„æˆ*å­æ¨¡å—*ã€‚ä¸å°†ç®¡é“ç§»åŠ¨åˆ°`cuda`ç›¸æ¯”ï¼Œå¯¹æ¨ç†æ—¶é—´å‡ ä¹æ²¡æœ‰å½±å“ï¼Œå¹¶ä¸”ä»ç„¶æä¾›ä¸€äº›å†…å­˜èŠ‚çœã€‚
- en: During model offloading, only one of the main components of the pipeline (typically
    the text encoder, UNet and VAE) is placed on the GPU while the others wait on
    the CPU. Components like the UNet that run for multiple iterations stay on the
    GPU until theyâ€™re no longer needed.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨¡å‹å¸è½½æœŸé—´ï¼Œç®¡é“çš„ä¸»è¦ç»„ä»¶ä¹‹ä¸€ï¼ˆé€šå¸¸æ˜¯æ–‡æœ¬ç¼–ç å™¨ã€UNetå’ŒVAEï¼‰è¢«æ”¾ç½®åœ¨GPUä¸Šï¼Œè€Œå…¶ä»–ç»„ä»¶åˆ™åœ¨CPUä¸Šç­‰å¾…ã€‚åƒUNetè¿™æ ·è¿è¡Œå¤šæ¬¡è¿­ä»£çš„ç»„ä»¶ä¼šä¸€ç›´ä¿ç•™åœ¨GPUä¸Šï¼Œç›´åˆ°ä¸å†éœ€è¦ä¸ºæ­¢ã€‚
- en: 'Enable model offloading by calling [enable_model_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/gligen#diffusers.StableDiffusionGLIGENTextImagePipeline.enable_model_cpu_offload)
    on the pipeline:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åœ¨ç®¡é“ä¸Šè°ƒç”¨[enable_model_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/gligen#diffusers.StableDiffusionGLIGENTextImagePipeline.enable_model_cpu_offload)å¯ç”¨æ¨¡å‹å¸è½½ï¼š
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In order to properly offload models after theyâ€™re called, it is required to
    run the entire pipeline and models are called in the pipelineâ€™s expected order.
    Exercise caution if models are reused outside the context of the pipeline after
    hooks have been installed. See [Removing Hooks](https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.hooks.remove_hook_from_module)
    for more information.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åœ¨è°ƒç”¨æ¨¡å‹åæ­£ç¡®å¸è½½æ¨¡å‹ï¼Œéœ€è¦è¿è¡Œæ•´ä¸ªç®¡é“ï¼Œå¹¶æŒ‰ç…§ç®¡é“çš„é¢„æœŸé¡ºåºè°ƒç”¨æ¨¡å‹ã€‚å¦‚æœåœ¨å®‰è£…é’©å­ååœ¨ç®¡é“ä¸Šä¸‹æ–‡ä¹‹å¤–é‡ç”¨æ¨¡å‹ï¼Œè¯·è°¨æ…è¡Œäº‹ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[Removing
    Hooks](https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.hooks.remove_hook_from_module)ã€‚
- en: '[enable_model_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/gligen#diffusers.StableDiffusionGLIGENTextImagePipeline.enable_model_cpu_offload)
    is a stateful operation that installs hooks on the models and state on the pipeline.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[enable_model_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/gligen#diffusers.StableDiffusionGLIGENTextImagePipeline.enable_model_cpu_offload)
    æ˜¯ä¸€ä¸ªæœ‰çŠ¶æ€çš„æ“ä½œï¼Œå®ƒåœ¨æ¨¡å‹ä¸Šå®‰è£…é’©å­ï¼Œå¹¶åœ¨ç®¡é“ä¸Šå®‰è£…çŠ¶æ€ã€‚'
- en: Channels-last memory format
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€šé“æœ€åçš„å†…å­˜æ ¼å¼
- en: The channels-last memory format is an alternative way of ordering NCHW tensors
    in memory to preserve dimension ordering. Channels-last tensors are ordered in
    such a way that the channels become the densest dimension (storing images pixel-per-pixel).
    Since not all operators currently support the channels-last format, it may result
    in worst performance but you should still try and see if it works for your model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: é€šé“æœ€åçš„å†…å­˜æ ¼å¼æ˜¯åœ¨å†…å­˜ä¸­å¯¹NCHWå¼ é‡è¿›è¡Œæ’åºçš„å¦ä¸€ç§æ–¹å¼ï¼Œä»¥ä¿ç•™ç»´åº¦æ’åºã€‚é€šé“æœ€åçš„å¼ é‡æ˜¯æŒ‰ç…§é€šé“æˆä¸ºæœ€å¯†é›†ç»´åº¦çš„æ–¹å¼æ’åºçš„ï¼ˆä»¥åƒç´ ä¸ºå•ä½å­˜å‚¨å›¾åƒï¼‰ã€‚ç”±äºç›®å‰å¹¶éæ‰€æœ‰è¿ç®—ç¬¦éƒ½æ”¯æŒé€šé“æœ€åçš„æ ¼å¼ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½è¾ƒå·®ï¼Œä½†æ‚¨ä»åº”å°è¯•å¹¶æŸ¥çœ‹å®ƒæ˜¯å¦é€‚ç”¨äºæ‚¨çš„æ¨¡å‹ã€‚
- en: 'For example, to set the pipelineâ€™s UNet to use the channels-last format:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¦å°†ç®¡é“çš„UNetè®¾ç½®ä¸ºä½¿ç”¨é€šé“æœ€åçš„æ ¼å¼ï¼š
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Tracing
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è·Ÿè¸ª
- en: Tracing runs an example input tensor through the model and captures the operations
    that are performed on it as that input makes its way through the modelâ€™s layers.
    The executable or `ScriptFunction` that is returned is optimized with just-in-time
    compilation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è·Ÿè¸ªé€šè¿‡æ¨¡å‹çš„ç¤ºä¾‹è¾“å…¥å¼ é‡ï¼Œå¹¶æ•è·åœ¨å…¶é€šè¿‡æ¨¡å‹çš„å±‚æ—¶æ‰§è¡Œçš„æ“ä½œã€‚è¿”å›çš„å¯æ‰§è¡Œæ–‡ä»¶æˆ–`ScriptFunction`ç»è¿‡å³æ—¶ç¼–è¯‘è¿›è¡Œäº†ä¼˜åŒ–ã€‚
- en: 'To trace a UNet:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è·Ÿè¸ªä¸€ä¸ªUNetï¼š
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Replace the `unet` attribute of the pipeline with the traced model:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨è·Ÿè¸ªçš„æ¨¡å‹æ›¿æ¢ç®¡é“çš„`unet`å±æ€§ï¼š
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Memory-efficient attention
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›
- en: Recent work on optimizing bandwidth in the attention block has generated huge
    speed-ups and reductions in GPU memory usage. The most recent type of memory-efficient
    attention is [Flash Attention](https://arxiv.org/abs/2205.14135) (you can check
    out the original code at [HazyResearch/flash-attention](https://github.com/HazyResearch/flash-attention)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘å…³äºåœ¨æ³¨æ„åŠ›å—ä¸­ä¼˜åŒ–å¸¦å®½çš„å·¥ä½œäº§ç”Ÿäº†å·¨å¤§çš„åŠ é€Ÿå’ŒGPUå†…å­˜ä½¿ç”¨é‡çš„å‡å°‘ã€‚æœ€æ–°çš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ç±»å‹æ˜¯[Flash Attention](https://arxiv.org/abs/2205.14135)ï¼ˆæ‚¨å¯ä»¥åœ¨[HazyResearch/flash-attention](https://github.com/HazyResearch/flash-attention)æŸ¥çœ‹åŸå§‹ä»£ç ï¼‰ã€‚
- en: If you have PyTorch >= 2.0 installed, you should not expect a speed-up for inference
    when enabling `xformers`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå·²å®‰è£…PyTorch >= 2.0ï¼Œåˆ™åœ¨å¯ç”¨`xformers`æ—¶ä¸åº”æœŸæœ›æ¨ç†é€Ÿåº¦æå‡ã€‚
- en: 'To use Flash Attention, install the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Flash Attentionï¼Œå®‰è£…ä»¥ä¸‹å†…å®¹ï¼š
- en: PyTorch > 1.12
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch > 1.12
- en: CUDA available
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDAå¯ç”¨
- en: '[xFormers](xformers)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[xFormers](xformers)'
- en: 'Then call [enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.enable_xformers_memory_efficient_attention)
    on the pipeline:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨ç®¡é“ä¸Šè°ƒç”¨[enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.enable_xformers_memory_efficient_attention)ï¼š
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The iteration speed when using `xformers` should match the iteration speed of
    PyTorch 2.0 as described [here](torch2.0).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`xformers`æ—¶çš„è¿­ä»£é€Ÿåº¦åº”è¯¥ä¸PyTorch 2.0çš„è¿­ä»£é€Ÿåº¦ç›¸åŒ¹é…ï¼Œå¦‚[torch2.0](torch2.0)ä¸­æ‰€è¿°ã€‚
