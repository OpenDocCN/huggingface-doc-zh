["```py\n# uncomment to install the necessary libraries in Colab\n#!pip install -q diffusers transformers accelerate\n```", "```py\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipeline = pipeline.to(\"cuda\")\n```", "```py\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\n\npipeline = StableDiffusionXLPipeline.from_single_file(\n    \"https://huggingface.co/stabilityai/sdxl-turbo/blob/main/sd_xl_turbo_1.0_fp16.safetensors\", torch_dtype=torch.float16)\npipeline = pipeline.to(\"cuda\")\n```", "```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline_text2image = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipeline_text2image = pipeline_text2image.to(\"cuda\")\n\nprompt = \"A cinematic shot of a baby racoon wearing an intricate italian priest robe.\"\n\nimage = pipeline_text2image(prompt=prompt, guidance_scale=0.0, num_inference_steps=1).images[0]\nimage\n```", "```py\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import load_image, make_image_grid\n\n# use from_pipe to avoid consuming additional memory when loading a checkpoint\npipeline = AutoPipelineForImage2Image.from_pipe(pipeline_text2image).to(\"cuda\")\n\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\")\ninit_image = init_image.resize((512, 512))\n\nprompt = \"cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k\"\n\nimage = pipeline(prompt, image=init_image, strength=0.5, guidance_scale=0.0, num_inference_steps=2).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```", "```py\npipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n```", "```py\npipe.upcast_vae()\n```"]