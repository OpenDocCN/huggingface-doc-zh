- en: Utilities for Trainer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/internal/trainer_utils](https://huggingface.co/docs/transformers/v4.37.2/en/internal/trainer_utils)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/33.61f676ee.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: This page lists all the utility functions used by [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer).
  prefs: []
  type: TYPE_NORMAL
- en: Most of those are only useful if you are studying the code of the Trainer in
    the library.
  prefs: []
  type: TYPE_NORMAL
- en: Utilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.EvalPrediction'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_utils.py#L134)'
  prefs: []
  type: TYPE_NORMAL
- en: '( predictions: Union label_ids: Union inputs: Union = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**predictions** (`np.ndarray`) — Predictions of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**label_ids** (`np.ndarray`) — Targets to be matched.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs** (`np.ndarray`, *optional*) —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation output (always contains labels), to be used to compute metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '### class transformers.IntervalStrategy'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_utils.py#L208)'
  prefs: []
  type: TYPE_NORMAL
- en: ( value names = None module = None qualname = None type = None start = 1 )
  prefs: []
  type: TYPE_NORMAL
- en: An enumeration.
  prefs: []
  type: TYPE_NORMAL
- en: '#### transformers.enable_full_determinism'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_utils.py#L58)'
  prefs: []
  type: TYPE_NORMAL
- en: '( seed: int warn_only: bool = False )'
  prefs: []
  type: TYPE_NORMAL
- en: Helper function for reproducible behavior during distributed training. See
  prefs: []
  type: TYPE_NORMAL
- en: '[https://pytorch.org/docs/stable/notes/randomness.html](https://pytorch.org/docs/stable/notes/randomness.html)
    for pytorch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism](https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism)
    for tensorflow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### transformers.set_seed'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_utils.py#L85)'
  prefs: []
  type: TYPE_NORMAL
- en: '( seed: int )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**seed** (`int`) — The seed to set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helper function for reproducible behavior to set the seed in `random`, `numpy`,
    `torch` and/or `tf` (if installed).
  prefs: []
  type: TYPE_NORMAL
- en: '#### transformers.torch_distributed_zero_first'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L242)'
  prefs: []
  type: TYPE_NORMAL
- en: '( local_rank: int )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**local_rank** (`int`) — The rank of the local process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decorator to make all processes in distributed training wait for each local_master
    to do something.
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks internals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.trainer_callback.CallbackHandler'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_callback.py#L307)'
  prefs: []
  type: TYPE_NORMAL
- en: ( callbacks model tokenizer optimizer lr_scheduler )
  prefs: []
  type: TYPE_NORMAL
- en: Internal class that just calls the list of callbacks in order.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.trainer_pt_utils.DistributedTensorGatherer'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L371)'
  prefs: []
  type: TYPE_NORMAL
- en: ( world_size num_samples make_multiple_of = None padding_index = -100 )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**world_size** (`int`) — The number of processes used in the distributed training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_samples** (`int`) — The number of samples in our dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**make_multiple_of** (`int`, *optional*) — If passed, the class assumes the
    datasets passed to each process are made to be a multiple of this argument (by
    adding samples).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**padding_index** (`int`, *optional*, defaults to -100) — The padding index
    to use if the arrays don’t all have the same sequence length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A class responsible for properly gathering tensors (or nested list/tuple of
    tensors) on the CPU by chunks.
  prefs: []
  type: TYPE_NORMAL
- en: 'If our dataset has 16 samples with a batch size of 2 on 3 processes and we
    gather then transfer on CPU at every step, our sampler will generate the following
    indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'to get something of size a multiple of 3 (so that each process gets the same
    dataset length). Then process 0, 1 and 2 will be responsible of making predictions
    for the following samples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'P0: `[0, 1, 2, 3, 4, 5]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P1: `[6, 7, 8, 9, 10, 11]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P2: `[12, 13, 14, 15, 0, 1]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first batch treated on each process will be
  prefs: []
  type: TYPE_NORMAL
- en: 'P0: `[0, 1]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P1: `[6, 7]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P2: `[12, 13]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So if we gather at the end of the first batch, we will get a tensor (nested
    list/tuple of tensor) corresponding to the following indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[0, 1, 6, 7, 12, 13]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we directly concatenate our results without taking any precautions, the
    user will then get the predictions for the indices in this order at the end of
    the prediction loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[0, 1, 6, 7, 12, 13, 2, 3, 8, 9, 14, 15, 4, 5, 10, 11, 0, 1]`'
  prefs: []
  type: TYPE_NORMAL
- en: For some reason, that’s not going to roll their boat. This class is there to
    solve that problem.
  prefs: []
  type: TYPE_NORMAL
- en: '#### add_arrays'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L431)'
  prefs: []
  type: TYPE_NORMAL
- en: ( arrays )
  prefs: []
  type: TYPE_NORMAL
- en: Add `arrays` to the internal storage, Will initialize the storage to the full
    size at the first arrays passed so that if we’re bound to get an OOM, it happens
    at the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: '#### finalize'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/trainer_pt_utils.py#L467)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Return the properly gathered arrays and truncate to the number of samples (since
    the sampler added some extras to get each process a dataset of the same length).
  prefs: []
  type: TYPE_NORMAL
- en: Trainer Argument Parser
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.HfArgumentParser'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/hf_argparser.py#L109)'
  prefs: []
  type: TYPE_NORMAL
- en: '( dataclass_types: Union **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: This subclass of `argparse.ArgumentParser` uses type hints on dataclasses to
    generate arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The class is designed to play well with the native argparse. In particular,
    you can add more (non-dataclass backed) arguments to the parser after initialization
    and you’ll get the output back after parsing as an additional namespace. Optional:
    To create sub argument groups use the `_argument_group_name` attribute in the
    dataclass.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### parse_args_into_dataclasses'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/hf_argparser.py#L265)'
  prefs: []
  type: TYPE_NORMAL
- en: ( args = None return_remaining_strings = False look_for_args_file = True args_filename
    = None args_file_flag = None ) → Tuple consisting of
  prefs: []
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: Tuple consisting of
  prefs: []
  type: TYPE_NORMAL
- en: the dataclass instances in the same order as they were passed to the initializer.abspath
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if applicable, an additional namespace for more (non-dataclass backed) arguments
    added to the parser after initialization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The potential list of remaining argument strings. (same as argparse.ArgumentParser.parse_known_args)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parse command-line args into instances of the specified dataclass types.
  prefs: []
  type: TYPE_NORMAL
- en: 'This relies on argparse’s `ArgumentParser.parse_known_args`. See the doc at:
    docs.python.org/3.7/library/argparse.html#argparse.ArgumentParser.parse_args'
  prefs: []
  type: TYPE_NORMAL
- en: '#### parse_dict'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/hf_argparser.py#L351)'
  prefs: []
  type: TYPE_NORMAL
- en: '( args: Dict allow_extra_keys: bool = False ) → Tuple consisting of'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**args** (`dict`) — dict containing config values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**allow_extra_keys** (`bool`, *optional*, defaults to `False`) — Defaults to
    False. If False, will raise an exception if the dict contains keys that are not
    parsed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: Tuple consisting of
  prefs: []
  type: TYPE_NORMAL
- en: the dataclass instances in the same order as they were passed to the initializer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternative helper method that does not use `argparse` at all, instead uses
    a dict and populating the dataclass types.
  prefs: []
  type: TYPE_NORMAL
- en: '#### parse_json_file'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/hf_argparser.py#L379)'
  prefs: []
  type: TYPE_NORMAL
- en: '( json_file: str allow_extra_keys: bool = False ) → Tuple consisting of'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**json_file** (`str` or `os.PathLike`) — File name of the json file to parse'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**allow_extra_keys** (`bool`, *optional*, defaults to `False`) — Defaults to
    False. If False, will raise an exception if the json file contains keys that are
    not parsed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: Tuple consisting of
  prefs: []
  type: TYPE_NORMAL
- en: the dataclass instances in the same order as they were passed to the initializer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternative helper method that does not use `argparse` at all, instead loading
    a json file and populating the dataclass types.
  prefs: []
  type: TYPE_NORMAL
- en: '#### parse_yaml_file'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/hf_argparser.py#L401)'
  prefs: []
  type: TYPE_NORMAL
- en: '( yaml_file: str allow_extra_keys: bool = False ) → Tuple consisting of'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**yaml_file** (`str` or `os.PathLike`) — File name of the yaml file to parse'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**allow_extra_keys** (`bool`, *optional*, defaults to `False`) — Defaults to
    False. If False, will raise an exception if the json file contains keys that are
    not parsed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: Tuple consisting of
  prefs: []
  type: TYPE_NORMAL
- en: the dataclass instances in the same order as they were passed to the initializer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternative helper method that does not use `argparse` at all, instead loading
    a yaml file and populating the dataclass types.
  prefs: []
  type: TYPE_NORMAL
- en: Debug Utilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.debug_utils.DebugUnderflowOverflow'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/debug_utils.py#L27)'
  prefs: []
  type: TYPE_NORMAL
- en: ( model max_frames_to_save = 21 trace_batch_nums = [] abort_after_batch_num
    = None )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**model** (`nn.Module`) — The model to debug.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_frames_to_save** (`int`, *optional*, defaults to 21) — How many frames
    back to record'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**trace_batch_nums(`List[int]`,** *optional*, defaults to `[]`) — Which batch
    numbers to trace (turns detection off)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**abort_after_batch_num** (`int“, *optional*) — Whether to abort after a certain
    batch number has finished'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This debug class helps detect and understand where the model starts getting
    very large or very small, and more importantly `nan` or `inf` weight and activation
    elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 2 working modes:'
  prefs: []
  type: TYPE_NORMAL
- en: Underflow/overflow detection (default)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specific batch absolute min/max tracing without detection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mode 1: Underflow/overflow detection'
  prefs: []
  type: TYPE_NORMAL
- en: 'To activate the underflow/overflow detection, initialize the object with the
    model :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: then run the training as normal and if `nan` or `inf` gets detected in at least
    one of the weight, input or output elements this module will throw an exception
    and will print `max_frames_to_save` frames that lead to this event, each frame
    reporting
  prefs: []
  type: TYPE_NORMAL
- en: the fully qualified module name plus the class name whose `forward` was run
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the absolute min and max value of all elements for each module weights, and
    the inputs and output
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, here is the header and the last few frames in detection report
    for `google/mt5-small` run in fp16
  prefs: []
  type: TYPE_NORMAL
- en: 'mixed precision :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can see here, that `T5DenseGatedGeluDense.forward` resulted in output activations,
    whose absolute max value was around 62.7K, which is very close to fp16’s top limit
    of 64K. In the next frame we have `Dropout` which renormalizes the weights, after
    it zeroed some of the elements, which pushes the absolute max value to more than
    64K, and we get an overlow.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see it’s the previous frames that we need to look into when the numbers
    start going into very large for fp16 numbers.
  prefs: []
  type: TYPE_NORMAL
- en: The tracking is done in a forward hook, which gets invoked immediately after
    `forward` has completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default the last 21 frames are printed. You can change the default to adjust
    for your needs. For example :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: To validate that you have set up this debugging feature correctly, and you intend
    to use it in a training that may take hours to complete, first run it with normal
    tracing enabled for one of a few batches as explained in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Mode 2\. Specific batch absolute min/max tracing without detection
  prefs: []
  type: TYPE_NORMAL
- en: The second work mode is per-batch tracing with the underflow/overflow detection
    feature turned off.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you want to watch the absolute min and max values for all the ingredients
    of each `forward` call of a
  prefs: []
  type: TYPE_NORMAL
- en: 'given batch, and only do that for batches 1 and 3\. Then you instantiate this
    class as :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: And now full batches 1 and 3 will be traced using the same format as explained
    above. Batches are 0-indexed.
  prefs: []
  type: TYPE_NORMAL
- en: This is helpful if you know that the program starts misbehaving after a certain
    batch number, so you can fast-forward right to that area.
  prefs: []
  type: TYPE_NORMAL
- en: 'Early stopping:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also specify the batch number after which to stop the training, with
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This feature is mainly useful in the tracing mode, but you can use it for any
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance**:'
  prefs: []
  type: TYPE_NORMAL
- en: As this module measures absolute `min`/``max` of each weight of the model on
    every forward it’ll slow the training down. Therefore remember to turn it off
    once the debugging needs have been met.
  prefs: []
  type: TYPE_NORMAL
