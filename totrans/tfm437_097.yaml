- en: The Transformer model family
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer模型家族
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_summary](https://huggingface.co/docs/transformers/v4.37.2/en/model_summary)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_summary](https://huggingface.co/docs/transformers/v4.37.2/en/model_summary)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Since its introduction in 2017, the [original Transformer](https://arxiv.org/abs/1706.03762)
    model has inspired many new and exciting models that extend beyond natural language
    processing (NLP) tasks. There are models for [predicting the folded structure
    of proteins](https://huggingface.co/blog/deep-learning-with-proteins), [training
    a cheetah to run](https://huggingface.co/blog/train-decision-transformers), and
    [time series forecasting](https://huggingface.co/blog/time-series-transformers).
    With so many Transformer variants available, it can be easy to miss the bigger
    picture. What all these models have in common is they’re based on the original
    Transformer architecture. Some models only use the encoder or decoder, while others
    use both. This provides a useful taxonomy to categorize and examine the high-level
    differences within models in the Transformer family, and it’ll help you understand
    Transformers you haven’t encountered before.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 自2017年引入[原始Transformer](https://arxiv.org/abs/1706.03762)模型以来，它已经激发了许多新颖且令人兴奋的模型，超越了自然语言处理（NLP）任务。有用于[预测蛋白质的折叠结构](https://huggingface.co/blog/deep-learning-with-proteins)、[训练猎豹奔跑](https://huggingface.co/blog/train-decision-transformers)和[时间序列预测](https://huggingface.co/blog/time-series-transformers)的模型。有这么多Transformer变体可用，很容易忽略更大的画面。所有这些模型的共同之处是它们都基于原始Transformer架构。一些模型只使用编码器或解码器，而其他一些则同时使用两者。这提供了一个有用的分类法，可以对Transformer家族中的模型进行分类和检查高层次的差异，这将帮助您理解以前未遇到的Transformer。
- en: If you aren’t familiar with the original Transformer model or need a refresher,
    check out the [How do Transformers work](https://huggingface.co/course/chapter1/4?fw=pt)
    chapter from the Hugging Face course.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉原始Transformer模型或需要复习，请查看Hugging Face课程中的[Transformer工作原理](https://huggingface.co/course/chapter1/4?fw=pt)章节。
- en: '[https://www.youtube.com/embed/H39Z_720T5s](https://www.youtube.com/embed/H39Z_720T5s)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/embed/H39Z_720T5s](https://www.youtube.com/embed/H39Z_720T5s)'
- en: Computer vision
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FacQBpeFBVvrDUlzFlkejoz%2FModelscape-timeline%3Fnode-id%3D0%253A1%26t%3Dm0zJ7m2BQ9oe0WtO-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FacQBpeFBVvrDUlzFlkejoz%2FModelscape-timeline%3Fnode-id%3D0%253A1%26t%3Dm0zJ7m2BQ9oe0WtO-1)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FacQBpeFBVvrDUlzFlkejoz%2FModelscape-timeline%3Fnode-id%3D0%253A1%26t%3Dm0zJ7m2BQ9oe0WtO-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FacQBpeFBVvrDUlzFlkejoz%2FModelscape-timeline%3Fnode-id%3D0%253A1%26t%3Dm0zJ7m2BQ9oe0WtO-1)'
- en: Convolutional network
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积网络
- en: For a long time, convolutional networks (CNNs) were the dominant paradigm for
    computer vision tasks until the [Vision Transformer](https://arxiv.org/abs/2010.11929)
    demonstrated its scalability and efficiency. Even then, some of a CNN’s best qualities,
    like translation invariance, are so powerful (especially for certain tasks) that
    some Transformers incorporate convolutions in their architecture. [ConvNeXt](model_doc/convnext)
    flipped this exchange around and incorporated design choices from Transformers
    to modernize a CNN. For example, ConvNeXt uses non-overlapping sliding windows
    to patchify an image and a larger kernel to increase its global receptive field.
    ConvNeXt also makes several layer design choices to be more memory-efficient and
    improve performance, so it competes favorably with Transformers!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 长时间以来，卷积网络（CNNs）一直是计算机视觉任务的主导范式，直到[视觉Transformer](https://arxiv.org/abs/2010.11929)展示了其可扩展性和效率。即使如此，CNN的一些最佳特性，如平移不变性，是如此强大（尤其对于某些任务），以至于一些Transformer在其架构中引入了卷积。[ConvNeXt](model_doc/convnext)颠倒了这种交换，并从Transformer中引入设计选择来现代化CNN。例如，ConvNeXt使用非重叠滑动窗口将图像分块化，并使用更大的内核来增加其全局感受野。ConvNeXt还做出了几个层设计选择，以提高内存效率和性能，因此它与Transformer竞争有利！
- en: Encoder
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器
- en: The [Vision Transformer (ViT)](model_doc/vit) opened the door to computer vision
    tasks without convolutions. ViT uses a standard Transformer encoder, but its main
    breakthrough was how it treated an image. It splits an image into fixed-size patches
    and uses them to create an embedding, just like how a sentence is split into tokens.
    ViT capitalized on the Transformers’ efficient architecture to demonstrate competitive
    results with the CNNs at the time while requiring fewer resources to train. ViT
    was soon followed by other vision models that could also handle dense vision tasks
    like segmentation as well as detection.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[视觉Transformer（ViT）](model_doc/vit)为计算机视觉任务打开了没有卷积的大门。ViT使用标准Transformer编码器，但其主要突破在于它如何处理图像。它将图像分割成固定大小的补丁，并使用它们创建嵌入，就像将句子分割成标记一样。ViT利用Transformer的高效架构展示了与当时的CNN竞争力的结果，同时需要更少的资源进行训练。ViT很快被其他视觉模型跟随，这些模型也可以处理像分割和检测这样的密集视觉任务。'
- en: One of these models is the [Swin](model_doc/swin) Transformer. It builds hierarchical
    feature maps (like a CNN 👀 and unlike ViT) from smaller-sized patches and merges
    them with neighboring patches in deeper layers. Attention is only computed within
    a local window, and the window is shifted between attention layers to create connections
    to help the model learn better. Since the Swin Transformer can produce hierarchical
    feature maps, it is a good candidate for dense prediction tasks like segmentation
    and detection. The [SegFormer](model_doc/segformer) also uses a Transformer encoder
    to build hierarchical feature maps, but it adds a simple multilayer perceptron
    (MLP) decoder on top to combine all the feature maps and make a prediction.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个模型是[Swin](model_doc/swin) Transformer。它从较小的补丁中构建分层特征图（类似于CNN👀，不同于ViT），并在更深层中将它们与相邻的补丁合并。注意力仅在局部窗口内计算，并且在注意力层之间移动窗口以创建连接以帮助模型学习更好。由于Swin
    Transformer可以生成分层特征图，因此它是密集预测任务（如分割和检测）的良好候选。[SegFormer](model_doc/segformer)
    也使用Transformer编码器构建分层特征图，但它在顶部添加了一个简单的多层感知器（MLP）解码器，以组合所有特征图并进行预测。
- en: Other vision models, like BeIT and ViTMAE, drew inspiration from BERT’s pretraining
    objective. [BeIT](model_doc/beit) is pretrained by *masked image modeling (MIM)*;
    the image patches are randomly masked, and the image is also tokenized into visual
    tokens. BeIT is trained to predict the visual tokens corresponding to the masked
    patches. [ViTMAE](model_doc/vitmae) has a similar pretraining objective, except
    it must predict the pixels instead of visual tokens. What’s unusual is 75% of
    the image patches are masked! The decoder reconstructs the pixels from the masked
    tokens and encoded patches. After pretraining, the decoder is thrown away, and
    the encoder is ready to be used in downstream tasks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其他视觉模型，如BeIT和ViTMAE，从BERT的预训练目标中汲取灵感。[BeIT](model_doc/beit) 通过*masked image
    modeling (MIM)* 进行预训练；图像补丁被随机屏蔽，图像也被标记为视觉标记。BeIT 被训练以预测与被屏蔽补丁对应的视觉标记。[ViTMAE](model_doc/vitmae)
    有一个类似的预训练目标，只是它必须预测像素而不是视觉标记。不寻常的是，75%的图像补丁被屏蔽！解码器从被屏蔽的标记和编码的补丁中重建像素。预训练后，解码器被丢弃，编码器准备好用于下游任务。
- en: Decoder
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码器
- en: Decoder-only vision models are rare because most vision models rely on an encoder
    to learn an image representation. But for use cases like image generation, the
    decoder is a natural fit, as we’ve seen from text generation models like GPT-2\.
    [ImageGPT](model_doc/imagegpt) uses the same architecture as GPT-2, but instead
    of predicting the next token in a sequence, it predicts the next pixel in an image.
    In addition to image generation, ImageGPT could also be finetuned for image classification.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 仅解码器的视觉模型很少，因为大多数视觉模型依赖编码器来学习图像表示。但对于像图像生成这样的用例，解码器是一个自然的选择，正如我们从GPT-2等文本生成模型中看到的那样。[ImageGPT](model_doc/imagegpt)
    使用与GPT-2相同的架构，但它不是预测序列中的下一个标记，而是预测图像中的下一个像素。除了图像生成，ImageGPT也可以进行微调以用于图像分类。
- en: Encoder-decoder
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器-解码器
- en: Vision models commonly use an encoder (also known as a backbone) to extract
    important image features before passing them to a Transformer decoder. [DETR](model_doc/detr)
    has a pretrained backbone, but it also uses the complete Transformer encoder-decoder
    architecture for object detection. The encoder learns image representations and
    combines them with object queries (each object query is a learned embedding that
    focuses on a region or object in an image) in the decoder. DETR predicts the bounding
    box coordinates and class label for each object query.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉模型通常使用编码器（也称为骨干）来提取重要的图像特征，然后将它们传递给Transformer解码器。[DETR](model_doc/detr) 有一个预训练的骨干，但它还使用完整的Transformer编码器-解码器架构进行目标检测。编码器学习图像表示，并将其与对象查询（每个对象查询是一个专注于图像中的区域或对象的学习嵌入）结合在解码器中。DETR
    预测每个对象查询的边界框坐标和类别标签。
- en: Natural language processing
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FUhbQAZDlpYW5XEpdFy6GoG%2Fnlp-model-timeline%3Fnode-id%3D0%253A1%26t%3D4mZMr4r1vDEYGJ50-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FUhbQAZDlpYW5XEpdFy6GoG%2Fnlp-model-timeline%3Fnode-id%3D0%253A1%26t%3D4mZMr4r1vDEYGJ50-1)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FUhbQAZDlpYW5XEpdFy6GoG%2Fnlp-model-timeline%3Fnode-id%3D0%253A1%26t%3D4mZMr4r1vDEYGJ50-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FUhbQAZDlpYW5XEpdFy6GoG%2Fnlp-model-timeline%3Fnode-id%3D0%253A1%26t%3D4mZMr4r1vDEYGJ50-1)'
- en: Encoder
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器
- en: '[BERT](model_doc/bert) is an encoder-only Transformer that randomly masks certain
    tokens in the input to avoid seeing other tokens, which would allow it to “cheat”.
    The pretraining objective is to predict the masked token based on the context.
    This allows BERT to fully use the left and right contexts to help it learn a deeper
    and richer representation of the inputs. However, there was still room for improvement
    in BERT’s pretraining strategy. [RoBERTa](model_doc/roberta) improved upon this
    by introducing a new pretraining recipe that includes training for longer and
    on larger batches, randomly masking tokens at each epoch instead of just once
    during preprocessing, and removing the next-sentence prediction objective.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[BERT](model_doc/bert) 是一个仅包含编码器的Transformer，它会随机屏蔽输入中的某些标记，以避免看到其他标记，这样可以防止其“作弊”。预训练的目标是基于上下文预测被屏蔽的标记。这使得BERT能够充分利用左右上下文来帮助学习输入的更深层和更丰富的表示。然而，BERT的预训练策略仍有改进的空间。[RoBERTa](model_doc/roberta)
    通过引入一个新的预训练配方来改进这一点，该配方包括更长时间和更大批次的训练，在每个时代随机屏蔽标记，而不仅仅是在预处理期间一次，以及移除下一个句子预测目标。'
- en: The dominant strategy to improve performance is to increase the model size.
    But training large models is computationally expensive. One way to reduce computational
    costs is using a smaller model like [DistilBERT](model_doc/distilbert). DistilBERT
    uses [knowledge distillation](https://arxiv.org/abs/1503.02531) - a compression
    technique - to create a smaller version of BERT while keeping nearly all of its
    language understanding capabilities.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 提高性能的主要策略是增加模型大小。但是训练大型模型在计算上是昂贵的。减少计算成本的一种方法是使用像[DistilBERT](model_doc/distilbert)这样的较小模型。DistilBERT使用[知识蒸馏](https://arxiv.org/abs/1503.02531)
    - 一种压缩技术 - 来创建一个较小版本的BERT，同时保留几乎所有的语言理解能力。
- en: 'However, most Transformer models continued to trend towards more parameters,
    leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert)
    reduces memory consumption by lowering the number of parameters in two ways: separating
    the larger vocabulary embedding into two smaller matrices and allowing layers
    to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention
    mechanism where the word and its position are separately encoded in two vectors.
    The attention is computed from these separate vectors instead of a single vector
    containing the word and position embeddings. [Longformer](model_doc/longformer)
    also focused on making attention more efficient, especially for processing documents
    with longer sequence lengths. It uses a combination of local windowed attention
    (attention only calculated from fixed window size around each token) and global
    attention (only for specific task tokens like `[CLS]` for classification) to create
    a sparse attention matrix instead of a full attention matrix.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数Transformer模型继续朝着更多参数的方向发展，导致出现了专注于提高训练效率的新模型。[ALBERT](model_doc/albert)通过两种方式降低参数数量来减少内存消耗：将更大的词汇嵌入分为两个较小的矩阵，并允许层共享参数。[DeBERTa](model_doc/deberta)添加了一个解耦的注意机制，其中单词及其位置分别编码在两个向量中。注意力是从这些单独的向量计算而来，而不是从包含单词和位置嵌入的单个向量中计算。[Longformer](model_doc/longformer)也专注于使注意力更加高效，特别是用于处理具有更长序列长度的文档。它使用局部窗口注意力（仅计算围绕每个标记的固定窗口大小的注意力）和全局注意力（仅用于特定任务标记，如`[CLS]`用于分类）的组合，以创建一个稀疏的注意力矩阵，而不是完整的注意力矩阵。
- en: Decoder
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码器
- en: '[GPT-2](model_doc/gpt2) is a decoder-only Transformer that predicts the next
    word in the sequence. It masks tokens to the right so the model can’t “cheat”
    by looking ahead. By pretraining on a massive body of text, GPT-2 became really
    good at generating text, even if the text is only sometimes accurate or true.
    But GPT-2 lacked the bidirectional context from BERT’s pretraining, which made
    it unsuitable for certain tasks. [XLNET](model_doc/xlnet) combines the best of
    both BERT and GPT-2’s pretraining objectives by using a permutation language modeling
    objective (PLM) that allows it to learn bidirectionally.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPT-2](model_doc/gpt2)是一个仅解码器的Transformer，用于预测序列中的下一个单词。它会屏蔽右侧的标记，以防模型通过向前查看来“作弊”。通过在大量文本上进行预训练，GPT-2在生成文本方面表现得非常出色，即使文本有时并不准确或真实。但是GPT-2缺乏BERT预训练的双向上下文，这使得它不适用于某些任务。[XLNET](model_doc/xlnet)结合了BERT和GPT-2的预训练目标的优点，使用排列语言建模目标（PLM）使其能够双向学习。'
- en: After GPT-2, language models grew even bigger and are now known as *large language
    models (LLMs)*. LLMs demonstrate few- or even zero-shot learning if pretrained
    on a large enough dataset. [GPT-J](model_doc/gptj) is an LLM with 6B parameters
    and trained on 400B tokens. GPT-J was followed by [OPT](model_doc/opt), a family
    of decoder-only models, the largest of which is 175B and trained on 180B tokens.
    [BLOOM](model_doc/bloom) was released around the same time, and the largest model
    in the family has 176B parameters and is trained on 366B tokens in 46 languages
    and 13 programming languages.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPT-2之后，语言模型变得更大，现在被称为*大型语言模型（LLMs）*。如果在足够大的数据集上进行预训练，LLMs可以展示少量甚至零-shot学习。[GPT-J](model_doc/gptj)是一个具有6B参数并在400B标记上训练的LLM。GPT-J之后是[OPT](model_doc/opt)，一系列仅解码器模型，其中最大的模型为175B，并在180B标记上训练。[BLOOM](model_doc/bloom)也在同一时间发布，该系列中最大的模型有176B参数，并在46种语言和13种编程语言中训练了366B标记。
- en: Encoder-decoder
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器-解码器
- en: '[BART](model_doc/bart) keeps the original Transformer architecture, but it
    modifies the pretraining objective with *text infilling* corruption, where some
    text spans are replaced with a single `mask` token. The decoder predicts the uncorrupted
    tokens (future tokens are masked) and uses the encoder’s hidden states to help
    it. [Pegasus](model_doc/pegasus) is similar to BART, but Pegasus masks entire
    sentences instead of text spans. In addition to masked language modeling, Pegasus
    is pretrained by gap sentence generation (GSG). The GSG objective masks whole
    sentences important to a document, replacing them with a `mask` token. The decoder
    must generate the output from the remaining sentences. [T5](model_doc/t5) is a
    more unique model that casts all NLP tasks into a text-to-text problem using specific
    prefixes. For example, the prefix `Summarize:` indicates a summarization task.
    T5 is pretrained by supervised (GLUE and SuperGLUE) training and self-supervised
    training (randomly sample and drop out 15% of tokens).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[BART](model_doc/bart)保留了原始的Transformer架构，但通过*文本填充*损坏修改了预训练目标，其中一些文本段被替换为单个`mask`标记。解码器预测未损坏的标记（未来标记被屏蔽），并使用编码器的隐藏状态来帮助它。[Pegasus](model_doc/pegasus)类似于BART，但Pegasus屏蔽整个句子而不是文本段。除了遮蔽语言建模，Pegasus还通过间隙句子生成（GSG）进行预训练。GSG目标屏蔽了对文档重要的整个句子，并用`mask`标记替换它们。解码器必须从剩余的句子中生成输出。[T5](model_doc/t5)是一个更独特的模型，将所有NLP任务都转化为使用特定前缀的文本到文本问题。例如，前缀`Summarize:`表示一个总结任务。T5通过监督（GLUE和SuperGLUE）训练和自监督训练（随机抽样并丢弃15%的标记）进行预训练。'
- en: Audio
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 音频
- en: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2Fvrchl8jDV9YwNVPWu2W0kK%2Fspeech-and-audio-model-timeline%3Fnode-id%3D0%253A1%26t%3DmM4H8pPMuK23rClL-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2Fvrchl8jDV9YwNVPWu2W0kK%2Fspeech-and-audio-model-timeline%3Fnode-id%3D0%253A1%26t%3DmM4H8pPMuK23rClL-1)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2Fvrchl8jDV9YwNVPWu2W0kK%2Fspeech-and-audio-model-timeline%3Fnode-id%3D0%253A1%26t%3DmM4H8pPMuK23rClL-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2Fvrchl8jDV9YwNVPWu2W0kK%2Fspeech-and-audio-model-timeline%3Fnode-id%3D0%253A1%26t%3DmM4H8pPMuK23rClL-1)'
- en: Encoder
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器
- en: '[Wav2Vec2](model_doc/wav2vec2) uses a Transformer encoder to learn speech representations
    directly from raw audio waveforms. It is pretrained with a contrastive task to
    determine the true speech representation from a set of false ones. [HuBERT](model_doc/hubert)
    is similar to Wav2Vec2 but has a different training process. Target labels are
    created by a clustering step in which segments of similar audio are assigned to
    a cluster which becomes a hidden unit. The hidden unit is mapped to an embedding
    to make a prediction.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2](model_doc/wav2vec2)使用Transformer编码器直接从原始音频波形中学习语音表示。它通过对比任务进行预训练，以确定一组错误的语音表示中的真实语音表示。[HuBERT](model_doc/hubert)类似于Wav2Vec2，但训练过程不同。目标标签是通过聚类步骤创建的，其中相似音频片段被分配到一个成为隐藏单元的簇中。隐藏单元被映射到一个嵌入以进行预测。'
- en: Encoder-decoder
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器-解码器
- en: '[Speech2Text](model_doc/speech_to_text) is a speech model designed for automatic
    speech recognition (ASR) and speech translation. The model accepts log mel-filter
    bank features extracted from the audio waveform and pretrained autoregressively
    to generate a transcript or translation. [Whisper](model_doc/whisper) is also
    an ASR model, but unlike many other speech models, it is pretrained on a massive
    amount of ✨ labeled ✨ audio transcription data for zero-shot performance. A large
    chunk of the dataset also contains non-English languages, meaning Whisper can
    also be used for low-resource languages. Structurally, Whisper is similar to Speech2Text.
    The audio signal is converted to a log-mel spectrogram encoded by the encoder.
    The decoder generates the transcript autoregressively from the encoder’s hidden
    states and the previous tokens.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[Speech2Text](model_doc/speech_to_text)是一个专为自动语音识别（ASR）和语音翻译设计的语音模型。该模型接受从音频波形中提取的对数梅尔滤波器特征，并预训练自回归地生成转录或翻译。[Whisper](model_doc/whisper)也是一个ASR模型，但与许多其他语音模型不同，它是在大量✨标记的✨音频转录数据上进行预训练，以实现零样本性能。数据集中还包含大量非英语语言，这意味着Whisper也可以用于资源稀缺的语言。在结构上，Whisper类似于Speech2Text。音频信号被转换为由编码器编码的对数梅尔频谱图。解码器从编码器的隐藏状态和先前的标记中自回归地生成转录。'
- en: Multimodal
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多模态
- en: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FcX125FQHXJS2gxeICiY93p%2Fmultimodal%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FcX125FQHXJS2gxeICiY93p%2Fmultimodal%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FcX125FQHXJS2gxeICiY93p%2Fmultimodal%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FcX125FQHXJS2gxeICiY93p%2Fmultimodal%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1)'
- en: Encoder
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器
- en: '[VisualBERT](model_doc/visual_bert) is a multimodal model for vision-language
    tasks released shortly after BERT. It combines BERT and a pretrained object detection
    system to extract image features into visual embeddings, passed alongside text
    embeddings to BERT. VisualBERT predicts the masked text based on the unmasked
    text and the visual embeddings, and it also has to predict whether the text is
    aligned with the image. When ViT was released, [ViLT](model_doc/vilt) adopted
    ViT in its architecture because it was easier to get the image embeddings this
    way. The image embeddings are jointly processed with the text embeddings. From
    there, ViLT is pretrained by image text matching, masked language modeling, and
    whole word masking.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[VisualBERT](model_doc/visual_bert)是一个用于视觉-语言任务的多模态模型，发布于BERT之后不久。它结合了BERT和一个预训练的目标检测系统，将图像特征提取为视觉嵌入，与文本嵌入一起传递给BERT。VisualBERT基于未屏蔽的文本和视觉嵌入预测被屏蔽的文本，并且还必须预测文本是否与图像对齐。当ViT发布时，[ViLT](model_doc/vilt)采用了ViT的架构，因为这样更容易获取图像嵌入。图像嵌入与文本嵌入一起进行处理。从那里，ViLT通过图像文本匹配、屏蔽语言建模和整词屏蔽进行预训练。'
- en: '[CLIP](model_doc/clip) takes a different approach and makes a pair prediction
    of (`image`, `text`) . An image encoder (ViT) and a text encoder (Transformer)
    are jointly trained on a 400 million (`image`, `text`) pair dataset to maximize
    the similarity between the image and text embeddings of the (`image`, `text`)
    pairs. After pretraining, you can use natural language to instruct CLIP to predict
    the text given an image or vice versa. [OWL-ViT](model_doc/owlvit) builds on top
    of CLIP by using it as its backbone for zero-shot object detection. After pretraining,
    an object detection head is added to make a set prediction over the (`class`,
    `bounding box`) pairs.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIP](model_doc/clip)采用了不同的方法，对(`图像`，`文本`)进行一对预测。一个图像编码器（ViT）和一个文本编码器（Transformer）在一个包含4亿个(`图像`，`文本`)对的数据集上进行联合训练，以最大化(`图像`，`文本`)对的图像和文本嵌入之间的相似性。在预训练之后，您可以使用自然语言指示CLIP预测给定图像的文本，反之亦然。[OWL-ViT](model_doc/owlvit)在CLIP的基础上构建，将其作为零样本目标检测的骨干。在预训练之后，添加了一个目标检测头，以对(`类别`，`边界框`)对进行集合预测。'
- en: Encoder-decoder
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器-解码器
- en: Optical character recognition (OCR) is a long-standing text recognition task
    that typically involves several components to understand the image and generate
    the text. [TrOCR](model_doc/trocr) simplifies the process using an end-to-end
    Transformer. The encoder is a ViT-style model for image understanding and processes
    the image as fixed-size patches. The decoder accepts the encoder’s hidden states
    and autoregressively generates text. [Donut](model_doc/donut) is a more general
    visual document understanding model that doesn’t rely on OCR-based approaches.
    It uses a Swin Transformer as the encoder and multilingual BART as the decoder.
    Donut is pretrained to read text by predicting the next word based on the image
    and text annotations. The decoder generates a token sequence given a prompt. The
    prompt is represented by a special token for each downstream task. For example,
    document parsing has a special `parsing` token that is combined with the encoder
    hidden states to parse the document into a structured output format (JSON).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FiB3Y6RvWYki7ZuKO6tNgZq%2Freinforcement-learning%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FiB3Y6RvWYki7ZuKO6tNgZq%2Freinforcement-learning%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Decoder
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Decision and Trajectory Transformer casts the state, action, and reward
    as a sequence modeling problem. The [Decision Transformer](model_doc/decision_transformer)
    generates a series of actions that lead to a future desired return based on returns-to-go,
    past states, and actions. For the last *K* timesteps, each of the three modalities
    are converted into token embeddings and processed by a GPT-like model to predict
    a future action token. [Trajectory Transformer](model_doc/trajectory_transformer)
    also tokenizes the states, actions, and rewards and processes them with a GPT
    architecture. Unlike the Decision Transformer, which is focused on reward conditioning,
    the Trajectory Transformer generates future actions with beam search.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
