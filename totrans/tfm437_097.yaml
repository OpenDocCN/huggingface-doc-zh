- en: The Transformer model family
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformeræ¨¡å‹å®¶æ—
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_summary](https://huggingface.co/docs/transformers/v4.37.2/en/model_summary)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_summary](https://huggingface.co/docs/transformers/v4.37.2/en/model_summary)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Since its introduction in 2017, the [original Transformer](https://arxiv.org/abs/1706.03762)
    model has inspired many new and exciting models that extend beyond natural language
    processing (NLP) tasks. There are models for [predicting the folded structure
    of proteins](https://huggingface.co/blog/deep-learning-with-proteins), [training
    a cheetah to run](https://huggingface.co/blog/train-decision-transformers), and
    [time series forecasting](https://huggingface.co/blog/time-series-transformers).
    With so many Transformer variants available, it can be easy to miss the bigger
    picture. What all these models have in common is theyâ€™re based on the original
    Transformer architecture. Some models only use the encoder or decoder, while others
    use both. This provides a useful taxonomy to categorize and examine the high-level
    differences within models in the Transformer family, and itâ€™ll help you understand
    Transformers you havenâ€™t encountered before.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ª2017å¹´å¼•å…¥[åŸå§‹Transformer](https://arxiv.org/abs/1706.03762)æ¨¡å‹ä»¥æ¥ï¼Œå®ƒå·²ç»æ¿€å‘äº†è®¸å¤šæ–°é¢–ä¸”ä»¤äººå…´å¥‹çš„æ¨¡å‹ï¼Œè¶…è¶Šäº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ã€‚æœ‰ç”¨äº[é¢„æµ‹è›‹ç™½è´¨çš„æŠ˜å ç»“æ„](https://huggingface.co/blog/deep-learning-with-proteins)ã€[è®­ç»ƒçŒè±¹å¥”è·‘](https://huggingface.co/blog/train-decision-transformers)å’Œ[æ—¶é—´åºåˆ—é¢„æµ‹](https://huggingface.co/blog/time-series-transformers)çš„æ¨¡å‹ã€‚æœ‰è¿™ä¹ˆå¤šTransformerå˜ä½“å¯ç”¨ï¼Œå¾ˆå®¹æ˜“å¿½ç•¥æ›´å¤§çš„ç”»é¢ã€‚æ‰€æœ‰è¿™äº›æ¨¡å‹çš„å…±åŒä¹‹å¤„æ˜¯å®ƒä»¬éƒ½åŸºäºåŸå§‹Transformeræ¶æ„ã€‚ä¸€äº›æ¨¡å‹åªä½¿ç”¨ç¼–ç å™¨æˆ–è§£ç å™¨ï¼Œè€Œå…¶ä»–ä¸€äº›åˆ™åŒæ—¶ä½¿ç”¨ä¸¤è€…ã€‚è¿™æä¾›äº†ä¸€ä¸ªæœ‰ç”¨çš„åˆ†ç±»æ³•ï¼Œå¯ä»¥å¯¹Transformerå®¶æ—ä¸­çš„æ¨¡å‹è¿›è¡Œåˆ†ç±»å’Œæ£€æŸ¥é«˜å±‚æ¬¡çš„å·®å¼‚ï¼Œè¿™å°†å¸®åŠ©æ‚¨ç†è§£ä»¥å‰æœªé‡åˆ°çš„Transformerã€‚
- en: If you arenâ€™t familiar with the original Transformer model or need a refresher,
    check out the [How do Transformers work](https://huggingface.co/course/chapter1/4?fw=pt)
    chapter from the Hugging Face course.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰åŸå§‹Transformeræ¨¡å‹æˆ–éœ€è¦å¤ä¹ ï¼Œè¯·æŸ¥çœ‹Hugging Faceè¯¾ç¨‹ä¸­çš„[Transformerå·¥ä½œåŸç†](https://huggingface.co/course/chapter1/4?fw=pt)ç« èŠ‚ã€‚
- en: '[https://www.youtube.com/embed/H39Z_720T5s](https://www.youtube.com/embed/H39Z_720T5s)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/embed/H39Z_720T5s](https://www.youtube.com/embed/H39Z_720T5s)'
- en: Computer vision
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¡ç®—æœºè§†è§‰
- en: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FacQBpeFBVvrDUlzFlkejoz%2FModelscape-timeline%3Fnode-id%3D0%253A1%26t%3Dm0zJ7m2BQ9oe0WtO-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FacQBpeFBVvrDUlzFlkejoz%2FModelscape-timeline%3Fnode-id%3D0%253A1%26t%3Dm0zJ7m2BQ9oe0WtO-1)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FacQBpeFBVvrDUlzFlkejoz%2FModelscape-timeline%3Fnode-id%3D0%253A1%26t%3Dm0zJ7m2BQ9oe0WtO-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FacQBpeFBVvrDUlzFlkejoz%2FModelscape-timeline%3Fnode-id%3D0%253A1%26t%3Dm0zJ7m2BQ9oe0WtO-1)'
- en: Convolutional network
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å·ç§¯ç½‘ç»œ
- en: For a long time, convolutional networks (CNNs) were the dominant paradigm for
    computer vision tasks until the [Vision Transformer](https://arxiv.org/abs/2010.11929)
    demonstrated its scalability and efficiency. Even then, some of a CNNâ€™s best qualities,
    like translation invariance, are so powerful (especially for certain tasks) that
    some Transformers incorporate convolutions in their architecture. [ConvNeXt](model_doc/convnext)
    flipped this exchange around and incorporated design choices from Transformers
    to modernize a CNN. For example, ConvNeXt uses non-overlapping sliding windows
    to patchify an image and a larger kernel to increase its global receptive field.
    ConvNeXt also makes several layer design choices to be more memory-efficient and
    improve performance, so it competes favorably with Transformers!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: é•¿æ—¶é—´ä»¥æ¥ï¼Œå·ç§¯ç½‘ç»œï¼ˆCNNsï¼‰ä¸€ç›´æ˜¯è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ä¸»å¯¼èŒƒå¼ï¼Œç›´åˆ°[è§†è§‰Transformer](https://arxiv.org/abs/2010.11929)å±•ç¤ºäº†å…¶å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚å³ä½¿å¦‚æ­¤ï¼ŒCNNçš„ä¸€äº›æœ€ä½³ç‰¹æ€§ï¼Œå¦‚å¹³ç§»ä¸å˜æ€§ï¼Œæ˜¯å¦‚æ­¤å¼ºå¤§ï¼ˆå°¤å…¶å¯¹äºæŸäº›ä»»åŠ¡ï¼‰ï¼Œä»¥è‡³äºä¸€äº›Transformeråœ¨å…¶æ¶æ„ä¸­å¼•å…¥äº†å·ç§¯ã€‚[ConvNeXt](model_doc/convnext)é¢ å€’äº†è¿™ç§äº¤æ¢ï¼Œå¹¶ä»Transformerä¸­å¼•å…¥è®¾è®¡é€‰æ‹©æ¥ç°ä»£åŒ–CNNã€‚ä¾‹å¦‚ï¼ŒConvNeXtä½¿ç”¨éé‡å æ»‘åŠ¨çª—å£å°†å›¾åƒåˆ†å—åŒ–ï¼Œå¹¶ä½¿ç”¨æ›´å¤§çš„å†…æ ¸æ¥å¢åŠ å…¶å…¨å±€æ„Ÿå—é‡ã€‚ConvNeXtè¿˜åšå‡ºäº†å‡ ä¸ªå±‚è®¾è®¡é€‰æ‹©ï¼Œä»¥æé«˜å†…å­˜æ•ˆç‡å’Œæ€§èƒ½ï¼Œå› æ­¤å®ƒä¸Transformerç«äº‰æœ‰åˆ©ï¼
- en: Encoder
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨
- en: The [Vision Transformer (ViT)](model_doc/vit) opened the door to computer vision
    tasks without convolutions. ViT uses a standard Transformer encoder, but its main
    breakthrough was how it treated an image. It splits an image into fixed-size patches
    and uses them to create an embedding, just like how a sentence is split into tokens.
    ViT capitalized on the Transformersâ€™ efficient architecture to demonstrate competitive
    results with the CNNs at the time while requiring fewer resources to train. ViT
    was soon followed by other vision models that could also handle dense vision tasks
    like segmentation as well as detection.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[è§†è§‰Transformerï¼ˆViTï¼‰](model_doc/vit)ä¸ºè®¡ç®—æœºè§†è§‰ä»»åŠ¡æ‰“å¼€äº†æ²¡æœ‰å·ç§¯çš„å¤§é—¨ã€‚ViTä½¿ç”¨æ ‡å‡†Transformerç¼–ç å™¨ï¼Œä½†å…¶ä¸»è¦çªç ´åœ¨äºå®ƒå¦‚ä½•å¤„ç†å›¾åƒã€‚å®ƒå°†å›¾åƒåˆ†å‰²æˆå›ºå®šå¤§å°çš„è¡¥ä¸ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬åˆ›å»ºåµŒå…¥ï¼Œå°±åƒå°†å¥å­åˆ†å‰²æˆæ ‡è®°ä¸€æ ·ã€‚ViTåˆ©ç”¨Transformerçš„é«˜æ•ˆæ¶æ„å±•ç¤ºäº†ä¸å½“æ—¶çš„CNNç«äº‰åŠ›çš„ç»“æœï¼ŒåŒæ—¶éœ€è¦æ›´å°‘çš„èµ„æºè¿›è¡Œè®­ç»ƒã€‚ViTå¾ˆå¿«è¢«å…¶ä»–è§†è§‰æ¨¡å‹è·Ÿéšï¼Œè¿™äº›æ¨¡å‹ä¹Ÿå¯ä»¥å¤„ç†åƒåˆ†å‰²å’Œæ£€æµ‹è¿™æ ·çš„å¯†é›†è§†è§‰ä»»åŠ¡ã€‚'
- en: One of these models is the [Swin](model_doc/swin) Transformer. It builds hierarchical
    feature maps (like a CNN ğŸ‘€ and unlike ViT) from smaller-sized patches and merges
    them with neighboring patches in deeper layers. Attention is only computed within
    a local window, and the window is shifted between attention layers to create connections
    to help the model learn better. Since the Swin Transformer can produce hierarchical
    feature maps, it is a good candidate for dense prediction tasks like segmentation
    and detection. The [SegFormer](model_doc/segformer) also uses a Transformer encoder
    to build hierarchical feature maps, but it adds a simple multilayer perceptron
    (MLP) decoder on top to combine all the feature maps and make a prediction.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ä¸€ä¸ªæ¨¡å‹æ˜¯[Swin](model_doc/swin) Transformerã€‚å®ƒä»è¾ƒå°çš„è¡¥ä¸ä¸­æ„å»ºåˆ†å±‚ç‰¹å¾å›¾ï¼ˆç±»ä¼¼äºCNNğŸ‘€ï¼Œä¸åŒäºViTï¼‰ï¼Œå¹¶åœ¨æ›´æ·±å±‚ä¸­å°†å®ƒä»¬ä¸ç›¸é‚»çš„è¡¥ä¸åˆå¹¶ã€‚æ³¨æ„åŠ›ä»…åœ¨å±€éƒ¨çª—å£å†…è®¡ç®—ï¼Œå¹¶ä¸”åœ¨æ³¨æ„åŠ›å±‚ä¹‹é—´ç§»åŠ¨çª—å£ä»¥åˆ›å»ºè¿æ¥ä»¥å¸®åŠ©æ¨¡å‹å­¦ä¹ æ›´å¥½ã€‚ç”±äºSwin
    Transformerå¯ä»¥ç”Ÿæˆåˆ†å±‚ç‰¹å¾å›¾ï¼Œå› æ­¤å®ƒæ˜¯å¯†é›†é¢„æµ‹ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²å’Œæ£€æµ‹ï¼‰çš„è‰¯å¥½å€™é€‰ã€‚[SegFormer](model_doc/segformer)
    ä¹Ÿä½¿ç”¨Transformerç¼–ç å™¨æ„å»ºåˆ†å±‚ç‰¹å¾å›¾ï¼Œä½†å®ƒåœ¨é¡¶éƒ¨æ·»åŠ äº†ä¸€ä¸ªç®€å•çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰è§£ç å™¨ï¼Œä»¥ç»„åˆæ‰€æœ‰ç‰¹å¾å›¾å¹¶è¿›è¡Œé¢„æµ‹ã€‚
- en: Other vision models, like BeIT and ViTMAE, drew inspiration from BERTâ€™s pretraining
    objective. [BeIT](model_doc/beit) is pretrained by *masked image modeling (MIM)*;
    the image patches are randomly masked, and the image is also tokenized into visual
    tokens. BeIT is trained to predict the visual tokens corresponding to the masked
    patches. [ViTMAE](model_doc/vitmae) has a similar pretraining objective, except
    it must predict the pixels instead of visual tokens. Whatâ€™s unusual is 75% of
    the image patches are masked! The decoder reconstructs the pixels from the masked
    tokens and encoded patches. After pretraining, the decoder is thrown away, and
    the encoder is ready to be used in downstream tasks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–è§†è§‰æ¨¡å‹ï¼Œå¦‚BeITå’ŒViTMAEï¼Œä»BERTçš„é¢„è®­ç»ƒç›®æ ‡ä¸­æ±²å–çµæ„Ÿã€‚[BeIT](model_doc/beit) é€šè¿‡*masked image
    modeling (MIM)* è¿›è¡Œé¢„è®­ç»ƒï¼›å›¾åƒè¡¥ä¸è¢«éšæœºå±è”½ï¼Œå›¾åƒä¹Ÿè¢«æ ‡è®°ä¸ºè§†è§‰æ ‡è®°ã€‚BeIT è¢«è®­ç»ƒä»¥é¢„æµ‹ä¸è¢«å±è”½è¡¥ä¸å¯¹åº”çš„è§†è§‰æ ‡è®°ã€‚[ViTMAE](model_doc/vitmae)
    æœ‰ä¸€ä¸ªç±»ä¼¼çš„é¢„è®­ç»ƒç›®æ ‡ï¼Œåªæ˜¯å®ƒå¿…é¡»é¢„æµ‹åƒç´ è€Œä¸æ˜¯è§†è§‰æ ‡è®°ã€‚ä¸å¯»å¸¸çš„æ˜¯ï¼Œ75%çš„å›¾åƒè¡¥ä¸è¢«å±è”½ï¼è§£ç å™¨ä»è¢«å±è”½çš„æ ‡è®°å’Œç¼–ç çš„è¡¥ä¸ä¸­é‡å»ºåƒç´ ã€‚é¢„è®­ç»ƒåï¼Œè§£ç å™¨è¢«ä¸¢å¼ƒï¼Œç¼–ç å™¨å‡†å¤‡å¥½ç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚
- en: Decoder
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£ç å™¨
- en: Decoder-only vision models are rare because most vision models rely on an encoder
    to learn an image representation. But for use cases like image generation, the
    decoder is a natural fit, as weâ€™ve seen from text generation models like GPT-2\.
    [ImageGPT](model_doc/imagegpt) uses the same architecture as GPT-2, but instead
    of predicting the next token in a sequence, it predicts the next pixel in an image.
    In addition to image generation, ImageGPT could also be finetuned for image classification.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…è§£ç å™¨çš„è§†è§‰æ¨¡å‹å¾ˆå°‘ï¼Œå› ä¸ºå¤§å¤šæ•°è§†è§‰æ¨¡å‹ä¾èµ–ç¼–ç å™¨æ¥å­¦ä¹ å›¾åƒè¡¨ç¤ºã€‚ä½†å¯¹äºåƒå›¾åƒç”Ÿæˆè¿™æ ·çš„ç”¨ä¾‹ï¼Œè§£ç å™¨æ˜¯ä¸€ä¸ªè‡ªç„¶çš„é€‰æ‹©ï¼Œæ­£å¦‚æˆ‘ä»¬ä»GPT-2ç­‰æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ä¸­çœ‹åˆ°çš„é‚£æ ·ã€‚[ImageGPT](model_doc/imagegpt)
    ä½¿ç”¨ä¸GPT-2ç›¸åŒçš„æ¶æ„ï¼Œä½†å®ƒä¸æ˜¯é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œè€Œæ˜¯é¢„æµ‹å›¾åƒä¸­çš„ä¸‹ä¸€ä¸ªåƒç´ ã€‚é™¤äº†å›¾åƒç”Ÿæˆï¼ŒImageGPTä¹Ÿå¯ä»¥è¿›è¡Œå¾®è°ƒä»¥ç”¨äºå›¾åƒåˆ†ç±»ã€‚
- en: Encoder-decoder
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨-è§£ç å™¨
- en: Vision models commonly use an encoder (also known as a backbone) to extract
    important image features before passing them to a Transformer decoder. [DETR](model_doc/detr)
    has a pretrained backbone, but it also uses the complete Transformer encoder-decoder
    architecture for object detection. The encoder learns image representations and
    combines them with object queries (each object query is a learned embedding that
    focuses on a region or object in an image) in the decoder. DETR predicts the bounding
    box coordinates and class label for each object query.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è§†è§‰æ¨¡å‹é€šå¸¸ä½¿ç”¨ç¼–ç å™¨ï¼ˆä¹Ÿç§°ä¸ºéª¨å¹²ï¼‰æ¥æå–é‡è¦çš„å›¾åƒç‰¹å¾ï¼Œç„¶åå°†å®ƒä»¬ä¼ é€’ç»™Transformerè§£ç å™¨ã€‚[DETR](model_doc/detr) æœ‰ä¸€ä¸ªé¢„è®­ç»ƒçš„éª¨å¹²ï¼Œä½†å®ƒè¿˜ä½¿ç”¨å®Œæ•´çš„Transformerç¼–ç å™¨-è§£ç å™¨æ¶æ„è¿›è¡Œç›®æ ‡æ£€æµ‹ã€‚ç¼–ç å™¨å­¦ä¹ å›¾åƒè¡¨ç¤ºï¼Œå¹¶å°†å…¶ä¸å¯¹è±¡æŸ¥è¯¢ï¼ˆæ¯ä¸ªå¯¹è±¡æŸ¥è¯¢æ˜¯ä¸€ä¸ªä¸“æ³¨äºå›¾åƒä¸­çš„åŒºåŸŸæˆ–å¯¹è±¡çš„å­¦ä¹ åµŒå…¥ï¼‰ç»“åˆåœ¨è§£ç å™¨ä¸­ã€‚DETR
    é¢„æµ‹æ¯ä¸ªå¯¹è±¡æŸ¥è¯¢çš„è¾¹ç•Œæ¡†åæ ‡å’Œç±»åˆ«æ ‡ç­¾ã€‚
- en: Natural language processing
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡ªç„¶è¯­è¨€å¤„ç†
- en: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FUhbQAZDlpYW5XEpdFy6GoG%2Fnlp-model-timeline%3Fnode-id%3D0%253A1%26t%3D4mZMr4r1vDEYGJ50-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FUhbQAZDlpYW5XEpdFy6GoG%2Fnlp-model-timeline%3Fnode-id%3D0%253A1%26t%3D4mZMr4r1vDEYGJ50-1)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FUhbQAZDlpYW5XEpdFy6GoG%2Fnlp-model-timeline%3Fnode-id%3D0%253A1%26t%3D4mZMr4r1vDEYGJ50-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FUhbQAZDlpYW5XEpdFy6GoG%2Fnlp-model-timeline%3Fnode-id%3D0%253A1%26t%3D4mZMr4r1vDEYGJ50-1)'
- en: Encoder
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨
- en: '[BERT](model_doc/bert) is an encoder-only Transformer that randomly masks certain
    tokens in the input to avoid seeing other tokens, which would allow it to â€œcheatâ€.
    The pretraining objective is to predict the masked token based on the context.
    This allows BERT to fully use the left and right contexts to help it learn a deeper
    and richer representation of the inputs. However, there was still room for improvement
    in BERTâ€™s pretraining strategy. [RoBERTa](model_doc/roberta) improved upon this
    by introducing a new pretraining recipe that includes training for longer and
    on larger batches, randomly masking tokens at each epoch instead of just once
    during preprocessing, and removing the next-sentence prediction objective.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[BERT](model_doc/bert) æ˜¯ä¸€ä¸ªä»…åŒ…å«ç¼–ç å™¨çš„Transformerï¼Œå®ƒä¼šéšæœºå±è”½è¾“å…¥ä¸­çš„æŸäº›æ ‡è®°ï¼Œä»¥é¿å…çœ‹åˆ°å…¶ä»–æ ‡è®°ï¼Œè¿™æ ·å¯ä»¥é˜²æ­¢å…¶â€œä½œå¼Šâ€ã€‚é¢„è®­ç»ƒçš„ç›®æ ‡æ˜¯åŸºäºä¸Šä¸‹æ–‡é¢„æµ‹è¢«å±è”½çš„æ ‡è®°ã€‚è¿™ä½¿å¾—BERTèƒ½å¤Ÿå……åˆ†åˆ©ç”¨å·¦å³ä¸Šä¸‹æ–‡æ¥å¸®åŠ©å­¦ä¹ è¾“å…¥çš„æ›´æ·±å±‚å’Œæ›´ä¸°å¯Œçš„è¡¨ç¤ºã€‚ç„¶è€Œï¼ŒBERTçš„é¢„è®­ç»ƒç­–ç•¥ä»æœ‰æ”¹è¿›çš„ç©ºé—´ã€‚[RoBERTa](model_doc/roberta)
    é€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°çš„é¢„è®­ç»ƒé…æ–¹æ¥æ”¹è¿›è¿™ä¸€ç‚¹ï¼Œè¯¥é…æ–¹åŒ…æ‹¬æ›´é•¿æ—¶é—´å’Œæ›´å¤§æ‰¹æ¬¡çš„è®­ç»ƒï¼Œåœ¨æ¯ä¸ªæ—¶ä»£éšæœºå±è”½æ ‡è®°ï¼Œè€Œä¸ä»…ä»…æ˜¯åœ¨é¢„å¤„ç†æœŸé—´ä¸€æ¬¡ï¼Œä»¥åŠç§»é™¤ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ç›®æ ‡ã€‚'
- en: The dominant strategy to improve performance is to increase the model size.
    But training large models is computationally expensive. One way to reduce computational
    costs is using a smaller model like [DistilBERT](model_doc/distilbert). DistilBERT
    uses [knowledge distillation](https://arxiv.org/abs/1503.02531) - a compression
    technique - to create a smaller version of BERT while keeping nearly all of its
    language understanding capabilities.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æé«˜æ€§èƒ½çš„ä¸»è¦ç­–ç•¥æ˜¯å¢åŠ æ¨¡å‹å¤§å°ã€‚ä½†æ˜¯è®­ç»ƒå¤§å‹æ¨¡å‹åœ¨è®¡ç®—ä¸Šæ˜¯æ˜‚è´µçš„ã€‚å‡å°‘è®¡ç®—æˆæœ¬çš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨åƒ[DistilBERT](model_doc/distilbert)è¿™æ ·çš„è¾ƒå°æ¨¡å‹ã€‚DistilBERTä½¿ç”¨[çŸ¥è¯†è’¸é¦](https://arxiv.org/abs/1503.02531)
    - ä¸€ç§å‹ç¼©æŠ€æœ¯ - æ¥åˆ›å»ºä¸€ä¸ªè¾ƒå°ç‰ˆæœ¬çš„BERTï¼ŒåŒæ—¶ä¿ç•™å‡ ä¹æ‰€æœ‰çš„è¯­è¨€ç†è§£èƒ½åŠ›ã€‚
- en: 'However, most Transformer models continued to trend towards more parameters,
    leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert)
    reduces memory consumption by lowering the number of parameters in two ways: separating
    the larger vocabulary embedding into two smaller matrices and allowing layers
    to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention
    mechanism where the word and its position are separately encoded in two vectors.
    The attention is computed from these separate vectors instead of a single vector
    containing the word and position embeddings. [Longformer](model_doc/longformer)
    also focused on making attention more efficient, especially for processing documents
    with longer sequence lengths. It uses a combination of local windowed attention
    (attention only calculated from fixed window size around each token) and global
    attention (only for specific task tokens like `[CLS]` for classification) to create
    a sparse attention matrix instead of a full attention matrix.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¤§å¤šæ•°Transformeræ¨¡å‹ç»§ç»­æœç€æ›´å¤šå‚æ•°çš„æ–¹å‘å‘å±•ï¼Œå¯¼è‡´å‡ºç°äº†ä¸“æ³¨äºæé«˜è®­ç»ƒæ•ˆç‡çš„æ–°æ¨¡å‹ã€‚[ALBERT](model_doc/albert)é€šè¿‡ä¸¤ç§æ–¹å¼é™ä½å‚æ•°æ•°é‡æ¥å‡å°‘å†…å­˜æ¶ˆè€—ï¼šå°†æ›´å¤§çš„è¯æ±‡åµŒå…¥åˆ†ä¸ºä¸¤ä¸ªè¾ƒå°çš„çŸ©é˜µï¼Œå¹¶å…è®¸å±‚å…±äº«å‚æ•°ã€‚[DeBERTa](model_doc/deberta)æ·»åŠ äº†ä¸€ä¸ªè§£è€¦çš„æ³¨æ„æœºåˆ¶ï¼Œå…¶ä¸­å•è¯åŠå…¶ä½ç½®åˆ†åˆ«ç¼–ç åœ¨ä¸¤ä¸ªå‘é‡ä¸­ã€‚æ³¨æ„åŠ›æ˜¯ä»è¿™äº›å•ç‹¬çš„å‘é‡è®¡ç®—è€Œæ¥ï¼Œè€Œä¸æ˜¯ä»åŒ…å«å•è¯å’Œä½ç½®åµŒå…¥çš„å•ä¸ªå‘é‡ä¸­è®¡ç®—ã€‚[Longformer](model_doc/longformer)ä¹Ÿä¸“æ³¨äºä½¿æ³¨æ„åŠ›æ›´åŠ é«˜æ•ˆï¼Œç‰¹åˆ«æ˜¯ç”¨äºå¤„ç†å…·æœ‰æ›´é•¿åºåˆ—é•¿åº¦çš„æ–‡æ¡£ã€‚å®ƒä½¿ç”¨å±€éƒ¨çª—å£æ³¨æ„åŠ›ï¼ˆä»…è®¡ç®—å›´ç»•æ¯ä¸ªæ ‡è®°çš„å›ºå®šçª—å£å¤§å°çš„æ³¨æ„åŠ›ï¼‰å’Œå…¨å±€æ³¨æ„åŠ›ï¼ˆä»…ç”¨äºç‰¹å®šä»»åŠ¡æ ‡è®°ï¼Œå¦‚`[CLS]`ç”¨äºåˆ†ç±»ï¼‰çš„ç»„åˆï¼Œä»¥åˆ›å»ºä¸€ä¸ªç¨€ç–çš„æ³¨æ„åŠ›çŸ©é˜µï¼Œè€Œä¸æ˜¯å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µã€‚
- en: Decoder
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£ç å™¨
- en: '[GPT-2](model_doc/gpt2) is a decoder-only Transformer that predicts the next
    word in the sequence. It masks tokens to the right so the model canâ€™t â€œcheatâ€
    by looking ahead. By pretraining on a massive body of text, GPT-2 became really
    good at generating text, even if the text is only sometimes accurate or true.
    But GPT-2 lacked the bidirectional context from BERTâ€™s pretraining, which made
    it unsuitable for certain tasks. [XLNET](model_doc/xlnet) combines the best of
    both BERT and GPT-2â€™s pretraining objectives by using a permutation language modeling
    objective (PLM) that allows it to learn bidirectionally.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPT-2](model_doc/gpt2)æ˜¯ä¸€ä¸ªä»…è§£ç å™¨çš„Transformerï¼Œç”¨äºé¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚å®ƒä¼šå±è”½å³ä¾§çš„æ ‡è®°ï¼Œä»¥é˜²æ¨¡å‹é€šè¿‡å‘å‰æŸ¥çœ‹æ¥â€œä½œå¼Šâ€ã€‚é€šè¿‡åœ¨å¤§é‡æ–‡æœ¬ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ŒGPT-2åœ¨ç”Ÿæˆæ–‡æœ¬æ–¹é¢è¡¨ç°å¾—éå¸¸å‡ºè‰²ï¼Œå³ä½¿æ–‡æœ¬æœ‰æ—¶å¹¶ä¸å‡†ç¡®æˆ–çœŸå®ã€‚ä½†æ˜¯GPT-2ç¼ºä¹BERTé¢„è®­ç»ƒçš„åŒå‘ä¸Šä¸‹æ–‡ï¼Œè¿™ä½¿å¾—å®ƒä¸é€‚ç”¨äºæŸäº›ä»»åŠ¡ã€‚[XLNET](model_doc/xlnet)ç»“åˆäº†BERTå’ŒGPT-2çš„é¢„è®­ç»ƒç›®æ ‡çš„ä¼˜ç‚¹ï¼Œä½¿ç”¨æ’åˆ—è¯­è¨€å»ºæ¨¡ç›®æ ‡ï¼ˆPLMï¼‰ä½¿å…¶èƒ½å¤ŸåŒå‘å­¦ä¹ ã€‚'
- en: After GPT-2, language models grew even bigger and are now known as *large language
    models (LLMs)*. LLMs demonstrate few- or even zero-shot learning if pretrained
    on a large enough dataset. [GPT-J](model_doc/gptj) is an LLM with 6B parameters
    and trained on 400B tokens. GPT-J was followed by [OPT](model_doc/opt), a family
    of decoder-only models, the largest of which is 175B and trained on 180B tokens.
    [BLOOM](model_doc/bloom) was released around the same time, and the largest model
    in the family has 176B parameters and is trained on 366B tokens in 46 languages
    and 13 programming languages.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨GPT-2ä¹‹åï¼Œè¯­è¨€æ¨¡å‹å˜å¾—æ›´å¤§ï¼Œç°åœ¨è¢«ç§°ä¸º*å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰*ã€‚å¦‚æœåœ¨è¶³å¤Ÿå¤§çš„æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ŒLLMså¯ä»¥å±•ç¤ºå°‘é‡ç”šè‡³é›¶-shotå­¦ä¹ ã€‚[GPT-J](model_doc/gptj)æ˜¯ä¸€ä¸ªå…·æœ‰6Bå‚æ•°å¹¶åœ¨400Bæ ‡è®°ä¸Šè®­ç»ƒçš„LLMã€‚GPT-Jä¹‹åæ˜¯[OPT](model_doc/opt)ï¼Œä¸€ç³»åˆ—ä»…è§£ç å™¨æ¨¡å‹ï¼Œå…¶ä¸­æœ€å¤§çš„æ¨¡å‹ä¸º175Bï¼Œå¹¶åœ¨180Bæ ‡è®°ä¸Šè®­ç»ƒã€‚[BLOOM](model_doc/bloom)ä¹Ÿåœ¨åŒä¸€æ—¶é—´å‘å¸ƒï¼Œè¯¥ç³»åˆ—ä¸­æœ€å¤§çš„æ¨¡å‹æœ‰176Bå‚æ•°ï¼Œå¹¶åœ¨46ç§è¯­è¨€å’Œ13ç§ç¼–ç¨‹è¯­è¨€ä¸­è®­ç»ƒäº†366Bæ ‡è®°ã€‚
- en: Encoder-decoder
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨-è§£ç å™¨
- en: '[BART](model_doc/bart) keeps the original Transformer architecture, but it
    modifies the pretraining objective with *text infilling* corruption, where some
    text spans are replaced with a single `mask` token. The decoder predicts the uncorrupted
    tokens (future tokens are masked) and uses the encoderâ€™s hidden states to help
    it. [Pegasus](model_doc/pegasus) is similar to BART, but Pegasus masks entire
    sentences instead of text spans. In addition to masked language modeling, Pegasus
    is pretrained by gap sentence generation (GSG). The GSG objective masks whole
    sentences important to a document, replacing them with a `mask` token. The decoder
    must generate the output from the remaining sentences. [T5](model_doc/t5) is a
    more unique model that casts all NLP tasks into a text-to-text problem using specific
    prefixes. For example, the prefix `Summarize:` indicates a summarization task.
    T5 is pretrained by supervised (GLUE and SuperGLUE) training and self-supervised
    training (randomly sample and drop out 15% of tokens).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[BART](model_doc/bart)ä¿ç•™äº†åŸå§‹çš„Transformeræ¶æ„ï¼Œä½†é€šè¿‡*æ–‡æœ¬å¡«å……*æŸåä¿®æ”¹äº†é¢„è®­ç»ƒç›®æ ‡ï¼Œå…¶ä¸­ä¸€äº›æ–‡æœ¬æ®µè¢«æ›¿æ¢ä¸ºå•ä¸ª`mask`æ ‡è®°ã€‚è§£ç å™¨é¢„æµ‹æœªæŸåçš„æ ‡è®°ï¼ˆæœªæ¥æ ‡è®°è¢«å±è”½ï¼‰ï¼Œå¹¶ä½¿ç”¨ç¼–ç å™¨çš„éšè—çŠ¶æ€æ¥å¸®åŠ©å®ƒã€‚[Pegasus](model_doc/pegasus)ç±»ä¼¼äºBARTï¼Œä½†Pegasuså±è”½æ•´ä¸ªå¥å­è€Œä¸æ˜¯æ–‡æœ¬æ®µã€‚é™¤äº†é®è”½è¯­è¨€å»ºæ¨¡ï¼ŒPegasusè¿˜é€šè¿‡é—´éš™å¥å­ç”Ÿæˆï¼ˆGSGï¼‰è¿›è¡Œé¢„è®­ç»ƒã€‚GSGç›®æ ‡å±è”½äº†å¯¹æ–‡æ¡£é‡è¦çš„æ•´ä¸ªå¥å­ï¼Œå¹¶ç”¨`mask`æ ‡è®°æ›¿æ¢å®ƒä»¬ã€‚è§£ç å™¨å¿…é¡»ä»å‰©ä½™çš„å¥å­ä¸­ç”Ÿæˆè¾“å‡ºã€‚[T5](model_doc/t5)æ˜¯ä¸€ä¸ªæ›´ç‹¬ç‰¹çš„æ¨¡å‹ï¼Œå°†æ‰€æœ‰NLPä»»åŠ¡éƒ½è½¬åŒ–ä¸ºä½¿ç”¨ç‰¹å®šå‰ç¼€çš„æ–‡æœ¬åˆ°æ–‡æœ¬é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå‰ç¼€`Summarize:`è¡¨ç¤ºä¸€ä¸ªæ€»ç»“ä»»åŠ¡ã€‚T5é€šè¿‡ç›‘ç£ï¼ˆGLUEå’ŒSuperGLUEï¼‰è®­ç»ƒå’Œè‡ªç›‘ç£è®­ç»ƒï¼ˆéšæœºæŠ½æ ·å¹¶ä¸¢å¼ƒ15%çš„æ ‡è®°ï¼‰è¿›è¡Œé¢„è®­ç»ƒã€‚'
- en: Audio
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éŸ³é¢‘
- en: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2Fvrchl8jDV9YwNVPWu2W0kK%2Fspeech-and-audio-model-timeline%3Fnode-id%3D0%253A1%26t%3DmM4H8pPMuK23rClL-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2Fvrchl8jDV9YwNVPWu2W0kK%2Fspeech-and-audio-model-timeline%3Fnode-id%3D0%253A1%26t%3DmM4H8pPMuK23rClL-1)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2Fvrchl8jDV9YwNVPWu2W0kK%2Fspeech-and-audio-model-timeline%3Fnode-id%3D0%253A1%26t%3DmM4H8pPMuK23rClL-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2Fvrchl8jDV9YwNVPWu2W0kK%2Fspeech-and-audio-model-timeline%3Fnode-id%3D0%253A1%26t%3DmM4H8pPMuK23rClL-1)'
- en: Encoder
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨
- en: '[Wav2Vec2](model_doc/wav2vec2) uses a Transformer encoder to learn speech representations
    directly from raw audio waveforms. It is pretrained with a contrastive task to
    determine the true speech representation from a set of false ones. [HuBERT](model_doc/hubert)
    is similar to Wav2Vec2 but has a different training process. Target labels are
    created by a clustering step in which segments of similar audio are assigned to
    a cluster which becomes a hidden unit. The hidden unit is mapped to an embedding
    to make a prediction.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wav2Vec2](model_doc/wav2vec2)ä½¿ç”¨Transformerç¼–ç å™¨ç›´æ¥ä»åŸå§‹éŸ³é¢‘æ³¢å½¢ä¸­å­¦ä¹ è¯­éŸ³è¡¨ç¤ºã€‚å®ƒé€šè¿‡å¯¹æ¯”ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥ç¡®å®šä¸€ç»„é”™è¯¯çš„è¯­éŸ³è¡¨ç¤ºä¸­çš„çœŸå®è¯­éŸ³è¡¨ç¤ºã€‚[HuBERT](model_doc/hubert)ç±»ä¼¼äºWav2Vec2ï¼Œä½†è®­ç»ƒè¿‡ç¨‹ä¸åŒã€‚ç›®æ ‡æ ‡ç­¾æ˜¯é€šè¿‡èšç±»æ­¥éª¤åˆ›å»ºçš„ï¼Œå…¶ä¸­ç›¸ä¼¼éŸ³é¢‘ç‰‡æ®µè¢«åˆ†é…åˆ°ä¸€ä¸ªæˆä¸ºéšè—å•å…ƒçš„ç°‡ä¸­ã€‚éšè—å•å…ƒè¢«æ˜ å°„åˆ°ä¸€ä¸ªåµŒå…¥ä»¥è¿›è¡Œé¢„æµ‹ã€‚'
- en: Encoder-decoder
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨-è§£ç å™¨
- en: '[Speech2Text](model_doc/speech_to_text) is a speech model designed for automatic
    speech recognition (ASR) and speech translation. The model accepts log mel-filter
    bank features extracted from the audio waveform and pretrained autoregressively
    to generate a transcript or translation. [Whisper](model_doc/whisper) is also
    an ASR model, but unlike many other speech models, it is pretrained on a massive
    amount of âœ¨ labeled âœ¨ audio transcription data for zero-shot performance. A large
    chunk of the dataset also contains non-English languages, meaning Whisper can
    also be used for low-resource languages. Structurally, Whisper is similar to Speech2Text.
    The audio signal is converted to a log-mel spectrogram encoded by the encoder.
    The decoder generates the transcript autoregressively from the encoderâ€™s hidden
    states and the previous tokens.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[Speech2Text](model_doc/speech_to_text)æ˜¯ä¸€ä¸ªä¸“ä¸ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè¯­éŸ³ç¿»è¯‘è®¾è®¡çš„è¯­éŸ³æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ¥å—ä»éŸ³é¢‘æ³¢å½¢ä¸­æå–çš„å¯¹æ•°æ¢…å°”æ»¤æ³¢å™¨ç‰¹å¾ï¼Œå¹¶é¢„è®­ç»ƒè‡ªå›å½’åœ°ç”Ÿæˆè½¬å½•æˆ–ç¿»è¯‘ã€‚[Whisper](model_doc/whisper)ä¹Ÿæ˜¯ä¸€ä¸ªASRæ¨¡å‹ï¼Œä½†ä¸è®¸å¤šå…¶ä»–è¯­éŸ³æ¨¡å‹ä¸åŒï¼Œå®ƒæ˜¯åœ¨å¤§é‡âœ¨æ ‡è®°çš„âœ¨éŸ³é¢‘è½¬å½•æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥å®ç°é›¶æ ·æœ¬æ€§èƒ½ã€‚æ•°æ®é›†ä¸­è¿˜åŒ…å«å¤§é‡éè‹±è¯­è¯­è¨€ï¼Œè¿™æ„å‘³ç€Whisperä¹Ÿå¯ä»¥ç”¨äºèµ„æºç¨€ç¼ºçš„è¯­è¨€ã€‚åœ¨ç»“æ„ä¸Šï¼ŒWhisperç±»ä¼¼äºSpeech2Textã€‚éŸ³é¢‘ä¿¡å·è¢«è½¬æ¢ä¸ºç”±ç¼–ç å™¨ç¼–ç çš„å¯¹æ•°æ¢…å°”é¢‘è°±å›¾ã€‚è§£ç å™¨ä»ç¼–ç å™¨çš„éšè—çŠ¶æ€å’Œå…ˆå‰çš„æ ‡è®°ä¸­è‡ªå›å½’åœ°ç”Ÿæˆè½¬å½•ã€‚'
- en: Multimodal
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤šæ¨¡æ€
- en: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FcX125FQHXJS2gxeICiY93p%2Fmultimodal%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FcX125FQHXJS2gxeICiY93p%2Fmultimodal%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FcX125FQHXJS2gxeICiY93p%2Fmultimodal%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FcX125FQHXJS2gxeICiY93p%2Fmultimodal%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1)'
- en: Encoder
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨
- en: '[VisualBERT](model_doc/visual_bert) is a multimodal model for vision-language
    tasks released shortly after BERT. It combines BERT and a pretrained object detection
    system to extract image features into visual embeddings, passed alongside text
    embeddings to BERT. VisualBERT predicts the masked text based on the unmasked
    text and the visual embeddings, and it also has to predict whether the text is
    aligned with the image. When ViT was released, [ViLT](model_doc/vilt) adopted
    ViT in its architecture because it was easier to get the image embeddings this
    way. The image embeddings are jointly processed with the text embeddings. From
    there, ViLT is pretrained by image text matching, masked language modeling, and
    whole word masking.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[VisualBERT](model_doc/visual_bert)æ˜¯ä¸€ä¸ªç”¨äºè§†è§‰-è¯­è¨€ä»»åŠ¡çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå‘å¸ƒäºBERTä¹‹åä¸ä¹…ã€‚å®ƒç»“åˆäº†BERTå’Œä¸€ä¸ªé¢„è®­ç»ƒçš„ç›®æ ‡æ£€æµ‹ç³»ç»Ÿï¼Œå°†å›¾åƒç‰¹å¾æå–ä¸ºè§†è§‰åµŒå…¥ï¼Œä¸æ–‡æœ¬åµŒå…¥ä¸€èµ·ä¼ é€’ç»™BERTã€‚VisualBERTåŸºäºæœªå±è”½çš„æ–‡æœ¬å’Œè§†è§‰åµŒå…¥é¢„æµ‹è¢«å±è”½çš„æ–‡æœ¬ï¼Œå¹¶ä¸”è¿˜å¿…é¡»é¢„æµ‹æ–‡æœ¬æ˜¯å¦ä¸å›¾åƒå¯¹é½ã€‚å½“ViTå‘å¸ƒæ—¶ï¼Œ[ViLT](model_doc/vilt)é‡‡ç”¨äº†ViTçš„æ¶æ„ï¼Œå› ä¸ºè¿™æ ·æ›´å®¹æ˜“è·å–å›¾åƒåµŒå…¥ã€‚å›¾åƒåµŒå…¥ä¸æ–‡æœ¬åµŒå…¥ä¸€èµ·è¿›è¡Œå¤„ç†ã€‚ä»é‚£é‡Œï¼ŒViLTé€šè¿‡å›¾åƒæ–‡æœ¬åŒ¹é…ã€å±è”½è¯­è¨€å»ºæ¨¡å’Œæ•´è¯å±è”½è¿›è¡Œé¢„è®­ç»ƒã€‚'
- en: '[CLIP](model_doc/clip) takes a different approach and makes a pair prediction
    of (`image`, `text`) . An image encoder (ViT) and a text encoder (Transformer)
    are jointly trained on a 400 million (`image`, `text`) pair dataset to maximize
    the similarity between the image and text embeddings of the (`image`, `text`)
    pairs. After pretraining, you can use natural language to instruct CLIP to predict
    the text given an image or vice versa. [OWL-ViT](model_doc/owlvit) builds on top
    of CLIP by using it as its backbone for zero-shot object detection. After pretraining,
    an object detection head is added to make a set prediction over the (`class`,
    `bounding box`) pairs.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIP](model_doc/clip)é‡‡ç”¨äº†ä¸åŒçš„æ–¹æ³•ï¼Œå¯¹(`å›¾åƒ`ï¼Œ`æ–‡æœ¬`)è¿›è¡Œä¸€å¯¹é¢„æµ‹ã€‚ä¸€ä¸ªå›¾åƒç¼–ç å™¨ï¼ˆViTï¼‰å’Œä¸€ä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼ˆTransformerï¼‰åœ¨ä¸€ä¸ªåŒ…å«4äº¿ä¸ª(`å›¾åƒ`ï¼Œ`æ–‡æœ¬`)å¯¹çš„æ•°æ®é›†ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼Œä»¥æœ€å¤§åŒ–(`å›¾åƒ`ï¼Œ`æ–‡æœ¬`)å¯¹çš„å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚åœ¨é¢„è®­ç»ƒä¹‹åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨è‡ªç„¶è¯­è¨€æŒ‡ç¤ºCLIPé¢„æµ‹ç»™å®šå›¾åƒçš„æ–‡æœ¬ï¼Œåä¹‹äº¦ç„¶ã€‚[OWL-ViT](model_doc/owlvit)åœ¨CLIPçš„åŸºç¡€ä¸Šæ„å»ºï¼Œå°†å…¶ä½œä¸ºé›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹çš„éª¨å¹²ã€‚åœ¨é¢„è®­ç»ƒä¹‹åï¼Œæ·»åŠ äº†ä¸€ä¸ªç›®æ ‡æ£€æµ‹å¤´ï¼Œä»¥å¯¹(`ç±»åˆ«`ï¼Œ`è¾¹ç•Œæ¡†`)å¯¹è¿›è¡Œé›†åˆé¢„æµ‹ã€‚'
- en: Encoder-decoder
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨-è§£ç å™¨
- en: Optical character recognition (OCR) is a long-standing text recognition task
    that typically involves several components to understand the image and generate
    the text. [TrOCR](model_doc/trocr) simplifies the process using an end-to-end
    Transformer. The encoder is a ViT-style model for image understanding and processes
    the image as fixed-size patches. The decoder accepts the encoderâ€™s hidden states
    and autoregressively generates text. [Donut](model_doc/donut) is a more general
    visual document understanding model that doesnâ€™t rely on OCR-based approaches.
    It uses a Swin Transformer as the encoder and multilingual BART as the decoder.
    Donut is pretrained to read text by predicting the next word based on the image
    and text annotations. The decoder generates a token sequence given a prompt. The
    prompt is represented by a special token for each downstream task. For example,
    document parsing has a special `parsing` token that is combined with the encoder
    hidden states to parse the document into a structured output format (JSON).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FiB3Y6RvWYki7ZuKO6tNgZq%2Freinforcement-learning%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FiB3Y6RvWYki7ZuKO6tNgZq%2Freinforcement-learning%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Decoder
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Decision and Trajectory Transformer casts the state, action, and reward
    as a sequence modeling problem. The [Decision Transformer](model_doc/decision_transformer)
    generates a series of actions that lead to a future desired return based on returns-to-go,
    past states, and actions. For the last *K* timesteps, each of the three modalities
    are converted into token embeddings and processed by a GPT-like model to predict
    a future action token. [Trajectory Transformer](model_doc/trajectory_transformer)
    also tokenizes the states, actions, and rewards and processes them with a GPT
    architecture. Unlike the Decision Transformer, which is focused on reward conditioning,
    the Trajectory Transformer generates future actions with beam search.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
