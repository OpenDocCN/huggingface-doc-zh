["```py\n>>> from transformers import GPT2Config, GPT2Model\n\n>>> # Initializing a GPT2 configuration\n>>> configuration = GPT2Config()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = GPT2Model(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import GPT2Tokenizer\n\n>>> tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[15496, 995]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[18435, 995]\n```", "```py\n>>> from transformers import GPT2TokenizerFast\n\n>>> tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[15496, 995]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[18435, 995]\n```", "```py\n>>> from transformers import AutoTokenizer, GPT2Model\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> model = GPT2Model.from_pretrained(\"gpt2\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, GPT2LMHeadModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, GPT2DoubleHeadsModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> model = GPT2DoubleHeadsModel.from_pretrained(\"gpt2\")\n\n>>> # Add a [CLS] to the vocabulary (we should train it also!)\n>>> num_added_tokens = tokenizer.add_special_tokens({\"cls_token\": \"[CLS]\"})\n>>> # Update the model embeddings with the new vocabulary size\n>>> embedding_layer = model.resize_token_embeddings(len(tokenizer))\n\n>>> choices = [\"Hello, my dog is cute [CLS]\", \"Hello, my cat is cute [CLS]\"]\n>>> encoded_choices = [tokenizer.encode(s) for s in choices]\n>>> cls_token_location = [tokens.index(tokenizer.cls_token_id) for tokens in encoded_choices]\n\n>>> input_ids = torch.tensor(encoded_choices).unsqueeze(0)  # Batch size: 1, number of choices: 2\n>>> mc_token_ids = torch.tensor([cls_token_location])  # Batch size: 1\n\n>>> outputs = model(input_ids, mc_token_ids=mc_token_ids)\n>>> lm_logits = outputs.logits\n>>> mc_logits = outputs.mc_logits\n```", "```py\n>>> from transformers import AutoTokenizer, GPT2ForQuestionAnswering\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> model = GPT2ForQuestionAnswering.from_pretrained(\"gpt2\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n\n>>> # target is \"nice puppet\"\n>>> target_start_index = torch.tensor([14])\n>>> target_end_index = torch.tensor([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = outputs.loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, GPT2ForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialogRPT-updown\")\n>>> model = GPT2ForSequenceClassification.from_pretrained(\"microsoft/DialogRPT-updown\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = GPT2ForSequenceClassification.from_pretrained(\"microsoft/DialogRPT-updown\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, GPT2ForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialogRPT-updown\")\n>>> model = GPT2ForSequenceClassification.from_pretrained(\"microsoft/DialogRPT-updown\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = GPT2ForSequenceClassification.from_pretrained(\n...     \"microsoft/DialogRPT-updown\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> from transformers import AutoTokenizer, GPT2ForTokenClassification\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"brad1141/gpt2-finetuned-comp2\")\n>>> model = GPT2ForTokenClassification.from_pretrained(\"brad1141/gpt2-finetuned-comp2\")\n\n>>> inputs = tokenizer(\n...     \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n... )\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_token_class_ids = logits.argmax(-1)\n\n>>> # Note that tokens are classified rather then input words which means that\n>>> # there might be more predicted token classes than words.\n>>> # Multiple token classes might account for the same word\n>>> predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n>>> predicted_tokens_classes\n['Lead', 'Lead', 'Lead', 'Position', 'Lead', 'Lead', 'Lead', 'Lead', 'Lead', 'Lead', 'Lead', 'Lead']\n\n>>> labels = predicted_token_class_ids\n>>> loss = model(**inputs, labels=labels).loss\n>>> round(loss.item(), 2)\n0.25\n```", "```py\n>>> from transformers import AutoTokenizer, TFGPT2Model\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> model = TFGPT2Model.from_pretrained(\"gpt2\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, TFGPT2LMHeadModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n>>> logits = outputs.logits\n```", "```py\n>>> import tensorflow as tf\n>>> from transformers import AutoTokenizer, TFGPT2DoubleHeadsModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> model = TFGPT2DoubleHeadsModel.from_pretrained(\"gpt2\")\n\n>>> # Add a [CLS] to the vocabulary (we should train it also!)\n>>> num_added_tokens = tokenizer.add_special_tokens({\"cls_token\": \"[CLS]\"})\n\n>>> embedding_layer = model.resize_token_embeddings(\n...     len(tokenizer)\n... )  # Update the model embeddings with the new vocabulary size\n\n>>> choices = [\"Hello, my dog is cute [CLS]\", \"Hello, my cat is cute [CLS]\"]\n>>> encoded_choices = [tokenizer.encode(s) for s in choices]\n>>> cls_token_location = [tokens.index(tokenizer.cls_token_id) for tokens in encoded_choices]\n\n>>> input_ids = tf.constant(encoded_choices)[None, :]  # Batch size: 1, number of choices: 2\n>>> mc_token_ids = tf.constant([cls_token_location])  # Batch size: 1\n\n>>> outputs = model(input_ids, mc_token_ids=mc_token_ids)\n>>> lm_prediction_scores, mc_prediction_scores = outputs[:2]\n```", "```py\n>>> from transformers import AutoTokenizer, TFGPT2ForSequenceClassification\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialogRPT-updown\")\n>>> model = TFGPT2ForSequenceClassification.from_pretrained(\"microsoft/DialogRPT-updown\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n\n>>> logits = model(**inputs).logits\n\n>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n```", "```py\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = TFGPT2ForSequenceClassification.from_pretrained(\"microsoft/DialogRPT-updown\", num_labels=num_labels)\n\n>>> labels = tf.constant(1)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\nfrom transformers import TFGPT2Tokenizer\n\ntf_tokenizer = TFGPT2Tokenizer.from_pretrained(\"gpt2\")\n```", "```py\nfrom transformers import AutoTokenizer, TFGPT2Tokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntf_tokenizer = TFGPT2Tokenizer.from_tokenizer(tokenizer)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxGPT2Model\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> model = FlaxGPT2Model.from_pretrained(\"gpt2\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxGPT2LMHeadModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n>>> model = FlaxGPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"np\")\n>>> outputs = model(**inputs)\n\n>>> # retrieve logts for next token\n>>> next_token_logits = outputs.logits[:, -1]\n```"]