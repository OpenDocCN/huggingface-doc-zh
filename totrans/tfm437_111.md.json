["```py\n( )\n```", "```py\n( args state model )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( early_stopping_patience: int = 1 early_stopping_threshold: Optional = 0.0 )\n```", "```py\n( tb_writer = None )\n```", "```py\n( )\n```", "```py\n( args state model **kwargs )\n```", "```py\n( )\n```", "```py\n( args state model )\n```", "```py\n( azureml_run = None )\n```", "```py\n( )\n```", "```py\n( api_token: Optional = None project: Optional = None name: Optional = None base_namespace: str = 'finetuning' run = None log_parameters: bool = True log_checkpoints: Optional = None **neptune_run_kwargs )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( *args **kwargs )\n```", "```py\n( save_log_history: bool = True sync_checkpoints: bool = True )\n```", "```py\n# Note: This example skips over some setup steps for brevity.\nfrom flytekit import current_context, task\n\n@task\ndef train_hf_transformer():\n    cp = current_context().checkpoint\n    trainer = Trainer(..., callbacks=[FlyteCallback()])\n    output = trainer.train(resume_from_checkpoint=cp.restore())\n```", "```py\n( live: Optional = None log_model: Union = None **kwargs )\n```", "```py\n( args state model )\n```", "```py\n( )\n```", "```py\nclass PrinterCallback(TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        _ = logs.pop(\"total_flos\", None)\n        if state.is_local_process_zero:\n            print(logs)\n```", "```py\n( args: TrainingArguments state: TrainerState control: TrainerControl **kwargs )\n```", "```py\n( args: TrainingArguments state: TrainerState control: TrainerControl **kwargs )\n```", "```py\n( args: TrainingArguments state: TrainerState control: TrainerControl **kwargs )\n```", "```py\n( args: TrainingArguments state: TrainerState control: TrainerControl **kwargs )\n```", "```py\n( args: TrainingArguments state: TrainerState control: TrainerControl **kwargs )\n```", "```py\n( args: TrainingArguments state: TrainerState control: TrainerControl metrics **kwargs )\n```", "```py\n( args: TrainingArguments state: TrainerState control: TrainerControl **kwargs )\n```", "```py\n( args: TrainingArguments state: TrainerState control: TrainerControl **kwargs )\n```", "```py\n( args: TrainingArguments state: TrainerState control: TrainerControl **kwargs )\n```", "```py\n( args: TrainingArguments state: TrainerState control: TrainerControl **kwargs )\n```", "```py\n( args: TrainingArguments state: TrainerState control: TrainerControl **kwargs )\n```", "```py\n( args: TrainingArguments state: TrainerState control: TrainerControl **kwargs )\n```", "```py\n( args: TrainingArguments state: TrainerState control: TrainerControl **kwargs )\n```", "```py\nclass MyCallback(TrainerCallback):\n    \"A callback that prints a message at the beginning of training\"\n\n    def on_train_begin(self, args, state, control, **kwargs):\n        print(\"Starting training\")\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    callbacks=[MyCallback],  # We can either pass the callback class this way or an instance of it (MyCallback())\n)\n```", "```py\ntrainer = Trainer(...)\ntrainer.add_callback(MyCallback)\n# Alternatively, we can pass an instance of the callback class\ntrainer.add_callback(MyCallback())\n```", "```py\n( epoch: Optional = None global_step: int = 0 max_steps: int = 0 logging_steps: int = 500 eval_steps: int = 500 save_steps: int = 500 train_batch_size: int = None num_train_epochs: int = 0 num_input_tokens_seen: int = 0 total_flos: float = 0 log_history: List = None best_metric: Optional = None best_model_checkpoint: Optional = None is_local_process_zero: bool = True is_world_process_zero: bool = True is_hyper_param_search: bool = False trial_name: str = None trial_params: Dict = None )\n```", "```py\n( json_path: str )\n```", "```py\n( json_path: str )\n```", "```py\n( should_training_stop: bool = False should_epoch_stop: bool = False should_save: bool = False should_evaluate: bool = False should_log: bool = False )\n```"]