- en: Using Optimum Neuron on Amazon SageMaker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨äºšé©¬é€ŠSageMakerä¸Šä½¿ç”¨Optimum Neuron
- en: 'Original text: [https://huggingface.co/docs/optimum-neuron/guides/sagemaker](https://huggingface.co/docs/optimum-neuron/guides/sagemaker)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/optimum-neuron/guides/sagemaker](https://huggingface.co/docs/optimum-neuron/guides/sagemaker)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Optimum Neuron](https://github.com/huggingface/optimum-neuron) is integrated
    into Amazon SageMaker through the Hugging Face Deep Learning Containers for AWS
    Accelerators like Inferentia2 and Trainium1\. This allows you to easily train
    and deploy ğŸ¤— Transformers and Diffusers models on Amazon SageMaker leveraging
    AWS accelerators.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Optimum Neuron](https://github.com/huggingface/optimum-neuron)å·²ç»é›†æˆåˆ°äºšé©¬é€ŠSageMakerä¸­ï¼Œé€šè¿‡Hugging
    Faceæ·±åº¦å­¦ä¹ å®¹å™¨æ”¯æŒAWSåŠ é€Ÿå™¨ï¼Œå¦‚Inferentia2å’ŒTrainium1ã€‚è¿™ä½¿å¾—æ‚¨å¯ä»¥è½»æ¾åœ°åœ¨äºšé©¬é€ŠSageMakerä¸Šè®­ç»ƒå’Œéƒ¨ç½²ğŸ¤— Transformerså’ŒDiffusersæ¨¡å‹ï¼Œåˆ©ç”¨AWSåŠ é€Ÿå™¨ã€‚'
- en: The Hugging Face DLC images come with pre-installed Optimum Neuron and tools
    to compile models for efficient inference on Inferentia2 and Trainium1\. This
    makes deploying large transformer models simple and optimized out of the box.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face DLCé•œåƒé¢„è£…äº†Optimum Neuronå’Œç”¨äºåœ¨Inferentia2å’ŒTrainium1ä¸Šè¿›è¡Œé«˜æ•ˆæ¨ç†æ¨¡å‹ç¼–è¯‘çš„å·¥å…·ã€‚è¿™ä½¿å¾—éƒ¨ç½²å¤§å‹è½¬æ¢å™¨æ¨¡å‹å˜å¾—ç®€å•ä¸”å¼€ç®±å³ç”¨ä¼˜åŒ–ã€‚
- en: Below is a list of available end-to-end tutorials on using Optimum Neuron via
    the Hugging Face DLC to train and deploy models on Amazon SageMaker. Follow the
    end-to-end examples to learn how Optimum Neuron integrates with SageMaker through
    the Hugging Face DLC images to unlock performance and cost benefits.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä½¿ç”¨Hugging Face DLCé€šè¿‡Optimum Neuronåœ¨äºšé©¬é€ŠSageMakerä¸Šè®­ç»ƒå’Œéƒ¨ç½²æ¨¡å‹çš„ç«¯åˆ°ç«¯æ•™ç¨‹åˆ—è¡¨ã€‚è·Ÿéšç«¯åˆ°ç«¯ç¤ºä¾‹å­¦ä¹ Optimum
    Neuronå¦‚ä½•é€šè¿‡Hugging Face DLCé•œåƒä¸SageMakeré›†æˆï¼Œè§£é”æ€§èƒ½å’Œæˆæœ¬ä¼˜åŠ¿ã€‚
- en: Deploy Embedding Models on Inferentia2 for Efficient Similarity Search
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨Inferentia2ä¸Šéƒ¨ç½²åµŒå…¥æ¨¡å‹ä»¥è¿›è¡Œé«˜æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢
- en: Tutorial on how to deploy a text embedding model (BGE-Base) for efficient and
    fast embedding generation on AWS Inferentia2 using Amazon SageMaker; The post
    shows how Inferentia2 can be a great option for not only efficient and fast but
    also cost-effective inference of embeddings compared to GPUs or services like
    OpenAI and Amazon Bedrock.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ•™ç¨‹ä»‹ç»å¦‚ä½•åœ¨AWS Inferentia2ä¸Šä½¿ç”¨äºšé©¬é€ŠSageMakeréƒ¨ç½²æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼ˆBGE-Baseï¼‰ï¼Œå®ç°é«˜æ•ˆå¿«é€Ÿçš„åµŒå…¥ç”Ÿæˆï¼›è¯¥æ–‡ç« å±•ç¤ºäº†Inferentia2ä¸ä»…å¯ä»¥æˆä¸ºé«˜æ•ˆå¿«é€Ÿï¼Œè€Œä¸”æˆæœ¬æ•ˆç›Šçš„åµŒå…¥æ¨ç†é€‰æ‹©ï¼Œç›¸æ¯”GPUæˆ–OpenAIå’ŒAmazon
    Bedrockç­‰æœåŠ¡ã€‚
- en: '[Tutorial](https://www.philschmid.de/inferentia2-embeddings)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ•™ç¨‹](https://www.philschmid.de/inferentia2-embeddings)'
- en: '[GitHub Repo](https://github.com/philschmid/huggingface-inferentia2-samples/blob/main/llama2-7b/sagemaker-notebook.ipynb)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub ä»“åº“](https://github.com/philschmid/huggingface-inferentia2-samples/blob/main/llama2-7b/sagemaker-notebook.ipynb)'
- en: Deploy Llama 2 7B on AWS inferentia2 with Amazon SageMaker
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨AWS Inferentia2ä¸Šä½¿ç”¨äºšé©¬é€ŠSageMakeréƒ¨ç½²Llama 2 7B
- en: Tutorial on how to deploy the conversational Llama 2 model on AWS Inferentia2
    using Amazon SageMaker for low-latency inference; Shows how to leverage Inferentia2
    and SageMaker to go from model training to production deployment with just a few
    lines of code.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ•™ç¨‹ä»‹ç»å¦‚ä½•åœ¨AWS Inferentia2ä¸Šä½¿ç”¨äºšé©¬é€ŠSageMakeréƒ¨ç½²å¯¹è¯Llama 2æ¨¡å‹è¿›è¡Œä½å»¶è¿Ÿæ¨ç†ï¼›å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨Inferentia2å’ŒSageMakerä»æ¨¡å‹è®­ç»ƒåˆ°ç”Ÿäº§éƒ¨ç½²åªéœ€å‡ è¡Œä»£ç ã€‚
- en: '[Tutorial](https://www.philschmid.de/inferentia2-llama-7b)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ•™ç¨‹](https://www.philschmid.de/inferentia2-llama-7b)'
- en: '[GitHub Repo](https://github.com/philschmid/huggingface-inferentia2-samples/blob/main/stable-diffusion-xl/sagemaker-notebook.ipynb)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub ä»“åº“](https://github.com/philschmid/huggingface-inferentia2-samples/blob/main/stable-diffusion-xl/sagemaker-notebook.ipynb)'
- en: Deploy Stable Diffusion XL on AWS inferentia2 with Amazon SageMaker
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨AWS Inferentia2ä¸Šä½¿ç”¨äºšé©¬é€ŠSageMakeréƒ¨ç½²Stable Diffusion XL
- en: Tutorial on how to deploy Stable Diffusion XL model on AWS Inferentia2 using
    Optimum Neuron and Amazon SageMaker for efficient 1024x1024 image generation achieving
    ~6 seconds per image; The post shows how a single `inf2.xlarge` instance costing
    $0.99/hour can achieve ~10 images per minute, making Inferentia2 a great option
    for not only efficient and fast but also cost-effective inference of images compared
    to GPUs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ•™ç¨‹ä»‹ç»å¦‚ä½•åœ¨AWS Inferentia2ä¸Šä½¿ç”¨Optimum Neuronå’Œäºšé©¬é€ŠSageMakeréƒ¨ç½²Stable Diffusion XLæ¨¡å‹ï¼Œå®ç°1024x1024å›¾åƒç”Ÿæˆï¼Œæ¯å¼ å›¾åƒçº¦6ç§’ï¼›è¯¥æ–‡ç« å±•ç¤ºäº†å¦‚ä½•é€šè¿‡å•ä¸ª`inf2.xlarge`å®ä¾‹ï¼ˆæ¯å°æ—¶æˆæœ¬ä¸º$0.99ï¼‰å¯ä»¥å®ç°æ¯åˆ†é’Ÿçº¦10å¼ å›¾åƒï¼Œä½¿å¾—Inferentia2æˆä¸ºå›¾åƒæ¨ç†çš„é«˜æ•ˆã€å¿«é€Ÿä¸”æˆæœ¬æ•ˆç›Šçš„é€‰æ‹©ï¼Œç›¸æ¯”GPUã€‚
- en: '[Tutorial](https://www.philschmid.de/inferentia2-stable-diffusion-xl)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ•™ç¨‹](https://www.philschmid.de/inferentia2-stable-diffusion-xl)'
- en: '[GitHub Repo](https://github.com/Placeholder/stable-diffusion-inferentia)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub ä»“åº“](https://github.com/Placeholder/stable-diffusion-inferentia)'
- en: Deploy BERT for Text Classification on AWS inferentia2 with Amazon SageMaker
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨AWS Inferentia2ä¸Šä½¿ç”¨äºšé©¬é€ŠSageMakeréƒ¨ç½²BERTè¿›è¡Œæ–‡æœ¬åˆ†ç±»
- en: Tutorial on how to optimize and deploy BERT model on AWS Inferentia2 using Optimum
    Neuron and Amazon SageMaker for efficient text classification achieving 4ms latency;
    The post shows how a single inf2.xlarge instance costing $0.99/hour can achieve
    116 inferences/sec and 500 inferences/sec without network overhead, making Inferentia2
    a great option for low-latency and cost-effective inference compared to GPUs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ•™ç¨‹ä»‹ç»å¦‚ä½•ä½¿ç”¨Optimum Neuronå’Œäºšé©¬é€ŠSageMakeråœ¨AWS Inferentia2ä¸Šä¼˜åŒ–å’Œéƒ¨ç½²BERTæ¨¡å‹ï¼Œå®ç°é«˜æ•ˆçš„æ–‡æœ¬åˆ†ç±»ï¼Œè¾¾åˆ°4æ¯«ç§’çš„å»¶è¿Ÿï¼›è¯¥æ–‡ç« å±•ç¤ºäº†å¦‚ä½•é€šè¿‡å•ä¸ªinf2.xlargeå®ä¾‹ï¼ˆæ¯å°æ—¶æˆæœ¬ä¸º$0.99ï¼‰å¯ä»¥å®ç°116æ¬¡æ¨ç†/ç§’å’Œ500æ¬¡æ¨ç†/ç§’ï¼Œæ— éœ€ç½‘ç»œå¼€é”€ï¼Œä½¿å¾—Inferentia2ç›¸æ¯”GPUæˆä¸ºä½å»¶è¿Ÿå’Œæˆæœ¬æ•ˆç›Šæ¨ç†çš„ç»ä½³é€‰æ‹©ã€‚
- en: '[Tutorial](https://www.philschmid.de/optimize-deploy-bert-inf2)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ•™ç¨‹](https://www.philschmid.de/optimize-deploy-bert-inf2)'
- en: '[GitHub Repo](https://github.com/philschmid/huggingface-inferentia2-samples/blob/main/bert-transformers/sagemaker-notebook.ipynb)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub ä»“åº“](https://github.com/philschmid/huggingface-inferentia2-samples/blob/main/bert-transformers/sagemaker-notebook.ipynb)'
