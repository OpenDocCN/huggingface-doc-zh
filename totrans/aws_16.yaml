- en: Using Optimum Neuron on Amazon SageMaker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在亚马逊SageMaker上使用Optimum Neuron
- en: 'Original text: [https://huggingface.co/docs/optimum-neuron/guides/sagemaker](https://huggingface.co/docs/optimum-neuron/guides/sagemaker)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/optimum-neuron/guides/sagemaker](https://huggingface.co/docs/optimum-neuron/guides/sagemaker)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Optimum Neuron](https://github.com/huggingface/optimum-neuron) is integrated
    into Amazon SageMaker through the Hugging Face Deep Learning Containers for AWS
    Accelerators like Inferentia2 and Trainium1\. This allows you to easily train
    and deploy 🤗 Transformers and Diffusers models on Amazon SageMaker leveraging
    AWS accelerators.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Optimum Neuron](https://github.com/huggingface/optimum-neuron)已经集成到亚马逊SageMaker中，通过Hugging
    Face深度学习容器支持AWS加速器，如Inferentia2和Trainium1。这使得您可以轻松地在亚马逊SageMaker上训练和部署🤗 Transformers和Diffusers模型，利用AWS加速器。'
- en: The Hugging Face DLC images come with pre-installed Optimum Neuron and tools
    to compile models for efficient inference on Inferentia2 and Trainium1\. This
    makes deploying large transformer models simple and optimized out of the box.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face DLC镜像预装了Optimum Neuron和用于在Inferentia2和Trainium1上进行高效推理模型编译的工具。这使得部署大型转换器模型变得简单且开箱即用优化。
- en: Below is a list of available end-to-end tutorials on using Optimum Neuron via
    the Hugging Face DLC to train and deploy models on Amazon SageMaker. Follow the
    end-to-end examples to learn how Optimum Neuron integrates with SageMaker through
    the Hugging Face DLC images to unlock performance and cost benefits.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用Hugging Face DLC通过Optimum Neuron在亚马逊SageMaker上训练和部署模型的端到端教程列表。跟随端到端示例学习Optimum
    Neuron如何通过Hugging Face DLC镜像与SageMaker集成，解锁性能和成本优势。
- en: Deploy Embedding Models on Inferentia2 for Efficient Similarity Search
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Inferentia2上部署嵌入模型以进行高效的相似性搜索
- en: Tutorial on how to deploy a text embedding model (BGE-Base) for efficient and
    fast embedding generation on AWS Inferentia2 using Amazon SageMaker; The post
    shows how Inferentia2 can be a great option for not only efficient and fast but
    also cost-effective inference of embeddings compared to GPUs or services like
    OpenAI and Amazon Bedrock.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 教程介绍如何在AWS Inferentia2上使用亚马逊SageMaker部署文本嵌入模型（BGE-Base），实现高效快速的嵌入生成；该文章展示了Inferentia2不仅可以成为高效快速，而且成本效益的嵌入推理选择，相比GPU或OpenAI和Amazon
    Bedrock等服务。
- en: '[Tutorial](https://www.philschmid.de/inferentia2-embeddings)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[教程](https://www.philschmid.de/inferentia2-embeddings)'
- en: '[GitHub Repo](https://github.com/philschmid/huggingface-inferentia2-samples/blob/main/llama2-7b/sagemaker-notebook.ipynb)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub 仓库](https://github.com/philschmid/huggingface-inferentia2-samples/blob/main/llama2-7b/sagemaker-notebook.ipynb)'
- en: Deploy Llama 2 7B on AWS inferentia2 with Amazon SageMaker
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在AWS Inferentia2上使用亚马逊SageMaker部署Llama 2 7B
- en: Tutorial on how to deploy the conversational Llama 2 model on AWS Inferentia2
    using Amazon SageMaker for low-latency inference; Shows how to leverage Inferentia2
    and SageMaker to go from model training to production deployment with just a few
    lines of code.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 教程介绍如何在AWS Inferentia2上使用亚马逊SageMaker部署对话Llama 2模型进行低延迟推理；展示了如何利用Inferentia2和SageMaker从模型训练到生产部署只需几行代码。
- en: '[Tutorial](https://www.philschmid.de/inferentia2-llama-7b)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[教程](https://www.philschmid.de/inferentia2-llama-7b)'
- en: '[GitHub Repo](https://github.com/philschmid/huggingface-inferentia2-samples/blob/main/stable-diffusion-xl/sagemaker-notebook.ipynb)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub 仓库](https://github.com/philschmid/huggingface-inferentia2-samples/blob/main/stable-diffusion-xl/sagemaker-notebook.ipynb)'
- en: Deploy Stable Diffusion XL on AWS inferentia2 with Amazon SageMaker
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在AWS Inferentia2上使用亚马逊SageMaker部署Stable Diffusion XL
- en: Tutorial on how to deploy Stable Diffusion XL model on AWS Inferentia2 using
    Optimum Neuron and Amazon SageMaker for efficient 1024x1024 image generation achieving
    ~6 seconds per image; The post shows how a single `inf2.xlarge` instance costing
    $0.99/hour can achieve ~10 images per minute, making Inferentia2 a great option
    for not only efficient and fast but also cost-effective inference of images compared
    to GPUs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 教程介绍如何在AWS Inferentia2上使用Optimum Neuron和亚马逊SageMaker部署Stable Diffusion XL模型，实现1024x1024图像生成，每张图像约6秒；该文章展示了如何通过单个`inf2.xlarge`实例（每小时成本为$0.99）可以实现每分钟约10张图像，使得Inferentia2成为图像推理的高效、快速且成本效益的选择，相比GPU。
- en: '[Tutorial](https://www.philschmid.de/inferentia2-stable-diffusion-xl)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[教程](https://www.philschmid.de/inferentia2-stable-diffusion-xl)'
- en: '[GitHub Repo](https://github.com/Placeholder/stable-diffusion-inferentia)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub 仓库](https://github.com/Placeholder/stable-diffusion-inferentia)'
- en: Deploy BERT for Text Classification on AWS inferentia2 with Amazon SageMaker
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在AWS Inferentia2上使用亚马逊SageMaker部署BERT进行文本分类
- en: Tutorial on how to optimize and deploy BERT model on AWS Inferentia2 using Optimum
    Neuron and Amazon SageMaker for efficient text classification achieving 4ms latency;
    The post shows how a single inf2.xlarge instance costing $0.99/hour can achieve
    116 inferences/sec and 500 inferences/sec without network overhead, making Inferentia2
    a great option for low-latency and cost-effective inference compared to GPUs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 教程介绍如何使用Optimum Neuron和亚马逊SageMaker在AWS Inferentia2上优化和部署BERT模型，实现高效的文本分类，达到4毫秒的延迟；该文章展示了如何通过单个inf2.xlarge实例（每小时成本为$0.99）可以实现116次推理/秒和500次推理/秒，无需网络开销，使得Inferentia2相比GPU成为低延迟和成本效益推理的绝佳选择。
- en: '[Tutorial](https://www.philschmid.de/optimize-deploy-bert-inf2)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[教程](https://www.philschmid.de/optimize-deploy-bert-inf2)'
- en: '[GitHub Repo](https://github.com/philschmid/huggingface-inferentia2-samples/blob/main/bert-transformers/sagemaker-notebook.ipynb)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub 仓库](https://github.com/philschmid/huggingface-inferentia2-samples/blob/main/bert-transformers/sagemaker-notebook.ipynb)'
