# unCLIP

> 原始文本：[`huggingface.co/docs/diffusers/api/pipelines/unclip`](https://huggingface.co/docs/diffusers/api/pipelines/unclip)

[具有 CLIP 潜在特征的分层文本条件图像生成](https://huggingface.co/papers/2204.06125) 是由 Aditya Ramesh，Prafulla Dhariwal，Alex Nichol，Casey Chu，Mark Chen 撰写的。🤗 Diffusers 中的 unCLIP 模型来自 kakaobrain 的[karlo](https://github.com/kakaobrain/karlo)。

论文摘要如下：

*像 CLIP 这样的对比模型已经被证明可以学习到捕捉语义和风格的图像的稳健表示。为了利用这些表示来进行图像生成，我们提出了一个两阶段模型：一个先验模型根据文本标题生成一个 CLIP 图像嵌入，一个解码器根据图像嵌入生成一个图像。我们展示了明确生成图像表示可以提高图像多样性，同时最小化逼真度和标题相似性的损失。我们的解码器根据图像表示可以产生保留其语义和风格的图像变体，同时变化非必要的细节，这些细节在图像表示中不存在。此外，CLIP 的联合嵌入空间使得可以通过语言引导的图像操作以零样本的方式进行。我们使用扩散模型进行解码器，并尝试先验模型使用自回归模型和扩散模型，发现后者在计算上更有效，并产生更高质量的样本。*

您可以在[lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch)找到 lucidrains 的 DALL-E 2 重建。

确保查看调度程序指南以了解如何在调度程序速度和质量之间进行权衡，并查看重用组件跨管道部分以了解如何有效地将相同组件加载到多个管道中。

## UnCLIPPipeline

### `class diffusers.UnCLIPPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip.py#L34)

```py
( prior: PriorTransformer decoder: UNet2DConditionModel text_encoder: CLIPTextModelWithProjection tokenizer: CLIPTokenizer text_proj: UnCLIPTextProjModel super_res_first: UNet2DModel super_res_last: UNet2DModel prior_scheduler: UnCLIPScheduler decoder_scheduler: UnCLIPScheduler super_res_scheduler: UnCLIPScheduler )
```

参数

+   `text_encoder` ([CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection)) — 冻结的文本编码器。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 用于对文本进行标记化的 `CLIPTokenizer`。

+   `prior` (PriorTransformer) — 用于从文本嵌入近似图像嵌入的规范 unCLIP 先验。

+   `text_proj` (`UnCLIPTextProjModel`) — 在传递给解码器之前准备和组合嵌入的实用类。

+   `decoder` (UNet2DConditionModel) — 将图像嵌入反转为图像的解码器。

+   `super_res_first` (UNet2DModel) — 超分辨率 UNet。用于超分辨率扩散过程的除最后一步之外的所有步骤。

+   `super_res_last` (UNet2DModel) — 超分辨率 UNet。用于超分辨率扩散过程的最后一步。

+   `prior_scheduler` (`UnCLIPScheduler`) — 用于先验去噪过程的调度程序（修改的 DDPMScheduler）。

+   `decoder_scheduler` (`UnCLIPScheduler`) — 用于解码器去噪过程的调度程序（修改的 DDPMScheduler）。

+   `super_res_scheduler` (`UnCLIPScheduler`) — 用于超分辨率去噪过程的调度程序（修改的 DDPMScheduler）。

使用 unCLIP 进行文本到图像生成的管道。

该模型继承自 DiffusionPipeline。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip.py#L211)

```py
( prompt: Union = None num_images_per_prompt: int = 1 prior_num_inference_steps: int = 25 decoder_num_inference_steps: int = 25 super_res_num_inference_steps: int = 7 generator: Union = None prior_latents: Optional = None decoder_latents: Optional = None super_res_latents: Optional = None text_model_output: Union = None text_attention_mask: Optional = None prior_guidance_scale: float = 4.0 decoder_guidance_scale: float = 8.0 output_type: Optional = 'pil' return_dict: bool = True ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`) — 用于指导图像生成的提示。只有在传递 `text_model_output` 和 `text_attention_mask` 时才能将其留空。

+   `num_images_per_prompt` (`int`, *optional*, 默认为 1) — 每个提示生成的图像数量。

+   `prior_num_inference_steps` (`int`, *optional*, 默认为 25) — 先验的去噪步骤数量。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `decoder_num_inference_steps` (`int`, *optional*, 默认为 25) — 解码器的去噪步骤数量。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `super_res_num_inference_steps` (`int`, *optional*, 默认为 7) — 超分辨率去噪步骤的数量。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 用于使生成结果确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `prior_latents` (`torch.FloatTensor`，形状为 (batch size, embeddings dimension)，*optional*) — 预先生成的嘈杂潜变量，用作先验的输入。

+   `decoder_latents` (`torch.FloatTensor`，形状为 (batch size, channels, height, width)，*optional*) — 预先生成的嘈杂潜变量，用作解码器的输入。

+   `super_res_latents` (`torch.FloatTensor`，形状为 (batch size, channels, super res height, super res width)，*optional*) — 预先生成的嘈杂潜变量，用作解码器的输入。

+   `prior_guidance_scale` (`float`, *optional*, 默认为 4.0) — 更高的引导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用引导比例。

+   `decoder_guidance_scale` (`float`, *optional*, 默认为 4.0) — 更高的引导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用引导比例。

+   `text_model_output` (`CLIPTextModelOutput`, *optional*) — 可从文本编码器派生的预定义 `CLIPTextModel` 输出。预定义的文本输出可以用于诸如文本嵌入插值之类的任务。在这种情况下，确保还传递 `text_attention_mask`。`prompt` 可以留空。

+   `text_attention_mask` (`torch.Tensor`, *optional*) — 预定义的 CLIP 文本注意力掩码，可以从分词器中派生。当传递 `text_model_output` 时，预定义的文本注意力掩码是必要的。

+   `output_type` (`str`, *optional*, 默认为`"pil"`) — 生成图像的输出格式。选择 `PIL.Image` 或 `np.array` 之间的一个。

+   `return_dict` (`bool`, *optional*, 默认为 `True`) — 是否返回 ImagePipelineOutput 而不是一个普通的 tuple。

返回

ImagePipelineOutput 或 `tuple`

如果 `return_dict` 为 `True`，则返回 ImagePipelineOutput，否则返回一个 `tuple`，其中第一个元素是包含生成图像的列表。

用于生成的管道的调用函数。

## UnCLIPImageVariationPipeline

### `class diffusers.UnCLIPImageVariationPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py#L39)

```py
( decoder: UNet2DConditionModel text_encoder: CLIPTextModelWithProjection tokenizer: CLIPTokenizer text_proj: UnCLIPTextProjModel feature_extractor: CLIPImageProcessor image_encoder: CLIPVisionModelWithProjection super_res_first: UNet2DModel super_res_last: UNet2DModel decoder_scheduler: UnCLIPScheduler super_res_scheduler: UnCLIPScheduler )
```

参数

+   `text_encoder`（[CLIPTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection)）— 冻结的文本编码器。

+   `tokenizer`（[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)）— 用于对文本进行标记化的 `CLIPTokenizer`。

+   `feature_extractor`（[CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)）— 从生成的图像中提取特征的模型，用作 `image_encoder` 的输入。

+   `image_encoder`（[CLIPVisionModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection)）— 冻结的 CLIP 图像编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `text_proj`（`UnCLIPTextProjModel`）— 在传递给解码器之前准备和组合嵌入的实用类。

+   `decoder`（UNet2DConditionModel）— 将图像嵌入反转为图像的解码器。

+   `super_res_first`（UNet2DModel）— 超分辨率 UNet。用于超分辨率扩散过程中的所有步骤，除了最后一步。

+   `super_res_last`（UNet2DModel）— 超分辨率 UNet。用于超分辨率扩散过程的最后一步。

+   `decoder_scheduler`（`UnCLIPScheduler`）— 用于解码器去噪过程的调度器（修改自 DDPMScheduler）。

+   `super_res_scheduler`（`UnCLIPScheduler`）— 用于超分辨率去噪过程的调度器（修改自 DDPMScheduler）。

使用 UnCLIP 从输入图像生成图像变化的流水线。

此模型继承自 DiffusionPipeline。查看超类文档以获取所有流水线实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py#L199)

```py
( image: Union = None num_images_per_prompt: int = 1 decoder_num_inference_steps: int = 25 super_res_num_inference_steps: int = 7 generator: Optional = None decoder_latents: Optional = None super_res_latents: Optional = None image_embeddings: Optional = None decoder_guidance_scale: float = 8.0 output_type: Optional = 'pil' return_dict: bool = True ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `image`（`PIL.Image.Image` 或 `List[PIL.Image.Image]` 或 `torch.FloatTensor`）— 代表图像批次的 `Image` 或张量，用作起始点。如果提供张量，则需要与 `CLIPImageProcessor` 的 [配置](https://huggingface.co/fusing/karlo-image-variations-diffusers/blob/main/feature_extractor/preprocessor_config.json) 兼容。只有在传递 `image_embeddings` 时才能保持为 `None`。

+   `num_images_per_prompt`（`int`，*可选*，默认为 1）— 每个提示生成的图像数量。

+   `decoder_num_inference_steps`（`int`，*可选*，默认为 25）— 解码器的去噪步骤数。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `super_res_num_inference_steps`（`int`，*可选*，默认为 7）— 超分辨率的去噪步骤数。更多的去噪步骤通常会导致图像质量更高，但推理速度较慢。

+   `generator`（`torch.Generator`，*可选*）— 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `decoder_latents`（`torch.FloatTensor`，形状为（批量大小，通道数，高度，宽度），*可选*）— 预先生成的噪声潜变量，用作解码器的输入。

+   `super_res_latents` (`torch.FloatTensor`，形状为(batch size, channels, super res height, super res width)，*optional*) — 预生成的噪声潜变量，用作解码器的输入。

+   `decoder_guidance_scale` (`float`, *optional*, 默认值为 4.0) — 较高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。 当`guidance_scale > 1`时启用引导比例。

+   `image_embeddings` (`torch.Tensor`, *optional*) — 可从图像编码器派生的预定义图像嵌入。 可以传递预定义的图像嵌入以用于诸如图像插值之类的任务。 `image` 可以保持为 `None`。

+   `output_type` (`str`, *optional*, 默认值为`"pil"`) — 生成图像的输出格式。 选择`PIL.Image`或`np.array`之间。

+   `return_dict` (`bool`, *optional*, 默认值为`True`) — 是否返回一个 ImagePipelineOutput 而不是一个普通的元组。

返回

ImagePipelineOutput 或 `tuple`

如果`return_dict`为`True`，则返回 ImagePipelineOutput，否则返回一个元组，其中第一个元素是包含生成图像的列表。

用于生成的管道的调用函数。

## ImagePipelineOutput

### `class diffusers.ImagePipelineOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L116)

```py
( images: Union )
```

参数

+   `images` (`List[PIL.Image.Image]` 或 `np.ndarray`) — 长度为`batch_size`的去噪 PIL 图像列表或形状为`(batch_size, height, width, num_channels)`的 NumPy 数组。

图像管道的输出类。
