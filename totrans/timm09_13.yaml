- en: Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/timm/reference/optimizers](https://huggingface.co/docs/timm/reference/optimizers)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: This page contains the API reference documentation for learning rate optimizers
    included in `timm`.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Factory functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '#### `timm.optim.create_optimizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/optim_factory.py#L182)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Legacy optimizer factory for backwards compatibility. NOTE: Use create_optimizer_v2
    for new code.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `timm.optim.create_optimizer_v2`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/optim_factory.py#L193)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model_or_params` (nn.Module) — model containing parameters to optimize opt
    — name of optimizer to create lr — initial learning rate weight_decay — weight
    decay to apply in optimizer momentum — momentum for momentum based optimizers
    (others may use betas via kwargs) foreach — Enable / disable foreach (multi-tensor)
    operation if True / False. Choose safe default if None filter_bias_and_bn — filter
    out bias, bn and other 1d params from weight decay **kwargs — extra optimizer
    specific kwargs to pass through'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create an optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'TODO currently the model is passed in and all parameters are selected for optimization.
    For more general use an interface that allows selection of parameters to optimize
    and lr groups, one of:'
  prefs: []
  type: TYPE_NORMAL
- en: a filter fn interface that further breaks params into groups in a weight_decay
    compatible fashion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: expose the parameters interface and leave it up to caller
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizer Classes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class timm.optim.AdaBelief`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adabelief.py#L6)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr` (float, optional) — learning rate (default: 1e-3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`betas` (Tuple[float, float], optional) — coefficients used for computing running
    averages of gradient and its square (default: (0.9, 0.999))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eps` (float, optional) — term added to the denominator to improve numerical
    stability (default: 1e-16)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight_decay` (float, optional) — weight decay (L2 penalty) (default: 0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`amsgrad` (boolean, optional) — whether to use the AMSGrad variant of this
    algorithm from the paper `On the Convergence of Adam and Beyond`_ (default: False)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoupled_decay` (boolean, optional) — (default: True) If set as True, then
    the optimizer uses decoupled weight decay as in AdamW'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fixed_decay` (boolean, optional) — (default: False) This is used when weight*decouple
    is set as True. When fixed_decay == True, the weight decay is performed as $W*{new}
    = W*{old} - W*{old} \times decay$. When fixed*decay == False, the weight decay
    is performed as $W*{new} = W*{old} - W*{old} \times decay \times lr$. Note that
    in this case, the weight decay ratio decreases with learning rate (lr).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rectify` (boolean, optional) — (default: True) If set as True, then perform
    the rectified update similar to RAdam'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`degenerated_to_sgd` (boolean, optional) (default —True) If set as True, then
    perform SGD update when variance of gradient is high'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implements AdaBelief algorithm. Modified from Adam in PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: 'reference: AdaBelief Optimizer, adapting stepsizes by the belief in observed
    gradients, NeurIPS 2020'
  prefs: []
  type: TYPE_NORMAL
- en: For a complete table of recommended hyperparameters, see [https://github.com/juntang-zhuang/Adabelief-Optimizer’](https://github.com/juntang-zhuang/Adabelief-Optimizer')
    For example train/args for EfficientNet see these gists
  prefs: []
  type: TYPE_NORMAL
- en: 'link to train_scipt: [https://gist.github.com/juntang-zhuang/0a501dd51c02278d952cf159bc233037](https://gist.github.com/juntang-zhuang/0a501dd51c02278d952cf159bc233037)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'link to args.yaml: [https://gist.github.com/juntang-zhuang/517ce3c27022b908bb93f78e4f786dc3](https://gist.github.com/juntang-zhuang/517ce3c27022b908bb93f78e4f786dc3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adabelief.py#L89)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs a single optimization step.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class timm.optim.Adafactor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adafactor.py#L16)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr` (float, optional) — external learning rate (default: None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eps` (tuple[float, float]) — regularization constants for square gradient
    and parameter scale respectively (default: (1e-30, 1e-3))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_threshold` (float) — threshold of root mean square of final gradient
    update (default: 1.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decay_rate` (float) — coefficient used to compute running averages of square
    gradient (default: -0.8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta1` (float) — coefficient used for computing running averages of gradient
    (default: None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight_decay` (float, optional) — weight decay (L2 penalty) (default: 0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scale_parameter` (bool) — if True, learning rate is scaled by root mean square
    of parameter (default: True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`warmup_init` (bool) — time-dependent learning rate computation depends on
    whether warm-up initialization is being used (default: False)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Implements Adafactor algorithm. This implementation is based on: `Adafactor:
    Adaptive Learning Rates with Sublinear Memory Cost` (see [https://arxiv.org/abs/1804.04235](https://arxiv.org/abs/1804.04235))'
  prefs: []
  type: TYPE_NORMAL
- en: Note that this optimizer internally adjusts the learning rate depending on the
    *scale_parameter*, *relative_step* and *warmup_init* options.
  prefs: []
  type: TYPE_NORMAL
- en: To use a manual (external) learning rate schedule you should set `scale_parameter=False`
    and `relative_step=False`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adafactor.py#L79)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs a single optimization step.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class timm.optim.Adahessian`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L9)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr` (float, optional) — learning rate (default: 0.1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`betas` ((float, float), optional) — coefficients used for computing running
    averages of gradient and the squared hessian trace (default: (0.9, 0.999))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eps` (float, optional) — term added to the denominator to improve numerical
    stability (default: 1e-8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight_decay` (float, optional) — weight decay (L2 penalty) (default: 0.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hessian_power` (float, optional) — exponent of the hessian trace (default:
    1.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update_each` (int, optional) — compute the hessian trace approximation only
    after *this* number of steps (to save time) (default: 1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_samples` (int, optional) — how many times to sample `z` for the approximation
    of the hessian trace (default: 1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Implements the AdaHessian algorithm from “ADAHESSIAN: An Adaptive Second OrderOptimizer
    for Machine Learning”'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_params`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L58)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Gets all parameters in all param_groups with gradients
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_hessian`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L74)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Computes the Hutchinson approximation of the hessian trace and accumulates it
    for each trainable parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L102)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`closure` (callable, optional) — a closure that reevaluates the model and returns
    the loss (default — None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs a single optimization step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `zero_hessian`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L65)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Zeros out the accumalated hessian traces.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class timm.optim.AdamP`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adamp.py#L43)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '### `class timm.optim.AdamW`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adamw.py#L12)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr` (float, optional) — learning rate (default: 1e-3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`betas` (Tuple[float, float], optional) — coefficients used for computing running
    averages of gradient and its square (default: (0.9, 0.999))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eps` (float, optional) — term added to the denominator to improve numerical
    stability (default: 1e-8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight_decay` (float, optional) — weight decay coefficient (default: 1e-2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`amsgrad` (boolean, optional) — whether to use the AMSGrad variant of this
    algorithm from the paper `On the Convergence of Adam and Beyond`_ (default: False)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implements AdamW algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The original Adam algorithm was proposed in `Adam: A Method for Stochastic
    Optimization`*. The AdamW variant was proposed in `Decoupled Weight Decay Regularization`*.'
  prefs: []
  type: TYPE_NORMAL
- en: '.. _Adam\: A Method for Stochastic Optimization: [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)
    .. _Decoupled Weight Decay Regularization: [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101)
    .. _On the Convergence of Adam and Beyond: [https://openreview.net/forum?id=ryQu7f-RZ](https://openreview.net/forum?id=ryQu7f-RZ)'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adamw.py#L58)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs a single optimization step.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class timm.optim.Lamb`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lamb.py#L60)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr` (float, optional) — learning rate. (default: 1e-3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`betas` (Tuple[float, float], optional) — coefficients used for computing running
    averages of gradient and its norm. (default: (0.9, 0.999))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eps` (float, optional) — term added to the denominator to improve numerical
    stability. (default: 1e-8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight_decay` (float, optional) — weight decay (L2 penalty) (default: 0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`grad_averaging` (bool, optional) — whether apply (1-beta2) to grad when calculating
    running averages of gradient. (default: True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_grad_norm` (float, optional) — value used to clip global grad norm (default:
    1.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trust_clip` (bool) — enable LAMBC trust ratio clipping (default: False)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`always_adapt` (boolean, optional) — Apply adaptive learning rate to 0.0 weight
    decay parameter (default: False)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Implements a pure pytorch variant of FuseLAMB (NvLamb variant) optimizer from
    apex.optimizers.FusedLAMB reference: [https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/Transformer-XL/pytorch/lamb.py](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/Transformer-XL/pytorch/lamb.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'LAMB was proposed in `Large Batch Optimization for Deep Learning: Training
    BERT in 76 minutes`_.'
  prefs: []
  type: TYPE_NORMAL
- en: '.. _Large Batch Optimization for Deep Learning - Training BERT in 76 minutes:
    [https://arxiv.org/abs/1904.00962](https://arxiv.org/abs/1904.00962) .. _On the
    Convergence of Adam and Beyond: [https://openreview.net/forum?id=ryQu7f-RZ](https://openreview.net/forum?id=ryQu7f-RZ)'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lamb.py#L96)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs a single optimization step.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class timm.optim.Lars`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lars.py#L17)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr` (float, optional) — learning rate (default: 1.0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`momentum` (float, optional) — momentum factor (default: 0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight_decay` (float, optional) — weight decay (L2 penalty) (default: 0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dampening` (float, optional) — dampening for momentum (default: 0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nesterov` (bool, optional) — enables Nesterov momentum (default: False)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trust_coeff` (float) — trust coefficient for computing adaptive lr / trust_ratio
    (default: 0.001)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eps` (float) — eps for division denominator (default: 1e-8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trust_clip` (bool) — enable LARC trust ratio clipping (default: False)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`always_adapt` (bool) — always apply LARS LR adapt, otherwise only when group
    weight_decay != 0 (default: False)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LARS for PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper: `Large batch training of Convolutional Networks` - [https://arxiv.org/pdf/1708.03888.pdf](https://arxiv.org/pdf/1708.03888.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lars.py#L75)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs a single optimization step.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class timm.optim.Lookahead`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lookahead.py#L15)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '### `class timm.optim.MADGRAD`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/madgrad.py#L24)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`params` (iterable) — Iterable of parameters to optimize or dicts defining
    parameter groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr` (float) — Learning rate (default: 1e-2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`momentum` (float) — Momentum value in the range [0,1) (default: 0.9).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight_decay` (float) — Weight decay, i.e. a L2 penalty (default: 0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eps` (float) — Term added to the denominator outside of the root operation
    to improve numerical stability. (default: 1e-6).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MADGRAD_: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic
    Optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: '.. _MADGRAD: [https://arxiv.org/abs/2101.11075](https://arxiv.org/abs/2101.11075)'
  prefs: []
  type: TYPE_NORMAL
- en: MADGRAD is a general purpose optimizer that can be used in place of SGD or Adam
    may converge faster and generalize better. Currently GPU-only. Typically, the
    same learning rate schedule that is used for SGD or Adam may be used. The overall
    learning rate is not comparable to either method and should be determined by a
    hyper-parameter sweep.
  prefs: []
  type: TYPE_NORMAL
- en: MADGRAD requires less weight decay than other methods, often as little as zero.
    Momentum values used for SGD or Adam’s beta1 should work here also.
  prefs: []
  type: TYPE_NORMAL
- en: On sparse problems both weight_decay and momentum should be set to 0.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/madgrad.py#L85)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs a single optimization step.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class timm.optim.Nadam`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/nadam.py#L7)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr` (float, optional) — learning rate (default: 2e-3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`betas` (Tuple[float, float], optional) — coefficients used for computing running
    averages of gradient and its square'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eps` (float, optional) — term added to the denominator to improve numerical
    stability (default: 1e-8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight_decay` (float, optional) — weight decay (L2 penalty) (default: 0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`schedule_decay` (float, optional) — momentum schedule decay (default: 4e-3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implements Nadam algorithm (a variant of Adam based on Nesterov momentum).
  prefs: []
  type: TYPE_NORMAL
- en: It has been proposed in `Incorporating Nesterov Momentum into Adam`__.
  prefs: []
  type: TYPE_NORMAL
- en: '**[http://cs229.stanford.edu/proj2015/054_report.pdf](http://cs229.stanford.edu/proj2015/054_report.pdf)**
    [http://www.cs.toronto.edu/~fritz/absps/momentum.pdf](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Originally taken from: [https://github.com/pytorch/pytorch/pull/1408](https://github.com/pytorch/pytorch/pull/1408)
    NOTE: Has potential issues but does work well on some problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/nadam.py#L43)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs a single optimization step.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class timm.optim.NvNovoGrad`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/nvnovograd.py#L13)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr` (float, optional) — learning rate (default: 1e-3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`betas` (Tuple[float, float], optional) — coefficients used for computing running
    averages of gradient and its square (default: (0.95, 0.98))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eps` (float, optional) — term added to the denominator to improve numerical
    stability (default: 1e-8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight_decay` (float, optional) — weight decay (L2 penalty) (default: 0) grad_averaging
    — gradient averaging'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`amsgrad` (boolean, optional) — whether to use the AMSGrad variant of this
    algorithm from the paper `On the Convergence of Adam and Beyond`_ (default: False)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implements Novograd algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/nvnovograd.py#L54)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`closure` (callable, optional) — A closure that reevaluates the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`and` returns the loss. —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs a single optimization step.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class timm.optim.RAdam`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/radam.py#L10)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '### `class timm.optim.RMSpropTF`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/rmsprop_tf.py#L14)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`params` (iterable) — iterable of parameters to optimize or dicts defining
    parameter groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr` (float, optional) — learning rate (default: 1e-2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`momentum` (float, optional) — momentum factor (default: 0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha` (float, optional) — smoothing (decay) constant (default: 0.9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eps` (float, optional) — term added to the denominator to improve numerical
    stability (default: 1e-10)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`centered` (bool, optional) — if `True`, compute the centered RMSProp, the
    gradient is normalized by an estimation of its variance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight_decay` (float, optional) — weight decay (L2 penalty) (default: 0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoupled_decay` (bool, optional) — decoupled weight decay as per [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr_in_momentum` (bool, optional) — learning rate scaling is included in the
    momentum buffer update as per defaults in Tensorflow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implements RMSprop algorithm (TensorFlow style epsilon)
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE: This is a direct cut-and-paste of PyTorch RMSprop with eps applied before
    sqrt and a few other modifications to closer match Tensorflow for matching hyper-params.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Noteworthy changes include:'
  prefs: []
  type: TYPE_NORMAL
- en: Epsilon applied inside square-root
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: square_avg initialized to ones
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LR scaling of update accumulated in momentum buffer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Proposed by G. Hinton in his [course](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: The centered version first appears in [Generating Sequences With Recurrent Neural
    Networks](https://arxiv.org/pdf/1308.0850v5.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/rmsprop_tf.py#L72)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`closure` (callable, optional) — A closure that reevaluates the model and returns
    the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs a single optimization step.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class timm.optim.SGDP`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/sgdp.py#L19)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
