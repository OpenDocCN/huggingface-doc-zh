- en: Types of Evaluations in ğŸ¤— Evaluate
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¤—è¯„ä¼°ä¸­çš„è¯„ä¼°ç±»å‹
- en: 'Original text: [https://huggingface.co/docs/evaluate/types_of_evaluations](https://huggingface.co/docs/evaluate/types_of_evaluations)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/evaluate/types_of_evaluations](https://huggingface.co/docs/evaluate/types_of_evaluations)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the ğŸ¤— Evaluate library is to support different types of evaluation,
    depending on different goals, datasets and models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤—è¯„ä¼°åº“çš„ç›®æ ‡æ˜¯æ”¯æŒä¸åŒç±»å‹çš„è¯„ä¼°ï¼Œå–å†³äºä¸åŒçš„ç›®æ ‡ã€æ•°æ®é›†å’Œæ¨¡å‹ã€‚
- en: 'Here are the types of evaluations that are currently supported with a few examples
    for each:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯å½“å‰æ”¯æŒçš„è¯„ä¼°ç±»å‹åŠå…¶å„è‡ªçš„å‡ ä¸ªç¤ºä¾‹ï¼š
- en: Metrics
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åº¦é‡
- en: A metric measures the performance of a model on a given dataset. This is often
    based on an existing ground truth (i.e. a set of references), but there are also
    *referenceless metrics* which allow evaluating generated text by leveraging a
    pretrained model such as [GPT-2](https://huggingface.co/gpt2).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åº¦é‡æ ‡å‡†è¡¡é‡æ¨¡å‹åœ¨ç»™å®šæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚è¿™é€šå¸¸åŸºäºç°æœ‰çš„åœ°é¢çœŸç›¸ï¼ˆå³ä¸€ç»„å‚è€ƒï¼‰ï¼Œä½†ä¹Ÿæœ‰*æ— å‚è€ƒåº¦é‡æ ‡å‡†*ï¼Œå…è®¸é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚[GPT-2](https://huggingface.co/gpt2)ï¼‰æ¥è¯„ä¼°ç”Ÿæˆçš„æ–‡æœ¬ã€‚
- en: 'Examples of metrics include:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åº¦é‡çš„ç¤ºä¾‹åŒ…æ‹¬ï¼š
- en: '[Accuracy](https://huggingface.co/metrics/accuracy) : the proportion of correct
    predictions among the total number of cases processed.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å‡†ç¡®ç‡](https://huggingface.co/metrics/accuracy)ï¼šåœ¨å¤„ç†çš„æ€»æ¡ˆä¾‹ä¸­ï¼Œæ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ã€‚'
- en: '[Exact Match](https://huggingface.co/metrics/exact_match): the rate at which
    the input predicted strings exactly match their references.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ç²¾ç¡®åŒ¹é…](https://huggingface.co/metrics/exact_match)ï¼šè¾“å…¥é¢„æµ‹å­—ç¬¦ä¸²ä¸å…¶å‚è€ƒå®Œå…¨åŒ¹é…çš„é€Ÿç‡ã€‚'
- en: '[Mean Intersection over union (IoUO)](https://huggingface.co/metrics/mean_iou):
    the area of overlap between the predicted segmentation of an image and the ground
    truth divided by the area of union between the predicted segmentation and the
    ground truth.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¹³å‡äº¤é›†è”åˆï¼ˆIoUOï¼‰](https://huggingface.co/metrics/mean_iou)ï¼šå›¾åƒé¢„æµ‹åˆ†å‰²ä¸åœ°é¢çœŸç›¸ä¹‹é—´çš„é‡å åŒºåŸŸä¸å›¾åƒé¢„æµ‹åˆ†å‰²ä¸åœ°é¢çœŸç›¸ä¹‹é—´çš„è”åˆåŒºåŸŸä¹‹æ¯”ã€‚'
- en: Metrics are often used to track model performance on benchmark datasets, and
    to report progress on tasks such as [machine translation](https://huggingface.co/tasks/translation)
    and [image classification](https://huggingface.co/tasks/image-classification).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åº¦é‡æ ‡å‡†é€šå¸¸ç”¨äºè·Ÿè¸ªæ¨¡å‹åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œå¹¶æŠ¥å‘Šè¯¸å¦‚[æœºå™¨ç¿»è¯‘](https://huggingface.co/tasks/translation)å’Œ[å›¾åƒåˆ†ç±»](https://huggingface.co/tasks/image-classification)ç­‰ä»»åŠ¡çš„è¿›å±•ã€‚
- en: Comparisons
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒ
- en: Comparisons can be useful to compare the performance of two or more models on
    a single test dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒå¯ä»¥ç”¨äºæ¯”è¾ƒå•ä¸ªæµ‹è¯•æ•°æ®é›†ä¸Šä¸¤ä¸ªæˆ–å¤šä¸ªæ¨¡å‹çš„æ€§èƒ½ã€‚
- en: For instance, the [McNemar Test](https://github.com/huggingface/evaluate/tree/main/comparisons/mcnemar)
    is a paired nonparametric statistical hypothesis test that takes the predictions
    of two models and compares them, aiming to measure whether the modelsâ€™s predictions
    diverge or not. The p value it outputs, which ranges from `0.0` to `1.0`, indicates
    the difference between the two modelsâ€™ predictions, with a lower p value indicating
    a more significant difference.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œ[éº¦å…‹é©¬å°”æ£€éªŒ](https://github.com/huggingface/evaluate/tree/main/comparisons/mcnemar)æ˜¯ä¸€ç§æˆå¯¹çš„éå‚æ•°ç»Ÿè®¡å‡è®¾æ£€éªŒï¼Œå®ƒæ¥å—ä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹å¹¶æ¯”è¾ƒå®ƒä»¬ï¼Œæ—¨åœ¨è¡¡é‡æ¨¡å‹çš„é¢„æµ‹æ˜¯å¦å‘æ•£ã€‚å®ƒè¾“å‡ºçš„på€¼èŒƒå›´ä»`0.0`åˆ°`1.0`ï¼ŒæŒ‡ç¤ºä¸¤ä¸ªæ¨¡å‹é¢„æµ‹ä¹‹é—´çš„å·®å¼‚ï¼Œè¾ƒä½çš„på€¼è¡¨ç¤ºæ›´æ˜¾è‘—çš„å·®å¼‚ã€‚
- en: Comparisons have yet to be systematically used when comparing and reporting
    model performance, however they are useful tools to go beyond simply comparing
    leaderboard scores and for getting more information on the way model prediction
    differ.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨æ¯”è¾ƒå’ŒæŠ¥å‘Šæ¨¡å‹æ€§èƒ½æ—¶ï¼Œæ¯”è¾ƒå°šæœªç³»ç»Ÿåœ°ä½¿ç”¨ï¼Œä½†å®ƒä»¬æ˜¯è¶…è¶Šç®€å•æ¯”è¾ƒæ’è¡Œæ¦œåˆ†æ•°å¹¶è·å–æœ‰å…³æ¨¡å‹é¢„æµ‹å·®å¼‚æ–¹å¼çš„æœ‰ç”¨å·¥å…·ã€‚
- en: Measurements
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æµ‹é‡
- en: In the ğŸ¤— Evaluate library, measurements are tools for gaining more insights
    on datasets and model predictions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ğŸ¤—è¯„ä¼°åº“ä¸­ï¼Œæµ‹é‡æ˜¯è·å–æœ‰å…³æ•°æ®é›†å’Œæ¨¡å‹é¢„æµ‹æ›´å¤šè§è§£çš„å·¥å…·ã€‚
- en: For instance, in the case of datasets, it can be useful to calculate the [average
    word length](https://github.com/huggingface/evaluate/tree/main/measurements/word_length)
    of a datasetâ€™s entries, and how it is distributed â€” this can help when choosing
    the maximum input length for [Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥è®¡ç®—æ•°æ®é›†æ¡ç›®çš„[å¹³å‡è¯é•¿](https://github.com/huggingface/evaluate/tree/main/measurements/word_length)ï¼Œä»¥åŠå…¶åˆ†å¸ƒæƒ…å†µâ€”â€”è¿™åœ¨é€‰æ‹©[Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer)çš„æœ€å¤§è¾“å…¥é•¿åº¦æ—¶ä¼šæœ‰æ‰€å¸®åŠ©ã€‚
- en: In the case of model predictions, it can help to calculate the average [perplexity](https://huggingface.co/metrics/perplexity)
    of model predictions using different models such as [GPT-2](https://huggingface.co/gpt2)
    and [BERT](https://huggingface.co/bert-base-uncased), which can indicate the quality
    of generated text when no reference is available.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨¡å‹é¢„æµ‹çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥å¸®åŠ©ä½¿ç”¨ä¸åŒæ¨¡å‹ï¼ˆå¦‚[GPT-2](https://huggingface.co/gpt2)å’Œ[BERT](https://huggingface.co/bert-base-uncased)ï¼‰è®¡ç®—æ¨¡å‹é¢„æµ‹çš„å¹³å‡[å›°æƒ‘åº¦](https://huggingface.co/metrics/perplexity)ï¼Œè¿™å¯ä»¥æŒ‡ç¤ºåœ¨æ²¡æœ‰å‚è€ƒæ–‡æœ¬æ—¶ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚
- en: All three types of evaluation supported by the ğŸ¤— Evaluate library are meant
    to be mutually complementary, and help our community carry out more mindful and
    responsible evaluation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤—è¯„ä¼°åº“æ”¯æŒçš„ä¸‰ç§è¯„ä¼°ç±»å‹æ—¨åœ¨ç›¸äº’è¡¥å……ï¼Œå¹¶å¸®åŠ©æˆ‘ä»¬çš„ç¤¾åŒºè¿›è¡Œæ›´åŠ å®¡æ…å’Œè´Ÿè´£ä»»çš„è¯„ä¼°ã€‚
- en: We will continue adding more types of metrics, measurements and comparisons
    in coming months, and are counting on community involvement (via [PRs](https://github.com/huggingface/evaluate/compare)
    and [issues](https://github.com/huggingface/evaluate/issues/new/choose)) to make
    the library as extensive and inclusive as possible!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœªæ¥å‡ ä¸ªæœˆä¸­ï¼Œæˆ‘ä»¬å°†ç»§ç»­æ·»åŠ æ›´å¤šç±»å‹çš„åº¦é‡ã€æµ‹é‡å’Œæ¯”è¾ƒï¼Œå¹¶ä¾é ç¤¾åŒºå‚ä¸ï¼ˆé€šè¿‡[PRs](https://github.com/huggingface/evaluate/compare)å’Œ[issues](https://github.com/huggingface/evaluate/issues/new/choose)ï¼‰ä½¿è¯¥åº“å°½å¯èƒ½å¹¿æ³›å’ŒåŒ…å®¹ï¼
