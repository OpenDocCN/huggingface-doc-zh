- en: Types of Evaluations in ðŸ¤— Evaluate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/evaluate/types_of_evaluations](https://huggingface.co/docs/evaluate/types_of_evaluations)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/assets/pages/__layout.svelte-hf-doc-builder.css">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/start-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/vendor-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/paths-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/pages/__layout.svelte-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/pages/types_of_evaluations.mdx-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/IconCopyLink-hf-doc-builder.js">
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the ðŸ¤— Evaluate library is to support different types of evaluation,
    depending on different goals, datasets and models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the types of evaluations that are currently supported with a few examples
    for each:'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A metric measures the performance of a model on a given dataset. This is often
    based on an existing ground truth (i.e. a set of references), but there are also
    *referenceless metrics* which allow evaluating generated text by leveraging a
    pretrained model such as [GPT-2](https://huggingface.co/gpt2).
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of metrics include:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Accuracy](https://huggingface.co/metrics/accuracy) : the proportion of correct
    predictions among the total number of cases processed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Exact Match](https://huggingface.co/metrics/exact_match): the rate at which
    the input predicted strings exactly match their references.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mean Intersection over union (IoUO)](https://huggingface.co/metrics/mean_iou):
    the area of overlap between the predicted segmentation of an image and the ground
    truth divided by the area of union between the predicted segmentation and the
    ground truth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics are often used to track model performance on benchmark datasets, and
    to report progress on tasks such as [machine translation](https://huggingface.co/tasks/translation)
    and [image classification](https://huggingface.co/tasks/image-classification).
  prefs: []
  type: TYPE_NORMAL
- en: Comparisons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Comparisons can be useful to compare the performance of two or more models on
    a single test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the [McNemar Test](https://github.com/huggingface/evaluate/tree/main/comparisons/mcnemar)
    is a paired nonparametric statistical hypothesis test that takes the predictions
    of two models and compares them, aiming to measure whether the modelsâ€™s predictions
    diverge or not. The p value it outputs, which ranges from `0.0` to `1.0`, indicates
    the difference between the two modelsâ€™ predictions, with a lower p value indicating
    a more significant difference.
  prefs: []
  type: TYPE_NORMAL
- en: Comparisons have yet to be systematically used when comparing and reporting
    model performance, however they are useful tools to go beyond simply comparing
    leaderboard scores and for getting more information on the way model prediction
    differ.
  prefs: []
  type: TYPE_NORMAL
- en: Measurements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the ðŸ¤— Evaluate library, measurements are tools for gaining more insights
    on datasets and model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in the case of datasets, it can be useful to calculate the [average
    word length](https://github.com/huggingface/evaluate/tree/main/measurements/word_length)
    of a datasetâ€™s entries, and how it is distributed â€” this can help when choosing
    the maximum input length for [Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer).
  prefs: []
  type: TYPE_NORMAL
- en: In the case of model predictions, it can help to calculate the average [perplexity](https://huggingface.co/metrics/perplexity)
    of model predictions using different models such as [GPT-2](https://huggingface.co/gpt2)
    and [BERT](https://huggingface.co/bert-base-uncased), which can indicate the quality
    of generated text when no reference is available.
  prefs: []
  type: TYPE_NORMAL
- en: All three types of evaluation supported by the ðŸ¤— Evaluate library are meant
    to be mutually complementary, and help our community carry out more mindful and
    responsible evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue adding more types of metrics, measurements and comparisons
    in coming months, and are counting on community involvement (via [PRs](https://github.com/huggingface/evaluate/compare)
    and [issues](https://github.com/huggingface/evaluate/issues/new/choose)) to make
    the library as extensive and inclusive as possible!
  prefs: []
  type: TYPE_NORMAL
