- en: Glossary
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ¯è¯­è¡¨
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/glossary](https://huggingface.co/docs/transformers/v4.37.2/en/glossary)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/glossary](https://huggingface.co/docs/transformers/v4.37.2/en/glossary)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This glossary defines general machine learning and ğŸ¤— Transformers terms to help
    you better understand the documentation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæœ¯è¯­è¡¨å®šä¹‰äº†ä¸€èˆ¬çš„æœºå™¨å­¦ä¹ å’ŒğŸ¤— Transformersæœ¯è¯­ï¼Œä»¥å¸®åŠ©æ‚¨æ›´å¥½åœ°ç†è§£æ–‡æ¡£ã€‚
- en: A
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A
- en: attention mask
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æ©ç 
- en: The attention mask is an optional argument used when batching sequences together.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æ©ç æ˜¯ä¸€ä¸ªå¯é€‰å‚æ•°ï¼Œç”¨äºå°†åºåˆ—æ‰¹å¤„ç†åœ¨ä¸€èµ·æ—¶ä½¿ç”¨ã€‚
- en: '[https://www.youtube-nocookie.com/embed/M6adb1j2jPI](https://www.youtube-nocookie.com/embed/M6adb1j2jPI)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/M6adb1j2jPI](https://www.youtube-nocookie.com/embed/M6adb1j2jPI)'
- en: This argument indicates to the model which tokens should be attended to, and
    which should not.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå‚æ•°æŒ‡ç¤ºæ¨¡å‹åº”è¯¥å…³æ³¨å“ªäº›ä»¤ç‰Œï¼Œå“ªäº›ä¸åº”è¯¥ã€‚
- en: 'For example, consider these two sequences:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè€ƒè™‘è¿™ä¸¤ä¸ªåºåˆ—ï¼š
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The encoded versions have different lengths:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç ç‰ˆæœ¬çš„é•¿åº¦ä¸åŒï¼š
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Therefore, we canâ€™t put them together in the same tensor as-is. The first sequence
    needs to be padded up to the length of the second one, or the second one needs
    to be truncated down to the length of the first one.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬ä¸èƒ½å°†å®ƒä»¬ç›´æ¥æ”¾åœ¨åŒä¸€ä¸ªå¼ é‡ä¸­ã€‚ç¬¬ä¸€ä¸ªåºåˆ—éœ€è¦å¡«å……åˆ°ç¬¬äºŒä¸ªåºåˆ—çš„é•¿åº¦ï¼Œæˆ–è€…ç¬¬äºŒä¸ªåºåˆ—éœ€è¦æˆªæ–­åˆ°ç¬¬ä¸€ä¸ªåºåˆ—çš„é•¿åº¦ã€‚
- en: 'In the first case, the list of IDs will be extended by the padding indices.
    We can pass a list to the tokenizer and ask it to pad like this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ä¸€ç§æƒ…å†µä¸‹ï¼ŒIDåˆ—è¡¨å°†é€šè¿‡å¡«å……ç´¢å¼•è¿›è¡Œæ‰©å±•ã€‚æˆ‘ä»¬å¯ä»¥å°†åˆ—è¡¨ä¼ é€’ç»™åˆ†è¯å™¨ï¼Œå¹¶è¦æ±‚å®ƒè¿™æ ·å¡«å……ï¼š
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can see that 0s have been added on the right of the first sentence to make
    it the same length as the second one:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨ç¬¬ä¸€å¥çš„å³ä¾§æ·»åŠ äº†0ï¼Œä½¿å…¶ä¸ç¬¬äºŒå¥é•¿åº¦ç›¸åŒï¼š
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This can then be converted into a tensor in PyTorch or TensorFlow. The attention
    mask is a binary tensor indicating the position of the padded indices so that
    the model does not attend to them. For the [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer),
    `1` indicates a value that should be attended to, while `0` indicates a padded
    value. This attention mask is in the dictionary returned by the tokenizer under
    the key â€œattention_maskâ€:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå¯ä»¥åœ¨PyTorchæˆ–TensorFlowä¸­å°†å…¶è½¬æ¢ä¸ºå¼ é‡ã€‚æ³¨æ„åŠ›æ©ç æ˜¯ä¸€ä¸ªäºŒè¿›åˆ¶å¼ é‡ï¼ŒæŒ‡ç¤ºå¡«å……ç´¢å¼•çš„ä½ç½®ï¼Œä»¥ä¾¿æ¨¡å‹ä¸ä¼šå…³æ³¨å®ƒä»¬ã€‚å¯¹äº[BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)ï¼Œ`1`è¡¨ç¤ºåº”è¯¥å…³æ³¨çš„å€¼ï¼Œè€Œ`0`è¡¨ç¤ºå¡«å……å€¼ã€‚è¿™ä¸ªæ³¨æ„åŠ›æ©ç åœ¨ä»¤ç‰ŒåŒ–å™¨è¿”å›çš„å­—å…¸ä¸­ï¼Œé”®ä¸ºâ€œattention_maskâ€ï¼š
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: autoencoding models
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç æ¨¡å‹
- en: See [encoder models](#encoder-models) and [masked language modeling](#masked-language-modeling-mlm)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å‚è§[ç¼–ç å™¨æ¨¡å‹](#encoder-models)å’Œ[æ©ç è¯­è¨€å»ºæ¨¡](#masked-language-modeling-mlm)
- en: autoregressive models
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ªå›å½’æ¨¡å‹
- en: See [causal language modeling](#causal-language-modeling) and [decoder models](#decoder-models)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å‚è§[å› æœè¯­è¨€å»ºæ¨¡](#causal-language-modeling)å’Œ[è§£ç å™¨æ¨¡å‹](#decoder-models)
- en: B
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B
- en: backbone
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: éª¨å¹²
- en: The backbone is the network (embeddings and layers) that outputs the raw hidden
    states or features. It is usually connected to a [head](#head) which accepts the
    features as its input to make a prediction. For example, [ViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTModel)
    is a backbone without a specific head on top. Other models can also use `VitModel`
    as a backbone such as [DPT](model_doc/dpt).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: éª¨å¹²æ˜¯è¾“å‡ºåŸå§‹éšè—çŠ¶æ€æˆ–ç‰¹å¾çš„ç½‘ç»œï¼ˆåµŒå…¥å’Œå±‚ï¼‰ã€‚é€šå¸¸ä¸ä¸€ä¸ª[å¤´](#head)è¿æ¥ï¼Œè¯¥å¤´æ¥å—ç‰¹å¾ä½œä¸ºå…¶è¾“å…¥ä»¥è¿›è¡Œé¢„æµ‹ã€‚ä¾‹å¦‚ï¼Œ[ViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTModel)æ˜¯ä¸€ä¸ªæ²¡æœ‰ç‰¹å®šå¤´éƒ¨çš„éª¨å¹²ã€‚å…¶ä»–æ¨¡å‹ä¹Ÿå¯ä»¥ä½¿ç”¨`VitModel`ä½œä¸ºéª¨å¹²ï¼Œå¦‚[DPT](model_doc/dpt)ã€‚
- en: C
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C
- en: causal language modeling
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å› æœè¯­è¨€å»ºæ¨¡
- en: A pretraining task where the model reads the texts in order and has to predict
    the next word. Itâ€™s usually done by reading the whole sentence but using a mask
    inside the model to hide the future tokens at a certain timestep.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé¢„è®­ç»ƒä»»åŠ¡ï¼Œå…¶ä¸­æ¨¡å‹æŒ‰é¡ºåºé˜…è¯»æ–‡æœ¬ï¼Œå¹¶é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚é€šå¸¸é€šè¿‡é˜…è¯»æ•´ä¸ªå¥å­æ¥å®Œæˆï¼Œä½†åœ¨æ¨¡å‹å†…éƒ¨ä½¿ç”¨æ©ç æ¥éšè—æŸä¸ªæ—¶é—´æ­¥é•¿çš„æœªæ¥ä»¤ç‰Œã€‚
- en: channel
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šé“
- en: 'Color images are made up of some combination of values in three channels: red,
    green, and blue (RGB) and grayscale images only have one channel. In ğŸ¤— Transformers,
    the channel can be the first or last dimension of an imageâ€™s tensor: [`n_channels`,
    `height`, `width`] or [`height`, `width`, `n_channels`].'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å½©è‰²å›¾åƒç”±ä¸‰ä¸ªé€šé“ä¸­çš„å€¼çš„æŸç§ç»„åˆç»„æˆï¼šçº¢è‰²ã€ç»¿è‰²å’Œè“è‰²ï¼ˆRGBï¼‰ï¼Œè€Œç°åº¦å›¾åƒåªæœ‰ä¸€ä¸ªé€šé“ã€‚åœ¨ğŸ¤— Transformersä¸­ï¼Œé€šé“å¯ä»¥æ˜¯å›¾åƒå¼ é‡çš„ç¬¬ä¸€ä¸ªæˆ–æœ€åä¸€ä¸ªç»´åº¦ï¼š[`n_channels`ï¼Œ`height`ï¼Œ`width`]æˆ–[`height`ï¼Œ`width`ï¼Œ`n_channels`]ã€‚
- en: connectionist temporal classification (CTC)
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¿æ¥ä¸»ä¹‰æ—¶é—´åˆ†ç±»ï¼ˆCTCï¼‰
- en: An algorithm which allows a model to learn without knowing exactly how the input
    and output are aligned; CTC calculates the distribution of all possible outputs
    for a given input and chooses the most likely output from it. CTC is commonly
    used in speech recognition tasks because speech doesnâ€™t always cleanly align with
    the transcript for a variety of reasons such as a speakerâ€™s different speech rates.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§å…è®¸æ¨¡å‹å­¦ä¹ è€Œä¸çŸ¥é“è¾“å…¥å’Œè¾“å‡ºå¦‚ä½•å¯¹é½çš„ç®—æ³•ï¼›CTCè®¡ç®—ç»™å®šè¾“å…¥çš„æ‰€æœ‰å¯èƒ½è¾“å‡ºçš„åˆ†å¸ƒï¼Œå¹¶ä»ä¸­é€‰æ‹©æœ€å¯èƒ½çš„è¾“å‡ºã€‚CTCé€šå¸¸ç”¨äºè¯­éŸ³è¯†åˆ«ä»»åŠ¡ï¼Œå› ä¸ºè¯­éŸ³ä¸æ€»æ˜¯ä¸æ–‡æœ¬å¹²å‡€åœ°å¯¹é½ï¼ŒåŸå› æœ‰å¾ˆå¤šï¼Œæ¯”å¦‚è¯´è¯è€…çš„ä¸åŒè¯­é€Ÿã€‚
- en: convolution
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å·ç§¯
- en: A type of layer in a neural network where the input matrix is multiplied element-wise
    by a smaller matrix (kernel or filter) and the values are summed up in a new matrix.
    This is known as a convolutional operation which is repeated over the entire input
    matrix. Each operation is applied to a different segment of the input matrix.
    Convolutional neural networks (CNNs) are commonly used in computer vision.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œä¸­çš„ä¸€ç§å±‚ç±»å‹ï¼Œå…¶ä¸­è¾“å…¥çŸ©é˜µä¸è¾ƒå°çš„çŸ©é˜µï¼ˆå·ç§¯æ ¸æˆ–æ»¤æ³¢å™¨ï¼‰é€å…ƒç´ ç›¸ä¹˜ï¼Œç„¶ååœ¨æ–°çŸ©é˜µä¸­æ±‚å’Œã€‚è¿™è¢«ç§°ä¸ºå·ç§¯æ“ä½œï¼Œå®ƒåœ¨æ•´ä¸ªè¾“å…¥çŸ©é˜µä¸Šé‡å¤ã€‚æ¯ä¸ªæ“ä½œåº”ç”¨äºè¾“å…¥çŸ©é˜µçš„ä¸åŒéƒ¨åˆ†ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰é€šå¸¸ç”¨äºè®¡ç®—æœºè§†è§‰ã€‚
- en: D
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D
- en: DataParallel (DP)
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DataParallelï¼ˆDPï¼‰
- en: Parallelism technique for training on multiple GPUs where the same setup is
    replicated multiple times, with each instance receiving a distinct data slice.
    The processing is done in parallel and all setups are synchronized at the end
    of each training step.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤šä¸ªGPUä¸Šè¿›è¡Œè®­ç»ƒçš„å¹¶è¡ŒæŠ€æœ¯ï¼Œå…¶ä¸­ç›¸åŒçš„è®¾ç½®è¢«å¤åˆ¶å¤šæ¬¡ï¼Œæ¯ä¸ªå®ä¾‹æ¥æ”¶ä¸€ä¸ªä¸åŒçš„æ•°æ®åˆ‡ç‰‡ã€‚å¤„ç†æ˜¯å¹¶è¡Œè¿›è¡Œçš„ï¼Œå¹¶ä¸”æ‰€æœ‰è®¾ç½®åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ç»“æŸæ—¶éƒ½æ˜¯åŒæ­¥çš„ã€‚
- en: Learn more about how DataParallel works [here](perf_train_gpu_many#dataparallel-vs-distributeddataparallel).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: äº†è§£æœ‰å…³DataParallelå¦‚ä½•å·¥ä½œçš„æ›´å¤šä¿¡æ¯[åœ¨è¿™é‡Œ](perf_train_gpu_many#dataparallel-vs-distributeddataparallel)ã€‚
- en: decoder input IDs
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£ç å™¨è¾“å…¥ID
- en: This input is specific to encoder-decoder models, and contains the input IDs
    that will be fed to the decoder. These inputs should be used for sequence to sequence
    tasks, such as translation or summarization, and are usually built in a way specific
    to each model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¾“å…¥æ˜¯ç‰¹å®šäºç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„ï¼ŒåŒ…å«å°†é¦ˆé€ç»™è§£ç å™¨çš„è¾“å…¥IDã€‚è¿™äº›è¾“å…¥åº”è¯¥ç”¨äºåºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼Œä¾‹å¦‚ç¿»è¯‘æˆ–æ‘˜è¦ï¼Œå¹¶ä¸”é€šå¸¸ä»¥æ¯ä¸ªæ¨¡å‹ç‰¹å®šçš„æ–¹å¼æ„å»ºã€‚
- en: Most encoder-decoder models (BART, T5) create their `decoder_input_ids` on their
    own from the `labels`. In such models, passing the `labels` is the preferred way
    to handle training.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼ˆBARTï¼ŒT5ï¼‰ä¼šè‡ªè¡Œä»`labels`åˆ›å»ºå®ƒä»¬çš„`decoder_input_ids`ã€‚åœ¨è¿™ç§æ¨¡å‹ä¸­ï¼Œä¼ é€’`labels`æ˜¯å¤„ç†è®­ç»ƒçš„é¦–é€‰æ–¹å¼ã€‚
- en: Please check each modelâ€™s docs to see how they handle these input IDs for sequence
    to sequence training.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æŸ¥çœ‹æ¯ä¸ªæ¨¡å‹çš„æ–‡æ¡£ï¼Œäº†è§£å®ƒä»¬å¦‚ä½•å¤„ç†è¿™äº›è¾“å…¥IDä»¥è¿›è¡Œåºåˆ—åˆ°åºåˆ—è®­ç»ƒã€‚
- en: decoder models
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£ç å™¨æ¨¡å‹
- en: Also referred to as autoregressive models, decoder models involve a pretraining
    task (called causal language modeling) where the model reads the texts in order
    and has to predict the next word. Itâ€™s usually done by reading the whole sentence
    with a mask to hide future tokens at a certain timestep.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç å™¨æ¨¡å‹ï¼Œä¹Ÿç§°ä¸ºè‡ªå›å½’æ¨¡å‹ï¼Œæ¶‰åŠä¸€ä¸ªé¢„è®­ç»ƒä»»åŠ¡ï¼ˆç§°ä¸ºå› æœè¯­è¨€å»ºæ¨¡ï¼‰ï¼Œå…¶ä¸­æ¨¡å‹æŒ‰é¡ºåºé˜…è¯»æ–‡æœ¬ï¼Œå¹¶é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚é€šå¸¸é€šè¿‡åœ¨æŸä¸ªæ—¶é—´æ­¥éª¤éšè—æœªæ¥æ ‡è®°çš„æ©ç æ¥è¯»å–æ•´ä¸ªå¥å­æ¥å®Œæˆã€‚
- en: '[https://www.youtube-nocookie.com/embed/d_ixlCubqQw](https://www.youtube-nocookie.com/embed/d_ixlCubqQw)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/d_ixlCubqQw](https://www.youtube-nocookie.com/embed/d_ixlCubqQw)'
- en: deep learning (DL)
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰
- en: Machine learning algorithms which uses neural networks with several layers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œçš„æœºå™¨å­¦ä¹ ç®—æ³•ã€‚
- en: E
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E
- en: encoder models
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨æ¨¡å‹
- en: Also known as autoencoding models, encoder models take an input (such as text
    or images) and transform them into a condensed numerical representation called
    an embedding. Oftentimes, encoder models are pretrained using techniques like
    [masked language modeling](#masked-language-modeling-mlm), which masks parts of
    the input sequence and forces the model to create more meaningful representations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿç§°ä¸ºè‡ªç¼–ç æ¨¡å‹ï¼Œç¼–ç å™¨æ¨¡å‹æ¥å—è¾“å…¥ï¼ˆå¦‚æ–‡æœ¬æˆ–å›¾åƒï¼‰å¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºç§°ä¸ºåµŒå…¥çš„ç®€åŒ–æ•°å€¼è¡¨ç¤ºã€‚é€šå¸¸ï¼Œç¼–ç å™¨æ¨¡å‹ä½¿ç”¨è¯¸å¦‚[masked language modeling](#masked-language-modeling-mlm)ä¹‹ç±»çš„æŠ€æœ¯è¿›è¡Œé¢„è®­ç»ƒï¼Œè¯¥æŠ€æœ¯å¯¹è¾“å…¥åºåˆ—çš„éƒ¨åˆ†è¿›è¡Œæ©ç ï¼Œå¹¶è¿«ä½¿æ¨¡å‹åˆ›å»ºæ›´æœ‰æ„ä¹‰çš„è¡¨ç¤ºã€‚
- en: '[https://www.youtube-nocookie.com/embed/H39Z_720T5s](https://www.youtube-nocookie.com/embed/H39Z_720T5s)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/H39Z_720T5s](https://www.youtube-nocookie.com/embed/H39Z_720T5s)'
- en: F
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: F
- en: feature extraction
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç‰¹å¾æå–
- en: The process of selecting and transforming raw data into a set of features that
    are more informative and useful for machine learning algorithms. Some examples
    of feature extraction include transforming raw text into word embeddings and extracting
    important features such as edges or shapes from image/video data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å°†åŸå§‹æ•°æ®é€‰æ‹©å’Œè½¬æ¢ä¸ºä¸€ç»„æ›´å…·ä¿¡æ¯æ€§å’Œæœ‰ç”¨çš„ç‰¹å¾çš„è¿‡ç¨‹ï¼Œä»¥ä¾›æœºå™¨å­¦ä¹ ç®—æ³•ä½¿ç”¨ã€‚ç‰¹å¾æå–çš„ä¸€äº›ç¤ºä¾‹åŒ…æ‹¬å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºè¯åµŒå…¥ï¼Œä»å›¾åƒ/è§†é¢‘æ•°æ®ä¸­æå–é‡è¦ç‰¹å¾ï¼Œå¦‚è¾¹ç¼˜æˆ–å½¢çŠ¶ã€‚
- en: feed forward chunking
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å‰é¦ˆåˆ†å—
- en: In each residual attention block in transformers the self-attention layer is
    usually followed by 2 feed forward layers. The intermediate embedding size of
    the feed forward layers is often bigger than the hidden size of the model (e.g.,
    for `bert-base-uncased`).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨transformersä¸­çš„æ¯ä¸ªæ®‹å·®æ³¨æ„åŠ›å—ä¸­ï¼Œè‡ªæ³¨æ„åŠ›å±‚é€šå¸¸åé¢è·Ÿç€2ä¸ªå‰é¦ˆå±‚ã€‚å‰é¦ˆå±‚çš„ä¸­é—´åµŒå…¥å¤§å°é€šå¸¸å¤§äºæ¨¡å‹çš„éšè—å¤§å°ï¼ˆä¾‹å¦‚ï¼Œå¯¹äº`bert-base-uncased`ï¼‰ã€‚
- en: 'For an input of size `[batch_size, sequence_length]`, the memory required to
    store the intermediate feed forward embeddings `[batch_size, sequence_length,
    config.intermediate_size]` can account for a large fraction of the memory use.
    The authors of [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
    noticed that since the computation is independent of the `sequence_length` dimension,
    it is mathematically equivalent to compute the output embeddings of both feed
    forward layers `[batch_size, config.hidden_size]_0, ..., [batch_size, config.hidden_size]_n`
    individually and concat them afterward to `[batch_size, sequence_length, config.hidden_size]`
    with `n = sequence_length`, which trades increased computation time against reduced
    memory use, but yields a mathematically **equivalent** result.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯¹äºå¤§å°ä¸º`[batch_size, sequence_length]`çš„è¾“å…¥ï¼Œå­˜å‚¨ä¸­é—´å‰é¦ˆåµŒå…¥`[batch_size, sequence_length,
    config.intermediate_size]`æ‰€éœ€çš„å†…å­˜å¯èƒ½å æ®äº†å¤§éƒ¨åˆ†å†…å­˜ä½¿ç”¨é‡ã€‚[Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)çš„ä½œè€…æ³¨æ„åˆ°ï¼Œç”±äºè®¡ç®—ä¸`sequence_length`ç»´åº¦æ— å…³ï¼Œæ•°å­¦ä¸Šç­‰ä»·äºåˆ†åˆ«è®¡ç®—ä¸¤ä¸ªå‰é¦ˆå±‚çš„è¾“å‡ºåµŒå…¥`[batch_size,
    config.hidden_size]_0, ..., [batch_size, config.hidden_size]_n`ï¼Œç„¶åå°†å®ƒä»¬è¿æ¥åˆ°`[batch_size,
    sequence_length, config.hidden_size]`ï¼Œå…¶ä¸­`n = sequence_length`ï¼Œè¿™ä¼šå¢åŠ è®¡ç®—æ—¶é—´ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨ï¼Œä½†äº§ç”Ÿæ•°å­¦ä¸Š**ç­‰ä»·**çš„ç»“æœã€‚'
- en: For models employing the function [apply_chunking_to_forward()](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.apply_chunking_to_forward),
    the `chunk_size` defines the number of output embeddings that are computed in
    parallel and thus defines the trade-off between memory and time complexity. If
    `chunk_size` is set to 0, no feed forward chunking is done.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä½¿ç”¨å‡½æ•°[apply_chunking_to_forward()](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.apply_chunking_to_forward)çš„æ¨¡å‹ï¼Œ`chunk_size`å®šä¹‰äº†å¹¶è¡Œè®¡ç®—çš„è¾“å‡ºåµŒå…¥æ•°é‡ï¼Œä»è€Œå®šä¹‰äº†å†…å­˜å’Œæ—¶é—´å¤æ‚åº¦ä¹‹é—´çš„æƒè¡¡ã€‚å¦‚æœå°†`chunk_size`è®¾ç½®ä¸º0ï¼Œåˆ™ä¸ä¼šè¿›è¡Œå‰é¦ˆåˆ†å—ã€‚
- en: finetuned models
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¾®è°ƒæ¨¡å‹
- en: Finetuning is a form of transfer learning which involves taking a pretrained
    model, freezing its weights, and replacing the output layer with a newly added
    [model head](#head). The model head is trained on your target dataset.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å¾®è°ƒæ˜¯ä¸€ç§è¿ç§»å­¦ä¹ å½¢å¼ï¼Œæ¶‰åŠé‡‡ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå†»ç»“å…¶æƒé‡ï¼Œå¹¶ç”¨æ–°æ·»åŠ çš„[model head](#head)æ›¿æ¢è¾“å‡ºå±‚ã€‚æ¨¡å‹å¤´åœ¨ç›®æ ‡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚
- en: See the [Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training)
    tutorial for more details, and learn how to fine-tune models with ğŸ¤— Transformers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training)æ•™ç¨‹ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œå¹¶äº†è§£å¦‚ä½•ä½¿ç”¨ğŸ¤—
    Transformerså¾®è°ƒæ¨¡å‹ã€‚
- en: H
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: H
- en: head
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¤´
- en: 'The model head refers to the last layer of a neural network that accepts the
    raw hidden states and projects them onto a different dimension. There is a different
    model head for each task. For example:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¤´æŒ‡çš„æ˜¯ç¥ç»ç½‘ç»œçš„æœ€åä¸€å±‚ï¼Œå®ƒæ¥å—åŸå§‹éšè—çŠ¶æ€å¹¶å°†å…¶æŠ•å½±åˆ°ä¸åŒçš„ç»´åº¦ã€‚æ¯ä¸ªä»»åŠ¡éƒ½æœ‰ä¸€ä¸ªä¸åŒçš„æ¨¡å‹å¤´ã€‚ä¾‹å¦‚ï¼š
- en: '[GPT2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification)
    is a sequence classification head - a linear layer - on top of the base [GPT2Model](/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2Model).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification)æ˜¯ä¸€ä¸ªåºåˆ—åˆ†ç±»å¤´
    - ä¸€ä¸ªçº¿æ€§å±‚ - ä½äºåŸºç¡€[GPT2Model](/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2Model)ä¹‹ä¸Šã€‚'
- en: '[ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)
    is an image classification head - a linear layer on top of the final hidden state
    of the `CLS` token - on top of the base [ViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTModel).'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)æ˜¯ä¸€ä¸ªå›¾åƒåˆ†ç±»å¤´
    - ä½äºåŸºç¡€[ViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTModel)çš„`CLS`ä»¤ç‰Œçš„æœ€ç»ˆéšè—çŠ¶æ€ä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚'
- en: '[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    is a language modeling head with [CTC](#connectionist-temporal-classification-(CTC))
    on top of the base [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)æ˜¯ä¸€ä¸ªå¸¦æœ‰[CTC](#connectionist-temporal-classification-(CTC))çš„è¯­è¨€å»ºæ¨¡å¤´ï¼Œä½äºåŸºç¡€[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)ä¹‹ä¸Šã€‚'
- en: I
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æˆ‘
- en: image patch
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å›¾åƒè¡¥ä¸
- en: Vision-based Transformers models split an image into smaller patches which are
    linearly embedded, and then passed as a sequence to the model. You can find the
    `patch_size` - or resolution - of the model in its configuration.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè§†è§‰çš„Transformeræ¨¡å‹å°†å›¾åƒåˆ†æˆè¾ƒå°çš„è¡¥ä¸ï¼Œè¿™äº›è¡¥ä¸è¢«çº¿æ€§åµŒå…¥ï¼Œç„¶åä½œä¸ºåºåˆ—ä¼ é€’ç»™æ¨¡å‹ã€‚æ‚¨å¯ä»¥åœ¨é…ç½®ä¸­æ‰¾åˆ°æ¨¡å‹çš„`patch_size`
    - æˆ–åˆ†è¾¨ç‡ã€‚
- en: inference
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: Inference is the process of evaluating a model on new data after training is
    complete. See the [Pipeline for inference](https://huggingface.co/docs/transformers/pipeline_tutorial)
    tutorial to learn how to perform inference with ğŸ¤— Transformers.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨ç†æ˜¯åœ¨è®­ç»ƒå®Œæˆåå¯¹æ–°æ•°æ®è¯„ä¼°æ¨¡å‹çš„è¿‡ç¨‹ã€‚è¯·å‚é˜…[ç”¨äºæ¨ç†çš„Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial)æ•™ç¨‹ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨ğŸ¤—
    Transformersæ‰§è¡Œæ¨ç†ã€‚
- en: input IDs
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¾“å…¥ID
- en: The input ids are often the only required parameters to be passed to the model
    as input. They are token indices, numerical representations of tokens building
    the sequences that will be used as input by the model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥IDé€šå¸¸æ˜¯ä¼ é€’ç»™æ¨¡å‹çš„å”¯ä¸€å¿…éœ€å‚æ•°ã€‚å®ƒä»¬æ˜¯æ ‡è®°ç´¢å¼•ï¼Œæ˜¯æ„å»ºåºåˆ—çš„æ ‡è®°çš„æ•°å€¼è¡¨ç¤ºï¼Œè¿™äº›åºåˆ—å°†ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ä½¿ç”¨ã€‚
- en: '[https://www.youtube-nocookie.com/embed/VFp38yj8h3A](https://www.youtube-nocookie.com/embed/VFp38yj8h3A)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/VFp38yj8h3A](https://www.youtube-nocookie.com/embed/VFp38yj8h3A)'
- en: 'Each tokenizer works differently but the underlying mechanism remains the same.
    Hereâ€™s an example using the BERT tokenizer, which is a [WordPiece](https://arxiv.org/pdf/1609.08144.pdf)
    tokenizer:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªåˆ†è¯å™¨çš„å·¥ä½œæ–¹å¼ä¸åŒï¼Œä½†åŸºæœ¬æœºåˆ¶ä¿æŒä¸å˜ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨BERTåˆ†è¯å™¨çš„ç¤ºä¾‹ï¼Œå®ƒæ˜¯ä¸€ä¸ª[WordPiece](https://arxiv.org/pdf/1609.08144.pdf)åˆ†è¯å™¨ï¼š
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The tokenizer takes care of splitting the sequence into tokens available in
    the tokenizer vocabulary.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†è¯å™¨è´Ÿè´£å°†åºåˆ—æ‹†åˆ†ä¸ºåˆ†è¯å™¨è¯æ±‡è¡¨ä¸­å¯ç”¨çš„æ ‡è®°ã€‚
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The tokens are either words or subwords. Here for instance, â€œVRAMâ€ wasnâ€™t in
    the model vocabulary, so itâ€™s been split in â€œVâ€, â€œRAâ€ and â€œMâ€. To indicate those
    tokens are not separate words but parts of the same word, a double-hash prefix
    is added for â€œRAâ€ and â€œMâ€:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ ‡è®°å¯ä»¥æ˜¯å•è¯æˆ–å­è¯ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿™é‡Œï¼Œâ€œVRAMâ€ä¸åœ¨æ¨¡å‹è¯æ±‡è¡¨ä¸­ï¼Œå› æ­¤å®ƒè¢«åˆ†å‰²ä¸ºâ€œVâ€ã€â€œRAâ€å’Œâ€œMâ€ã€‚ä¸ºäº†æŒ‡ç¤ºè¿™äº›æ ‡è®°ä¸æ˜¯å•ç‹¬çš„å•è¯è€Œæ˜¯åŒä¸€ä¸ªå•è¯çš„éƒ¨åˆ†ï¼Œä¸ºâ€œRAâ€å’Œâ€œMâ€æ·»åŠ äº†åŒå“ˆå¸Œå‰ç¼€ï¼š
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: These tokens can then be converted into IDs which are understandable by the
    model. This can be done by directly feeding the sentence to the tokenizer, which
    leverages the Rust implementation of [ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers)
    for peak performance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¿™äº›æ ‡è®°å¯ä»¥è½¬æ¢ä¸ºæ¨¡å‹å¯ç†è§£çš„IDã€‚è¿™å¯ä»¥é€šè¿‡ç›´æ¥å°†å¥å­æä¾›ç»™åˆ†è¯å™¨æ¥å®Œæˆï¼Œåˆ†è¯å™¨åˆ©ç”¨[Rustå®ç°çš„ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers)ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The tokenizer returns a dictionary with all the arguments necessary for its
    corresponding model to work properly. The token indices are under the key `input_ids`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†è¯å™¨è¿”å›ä¸€ä¸ªåŒ…å«å…¶å¯¹åº”æ¨¡å‹æ­£å¸¸å·¥ä½œæ‰€éœ€çš„æ‰€æœ‰å‚æ•°çš„å­—å…¸ã€‚æ ‡è®°ç´¢å¼•ä½äºé”®`input_ids`ä¸‹ï¼š
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that the tokenizer automatically adds â€œspecial tokensâ€ (if the associated
    model relies on them) which are special IDs the model sometimes uses.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œåˆ†è¯å™¨ä¼šè‡ªåŠ¨æ·»åŠ â€œç‰¹æ®Šæ ‡è®°â€ï¼ˆå¦‚æœç›¸å…³æ¨¡å‹ä¾èµ–äºå®ƒä»¬ï¼‰ï¼Œè¿™äº›ç‰¹æ®Šæ ‡è®°æ˜¯æ¨¡å‹æœ‰æ—¶ä½¿ç”¨çš„ç‰¹æ®ŠIDã€‚
- en: If we decode the previous sequence of ids,
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬è§£ç å…ˆå‰çš„IDåºåˆ—ï¼Œ
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: we will see
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†çœ‹åˆ°
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: because this is the way a [BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel)
    is going to expect its inputs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºè¿™æ˜¯[BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel)é¢„æœŸå…¶è¾“å…¥çš„æ–¹å¼ã€‚
- en: L
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L
- en: labels
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ ‡ç­¾
- en: 'The labels are an optional argument which can be passed in order for the model
    to compute the loss itself. These labels should be the expected prediction of
    the model: it will use the standard loss in order to compute the loss between
    its predictions and the expected value (the label).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡ç­¾æ˜¯ä¸€ä¸ªå¯é€‰å‚æ•°ï¼Œå¯ä»¥ä¼ é€’ç»™æ¨¡å‹ä»¥ä¾¿è®¡ç®—æŸå¤±ã€‚è¿™äº›æ ‡ç­¾åº”è¯¥æ˜¯æ¨¡å‹çš„é¢„æœŸé¢„æµ‹ï¼šå®ƒå°†ä½¿ç”¨æ ‡å‡†æŸå¤±æ¥è®¡ç®—å…¶é¢„æµ‹å€¼å’Œé¢„æœŸå€¼ï¼ˆæ ‡ç­¾ï¼‰ä¹‹é—´çš„æŸå¤±ã€‚
- en: 'These labels are different according to the model head, for example:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ ‡ç­¾æ ¹æ®æ¨¡å‹å¤´ä¸åŒè€Œä¸åŒï¼Œä¾‹å¦‚ï¼š
- en: For sequence classification models, ([BertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertForSequenceClassification)),
    the model expects a tensor of dimension `(batch_size)` with each value of the
    batch corresponding to the expected label of the entire sequence.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºåºåˆ—åˆ†ç±»æ¨¡å‹ï¼ˆ[BertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertForSequenceClassification)ï¼‰ï¼Œæ¨¡å‹æœŸæœ›ä¸€ä¸ªç»´åº¦ä¸º`(batch_size)`çš„å¼ é‡ï¼Œå…¶ä¸­æ‰¹æ¬¡çš„æ¯ä¸ªå€¼å¯¹åº”äºæ•´ä¸ªåºåˆ—çš„é¢„æœŸæ ‡ç­¾ã€‚
- en: For token classification models, ([BertForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertForTokenClassification)),
    the model expects a tensor of dimension `(batch_size, seq_length)` with each value
    corresponding to the expected label of each individual token.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ ‡è®°åˆ†ç±»æ¨¡å‹ï¼Œï¼ˆ[BertForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertForTokenClassification)ï¼‰ï¼Œæ¨¡å‹æœŸæœ›ä¸€ä¸ªç»´åº¦ä¸º`(batch_size,
    seq_length)`çš„å¼ é‡ï¼Œæ¯ä¸ªå€¼å¯¹åº”äºæ¯ä¸ªå•ç‹¬æ ‡è®°çš„é¢„æœŸæ ‡ç­¾ã€‚
- en: 'For masked language modeling, ([BertForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertForMaskedLM)),
    the model expects a tensor of dimension `(batch_size, seq_length)` with each value
    corresponding to the expected label of each individual token: the labels being
    the token ID for the masked token, and values to be ignored for the rest (usually
    -100).'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ©ç è¯­è¨€å»ºæ¨¡ï¼Œï¼ˆ[BertForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertForMaskedLM)ï¼‰ï¼Œæ¨¡å‹æœŸæœ›ä¸€ä¸ªç»´åº¦ä¸º`(batch_size,
    seq_length)`çš„å¼ é‡ï¼Œæ¯ä¸ªå€¼å¯¹åº”äºæ¯ä¸ªå•ç‹¬æ ‡è®°çš„é¢„æœŸæ ‡ç­¾ï¼šæ ‡ç­¾æ˜¯è¢«æ©ç æ ‡è®°çš„æ ‡è®°IDï¼Œå…¶ä½™æ ‡è®°çš„å€¼å°†è¢«å¿½ç•¥ï¼ˆé€šå¸¸ä¸º-100ï¼‰ã€‚
- en: For sequence to sequence tasks, ([BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration),
    [MBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/mbart#transformers.MBartForConditionalGeneration)),
    the model expects a tensor of dimension `(batch_size, tgt_seq_length)` with each
    value corresponding to the target sequences associated with each input sequence.
    During training, both BART and T5 will make the appropriate `decoder_input_ids`
    and decoder attention masks internally. They usually do not need to be supplied.
    This does not apply to models leveraging the Encoder-Decoder framework.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œï¼ˆ[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration),
    [MBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/mbart#transformers.MBartForConditionalGeneration)ï¼‰ï¼Œæ¨¡å‹æœŸæœ›ä¸€ä¸ªç»´åº¦ä¸º`(batch_size,
    tgt_seq_length)`çš„å¼ é‡ï¼Œæ¯ä¸ªå€¼å¯¹åº”äºä¸æ¯ä¸ªè¾“å…¥åºåˆ—ç›¸å…³è”çš„ç›®æ ‡åºåˆ—ã€‚åœ¨è®­ç»ƒæœŸé—´ï¼ŒBARTå’ŒT5éƒ½ä¼šåœ¨å†…éƒ¨ç”Ÿæˆé€‚å½“çš„`decoder_input_ids`å’Œè§£ç å™¨æ³¨æ„åŠ›æ©ç ã€‚é€šå¸¸ä¸éœ€è¦æä¾›å®ƒä»¬ã€‚è¿™ä¸é€‚ç”¨äºåˆ©ç”¨ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶çš„æ¨¡å‹ã€‚
- en: For image classification models, ([ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)),
    the model expects a tensor of dimension `(batch_size)` with each value of the
    batch corresponding to the expected label of each individual image.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºå›¾åƒåˆ†ç±»æ¨¡å‹ï¼Œï¼ˆ[ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)ï¼‰ï¼Œæ¨¡å‹æœŸæœ›ä¸€ä¸ªç»´åº¦ä¸º`(batch_size)`çš„å¼ é‡ï¼Œæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªå€¼å¯¹åº”äºæ¯ä¸ªå•ç‹¬å›¾åƒçš„é¢„æœŸæ ‡ç­¾ã€‚
- en: For semantic segmentation models, ([SegformerForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation)),
    the model expects a tensor of dimension `(batch_size, height, width)` with each
    value of the batch corresponding to the expected label of each individual pixel.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œï¼ˆ[SegformerForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation)ï¼‰ï¼Œæ¨¡å‹æœŸæœ›ä¸€ä¸ªç»´åº¦ä¸º`(batch_size,
    height, width)`çš„å¼ é‡ï¼Œæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªå€¼å¯¹åº”äºæ¯ä¸ªå•ç‹¬åƒç´ çš„é¢„æœŸæ ‡ç­¾ã€‚
- en: For object detection models, ([DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)),
    the model expects a list of dictionaries with a `class_labels` and `boxes` key
    where each value of the batch corresponds to the expected label and number of
    bounding boxes of each individual image.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œï¼ˆ[DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)ï¼‰ï¼Œæ¨¡å‹æœŸæœ›ä¸€ä¸ªå¸¦æœ‰`class_labels`å’Œ`boxes`é”®çš„å­—å…¸åˆ—è¡¨ï¼Œæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªå€¼å¯¹åº”äºæ¯ä¸ªå•ç‹¬å›¾åƒçš„é¢„æœŸæ ‡ç­¾å’Œè¾¹ç•Œæ¡†æ•°é‡ã€‚
- en: For automatic speech recognition models, ([Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)),
    the model expects a tensor of dimension `(batch_size, target_length)` with each
    value corresponding to the expected label of each individual token.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œï¼ˆ[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)ï¼‰ï¼Œæ¨¡å‹æœŸæœ›ä¸€ä¸ªç»´åº¦ä¸º`(batch_size,
    target_length)`çš„å¼ é‡ï¼Œæ¯ä¸ªå€¼å¯¹åº”äºæ¯ä¸ªå•ç‹¬æ ‡è®°çš„é¢„æœŸæ ‡ç­¾ã€‚
- en: Each modelâ€™s labels may be different, so be sure to always check the documentation
    of each model for more information about their specific labels!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ¨¡å‹çš„æ ‡ç­¾å¯èƒ½ä¸åŒï¼Œå› æ­¤è¯·åŠ¡å¿…å§‹ç»ˆæŸ¥çœ‹æ¯ä¸ªæ¨¡å‹çš„æ–‡æ¡£ä»¥è·å–æœ‰å…³å…¶ç‰¹å®šæ ‡ç­¾çš„æ›´å¤šä¿¡æ¯ï¼
- en: The base models ([BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel))
    do not accept labels, as these are the base transformer models, simply outputting
    features.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºç¡€æ¨¡å‹ï¼ˆ[BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel)ï¼‰ä¸æ¥å—æ ‡ç­¾ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åŸºç¡€å˜å‹å™¨æ¨¡å‹ï¼Œåªè¾“å‡ºç‰¹å¾ã€‚
- en: large language models (LLM)
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰
- en: A generic term that refers to transformer language models (GPT-3, BLOOM, OPT)
    that were trained on a large quantity of data. These models also tend to have
    a large number of learnable parameters (e.g. 175 billion for GPT-3).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ³›æŒ‡ï¼ŒæŒ‡çš„æ˜¯åœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒçš„å˜å‹å™¨è¯­è¨€æ¨¡å‹ï¼ˆGPT-3ã€BLOOMã€OPTï¼‰ã€‚è¿™äº›æ¨¡å‹é€šå¸¸ä¹Ÿå…·æœ‰å¤§é‡å¯å­¦ä¹ å‚æ•°ï¼ˆä¾‹å¦‚GPT-3çš„1750äº¿ï¼‰ã€‚
- en: M
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: M
- en: masked language modeling (MLM)
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰
- en: A pretraining task where the model sees a corrupted version of the texts, usually
    done by masking some tokens randomly, and has to predict the original text.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¨¡å‹çœ‹åˆ°æ–‡æœ¬çš„æŸåç‰ˆæœ¬ï¼Œé€šå¸¸æ˜¯é€šè¿‡éšæœºå±è”½ä¸€äº›æ ‡è®°æ¥å®Œæˆï¼Œå¹¶ä¸”å¿…é¡»é¢„æµ‹åŸå§‹æ–‡æœ¬çš„é¢„è®­ç»ƒä»»åŠ¡ã€‚
- en: multimodal
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¤šæ¨¡æ€
- en: A task that combines texts with another kind of inputs (for instance images).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ–‡æœ¬ä¸å…¶ä»–ç±»å‹çš„è¾“å…¥ï¼ˆä¾‹å¦‚å›¾åƒï¼‰ç»“åˆçš„ä»»åŠ¡ã€‚
- en: N
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: N
- en: Natural language generation (NLG)
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰
- en: All tasks related to generating text (for instance, [Write With Transformers](https://transformer.huggingface.co/),
    translation).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ä¸ç”Ÿæˆæ–‡æœ¬ç›¸å…³çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œ[ä½¿ç”¨Transformerå†™ä½œ](https://transformer.huggingface.co/)ï¼Œç¿»è¯‘ï¼‰ã€‚
- en: Natural language processing (NLP)
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰
- en: A generic way to say â€œdeal with textsâ€.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé€šç”¨çš„è¯´æ³•æ˜¯â€œå¤„ç†æ–‡æœ¬â€ã€‚
- en: Natural language understanding (NLU)
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰
- en: All tasks related to understanding what is in a text (for instance classifying
    the whole text, individual words).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ä¸ç†è§£æ–‡æœ¬å†…å®¹ç›¸å…³çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚å¯¹æ•´ä¸ªæ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œå¯¹å•è¯è¿›è¡Œåˆ†ç±»ï¼‰ã€‚
- en: P
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: P
- en: pipeline
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç®¡é“
- en: A pipeline in ğŸ¤— Transformers is an abstraction referring to a series of steps
    that are executed in a specific order to preprocess and transform data and return
    a prediction from a model. Some example stages found in a pipeline might be data
    preprocessing, feature extraction, and normalization.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ğŸ¤— Transformersä¸­ï¼Œç®¡é“æ˜¯ä¸€ä¸ªæŠ½è±¡ï¼ŒæŒ‡çš„æ˜¯æŒ‰ç‰¹å®šé¡ºåºæ‰§è¡Œçš„ä¸€ç³»åˆ—æ­¥éª¤ï¼Œç”¨äºé¢„å¤„ç†å’Œè½¬æ¢æ•°æ®ï¼Œå¹¶ä»æ¨¡å‹è¿”å›é¢„æµ‹ã€‚ç®¡é“ä¸­å¯èƒ½åŒ…å«çš„ä¸€äº›ç¤ºä¾‹é˜¶æ®µå¯èƒ½æ˜¯æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾æå–å’Œå½’ä¸€åŒ–ã€‚
- en: For more details, see [Pipelines for inference](https://huggingface.co/docs/transformers/pipeline_tutorial).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ç”¨äºæ¨æ–­çš„ç®¡é“](https://huggingface.co/docs/transformers/pipeline_tutorial)ã€‚
- en: PipelineParallel (PP)
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PipelineParallelï¼ˆPPï¼‰
- en: Parallelism technique in which the model is split up vertically (layer-level)
    across multiple GPUs, so that only one or several layers of the model are placed
    on a single GPU. Each GPU processes in parallel different stages of the pipeline
    and working on a small chunk of the batch. Learn more about how PipelineParallel
    works [here](perf_train_gpu_many#from-naive-model-parallelism-to-pipeline-parallelism).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§å¹¶è¡ŒæŠ€æœ¯ï¼Œå…¶ä¸­æ¨¡å‹åœ¨å¤šä¸ªGPUä¸Šå‚ç›´åˆ†å‰²ï¼ˆå±‚çº§ï¼‰ï¼Œä»¥ä¾¿å°†æ¨¡å‹çš„ä¸€ä¸ªæˆ–å¤šä¸ªå±‚æ”¾ç½®åœ¨å•ä¸ªGPUä¸Šã€‚æ¯ä¸ªGPUå¹¶è¡Œå¤„ç†ç®¡é“çš„ä¸åŒé˜¶æ®µï¼Œå¹¶å¤„ç†ä¸€å°æ‰¹æ¬¡çš„æ•°æ®ã€‚äº†è§£æœ‰å…³PipelineParallelå¦‚ä½•å·¥ä½œçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[è¿™é‡Œ](perf_train_gpu_many#from-naive-model-parallelism-to-pipeline-parallelism)ã€‚
- en: pixel values
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åƒç´ å€¼
- en: A tensor of the numerical representations of an image that is passed to a model.
    The pixel values have a shape of [`batch_size`, `num_channels`, `height`, `width`],
    and are generated from an image processor.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ é€’ç»™æ¨¡å‹çš„å›¾åƒçš„æ•°å€¼è¡¨ç¤ºçš„å¼ é‡ã€‚åƒç´ å€¼çš„å½¢çŠ¶ä¸º[`batch_size`, `num_channels`, `height`, `width`]ï¼Œå¹¶ä¸”æ˜¯ä»å›¾åƒå¤„ç†å™¨ç”Ÿæˆçš„ã€‚
- en: pooling
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ± åŒ–
- en: An operation that reduces a matrix into a smaller matrix, either by taking the
    maximum or average of the pooled dimension(s). Pooling layers are commonly found
    between convolutional layers to downsample the feature representation.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å°†çŸ©é˜µç¼©å°ä¸ºè¾ƒå°çŸ©é˜µçš„æ“ä½œï¼Œå¯ä»¥é€šè¿‡å–æ± åŒ–ç»´åº¦çš„æœ€å¤§å€¼æˆ–å¹³å‡å€¼æ¥å®ç°ã€‚æ± åŒ–å±‚é€šå¸¸ä½äºå·ç§¯å±‚ä¹‹é—´ï¼Œç”¨äºé™é‡‡æ ·ç‰¹å¾è¡¨ç¤ºã€‚
- en: position IDs
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½ç½®ID
- en: Contrary to RNNs that have the position of each token embedded within them,
    transformers are unaware of the position of each token. Therefore, the position
    IDs (`position_ids`) are used by the model to identify each tokenâ€™s position in
    the list of tokens.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸RNNä¸åŒï¼ŒRNNçš„æ¯ä¸ªæ ‡è®°çš„ä½ç½®éƒ½åµŒå…¥åœ¨å…¶ä¸­ï¼Œtransformersä¸çŸ¥é“æ¯ä¸ªæ ‡è®°çš„ä½ç½®ã€‚å› æ­¤ï¼Œæ¨¡å‹ä½¿ç”¨ä½ç½®IDï¼ˆ`position_ids`ï¼‰æ¥è¯†åˆ«åˆ—è¡¨ä¸­æ¯ä¸ªæ ‡è®°çš„ä½ç½®ã€‚
- en: They are an optional parameter. If no `position_ids` are passed to the model,
    the IDs are automatically created as absolute positional embeddings.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬æ˜¯ä¸€ä¸ªå¯é€‰å‚æ•°ã€‚å¦‚æœæ²¡æœ‰å°†`position_ids`ä¼ é€’ç»™æ¨¡å‹ï¼Œé‚£ä¹ˆè¿™äº›IDå°†è‡ªåŠ¨åˆ›å»ºä¸ºç»å¯¹ä½ç½®åµŒå…¥ã€‚
- en: Absolute positional embeddings are selected in the range `[0, config.max_position_embeddings
    - 1]`. Some models use other types of positional embeddings, such as sinusoidal
    position embeddings or relative position embeddings.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ç»å¯¹ä½ç½®åµŒå…¥è¢«é€‰æ‹©åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`å†…ã€‚ä¸€äº›æ¨¡å‹ä½¿ç”¨å…¶ä»–ç±»å‹çš„ä½ç½®åµŒå…¥ï¼Œå¦‚æ­£å¼¦ä½ç½®åµŒå…¥æˆ–ç›¸å¯¹ä½ç½®åµŒå…¥ã€‚
- en: preprocessing
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†
- en: The task of preparing raw data into a format that can be easily consumed by
    machine learning models. For example, text is typically preprocessed by tokenization.
    To gain a better idea of what preprocessing looks like for other input types,
    check out the [Preprocess](https://huggingface.co/docs/transformers/preprocessing)
    tutorial.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: å°†åŸå§‹æ•°æ®å‡†å¤‡æˆæœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥è½»æ¾æ¶ˆåŒ–çš„æ ¼å¼çš„ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œæ–‡æœ¬é€šå¸¸é€šè¿‡æ ‡è®°åŒ–è¿›è¡Œé¢„å¤„ç†ã€‚è¦äº†è§£å…¶ä»–è¾“å…¥ç±»å‹çš„é¢„å¤„ç†æ˜¯ä»€ä¹ˆæ ·å­ï¼Œå¯ä»¥æŸ¥çœ‹[é¢„å¤„ç†](https://huggingface.co/docs/transformers/preprocessing)æ•™ç¨‹ã€‚
- en: pretrained model
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é¢„è®­ç»ƒæ¨¡å‹
- en: A model that has been pretrained on some data (for instance all of Wikipedia).
    Pretraining methods involve a self-supervised objective, which can be reading
    the text and trying to predict the next word (see [causal language modeling](#causal-language-modeling))
    or masking some words and trying to predict them (see [masked language modeling](#masked-language-modeling-mlm)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåœ¨æŸäº›æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒçš„æ¨¡å‹ï¼ˆä¾‹å¦‚æ•´ä¸ªç»´åŸºç™¾ç§‘ï¼‰ã€‚é¢„è®­ç»ƒæ–¹æ³•æ¶‰åŠè‡ªç›‘ç£ç›®æ ‡ï¼Œå¯ä»¥æ˜¯é˜…è¯»æ–‡æœ¬å¹¶å°è¯•é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼ˆå‚è§[å› æœè¯­è¨€å»ºæ¨¡](#causal-language-modeling)ï¼‰ï¼Œæˆ–è€…é®è”½ä¸€äº›å•è¯å¹¶å°è¯•é¢„æµ‹å®ƒä»¬ï¼ˆå‚è§[é®è”½è¯­è¨€å»ºæ¨¡](#masked-language-modeling-mlm)ï¼‰ã€‚
- en: Speech and vision models have their own pretraining objectives. For example,
    Wav2Vec2 is a speech model pretrained on a contrastive task which requires the
    model to identify the â€œtrueâ€ speech representation from a set of â€œfalseâ€ speech
    representations. On the other hand, BEiT is a vision model pretrained on a masked
    image modeling task which masks some of the image patches and requires the model
    to predict the masked patches (similar to the masked language modeling objective).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­éŸ³å’Œè§†è§‰æ¨¡å‹æœ‰è‡ªå·±çš„é¢„è®­ç»ƒç›®æ ‡ã€‚ä¾‹å¦‚ï¼ŒWav2Vec2æ˜¯ä¸€ä¸ªè¯­éŸ³æ¨¡å‹ï¼Œå®ƒåœ¨å¯¹æ¯”ä»»åŠ¡ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œè¦æ±‚æ¨¡å‹ä»ä¸€ç»„â€œå‡â€è¯­éŸ³è¡¨ç¤ºä¸­è¯†åˆ«â€œçœŸå®â€è¯­éŸ³è¡¨ç¤ºã€‚å¦ä¸€æ–¹é¢ï¼ŒBEiTæ˜¯ä¸€ä¸ªè§†è§‰æ¨¡å‹ï¼Œå®ƒåœ¨ä¸€ä¸ªé®è”½å›¾åƒå»ºæ¨¡ä»»åŠ¡ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œé®è”½äº†ä¸€äº›å›¾åƒå—ï¼Œå¹¶è¦æ±‚æ¨¡å‹é¢„æµ‹è¢«é®è”½çš„å—ï¼ˆç±»ä¼¼äºé®è”½è¯­è¨€å»ºæ¨¡ç›®æ ‡ï¼‰ã€‚
- en: R
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: R
- en: recurrent neural network (RNN)
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰
- en: A type of model that uses a loop over a layer to process texts.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§ä½¿ç”¨å¾ªç¯å¤„ç†æ–‡æœ¬çš„æ¨¡å‹ç±»å‹ã€‚
- en: representation learning
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºå­¦ä¹ 
- en: A subfield of machine learning which focuses on learning meaningful representations
    of raw data. Some examples of representation learning techniques include word
    embeddings, autoencoders, and Generative Adversarial Networks (GANs).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§æœºå™¨å­¦ä¹ çš„å­é¢†åŸŸï¼Œä¸“æ³¨äºå­¦ä¹ åŸå§‹æ•°æ®çš„æœ‰æ„ä¹‰è¡¨ç¤ºã€‚ä¸€äº›è¡¨ç¤ºå­¦ä¹ æŠ€æœ¯çš„ä¾‹å­åŒ…æ‹¬è¯åµŒå…¥ã€è‡ªç¼–ç å™¨å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€‚
- en: S
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: S
- en: sampling rate
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é‡‡æ ·ç‡
- en: A measurement in hertz of the number of samples (the audio signal) taken per
    second. The sampling rate is a result of discretizing a continuous signal such
    as speech.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ç§’é‡‡æ ·çš„æ ·æœ¬æ•°ï¼ˆéŸ³é¢‘ä¿¡å·ï¼‰ã€‚é‡‡æ ·ç‡æ˜¯å°†è¿ç»­ä¿¡å·ï¼ˆå¦‚è¯­éŸ³ï¼‰ç¦»æ•£åŒ–çš„ç»“æœã€‚
- en: self-attention
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ªæ³¨æ„åŠ›
- en: Each element of the input finds out which other elements of the input they should
    attend to.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥çš„æ¯ä¸ªå…ƒç´ æ‰¾å‡ºå®ƒä»¬åº”è¯¥å…³æ³¨çš„å…¶ä»–è¾“å…¥å…ƒç´ ã€‚
- en: self-supervised learning
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è‡ªç›‘ç£å­¦ä¹ 
- en: A category of machine learning techniques in which a model creates its own learning
    objective from unlabeled data. It differs from [unsupervised learning](#unsupervised-learning)
    and [supervised learning](#supervised-learning) in that the learning process is
    supervised, but not explicitly from the user.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç±»æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå…¶ä¸­æ¨¡å‹ä»æœªæ ‡è®°æ•°æ®ä¸­åˆ›å»ºè‡ªå·±çš„å­¦ä¹ ç›®æ ‡ã€‚å®ƒä¸[æ— ç›‘ç£å­¦ä¹ ](#unsupervised-learning)å’Œ[ç›‘ç£å­¦ä¹ ](#supervised-learning)ä¸åŒï¼Œå­¦ä¹ è¿‡ç¨‹æ˜¯å—ç›‘ç£çš„ï¼Œä½†ä¸æ˜¯æ˜ç¡®æ¥è‡ªç”¨æˆ·ã€‚
- en: One example of self-supervised learning is [masked language modeling](#masked-language-modeling-mlm),
    where a model is passed sentences with a proportion of its tokens removed and
    learns to predict the missing tokens.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç›‘ç£å­¦ä¹ çš„ä¸€ä¸ªä¾‹å­æ˜¯[æ©ç è¯­è¨€å»ºæ¨¡](#masked-language-modeling-mlm)ï¼Œå…¶ä¸­æ¨¡å‹ä¼ é€’å¸¦æœ‰ä¸€å®šæ¯”ä¾‹æ ‡è®°çš„å¥å­ï¼Œå¹¶å­¦ä¹ é¢„æµ‹ç¼ºå¤±çš„æ ‡è®°ã€‚
- en: semi-supervised learning
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŠç›‘ç£å­¦ä¹ 
- en: A broad category of machine learning training techniques that leverages a small
    amount of labeled data with a larger quantity of unlabeled data to improve the
    accuracy of a model, unlike [supervised learning](#supervised-learning) and [unsupervised
    learning](#unsupervised-learning).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§å¹¿æ³›çš„æœºå™¨å­¦ä¹ è®­ç»ƒæŠ€æœ¯ç±»åˆ«ï¼Œåˆ©ç”¨å°‘é‡æ ‡è®°æ•°æ®å’Œå¤§é‡æœªæ ‡è®°æ•°æ®æ¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œä¸[ç›‘ç£å­¦ä¹ ](#supervised-learning)å’Œ[æ— ç›‘ç£å­¦ä¹ ](#unsupervised-learning)ä¸åŒã€‚
- en: An example of a semi-supervised learning approach is â€œself-trainingâ€, in which
    a model is trained on labeled data, and then used to make predictions on the unlabeled
    data. The portion of the unlabeled data that the model predicts with the most
    confidence gets added to the labeled dataset and used to retrain the model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: åŠç›‘ç£å­¦ä¹ æ–¹æ³•çš„ä¸€ä¸ªä¾‹å­æ˜¯â€œè‡ªè®­ç»ƒâ€ï¼Œå…¶ä¸­æ¨¡å‹åœ¨æ ‡è®°æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç„¶åç”¨äºå¯¹æœªæ ‡è®°æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚æ¨¡å‹ä»¥æœ€å¤§ç½®ä¿¡åº¦é¢„æµ‹çš„æœªæ ‡è®°æ•°æ®éƒ¨åˆ†è¢«æ·»åŠ åˆ°æ ‡è®°æ•°æ®é›†ä¸­ï¼Œå¹¶ç”¨äºé‡æ–°è®­ç»ƒæ¨¡å‹ã€‚
- en: sequence-to-sequence (seq2seq)
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åºåˆ—åˆ°åºåˆ—ï¼ˆseq2seqï¼‰
- en: Models that generate a new sequence from an input, like translation models,
    or summarization models (such as [Bart](model_doc/bart) or [T5](model_doc/t5)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¾“å…¥ç”Ÿæˆæ–°åºåˆ—çš„æ¨¡å‹ï¼Œæ¯”å¦‚ç¿»è¯‘æ¨¡å‹æˆ–æ€»ç»“æ¨¡å‹ï¼ˆæ¯”å¦‚[Bart](model_doc/bart)æˆ–[T5](model_doc/t5)ï¼‰ã€‚
- en: Sharded DDP
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ†ç‰‡DDP
- en: Another name for the foundational [ZeRO](#zero-redundancy-optimizer--zero-)
    concept as used by various other implementations of ZeRO.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºZeROæ¦‚å¿µçš„å¦ä¸€ä¸ªåç§°ï¼Œè¢«å„ç§å…¶ä»–ZeROå®ç°æ‰€ä½¿ç”¨ã€‚
- en: stride
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ­¥å¹…
- en: In [convolution](#convolution) or [pooling](#pooling), the stride refers to
    the distance the kernel is moved over a matrix. A stride of 1 means the kernel
    is moved one pixel over at a time, and a stride of 2 means the kernel is moved
    two pixels over at a time.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[å·ç§¯](#convolution)æˆ–[æ± åŒ–](#pooling)ä¸­ï¼Œæ­¥å¹…æŒ‡çš„æ˜¯æ ¸åœ¨çŸ©é˜µä¸Šç§»åŠ¨çš„è·ç¦»ã€‚æ­¥å¹…ä¸º1è¡¨ç¤ºæ ¸æ¯æ¬¡ç§»åŠ¨ä¸€ä¸ªåƒç´ ï¼Œæ­¥å¹…ä¸º2è¡¨ç¤ºæ ¸æ¯æ¬¡ç§»åŠ¨ä¸¤ä¸ªåƒç´ ã€‚
- en: supervised learning
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç›‘ç£å­¦ä¹ 
- en: A form of model training that directly uses labeled data to correct and instruct
    model performance. Data is fed into the model being trained, and its predictions
    are compared to the known labels. The model updates its weights based on how incorrect
    its predictions were, and the process is repeated to optimize model performance.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§ç›´æ¥ä½¿ç”¨æ ‡è®°æ•°æ®æ¥çº æ­£å’ŒæŒ‡å¯¼æ¨¡å‹æ€§èƒ½çš„æ¨¡å‹è®­ç»ƒå½¢å¼ã€‚æ•°æ®è¢«é¦ˆé€åˆ°æ­£åœ¨è®­ç»ƒçš„æ¨¡å‹ä¸­ï¼Œå…¶é¢„æµ‹ä¸å·²çŸ¥æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒã€‚æ¨¡å‹æ ¹æ®å…¶é¢„æµ‹çš„ä¸æ­£ç¡®ç¨‹åº¦æ›´æ–°å…¶æƒé‡ï¼Œå¹¶é‡å¤è¯¥è¿‡ç¨‹ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚
- en: T
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: T
- en: Tensor Parallelism (TP)
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¼ é‡å¹¶è¡Œæ€§ï¼ˆTPï¼‰
- en: Parallelism technique for training on multiple GPUs in which each tensor is
    split up into multiple chunks, so instead of having the whole tensor reside on
    a single GPU, each shard of the tensor resides on its designated GPU. Shards gets
    processed separately and in parallel on different GPUs and the results are synced
    at the end of the processing step. This is what is sometimes called horizontal
    parallelism, as the splitting happens on horizontal level. Learn more about Tensor
    Parallelism [here](perf_train_gpu_many#tensor-parallelism).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤šä¸ªGPUä¸Šè¿›è¡Œè®­ç»ƒçš„å¹¶è¡ŒæŠ€æœ¯ï¼Œå…¶ä¸­æ¯ä¸ªå¼ é‡è¢«åˆ†å‰²æˆå¤šä¸ªå—ï¼Œå› æ­¤æ¯ä¸ªå¼ é‡çš„ç¢ç‰‡éƒ½é©»ç•™åœ¨å…¶æŒ‡å®šçš„GPUä¸Šï¼Œè€Œä¸æ˜¯æ•´ä¸ªå¼ é‡é©»ç•™åœ¨å•ä¸ªGPUä¸Šã€‚ç¢ç‰‡åœ¨ä¸åŒGPUä¸Šåˆ†åˆ«å¹¶è¡Œå¤„ç†ï¼Œå¹¶åœ¨å¤„ç†æ­¥éª¤ç»“æŸæ—¶è¿›è¡ŒåŒæ­¥ã€‚è¿™æœ‰æ—¶è¢«ç§°ä¸ºæ°´å¹³å¹¶è¡Œï¼Œå› ä¸ºåˆ†å‰²å‘ç”Ÿåœ¨æ°´å¹³çº§åˆ«ã€‚åœ¨è¿™é‡Œäº†è§£æ›´å¤šå…³äºå¼ é‡å¹¶è¡Œæ€§çš„ä¿¡æ¯ã€‚
- en: token
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ ‡è®°
- en: A part of a sentence, usually a word, but can also be a subword (non-common
    words are often split in subwords) or a punctuation symbol.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: å¥å­çš„ä¸€éƒ¨åˆ†ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªè¯ï¼Œä½†ä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªå­è¯ï¼ˆä¸å¸¸è§çš„è¯é€šå¸¸è¢«åˆ†å‰²ä¸ºå­è¯ï¼‰æˆ–æ ‡ç‚¹ç¬¦å·ã€‚
- en: token Type IDs
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ ‡è®°ç±»å‹ID
- en: Some modelsâ€™ purpose is to do classification on pairs of sentences or question
    answering.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›æ¨¡å‹çš„ç›®çš„æ˜¯å¯¹å¥å­å¯¹æˆ–é—®é¢˜å›ç­”è¿›è¡Œåˆ†ç±»ã€‚
- en: '[https://www.youtube-nocookie.com/embed/0u3ioSwev3s](https://www.youtube-nocookie.com/embed/0u3ioSwev3s)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/0u3ioSwev3s](https://www.youtube-nocookie.com/embed/0u3ioSwev3s)'
- en: 'These require two different sequences to be joined in a single â€œinput_idsâ€
    entry, which usually is performed with the help of special tokens, such as the
    classifier (`[CLS]`) and separator (`[SEP]`) tokens. For example, the BERT model
    builds its two sequence input as such:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›éœ€è¦å°†ä¸¤ä¸ªä¸åŒåºåˆ—è¿æ¥åœ¨ä¸€ä¸ªâ€œinput_idsâ€æ¡ç›®ä¸­ï¼Œé€šå¸¸ä½¿ç”¨ç‰¹æ®Šæ ‡è®°çš„å¸®åŠ©æ¥æ‰§è¡Œï¼Œä¾‹å¦‚åˆ†ç±»å™¨ï¼ˆ`[CLS]`ï¼‰å’Œåˆ†éš”ç¬¦ï¼ˆ`[SEP]`ï¼‰æ ‡è®°ã€‚ä¾‹å¦‚ï¼ŒBERTæ¨¡å‹æ„å»ºå…¶ä¸¤ä¸ªåºåˆ—è¾“å…¥å¦‚ä¸‹ï¼š
- en: '[PRE12]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can use our tokenizer to automatically generate such a sentence by passing
    the two sequences to `tokenizer` as two arguments (and not a list, like before)
    like this:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„åˆ†è¯å™¨é€šè¿‡å°†ä¸¤ä¸ªåºåˆ—ä½œä¸ºä¸¤ä¸ªå‚æ•°ï¼ˆè€Œä¸æ˜¯åƒä»¥å‰é‚£æ ·ä½œä¸ºåˆ—è¡¨ï¼‰ä¼ é€’ç»™`tokenizer`æ¥è‡ªåŠ¨ç”Ÿæˆè¿™æ ·ä¸€ä¸ªå¥å­ï¼Œå°±åƒè¿™æ ·ï¼š
- en: '[PRE13]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'which will return:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¿”å›ï¼š
- en: '[PRE14]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This is enough for some models to understand where one sequence ends and where
    another begins. However, other models, such as BERT, also deploy token type IDs
    (also called segment IDs). They are represented as a binary mask identifying the
    two types of sequence in the model.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¹äºä¸€äº›æ¨¡å‹æ¥è¯´è¶³å¤Ÿäº†è§£ä¸€ä¸ªåºåˆ—çš„ç»“æŸå’Œå¦ä¸€ä¸ªåºåˆ—çš„å¼€å§‹ã€‚ç„¶è€Œï¼Œå…¶ä»–æ¨¡å‹ï¼Œå¦‚BERTï¼Œè¿˜éƒ¨ç½²äº†æ ‡è®°ç±»å‹IDï¼ˆä¹Ÿç§°ä¸ºæ®µIDï¼‰ã€‚å®ƒä»¬è¡¨ç¤ºæ¨¡å‹ä¸­ä¸¤ç§åºåˆ—çš„äºŒè¿›åˆ¶æ©ç ã€‚
- en: 'The tokenizer returns this mask as the â€œtoken_type_idsâ€ entry:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†è¯å™¨å°†æ­¤æ©ç è¿”å›ä¸ºâ€œtoken_type_idsâ€æ¡ç›®ï¼š
- en: '[PRE15]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The first sequence, the â€œcontextâ€ used for the question, has all its tokens
    represented by a `0`, whereas the second sequence, corresponding to the â€œquestionâ€,
    has all its tokens represented by a `1`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªåºåˆ—ï¼Œç”¨äºé—®é¢˜çš„â€œä¸Šä¸‹æ–‡â€ï¼Œå…¶æ‰€æœ‰æ ‡è®°éƒ½è¡¨ç¤ºä¸º`0`ï¼Œè€Œç¬¬äºŒä¸ªåºåˆ—ï¼Œå¯¹åº”äºâ€œé—®é¢˜â€ï¼Œå…¶æ‰€æœ‰æ ‡è®°éƒ½è¡¨ç¤ºä¸º`1`ã€‚
- en: Some models, like [XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel)
    use an additional token represented by a `2`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›æ¨¡å‹ï¼Œæ¯”å¦‚[XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel)ä½¿ç”¨ä¸€ä¸ªé¢å¤–çš„æ ‡è®°ï¼Œè¡¨ç¤ºä¸º`2`ã€‚
- en: transfer learning
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¿ç§»å­¦ä¹ 
- en: A technique that involves taking a pretrained model and adapting it to a dataset
    specific to your task. Instead of training a model from scratch, you can leverage
    knowledge obtained from an existing model as a starting point. This speeds up
    the learning process and reduces the amount of training data needed.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§æŠ€æœ¯ï¼Œæ¶‰åŠé‡‡ç”¨é¢„è®­ç»ƒæ¨¡å‹å¹¶å°†å…¶è°ƒæ•´ä¸ºç‰¹å®šäºæ‚¨ä»»åŠ¡çš„æ•°æ®é›†ã€‚æ‚¨å¯ä»¥åˆ©ç”¨ä»ç°æœ‰æ¨¡å‹è·å¾—çš„çŸ¥è¯†ä½œä¸ºèµ·ç‚¹ï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚è¿™åŠ å¿«äº†å­¦ä¹ è¿‡ç¨‹å¹¶å‡å°‘äº†æ‰€éœ€çš„è®­ç»ƒæ•°æ®é‡ã€‚
- en: transformer
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å˜å‹å™¨
- en: Self-attention based deep learning model architecture.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè‡ªæ³¨æ„åŠ›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ¶æ„ã€‚
- en: U
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: U
- en: unsupervised learning
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ— ç›‘ç£å­¦ä¹ 
- en: A form of model training in which data provided to the model is not labeled.
    Unsupervised learning techniques leverage statistical information of the data
    distribution to find patterns useful for the task at hand.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§æ¨¡å‹è®­ç»ƒå½¢å¼ï¼Œå…¶ä¸­æä¾›ç»™æ¨¡å‹çš„æ•°æ®æ²¡æœ‰æ ‡è®°ã€‚æ— ç›‘ç£å­¦ä¹ æŠ€æœ¯åˆ©ç”¨æ•°æ®åˆ†å¸ƒçš„ç»Ÿè®¡ä¿¡æ¯æ¥æ‰¾åˆ°å¯¹å½“å‰ä»»åŠ¡æœ‰ç”¨çš„æ¨¡å¼ã€‚
- en: Z
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Z
- en: Zero Redundancy Optimizer (ZeRO)
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é›¶å†—ä½™ä¼˜åŒ–å™¨ï¼ˆZeROï¼‰
- en: Parallelism technique which performs sharding of the tensors somewhat similar
    to [TensorParallel](#tensor-parallelism-tp), except the whole tensor gets reconstructed
    in time for a forward or backward computation, therefore the model doesnâ€™t need
    to be modified. This method also supports various offloading techniques to compensate
    for limited GPU memory. Learn more about ZeRO [here](perf_train_gpu_many#zero-data-parallelism).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§å¹¶è¡ŒæŠ€æœ¯ï¼Œç±»ä¼¼äº[TensorParallel](#tensor-parallelism-tp)å¯¹å¼ é‡è¿›è¡Œåˆ†ç‰‡ï¼Œé™¤äº†æ•´ä¸ªå¼ é‡åœ¨å‰å‘æˆ–åå‘è®¡ç®—æ—¶ä¼šé‡æ–°æ„å»ºï¼Œå› æ­¤æ¨¡å‹ä¸éœ€è¦è¢«ä¿®æ”¹ã€‚è¿™ç§æ–¹æ³•è¿˜æ”¯æŒå„ç§å¸è½½æŠ€æœ¯ï¼Œä»¥å¼¥è¡¥æœ‰é™çš„GPUå†…å­˜ã€‚åœ¨è¿™é‡Œäº†è§£æ›´å¤šå…³äºZeROçš„ä¿¡æ¯(perf_train_gpu_many#zero-data-parallelism)ã€‚
