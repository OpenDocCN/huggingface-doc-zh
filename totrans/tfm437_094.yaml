- en: Glossary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/docs/transformers/v4.37.2/en/glossary](https://huggingface.co/docs/transformers/v4.37.2/en/glossary)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/21.61b7afc5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Youtube.e1129c6f.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: This glossary defines general machine learning and ü§ó Transformers terms to help
    you better understand the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: attention mask
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The attention mask is an optional argument used when batching sequences together.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/M6adb1j2jPI](https://www.youtube-nocookie.com/embed/M6adb1j2jPI)'
  prefs: []
  type: TYPE_NORMAL
- en: This argument indicates to the model which tokens should be attended to, and
    which should not.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider these two sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The encoded versions have different lengths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, we can‚Äôt put them together in the same tensor as-is. The first sequence
    needs to be padded up to the length of the second one, or the second one needs
    to be truncated down to the length of the first one.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first case, the list of IDs will be extended by the padding indices.
    We can pass a list to the tokenizer and ask it to pad like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that 0s have been added on the right of the first sentence to make
    it the same length as the second one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This can then be converted into a tensor in PyTorch or TensorFlow. The attention
    mask is a binary tensor indicating the position of the padded indices so that
    the model does not attend to them. For the [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer),
    `1` indicates a value that should be attended to, while `0` indicates a padded
    value. This attention mask is in the dictionary returned by the tokenizer under
    the key ‚Äúattention_mask‚Äù:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: autoencoding models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: See [encoder models](#encoder-models) and [masked language modeling](#masked-language-modeling-mlm)
  prefs: []
  type: TYPE_NORMAL
- en: autoregressive models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: See [causal language modeling](#causal-language-modeling) and [decoder models](#decoder-models)
  prefs: []
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: backbone
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The backbone is the network (embeddings and layers) that outputs the raw hidden
    states or features. It is usually connected to a [head](#head) which accepts the
    features as its input to make a prediction. For example, [ViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTModel)
    is a backbone without a specific head on top. Other models can also use `VitModel`
    as a backbone such as [DPT](model_doc/dpt).
  prefs: []
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: causal language modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A pretraining task where the model reads the texts in order and has to predict
    the next word. It‚Äôs usually done by reading the whole sentence but using a mask
    inside the model to hide the future tokens at a certain timestep.
  prefs: []
  type: TYPE_NORMAL
- en: channel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Color images are made up of some combination of values in three channels: red,
    green, and blue (RGB) and grayscale images only have one channel. In ü§ó Transformers,
    the channel can be the first or last dimension of an image‚Äôs tensor: [`n_channels`,
    `height`, `width`] or [`height`, `width`, `n_channels`].'
  prefs: []
  type: TYPE_NORMAL
- en: connectionist temporal classification (CTC)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An algorithm which allows a model to learn without knowing exactly how the input
    and output are aligned; CTC calculates the distribution of all possible outputs
    for a given input and chooses the most likely output from it. CTC is commonly
    used in speech recognition tasks because speech doesn‚Äôt always cleanly align with
    the transcript for a variety of reasons such as a speaker‚Äôs different speech rates.
  prefs: []
  type: TYPE_NORMAL
- en: convolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A type of layer in a neural network where the input matrix is multiplied element-wise
    by a smaller matrix (kernel or filter) and the values are summed up in a new matrix.
    This is known as a convolutional operation which is repeated over the entire input
    matrix. Each operation is applied to a different segment of the input matrix.
    Convolutional neural networks (CNNs) are commonly used in computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DataParallel (DP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parallelism technique for training on multiple GPUs where the same setup is
    replicated multiple times, with each instance receiving a distinct data slice.
    The processing is done in parallel and all setups are synchronized at the end
    of each training step.
  prefs: []
  type: TYPE_NORMAL
- en: Learn more about how DataParallel works [here](perf_train_gpu_many#dataparallel-vs-distributeddataparallel).
  prefs: []
  type: TYPE_NORMAL
- en: decoder input IDs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This input is specific to encoder-decoder models, and contains the input IDs
    that will be fed to the decoder. These inputs should be used for sequence to sequence
    tasks, such as translation or summarization, and are usually built in a way specific
    to each model.
  prefs: []
  type: TYPE_NORMAL
- en: Most encoder-decoder models (BART, T5) create their `decoder_input_ids` on their
    own from the `labels`. In such models, passing the `labels` is the preferred way
    to handle training.
  prefs: []
  type: TYPE_NORMAL
- en: Please check each model‚Äôs docs to see how they handle these input IDs for sequence
    to sequence training.
  prefs: []
  type: TYPE_NORMAL
- en: decoder models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Also referred to as autoregressive models, decoder models involve a pretraining
    task (called causal language modeling) where the model reads the texts in order
    and has to predict the next word. It‚Äôs usually done by reading the whole sentence
    with a mask to hide future tokens at a certain timestep.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/d_ixlCubqQw](https://www.youtube-nocookie.com/embed/d_ixlCubqQw)'
  prefs: []
  type: TYPE_NORMAL
- en: deep learning (DL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning algorithms which uses neural networks with several layers.
  prefs: []
  type: TYPE_NORMAL
- en: E
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: encoder models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Also known as autoencoding models, encoder models take an input (such as text
    or images) and transform them into a condensed numerical representation called
    an embedding. Oftentimes, encoder models are pretrained using techniques like
    [masked language modeling](#masked-language-modeling-mlm), which masks parts of
    the input sequence and forces the model to create more meaningful representations.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/H39Z_720T5s](https://www.youtube-nocookie.com/embed/H39Z_720T5s)'
  prefs: []
  type: TYPE_NORMAL
- en: F
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: feature extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The process of selecting and transforming raw data into a set of features that
    are more informative and useful for machine learning algorithms. Some examples
    of feature extraction include transforming raw text into word embeddings and extracting
    important features such as edges or shapes from image/video data.
  prefs: []
  type: TYPE_NORMAL
- en: feed forward chunking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In each residual attention block in transformers the self-attention layer is
    usually followed by 2 feed forward layers. The intermediate embedding size of
    the feed forward layers is often bigger than the hidden size of the model (e.g.,
    for `bert-base-uncased`).
  prefs: []
  type: TYPE_NORMAL
- en: 'For an input of size `[batch_size, sequence_length]`, the memory required to
    store the intermediate feed forward embeddings `[batch_size, sequence_length,
    config.intermediate_size]` can account for a large fraction of the memory use.
    The authors of [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
    noticed that since the computation is independent of the `sequence_length` dimension,
    it is mathematically equivalent to compute the output embeddings of both feed
    forward layers `[batch_size, config.hidden_size]_0, ..., [batch_size, config.hidden_size]_n`
    individually and concat them afterward to `[batch_size, sequence_length, config.hidden_size]`
    with `n = sequence_length`, which trades increased computation time against reduced
    memory use, but yields a mathematically **equivalent** result.'
  prefs: []
  type: TYPE_NORMAL
- en: For models employing the function [apply_chunking_to_forward()](/docs/transformers/v4.37.2/en/internal/modeling_utils#transformers.apply_chunking_to_forward),
    the `chunk_size` defines the number of output embeddings that are computed in
    parallel and thus defines the trade-off between memory and time complexity. If
    `chunk_size` is set to 0, no feed forward chunking is done.
  prefs: []
  type: TYPE_NORMAL
- en: finetuned models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finetuning is a form of transfer learning which involves taking a pretrained
    model, freezing its weights, and replacing the output layer with a newly added
    [model head](#head). The model head is trained on your target dataset.
  prefs: []
  type: TYPE_NORMAL
- en: See the [Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training)
    tutorial for more details, and learn how to fine-tune models with ü§ó Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: H
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: head
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The model head refers to the last layer of a neural network that accepts the
    raw hidden states and projects them onto a different dimension. There is a different
    model head for each task. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[GPT2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification)
    is a sequence classification head - a linear layer - on top of the base [GPT2Model](/docs/transformers/v4.37.2/en/model_doc/gpt2#transformers.GPT2Model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)
    is an image classification head - a linear layer on top of the final hidden state
    of the `CLS` token - on top of the base [ViTModel](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)
    is a language modeling head with [CTC](#connectionist-temporal-classification-(CTC))
    on top of the base [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: image patch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vision-based Transformers models split an image into smaller patches which are
    linearly embedded, and then passed as a sequence to the model. You can find the
    `patch_size` - or resolution - of the model in its configuration.
  prefs: []
  type: TYPE_NORMAL
- en: inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inference is the process of evaluating a model on new data after training is
    complete. See the [Pipeline for inference](https://huggingface.co/docs/transformers/pipeline_tutorial)
    tutorial to learn how to perform inference with ü§ó Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: input IDs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The input ids are often the only required parameters to be passed to the model
    as input. They are token indices, numerical representations of tokens building
    the sequences that will be used as input by the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/VFp38yj8h3A](https://www.youtube-nocookie.com/embed/VFp38yj8h3A)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each tokenizer works differently but the underlying mechanism remains the same.
    Here‚Äôs an example using the BERT tokenizer, which is a [WordPiece](https://arxiv.org/pdf/1609.08144.pdf)
    tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The tokenizer takes care of splitting the sequence into tokens available in
    the tokenizer vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The tokens are either words or subwords. Here for instance, ‚ÄúVRAM‚Äù wasn‚Äôt in
    the model vocabulary, so it‚Äôs been split in ‚ÄúV‚Äù, ‚ÄúRA‚Äù and ‚ÄúM‚Äù. To indicate those
    tokens are not separate words but parts of the same word, a double-hash prefix
    is added for ‚ÄúRA‚Äù and ‚ÄúM‚Äù:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: These tokens can then be converted into IDs which are understandable by the
    model. This can be done by directly feeding the sentence to the tokenizer, which
    leverages the Rust implementation of [ü§ó Tokenizers](https://github.com/huggingface/tokenizers)
    for peak performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The tokenizer returns a dictionary with all the arguments necessary for its
    corresponding model to work properly. The token indices are under the key `input_ids`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that the tokenizer automatically adds ‚Äúspecial tokens‚Äù (if the associated
    model relies on them) which are special IDs the model sometimes uses.
  prefs: []
  type: TYPE_NORMAL
- en: If we decode the previous sequence of ids,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: we will see
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: because this is the way a [BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel)
    is going to expect its inputs.
  prefs: []
  type: TYPE_NORMAL
- en: L
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: labels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The labels are an optional argument which can be passed in order for the model
    to compute the loss itself. These labels should be the expected prediction of
    the model: it will use the standard loss in order to compute the loss between
    its predictions and the expected value (the label).'
  prefs: []
  type: TYPE_NORMAL
- en: 'These labels are different according to the model head, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: For sequence classification models, ([BertForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertForSequenceClassification)),
    the model expects a tensor of dimension `(batch_size)` with each value of the
    batch corresponding to the expected label of the entire sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For token classification models, ([BertForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertForTokenClassification)),
    the model expects a tensor of dimension `(batch_size, seq_length)` with each value
    corresponding to the expected label of each individual token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For masked language modeling, ([BertForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertForMaskedLM)),
    the model expects a tensor of dimension `(batch_size, seq_length)` with each value
    corresponding to the expected label of each individual token: the labels being
    the token ID for the masked token, and values to be ignored for the rest (usually
    -100).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For sequence to sequence tasks, ([BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration),
    [MBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/mbart#transformers.MBartForConditionalGeneration)),
    the model expects a tensor of dimension `(batch_size, tgt_seq_length)` with each
    value corresponding to the target sequences associated with each input sequence.
    During training, both BART and T5 will make the appropriate `decoder_input_ids`
    and decoder attention masks internally. They usually do not need to be supplied.
    This does not apply to models leveraging the Encoder-Decoder framework.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For image classification models, ([ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)),
    the model expects a tensor of dimension `(batch_size)` with each value of the
    batch corresponding to the expected label of each individual image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For semantic segmentation models, ([SegformerForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation)),
    the model expects a tensor of dimension `(batch_size, height, width)` with each
    value of the batch corresponding to the expected label of each individual pixel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For object detection models, ([DetrForObjectDetection](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrForObjectDetection)),
    the model expects a list of dictionaries with a `class_labels` and `boxes` key
    where each value of the batch corresponds to the expected label and number of
    bounding boxes of each individual image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For automatic speech recognition models, ([Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)),
    the model expects a tensor of dimension `(batch_size, target_length)` with each
    value corresponding to the expected label of each individual token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each model‚Äôs labels may be different, so be sure to always check the documentation
    of each model for more information about their specific labels!
  prefs: []
  type: TYPE_NORMAL
- en: The base models ([BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel))
    do not accept labels, as these are the base transformer models, simply outputting
    features.
  prefs: []
  type: TYPE_NORMAL
- en: large language models (LLM)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A generic term that refers to transformer language models (GPT-3, BLOOM, OPT)
    that were trained on a large quantity of data. These models also tend to have
    a large number of learnable parameters (e.g. 175 billion for GPT-3).
  prefs: []
  type: TYPE_NORMAL
- en: M
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: masked language modeling (MLM)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A pretraining task where the model sees a corrupted version of the texts, usually
    done by masking some tokens randomly, and has to predict the original text.
  prefs: []
  type: TYPE_NORMAL
- en: multimodal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A task that combines texts with another kind of inputs (for instance images).
  prefs: []
  type: TYPE_NORMAL
- en: N
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Natural language generation (NLG)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All tasks related to generating text (for instance, [Write With Transformers](https://transformer.huggingface.co/),
    translation).
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing (NLP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A generic way to say ‚Äúdeal with texts‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language understanding (NLU)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All tasks related to understanding what is in a text (for instance classifying
    the whole text, individual words).
  prefs: []
  type: TYPE_NORMAL
- en: P
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A pipeline in ü§ó Transformers is an abstraction referring to a series of steps
    that are executed in a specific order to preprocess and transform data and return
    a prediction from a model. Some example stages found in a pipeline might be data
    preprocessing, feature extraction, and normalization.
  prefs: []
  type: TYPE_NORMAL
- en: For more details, see [Pipelines for inference](https://huggingface.co/docs/transformers/pipeline_tutorial).
  prefs: []
  type: TYPE_NORMAL
- en: PipelineParallel (PP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parallelism technique in which the model is split up vertically (layer-level)
    across multiple GPUs, so that only one or several layers of the model are placed
    on a single GPU. Each GPU processes in parallel different stages of the pipeline
    and working on a small chunk of the batch. Learn more about how PipelineParallel
    works [here](perf_train_gpu_many#from-naive-model-parallelism-to-pipeline-parallelism).
  prefs: []
  type: TYPE_NORMAL
- en: pixel values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A tensor of the numerical representations of an image that is passed to a model.
    The pixel values have a shape of [`batch_size`, `num_channels`, `height`, `width`],
    and are generated from an image processor.
  prefs: []
  type: TYPE_NORMAL
- en: pooling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An operation that reduces a matrix into a smaller matrix, either by taking the
    maximum or average of the pooled dimension(s). Pooling layers are commonly found
    between convolutional layers to downsample the feature representation.
  prefs: []
  type: TYPE_NORMAL
- en: position IDs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Contrary to RNNs that have the position of each token embedded within them,
    transformers are unaware of the position of each token. Therefore, the position
    IDs (`position_ids`) are used by the model to identify each token‚Äôs position in
    the list of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: They are an optional parameter. If no `position_ids` are passed to the model,
    the IDs are automatically created as absolute positional embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Absolute positional embeddings are selected in the range `[0, config.max_position_embeddings
    - 1]`. Some models use other types of positional embeddings, such as sinusoidal
    position embeddings or relative position embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The task of preparing raw data into a format that can be easily consumed by
    machine learning models. For example, text is typically preprocessed by tokenization.
    To gain a better idea of what preprocessing looks like for other input types,
    check out the [Preprocess](https://huggingface.co/docs/transformers/preprocessing)
    tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: pretrained model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A model that has been pretrained on some data (for instance all of Wikipedia).
    Pretraining methods involve a self-supervised objective, which can be reading
    the text and trying to predict the next word (see [causal language modeling](#causal-language-modeling))
    or masking some words and trying to predict them (see [masked language modeling](#masked-language-modeling-mlm)).
  prefs: []
  type: TYPE_NORMAL
- en: Speech and vision models have their own pretraining objectives. For example,
    Wav2Vec2 is a speech model pretrained on a contrastive task which requires the
    model to identify the ‚Äútrue‚Äù speech representation from a set of ‚Äúfalse‚Äù speech
    representations. On the other hand, BEiT is a vision model pretrained on a masked
    image modeling task which masks some of the image patches and requires the model
    to predict the masked patches (similar to the masked language modeling objective).
  prefs: []
  type: TYPE_NORMAL
- en: R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: recurrent neural network (RNN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A type of model that uses a loop over a layer to process texts.
  prefs: []
  type: TYPE_NORMAL
- en: representation learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A subfield of machine learning which focuses on learning meaningful representations
    of raw data. Some examples of representation learning techniques include word
    embeddings, autoencoders, and Generative Adversarial Networks (GANs).
  prefs: []
  type: TYPE_NORMAL
- en: S
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: sampling rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A measurement in hertz of the number of samples (the audio signal) taken per
    second. The sampling rate is a result of discretizing a continuous signal such
    as speech.
  prefs: []
  type: TYPE_NORMAL
- en: self-attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each element of the input finds out which other elements of the input they should
    attend to.
  prefs: []
  type: TYPE_NORMAL
- en: self-supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A category of machine learning techniques in which a model creates its own learning
    objective from unlabeled data. It differs from [unsupervised learning](#unsupervised-learning)
    and [supervised learning](#supervised-learning) in that the learning process is
    supervised, but not explicitly from the user.
  prefs: []
  type: TYPE_NORMAL
- en: One example of self-supervised learning is [masked language modeling](#masked-language-modeling-mlm),
    where a model is passed sentences with a proportion of its tokens removed and
    learns to predict the missing tokens.
  prefs: []
  type: TYPE_NORMAL
- en: semi-supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A broad category of machine learning training techniques that leverages a small
    amount of labeled data with a larger quantity of unlabeled data to improve the
    accuracy of a model, unlike [supervised learning](#supervised-learning) and [unsupervised
    learning](#unsupervised-learning).
  prefs: []
  type: TYPE_NORMAL
- en: An example of a semi-supervised learning approach is ‚Äúself-training‚Äù, in which
    a model is trained on labeled data, and then used to make predictions on the unlabeled
    data. The portion of the unlabeled data that the model predicts with the most
    confidence gets added to the labeled dataset and used to retrain the model.
  prefs: []
  type: TYPE_NORMAL
- en: sequence-to-sequence (seq2seq)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Models that generate a new sequence from an input, like translation models,
    or summarization models (such as [Bart](model_doc/bart) or [T5](model_doc/t5)).
  prefs: []
  type: TYPE_NORMAL
- en: Sharded DDP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another name for the foundational [ZeRO](#zero-redundancy-optimizer--zero-)
    concept as used by various other implementations of ZeRO.
  prefs: []
  type: TYPE_NORMAL
- en: stride
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [convolution](#convolution) or [pooling](#pooling), the stride refers to
    the distance the kernel is moved over a matrix. A stride of 1 means the kernel
    is moved one pixel over at a time, and a stride of 2 means the kernel is moved
    two pixels over at a time.
  prefs: []
  type: TYPE_NORMAL
- en: supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A form of model training that directly uses labeled data to correct and instruct
    model performance. Data is fed into the model being trained, and its predictions
    are compared to the known labels. The model updates its weights based on how incorrect
    its predictions were, and the process is repeated to optimize model performance.
  prefs: []
  type: TYPE_NORMAL
- en: T
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tensor Parallelism (TP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parallelism technique for training on multiple GPUs in which each tensor is
    split up into multiple chunks, so instead of having the whole tensor reside on
    a single GPU, each shard of the tensor resides on its designated GPU. Shards gets
    processed separately and in parallel on different GPUs and the results are synced
    at the end of the processing step. This is what is sometimes called horizontal
    parallelism, as the splitting happens on horizontal level. Learn more about Tensor
    Parallelism [here](perf_train_gpu_many#tensor-parallelism).
  prefs: []
  type: TYPE_NORMAL
- en: token
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A part of a sentence, usually a word, but can also be a subword (non-common
    words are often split in subwords) or a punctuation symbol.
  prefs: []
  type: TYPE_NORMAL
- en: token Type IDs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some models‚Äô purpose is to do classification on pairs of sentences or question
    answering.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/0u3ioSwev3s](https://www.youtube-nocookie.com/embed/0u3ioSwev3s)'
  prefs: []
  type: TYPE_NORMAL
- en: 'These require two different sequences to be joined in a single ‚Äúinput_ids‚Äù
    entry, which usually is performed with the help of special tokens, such as the
    classifier (`[CLS]`) and separator (`[SEP]`) tokens. For example, the BERT model
    builds its two sequence input as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use our tokenizer to automatically generate such a sentence by passing
    the two sequences to `tokenizer` as two arguments (and not a list, like before)
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'which will return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This is enough for some models to understand where one sequence ends and where
    another begins. However, other models, such as BERT, also deploy token type IDs
    (also called segment IDs). They are represented as a binary mask identifying the
    two types of sequence in the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The tokenizer returns this mask as the ‚Äútoken_type_ids‚Äù entry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The first sequence, the ‚Äúcontext‚Äù used for the question, has all its tokens
    represented by a `0`, whereas the second sequence, corresponding to the ‚Äúquestion‚Äù,
    has all its tokens represented by a `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Some models, like [XLNetModel](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetModel)
    use an additional token represented by a `2`.
  prefs: []
  type: TYPE_NORMAL
- en: transfer learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A technique that involves taking a pretrained model and adapting it to a dataset
    specific to your task. Instead of training a model from scratch, you can leverage
    knowledge obtained from an existing model as a starting point. This speeds up
    the learning process and reduces the amount of training data needed.
  prefs: []
  type: TYPE_NORMAL
- en: transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Self-attention based deep learning model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: U
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: unsupervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A form of model training in which data provided to the model is not labeled.
    Unsupervised learning techniques leverage statistical information of the data
    distribution to find patterns useful for the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Z
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Zero Redundancy Optimizer (ZeRO)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parallelism technique which performs sharding of the tensors somewhat similar
    to [TensorParallel](#tensor-parallelism-tp), except the whole tensor gets reconstructed
    in time for a forward or backward computation, therefore the model doesn‚Äôt need
    to be modified. This method also supports various offloading techniques to compensate
    for limited GPU memory. Learn more about ZeRO [here](perf_train_gpu_many#zero-data-parallelism).
  prefs: []
  type: TYPE_NORMAL
