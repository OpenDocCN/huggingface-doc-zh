# 稳定的unCLIP

> 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/stable_unclip](https://huggingface.co/docs/diffusers/api/pipelines/stable_unclip)

稳定的unCLIP检查点是从[稳定扩散2.1](./stable_diffusion/stable_diffusion_2)检查点微调而来，以对CLIP图像嵌入进行条件化。稳定的unCLIP仍然以文本嵌入为条件。鉴于这两种单独的条件，稳定的unCLIP可用于文本引导的图像变化。与unCLIP先验结合使用时，还可用于完整的文本到图像生成。

该论文的摘要是：

*像CLIP这样的对比模型已被证明能够学习到捕捉语义和风格的图像的稳健表示。为了利用这些表示进行图像生成，我们提出了一个两阶段模型：一个先验模型，根据文本标题生成一个CLIP图像嵌入，以及一个解码器，根据图像嵌入生成一个图像。我们展示了明确生成图像表示可以提高图像的多样性，同时最小化逼真度和标题相似度的损失。我们的解码器以图像表示为条件还可以产生保留其语义和风格的图像变化，同时变化那些不包含在图像表示中的非必要细节。此外，CLIP的联合嵌入空间使得可以通过语言引导图像操作。我们使用解码模型进行解码，并尝试使用自回归模型和扩散模型进行先验，发现后者在计算上更有效，并产生更高质量的样本。*

## 提示

稳定的unCLIP在推断过程中接受`noise_level`作为输入，该参数确定向图像嵌入中添加多少噪音。较高的`noise_level`会增加最终去噪图像的变化。默认情况下，我们不向图像嵌入中添加任何额外的噪音（`noise_level = 0`）。

### 文本到图像生成

稳定的unCLIP可以通过与KakaoBrain开源的DALL-E 2复制模型[Karlo](https://huggingface.co/kakaobrain/karlo-v1-alpha)的先前模型进行流水线处理，用于文本到图像的生成：

```py
import torch
from diffusers import UnCLIPScheduler, DDPMScheduler, StableUnCLIPPipeline
from diffusers.models import PriorTransformer
from transformers import CLIPTokenizer, CLIPTextModelWithProjection

prior_model_id = "kakaobrain/karlo-v1-alpha"
data_type = torch.float16
prior = PriorTransformer.from_pretrained(prior_model_id, subfolder="prior", torch_dtype=data_type)

prior_text_model_id = "openai/clip-vit-large-patch14"
prior_tokenizer = CLIPTokenizer.from_pretrained(prior_text_model_id)
prior_text_model = CLIPTextModelWithProjection.from_pretrained(prior_text_model_id, torch_dtype=data_type)
prior_scheduler = UnCLIPScheduler.from_pretrained(prior_model_id, subfolder="prior_scheduler")
prior_scheduler = DDPMScheduler.from_config(prior_scheduler.config)

stable_unclip_model_id = "stabilityai/stable-diffusion-2-1-unclip-small"

pipe = StableUnCLIPPipeline.from_pretrained(
    stable_unclip_model_id,
    torch_dtype=data_type,
    variant="fp16",
    prior_tokenizer=prior_tokenizer,
    prior_text_encoder=prior_text_model,
    prior=prior,
    prior_scheduler=prior_scheduler,
)

pipe = pipe.to("cuda")
wave_prompt = "dramatic wave, the Oceans roar, Strong wave spiral across the oceans as the waves unfurl into roaring crests; perfect wave form; perfect wave shape; dramatic wave shape; wave shape unbelievable; wave; wave shape spectacular"

image = pipe(prompt=wave_prompt).images[0]
image
```

对于文本到图像，我们使用`stabilityai/stable-diffusion-2-1-unclip-small`，因为它是在CLIP ViT-L/14嵌入上训练的，与Karlo模型先验相同。[stabilityai/stable-diffusion-2-1-unclip](https://hf.co/stabilityai/stable-diffusion-2-1-unclip)是在OpenCLIP ViT-H上训练的，因此我们不建议使用它。

### 文本引导的图像到图像变化

```py
from diffusers import StableUnCLIPImg2ImgPipeline
from diffusers.utils import load_image
import torch

pipe = StableUnCLIPImg2ImgPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-1-unclip", torch_dtype=torch.float16, variation="fp16"
)
pipe = pipe.to("cuda")

url = "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/stable_unclip/tarsila_do_amaral.png"
init_image = load_image(url)

images = pipe(init_image).images
images[0].save("variation_image.png")
```

可选地，您还可以向`pipe`传递提示，例如：

```py
prompt = "A fantasy landscape, trending on artstation"

image = pipe(init_image, prompt=prompt).images[0]
image
```

请务必查看调度器[指南](../../using-diffusers/schedulers)，以了解如何探索调度器速度和质量之间的权衡，并查看[跨流水线重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分，以了解如何有效地将相同组件加载到多个流水线中。

## StableUnCLIPPipeline

### `class diffusers.StableUnCLIPPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L61)

```py
( prior_tokenizer: CLIPTokenizer prior_text_encoder: CLIPTextModelWithProjection prior: PriorTransformer prior_scheduler: KarrasDiffusionSchedulers image_normalizer: StableUnCLIPImageNormalizer image_noising_scheduler: KarrasDiffusionSchedulers tokenizer: CLIPTokenizer text_encoder: CLIPTextModelWithProjection unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers vae: AutoencoderKL )
```

参数

+   `prior_tokenizer`（`CLIPTokenizer`）—一个`CLIPTokenizer`。

+   `prior_text_encoder`（`CLIPTextModelWithProjection`）—冻结的`CLIPTextModelWithProjection`文本编码器。

+   `prior`（[PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer））—用于从文本嵌入中近似图像嵌入的经典unCLIP先验。

+   `prior_scheduler`（`KarrasDiffusionSchedulers`）—先验去噪过程中使用的调度器。

+   `image_normalizer`（`StableUnCLIPImageNormalizer`）—用于在应用噪音之前对预测的图像嵌入进行归一化，并在应用噪音后对图像嵌入进行反归一化。

+   `image_noising_scheduler`（`KarrasDiffusionSchedulers`）—用于向预测的图像嵌入添加噪音的噪音计划。要添加的噪音量由`noise_level`确定。

+   `tokenizer`（`CLIPTokenizer`）— 一个`CLIPTokenizer`。

+   `text_encoder`（`CLIPTextModel`）— 冻结的`CLIPTextModel`文本编码器。

+   `unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)）— 用于去噪编码图像潜在特征的[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)。

+   `scheduler`（`KarrasDiffusionSchedulers`）— 与`unet`结合使用的调度器，用于去噪编码图像潜在特征。

+   `vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）— 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。

使用稳定的unCLIP进行文本到图像生成的管道。

这个模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取为所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

该管道还继承了以下加载方法：

+   [load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion) 用于加载文本反演嵌入

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) 用于加载LoRA权重

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) 用于保存LoRA权重

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L649)

```py
( prompt: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 20 guidance_scale: float = 10.0 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Optional = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None noise_level: int = 0 prior_num_inference_steps: int = 25 prior_guidance_scale: float = 4.0 prior_latents: Optional = None clip_skip: Optional = None ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）— 用于指导图像生成的提示。如果未定义，则需要传递`prompt_embeds`。

+   `height`（`int`，*可选*，默认为`self.unet.config.sample_size * self.vae_scale_factor`）— 生成图像的像素高度。

+   `width`（`int`，*可选*，默认为`self.unet.config.sample_size * self.vae_scale_factor`）— 生成图像的像素宽度。

+   `num_inference_steps`（`int`，*可选*，默认为20）— 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale`（`float`，*可选*，默认为10.0）— 更高的指导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用指导比例。

+   `negative_prompt`（`str`或`List[str]`，*可选*）— 用于指导图像生成中不包括什么的提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用指导（`guidance_scale < 1`）时被忽略。

+   `num_images_per_prompt`（`int`，*可选*，默认为1）— 每个提示生成的图像数量。

+   `eta`（`float`，*可选*，默认为0.0）— 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数eta（η）。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度器中被忽略。

+   `generator`（`torch.Generator`或`List[torch.Generator]`，*可选*）— 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents`（`torch.FloatTensor`，*可选*）— 从高斯分布中采样的预生成嘈杂潜在特征，用作图像生成的输入。可以用来使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜在特征张量。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，文本嵌入将从`prompt`输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds`将从`negative_prompt`输入参数生成。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的一个。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回一个[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)而不是一个普通的元组。

+   `callback` (`Callable`, *可选*) — 在推理过程中每`callback_steps`步调用的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *可选*, 默认为1) — 调用`callback`函数的频率。如果未指定，则在每一步时调用回调函数。

+   `cross_attention_kwargs` (`dict`, *可选*) — 如果指定，则将其作为kwargs字典传递给`AttentionProcessor`，如[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的那样。

+   `noise_level` (`int`, *可选*, 默认为`0`) — 添加到图像嵌入中的噪声量。较高的`noise_level`会增加最终无噪声图像的方差。有关更多详细信息，请参阅[StableUnCLIPPipeline.noise_image_embeddings()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.StableUnCLIPPipeline.noise_image_embeddings)。

+   `prior_num_inference_steps` (`int`, *可选*, 默认为`25`) — 先验去噪过程中的去噪步数。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `prior_guidance_scale` (`float`, *可选*, 默认为`4.0`) — 更高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。

+   `prior_latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成噪声潜变量，用作先验去噪过程中图像嵌入生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机`generator`进行采样生成潜变量张量。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要从CLIP中跳过的层数。值为1意味着将使用前一层的输出来计算提示嵌入。

返回

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) 或 `tuple`

如果`return_dict`为True，则为`~ pipeline_utils.ImagePipelineOutput`，否则为一个`tuple`。返回元组时，第一个元素是包含生成图像的列表。

用于生成的管道的调用函数。

示例:

```py
>>> import torch
>>> from diffusers import StableUnCLIPPipeline

>>> pipe = StableUnCLIPPipeline.from_pretrained(
...     "fusing/stable-unclip-2-1-l", torch_dtype=torch.float16
... )  # TODO update model path
>>> pipe = pipe.to("cuda")

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> images = pipe(prompt).images
>>> images[0].save("astronaut_horse.png")
```

#### `enable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

参数

+   `slice_size` (`str` 或 `int`, *可选*, 默认为`"auto"`) — 当为`"auto"`时，将输入减半到注意力头，因此注意力将分两步计算。如果为`"max"`，则通过一次只运行一个切片来节省最大内存量。如果提供了一个数字，则使用`attention_head_dim // slice_size`个切片。在这种情况下，`attention_head_dim`必须是`slice_size`的倍数。

启用切片注意力计算。启用此选项时，注意力模块将输入张量分割为切片以在几个步骤中计算注意力。对于多个注意力头，计算将按顺序在每个头上执行。这对于节省一些内存以换取一点速度降低很有用。

⚠️ 如果您已经在使用PyTorch 2.0或xFormers的`scaled_dot_product_attention`（SDPA），请不要启用注意力切片。这些注意力计算已经非常高效，因此您不需要启用此功能。如果您在SDPA或xFormers上启用了注意力切片，可能会导致严重减速！

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

#### `disable_attention_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

禁用切片注意力计算。如果之前调用了`enable_attention_slicing`，则注意力在一步中计算。

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L159)

```py
( )
```

启用切片VAE解码。启用此选项时，VAE将在几个步骤中将输入张量分割为切片以计算解码。这对于节省一些内存并允许更大的批量大小很有用。

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L167)

```py
( )
```

禁用切片VAE解码。如果之前启用了`enable_vae_slicing`，则此方法将回到在一步中计算解码。

#### `enable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

参数

+   `attention_op` (`Callable`, *可选*) — 用作`op`参数传递给xFormers的[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)函数的默认`None`操作符的覆盖。

启用[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。启用此选项时，您应该观察到较低的GPU内存使用量和潜在的推理加速。训练期间的加速不能保证。

⚠️ 当内存高效注意力和切片注意力都启用时，内存高效注意力优先。

示例：

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

禁用[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L302)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示设备 — (`torch.device`): torch设备

+   `num_images_per_prompt` (`int`) — 应该为每个提示生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用无分类器指导

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用来指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即，如果`guidance_scale`小于`1`，则忽略）。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数生成negative_prompt_embeds。

+   `lora_scale`（`float`，*可选*）- 如果加载了LoRA层，则将应用于文本编码器的所有LoRA层的LoRA比例。

+   `clip_skip`（`int`，*可选*）- 在计算提示嵌入时要从CLIP中跳过的层数。值为1意味着将使用前一层的输出来计算提示嵌入。

将提示编码为文本编码器的隐藏状态。

#### `noise_image_embeddings`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L603)

```py
( image_embeds: Tensor noise_level: int noise: Optional = None generator: Optional = None )
```

向图像嵌入添加噪音。噪音的量由`noise_level`输入控制。较高的`noise_level`会增加最终未去噪图像的方差。

噪音以两种方式应用：

1.  噪音计划直接应用于嵌入。

1.  将正弦时间嵌入向量附加到输出。

在两种情况下，噪音的量由相同的`noise_level`控制。

在应用噪音之前对嵌入进行归一化，应用噪音后取消归一化。

## StableUnCLIPImg2ImgPipeline

### `class diffusers.StableUnCLIPImg2ImgPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L72)

```py
( feature_extractor: CLIPImageProcessor image_encoder: CLIPVisionModelWithProjection image_normalizer: StableUnCLIPImageNormalizer image_noising_scheduler: KarrasDiffusionSchedulers tokenizer: CLIPTokenizer text_encoder: CLIPTextModel unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers vae: AutoencoderKL )
```

参数

+   `feature_extractor`（`CLIPImageProcessor`）- 用于在编码之前对图像进行特征提取的特征提取器。

+   `image_encoder`（`CLIPVisionModelWithProjection`）- 用于编码图像的CLIP视觉模型。

+   `image_normalizer`（`StableUnCLIPImageNormalizer`）- 用于在应用噪音之前对预测的图像嵌入进行归一化，并在应用噪音后取消归一化图像嵌入。

+   `image_noising_scheduler`（`KarrasDiffusionSchedulers`）- 用于向预测的图像嵌入添加噪音的噪音计划。要添加的噪音量由`noise_level`确定。

+   `tokenizer`（`~transformers.CLIPTokenizer`）- 一个[`~transformers.CLIPTokenizer`]。

+   `text_encoder`（[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)）- 冻结的[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)文本编码器。

+   `unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)）- 用于去噪编码图像潜在的[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)。

+   `scheduler`（`KarrasDiffusionSchedulers`）- 用于与`unet`结合使用以去噪编码图像潜在的调度器。

+   `vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）- 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示形式。

使用稳定的unCLIP进行文本引导的图像生成管道。

该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

该管道还继承了以下加载方法：

+   [load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion) 用于加载文本反演嵌入

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) 用于加载LoRA权重

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) 用于保存LoRA权重

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L623)

```py
( image: Union = None prompt: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 20 guidance_scale: float = 10 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Optional = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None noise_level: int = 0 image_embeds: Optional = None clip_skip: Optional = None ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt` (`str` or `List[str]`, *optional*) — 指导图像生成的提示或提示。如果未定义，则将使用`prompt_embeds`或将prompt初始化为`""`。

+   `image` (`torch.FloatTensor` or `PIL.Image.Image`) — 代表图像批次的`Image`或张量。图像被编码为其CLIP嵌入，`unet`以此为条件。图像*不*由`vae`编码，然后像标准稳定扩散文本引导图像变化过程中那样作为去噪过程中的潜变量使用。

+   `height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *optional*, defaults to 20) — 去噪步骤的数量。更多的去噪步骤通常会导致图像质量更高，但推断速度较慢。

+   `guidance_scale` (`float`, *optional*, defaults to 10.0) — 较高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 指导图像生成中不包含的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用引导时（`guidance_scale < 1`）忽略。

+   `num_images_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *optional*, defaults to 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数eta（η）。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中被忽略。

+   `generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中采样的预生成嘈杂潜变量，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机`generator`进行采样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（prompt加权）。如果未提供，则从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入（prompt加权）。如果未提供，`negative_prompt_embeds`将从`negative_prompt`输入参数生成。

+   `output_type` (`str`, *optional*, defaults to `"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的一个。

+   `return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)而不是普通元组。

+   `callback` (`Callable`, *optional*) — 在推断过程中每隔`callback_steps`步调用的函数。该函数使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *optional*, defaults to 1) — 调用`callback`函数的频率。如果未指定，则在每一步调用回调函数。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给`AttentionProcessor`的kwargs字典，如[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的那样。

+   `noise_level`（`int`，*可选*，默认为`0`）-要添加到图像嵌入中的噪声量。较高的`noise_level`会增加最终无噪声图像中的方差。有关更多详细信息，请参阅[StableUnCLIPPipeline.noise_image_embeddings()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.StableUnCLIPPipeline.noise_image_embeddings)。

+   `image_embeds`（`torch.FloatTensor`，*可选*）-预生成的CLIP嵌入，用于对`unet`进行条件处理。这些潜在变量在去噪过程中不被使用。如果要提供预生成的潜在变量，请将它们作为`latents`传递给`__call__`。

+   `clip_skip`（`int`，*可选*）-在计算提示嵌入时要从CLIP中跳过的层数。值为1意味着将使用预终层的输出来计算提示嵌入。

返回

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)或`tuple`

如果`return_dict`为True，则为`~ pipeline_utils.ImagePipelineOutput`，否则为`tuple`。返回元组时，第一个元素是包含生成图像的列表。

用于生成的管道的调用函数。

示例：

```py
>>> import requests
>>> import torch
>>> from PIL import Image
>>> from io import BytesIO

>>> from diffusers import StableUnCLIPImg2ImgPipeline

>>> pipe = StableUnCLIPImg2ImgPipeline.from_pretrained(
...     "fusing/stable-unclip-2-1-l-img2img", torch_dtype=torch.float16
... )  # TODO update model path
>>> pipe = pipe.to("cuda")

>>> url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"

>>> response = requests.get(url)
>>> init_image = Image.open(BytesIO(response.content)).convert("RGB")
>>> init_image = init_image.resize((768, 512))

>>> prompt = "A fantasy landscape, trending on artstation"

>>> images = pipe(prompt, init_image).images
>>> images[0].save("fantasy_landscape.png")
```

启用注意力切片

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

参数

+   `slice_size`（`str`或`int`，*可选*，默认为`"auto"`）-当为`"auto"`时，将输入减半到注意力头部，因此注意力将在两个步骤中计算。如果为`"max"`，将通过一次只运行一个切片来保存最大内存量。如果提供一个数字，则使用`attention_head_dim // slice_size`个切片。在这种情况下，`attention_head_dim`必须是`slice_size`的倍数。

启用切片注意力计算。启用此选项时，注意力模块将输入张量分割成片段，以便在多个步骤中计算注意力。对于多个注意力头，计算将按顺序在每个头上执行。这对于节省一些内存以换取一点速度降低很有用。

⚠️ 如果您已经在使用PyTorch 2.0或xFormers的`scaled_dot_product_attention`（SDPA），请不要启用注意力切片。这些注意力计算已经非常高效，因此您不需要启用此功能。如果您在SDPA或xFormers中启用了注意力切片，可能会导致严重减速！

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

禁用注意力切片

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

禁用切片注意力计算。如果之前调用了`enable_attention_slicing`，则注意力将在一步中计算。

启用VAE切片

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L160)

```py
( )
```

启用切片VAE解码。启用此选项时，VAE将将输入张量分割成片段，以便在多个步骤中计算解码。这对于节省一些内存并允许更大的批量大小很有用。

禁用VAE切片

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L168)

```py
( )
```

禁用切片VAE解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到在一步中计算解码。

启用XFormers内存高效注意力

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

参数

+   `attention_op`（`Callable`，*可选*）-覆盖默认的`None`运算符，用作xFormers的[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)函数的`op`参数。

启用来自 [xFormers](https://facebookresearch.github.io/xformers/) 的内存高效注意力。启用此选项时，您应该观察到较低的 GPU 内存使用量和潜在的推理加速。训练期间的加速不被保证。

⚠️ 当启用内存高效注意力和切片注意力时，内存高效注意力优先。

示例：

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

禁用来自 [xFormers](https://facebookresearch.github.io/xformers/) 的内存高效注意力。

#### `encode_prompt`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L264)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示 — (`torch.device`): torch 设备

+   `num_images_per_prompt` (`int`) — 应该为每个提示生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用无分类器指导

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 不指导图像生成的提示或提示。如果未定义，则必须传递 `negative_prompt_embeds`。在不使用指导时被忽略（即，如果 `guidance_scale` 小于 `1`，则被忽略）。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，*例如* 提示加权。如果未提供，文本嵌入将从 `prompt` 输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，*例如* 提示加权。如果未提供，将从 `negative_prompt` 输入参数生成 negative_prompt_embeds。

+   `lora_scale` (`float`, *可选*) — 将应用于文本编码器的所有 LoRA 层的 LoRA 比例。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要从 CLIP 跳过的层数。值为 1 表示将使用预最终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

#### `noise_image_embeddings`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L577)

```py
( image_embeds: Tensor noise_level: int noise: Optional = None generator: Optional = None )
```

向图像嵌入添加噪声。噪声的量由 `noise_level` 输入控制。较高的 `noise_level` 增加了最终未去噪图像中的方差。

噪声以两种方式应用：

1.  嵌入直接应用噪声计划。

1.  一组正弦时间嵌入被附加到输出中。

在这两种情况下，噪声的量由相同的 `noise_level` 控制。

在应用噪声之前对嵌入进行归一化，应用噪声后取消归一化。

## ImagePipelineOutput

### `class diffusers.ImagePipelineOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L116)

```py
( images: Union )
```

参数

+   `images` (`List[PIL.Image.Image]` 或 `np.ndarray`) — 长度为 `batch_size` 的去噪 PIL 图像列表或形状为 `(batch_size, height, width, num_channels)` 的 NumPy 数组。

图像管道的输出类。
