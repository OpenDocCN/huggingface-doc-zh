# 完全分片数据并行

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/fsdp](https://huggingface.co/docs/transformers/v4.37.2/en/fsdp)

[完全分片数据并行（FSDP）](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)是一种数据并行方法，它将模型的参数、梯度和优化器状态分片到可用GPU（也称为工作器或*rank*）的数量上。与[分布式数据并行（DDP）](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)不同，FSDP减少了内存使用，因为模型在每个GPU上都有副本。这提高了GPU内存效率，并允许您在较少的GPU上训练更大的模型。FSDP与Accelerate集成，Accelerate是一个用于轻松管理分布式环境中训练的库，这意味着可以从[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类中使用它。

在开始之前，请确保已安装Accelerate，并且至少安装了PyTorch 2.1.0或更新版本。

```py
pip install accelerate
```

## FSDP配置

首先，运行[`accelerate config`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config)命令，为您的训练环境创建一个配置文件。Accelerate使用此配置文件根据您在`accelerate config`中选择的训练选项自动设置正确的训练环境。

```py
accelerate config
```

当您运行`accelerate config`时，您将被提示一系列选项来配置您的训练环境。本节涵盖了一些最重要的FSDP选项。要了解更多关于其他可用的FSDP选项，请查看[fsdp_config](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.fsdp_config)参数。

### 分片策略

FSDP提供了许多分片策略可供选择：

+   `FULL_SHARD` - 在工作器之间对模型参数、梯度和优化器状态进行分片；选择`1`作为此选项

+   `SHARD_GRAD_OP`- 在工作器之间对梯度和优化器状态进行分片；选择`2`作为此选项

+   `NO_SHARD` - 不对任何内容进行分片（相当于DDP）；选择`3`作为此选项

+   `HYBRID_SHARD` - 在每个工作器内对模型参数、梯度和优化器状态进行分片，每个工作器也有完整副本；选择`4`作为此选项

+   `HYBRID_SHARD_ZERO2` - 在每个工作器内对梯度和优化器状态进行分片，每个工作器也有完整副本；选择`5`作为此选项

这是通过`fsdp_sharding_strategy`标志启用的。

### CPU卸载

当参数和梯度不在使用时，您还可以将它们卸载到CPU以节省更多GPU内存，并帮助您适应大型模型，即使FSDP可能不足。这可以通过在运行`accelerate config`时设置`fsdp_offload_params: true`来启用。

### 包装策略

FSDP通过包装网络中的每一层来应用。包装通常以嵌套方式应用，其中完整权重在每次前向传递后被丢弃，以节省内存供下一层使用。*自动包装*策略是实现这一点的最简单方法，您无需更改任何代码。您应该选择`fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP`来包装一个Transformer层，并选择`fsdp_transformer_layer_cls_to_wrap`来指定要包装的层（例如`BertLayer`）。

否则，您可以选择基于大小的包装策略，如果某一层的参数超过一定数量，则应用FSDP。这可以通过将`fsdp_wrap_policy: SIZE_BASED_WRAP`和`min_num_param`设置为所需的大小阈值来启用。

### 检查点

中间检查点应该使用`fsdp_state_dict_type: SHARDED_STATE_DICT`保存，因为在rank 0上使用CPU卸载保存完整状态字典需要很长时间，并且经常由于广播期间的无限挂起而导致`NCCL Timeout`错误。您可以使用[load_state](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.load_state)`方法恢复训练。

```py
# directory containing checkpoints
accelerator.load_state("ckpt")
```

然而，当训练结束时，您希望保存完整状态字典，因为分片状态字典仅与FSDP兼容。

```py
if trainer.is_fsdp_enabled:
    trainer.accelerator.state.fsdp_plugin.set_state_dict_type("FULL_STATE_DICT")

trainer.save_model(script_args.output_dir)
```

### TPU

[PyTorch XLA](https://pytorch.org/xla/release/2.1/index.html)支持TPU的FSDP训练，可以通过修改`accelerate config`生成的FSDP配置文件来启用。除了上面指定的分片策略和包装选项之外，您还可以向文件中添加下面显示的参数。

```py
xla: True # must be set to True to enable PyTorch/XLA
xla_fsdp_settings: # XLA-specific FSDP parameters
xla_fsdp_grad_ckpt: True # use gradient checkpointing
```

[`xla_fsdp_settings`](https://github.com/pytorch/xla/blob/2e6e183e0724818f137c8135b34ef273dea33318/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py#L128)允许您配置FSDP的额外XLA特定参数。

## 启动训练

一个示例的FSDP配置文件可能如下所示：

```py
compute_environment: LOCAL_MACHINE
debug: false
distributed_type: FSDP
downcast_bf16: 'no'
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch_policy: BACKWARD_PRE
  fsdp_cpu_ram_efficient_loading: true
  fsdp_forward_prefetch: false
  fsdp_offload_params: true
  fsdp_sharding_strategy: 1
  fsdp_state_dict_type: SHARDED_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_transformer_layer_cls_to_wrap: BertLayer
  fsdp_use_orig_params: true
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 2
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

启动训练，请运行[`accelerate launch`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-launch)命令，它将自动使用您之前使用`accelerate config`创建的配置文件。

```py
accelerate launch my-trainer-script.py
```

```py
accelerate launch --fsdp="full shard" --fsdp_config="path/to/fsdp_config/ my-trainer-script.py
```

## 下一步

FSDP可以是训练非常大模型的强大工具，如果您有多个GPU或TPU可用。通过对模型参数、优化器和梯度状态进行分片，甚至在它们不活动时将它们卸载到CPU上，FSDP可以减少大规模训练的高成本。如果您有兴趣了解更多，以下内容可能有所帮助：

+   按照更详细的[FSDP](https://huggingface.co/docs/accelerate/usage_guides/fsdp)加速指南进行操作。

+   阅读[介绍PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)博文。

+   阅读[使用FSDP在Cloud TPU上扩展PyTorch模型](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/)博文。
