["```py\n>>> from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n\n>>> config_encoder = BertConfig()\n>>> config_decoder = BertConfig()\n\n>>> config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n>>> model = EncoderDecoderModel(config=config)\n```", "```py\n>>> from transformers import EncoderDecoderModel, BertTokenizer\n\n>>> tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")\n```", "```py\n>>> from transformers import AutoTokenizer, EncoderDecoderModel\n\n>>> # load a fine-tuned seq2seq model and corresponding tokenizer\n>>> model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n\n>>> # let's perform inference on a long piece of text\n>>> ARTICLE_TO_SUMMARIZE = (\n...     \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n...     \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n...     \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n... )\n>>> input_ids = tokenizer(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\").input_ids\n\n>>> # autoregressively generate summary (uses greedy decoding by default)\n>>> generated_ids = model.generate(input_ids)\n>>> generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> print(generated_text)\nnearly 800 thousand customers were affected by the shutoffs. the aim is to reduce the risk of wildfires. nearly 800, 000 customers were expected to be affected by high winds amid dry conditions. pg & e said it scheduled the blackouts to last through at least midday tomorrow.\n```", "```py\n>>> # a workaround to load from pytorch checkpoint\n>>> from transformers import EncoderDecoderModel, TFEncoderDecoderModel\n\n>>> _model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\")\n\n>>> _model.encoder.save_pretrained(\"./encoder\")\n>>> _model.decoder.save_pretrained(\"./decoder\")\n\n>>> model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(\n...     \"./encoder\", \"./decoder\", encoder_from_pt=True, decoder_from_pt=True\n... )\n>>> # This is only for copying some specific attributes of this particular model.\n>>> model.config = _model.config\n```", "```py\n>>> from transformers import BertTokenizer, EncoderDecoderModel\n\n>>> tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")\n\n>>> model.config.decoder_start_token_id = tokenizer.cls_token_id\n>>> model.config.pad_token_id = tokenizer.pad_token_id\n\n>>> input_ids = tokenizer(\n...     \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side.During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was  finished in 1930\\. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft).Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\",\n...     return_tensors=\"pt\",\n... ).input_ids\n\n>>> labels = tokenizer(\n...     \"the eiffel tower surpassed the washington monument to become the tallest structure in the world. it was the first structure to reach a height of 300 metres in paris in 1930\\. it is now taller than the chrysler building by 5\\. 2 metres ( 17 ft ) and is the second tallest free - standing structure in paris.\",\n...     return_tensors=\"pt\",\n... ).input_ids\n\n>>> # the forward function automatically creates the correct decoder_input_ids\n>>> loss = model(input_ids=input_ids, labels=labels).loss\n```", "```py\n( **kwargs )\n```", "```py\n>>> from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n\n>>> # Initializing a BERT bert-base-uncased style configuration\n>>> config_encoder = BertConfig()\n>>> config_decoder = BertConfig()\n\n>>> config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n\n>>> # Initializing a Bert2Bert model (with random weights) from the bert-base-uncased style configurations\n>>> model = EncoderDecoderModel(config=config)\n\n>>> # Accessing the model configuration\n>>> config_encoder = model.config.encoder\n>>> config_decoder = model.config.decoder\n>>> # set decoder config to causal lm\n>>> config_decoder.is_decoder = True\n>>> config_decoder.add_cross_attention = True\n\n>>> # Saving the model, including its configuration\n>>> model.save_pretrained(\"my-model\")\n\n>>> # loading model and config from pretrained folder\n>>> encoder_decoder_config = EncoderDecoderConfig.from_pretrained(\"my-model\")\n>>> model = EncoderDecoderModel.from_pretrained(\"my-model\", config=encoder_decoder_config)\n```", "```py\n( encoder_config: PretrainedConfig decoder_config: PretrainedConfig **kwargs ) \u2192 export const metadata = 'undefined';EncoderDecoderConfig\n```", "```py\n( config: Optional = None encoder: Optional = None decoder: Optional = None )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None past_key_values: Tuple = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import EncoderDecoderModel, BertTokenizer\n>>> import torch\n\n>>> tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n...     \"bert-base-uncased\", \"bert-base-uncased\"\n... )  # initialize Bert2Bert from pre-trained checkpoints\n\n>>> # training\n>>> model.config.decoder_start_token_id = tokenizer.cls_token_id\n>>> model.config.pad_token_id = tokenizer.pad_token_id\n>>> model.config.vocab_size = model.config.decoder.vocab_size\n\n>>> input_ids = tokenizer(\"This is a really long text\", return_tensors=\"pt\").input_ids\n>>> labels = tokenizer(\"This is the corresponding summary\", return_tensors=\"pt\").input_ids\n>>> outputs = model(input_ids=input_ids, labels=labels)\n>>> loss, logits = outputs.loss, outputs.logits\n\n>>> # save and load from pretrained\n>>> model.save_pretrained(\"bert2bert\")\n>>> model = EncoderDecoderModel.from_pretrained(\"bert2bert\")\n\n>>> # generation\n>>> generated = model.generate(input_ids)\n```", "```py\n( encoder_pretrained_model_name_or_path: str = None decoder_pretrained_model_name_or_path: str = None *model_args **kwargs )\n```", "```py\n>>> from transformers import EncoderDecoderModel\n\n>>> # initialize a bert2bert from two pretrained BERT models. Note that the cross-attention layers will be randomly initialized\n>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")\n>>> # saving model after fine-tuning\n>>> model.save_pretrained(\"./bert2bert\")\n>>> # load fine-tuned model\n>>> model = EncoderDecoderModel.from_pretrained(\"./bert2bert\")\n```", "```py\n( config: Optional[PretrainedConfig] = None encoder: Optional[TFPreTrainedModel] = None decoder: Optional[TFPreTrainedModel] = None )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: np.ndarray | tf.Tensor | None = None past_key_values: Tuple[Tuple[tf.Tensor]] | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None labels: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False **kwargs ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqLMOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import TFEncoderDecoderModel, BertTokenizer\n\n>>> # initialize a bert2gpt2 from a pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized\n>>> model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-cased\", \"gpt2\")\n\n>>> tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n\n>>> # forward\n>>> input_ids = tokenizer.encode(\n...     \"Hello, my dog is cute\", add_special_tokens=True, return_tensors=\"tf\"\n... )  # Batch size 1\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)\n\n>>> # training\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, labels=input_ids)\n>>> loss, logits = outputs.loss, outputs.logits\n\n>>> # save and load from pretrained\n>>> model.save_pretrained(\"bert2gpt2\")\n>>> model = TFEncoderDecoderModel.from_pretrained(\"bert2gpt2\")\n\n>>> # generation\n>>> generated = model.generate(input_ids, decoder_start_token_id=model.config.decoder.bos_token_id)\n```", "```py\n( encoder_pretrained_model_name_or_path: str = None decoder_pretrained_model_name_or_path: str = None *model_args **kwargs )\n```", "```py\n>>> from transformers import TFEncoderDecoderModel\n\n>>> # initialize a bert2gpt2 from two pretrained BERT models. Note that the cross-attention layers will be randomly initialized\n>>> model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"gpt2\")\n>>> # saving model after fine-tuning\n>>> model.save_pretrained(\"./bert2gpt2\")\n>>> # load fine-tuned model\n>>> model = TFEncoderDecoderModel.from_pretrained(\"./bert2gpt2\")\n```", "```py\n( config: EncoderDecoderConfig input_shape: Optional = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import FlaxEncoderDecoderModel, BertTokenizer, GPT2Tokenizer\n\n>>> # load a fine-tuned bert2gpt2 model\n>>> model = FlaxEncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2gpt2-cnn_dailymail-fp16\")\n>>> # load input & output tokenizer\n>>> tokenizer_input = BertTokenizer.from_pretrained(\"bert-base-cased\")\n>>> tokenizer_output = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n>>> article = '''Sigma Alpha Epsilon is under fire for a video showing party-bound fraternity members\n>>> singing a racist chant. SAE's national chapter suspended the students,\n>>> but University of Oklahoma President David Boren took it a step further,\n>>> saying the university's affiliation with the fraternity is permanently done.'''\n\n>>> input_ids = tokenizer_input(article, add_special_tokens=True, return_tensors=\"np\").input_ids\n\n>>> # use GPT2's eos_token as the pad as well as eos token\n>>> model.config.eos_token_id = model.config.decoder.eos_token_id\n>>> model.config.pad_token_id = model.config.eos_token_id\n\n>>> sequences = model.generate(input_ids, num_beams=4, max_length=12).sequences\n\n>>> summary = tokenizer_output.batch_decode(sequences, skip_special_tokens=True)[0]\n>>> assert summary == \"SAS Alpha Epsilon suspended Sigma Alpha Epsilon members\"\n```", "```py\n( encoder_pretrained_model_name_or_path: Union = None decoder_pretrained_model_name_or_path: Union = None *model_args **kwargs )\n```", "```py\n>>> from transformers import FlaxEncoderDecoderModel\n\n>>> # initialize a bert2gpt2 from pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized\n>>> model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-cased\", \"gpt2\")\n>>> # saving model after fine-tuning\n>>> model.save_pretrained(\"./bert2gpt2\")\n>>> # load fine-tuned model\n>>> model = FlaxEncoderDecoderModel.from_pretrained(\"./bert2gpt2\")\n```"]