# åº·å®šæ–¯åŸº

> åŸæ–‡é“¾æ¥ï¼š[`huggingface.co/docs/diffusers/using-diffusers/kandinsky`](https://huggingface.co/docs/diffusers/using-diffusers/kandinsky)

åº·å®šæ–¯åŸºæ¨¡å‹æ˜¯ä¸€ç³»åˆ—å¤šè¯­è¨€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚åº·å®šæ–¯åŸº 2.0 æ¨¡å‹ä½¿ç”¨ä¸¤ä¸ªå¤šè¯­è¨€æ–‡æœ¬ç¼–ç å™¨ï¼Œå¹¶å°†è¿™äº›ç»“æœè¿æ¥åˆ° UNet ä¸­ã€‚

åº·å®šæ–¯åŸº 2.1 æ”¹å˜äº†æ¶æ„ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå›¾åƒå…ˆå‰æ¨¡å‹ï¼ˆ[`CLIP`](https://huggingface.co/docs/transformers/model_doc/clip)ï¼‰ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒåµŒå…¥ä¹‹é—´çš„æ˜ å°„ã€‚è¯¥æ˜ å°„æä¾›æ›´å¥½çš„æ–‡æœ¬-å›¾åƒå¯¹é½ï¼Œå¹¶åœ¨è®­ç»ƒæœŸé—´ä¸æ–‡æœ¬åµŒå…¥ä¸€èµ·ä½¿ç”¨ï¼Œä»è€Œäº§ç”Ÿæ›´é«˜è´¨é‡çš„ç»“æœã€‚æœ€åï¼Œåº·å®šæ–¯åŸº 2.1 ä½¿ç”¨äº†ä¸€ä¸ª[è°ƒåˆ¶é‡åŒ–å‘é‡ï¼ˆMoVQï¼‰](https://huggingface.co/papers/2209.09002)è§£ç å™¨ï¼Œå®ƒæ·»åŠ äº†ä¸€ä¸ªç©ºé—´æ¡ä»¶å½’ä¸€åŒ–å±‚ä»¥å¢åŠ ç…§ç‰‡é€¼çœŸåº¦ï¼Œå°†æ½œåœ¨ç‰¹å¾è§£ç ä¸ºå›¾åƒã€‚

åº·å®šæ–¯åŸº 2.2 é€šè¿‡ç”¨æ›´å¤§çš„ CLIP-ViT-G æ¨¡å‹æ›¿æ¢å›¾åƒå…ˆå‰æ¨¡å‹çš„å›¾åƒç¼–ç å™¨æ¥æ”¹è¿›å…ˆå‰æ¨¡å‹ã€‚å›¾åƒå…ˆå‰æ¨¡å‹è¿˜ç»è¿‡é‡æ–°è®­ç»ƒï¼Œç”¨ä¸åŒåˆ†è¾¨ç‡å’Œé•¿å®½æ¯”çš„å›¾åƒç”Ÿæˆæ›´é«˜åˆ†è¾¨ç‡å’Œä¸åŒå°ºå¯¸çš„å›¾åƒã€‚

åº·å®šæ–¯åŸº 3 ç®€åŒ–äº†æ¶æ„ï¼Œå¹¶æ‘†è„±äº†æ¶‰åŠå…ˆå‰æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„ä¸¤é˜¶æ®µç”Ÿæˆè¿‡ç¨‹ã€‚ç›¸åï¼Œåº·å®šæ–¯åŸº 3 ä½¿ç”¨[Flan-UL2](https://huggingface.co/google/flan-ul2)æ¥ç¼–ç æ–‡æœ¬ï¼Œä¸€ä¸ªå¸¦æœ‰[BigGan-deep](https://hf.co/papers/1809.11096)å—çš„ UNetï¼Œä»¥åŠ[Sber-MoVQGAN](https://github.com/ai-forever/MoVQGAN)æ¥å°†æ½œåœ¨ç‰¹å¾è§£ç ä¸ºå›¾åƒã€‚é€šè¿‡ä½¿ç”¨æ›´å¤§çš„æ–‡æœ¬ç¼–ç å™¨å’Œ UNetï¼Œä¸»è¦å®ç°äº†æ–‡æœ¬ç†è§£å’Œç”Ÿæˆå›¾åƒè´¨é‡çš„æé«˜ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨åº·å®šæ–¯åŸºæ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€ä¿®å¤ã€æ’å€¼ç­‰æ“ä½œã€‚

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹åº“ï¼š

```py
# uncomment to install the necessary libraries in Colab
#!pip install -q diffusers transformers accelerate
```

åº·å®šæ–¯åŸº 2.1 å’Œ 2.2 çš„ç”¨æ³•éå¸¸ç›¸ä¼¼ï¼å”¯ä¸€çš„åŒºåˆ«æ˜¯åº·å®šæ–¯åŸº 2.2 åœ¨è§£ç æ½œåœ¨ç‰¹å¾æ—¶ä¸æ¥å—`prompt`ä½œä¸ºè¾“å…¥ã€‚ç›¸åï¼Œåº·å®šæ–¯åŸº 2.2 åœ¨è§£ç æ—¶åªæ¥å—`image_embeds`ã€‚

åº·å®šæ–¯åŸº 3 å…·æœ‰æ›´ç®€æ´çš„æ¶æ„ï¼Œä¸éœ€è¦å…ˆå‰çš„æ¨¡å‹ã€‚è¿™æ„å‘³ç€å®ƒçš„ç”¨æ³•ä¸å…¶ä»–æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚ç¨³å®šæ‰©æ•£ XLï¼‰ç›¸åŒã€‚

## æ–‡æœ¬åˆ°å›¾åƒ

è¦ä½¿ç”¨åº·å®šæ–¯åŸºæ¨¡å‹æ‰§è¡Œä»»ä½•ä»»åŠ¡ï¼Œæ‚¨å§‹ç»ˆè¦å…ˆè®¾ç½®å…ˆå‰æµç¨‹ä»¥å¯¹æç¤ºè¿›è¡Œç¼–ç å¹¶ç”Ÿæˆå›¾åƒåµŒå…¥ã€‚å…ˆå‰æµç¨‹è¿˜ä¼šç”Ÿæˆä¸è´Ÿæç¤º`""`å¯¹åº”çš„`negative_image_embeds`ã€‚ä¸ºäº†è·å¾—æ›´å¥½çš„ç»“æœï¼Œæ‚¨å¯ä»¥å‘å…ˆå‰æµç¨‹ä¼ é€’å®é™…çš„`negative_prompt`ï¼Œä½†è¿™ä¼šä½¿å…ˆå‰æµç¨‹çš„æœ‰æ•ˆæ‰¹é‡å¤§å°å¢åŠ  2 å€ã€‚

åº·å®šæ–¯åŸº 2.1 åº·å®šæ–¯åŸº 2.2 åº·å®šæ–¯åŸº 3

```py
from diffusers import KandinskyPriorPipeline, KandinskyPipeline
import torch

prior_pipeline = KandinskyPriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-1-prior", torch_dtype=torch.float16).to("cuda")
pipeline = KandinskyPipeline.from_pretrained("kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16).to("cuda")

prompt = "A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting"
negative_prompt = "low quality, bad quality" # optional to include a negative prompt, but results are usually better
image_embeds, negative_image_embeds = prior_pipeline(prompt, negative_prompt, guidance_scale=1.0).to_tuple()
```

ç°åœ¨å°†æ‰€æœ‰æç¤ºå’ŒåµŒå…¥ä¼ é€’ç»™ KandinskyPipeline ä»¥ç”Ÿæˆå›¾åƒï¼š

```py
image = pipeline(prompt, image_embeds=image_embeds, negative_prompt=negative_prompt, negative_image_embeds=negative_image_embeds, height=768, width=768).images[0]
image
```

![](img/7a49f6e1db66e1eebcfa846868181899.png)

ğŸ¤—æ‰©æ•£å™¨è¿˜æä¾›äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„ APIï¼Œå…¶ä¸­åŒ…æ‹¬ KandinskyCombinedPipeline å’Œ KandinskyV22CombinedPipelineï¼Œè¿™æ„å‘³ç€æ‚¨ä¸å¿…å•ç‹¬åŠ è½½å…ˆå‰çš„æ¨¡å‹å’Œæ–‡æœ¬åˆ°å›¾åƒçš„æµç¨‹ã€‚åˆå¹¶æµç¨‹ä¼šè‡ªåŠ¨åŠ è½½å…ˆå‰æ¨¡å‹å’Œè§£ç å™¨ã€‚å¦‚æœéœ€è¦ï¼Œæ‚¨ä»ç„¶å¯ä»¥é€šè¿‡`prior_guidance_scale`å’Œ`prior_num_inference_steps`å‚æ•°ä¸ºå…ˆå‰æµç¨‹è®¾ç½®ä¸åŒçš„å€¼ã€‚

ä½¿ç”¨ AutoPipelineForText2Image æ¥è‡ªåŠ¨è°ƒç”¨åº•å±‚çš„åˆå¹¶æµç¨‹ï¼š

åº·å®šæ–¯åŸº 2.1 åº·å®šæ–¯åŸº 2.2

```py
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained("kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16)
pipeline.enable_model_cpu_offload()

prompt = "A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting"
negative_prompt = "low quality, bad quality"

image = pipeline(prompt=prompt, negative_prompt=negative_prompt, prior_guidance_scale=1.0, guidance_scale=4.0, height=768, width=768).images[0]
image
```

## å›¾åƒåˆ°å›¾åƒ

å¯¹äºå›¾åƒåˆ°å›¾åƒï¼Œå°†åˆå§‹å›¾åƒå’Œæ–‡æœ¬æç¤ºä¼ é€’ç»™ç®¡é“ä»¥å¯¹å›¾åƒè¿›è¡Œæ¡ä»¶è®¾ç½®ã€‚é¦–å…ˆåŠ è½½å…ˆå‰ç®¡é“ï¼š

Kandinsky 2.1Kandinsky 2.2Kandinsky 3

```py
import torch
from diffusers import KandinskyImg2ImgPipeline, KandinskyPriorPipeline

prior_pipeline = KandinskyPriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-1-prior", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
pipeline = KandinskyImg2ImgPipeline.from_pretrained("kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
```

ä¸‹è½½ä¸€ä¸ªå›¾åƒè¿›è¡Œæ¡ä»¶è®¾ç½®ï¼š

```py
from diffusers.utils import load_image

# download image
url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
original_image = load_image(url)
original_image = original_image.resize((768, 512))
```

![](img/f2a11ed9e52e5fabb2124e5fe7bba075.png)

ä½¿ç”¨å…ˆå‰ç®¡é“ç”Ÿæˆ`image_embeds`å’Œ`negative_image_embeds`ï¼š

```py
prompt = "A fantasy landscape, Cinematic lighting"
negative_prompt = "low quality, bad quality"

image_embeds, negative_image_embeds = prior_pipeline(prompt, negative_prompt).to_tuple()
```

ç°åœ¨å°†åŸå§‹å›¾åƒå’Œæ‰€æœ‰æç¤ºå’ŒåµŒå…¥ä¼ é€’ç»™ç®¡é“ä»¥ç”Ÿæˆå›¾åƒï¼š

Kandinsky 2.1Kandinsky 2.2Kandinsky 3

```py
from diffusers.utils import make_image_grid

image = pipeline(prompt, negative_prompt=negative_prompt, image=original_image, image_embeds=image_embeds, negative_image_embeds=negative_image_embeds, height=768, width=768, strength=0.3).images[0]
make_image_grid([original_image.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)
```

![](img/8554b2d04f4b98afeb9692a00447ba5d.png)

ğŸ¤— Diffusers è¿˜æä¾›äº†ç«¯åˆ°ç«¯çš„ APIï¼Œä½¿ç”¨ KandinskyImg2ImgCombinedPipeline å’Œ KandinskyV22Img2ImgCombinedPipelineï¼Œè¿™æ„å‘³ç€æ‚¨ä¸å¿…å•ç‹¬åŠ è½½å…ˆå‰å’Œå›¾åƒåˆ°å›¾åƒç®¡é“ã€‚ç»„åˆç®¡é“ä¼šè‡ªåŠ¨åŠ è½½å…ˆå‰æ¨¡å‹å’Œè§£ç å™¨ã€‚å¦‚æœéœ€è¦ï¼Œä»ç„¶å¯ä»¥é€šè¿‡`prior_guidance_scale`å’Œ`prior_num_inference_steps`å‚æ•°ä¸ºå…ˆå‰ç®¡é“è®¾ç½®ä¸åŒçš„å€¼ã€‚

ä½¿ç”¨ AutoPipelineForImage2Image æ¥åœ¨å†…éƒ¨è‡ªåŠ¨è°ƒç”¨ç»„åˆç®¡é“ï¼š

Kandinsky 2.1Kandinsky 2.2

```py
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image
import torch

pipeline = AutoPipelineForImage2Image.from_pretrained("kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16, use_safetensors=True)
pipeline.enable_model_cpu_offload()

prompt = "A fantasy landscape, Cinematic lighting"
negative_prompt = "low quality, bad quality"

url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
original_image = load_image(url)

original_image.thumbnail((768, 768))

image = pipeline(prompt=prompt, negative_prompt=negative_prompt, image=original_image, strength=0.3).images[0]
make_image_grid([original_image.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)
```

## ä¿®å¤

âš ï¸ Kandinsky æ¨¡å‹ç°åœ¨ä½¿ç”¨â¬œï¸ **ç™½è‰²åƒç´ **æ¥ä»£è¡¨é®ç½©åŒºåŸŸï¼Œè€Œä¸æ˜¯é»‘è‰²åƒç´ ã€‚å¦‚æœæ‚¨æ­£åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨ KandinskyInpaintPipelineï¼Œæ‚¨éœ€è¦å°†é®ç½©æ›´æ”¹ä¸ºä½¿ç”¨ç™½è‰²åƒç´ ï¼š

```py
# For PIL input
import PIL.ImageOps
mask = PIL.ImageOps.invert(mask)

# For PyTorch and NumPy input
mask = 1 - mask
```

å¯¹äºä¿®å¤ï¼Œæ‚¨å°†éœ€è¦åŸå§‹å›¾åƒã€åŸå§‹å›¾åƒä¸­è¦æ›¿æ¢çš„åŒºåŸŸçš„é®ç½©ï¼Œä»¥åŠè¦ä¿®å¤çš„å†…å®¹çš„æ–‡æœ¬æç¤ºã€‚åŠ è½½å…ˆå‰ç®¡é“ï¼š

Kandinsky 2.1Kandinsky 2.2

```py
from diffusers import KandinskyInpaintPipeline, KandinskyPriorPipeline
from diffusers.utils import load_image, make_image_grid
import torch
import numpy as np
from PIL import Image

prior_pipeline = KandinskyPriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-1-prior", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
pipeline = KandinskyInpaintPipeline.from_pretrained("kandinsky-community/kandinsky-2-1-inpaint", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
```

åŠ è½½åˆå§‹å›¾åƒå¹¶åˆ›å»ºé®ç½©ï¼š

```py
init_image = load_image("https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png")
mask = np.zeros((768, 768), dtype=np.float32)
# mask area above cat's head
mask[:250, 250:-250] = 1
```

ä½¿ç”¨å…ˆå‰ç®¡é“ç”ŸæˆåµŒå…¥ï¼š

```py
prompt = "a hat"
prior_output = prior_pipeline(prompt)
```

ç°åœ¨å°†åˆå§‹å›¾åƒã€é®ç½©ã€æç¤ºå’ŒåµŒå…¥ä¼ é€’ç»™ç®¡é“ä»¥ç”Ÿæˆå›¾åƒï¼š

Kandinsky 2.1Kandinsky 2.2

```py
output_image = pipeline(prompt, image=init_image, mask_image=mask, **prior_output, height=768, width=768, num_inference_steps=150).images[0]
mask = Image.fromarray((mask*255).astype('uint8'), 'L')
make_image_grid([init_image, mask, output_image], rows=1, cols=3)
```

![](img/06d9ede591c4c816c0afb1ac1756de88.png)

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ç«¯åˆ°ç«¯çš„ KandinskyInpaintCombinedPipeline å’Œ KandinskyV22InpaintCombinedPipeline æ¥åœ¨å†…éƒ¨åŒæ—¶è°ƒç”¨å…ˆå‰å’Œè§£ç å™¨ç®¡é“ã€‚ä½¿ç”¨ AutoPipelineForInpainting æ¥å®ç°ï¼š

Kandinsky 2.1Kandinsky 2.2

```py
import torch
import numpy as np
from PIL import Image
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

pipe = AutoPipelineForInpainting.from_pretrained("kandinsky-community/kandinsky-2-1-inpaint", torch_dtype=torch.float16)
pipe.enable_model_cpu_offload()

init_image = load_image("https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png")
mask = np.zeros((768, 768), dtype=np.float32)
# mask area above cat's head
mask[:250, 250:-250] = 1
prompt = "a hat"

output_image = pipe(prompt=prompt, image=init_image, mask_image=mask).images[0]
mask = Image.fromarray((mask*255).astype('uint8'), 'L')
make_image_grid([init_image, mask, output_image], rows=1, cols=3)
```

## æ’å€¼

æ’å€¼å…è®¸æ‚¨æ¢ç´¢å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„æ½œåœ¨ç©ºé—´ï¼Œè¿™æ˜¯ä¸€ç§å¾ˆé…·çš„æ–¹å¼æ¥æŸ¥çœ‹å…ˆå‰æ¨¡å‹çš„ä¸­é—´è¾“å‡ºã€‚åŠ è½½å…ˆå‰ç®¡é“å’Œæ‚¨æƒ³è¦æ’å€¼çš„ä¸¤ä¸ªå›¾åƒï¼š

Kandinsky 2.1Kandinsky 2.2

```py
from diffusers import KandinskyPriorPipeline, KandinskyPipeline
from diffusers.utils import load_image, make_image_grid
import torch

prior_pipeline = KandinskyPriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-1-prior", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
img_1 = load_image("https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png")
img_2 = load_image("https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/starry_night.jpeg")
make_image_grid([img_1.resize((512,512)), img_2.resize((512,512))], rows=1, cols=2)
```

![](img/c39ee2cbaacad2963f3842a301c122a7.png)

ä¸€åªçŒ«

![](img/d28a21117fde9e1974b55867c4e29dea.png)

æ¢µé«˜çš„ã€Šæ˜Ÿå¤œã€‹ç»˜ç”»

æŒ‡å®šè¦æ’å€¼çš„æ–‡æœ¬æˆ–å›¾åƒï¼Œå¹¶ä¸ºæ¯ä¸ªæ–‡æœ¬æˆ–å›¾åƒè®¾ç½®æƒé‡ã€‚å°è¯•ä¸åŒçš„æƒé‡ä»¥æŸ¥çœ‹å®ƒä»¬å¦‚ä½•å½±å“æ’å€¼ï¼

```py
images_texts = ["a cat", img_1, img_2]
weights = [0.3, 0.3, 0.4]
```

è°ƒç”¨`interpolate`å‡½æ•°ç”ŸæˆåµŒå…¥ï¼Œç„¶åå°†å…¶ä¼ é€’ç»™ç®¡é“ç”Ÿæˆå›¾åƒï¼š

Kandinsky 2.1Kandinsky 2.2

```py
# prompt can be left empty
prompt = ""
prior_out = prior_pipeline.interpolate(images_texts, weights)

pipeline = KandinskyPipeline.from_pretrained("kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16, use_safetensors=True).to("cuda")

image = pipeline(prompt, **prior_out, height=768, width=768).images[0]
image
```

![](img/59fe929dc7d1553d026875441e579c7b.png)

## ControlNet

âš ï¸ ControlNet ä»…æ”¯æŒ Kandinsky 2.2ï¼

ControlNet ä½¿å¾—å¯ä»¥ä½¿ç”¨é¢å¤–çš„è¾“å…¥æ¡ä»¶åŒ–å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä¾‹å¦‚æ·±åº¦å›¾æˆ–è¾¹ç¼˜æ£€æµ‹ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ·±åº¦å›¾å¯¹ Kandinsky 2.2 è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œä»¥ä¾¿æ¨¡å‹ç†è§£å¹¶ä¿ç•™æ·±åº¦å›¾åƒçš„ç»“æ„ã€‚

è®©æˆ‘ä»¬åŠ è½½ä¸€å¼ å›¾åƒå¹¶æå–å…¶æ·±åº¦å›¾ï¼š

```py
from diffusers.utils import load_image

img = load_image(
    "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinskyv22/cat.png"
).resize((768, 768))
img
```

![](img/098067659c7d591c7d25aac963832424.png)

ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤— Transformers çš„`depth-estimation` [Pipeline](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)å¤„ç†å›¾åƒå¹¶æ£€ç´¢æ·±åº¦å›¾ï¼š

```py
import torch
import numpy as np

from transformers import pipeline

def make_hint(image, depth_estimator):
    image = depth_estimator(image)["depth"]
    image = np.array(image)
    image = image[:, :, None]
    image = np.concatenate([image, image, image], axis=2)
    detected_map = torch.from_numpy(image).float() / 255.0
    hint = detected_map.permute(2, 0, 1)
    return hint

depth_estimator = pipeline("depth-estimation")
hint = make_hint(img, depth_estimator).unsqueeze(0).half().to("cuda")
```

### æ–‡æœ¬åˆ°å›¾åƒ

åŠ è½½å…ˆå‰çš„ç®¡é“å’Œ KandinskyV22ControlnetPipelineï¼š

```py
from diffusers import KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline

prior_pipeline = KandinskyV22PriorPipeline.from_pretrained(
    "kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16, use_safetensors=True
).to("cuda")

pipeline = KandinskyV22ControlnetPipeline.from_pretrained(
    "kandinsky-community/kandinsky-2-2-controlnet-depth", torch_dtype=torch.float16
).to("cuda")
```

ä»æç¤ºå’Œè´Ÿæç¤ºç”Ÿæˆå›¾åƒåµŒå…¥ï¼š

```py
prompt = "A robot, 4k photo"
negative_prior_prompt = "lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature"

generator = torch.Generator(device="cuda").manual_seed(43)

image_emb, zero_image_emb = prior_pipeline(
    prompt=prompt, negative_prompt=negative_prior_prompt, generator=generator
).to_tuple()
```

æœ€åï¼Œå°†å›¾åƒåµŒå…¥å’Œæ·±åº¦å›¾åƒä¼ é€’ç»™ KandinskyV22ControlnetPipeline ä»¥ç”Ÿæˆå›¾åƒï¼š

```py
image = pipeline(image_embeds=image_emb, negative_image_embeds=zero_image_emb, hint=hint, num_inference_steps=50, generator=generator, height=768, width=768).images[0]
image
```

![](img/b044b806899a4ae7cdc6df24f2b71924.png)

### å›¾åƒåˆ°å›¾åƒ

å¯¹äºä½¿ç”¨ ControlNet çš„å›¾åƒåˆ°å›¾åƒï¼Œæ‚¨éœ€è¦ä½¿ç”¨ï¼š

+   KandinskyV22PriorEmb2EmbPipeline ä»æ–‡æœ¬æç¤ºå’Œå›¾åƒç”Ÿæˆå›¾åƒåµŒå…¥

+   KandinskyV22ControlnetImg2ImgPipeline ä»åˆå§‹å›¾åƒå’Œå›¾åƒåµŒå…¥ç”Ÿæˆå›¾åƒ

ä½¿ç”¨ğŸ¤— Transformers çš„`depth-estimation` [Pipeline](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)å¤„ç†å’Œæå–çŒ«çš„åˆå§‹å›¾åƒçš„æ·±åº¦å›¾ï¼š

```py
import torch
import numpy as np

from diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22ControlnetImg2ImgPipeline
from diffusers.utils import load_image
from transformers import pipeline

img = load_image(
    "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinskyv22/cat.png"
).resize((768, 768))

def make_hint(image, depth_estimator):
    image = depth_estimator(image)["depth"]
    image = np.array(image)
    image = image[:, :, None]
    image = np.concatenate([image, image, image], axis=2)
    detected_map = torch.from_numpy(image).float() / 255.0
    hint = detected_map.permute(2, 0, 1)
    return hint

depth_estimator = pipeline("depth-estimation")
hint = make_hint(img, depth_estimator).unsqueeze(0).half().to("cuda")
```

åŠ è½½å…ˆå‰çš„ç®¡é“å’Œ KandinskyV22ControlnetImg2ImgPipelineï¼š

```py
prior_pipeline = KandinskyV22PriorEmb2EmbPipeline.from_pretrained(
    "kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16, use_safetensors=True
).to("cuda")

pipeline = KandinskyV22ControlnetImg2ImgPipeline.from_pretrained(
    "kandinsky-community/kandinsky-2-2-controlnet-depth", torch_dtype=torch.float16
).to("cuda")
```

ä¼ é€’æ–‡æœ¬æç¤ºå’Œåˆå§‹å›¾åƒåˆ°å…ˆå‰çš„ç®¡é“ä»¥ç”Ÿæˆå›¾åƒåµŒå…¥ï¼š

```py
prompt = "A robot, 4k photo"
negative_prior_prompt = "lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature"

generator = torch.Generator(device="cuda").manual_seed(43)

img_emb = prior_pipeline(prompt=prompt, image=img, strength=0.85, generator=generator)
negative_emb = prior_pipeline(prompt=negative_prior_prompt, image=img, strength=1, generator=generator)
```

ç°åœ¨æ‚¨å¯ä»¥è¿è¡Œ KandinskyV22ControlnetImg2ImgPipeline ä»åˆå§‹å›¾åƒå’Œå›¾åƒåµŒå…¥ç”Ÿæˆå›¾åƒï¼š

```py
image = pipeline(image=img, strength=0.5, image_embeds=img_emb.image_embeds, negative_image_embeds=negative_emb.image_embeds, hint=hint, num_inference_steps=50, generator=generator, height=768, width=768).images[0]
make_image_grid([img.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)
```

![](img/3b9c48325073009cf6dc5d406bfe39d2.png)

## ä¼˜åŒ–

Kandinsky æ˜¯ç‹¬ç‰¹çš„ï¼Œå› ä¸ºå®ƒéœ€è¦ä¸€ä¸ªå…ˆå‰çš„ç®¡é“æ¥ç”Ÿæˆæ˜ å°„ï¼Œä»¥åŠç¬¬äºŒä¸ªç®¡é“æ¥å°†æ½œåœ¨å˜é‡è§£ç ä¸ºå›¾åƒã€‚ä¼˜åŒ–å·¥ä½œåº”è¯¥é›†ä¸­åœ¨ç¬¬äºŒä¸ªç®¡é“ä¸Šï¼Œå› ä¸ºé‚£é‡Œå®Œæˆäº†å¤§éƒ¨åˆ†è®¡ç®—ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ”¹è¿› Kandinsky æ¨ç†çš„æç¤ºã€‚

1.  å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ PyTorch < 2.0ï¼Œè¯·å¯ç”¨ xFormersï¼š

```py
  from diffusers import DiffusionPipeline
  import torch

  pipe = DiffusionPipeline.from_pretrained("kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16)
+ pipe.enable_xformers_memory_efficient_attention()
```

1.  å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ PyTorch >= 2.0ï¼Œè¯·å¯ç”¨`torch.compile`ä»¥è‡ªåŠ¨ä½¿ç”¨ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆSDPAï¼‰ï¼š

```py
  pipe.unet.to(memory_format=torch.channels_last)
+ pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead", fullgraph=True)
```

è¿™ä¸æ˜¾å¼è®¾ç½®æ³¨æ„åŠ›å¤„ç†å™¨ä¸º AttnAddedKVProcessor2_0 ç›¸åŒï¼š

```py
from diffusers.models.attention_processor import AttnAddedKVProcessor2_0

pipe.unet.set_attn_processor(AttnAddedKVProcessor2_0())
```

1.  ä½¿ç”¨ enable_model_cpu_offload()å°†æ¨¡å‹å¸è½½åˆ° CPU ä»¥é¿å…å†…å­˜ä¸è¶³é”™è¯¯ï¼š

```py
  from diffusers import DiffusionPipeline
  import torch

  pipe = DiffusionPipeline.from_pretrained("kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16)
+ pipe.enable_model_cpu_offload()
```

1.  é»˜è®¤æƒ…å†µä¸‹ï¼Œæ–‡æœ¬åˆ°å›¾åƒç®¡é“ä½¿ç”¨ DDIMSchedulerï¼Œä½†æ‚¨å¯ä»¥å°†å…¶æ›¿æ¢ä¸ºå¦ä¸€ä¸ªè°ƒåº¦ç¨‹åºï¼Œå¦‚ DDPMSchedulerï¼Œä»¥æŸ¥çœ‹å®ƒå¦‚ä½•å½±å“æ¨ç†é€Ÿåº¦å’Œå›¾åƒè´¨é‡ä¹‹é—´çš„æƒè¡¡ï¼š

```py
from diffusers import DDPMScheduler
from diffusers import DiffusionPipeline

scheduler = DDPMScheduler.from_pretrained("kandinsky-community/kandinsky-2-1", subfolder="ddpm_scheduler")
pipe = DiffusionPipeline.from_pretrained("kandinsky-community/kandinsky-2-1", scheduler=scheduler, torch_dtype=torch.float16, use_safetensors=True).to("cuda")
```
