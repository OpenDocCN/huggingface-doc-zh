- en: TVP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/tvp](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/tvp)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The text-visual prompting (TVP) framework was proposed in the paper [Text-Visual
    Prompting for Efficient 2D Temporal Video Grounding](https://arxiv.org/abs/2303.04995)
    by Yimeng Zhang, Xin Chen, Jinghan Jia, Sijia Liu, Ke Ding.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*In this paper, we study the problem of temporal video grounding (TVG), which
    aims to predict the starting/ending time points of moments described by a text
    sentence within a long untrimmed video. Benefiting from fine-grained 3D visual
    features, the TVG techniques have achieved remarkable progress in recent years.
    However, the high complexity of 3D convolutional neural networks (CNNs) makes
    extracting dense 3D visual features time-consuming, which calls for intensive
    memory and computing resources. Towards efficient TVG, we propose a novel text-visual
    prompting (TVP) framework, which incorporates optimized perturbation patterns
    (that we call ‘prompts’) into both visual inputs and textual features of a TVG
    model. In sharp contrast to 3D CNNs, we show that TVP allows us to effectively
    co-train vision encoder and language encoder in a 2D TVG model and improves the
    performance of cross-modal feature fusion using only low-complexity sparse 2D
    visual features. Further, we propose a Temporal-Distance IoU (TDIoU) loss for
    efficient learning of TVG. Experiments on two benchmark datasets, Charades-STA
    and ActivityNet Captions datasets, empirically show that the proposed TVP significantly
    boosts the performance of 2D TVG (e.g., 9.79% improvement on Charades-STA and
    30.77% improvement on ActivityNet Captions) and achieves 5× inference acceleration
    over TVG using 3D visual features.*'
  prefs: []
  type: TYPE_NORMAL
- en: This research addresses temporal video grounding (TVG), which is the process
    of pinpointing the start and end times of specific events in a long video, as
    described by a text sentence. Text-visual prompting (TVP), is proposed to enhance
    TVG. TVP involves integrating specially designed patterns, known as ‘prompts’,
    into both the visual (image-based) and textual (word-based) input components of
    a TVG model. These prompts provide additional spatial-temporal context, improving
    the model’s ability to accurately determine event timings in the video. The approach
    employs 2D visual inputs in place of 3D ones. Although 3D inputs offer more spatial-temporal
    detail, they are also more time-consuming to process. The use of 2D inputs with
    the prompting method aims to provide similar levels of context and accuracy more
    efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: '![drawing](../Images/2b0ade26768ecda5ef11dfd6c2da9b20.png) TVP architecture.
    Taken from the [original paper.](https://arxiv.org/abs/2303.04995)'
  prefs: []
  type: TYPE_IMG
- en: This model was contributed by [Jiqing Feng](https://huggingface.co/Jiqing).
    The original code can be found [here](https://github.com/intel/TVP).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips and examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompts are optimized perturbation patterns, which would be added to input video
    frames or text features. Universal set refers to using the same exact set of prompts
    for any input, this means that these prompts are added consistently to all video
    frames and text features, regardless of the input’s content.
  prefs: []
  type: TYPE_NORMAL
- en: TVP consists of a visual encoder and cross-modal encoder. A universal set of
    visual prompts and text prompts to be integrated into sampled video frames and
    textual features, respectively. Specially, a set of different visual prompts are
    applied to uniformly-sampled frames of one untrimmed video in order.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this model is to incorporate trainable prompts into both visual
    inputs and textual features to temporal video grounding(TVG) problems. In principle,
    one can apply any visual, cross-modal encoder in the proposed architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The [TvpProcessor](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpProcessor)
    wraps [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    and [TvpImageProcessor](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpImageProcessor)
    into a single instance to both encode the text and prepare the images respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The following example shows how to run temporal video grounding using [TvpProcessor](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpProcessor)
    and [TvpForVideoGrounding](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpForVideoGrounding).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Tips:'
  prefs: []
  type: TYPE_NORMAL
- en: This implementation of TVP uses [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    to generate text embeddings and Resnet-50 model to compute visual embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checkpoints for pre-trained [tvp-base](https://huggingface.co/Intel/tvp-base)
    is released.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please refer to [Table 2](https://arxiv.org/pdf/2303.04995.pdf) for TVP’s performance
    on Temporal Video Grounding task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TvpConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TvpConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tvp/configuration_tvp.py#L32)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`backbone_config` (`PretrainedConfig` or `dict`, *optional*) — The configuration
    of the backbone model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distance_loss_weight` (`float`, *optional*, defaults to 1.0) — The weight
    of distance loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`duration_loss_weight` (`float`, *optional*, defaults to 0.1) — The weight
    of duration loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`visual_prompter_type` (`str`, *optional*, defaults to `"framepad"`) — Visual
    prompt type. The type of padding. Framepad means padding on each frame. Should
    be one of “framepad” or “framedownpad”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`visual_prompter_apply` (`str`, *optional*, defaults to `"replace"`) — The
    way of applying visual prompt. Replace means use the value of prompt to change
    the original value in visual inputs. Should be one of “replace”, or “add”, or
    “remove”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`visual_prompt_size` (`int`, *optional*, defaults to 96) — The size of visual
    prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_img_size` (`int`, *optional*, defaults to 448) — The maximum size of frame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_frames` (`int`, *optional*, defaults to 48) — The number of frames extracted
    from a video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) — Vocabulary size of the
    Tvp text model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [TvpModel](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_grid_col_position_embeddings` (`int`, *optional*, defaults to 100) — The
    largest number of horizontal patches from a video frame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_grid_row_position_embeddings` (`int`, *optional*, defaults to 100) — The
    largest number of vertical patches from a video frame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probability of hidden layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` ``"quick_gelu"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — The
    dropout probability of attention layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [TvpModel](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpModel).
    It is used to instantiate an Tvp model according to the specified arguments, defining
    the model architecture. Instantiating a configuration with the defaults will yield
    a similar configuration to that of the Tvp [Intel/tvp-base](https://huggingface.co/Intel/tvp-base)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_backbone_config`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tvp/configuration_tvp.py#L152)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`backbone_config` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The backbone configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[TvpConfig](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpConfig)'
  prefs: []
  type: TYPE_NORMAL
- en: An instance of a configuration object
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a [TvpConfig](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpConfig)
    (or a derived class) from a pre-trained backbone model configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `to_dict`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tvp/configuration_tvp.py#L164)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict[str, any]`'
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary of all the attributes that make up this configuration instance,
  prefs: []
  type: TYPE_NORMAL
- en: Serializes this instance to a Python dictionary. Override the default [to_dict()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.to_dict).
  prefs: []
  type: TYPE_NORMAL
- en: TvpImageProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TvpImageProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tvp/image_processing_tvp.py#L83)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Whether to resize the
    image’s (height, width) dimensions to the specified `size`. Can be overridden
    by the `do_resize` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"longest_edge" -- 448}`):
    Size of the output image after resizing. The longest edge of the image will be
    resized to `size["longest_edge"]` while maintaining the aspect ratio of the original
    image. Can be overriden by `size` in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`)
    — Resampling filter to use if resizing the image. Can be overridden by the `resample`
    parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_center_crop` (`bool`, *optional*, defaults to `True`) — Whether to center
    crop the image to the specified `crop_size`. Can be overridden by the `do_center_crop`
    parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `{"height" -- 448, "width":
    448}`): Size of the image after applying the center crop. Can be overridden by
    the `crop_size` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) — Whether to rescale
    the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`
    parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — Defines
    the scale factor to use if rescaling the image. Can be overridden by the `rescale_factor`
    parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_pad` (`bool`, *optional*, defaults to `True`) — Whether to pad the image.
    Can be overridden by the `do_pad` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_size` (`Dict[str, int]`, *optional*, defaults to `{"height" -- 448, "width":
    448}`): Size of the image after applying the padding. Can be overridden by the
    `pad_size` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`constant_values` (`Union[float, Iterable[float]]`, *optional*, defaults to
    0) — The fill value to use when padding the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_mode` (`PaddingMode`, *optional*, defaults to `PaddingMode.CONSTANT`)
    — Use what kind of mode in padding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the image. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_flip_channel_order` (`bool`, *optional*, defaults to `True`) — Whether
    to flip the color channels from RGB to BGR. Can be overridden by the `do_flip_channel_order`
    parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`)
    — Mean to use if normalizing the image. This is a float or list of floats the
    length of the number of channels in the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`)
    — Standard deviation to use if normalizing the image. This is a float or list
    of floats the length of the number of channels in the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a Tvp image processor.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `preprocess`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tvp/image_processing_tvp.py#L337)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`videos` (`ImageInput` or `List[ImageInput]` or `List[List[ImageInput]]`) —
    Frames to preprocess.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) — Whether to
    resize the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — Size of the
    image after applying resize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `self.resample`)
    — Resampling filter to use if resizing the image. This can be one of the enum
    `PILImageResampling`, Only has an effect if `do_resize` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_center_crop` (`bool`, *optional*, defaults to `self.do_centre_crop`) —
    Whether to centre crop the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `self.crop_size`) —
    Size of the image after applying the centre crop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — Whether
    to rescale the image values between [0 - 1].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) —
    Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_pad` (`bool`, *optional*, defaults to `True`) — Whether to pad the image.
    Can be overridden by the `do_pad` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_size` (`Dict[str, int]`, *optional*, defaults to `{"height" -- 448, "width":
    448}`): Size of the image after applying the padding. Can be overridden by the
    `pad_size` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`constant_values` (`Union[float, Iterable[float]]`, *optional*, defaults to
    0) — The fill value to use when padding the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_mode` (`PaddingMode`, *optional*, defaults to “PaddingMode.CONSTANT”)
    — Use what kind of mode in padding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — Whether
    to normalize the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_flip_channel_order` (`bool`, *optional*, defaults to `self.do_flip_channel_order`)
    — Whether to flip the channel order of the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `self.image_mean`)
    — Image mean.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `self.image_std`)
    — Image standard deviation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unset: Return a list of `np.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChannelDimension.FIRST`: image in (num_channels, height, width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChannelDimension.LAST`: image in (height, width, num_channels) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unset: Use the inferred channel dimension format of the input image.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess an image or batch of images.
  prefs: []
  type: TYPE_NORMAL
- en: TvpProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TvpProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tvp/processing_tvp.py#L24)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`image_processor` ([TvpImageProcessor](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpImageProcessor),
    *optional*) — The image processor is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast),
    *optional*) — The tokenizer is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs an TVP processor which wraps a TVP image processor and a Bert tokenizer
    into a single processor.
  prefs: []
  type: TYPE_NORMAL
- en: '[TvpProcessor](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpProcessor)
    offers all the functionalities of [TvpImageProcessor](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpImageProcessor)
    and [BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast).
    See the [**call**()](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpProcessor.__call__)
    and `decode()` for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tvp/processing_tvp.py#L50)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text` (`str`, `List[str]`, `List[List[str]]`) — The sequence or batch of sequences
    to be encoded. Each sequence can be a string or a list of strings (pretokenized
    string). If the sequences are provided as list of strings (pretokenized), you
    must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`videos` (`List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`,
    `List[List[PIL.Image.Image]]`, `List[List[np.ndarrray]]`, — `List[List[torch.Tensor]]`):
    The video or batch of videos to be prepared. Each video should be a list of frames,
    which can be either PIL images or NumPy arrays. In case of NumPy arrays/PyTorch
    tensors, each frame should be of shape (H, W, C), where H and W are frame height
    and width, and C is a number of channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors of a particular framework. Acceptable
    values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return NumPy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''jax''`: Return JAX `jnp.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` — List of token ids to be fed to a model. Returned when `text`
    is not `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names` and if `text` is not `None`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_values` — Pixel values to be fed to a model. Returned when `videos`
    is not `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to prepare for the model one or several sequences(s) and image(s).
    This method forwards the `text` and `kwargs` arguments to BertTokenizerFast’s
    [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    if `text` is not `None` to encode the text. To prepare the image(s), this method
    forwards the `videos` and `kwargs` arguments to TvpImageProcessor’s [**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    if `videos` is not `None`. Please refer to the doctsring of the above two methods
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: TvpModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TvpModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tvp/modeling_tvp.py#L692)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([TvpConfig](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare Tvp Model transformer outputting BaseModelOutputWithPooling object
    without any specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tvp/modeling_tvp.py#L726)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Indices can be obtained using
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels,
    height, width)`) — Pixel values. Pixel values can be obtained using [TvpImageProcessor](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpImageProcessor).
    See [TvpImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.tvp.configuration_tvp.TvpConfig'>`)
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TvpModel](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: TvpForVideoGrounding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TvpForVideoGrounding`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tvp/modeling_tvp.py#L813)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([TvpConfig](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tvp Model with a video grounding head on top computing IoU, distance, and duration
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tvp/modeling_tvp.py#L828)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Indices can be obtained using
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels,
    height, width)`) — Pixel values. Pixel values can be obtained using [TvpImageProcessor](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpImageProcessor).
    See [TvpImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.FloatTensor` of shape `(batch_size, 3)`, *optional*) — The
    labels contains duration, start time, and end time of the video corresponding
    to the text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.tvp.modeling_tvp.TvpVideoGroundingOutput` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.tvp.modeling_tvp.TvpVideoGroundingOutput` or a tuple
    of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.tvp.configuration_tvp.TvpConfig'>`)
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss`
    is `True`) — Temporal-Distance IoU loss for video grounding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, 2)`) — Contains start_time/duration
    and end_time/duration. It is the time slot of the videos corresponding to the
    input texts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [TvpForVideoGrounding](/docs/transformers/v4.37.2/en/model_doc/tvp#transformers.TvpForVideoGrounding)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
