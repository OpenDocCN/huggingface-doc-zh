- en: MegatronGPT2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/megatron_gpt2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/megatron_gpt2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/183.9e3a068c.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The MegatronGPT2 model was proposed in [Megatron-LM: Training Multi-Billion
    Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
    by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper
    and Bryan Catanzaro.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Recent work in language modeling demonstrates that training large transformer
    models advances the state of the art in Natural Language Processing applications.
    However, very large models can be quite difficult to train due to memory constraints.
    In this work, we present our techniques for training very large transformer models
    and implement a simple, efficient intra-layer model parallel approach that enables
    training transformer models with billions of parameters. Our approach does not
    require a new compiler or library changes, is orthogonal and complimentary to
    pipeline model parallelism, and can be fully implemented with the insertion of
    a few communication operations in native PyTorch. We illustrate this approach
    by converging transformer based models up to 8.3 billion parameters using 512
    GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling
    efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs,
    which is 30% of peak FLOPs. To demonstrate that large language models can further
    advance the state of the art (SOTA), we train an 8.3 billion parameter transformer
    language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT.
    We show that careful attention to the placement of layer normalization in BERT-like
    models is critical to achieving increased performance as the model size grows.
    Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared
    to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%)
    datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared
    to SOTA accuracy of 89.4%).*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [jdemouth](https://huggingface.co/jdemouth). The
    original code can be found [here](https://github.com/NVIDIA/Megatron-LM). That
    repository contains a multi-GPU and multi-node implementation of the Megatron
    Language models. In particular, it contains a hybrid model parallel approach using
    “tensor parallel” and “pipeline parallel” techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have provided pretrained [GPT2-345M](https://ngc.nvidia.com/catalog/models/nvidia:megatron_lm_345m)
    checkpoints for use to evaluate or finetuning downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: To access these checkpoints, first [sign up](https://ngc.nvidia.com/signup)
    for and setup the NVIDIA GPU Cloud (NGC) Registry CLI. Further documentation for
    downloading models can be found in the [NGC documentation](https://docs.nvidia.com/dgx/ngc-registry-cli-user-guide/index.html#topic_6_4_1).
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can directly download the checkpoints using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once you have obtained the checkpoint from NVIDIA GPU Cloud (NGC), you have
    to convert it to a format that will easily be loaded by Hugging Face Transformers
    GPT2 implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command allows you to do the conversion. We assume that the folder
    `models/megatron_gpt2` contains `megatron_gpt2_345m_v0_0.zip` and that the command
    is run from that folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: MegatronGPT2 architecture is the same as OpenAI GPT-2 . Refer to [GPT-2 documentation](gpt2)
    for information on configuration classes and their parameters.
  prefs: []
  type: TYPE_NORMAL
