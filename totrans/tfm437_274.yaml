- en: MaskFormer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/maskformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/maskformer)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: This is a recently introduced model so the API hasn’t been tested extensively.
    There may be some bugs or slight breaking changes to fix it in the future. If
    you see something strange, file a [Github Issue](https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title).
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The MaskFormer model was proposed in [Per-Pixel Classification is Not All You
    Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278) by Bowen Cheng,
    Alexander G. Schwing, Alexander Kirillov. MaskFormer addresses semantic segmentation
    with a mask classification paradigm instead of performing classic pixel-level
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Modern approaches typically formulate semantic segmentation as a per-pixel
    classification task, while instance-level segmentation is handled with an alternative
    mask classification. Our key insight: mask classification is sufficiently general
    to solve both semantic- and instance-level segmentation tasks in a unified manner
    using the exact same model, loss, and training procedure. Following this observation,
    we propose MaskFormer, a simple mask classification model which predicts a set
    of binary masks, each associated with a single global class label prediction.
    Overall, the proposed mask classification-based method simplifies the landscape
    of effective approaches to semantic and panoptic segmentation tasks and shows
    excellent empirical results. In particular, we observe that MaskFormer outperforms
    per-pixel classification baselines when the number of classes is large. Our mask
    classification-based method outperforms both current state-of-the-art semantic
    (55.6 mIoU on ADE20K) and panoptic segmentation (52.7 PQ on COCO) models.*'
  prefs: []
  type: TYPE_NORMAL
- en: The figure below illustrates the architecture of MaskFormer. Taken from the
    [original paper](https://arxiv.org/abs/2107.06278).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f3d238eca99c99399c901e43c598a16.png)'
  prefs: []
  type: TYPE_IMG
- en: This model was contributed by [francesco](https://huggingface.co/francesco).
    The original code can be found [here](https://github.com/facebookresearch/MaskFormer).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MaskFormer’s Transformer decoder is identical to the decoder of [DETR](detr).
    During training, the authors of DETR did find it helpful to use auxiliary losses
    in the decoder, especially to help the model output the correct number of objects
    of each class. If you set the parameter `use_auxilary_loss` of [MaskFormerConfig](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerConfig)
    to `True`, then prediction feedforward neural networks and Hungarian losses are
    added after each decoder layer (with the FFNs sharing parameters).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to train the model in a distributed environment across multiple
    nodes, then one should update the `get_num_masks` function inside in the `MaskFormerLoss`
    class of `modeling_maskformer.py`. When training on multiple nodes, this should
    be set to the average number of target masks across all nodes, as can be seen
    in the original implementation [here](https://github.com/facebookresearch/MaskFormer/blob/da3e60d85fdeedcb31476b5edd7d328826ce56cc/mask_former/modeling/criterion.py#L169).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One can use [MaskFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerImageProcessor)
    to prepare images for the model and optional targets for the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To get the final segmentation, depending on the task, you can call [post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerFeatureExtractor.post_process_semantic_segmentation)
    or [post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerFeatureExtractor.post_process_panoptic_segmentation).
    Both tasks can be solved using [MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation)
    output, panoptic segmentation accepts an optional `label_ids_to_fuse` argument
    to fuse instances of the target object/s (e.g. sky) together.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image Segmentation
  prefs: []
  type: TYPE_NORMAL
- en: All notebooks that illustrate inference as well as fine-tuning on custom data
    with MaskFormer can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MaskFormer).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MaskFormer specific outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.models.maskformer.modeling_maskformer.MaskFormerModelOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/modeling_maskformer.py#L146)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Last hidden states (final feature map) of the last stage of
    the encoder model (backbone).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_decoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    num_channels, height, width)`) — Last hidden states (final feature map) of the
    last stage of the pixel decoder model (FPN).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformer_decoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`) — Last hidden states (final feature map) of the
    last stage of the transformer decoder model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the encoder model at the output of
    each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the pixel decoder model at the output
    of each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*,
    returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the transformer decoder at the output
    of each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` `tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    containing `encoder_hidden_states`, `pixel_decoder_hidden_states` and `decoder_hidden_states`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights from Detr’s decoder after the attention softmax, used to compute
    the weighted average in the self-attention heads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Class for outputs of [MaskFormerModel](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerModel).
    This class returns all the needed hidden states to compute the logits.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.models.maskformer.modeling_maskformer.MaskFormerForInstanceSegmentationOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/modeling_maskformer.py#L189)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.Tensor`, *optional*) — The computed loss, returned when labels
    are present.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class_queries_logits` (`torch.FloatTensor`) — A tensor of shape `(batch_size,
    num_queries, num_labels + 1)` representing the proposed classes for each query.
    Note the `+ 1` is needed because we incorporate the null class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`masks_queries_logits` (`torch.FloatTensor`) — A tensor of shape `(batch_size,
    num_queries, height, width)` representing the proposed masks for each query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Last hidden states (final feature map) of the last stage of
    the encoder model (backbone).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_decoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    num_channels, height, width)`) — Last hidden states (final feature map) of the
    last stage of the pixel decoder model (FPN).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformer_decoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`) — Last hidden states (final feature map) of the
    last stage of the transformer decoder model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the encoder model at the output of
    each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the pixel decoder model at the output
    of each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*,
    returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the transformer decoder at the output of each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` `tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    containing `encoder_hidden_states`, `pixel_decoder_hidden_states` and `decoder_hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights from Detr’s decoder after the attention softmax, used to compute
    the weighted average in the self-attention heads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Class for outputs of [MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation).
  prefs: []
  type: TYPE_NORMAL
- en: This output can be directly passed to [post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerFeatureExtractor.post_process_semantic_segmentation)
    or or [post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerFeatureExtractor.post_process_instance_segmentation)
    or [post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerFeatureExtractor.post_process_panoptic_segmentation)
    depending on the task. Please, see [`~MaskFormerImageProcessor] for details regarding
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: MaskFormerConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.MaskFormerConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/configuration_maskformer.py#L35)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`mask_feature_size` (`int`, *optional*, defaults to 256) — The masks’ features
    size, this value will also be used to specify the Feature Pyramid Network features’
    size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`no_object_weight` (`float`, *optional*, defaults to 0.1) — Weight to apply
    to the null (no object) class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_auxiliary_loss(bool,` *optional*, defaults to `False`) — If `True` `MaskFormerForInstanceSegmentationOutput`
    will contain the auxiliary losses computed using the logits from each decoder’s
    stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`backbone_config` (`Dict`, *optional*) — The configuration passed to the backbone,
    if unset, the configuration corresponding to `swin-base-patch4-window12-384` will
    be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_config` (`Dict`, *optional*) — The configuration passed to the transformer
    decoder model, if unset the base config for `detr-resnet-50` will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_std` (`float`, *optional*, defaults to 0.02) — The standard deviation
    of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_xavier_std` (`float`, *optional*, defaults to 1) — The scaling factor
    used for the Xavier initialization gain in the HM Attention map module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dice_weight` (`float`, *optional*, defaults to 1.0) — The weight for the dice
    loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_entropy_weight` (`float`, *optional*, defaults to 1.0) — The weight
    for the cross entropy loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_weight` (`float`, *optional*, defaults to 20.0) — The weight for the
    mask loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_auxiliary_logits` (`bool`, *optional*) — Should the model output its
    `auxiliary_logits` or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raises
  prefs: []
  type: TYPE_NORMAL
- en: '`ValueError`'
  prefs: []
  type: TYPE_NORMAL
- en: '`ValueError` — Raised if the backbone model type selected is not in `["swin"]`
    or the decoder model type selected is not in `["detr"]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [MaskFormerModel](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerModel).
    It is used to instantiate a MaskFormer model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the MaskFormer [facebook/maskformer-swin-base-ade](https://huggingface.co/facebook/maskformer-swin-base-ade)
    architecture trained on [ADE20k-150](https://huggingface.co/datasets/scene_parse_150).
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, MaskFormer only supports the [Swin Transformer](swin) as backbone.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_backbone_and_decoder_configs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/configuration_maskformer.py#L182)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`backbone_config` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The backbone configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_config` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — The transformer decoder configuration to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[MaskFormerConfig](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerConfig)'
  prefs: []
  type: TYPE_NORMAL
- en: An instance of a configuration object
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a [MaskFormerConfig](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerConfig)
    (or a derived class) from a pre-trained backbone model configuration and DETR
    model configuration.
  prefs: []
  type: TYPE_NORMAL
- en: MaskFormerImageProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.MaskFormerImageProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/image_processing_maskformer.py#L347)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Whether to resize the
    input to a certain `size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`int`, *optional*, defaults to 800) — Resize the input to the given
    size. Only has an effect if `do_resize` is set to `True`. If size is a sequence
    like `(width, height)`, output size will be matched to this. If size is an int,
    smaller edge of the image will be matched to this number. i.e, if `height > width`,
    then image will be rescaled to `(size * height / width, size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size_divisor` (`int`, *optional*, defaults to 32) — Some backbones need images
    divisible by a certain number. If not passed, it defaults to the value used in
    Swin Transformer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample` (`int`, *optional*, defaults to `Resampling.BILINEAR`) — An optional
    resampling filter. This can be one of `PIL.Image.Resampling.NEAREST`, `PIL.Image.Resampling.BOX`,
    `PIL.Image.Resampling.BILINEAR`, `PIL.Image.Resampling.HAMMING`, `PIL.Image.Resampling.BICUBIC`
    or `PIL.Image.Resampling.LANCZOS`. Only has an effect if `do_resize` is set to
    `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) — Whether to rescale
    the input to a certain `scale`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rescale_factor` (`float`, *optional*, defaults to `1/ 255`) — Rescale the
    input by the given factor. Only has an effect if `do_rescale` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) — Whether or not to
    normalize the input with mean and standard deviation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_mean` (`int`, *optional*, defaults to `[0.485, 0.456, 0.406]`) — The
    sequence of means for each channel, to be used when normalizing images. Defaults
    to the ImageNet mean.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_std` (`int`, *optional*, defaults to `[0.229, 0.224, 0.225]`) — The
    sequence of standard deviations for each channel, to be used when normalizing
    images. Defaults to the ImageNet std.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ignore_index` (`int`, *optional*) — Label to be assigned to background pixels
    in segmentation maps. If provided, segmentation map pixels denoted with 0 (background)
    will be replaced with `ignore_index`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_reduce_labels` (`bool`, *optional*, defaults to `False`) — Whether or not
    to decrement all label values of segmentation maps by 1\. Usually used for datasets
    where 0 is used for background, and background itself is not included in all classes
    of a dataset (e.g. ADE20k). The background label will be replaced by `ignore_index`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a MaskFormer image processor. The image processor can be used to
    prepare image(s) and optional targets for the model.
  prefs: []
  type: TYPE_NORMAL
- en: This image processor inherits from `BaseImageProcessor` which contains most
    of the main methods. Users should refer to this superclass for more information
    regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `preprocess`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/image_processing_maskformer.py#L677)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#### `encode_inputs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/image_processing_maskformer.py#L875)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values_list` (`List[ImageInput]`) — List of images (pixel values) to
    be padded. Each image should be a tensor of shape `(channels, height, width)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`segmentation_maps` (`ImageInput`, *optional*) — The corresponding semantic
    segmentation maps with the pixel-wise annotations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(`bool`, *optional*, defaults to `True`): Whether or not to pad images up to
    the largest image in a batch and create a pixel mask.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If left to the default, will return a pixel mask that is:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for pixels that are real (i.e. `not masked`),
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for pixels that are padding (i.e. `masked`).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`instance_id_to_semantic_id` (`List[Dict[int, int]]` or `Dict[int, int]`, *optional*)
    — A mapping between object instance ids and class ids. If passed, `segmentation_maps`
    is treated as an instance segmentation map where each pixel represents an instance
    id. Can be provided as a single dictionary with a global/dataset-level mapping
    or as a list of dictionaries (one per image), to map instance ids in each image
    separately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of NumPy arrays. If set to `''pt''`,
    return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A [BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)
    with the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` — Pixel values to be fed to a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_mask` — Pixel mask to be fed to a model (when `=True` or if `pixel_mask`
    is in `self.model_input_names`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_labels` — Optional list of mask labels of shape `(labels, height, width)`
    to be fed to a model (when `annotations` are provided).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class_labels` — Optional list of class labels of shape `(labels)` to be fed
    to a model (when `annotations` are provided). They identify the labels of `mask_labels`,
    e.g. the label of `mask_labels[i][j]` if `class_labels[i][j]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pad images up to the largest image in a batch and create a corresponding `pixel_mask`.
  prefs: []
  type: TYPE_NORMAL
- en: MaskFormer addresses semantic segmentation with a mask classification paradigm,
    thus input segmentation maps will be converted to lists of binary masks and their
    respective labels. Let’s see an example, assuming `segmentation_maps = [[2,6,7,9]]`,
    the output will contain `mask_labels = [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]`
    (four binary masks) and `class_labels = [2,6,7,9]`, the labels for each mask.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `post_process_semantic_segmentation`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/image_processing_maskformer.py#L1029)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`outputs` ([MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation))
    — Raw outputs of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_sizes` (`List[Tuple[int, int]]`, *optional*) — List of length (batch_size),
    where each list item (`Tuple[int, int]]`) corresponds to the requested final size
    (height, width) of each prediction. If left to None, predictions will not be resized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[torch.Tensor]`'
  prefs: []
  type: TYPE_NORMAL
- en: A list of length `batch_size`, where each item is a semantic segmentation map
    of shape (height, width) corresponding to the target_sizes entry (if `target_sizes`
    is specified). Each entry of each `torch.Tensor` correspond to a semantic class
    id.
  prefs: []
  type: TYPE_NORMAL
- en: Converts the output of [MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `post_process_instance_segmentation`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/image_processing_maskformer.py#L1079)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`outputs` ([MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation))
    — Raw outputs of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threshold` (`float`, *optional*, defaults to 0.5) — The probability score
    threshold to keep predicted instance masks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) — Threshold to use
    when turning the predicted masks into binary values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) — The
    overlap mask area threshold to merge or discard small disconnected parts within
    each binary instance mask.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_sizes` (`List[Tuple]`, *optional*) — List of length (batch_size), where
    each list item (`Tuple[int, int]]`) corresponds to the requested final size (height,
    width) of each prediction. If left to None, predictions will not be resized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_coco_annotation` (`bool`, *optional*, defaults to `False`) — If set
    to `True`, segmentation maps are returned in COCO run-length encoding (RLE) format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_binary_maps` (`bool`, *optional*, defaults to `False`) — If set to
    `True`, segmentation maps are returned as a concatenated tensor of binary segmentation
    maps (one per detected instance).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of dictionaries, one per image, each dictionary containing two keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`segmentation` — A tensor of shape `(height, width)` where each pixel represents
    a `segment_id` or `List[List]` run-length encoding (RLE) of the segmentation map
    if return_coco_annotation is set to `True`. Set to `None` if no mask if found
    above `threshold`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`segments_info` — A dictionary that contains additional information on each
    segment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`id` — An integer representing the `segment_id`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_id` — An integer representing the label / semantic class id corresponding
    to `segment_id`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score` — Prediction score of segment with `segment_id`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Converts the output of `MaskFormerForInstanceSegmentationOutput` into instance
    segmentation predictions. Only supports PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `post_process_panoptic_segmentation`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/image_processing_maskformer.py#L1193)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`outputs` (`MaskFormerForInstanceSegmentationOutput`) — The outputs from [MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threshold` (`float`, *optional*, defaults to 0.5) — The probability score
    threshold to keep predicted instance masks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) — Threshold to use
    when turning the predicted masks into binary values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) — The
    overlap mask area threshold to merge or discard small disconnected parts within
    each binary instance mask.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_ids_to_fuse` (`Set[int]`, *optional*) — The labels in this state will
    have all their instances be fused together. For instance we could say there can
    only be one sky in an image, but several persons, so the label ID for sky would
    be in that set, but not the one for person.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_sizes` (`List[Tuple]`, *optional*) — List of length (batch_size), where
    each list item (`Tuple[int, int]]`) corresponds to the requested final size (height,
    width) of each prediction in batch. If left to None, predictions will not be resized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of dictionaries, one per image, each dictionary containing two keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`segmentation` — a tensor of shape `(height, width)` where each pixel represents
    a `segment_id`, set to `None` if no mask if found above `threshold`. If `target_sizes`
    is specified, segmentation is resized to the corresponding `target_sizes` entry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`segments_info` — A dictionary that contains additional information on each
    segment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`id` — an integer representing the `segment_id`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_id` — An integer representing the label / semantic class id corresponding
    to `segment_id`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`was_fused` — a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False`
    otherwise. Multiple instances of the same class / label were fused and assigned
    a single `segment_id`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score` — Prediction score of segment with `segment_id`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Converts the output of `MaskFormerForInstanceSegmentationOutput` into image
    panoptic segmentation predictions. Only supports PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: MaskFormerFeatureExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.MaskFormerFeatureExtractor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/feature_extraction_maskformer.py#L26)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/image_processing_maskformer.py#L571)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#### `encode_inputs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/image_processing_maskformer.py#L875)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values_list` (`List[ImageInput]`) — List of images (pixel values) to
    be padded. Each image should be a tensor of shape `(channels, height, width)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`segmentation_maps` (`ImageInput`, *optional*) — The corresponding semantic
    segmentation maps with the pixel-wise annotations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(`bool`, *optional*, defaults to `True`): Whether or not to pad images up to
    the largest image in a batch and create a pixel mask.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If left to the default, will return a pixel mask that is:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1 for pixels that are real (i.e. `not masked`),
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for pixels that are padding (i.e. `masked`).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`instance_id_to_semantic_id` (`List[Dict[int, int]]` or `Dict[int, int]`, *optional*)
    — A mapping between object instance ids and class ids. If passed, `segmentation_maps`
    is treated as an instance segmentation map where each pixel represents an instance
    id. Can be provided as a single dictionary with a global/dataset-level mapping
    or as a list of dictionaries (one per image), to map instance ids in each image
    separately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of NumPy arrays. If set to `''pt''`,
    return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A [BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)
    with the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` — Pixel values to be fed to a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_mask` — Pixel mask to be fed to a model (when `=True` or if `pixel_mask`
    is in `self.model_input_names`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_labels` — Optional list of mask labels of shape `(labels, height, width)`
    to be fed to a model (when `annotations` are provided).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class_labels` — Optional list of class labels of shape `(labels)` to be fed
    to a model (when `annotations` are provided). They identify the labels of `mask_labels`,
    e.g. the label of `mask_labels[i][j]` if `class_labels[i][j]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pad images up to the largest image in a batch and create a corresponding `pixel_mask`.
  prefs: []
  type: TYPE_NORMAL
- en: MaskFormer addresses semantic segmentation with a mask classification paradigm,
    thus input segmentation maps will be converted to lists of binary masks and their
    respective labels. Let’s see an example, assuming `segmentation_maps = [[2,6,7,9]]`,
    the output will contain `mask_labels = [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]`
    (four binary masks) and `class_labels = [2,6,7,9]`, the labels for each mask.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `post_process_semantic_segmentation`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/image_processing_maskformer.py#L1029)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`outputs` ([MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation))
    — Raw outputs of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_sizes` (`List[Tuple[int, int]]`, *optional*) — List of length (batch_size),
    where each list item (`Tuple[int, int]]`) corresponds to the requested final size
    (height, width) of each prediction. If left to None, predictions will not be resized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[torch.Tensor]`'
  prefs: []
  type: TYPE_NORMAL
- en: A list of length `batch_size`, where each item is a semantic segmentation map
    of shape (height, width) corresponding to the target_sizes entry (if `target_sizes`
    is specified). Each entry of each `torch.Tensor` correspond to a semantic class
    id.
  prefs: []
  type: TYPE_NORMAL
- en: Converts the output of [MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `post_process_instance_segmentation`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/image_processing_maskformer.py#L1079)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`outputs` ([MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation))
    — Raw outputs of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threshold` (`float`, *optional*, defaults to 0.5) — The probability score
    threshold to keep predicted instance masks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) — Threshold to use
    when turning the predicted masks into binary values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) — The
    overlap mask area threshold to merge or discard small disconnected parts within
    each binary instance mask.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_sizes` (`List[Tuple]`, *optional*) — List of length (batch_size), where
    each list item (`Tuple[int, int]]`) corresponds to the requested final size (height,
    width) of each prediction. If left to None, predictions will not be resized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_coco_annotation` (`bool`, *optional*, defaults to `False`) — If set
    to `True`, segmentation maps are returned in COCO run-length encoding (RLE) format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_binary_maps` (`bool`, *optional*, defaults to `False`) — If set to
    `True`, segmentation maps are returned as a concatenated tensor of binary segmentation
    maps (one per detected instance).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of dictionaries, one per image, each dictionary containing two keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`segmentation` — A tensor of shape `(height, width)` where each pixel represents
    a `segment_id` or `List[List]` run-length encoding (RLE) of the segmentation map
    if return_coco_annotation is set to `True`. Set to `None` if no mask if found
    above `threshold`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`segments_info` — A dictionary that contains additional information on each
    segment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`id` — An integer representing the `segment_id`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_id` — An integer representing the label / semantic class id corresponding
    to `segment_id`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score` — Prediction score of segment with `segment_id`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Converts the output of `MaskFormerForInstanceSegmentationOutput` into instance
    segmentation predictions. Only supports PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `post_process_panoptic_segmentation`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/image_processing_maskformer.py#L1193)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`outputs` (`MaskFormerForInstanceSegmentationOutput`) — The outputs from [MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threshold` (`float`, *optional*, defaults to 0.5) — The probability score
    threshold to keep predicted instance masks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.5) — Threshold to use
    when turning the predicted masks into binary values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`overlap_mask_area_threshold` (`float`, *optional*, defaults to 0.8) — The
    overlap mask area threshold to merge or discard small disconnected parts within
    each binary instance mask.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_ids_to_fuse` (`Set[int]`, *optional*) — The labels in this state will
    have all their instances be fused together. For instance we could say there can
    only be one sky in an image, but several persons, so the label ID for sky would
    be in that set, but not the one for person.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_sizes` (`List[Tuple]`, *optional*) — List of length (batch_size), where
    each list item (`Tuple[int, int]]`) corresponds to the requested final size (height,
    width) of each prediction in batch. If left to None, predictions will not be resized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Dict]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of dictionaries, one per image, each dictionary containing two keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`segmentation` — a tensor of shape `(height, width)` where each pixel represents
    a `segment_id`, set to `None` if no mask if found above `threshold`. If `target_sizes`
    is specified, segmentation is resized to the corresponding `target_sizes` entry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`segments_info` — A dictionary that contains additional information on each
    segment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`id` — an integer representing the `segment_id`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_id` — An integer representing the label / semantic class id corresponding
    to `segment_id`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`was_fused` — a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False`
    otherwise. Multiple instances of the same class / label were fused and assigned
    a single `segment_id`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score` — Prediction score of segment with `segment_id`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Converts the output of `MaskFormerForInstanceSegmentationOutput` into image
    panoptic segmentation predictions. Only supports PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: MaskFormerModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.MaskFormerModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/modeling_maskformer.py#L1597)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([MaskFormerConfig](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare MaskFormer Model outputting raw hidden-states without any specific
    head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/modeling_maskformer.py#L1611)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MaskFormerImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for pixels that are real (i.e. `not masked`),
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for pixels that are padding (i.e. `masked`).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of Detr’s decoder attention layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `~MaskFormerModelOutput`
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.maskformer.modeling_maskformer.MaskFormerModelOutput](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.models.maskformer.modeling_maskformer.MaskFormerModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.maskformer.modeling_maskformer.MaskFormerModelOutput](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.models.maskformer.modeling_maskformer.MaskFormerModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MaskFormerConfig](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Last hidden states (final feature map) of the last stage of
    the encoder model (backbone).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_decoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    num_channels, height, width)`) — Last hidden states (final feature map) of the
    last stage of the pixel decoder model (FPN).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformer_decoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`) — Last hidden states (final feature map) of the
    last stage of the transformer decoder model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the encoder model at the output of
    each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the pixel decoder model at the output
    of each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*,
    returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the transformer decoder at the output
    of each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` `tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    containing `encoder_hidden_states`, `pixel_decoder_hidden_states` and `decoder_hidden_states`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights from Detr’s decoder after the attention softmax, used to compute
    the weighted average in the self-attention heads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [MaskFormerModel](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: MaskFormerForInstanceSegmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.MaskFormerForInstanceSegmentation`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/modeling_maskformer.py#L1700)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/maskformer/modeling_maskformer.py#L1795)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [MaskFormerImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_mask` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Mask to avoid performing attention on padding pixel values. Mask values selected
    in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for pixels that are real (i.e. `not masked`),
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for pixels that are padding (i.e. `masked`).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of Detr’s decoder attention layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `~MaskFormerModelOutput`
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_labels` (`List[torch.Tensor]`, *optional*) — List of mask labels of shape
    `(num_labels, height, width)` to be fed to a model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class_labels` (`List[torch.LongTensor]`, *optional*) — list of target class
    labels of shape `(num_labels, height, width)` to be fed to a model. They identify
    the labels of `mask_labels`, e.g. the label of `mask_labels[i][j]` if `class_labels[i][j]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.maskformer.modeling_maskformer.MaskFormerForInstanceSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.models.maskformer.modeling_maskformer.MaskFormerForInstanceSegmentationOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.maskformer.modeling_maskformer.MaskFormerForInstanceSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.models.maskformer.modeling_maskformer.MaskFormerForInstanceSegmentationOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([MaskFormerConfig](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.Tensor`, *optional*) — The computed loss, returned when labels
    are present.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class_queries_logits` (`torch.FloatTensor`) — A tensor of shape `(batch_size,
    num_queries, num_labels + 1)` representing the proposed classes for each query.
    Note the `+ 1` is needed because we incorporate the null class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`masks_queries_logits` (`torch.FloatTensor`) — A tensor of shape `(batch_size,
    num_queries, height, width)` representing the proposed masks for each query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Last hidden states (final feature map) of the last stage of
    the encoder model (backbone).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_decoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    num_channels, height, width)`) — Last hidden states (final feature map) of the
    last stage of the pixel decoder model (FPN).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformer_decoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`) — Last hidden states (final feature map) of the
    last stage of the transformer decoder model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the encoder model at the output of
    each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, num_channels, height, width)`.
    Hidden-states (also called feature maps) of the pixel decoder model at the output
    of each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*,
    returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the transformer decoder at the output of each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` `tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    containing `encoder_hidden_states`, `pixel_decoder_hidden_states` and `decoder_hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights from Detr’s decoder after the attention softmax, used to compute
    the weighted average in the self-attention heads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic segmentation example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Panoptic segmentation example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
