# RetriBERT

> 原文：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/retribert`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/retribert)

此模型仅处于维护模式，因此我们不会接受任何更改其代码的新 PR。

如果您在运行此模型时遇到任何问题，请重新安装支持此模型的最后一个版本：v4.30.0。您可以通过运行以下命令来执行：`pip install -U transformers==4.30.0`。

## 概述

RetriBERT 模型是在博文[Explain Anything Like I’m Five: A Model for Open Domain Long Form Question Answering](https://yjernite.github.io/lfqa.html)中提出的。RetriBERT 是一个小型模型，使用单个或一对 BERT 编码器进行文本的稠密语义索引。

这个模型是由[yjernite](https://huggingface.co/yjernite)贡献的。可以在[这里](https://github.com/huggingface/transformers/tree/main/examples/research-projects/distillation)找到训练和使用模型的代码。

## RetriBertConfig

### `class transformers.RetriBertConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/retribert/configuration_retribert.py#L31)

```py
( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 8 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 share_encoders = True projection_dim = 128 pad_token_id = 0 **kwargs )
```

参数

+   `vocab_size` (`int`, *可选*, 默认为 30522) — RetriBERT 模型的词汇表大小。定义了在调用 RetriBertModel 时可以表示的不同标记的数量。

+   `hidden_size` (`int`, *可选*, 默认为 768) — 编码器层和池化器层的维度。

+   `num_hidden_layers` (`int`, *可选*, 默认为 12) — Transformer 编码器中隐藏层的数量。

+   `num_attention_heads` (`int`, *可选*, 默认为 12) — Transformer 编码器中每个注意力层的注意力头数。

+   `intermediate_size` (`int`, *可选*, 默认为 3072) — Transformer 编码器中“中间”（通常称为前馈）层的维度。

+   `hidden_act` (`str`或`function`, *可选*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *可选*, 默认为 0.1) — 嵌入层、编码器和池化器中所有全连接层的丢失概率。

+   `attention_probs_dropout_prob` (`float`, *可选*, 默认为 0.1) — 注意力概率的丢失比率。

+   `max_position_embeddings` (`int`, *可选*, 默认为 512) — 此模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512 或 1024 或 2048）。

+   `type_vocab_size` (`int`, *可选*, 默认为 2) — 传递给 BertModel 的*token_type_ids*的词汇表大小。

+   `initializer_range` (`float`, *可选*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *可选*, 默认为 1e-12) — 层归一化层使用的 epsilon。

+   `share_encoders` (`bool`, *可选*, 默认为`True`) — 是否使用相同的 Bert 类型编码器来处理查询和文档

+   `projection_dim` (`int`, *可选*, 默认为 128) — 投影后的查询和文档表示的最终维度。

这是用于存储 RetriBertModel 配置的配置类。根据指定的参数实例化 RetriBertModel 模型，定义模型架构。使用默认值实例化配置将产生类似于 RetriBERT [yjernite/retribert-base-uncased](https://huggingface.co/yjernite/retribert-base-uncased)架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读来自 PretrainedConfig 的文档以获取更多信息。

## RetriBertTokenizer

### `class transformers.RetriBertTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/retribert/tokenization_retribert.py#L70)

```py
( vocab_file do_lower_case = True do_basic_tokenize = True never_split = None unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs )
```

参数

+   `vocab_file` (`str`) — 包含词汇表的文件。

+   `do_lower_case` (`bool`, *可选*, 默认为 `True`) — 在标记化时是否将输入转换为小写。

+   `do_basic_tokenize` (`bool`, *可选*, 默认为 `True`) — 在 WordPiece 之前是否进行基本标记化。

+   `never_split` (`Iterable`, *可选*) — 在标记化期间永远不会拆分的标记集合。仅在`do_basic_tokenize=True`时才有效。

+   `unk_token` (`str`, *可选*, 默认为 `"[UNK]"`) — 未知标记。词汇表中不存在的标记无法转换为 ID，而是设置为此标记。

+   `sep_token` (`str`, *可选*, 默认为 `"[SEP]"`) — 分隔符标记，在从多个序列构建序列时使用，例如用于序列分类的两个序列或用于文本和问题的问题回答。它还用作使用特殊标记构建的序列的最后一个标记。

+   `pad_token` (`str`, *可选*, 默认为 `"[PAD]"`) — 用于填充的标记，例如在批处理不同长度的序列时。

+   `cls_token` (`str`, *可选*, 默认为 `"[CLS]"`) — 在进行序列分类（整个序列的分类，而不是每个标记的分类）时使用的分类器标记。构建带有特殊标记的序列时，它是序列的第一个标记。

+   `mask_token` (`str`, *可选*, 默认为 `"[MASK]"`) — 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `tokenize_chinese_chars` (`bool`, *可选*, 默认为 `True`) — 是否标记化中文字符。这可能应该在日语中停用（参见此[问题](https://github.com/huggingface/transformers/issues/328)）。

+   `strip_accents` (`bool`, *可选*) — 是否去除所有重音符号。如果未指定此选项，则将由`lowercase`的值确定（与原始 BERT 相同）。

构建一个 RetriBERT 标记器。

RetriBertTokenizer 与 BertTokenizer 相同，并运行端到端的标记化：标点符号拆分和 wordpiece。

此标记器继承自 PreTrainedTokenizer，其中包含大多数主要方法。用户应参考：此超类以获取有关这些方法的更多信息。

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/retribert/tokenization_retribert.py#L214)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 将添加特殊标记的 ID 列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个 ID 列表。

返回

`List[int]`

具有适当特殊标记的输入 ID 列表。

通过连接和添加特殊标记从序列或序列对构建用于序列分类任务的模型输入。BERT 序列具有以下格式：

+   单个序列：`[CLS] X [SEP]`

+   一对序列：`[CLS] A [SEP] B [SEP]`

#### `convert_tokens_to_string`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/retribert/tokenization_retribert.py#L208)

```py
( tokens )
```

将一系列标记（字符串）转换为单个字符串。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/retribert/tokenization_retribert.py#L269)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID 列表。

+   `token_ids_1` (`List[int]`, *optional*) — 序列对的可选第二个 ID 列表。

返回

`List[int]`

根据给定序列的标记类型 ID 列表。

从传递的两个序列创建一个用于序列对分类任务的掩码。一个 BERT 序列

对偶掩码的格式如下：

```py
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |
```

如果 `token_ids_1` 为 `None`，此方法仅返回掩码的第一部分（0s）。

#### `get_special_tokens_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/retribert/tokenization_retribert.py#L240)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID 列表。

+   `token_ids_1` (`List[int]`, *optional*) — 序列对的可选第二个 ID 列表。

+   `already_has_special_tokens` (`bool`, *optional*, 默认为 `False`) — 标记列表是否已经使用特殊标记格式化为模型。

返回

`List[int]`

一个整数列表，范围为[0, 1]：1 表示特殊标记，0 表示序列标记。

从没有添加特殊标记的标记列表中检索序列 ID。在使用分词器的 `prepare_for_model` 方法添加特殊标记时调用此方法。

## RetriBertTokenizerFast

### `class transformers.RetriBertTokenizerFast`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/retribert/tokenization_retribert_fast.py#L54)

```py
( vocab_file = None tokenizer_file = None do_lower_case = True unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs )
```

参数

+   `vocab_file` (`str`) — 包含词汇表的文件。

+   `do_lower_case` (`bool`, *optional*, 默认为 `True`) — 在标记化时是否将输入转换为小写。

+   `unk_token` (`str`, *optional*, 默认为 `"[UNK]"`) — 未知标记。词汇表中不存在的标记无法转换为 ID，而是设置为此标记。

+   `sep_token` (`str`, *optional*, 默认为 `"[SEP]"`) — 分隔符标记，在从多个序列构建序列时使用，例如用于序列分类的两个序列或用于文本和问题的问题回答。它也用作使用特殊标记构建的序列的最后一个标记。

+   `pad_token` (`str`, *optional*, 默认为 `"[PAD]"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `cls_token` (`str`, *optional*, 默认为 `"[CLS]"`) — 分类器标记，用于进行序列分类（对整个序列进行分类，而不是每个标记的分类）。在构建带有特殊标记的序列时，它是序列的第一个标记。

+   `mask_token` (`str`, *optional*, 默认为 `"[MASK]"`) — 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `clean_text` (`bool`, *optional*, 默认为 `True`) — 是否在标记化之前清理文本，通过删除所有控制字符并将所有空格替换为经典空格。

+   `tokenize_chinese_chars` (`bool`, *optional*, 默认为 `True`) — 是否对中文字符进行标记化。这对于日语可能需要停用（参见[此问题](https://github.com/huggingface/transformers/issues/328)）。

+   `strip_accents` (`bool`, *optional*) — 是否去除所有重音符号。如果未指定此选项，则将由 `lowercase` 的值确定（与原始 BERT 相同）。

+   `wordpieces_prefix` (`str`, *optional*, 默认为 `"##"`) — 子词的前缀。

构建一个“快速”RetriBERT 分词器（由 HuggingFace 的*tokenizers*库支持）。

RetriBertTokenizerFast 与 BertTokenizerFast 相同，并进行端到端的标记化：标点符号拆分和词片段。

此标记器继承自 PreTrainedTokenizerFast，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `build_inputs_with_special_tokens`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/retribert/tokenization_retribert_fast.py#L148)

```py
( token_ids_0 token_ids_1 = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 将添加特殊标记的 ID 列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个 ID 列表。

返回值

`List[int]`

具有适当特殊标记的 输入 ID 列表。

通过连接和添加特殊标记，为序列分类任务构建模型输入，BERT 序列的格式如下：

+   单个序列：`[CLS] X [SEP]`

+   序列对：`[CLS] A [SEP] B [SEP]`

#### `create_token_type_ids_from_sequences`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/retribert/tokenization_retribert_fast.py#L173)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID 列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个 ID 列表。

返回值

`List[int]`

根据给定序列的 token type IDs 列表。

从传递的两个序列创建一个用于序列对分类任务的掩码。BERT 序列

pair mask 的格式如下：

```py
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |
```

如果 `token_ids_1` 为 `None`，则此方法仅返回掩码的第一部分（0）。

## RetriBertModel

### `class transformers.RetriBertModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/retribert/modeling_retribert.py#L84)

```py
( config: RetriBertConfig )
```

参数

+   `config` (RetriBertConfig) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 from_pretrained() 方法以加载模型权重。

基于 Bert 的模型，用于嵌入查询或文档以进行文档检索。

此模型继承自 PreTrainedModel。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/retribert/modeling_retribert.py#L176)

```py
( input_ids_query: LongTensor attention_mask_query: Optional input_ids_doc: LongTensor attention_mask_doc: Optional checkpoint_batch_size: int = -1 ) → export const metadata = 'undefined';`torch.FloatTensor“
```

参数

+   `input_ids_query` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 批次中查询的输入序列标记的索引。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode() 和 PreTrainedTokenizer.`call`()。

    输入 ID 是什么？

+   `attention_mask_query`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*） - 用于避免在填充标记索引上执行注意力。在`[0, 1]`中选择的掩码值：

    +   对于未被掩码的标记为 1，

    +   对于被掩码的标记为 0。

    注意掩码是什么？

+   `input_ids_doc`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`） - 批处理中文档中词汇的输入序列标记的索引。

+   `attention_mask_doc`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*） - 用于避免在文档填充标记索引上执行注意力的掩码。

+   `checkpoint_batch_size`（`int`，*可选*，默认为`-1`） - 如果大于 0，则使用梯度检查点，在 GPU 上一次仅计算`checkpoint_batch_size`个示例的序列表示。所有查询表示仍然与批处理中的所有文档表示进行比较。

返回

`torch.FloatTensor`

在尝试将每个查询与其对应的文档以及每个文档与其对应的查询匹配时获得的双向交叉熵损失
