["```py\n>>> from transformers import ConditionalDetrConfig, ConditionalDetrModel\n\n>>> # Initializing a Conditional DETR microsoft/conditional-detr-resnet-50 style configuration\n>>> configuration = ConditionalDetrConfig()\n\n>>> # Initializing a model (with random weights) from the microsoft/conditional-detr-resnet-50 style configuration\n>>> model = ConditionalDetrModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoImageProcessor, AutoModel\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/conditional-detr-resnet-50\")\n>>> model = AutoModel.from_pretrained(\"microsoft/conditional-detr-resnet-50\")\n\n>>> # prepare image for the model\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(**inputs)\n\n>>> # the last hidden states are the final query embeddings of the Transformer decoder\n>>> # these are of shape (batch_size, num_queries, hidden_size)\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 300, 256]\n```", "```py\n>>> from transformers import AutoImageProcessor, AutoModelForObjectDetection\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/conditional-detr-resnet-50\")\n>>> model = AutoModelForObjectDetection.from_pretrained(\"microsoft/conditional-detr-resnet-50\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n\n>>> # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n>>> target_sizes = torch.tensor([image.size[::-1]])\n>>> results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[\n...     0\n... ]\n>>> for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(\n...         f\"Detected {model.config.id2label[label.item()]} with confidence \"\n...         f\"{round(score.item(), 3)} at location {box}\"\n...     )\nDetected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]\nDetected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]\nDetected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]\nDetected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]\nDetected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]\n```", "```py\n>>> import io\n>>> import requests\n>>> from PIL import Image\n>>> import torch\n>>> import numpy\n\n>>> from transformers import (\n...     AutoImageProcessor,\n...     ConditionalDetrConfig,\n...     ConditionalDetrForSegmentation,\n... )\n>>> from transformers.image_transforms import rgb_to_id\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/conditional-detr-resnet-50\")\n\n>>> # randomly initialize all weights of the model\n>>> config = ConditionalDetrConfig()\n>>> model = ConditionalDetrForSegmentation(config)\n\n>>> # prepare image for the model\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(**inputs)\n\n>>> # Use the `post_process_panoptic_segmentation` method of the `image_processor` to retrieve post-processed panoptic segmentation maps\n>>> # Segmentation results are returned as a list of dictionaries\n>>> result = image_processor.post_process_panoptic_segmentation(outputs, target_sizes=[(300, 500)])\n>>> # A tensor of shape (height, width) where each value denotes a segment id, filled with -1 if no segment is found\n>>> panoptic_seg = result[0][\"segmentation\"]\n>>> # Get prediction score and segment_id to class_id mapping of each segment\n>>> panoptic_segments_info = result[0][\"segments_info\"]\n```"]