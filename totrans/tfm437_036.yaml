- en: Monocular depth estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/monocular_depth_estimation](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/monocular_depth_estimation)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/350.b162cd30.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: Monocular depth estimation is a computer vision task that involves predicting
    the depth information of a scene from a single image. In other words, it is the
    process of estimating the distance of objects in a scene from a single camera
    viewpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Monocular depth estimation has various applications, including 3D reconstruction,
    augmented reality, autonomous driving, and robotics. It is a challenging task
    as it requires the model to understand the complex relationships between objects
    in the scene and the corresponding depth information, which can be affected by
    factors such as lighting conditions, occlusion, and texture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The task illustrated in this tutorial is supported by the following model architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[DPT](../model_doc/dpt), [GLPN](../model_doc/glpn)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this guide you’ll learn how to:'
  prefs: []
  type: TYPE_NORMAL
- en: create a depth estimation pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: run depth estimation inference by hand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Depth estimation pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The simplest way to try out inference with a model supporting depth estimation
    is to use the corresponding [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline).
    Instantiate a pipeline from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?pipeline_tag=depth-estimation&sort=downloads):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, choose an image to analyze:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Photo of a busy street](../Images/7c44a18801e68f14d4d1cb85510d1bdc.png)'
  prefs: []
  type: TYPE_IMG
- en: Pass the image to the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The pipeline returns a dictionary with two entries. The first one, called `predicted_depth`,
    is a tensor with the values being the depth expressed in meters for each pixel.
    The second one, `depth`, is a PIL image that visualizes the depth estimation result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the visualized result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Depth estimation visualization](../Images/dac09b39a795bd23817e4cff71d4b6a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Depth estimation inference by hand
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you’ve seen how to use the depth estimation pipeline, let’s see how
    we can replicate the same result by hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by loading the model and associated processor from a [checkpoint on the
    Hugging Face Hub](https://huggingface.co/models?pipeline_tag=depth-estimation&sort=downloads).
    Here we’ll use the same checkpoint as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare the image input for the model using the `image_processor` that will
    take care of the necessary image transformations such as resizing and normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass the prepared inputs through the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Visualize the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Depth estimation visualization](../Images/dac09b39a795bd23817e4cff71d4b6a8.png)'
  prefs: []
  type: TYPE_IMG
