- en: VideoMAE
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VideoMAE
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/videomae](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/videomae)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/videomae](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/videomae)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The VideoMAE model was proposed in [VideoMAE: Masked Autoencoders are Data-Efficient
    Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602)
    by Zhan Tong, Yibing Song, Jue Wang, Limin Wang. VideoMAE extends masked auto
    encoders ([MAE](vit_mae)) to video, claiming state-of-the-art performance on several
    video classification benchmarks.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'VideoMAE 模型由 Zhan Tong, Yibing Song, Jue Wang, Limin Wang 在[VideoMAE: Masked
    Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602)中提出。VideoMAE
    将遮罩自动编码器（MAE）扩展到视频，声称在几个视频分类基准上表现出色。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Pre-training video transformers on extra large-scale datasets is generally
    required to achieve premier performance on relatively small datasets. In this
    paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners
    for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE
    and propose customized video tube masking and reconstruction. These simple designs
    turn out to be effective for overcoming information leakage caused by the temporal
    correlation during video reconstruction. We obtain three important findings on
    SSVP: (1) An extremely high proportion of masking ratio (i.e., 90% to 95%) still
    yields favorable performance of VideoMAE. The temporally redundant video content
    enables higher masking ratio than that of images. (2) VideoMAE achieves impressive
    results on very small datasets (i.e., around 3k-4k videos) without using any extra
    data. This is partially ascribed to the challenging task of video reconstruction
    to enforce high-level structure learning. (3) VideoMAE shows that data quality
    is more important than data quantity for SSVP. Domain shift between pre-training
    and target datasets are important issues in SSVP. Notably, our VideoMAE with the
    vanilla ViT backbone can achieve 83.9% on Kinects-400, 75.3% on Something-Something
    V2, 90.8% on UCF101, and 61.1% on HMDB51 without using any extra data.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*通常需要在额外大规模数据集上预训练视频变换器，才能在相对小的数据集上实现最佳性能。在本文中，我们展示了视频遮罩自动编码器（VideoMAE）是自监督视频预训练（SSVP）的数据高效学习者。我们受到最近的
    ImageMAE 的启发，提出了定制的视频管道遮罩和重建。这些简单的设计对于克服视频重建过程中由时间相关性引起的信息泄漏是有效的。我们在 SSVP 上得出了三个重要发现：（1）极高比例的遮罩比率（即
    90% 到 95%）仍然能够产生 VideoMAE 的良好性能。时间上冗余的视频内容使得遮罩比率比图像更高。 （2）VideoMAE 在非常小的数据集上（即约
    3k-4k 视频）取得了令人印象深刻的结果，而没有使用任何额外数据。这部分归因于视频重建任务的挑战，以强制进行高级结构学习。 （3）VideoMAE 表明，对于
    SSVP，数据质量比数据数量更重要。预训练和目标数据集之间的领域转移是 SSVP 中的重要问题。值得注意的是，我们的 VideoMAE 与基本的 ViT 骨干可以在
    Kinects-400 上达到 83.9%，在 Something-Something V2 上达到 75.3%，在 UCF101 上达到 90.8%，在
    HMDB51 上达到 61.1%，而没有使用任何额外数据。*'
- en: '![drawing](../Images/fdd4a271138d9c750f31d9363804143a.png) VideoMAE pre-training.
    Taken from the [original paper](https://arxiv.org/abs/2203.12602).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![图示](../Images/fdd4a271138d9c750f31d9363804143a.png) VideoMAE 预训练。摘自[原始论文](https://arxiv.org/abs/2203.12602)。'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code can be found [here](https://github.com/MCG-NJU/VideoMAE).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[nielsr](https://huggingface.co/nielsr)贡献。原始代码可在[此处](https://github.com/MCG-NJU/VideoMAE)找到。
- en: Resources
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with VideoMAE. If you’re interested in submitting a resource
    to be included here, please feel free to open a Pull Request and we’ll review
    it! The resource should ideally demonstrate something new instead of duplicating
    an existing resource.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 官方 Hugging Face 和社区（由 🌎 表示）资源列表，可帮助您开始使用 VideoMAE。如果您有兴趣提交资源以包含在此处，请随时提交拉取请求，我们将进行审核！资源应该展示一些新内容，而不是重复现有资源。
- en: '**Video classification**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**视频分类**'
- en: '[A notebook](https://github.com/huggingface/notebooks/blob/main/examples/video_classification.ipynb)
    that shows how to fine-tune a VideoMAE model on a custom dataset.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个笔记本](https://github.com/huggingface/notebooks/blob/main/examples/video_classification.ipynb)，展示如何在自定义数据集上微调
    VideoMAE 模型。'
- en: '[Video classification task guide](../tasks/video_classification)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[视频分类任务指南](../tasks/video_classification)'
- en: '[A 🤗 Space](https://huggingface.co/spaces/sayakpaul/video-classification-ucf101-subset)
    showing how to perform inference with a video classification model.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个 🤗 空间](https://huggingface.co/spaces/sayakpaul/video-classification-ucf101-subset)，展示如何使用视频分类模型进行推理。'
- en: VideoMAEConfig
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VideoMAEConfig
- en: '### `class transformers.VideoMAEConfig`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.VideoMAEConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/configuration_videomae.py#L28)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/configuration_videomae.py#L28)'
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image_size` (`int`, *optional*, defaults to 224) — The size (resolution) of
    each image.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_size` (`int`, *可选*, 默认为224) — 每个图像的大小（分辨率）。'
- en: '`patch_size` (`int`, *optional*, defaults to 16) — The size (resolution) of
    each patch.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *可选*, 默认为16) — 每个补丁的大小（分辨率）。'
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *可选*, 默认为3) — 输入通道数量。'
- en: '`num_frames` (`int`, *optional*, defaults to 16) — The number of frames in
    each video.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_frames` (`int`, *可选*, 默认为16) — 每个视频中的帧数。'
- en: '`tubelet_size` (`int`, *optional*, defaults to 2) — The number of tubelets.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tubelet_size` (`int`, *可选*, 默认为2) — 管道大小。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *可选*, 默认为768) — 编码器层和池化层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *可选*, 默认为12) — Transformer 编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, 默认为12) — Transformer编码器中每个注意力层的注意力头数量。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, 默认为3072) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`或`function`, *optional*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) — The dropout
    probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, 默认为0.0) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — The
    dropout ratio for the attention probabilities.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, 默认为0.0) — 注意力概率的dropout比率。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, 默认为1e-12) — 层归一化层使用的epsilon。'
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — Whether to add a bias
    to the queries, keys and values.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_bias` (`bool`, *optional*, 默认为`True`) — 是否为查询、键和值添加偏置。'
- en: '`use_mean_pooling` (`bool`, *optional*, defaults to `True`) — Whether to mean
    pool the final hidden states instead of using the final hidden state of the [CLS]
    token.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_mean_pooling` (`bool`, *optional*, 默认为`True`) — 是否对最终隐藏状态进行均值池化，而不是使用[CLS]标记的最终隐藏状态。'
- en: '`decoder_num_attention_heads` (`int`, *optional*, defaults to 6) — Number of
    attention heads for each attention layer in the decoder.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_num_attention_heads` (`int`, *optional*, 默认为6) — 解码器中每个注意力层的注意力头数量。'
- en: '`decoder_hidden_size` (`int`, *optional*, defaults to 384) — Dimensionality
    of the decoder.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_size` (`int`, *optional*, 默认为384) — 解码器的维度。'
- en: '`decoder_num_hidden_layers` (`int`, *optional*, defaults to 4) — Number of
    hidden layers in the decoder.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_num_hidden_layers` (`int`, *optional*, 默认为4) — 解码器中的隐藏层数量。'
- en: '`decoder_intermediate_size` (`int`, *optional*, defaults to 1536) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the decoder.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_intermediate_size` (`int`, *optional*, 默认为1536) — 解码器中“中间”（即前馈）层的维度。'
- en: '`norm_pix_loss` (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the target patch pixels.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_pix_loss` (`bool`, *optional*, 默认为`True`) — 是否对目标补丁像素进行归一化。'
- en: This is the configuration class to store the configuration of a [VideoMAEModel](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEModel).
    It is used to instantiate a VideoMAE model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the VideoMAE [MCG-NJU/videomae-base](https://huggingface.co/MCG-NJU/videomae-base)
    architecture.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[VideoMAEModel](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEModel)的配置。根据指定的参数实例化一个VideoMAE模型，定义模型架构。使用默认值实例化配置将产生类似于VideoMAE[MCG-NJU/videomae-base](https://huggingface.co/MCG-NJU/videomae-base)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: VideoMAEFeatureExtractor
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VideoMAEFeatureExtractor
- en: '### `class transformers.VideoMAEFeatureExtractor`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.VideoMAEFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/feature_extraction_videomae.py#L26)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/feature_extraction_videomae.py#L26)'
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `__call__`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Preprocess an image or a batch of images.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理一张图像或一批图像。
- en: VideoMAEImageProcessor
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VideoMAEImageProcessor
- en: '### `class transformers.VideoMAEImageProcessor`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.VideoMAEImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/image_processing_videomae.py#L62)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/image_processing_videomae.py#L62)'
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Whether to resize the
    image’s (height, width) dimensions to the specified `size`. Can be overridden
    by the `do_resize` parameter in the `preprocess` method.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *optional*, 默认为`True`) — 是否将图像的（高度，宽度）尺寸调整为指定的`size`。可以被`preprocess`方法中的`do_resize`参数覆盖。'
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"shortest_edge" -- 224}`):
    Size of the output image after resizing. The shortest edge of the image will be
    resized to `size["shortest_edge"]` while maintaining the aspect ratio of the original
    image. Can be overriden by `size` in the `preprocess` method.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]` *optional*, 默认为`{"shortest_edge" -- 224}`): 调整大小后的输出图像尺寸。图像的最短边将被调整为`size["shortest_edge"]`，同时保持原始图像的纵横比。可以被`preprocess`方法中的`size`覆盖。'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`)
    — Resampling filter to use if resizing the image. Can be overridden by the `resample`
    parameter in the `preprocess` method.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *optional*, 默认为 `Resampling.BILINEAR`) —
    如果调整图像大小，则要使用的重采样滤波器。可以被 `preprocess` 方法中的 `resample` 参数覆盖。'
- en: '`do_center_crop` (`bool`, *optional*, defaults to `True`) — Whether to center
    crop the image to the specified `crop_size`. Can be overridden by the `do_center_crop`
    parameter in the `preprocess` method.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_center_crop` (`bool`, *optional*, 默认为 `True`) — 是否将图像居中裁剪到指定的 `crop_size`。可以被
    `preprocess` 方法中的 `do_center_crop` 参数覆盖。'
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `{"height" -- 224, "width":
    224}`): Size of the image after applying the center crop. Can be overridden by
    the `crop_size` parameter in the `preprocess` method.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_size` (`Dict[str, int]`, *optional*, 默认为 `{"height" -- 224, "width":
    224}`): 应用中心裁剪后的图像大小。可以被 `preprocess` 方法中的 `crop_size` 参数覆盖。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) — Whether to rescale
    the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`
    parameter in the `preprocess` method.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *optional*, 默认为 `True`) — 是否按指定比例 `rescale_factor` 重新缩放图像。可以被
    `preprocess` 方法中的 `do_rescale` 参数覆盖。'
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — Defines
    the scale factor to use if rescaling the image. Can be overridden by the `rescale_factor`
    parameter in the `preprocess` method.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`int` 或 `float`, *optional*, 默认为 `1/255`) — 定义要使用的缩放因子，如果重新缩放图像。可以被
    `preprocess` 方法中的 `rescale_factor` 参数覆盖。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the image. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *optional*, 默认为 `True`) — 是否对图像进行归一化。可以被 `preprocess`
    方法中的 `do_normalize` 参数覆盖。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`)
    — Mean to use if normalizing the image. This is a float or list of floats the
    length of the number of channels in the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` 或 `List[float]`, *optional*, 默认为 `IMAGENET_STANDARD_MEAN`)
    — 如果归一化图像，则使用的均值。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_mean` 参数覆盖。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`)
    — Standard deviation to use if normalizing the image. This is a float or list
    of floats the length of the number of channels in the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` 或 `List[float]`, *optional*, 默认为 `IMAGENET_STANDARD_STD`)
    — 如果归一化图像，则使用的标准差。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_std` 参数覆盖。'
- en: Constructs a VideoMAE image processor.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 VideoMAE 图像处理器。
- en: '#### `preprocess`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/image_processing_videomae.py#L233)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/image_processing_videomae.py#L233)'
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`ImageInput`) — Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) — 要预处理的图像。期望单个或批量图像，像素值范围为 0 到 255。如果传入像素值在 0 到 1 之间的图像，请设置
    `do_rescale=False`。'
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) — Whether to
    resize the image.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *optional*, 默认为 `self.do_resize`) — 是否调整图像大小。'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — Size of the
    image after applying resize.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`, *optional*, 默认为 `self.size`) — 调整大小后的图像尺寸。'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `self.resample`)
    — Resampling filter to use if resizing the image. This can be one of the enum
    `PILImageResampling`, Only has an effect if `do_resize` is set to `True`.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *optional*, 默认为 `self.resample`) — 如果调整图像大小，则要使用的重采样滤波器。这可以是枚举
    `PILImageResampling` 中的一个，仅在 `do_resize` 设置为 `True` 时有效。'
- en: '`do_center_crop` (`bool`, *optional*, defaults to `self.do_centre_crop`) —
    Whether to centre crop the image.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_center_crop` (`bool`, *optional*, 默认为 `self.do_centre_crop`) — 是否居中裁剪图像。'
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `self.crop_size`) —
    Size of the image after applying the centre crop.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_size` (`Dict[str, int]`, *optional*, 默认为 `self.crop_size`) — 应用中心裁剪后的图像尺寸。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — Whether
    to rescale the image values between [0 - 1].'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *optional*, 默认为 `self.do_rescale`) — 是否将图像值重新缩放在 [0 -
    1] 之间。'
- en: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) —
    Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`, *optional*, 默认为 `self.rescale_factor`) — 如果 `do_rescale`
    设置为 `True`，则重新缩放图像的缩放因子。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — Whether
    to normalize the image.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *optional*, 默认为 `self.do_normalize`) — 是否对图像进行归一化。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `self.image_mean`)
    — Image mean.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` 或 `List[float]`, *optional*, 默认为 `self.image_mean`) —
    图像均值。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `self.image_std`)
    — Image standard deviation.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` 或 `List[float]`, *optional*, 默认为 `self.image_std`) — 图像标准差。'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 `TensorType`, *optional*) — 要返回的张量类型。可以是以下之一:'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '未设置: 返回一个 `np.ndarray` 列表。'
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW` 或 `''tf''`: 返回类型为 `tf.Tensor` 的批处理。'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH` 或 `''pt''`: 返回类型为 `torch.Tensor` 的批处理。'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY` 或 `''np''`: 返回类型为 `np.ndarray` 的批处理。'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX` 或 `''jax''`: 返回类型为 `jax.numpy.ndarray` 的批处理。'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format`（`ChannelDimension` 或 `str`，*可选*，默认为 `ChannelDimension.FIRST`）—
    输出图像的通道维度格式。可以是以下之一：'
- en: '`ChannelDimension.FIRST`: image in (num_channels, height, width) format.'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.FIRST`：图像格式为（通道数，高度，宽度）。'
- en: '`ChannelDimension.LAST`: image in (height, width, num_channels) format.'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ChannelDimension.LAST`：图像格式为（高度，宽度，通道数）。'
- en: 'Unset: Use the inferred channel dimension format of the input image.'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未设置：使用推断的输入图像的通道维度格式。
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format`（`ChannelDimension` 或 `str`，*可选*）— 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` 或 `ChannelDimension.FIRST`：图像格式为（通道数，高度，宽度）。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` 或 `ChannelDimension.LAST`：图像格式为（高度，宽度，通道数）。'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"` 或 `ChannelDimension.NONE`：图像格式为（高度，宽度）。'
- en: Preprocess an image or batch of images.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对图像或图像批次进行预处理。
- en: VideoMAEModel
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VideoMAEModel
- en: '### `class transformers.VideoMAEModel`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.VideoMAEModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/modeling_videomae.py#L521)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/modeling_videomae.py#L521)'
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([VideoMAEConfig](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[VideoMAEConfig](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The bare VideoMAE Model transformer outputting raw hidden-states without any
    specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的 VideoMAE 模型变压器输出原始隐藏状态，没有特定的头部。此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/modeling_videomae.py#L552)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/modeling_videomae.py#L552)'
- en: '[PRE7]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels,
    height, width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [VideoMAEImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为 `(batch_size, num_frames, num_channels, height, width)`
    的 `torch.FloatTensor`）— 像素值。可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    获取像素值。有关详细信息，请参阅 [VideoMAEImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为 `(num_heads,)` 或 `(num_layers, num_heads)` 的 `torch.FloatTensor`，*可选*）—
    用于使自注意力模块中的选定头部失效的掩码。掩码值选定在 `[0, 1]`：'
- en: 1 indicates the head is `not masked`,
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被掩盖，
- en: 0 indicates the head is `masked`.
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被“掩盖”。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: '`bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Boolean masked positions. Indicates which patches are masked (1)
    and which aren’t (0). Each video in the batch must have the same number of masked
    patches. If `None`, then all patches are considered. Sequence length is `(num_frames
    // tubelet_size) * (image_size // patch_size) ** 2`.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bool_masked_pos`（形状为 `(batch_size, sequence_length)` 的 `torch.BoolTensor`，*可选*）—
    布尔掩码位置。指示哪些补丁被掩盖（1）哪些不被掩盖（0）。批次中的每个视频必须具有相同数量的掩盖补丁。如果为 `None`，则认为所有补丁都被考虑。序列长度为
    `(num_frames // tubelet_size) * (image_size // patch_size) ** 2`。'
- en: Returns
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([VideoMAEConfig](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEConfig))
    and inputs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [VideoMAEModel](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEModel)
    forward method, overrides the `__call__` special method.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: VideoMAEForPreTraining
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`VideoMAEForPreTraining` includes the decoder on top for self-supervised pre-training.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.VideoMAEForPreTraining`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/modeling_videomae.py#L748)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([VideoMAEConfig](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The VideoMAE Model transformer with the decoder on top for self-supervised pre-training.
    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/modeling_videomae.py#L770)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels,
    height, width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [VideoMAEImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: output_hidden_states（`bool`，*可选*） - 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: return_dict（`bool`，*可选*） - 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。
- en: '`bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`)
    — Boolean masked positions. Indicates which patches are masked (1) and which aren’t
    (0). Each video in the batch must have the same number of masked patches. Sequence
    length is `(num_frames // tubelet_size) * (image_size // patch_size) ** 2`.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bool_masked_pos（形状为`(batch_size, sequence_length)`的`torch.BoolTensor`） - 布尔掩码位置。指示哪些补丁被掩盖（1）哪些没有（0）。批次中的每个视频必须具有相同数量的掩码补丁。序列长度为`(num_frames
    // tubelet_size) * (image_size // patch_size) ** 2`。
- en: Returns
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.videomae.modeling_videomae.VideoMAEForPreTrainingOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.videomae.modeling_videomae.VideoMAEForPreTrainingOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.videomae.modeling_videomae.VideoMAEForPreTrainingOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([VideoMAEConfig](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEConfig))
    and inputs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.videomae.modeling_videomae.VideoMAEForPreTrainingOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`），包括根据配置（[VideoMAEConfig](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`) — Pixel reconstruction loss.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: loss（形状为`(1,)`的`torch.FloatTensor`） - 像素重建损失。
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, patch_size ** 2 * num_channels)`)
    — Pixel reconstruction logits.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: logits（形状为`(batch_size, patch_size ** 2 * num_channels)`的`torch.FloatTensor`）
    - 像素重建logits。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: hidden_states（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    - 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。模型在每一层的输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights after the attention softmax, used to compute the weighted average
    in the self-attention heads.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: attentions（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    - 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [VideoMAEForPreTraining](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEForPreTraining)
    forward method, overrides the `__call__` special method.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[VideoMAEForPreTraining](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEForPreTraining)前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行前处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: VideoMAEForVideoClassification
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VideoMAEForVideoClassification
- en: '### `class transformers.VideoMAEForVideoClassification`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 视频分类的VideoMAEForVideoClassification类
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/modeling_videomae.py#L932)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/modeling_videomae.py#L932)'
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([VideoMAEConfig](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: config（[VideoMAEConfig](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEConfig)）
    - 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。
- en: VideoMAE Model transformer with a video classification head on top (a linear
    layer on top of the average pooled hidden states of all tokens) e.g. for ImageNet.
    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: VideoMAE模型变压器，顶部带有视频分类头（所有令牌的平均池化隐藏状态之上的线性层），例如用于ImageNet。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/modeling_videomae.py#L951)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/videomae/modeling_videomae.py#L951)'
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels,
    height, width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [VideoMAEImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_frames, num_channels,
    height, width)`) — 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获得。有关详细信息，请参阅[VideoMAEImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*)
    — 用于使自注意力模块中选择的头部失效的掩码。掩码值选定在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`masked`。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为`(batch_size,)`，*可选*) — 用于计算图像分类/回归损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels
    > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([VideoMAEConfig](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEConfig))
    and inputs.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[VideoMAEConfig](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`损失` (`torch.FloatTensor`，形状为`(1,)`，*可选*，在提供`labels`时返回) — 分类（如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the model at the output of each stage.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，在传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层，则为嵌入的输出+每个阶段的输出）隐藏状态（也称为特征图）在每个阶段的模型输出处。'
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，在传递`output_attentions=True`或当`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, patch_size, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力 softmax 之后的注意力权重，用于计算自注意力头部的加权平均值。
- en: The [VideoMAEForVideoClassification](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEForVideoClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[VideoMAEForVideoClassification](/docs/transformers/v4.37.2/en/model_doc/videomae#transformers.VideoMAEForVideoClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
