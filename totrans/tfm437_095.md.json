["```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(task=\"audio-classification\", model=\"superb/hubert-base-superb-er\")\n>>> preds = classifier(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n>>> preds\n[{'score': 0.4532, 'label': 'hap'},\n {'score': 0.3622, 'label': 'sad'},\n {'score': 0.0943, 'label': 'neu'},\n {'score': 0.0903, 'label': 'ang'}]\n```", "```py\n>>> from transformers import pipeline\n\n>>> transcriber = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n>>> transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n```", "```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(task=\"image-classification\")\n>>> preds = classifier(\n...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n... )\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n>>> print(*preds, sep=\"\\n\")\n{'score': 0.4335, 'label': 'lynx, catamount'}\n{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}\n{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}\n{'score': 0.0239, 'label': 'Egyptian cat'}\n{'score': 0.0229, 'label': 'tiger cat'}\n```", "```py\n>>> from transformers import pipeline\n\n>>> detector = pipeline(task=\"object-detection\")\n>>> preds = detector(\n...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n... )\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"], \"box\": pred[\"box\"]} for pred in preds]\n>>> preds\n[{'score': 0.9865,\n  'label': 'cat',\n  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]\n```", "```py\n>>> from transformers import pipeline\n\n>>> segmenter = pipeline(task=\"image-segmentation\")\n>>> preds = segmenter(\n...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n... )\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n>>> print(*preds, sep=\"\\n\")\n{'score': 0.9879, 'label': 'LABEL_184'}\n{'score': 0.9973, 'label': 'snow'}\n{'score': 0.9972, 'label': 'cat'}\n```", "```py\n>>> from transformers import pipeline\n\n>>> depth_estimator = pipeline(task=\"depth-estimation\")\n>>> preds = depth_estimator(\n...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n... )\n```", "```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(task=\"sentiment-analysis\")\n>>> preds = classifier(\"Hugging Face is the best thing since sliced bread!\")\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n>>> preds\n[{'score': 0.9991, 'label': 'POSITIVE'}]\n```", "```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(task=\"ner\")\n>>> preds = classifier(\"Hugging Face is a French company based in New York City.\")\n>>> preds = [\n...     {\n...         \"entity\": pred[\"entity\"],\n...         \"score\": round(pred[\"score\"], 4),\n...         \"index\": pred[\"index\"],\n...         \"word\": pred[\"word\"],\n...         \"start\": pred[\"start\"],\n...         \"end\": pred[\"end\"],\n...     }\n...     for pred in preds\n... ]\n>>> print(*preds, sep=\"\\n\")\n{'entity': 'I-ORG', 'score': 0.9968, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}\n{'entity': 'I-ORG', 'score': 0.9293, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}\n{'entity': 'I-ORG', 'score': 0.9763, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}\n{'entity': 'I-MISC', 'score': 0.9983, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}\n{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}\n{'entity': 'I-LOC', 'score': 0.9987, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}\n{'entity': 'I-LOC', 'score': 0.9992, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}\n```", "```py\n>>> from transformers import pipeline\n\n>>> question_answerer = pipeline(task=\"question-answering\")\n>>> preds = question_answerer(\n...     question=\"What is the name of the repository?\",\n...     context=\"The name of the repository is huggingface/transformers\",\n... )\n>>> print(\n...     f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n... )\nscore: 0.9327, start: 30, end: 54, answer: huggingface/transformers\n```", "```py\n>>> from transformers import pipeline\n\n>>> summarizer = pipeline(task=\"summarization\")\n>>> summarizer(\n...     \"In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\"\n... )\n[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]\n```", "```py\n>>> from transformers import pipeline\n\n>>> text = \"translate English to French: Hugging Face is a community-based open-source platform for machine learning.\"\n>>> translator = pipeline(task=\"translation\", model=\"t5-small\")\n>>> translator(text)\n[{'translation_text': \"Hugging Face est une tribune communautaire de l'apprentissage des machines.\"}]\n```", "```py\n    >>> from transformers import pipeline\n\n    >>> prompt = \"Hugging Face is a community-based open-source platform for machine learning.\"\n    >>> generator = pipeline(task=\"text-generation\")\n    >>> generator(prompt)  # doctest: +SKIP\n    ```", "```py\n    >>> text = \"Hugging Face is a community-based open-source <mask> for machine learning.\"\n    >>> fill_mask = pipeline(task=\"fill-mask\")\n    >>> preds = fill_mask(text, top_k=1)\n    >>> preds = [\n    ...     {\n    ...         \"score\": round(pred[\"score\"], 4),\n    ...         \"token\": pred[\"token\"],\n    ...         \"token_str\": pred[\"token_str\"],\n    ...         \"sequence\": pred[\"sequence\"],\n    ...     }\n    ...     for pred in preds\n    ... ]\n    >>> preds\n    [{'score': 0.2236,\n      'token': 1761,\n      'token_str': ' platform',\n      'sequence': 'Hugging Face is a community-based open-source platform for machine learning.'}]\n    ```", "```py\n>>> from transformers import pipeline\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"https://datasets-server.huggingface.co/assets/hf-internal-testing/example-documents/--/hf-internal-testing--example-documents/test/2/image/image.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> doc_question_answerer = pipeline(\"document-question-answering\", model=\"magorshunov/layoutlm-invoices\")\n>>> preds = doc_question_answerer(\n...     question=\"What is the total amount?\",\n...     image=image,\n... )\n>>> preds\n[{'score': 0.8531, 'answer': '17,000', 'start': 4, 'end': 4}]\n```"]