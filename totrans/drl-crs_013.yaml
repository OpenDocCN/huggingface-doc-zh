- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/learn/deep-rl-course/unit1/summary](https://huggingface.co/learn/deep-rl-course/unit1/summary)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/17.ef2dc63f.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'That was a lot of information! Let’s summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning is a computational approach of learning from actions.
    We build an agent that learns from the environment **by interacting with it through
    trial and error** and receiving rewards (negative or positive) as feedback.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of any RL agent is to maximize its expected cumulative reward (also
    called expected return) because RL is based on the **reward hypothesis**, which
    is that **all goals can be described as the maximization of the expected cumulative
    reward.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RL process is a loop that outputs a sequence of **state, action, reward
    and next state.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To calculate the expected cumulative reward (expected return), we discount
    the rewards: the rewards that come sooner (at the beginning of the game) **are
    more probable to happen since they are more predictable than the long term future
    reward.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To solve an RL problem, you want to **find an optimal policy**. The policy is
    the “brain” of your agent, which will tell us **what action to take given a state.**
    The optimal policy is the one which **gives you the actions that maximize the
    expected return.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two ways to find your optimal policy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By training your policy directly: **policy-based methods.**'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By training a value function that tells us the expected return the agent will
    get at each state and use this function to define our policy: **value-based methods.**'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we speak about Deep RL because we introduce **deep neural networks
    to estimate the action to take (policy-based) or to estimate the value of a state
    (value-based)** hence the name “deep”.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
