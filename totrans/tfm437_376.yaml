- en: VisionTextDualEncoder
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VisionTextDualEncoder
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '原文链接: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The [VisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel)
    can be used to initialize a vision-text dual encoder model with any pretrained
    vision autoencoding model as the vision encoder (*e.g.* [ViT](vit), [BEiT](beit),
    [DeiT](deit)) and any pretrained text autoencoding model as the text encoder (*e.g.*
    [RoBERTa](roberta), [BERT](bert)). Two projection layers are added on top of both
    the vision and text encoder to project the output embeddings to a shared latent
    space. The projection layers are randomly initialized so the model should be fine-tuned
    on a downstream task. This model can be used to align the vision-text embeddings
    using CLIP like contrastive image-text training and then can be used for zero-shot
    vision tasks such image-classification or retrieval.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[VisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel)
    可以用于使用任何预训练的视觉自编码模型作为视觉编码器（如 [ViT](vit), [BEiT](beit), [DeiT](deit)）和任何预训练的文本自编码模型作为文本编码器（如
    [RoBERTa](roberta), [BERT](bert)）初始化视觉文本双编码器模型。在视觉和文本编码器的顶部添加了两个投影层，将输出嵌入投影到共享的潜在空间。投影层是随机初始化的，因此模型应该在下游任务上进行微调。该模型可用于使用类似
    CLIP 的对比图像文本训练来对齐视觉文本嵌入，然后可用于零样本视觉任务，如图像分类或检索。'
- en: 'In [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991)
    it is shown how leveraging pre-trained (locked/frozen) image and text model for
    contrastive learning yields significant improvement on new zero-shot vision tasks
    such as image classification or retrieval.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '在 [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991)
    中展示了如何利用预训练的（锁定/冻结）图像和文本模型进行对比学习，从而在新的零样本视觉任务（如图像分类或检索）上取得显著改进。'
- en: VisionTextDualEncoderConfig
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VisionTextDualEncoderConfig
- en: '### `class transformers.VisionTextDualEncoderConfig`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.VisionTextDualEncoderConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py#L27)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py#L27)'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`projection_dim` (`int`, *optional*, defaults to 512) — Dimentionality of text
    and vision projection layers.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projection_dim` (`int`, *optional*, 默认为 512) — 文本和视觉投影层的维度。'
- en: '`logit_scale_init_value` (`float`, *optional*, defaults to 2.6592) — The inital
    value of the *logit_scale* paramter. Default is used as per the original CLIP
    implementation.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logit_scale_init_value` (`float`, *optional*, 默认为 2.6592) — *logit_scale*
    参数的初始值。默认值根据原始 CLIP 实现使用。'
- en: '`kwargs` (*optional*) — Dictionary of keyword arguments.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*optional*) — 关键字参数的字典。'
- en: '[VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig)
    is the configuration class to store the configuration of a [VisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel).
    It is used to instantiate [VisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel)
    model according to the specified arguments, defining the text model and vision
    model configs.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig)
    是用于存储 [VisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel)
    配置的配置类。它用于根据指定的参数实例化 [VisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel)
    模型，定义文本模型和视觉模型配置。'
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    的文档以获取更多信息。
- en: 'Examples:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#### `from_vision_text_configs`'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_vision_text_configs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py#L100)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py#L100)'
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Returns
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig)'
- en: An instance of a configuration object
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象的一个实例
- en: Instantiate a [VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig)
    (or a derived class) from text model configuration and vision model configuration.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本模型配置和视觉模型配置实例化一个 [VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig)（或派生类）。
- en: VisionTextDualEncoderProcessor
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VisionTextDualEncoderProcessor
- en: '### `class transformers.VisionTextDualEncoderProcessor`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.VisionTextDualEncoderProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py#L25)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py#L25)'
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image_processor` ([AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor),
    *optional*) — The image processor is a required input.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor` ([AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor),
    *optional*) — 图像处理器是必需的输入。'
- en: '`tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer),
    *optional*) — The tokenizer is a required input.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，*可选*）—
    标记器是必需的输入。'
- en: Constructs a VisionTextDualEncoder processor which wraps an image processor
    and a tokenizer into a single processor.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个VisionTextDualEncoder处理器，将图像处理器和标记器包装成一个单一处理器。
- en: '[VisionTextDualEncoderProcessor](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor)
    offers all the functionalities of [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    and [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See the `__call__()` and [decode()](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor.decode)
    for more information.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[VisionTextDualEncoderProcessor](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor)提供了[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)和[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)的所有功能。有关更多信息，请参阅`__call__()`和[decode()](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor.decode)。'
- en: '#### `batch_decode`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py#L117)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py#L117)'
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This method forwards all its arguments to VisionTextDualEncoderTokenizer’s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法将其所有参数转发给VisionTextDualEncoderTokenizer的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。有关更多信息，请参阅此方法的文档字符串。
- en: '#### `decode`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py#L124)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py#L124)'
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This method forwards all its arguments to VisionTextDualEncoderTokenizer’s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法将其所有参数转发给VisionTextDualEncoderTokenizer的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。有关更多信息，请参阅此方法的文档字符串。
- en: PytorchHide Pytorch content
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch内容
- en: VisionTextDualEncoderModel
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VisionTextDualEncoderModel
- en: '### `class transformers.VisionTextDualEncoderModel`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.VisionTextDualEncoderModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py#L161)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py#L161)'
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: This class can be used to initialize a vision-text dual encoder model with any
    pretrained vision autoencoding model as the vision encoder and any pretrained
    text model as the text encoder. The vision and text encoders are loaded via the
    [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    method. The projection layers are automatically added to the model and should
    be fine-tuned on a downstream task, like contrastive image-text modeling.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此类可用于使用任何预训练的视觉自编码模型作为视觉编码器和任何预训练的文本模型作为文本编码器初始化视觉文本双编码器模型。视觉和文本编码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)方法加载。投影层会自动添加到模型中，并应在下游任务（如对比图像文本建模）上进行微调。
- en: 'In [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991)
    it is shown how leveraging pre-trained (locked/frozen) image and text model for
    contrastive learning yields significant improvment on new zero-shot vision tasks
    such as image classification or retrieval.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '在[LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991)中展示了如何利用预训练（锁定/冻结）图像和文本模型进行对比学习，对新的零样本视觉任务（如图像分类或检索）产生了显著的改进。'
- en: After such a Vision-Text-Dual-Encoder model has been trained/fine-tuned, it
    can be saved/loaded just like any other models (see the examples for more information).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 训练/微调了这样一个Vision-Text-Dual-Encoder模型之后，它可以像任何其他模型一样保存/加载（有关更多信息，请参阅示例）。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py#L293)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py#L293)'
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    词汇表中输入序列标记的索引。默认情况下将忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参见 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在 `[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被“masked”的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“masked”的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *可选*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为 `[0, config.max_position_embeddings - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置 ID？](../glossary#position-ids)'
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Padding will be ignored by default should you provide
    it. Pixel values can be obtained using an image processor (e.g. if you use ViT
    as the encoder, you should use [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)).
    See [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — 像素值。默认情况下将忽略填充。可以使用图像处理器获取像素值（例如，如果您使用 ViT 作为编码器，应该使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)）。有关详细信息，请参见
    [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`return_loss` (`bool`, *optional*) — Whether or not to return the contrastive
    loss.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_loss` (`bool`, *可选*) — 是否返回对比损失。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的 `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的 `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是一个普通元组。'
- en: Returns
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.clip.modeling_clip.CLIPOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.clip.modeling_clip.CLIPOutput` 或 `tuple(torch.FloatTensor)`'
- en: A `transformers.models.clip.modeling_clip.CLIPOutput` or a tuple of `torch.FloatTensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration ([VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig))
    and inputs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `transformers.models.clip.modeling_clip.CLIPOutput` 或一个 `torch.FloatTensor`
    元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含各种元素，这取决于配置（[VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss`
    is `True`) — Contrastive loss for image-text similarity.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *可选*, 当 `return_loss` 为 `True`
    时返回) — 图像-文本相似性的对比损失。'
- en: '`logits_per_image:(torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`)
    — The scaled dot product scores between `image_embeds` and `text_embeds`. This
    represents the image-text similarity scores.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_per_image:(torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`)
    — `image_embeds` 和 `text_embeds` 之间的缩放点积分数。这代表了图像-文本相似性分数。'
- en: '`logits_per_text:(torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`)
    — The scaled dot product scores between `text_embeds` and `image_embeds`. This
    represents the text-image similarity scores.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_per_text:(torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`)
    — `text_embeds` 和 `image_embeds` 之间的缩放点积分数。这代表了文本-图像相似性分数。'
- en: '`text_embeds(torch.FloatTensor` of shape `(batch_size, output_dim`) — The text
    embeddings obtained by applying the projection layer to the pooled output of [CLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel).'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_embeds(torch.FloatTensor` of shape `(batch_size, output_dim`) — The
    image embeddings obtained by applying the projection layer to the pooled output
    of [CLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel).'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_model_output(BaseModelOutputWithPooling):` The output of the [CLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel).'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vision_model_output(BaseModelOutputWithPooling):` The output of the [CLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel).'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [VisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel)
    forward method, overrides the `__call__` special method.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: TensorFlowHide TensorFlow content
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: FlaxVisionTextDualEncoderModel
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxVisionTextDualEncoderModel`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py#L219)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) —
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This class can be used to initialize a vision-text dual encoder model with any
    pretrained vision autoencoding model as the vision encoder and any pretrained
    text model as the text encoder. The vision and text encoders are loaded via the
    [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    method. The projection layers are automatically added to the model and should
    be fine-tuned on a downstream task, like contrastive image-text modeling.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'In [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991)
    it is shown how leveraging pre-trained (locked/frozen) image and text model for
    contrastive learning yields significant improvment on new zero-shot vision tasks
    such as image classification or retrieval.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: After such a Vision-Text-Dual-Encoder model has been trained/fine-tuned, it
    can be saved/loaded just like any other models (see the examples for more information).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入大小、修剪头等）。
- en: This model is also a [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)
    subclass. Use it as a regular Flax linen Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个[flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)子类。将其用作常规的Flax亚麻模块，并参考Flax文档以获取与一般用法和行为相关的所有内容。
- en: 'Finally, this model supports inherent JAX features such as:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，此模型支持内在的JAX特性，例如：
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[即时（JIT）编译](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[向量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
- en: '#### `__call__`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py#L270)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py#L270)'
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. Padding will be ignored by default
    should you provide it.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`）— 词汇表中输入序列标记的索引。默认情况下将忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0,
    1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于未被“掩码”的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于被“掩码”的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of positions of each input sequence tokens in the position embeddings.
    Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0,
    config.max_position_embeddings - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Padding will be ignored by default should you provide
    it. Pixel values can be obtained using an image processor (e.g. if you use ViT
    as the encoder, you should use [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)).
    See [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—
    像素值。默认情况下将忽略填充。可以使用图像处理器获取像素值（例如，如果您使用ViT作为编码器，则应使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)）。有关详细信息，请参阅[ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput` or a tuple of
    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig))
    and inputs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（[VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig)）和输入的各种元素。
- en: '`logits_per_image:(jnp.ndarray` of shape `(image_batch_size, text_batch_size)`)
    — The scaled dot product scores between `image_embeds` and `text_embeds`. This
    represents the image-text similarity scores.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_per_image:(jnp.ndarray` of shape `(image_batch_size, text_batch_size)`)
    — `image_embeds`和`text_embeds`之间的缩放点积分数。这代表图像-文本相似性分数。'
- en: '`logits_per_text:(jnp.ndarray` of shape `(text_batch_size, image_batch_size)`)
    — The scaled dot product scores between `text_embeds` and `image_embeds`. This
    represents the text-image similarity scores.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_per_text:(jnp.ndarray` of shape `(text_batch_size, image_batch_size)`)
    — `text_embeds`和`image_embeds`之间的缩放点积分数。这代表文本-图像相似性分数。'
- en: '`text_embeds(jnp.ndarray` of shape `(batch_size, output_dim`) — The text embeddings
    obtained by applying the projection layer to the pooled output of [FlaxCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_embeds(jnp.ndarray` of shape `(batch_size, output_dim`) — 通过将[FlaxCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel)的池化输出应用于投影层获得的文本嵌入。'
- en: '`image_embeds(jnp.ndarray` of shape `(batch_size, output_dim`) — The image
    embeddings obtained by applying the projection layer to the pooled output of [FlaxCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPVisionModel).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeds(jnp.ndarray` of shape `(batch_size, output_dim`) — 通过将[FlaxCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPVisionModel)的池化输出应用于投影层获得的图像嵌入。'
- en: '`text_model_output(FlaxBaseModelOutputWithPooling):` The output of the [FlaxCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_model_output(FlaxBaseModelOutputWithPooling):` [FlaxCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel)的输出。'
- en: '`vision_model_output(FlaxBaseModelOutputWithPooling):` The output of the [FlaxCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPVisionModel).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vision_model_output(FlaxBaseModelOutputWithPooling):` [FlaxCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPVisionModel)的输出。'
- en: The [FlaxVisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel)
    forward method, overrides the `__call__` special method.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxVisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel)的前向方法重写了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在之后调用`Module`实例，而不是调用此函数，因为前者会负责运行前后处理步骤，而后者会默默忽略它们。
- en: 'Examples:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: JAXHide JAX content
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: JAXHide JAX content
- en: TFVisionTextDualEncoderModel
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFVisionTextDualEncoderModel
- en: '### `class transformers.TFVisionTextDualEncoderModel`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFVisionTextDualEncoderModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_tf_vision_text_dual_encoder.py#L175)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_tf_vision_text_dual_encoder.py#L175)'
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)）
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: This class can be used to initialize a vision-text dual encoder model with any
    pretrained vision autoencoding model as the vision encoder and any pretrained
    text model as the text encoder. The vision and text encoders are loaded via the
    [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)
    method. The projection layers are automatically added to the model and should
    be fine-tuned on a downstream task, like contrastive image-text modeling.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此类可用于使用任何预训练的视觉自编码模型作为视觉编码器和任何预训练的文本模型作为文本编码器初始化视觉文本双编码器模型。视觉和文本编码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)方法加载。投影层会自动添加到模型中，并应在下游任务（如对比图像-文本建模）上进行微调。
- en: 'In [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991)
    it is shown how leveraging pre-trained (locked/frozen) image and text model for
    contrastive learning yields significant improvment on new zero-shot vision tasks
    such as image classification or retrieval.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '在[LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991)中展示了如何利用预训练（锁定/冻结）的图像和文本模型进行对比学习，从而在新的零样本视觉任务（如图像分类或检索）上取得显著改进。'
- en: After such a Vision-Text-Dual-Encoder model has been trained/fine-tuned, it
    can be saved/loaded just like any other models (see the examples for more information).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 经过训练/微调的Vision-Text-Dual-Encoder模型可以像其他模型一样保存/加载（有关更多信息，请参见示例）。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a Keras [Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular Keras Model and refer to the TF documentation for
    all matter related to general usage and behavior.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是Keras [Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规Keras模型，并参考TF文档以获取与一般使用和行为相关的所有信息。
- en: '#### `call`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_tf_vision_text_dual_encoder.py#L347)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_tf_vision_text_dual_encoder.py#L347)'
- en: '[PRE13]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. Padding will be ignored by default
    should you provide it.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下，如果提供了填充，将被忽略。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *可选*)
    — 避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`position_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of positions of each input sequence tokens in the position embeddings.
    Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *可选*)
    — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`pixel_values` (`tf.Tensor` of shape `(batch_size, num_channels, height, width)`)
    — Pixel values. Padding will be ignored by default should you provide it. Pixel
    values can be obtained using an image processor (e.g. if you use ViT as the encoder,
    you should use [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)).
    See [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`tf.Tensor` of shape `(batch_size, num_channels, height, width)`)
    — 像素值。默认情况下，如果提供了填充，将被忽略。可以使用图像处理器获取像素值（例如，如果您使用ViT作为编码器，应该使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)）。有关详细信息，请参阅[ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`return_loss` (`bool`, *optional*) — Whether or not to return the contrastive
    loss.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_loss` (`bool`, *可选*) — 是否返回对比损失。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.clip.modeling_tf_clip.TFCLIPOutput` or `tuple(tf.Tensor)`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.clip.modeling_tf_clip.TFCLIPOutput`或`tuple(tf.Tensor)`'
- en: A `transformers.models.clip.modeling_tf_clip.TFCLIPOutput` or a tuple of `tf.Tensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration ([VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig))
    and inputs.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.clip.modeling_tf_clip.TFCLIPOutput`或一组`tf.Tensor`（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig)）和输入的各种元素。
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `return_loss`
    is `True`) — Contrastive loss for image-text similarity.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`tf.Tensor` of shape `(1,)`, *可选*, 当`return_loss`为`True`时返回) — 图像-文本相似性的对比损失。'
- en: '`logits_per_image:(tf.Tensor` of shape `(image_batch_size, text_batch_size)`)
    — The scaled dot product scores between `image_embeds` and `text_embeds`. This
    represents the image-text similarity scores.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_per_image:(tf.Tensor` of shape `(image_batch_size, text_batch_size)`)
    — `image_embeds`和`text_embeds`之间的缩放点积分数。这代表图像-文本相似性分数。'
- en: '`logits_per_text:(tf.Tensor` of shape `(text_batch_size, image_batch_size)`)
    — The scaled dot product scores between `text_embeds` and `image_embeds`. This
    represents the text-image similarity scores.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_per_text:(tf.Tensor` of shape `(text_batch_size, image_batch_size)`)
    — `text_embeds`和`image_embeds`之间的缩放点积分数。这代表文本-图像相似性分数。'
- en: '`text_embeds(tf.Tensor` of shape `(batch_size, output_dim`) — The text embeddings
    obtained by applying the projection layer to the pooled output of [TFCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPTextModel).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_embeds(tf.Tensor` of shape `(batch_size, output_dim`) — 通过将投影层应用于[TFCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPTextModel)的汇总输出获得的文本嵌入。'
- en: '`image_embeds(tf.Tensor` of shape `(batch_size, output_dim`) — The image embeddings
    obtained by applying the projection layer to the pooled output of [TFCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPVisionModel).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeds(tf.Tensor` of shape `(batch_size, output_dim`) — 通过将投影层应用于[TFCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPVisionModel)的汇总输出获得的图像嵌入。'
- en: '`text_model_output(~modeling_tf_utils.TFBaseModelOutputWithPooling):` The output
    of the [TFCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPTextModel).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_model_output(~modeling_tf_utils.TFBaseModelOutputWithPooling):` [TFCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPTextModel)的输出。'
- en: '`vision_model_output(~modeling_tf_utils.TFBaseModelOutputWithPooling):` The
    output of the [TFCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPVisionModel).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vision_model_output(~modeling_tf_utils.TFBaseModelOutputWithPooling):` [TFCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPVisionModel)的输出。'
- en: The [TFVisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.TFVisionTextDualEncoderModel)
    forward method, overrides the `__call__` special method.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFVisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.TFVisionTextDualEncoderModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Examples:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
