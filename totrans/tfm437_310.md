# MusicGen

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/musicgen](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/musicgen)

## 概述

MusicGen模型是由Jade Copet、Felix Kreuk、Itai Gat、Tal Remez、David Kant、Gabriel Synnaeve、Yossi Adi和Alexandre Défossez在论文[Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)中提出的。

MusicGen是一个单阶段自回归Transformer模型，能够生成高质量的音乐样本，其条件是文本描述或音频提示。文本描述通过一个冻结的文本编码器模型传递，以获得一系列隐藏状态表示。然后训练MusicGen来预测离散的音频标记，或称为*音频代码*，这些标记是通过音频压缩模型（如EnCodec）解码以恢复音频波形。

通过高效的标记交错模式，MusicGen不需要自监督的文本/音频提示语义表示，从而消除了预测一组码书（例如分层或上采样）所需级联多个模型的需要。相反，它能够在单次前向传递中生成所有码书。

论文摘要如下：

*我们解决了条件音乐生成的任务。我们引入了MusicGen，一个单一语言模型（LM），它在几个流的压缩离散音乐表示（即标记）上运行。与以往的工作不同，MusicGen由单阶段Transformer LM和高效的标记交错模式组成，消除了级联多个模型的需要，例如分层或上采样。遵循这种方法，我们展示了MusicGen如何能够生成高质量的样本，同时在文本描述或旋律特征的条件下，允许更好地控制生成的输出。我们进行了广泛的实证评估，考虑了自动和人类研究，显示所提出的方法在标准文本到音乐基准上优于评估的基线。通过消融研究，我们阐明了构成MusicGen的每个组件的重要性。*

该模型由[sanchit-gandhi](https://huggingface.co/sanchit-gandhi)贡献。原始代码可以在[这里](https://github.com/facebookresearch/audiocraft)找到。预训练检查点可以在[Hugging Face Hub](https://huggingface.co/models?sort=downloads&search=facebook%2Fmusicgen-)上找到。

## 使用提示

+   在从[这里](https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md#importing--exporting-models)下载原始检查点后，您可以使用位于`src/transformers/models/musicgen/convert_musicgen_transformers.py`的**转换脚本**进行转换，命令如下：

```py
python src/transformers/models/musicgen/convert_musicgen_transformers.py \
    --checkpoint small --pytorch_dump_folder /output/path --safe_serialization 
```

## 生成

MusicGen兼容两种生成模式：贪婪和抽样。实际上，抽样比贪婪产生的结果显著更好，因此我们鼓励尽可能使用抽样模式。抽样默认启用，并且可以通过在调用`MusicgenForConditionalGeneration.generate()`时设置`do_sample=True`来明确指定，或通过覆盖模型的生成配置（见下文）来指定。

生成受正弦位置嵌入的限制，输入限制为30秒。也就是说，MusicGen不能生成超过30秒的音频（1503个标记），输入音频通过音频提示生成也会对此限制有所贡献，因此，给定20秒的音频输入，MusicGen不能生成超过额外10秒的音频。

Transformers支持MusicGen的单声道（1通道）和立体声（2通道）变体。单声道版本生成一组代码书。立体声版本生成2组代码书，每个通道（左/右）各一个，并且每组代码书通过音频压缩模型独立解码。每个通道的音频流合并以产生最终的立体声输出。

### 无条件生成

无条件（或'null'）生成的输入可以通过方法`MusicgenForConditionalGeneration.get_unconditional_inputs()`获得：

```py
>>> from transformers import MusicgenForConditionalGeneration

>>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")
>>> unconditional_inputs = model.get_unconditional_inputs(num_samples=1)

>>> audio_values = model.generate(**unconditional_inputs, do_sample=True, max_new_tokens=256)
```

音频输出是一个形状为`(batch_size, num_channels, sequence_length)`的三维Torch张量。要听生成的音频样本，可以在ipynb笔记本中播放它们：

```py
from IPython.display import Audio

sampling_rate = model.config.audio_encoder.sampling_rate
Audio(audio_values[0].numpy(), rate=sampling_rate)
```

或者使用第三方库（例如`scipy`）将它们保存为`.wav`文件：

```py
>>> import scipy

>>> sampling_rate = model.config.audio_encoder.sampling_rate
>>> scipy.io.wavfile.write("musicgen_out.wav", rate=sampling_rate, data=audio_values[0, 0].numpy())
```

### 文本条件生成

模型可以通过使用[MusicgenProcessor](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor)预处理输入来生成受文本提示条件的音频样本：

```py
>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration

>>> processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
>>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

>>> inputs = processor(
...     text=["80s pop track with bassy drums and synth", "90s rock song with loud guitars and heavy drums"],
...     padding=True,
...     return_tensors="pt",
... )
>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)
```

`guidance_scale`用于分类器自由引导（CFG），设置条件对数（从文本提示预测）和无条件对数（从无条件或'null'提示预测）之间的权重。更高的引导比例鼓励模型生成更与输入提示密切相关的样本，通常以音频质量较差为代价。通过设置`guidance_scale > 1`启用CFG。为获得最佳结果，请使用`guidance_scale=3`（默认值）。

### 音频提示生成

相同的[MusicgenProcessor](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor)可用于预处理用于音频延续的音频提示。在以下示例中，我们使用🤗 Datasets库加载音频文件，可以通过以下命令进行pip安装：

```py
pip install --upgrade pip
pip install datasets[audio]
```

```py
>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
>>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

>>> dataset = load_dataset("sanchit-gandhi/gtzan", split="train", streaming=True)
>>> sample = next(iter(dataset))["audio"]

>>> # take the first half of the audio sample
>>> sample["array"] = sample["array"][: len(sample["array"]) // 2]

>>> inputs = processor(
...     audio=sample["array"],
...     sampling_rate=sample["sampling_rate"],
...     text=["80s blues track with groovy saxophone"],
...     padding=True,
...     return_tensors="pt",
... )
>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)
```

对于批量音频提示生成，可以通过使用[MusicgenProcessor](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor)类对生成的`audio_values`进行后处理，以去除填充：

```py
>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
>>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

>>> dataset = load_dataset("sanchit-gandhi/gtzan", split="train", streaming=True)
>>> sample = next(iter(dataset))["audio"]

>>> # take the first quarter of the audio sample
>>> sample_1 = sample["array"][: len(sample["array"]) // 4]

>>> # take the first half of the audio sample
>>> sample_2 = sample["array"][: len(sample["array"]) // 2]

>>> inputs = processor(
...     audio=[sample_1, sample_2],
...     sampling_rate=sample["sampling_rate"],
...     text=["80s blues track with groovy saxophone", "90s rock song with loud guitars and heavy drums"],
...     padding=True,
...     return_tensors="pt",
... )
>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)

>>> # post-process to remove padding from the batched audio
>>> audio_values = processor.batch_decode(audio_values, padding_mask=inputs.padding_mask)
```

### 生成配置

控制生成过程的默认参数，例如采样、引导比例和生成的标记数量，可以在模型的生成配置中找到，并根据需要进行更新：

```py
>>> from transformers import MusicgenForConditionalGeneration

>>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

>>> # inspect the default generation config
>>> model.generation_config

>>> # increase the guidance scale to 4.0
>>> model.generation_config.guidance_scale = 4.0

>>> # decrease the max length to 256 tokens
>>> model.generation_config.max_length = 256
```

请注意，传递给生成方法的任何参数都将**覆盖**生成配置中的参数，因此在调用生成时设置`do_sample=False`将覆盖生成配置中`model.generation_config.do_sample`的设置。

## 模型结构

MusicGen模型可以分解为三个不同的阶段：

1.  文本编码器：将文本输入映射到一系列隐藏状态表示。预训练的MusicGen模型使用来自T5或Flan-T5的冻结文本编码器

1.  MusicGen解码器：一个语言模型（LM），根据编码器隐藏状态表示自回归生成音频标记（或代码）

1.  音频编码器/解码器：用于将音频提示编码为提示标记，并通过解码器预测的音频标记恢复音频波形

因此，MusicGen模型可以作为独立的解码器模型使用，对应于类[MusicgenForCausalLM](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForCausalLM)，或作为包含文本编码器和音频编码器/解码器的复合模型使用，对应于类[MusicgenForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForConditionalGeneration)。如果只需从预训练检查点加载解码器，则可以首先指定正确的配置，或通过复合模型的`.decoder`属性访问：

```py
>>> from transformers import AutoConfig, MusicgenForCausalLM, MusicgenForConditionalGeneration

>>> # Option 1: get decoder config and pass to `.from_pretrained`
>>> decoder_config = AutoConfig.from_pretrained("facebook/musicgen-small").decoder
>>> decoder = MusicgenForCausalLM.from_pretrained("facebook/musicgen-small", **decoder_config)

>>> # Option 2: load the entire composite model, but only return the decoder
>>> decoder = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small").decoder
```

由于文本编码器和音频编码器/解码器模型在训练期间被冻结，MusicGen解码器[MusicgenForCausalLM](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForCausalLM)可以在编码器隐藏状态和音频代码的数据集上独立训练。对于推断，训练好的解码器可以与冻结的文本编码器和音频编码器/解码器结合，以恢复复合[MusicgenForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForConditionalGeneration)模型。

提示：

+   MusicGen是在Encodec的32kHz检查点上训练的。您应确保使用Encodec模型的兼容版本。

+   采样模式往往比贪婪模式提供更好的结果 - 您可以在调用`MusicgenForConditionalGeneration.generate()`时使用变量`do_sample`切换采样。

## MusicgenDecoderConfig

### `class transformers.MusicgenDecoderConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/configuration_musicgen.py#L30)

```py
( vocab_size = 2048 max_position_embeddings = 2048 num_hidden_layers = 24 ffn_dim = 4096 num_attention_heads = 16 layerdrop = 0.0 use_cache = True activation_function = 'gelu' hidden_size = 1024 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 initializer_factor = 0.02 scale_embedding = False num_codebooks = 4 audio_channels = 1 pad_token_id = 2048 bos_token_id = 2048 eos_token_id = None tie_word_embeddings = False **kwargs )
```

参数

+   `vocab_size`（`int`，*可选*，默认为2048）— MusicgenDecoder模型的词汇量。定义了在调用`MusicgenDecoder`时传递的`inputs_ids`可以表示的不同标记数量。

+   `hidden_size`（`int`，*可选*，默认为1024）— 层和池化层的维度。

+   `num_hidden_layers`（`int`，*可选*，默认为24）— 解码器层数。

+   `num_attention_heads`（`int`，*可选*，默认为16）— Transformer块中每个注意力层的注意力头数。

+   `ffn_dim`（`int`，*可选*，默认为4096）— Transformer块中“中间”（通常称为前馈）层的维度。

+   `activation_function`（`str`或`function`，*可选*，默认为`"gelu"`）— 解码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。

+   `dropout`（`float`，*可选*，默认为0.1）— 嵌入、文本编码器和池化器中所有全连接层的dropout概率。

+   `attention_dropout`（`float`，*可选*，默认为0.0）— 注意力概率的dropout比率。

+   `activation_dropout`（`float`，*可选*，默认为0.0）— 全连接层内部激活的dropout比率。

+   `max_position_embeddings`（`int`，*可选*，默认为2048）— 该模型可能被使用的最大序列长度。通常情况下，将其设置为一个较大的值以防万一（例如512、1024或2048）。

+   `initializer_factor`（`float`，*可选*，默认为0.02）— 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layerdrop`（`float`，*可选*，默认为0.0）— 解码器的LayerDrop概率。有关更多详细信息，请参阅[LayerDrop论文](见[https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))。

+   `scale_embedding`（`bool`，*可选*，默认为`False`）— 通过将其除以sqrt(hidden_size)来缩放嵌入。

+   `use_cache`（`bool`，*可选*，默认为`True`）— 模型是否应返回最后的键/值注意力（并非所有模型都使用）。

+   `num_codebooks`（`int`，*可选*，默认为4）— 转发到模型的并行码书的数量。

+   `tie_word_embeddings(bool,` *可选*，默认为`False`）— 输入和输出词嵌入是否应该绑定。

+   `audio_channels`（`int`，*可选*，默认为1）— 音频数据中的通道数。单声道为1，立体声为2。立体声模型为左/右输出通道生成单独的音频流。单声道模型生成单个音频流输出。

这是用于存储 `MusicgenDecoder` 配置的配置类。根据指定的参数实例化一个 MusicGen 解码器，定义模型架构。使用默认值实例化配置将产生类似于 MusicGen [facebook/musicgen-small](https://huggingface.co/facebook/musicgen-small) 架构的配置。

配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 的文档获取更多信息。

## MusicgenConfig

### `class transformers.MusicgenConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/configuration_musicgen.py#L139)

```py
( **kwargs )
```

参数

+   `kwargs` (*optional*) — 关键字参数的字典。特别是：

    +   `text_encoder` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig), *optional*) — 定义文本编码器配置的配置对象实例。

    +   `audio_encoder` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig), *optional*) — 定义音频编码器配置的配置对象实例。

    +   `decoder` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig), *optional*) — 定义解码器配置的配置对象实例。

这是用于存储 [MusicgenModel](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenModel) 配置的配置类。根据指定的参数实例化一个 MusicGen 模型，定义文本编码器、音频编码器和 MusicGen 解码器配置。

配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 的文档获取更多信息。

示例：

```py
>>> from transformers import (
...     MusicgenConfig,
...     MusicgenDecoderConfig,
...     T5Config,
...     EncodecConfig,
...     MusicgenForConditionalGeneration,
... )

>>> # Initializing text encoder, audio encoder, and decoder model configurations
>>> text_encoder_config = T5Config()
>>> audio_encoder_config = EncodecConfig()
>>> decoder_config = MusicgenDecoderConfig()

>>> configuration = MusicgenConfig.from_sub_models_config(
...     text_encoder_config, audio_encoder_config, decoder_config
... )

>>> # Initializing a MusicgenForConditionalGeneration (with random weights) from the facebook/musicgen-small style configuration
>>> model = MusicgenForConditionalGeneration(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
>>> config_text_encoder = model.config.text_encoder
>>> config_audio_encoder = model.config.audio_encoder
>>> config_decoder = model.config.decoder

>>> # Saving the model, including its configuration
>>> model.save_pretrained("musicgen-model")

>>> # loading model and config from pretrained folder
>>> musicgen_config = MusicgenConfig.from_pretrained("musicgen-model")
>>> model = MusicgenForConditionalGeneration.from_pretrained("musicgen-model", config=musicgen_config)
```

#### `from_sub_models_config`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/configuration_musicgen.py#L217)

```py
( text_encoder_config: PretrainedConfig audio_encoder_config: PretrainedConfig decoder_config: MusicgenDecoderConfig **kwargs ) → export const metadata = 'undefined';MusicgenConfig
```

返回

[MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)

配置对象实例

从文本编码器、音频编码器和解码器配置实例化一个 [MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)（或派生类）。

## MusicgenProcessor

### `class transformers.MusicgenProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/processing_musicgen.py#L26)

```py
( feature_extractor tokenizer )
```

参数

+   `feature_extractor` (`EncodecFeatureExtractor`) — 一个 [EncodecFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecFeatureExtractor) 的实例。特征提取器是一个必需的输入。

+   `tokenizer` (`T5Tokenizer`) — 一个 [T5Tokenizer](/docs/transformers/v4.37.2/en/model_doc/mt5#transformers.T5Tokenizer) 的实例。这是一个必需的输入。

构建一个 MusicGen 处理器，将 EnCodec 特征提取器和 T5 分词器封装成一个单一的处理器类。

[MusicgenProcessor](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor) 提供了 [EncodecFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecFeatureExtractor) 和 `TTokenizer` 的所有功能。查看 `__call__()` 和 [decode()](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor.decode) 获取更多信息。

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/processing_musicgen.py#L90)

```py
( *args **kwargs )
```

此方法用于解码来自MusicGen模型的音频输出批次或来自标记器的标记id批次。在解码标记id的情况下，此方法将其所有参数转发到T5Tokenizer的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。请参考此方法的文档字符串以获取更多信息。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/processing_musicgen.py#L108)

```py
( *args **kwargs )
```

此方法将其所有参数转发到T5Tokenizer的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。请参考此方法的文档字符串以获取更多信息。

## MusicgenModel

### `class transformers.MusicgenModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L835)

```py
( config: MusicgenDecoderConfig )
```

参数

+   `config`（[MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸的Musicgen解码器模型，输出没有特定头部的原始隐藏状态。

Musicgen模型是由Jade Copet、Felix Kreuk、Itai Gat、Tal Remez、David Kant、Gabriel Synnaeve、Yossi Adi、Alexandre Défossez在[Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)中提出的。它是一个在条件音乐生成任务上训练的编码器解码器变换器。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L855)

```py
( input_ids: LongTensor = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids`（形状为`(batch_size * num_codebooks, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引，对应于音频代码序列。

    通过使用音频编码器模型对音频提示进行编码以预测音频代码，可以获得索引，例如使用[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)。有关详细信息，请参阅[EncodecModel.encode()](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel.encode)。

    [什么是输入ID？](../glossary#input-ids)

    `input_ids`将在前向传递中自动从形状`(batch_size * num_codebooks, target_sequence_length)`转换为`(batch_size, num_codebooks, target_sequence_length)`。如果您从音频编码模型（如[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)）获取音频代码，请确保帧数等于1，并且在将其作为`input_ids`传递之前，将音频代码从`(frames, batch_size, num_codebooks, target_sequence_length)`重塑为`(batch_size * num_codebooks, target_sequence_length)`。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`中：

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `encoder_hidden_states`（形状为`(batch_size, encoder_sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 编码器最后一层的隐藏状态序列的输出。用于解码器的交叉注意力。

+   `encoder_attention_mask`（形状为`(batch_size, encoder_sequence_length)`的`torch.LongTensor`，*可选*）— 用于避免在编码器输入标记的填充标记索引上执行交叉注意力的掩码。掩码值选在`[0, 1]`中：

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）— 用于使注意力模块的选定头部失效的掩码。掩码值选在`[0, 1]`中：

    +   1表示头部未被`masked`，

    +   0表示头部被`masked`。

+   `cross_attn_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）— 用于使解码器中交叉注意力模块的选定头部失效的掩码，以避免在隐藏头部上执行交叉注意力。掩码值选在`[0, 1]`中：

    +   1表示头部未被`masked`，

    +   0表示头部被`masked`。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）— 长度为`config.n_layers`的`tuple(torch.FloatTensor)`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

    如果使用了`past_key_values`，用户可以选择仅输入最后一个形状为`(batch_size, 1)`的`decoder_input_ids`（那些没有将它们的过去键值状态提供给此模型的）而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

[MusicgenModel](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenModel)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

## MusicgenForCausalLM

### `class transformers.MusicgenForCausalLM`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L906)

```py
( config: MusicgenDecoderConfig )
```

参数

+   `config`（[MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)）— 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

MusicGen解码器模型在顶部带有语言建模头。

Musicgen模型由Jade Copet、Felix Kreuk、Itai Gat、Tal Remez、David Kant、Gabriel Synnaeve、Yossi Adi、Alexandre Défossez在[Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)中提出。这是一个在条件音乐生成任务上训练的编码器解码器变换器。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库实现的通用方法，例如下载或保存，调整输入嵌入大小，修剪头等。

该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L942)

```py
( input_ids: LongTensor = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids`（形状为`(batch_size * num_codebooks, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引，对应于音频代码序列。

    可以通过使用音频编码器模型对音频提示进行编码来获取索引，以预测音频代码，例如使用[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)。查看[EncodecModel.encode()](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel.encode)获取详细信息。

    [什么是输入ID？](../glossary#input-ids)

    `input_ids`将在前向传递中自动从形状`(batch_size * num_codebooks, target_sequence_length)`转换为`(batch_size, num_codebooks, target_sequence_length)`。如果您从音频编码模型（例如[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)）获取音频代码，请确保帧数等于1，并且在将音频代码从`(frames, batch_size, num_codebooks, target_sequence_length)`重塑为`(batch_size * num_codebooks, target_sequence_length)`之前，将其作为`input_ids`传递。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`中：

    +   1表示`未被掩盖`的标记，

    +   0表示`被掩盖`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `encoder_hidden_states`（形状为`(batch_size, encoder_sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 编码器最后一层的隐藏状态序列。用于解码器的交叉注意力。

+   `encoder_attention_mask`（形状为`(batch_size, encoder_sequence_length)`的`torch.LongTensor`，*可选*）— 用于避免在编码器输入ID的填充标记索引上执行交叉注意力的掩码。掩码值选在`[0, 1]`中：

    +   1表示`未被掩盖`的标记，

    +   0表示`被掩盖`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）— 用于使注意力模块的选定头部无效的掩码。掩码值选在`[0, 1]`中：

    +   1表示头部`未被掩盖`，

    +   0表示头部被`掩盖`。

+   `cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*) — 用于将解码器中交叉注意力模块的选定头部置零的掩码，以避免在隐藏头部上执行交叉注意力。掩码值在`[0, 1]`中选择：

    +   1表示头部未被`masked`。

    +   0表示头部被`masked`。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（请参见`past_key_values`输入）。

    如果使用了`past_key_values`，用户可以选择仅输入形状为`(batch_size, 1)`的最后`decoder_input_ids`（那些没有将它们的过去键值状态提供给此模型的）而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

+   `labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于语言建模的标签。请注意，模型内部**移动**标签，即您可以设置`labels = input_ids`。索引在`[-100, 0, ..., config.vocab_size]`中选择。所有设置为`-100`的标签都被忽略（被`masked`），损失仅计算标签在`[0, ..., config.vocab_size]`中的标签。

    返回：[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)或`tuple(torch.FloatTensor)`：一个[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`，或者当`config.return_dict=False`时）包含根据配置（[MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)）和输入的各种元素。 

    +   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) — 语言建模损失。

    +   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头部的预测分数（SoftMax之前每个词汇标记的分数）。

    +   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

        包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

    +   `decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有嵌入层，+ 一个用于每个层的输出）。

        解码器在每一层的输出隐藏状态以及初始嵌入输出。

    +   `decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。

        解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

    +   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。

        解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

    +   `encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 模型编码器最后一层的隐藏状态序列输出。

    +   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有嵌入层，+ 一个用于每个层的输出）。

        编码器在每一层的输出隐藏状态以及初始嵌入输出。

    +   `encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。

        编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

[MusicgenForCausalLM](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForCausalLM)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

## MusicgenForConditionalGeneration

### `class transformers.MusicgenForConditionalGeneration`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L1404)

```py
( config: Optional = None text_encoder: Optional = None audio_encoder: Optional = None decoder: Optional = None )
```

参数

+   `config` ([MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

具有文本编码器、音频编码器和Musicgen解码器的复合MusicGen模型，用于具有文本和/或音频提示的音乐生成任务。

Musicgen模型由Jade Copet、Felix Kreuk、Itai Gat、Tal Remez、David Kant、Gabriel Synnaeve、Yossi Adi、Alexandre Défossez在[Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)中提出。它是一个在条件音乐生成任务上训练的编码器解码器transformer。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。请查看超类文档，了解库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

该模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L1760)

```py
( input_ids: Optional = None attention_mask: Optional = None input_values: Optional = None padding_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None past_key_values: Tuple = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs ) → export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。默认情况下将忽略填充。如果提供填充，则默认将忽略。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值为`[0, 1]`：

    +   1表示未被`masked`的标记。

    +   0表示被`masked`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `decoder_input_ids`（形状为`(batch_size * num_codebooks, target_sequence_length)`的`torch.LongTensor`，*可选*）— 词汇表中解码器输入序列标记的索引，对应于音频代码序列。

    可以通过使用音频编码器模型对音频提示进行编码以预测音频代码来获取索引，例如使用[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)。有关详细信息，请参阅[EncodecModel.encode()](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel.encode)。

    [什么是解码器输入ID？](../glossary#decoder-input-ids)

    `decoder_input_ids`将在前向传递中自动从形状`(batch_size * num_codebooks, target_sequence_length)`转换为`(batch_size, num_codebooks, target_sequence_length)`。如果您从音频编码模型（如[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)）获取音频代码，请确保帧数等于1，并且在将音频代码从`(frames, batch_size, num_codebooks, target_sequence_length)`重塑为`(batch_size * num_codebooks, target_sequence_length)`之前，将其作为`decoder_input_ids`传递。

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）— 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。

+   `head_mask`（形状为`(encoder_layers, encoder_attention_heads)`的`torch.Tensor`，*可选*）— 用于使编码器中注意力模块的选定头部失效的掩码。选择的掩码值为`[0, 1]`。

    +   1表示头部未被`masked`。

    +   0表示头部被`masked`。

+   `decoder_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）— 用于使解码器中注意力模块的选定头部失效的掩码。选择的掩码值为`[0, 1]`。

    +   1表示头部未被`masked`。

    +   0表示头部被`masked`。

+   `cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*) — 在解码器中将交叉注意力模块的选定头部置零的掩码。掩码值选定在`[0, 1]`中：

    +   1表示头部未被掩盖，

    +   0表示头部被掩盖。

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组包括(`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state`的形状为`(batch_size, sequence_length, hidden_size)`，*optional*)是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含可以用于加速顺序解码的预计算隐藏状态（自注意力块和交叉注意力块中的键和值）（请参见`past_key_values`输入）。

    如果使用`past_key_values`，用户可以选择仅输入形状为`(batch_size, 1)`的最后一个`decoder_input_ids`（那些没有将其过去的键值状态提供给此模型的）而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示而不是传递`decoder_input_ids`。如果使用`past_key_values`，则可以选择仅输入最后一个`decoder_inputs_embeds`（参见`past_key_values`）。这很有用，如果您想要更多控制如何将`decoder_input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵。

    如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。

+   `use_cache` (`bool`, *optional*) — 如果设置为`True`，将返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput) 或 `tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包括根据配置（[MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)）和输入的各种元素。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) — 语言建模损失。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）- 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）- 长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个和每层输出的一个）。

    每层解码器的隐藏状态加上初始嵌入输出。

+   `decoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个和每层输出的一个）。

    每层编码器的隐藏状态加上初始嵌入输出。

+   `encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

[MusicgenForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForConditionalGeneration)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration
>>> import torch

>>> processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
>>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

>>> inputs = processor(
...     text=["80s pop track with bassy drums and synth", "90s rock song with loud guitars and heavy drums"],
...     padding=True,
...     return_tensors="pt",
... )

>>> pad_token_id = model.generation_config.pad_token_id
>>> decoder_input_ids = (
...     torch.ones((inputs.input_ids.shape[0] * model.decoder.num_codebooks, 1), dtype=torch.long)
...     * pad_token_id
... )

>>> logits = model(**inputs, decoder_input_ids=decoder_input_ids).logits
>>> logits.shape  # (bsz * num_codebooks, tgt_len, vocab_size)
torch.Size([8, 1, 2048])
```
