# MusicGen

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/musicgen](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/musicgen)

## æ¦‚è¿°

MusicGenæ¨¡å‹æ˜¯ç”±Jade Copetã€Felix Kreukã€Itai Gatã€Tal Remezã€David Kantã€Gabriel Synnaeveã€Yossi Adiå’ŒAlexandre DÃ©fossezåœ¨è®ºæ–‡[Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)ä¸­æå‡ºçš„ã€‚

MusicGenæ˜¯ä¸€ä¸ªå•é˜¶æ®µè‡ªå›å½’Transformeræ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„éŸ³ä¹æ ·æœ¬ï¼Œå…¶æ¡ä»¶æ˜¯æ–‡æœ¬æè¿°æˆ–éŸ³é¢‘æç¤ºã€‚æ–‡æœ¬æè¿°é€šè¿‡ä¸€ä¸ªå†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹ä¼ é€’ï¼Œä»¥è·å¾—ä¸€ç³»åˆ—éšè—çŠ¶æ€è¡¨ç¤ºã€‚ç„¶åè®­ç»ƒMusicGenæ¥é¢„æµ‹ç¦»æ•£çš„éŸ³é¢‘æ ‡è®°ï¼Œæˆ–ç§°ä¸º*éŸ³é¢‘ä»£ç *ï¼Œè¿™äº›æ ‡è®°æ˜¯é€šè¿‡éŸ³é¢‘å‹ç¼©æ¨¡å‹ï¼ˆå¦‚EnCodecï¼‰è§£ç ä»¥æ¢å¤éŸ³é¢‘æ³¢å½¢ã€‚

é€šè¿‡é«˜æ•ˆçš„æ ‡è®°äº¤é”™æ¨¡å¼ï¼ŒMusicGenä¸éœ€è¦è‡ªç›‘ç£çš„æ–‡æœ¬/éŸ³é¢‘æç¤ºè¯­ä¹‰è¡¨ç¤ºï¼Œä»è€Œæ¶ˆé™¤äº†é¢„æµ‹ä¸€ç»„ç ä¹¦ï¼ˆä¾‹å¦‚åˆ†å±‚æˆ–ä¸Šé‡‡æ ·ï¼‰æ‰€éœ€çº§è”å¤šä¸ªæ¨¡å‹çš„éœ€è¦ã€‚ç›¸åï¼Œå®ƒèƒ½å¤Ÿåœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­ç”Ÿæˆæ‰€æœ‰ç ä¹¦ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*æˆ‘ä»¬è§£å†³äº†æ¡ä»¶éŸ³ä¹ç”Ÿæˆçš„ä»»åŠ¡ã€‚æˆ‘ä»¬å¼•å…¥äº†MusicGenï¼Œä¸€ä¸ªå•ä¸€è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ï¼Œå®ƒåœ¨å‡ ä¸ªæµçš„å‹ç¼©ç¦»æ•£éŸ³ä¹è¡¨ç¤ºï¼ˆå³æ ‡è®°ï¼‰ä¸Šè¿è¡Œã€‚ä¸ä»¥å¾€çš„å·¥ä½œä¸åŒï¼ŒMusicGenç”±å•é˜¶æ®µTransformer LMå’Œé«˜æ•ˆçš„æ ‡è®°äº¤é”™æ¨¡å¼ç»„æˆï¼Œæ¶ˆé™¤äº†çº§è”å¤šä¸ªæ¨¡å‹çš„éœ€è¦ï¼Œä¾‹å¦‚åˆ†å±‚æˆ–ä¸Šé‡‡æ ·ã€‚éµå¾ªè¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å±•ç¤ºäº†MusicGenå¦‚ä½•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„æ ·æœ¬ï¼ŒåŒæ—¶åœ¨æ–‡æœ¬æè¿°æˆ–æ—‹å¾‹ç‰¹å¾çš„æ¡ä»¶ä¸‹ï¼Œå…è®¸æ›´å¥½åœ°æ§åˆ¶ç”Ÿæˆçš„è¾“å‡ºã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®è¯è¯„ä¼°ï¼Œè€ƒè™‘äº†è‡ªåŠ¨å’Œäººç±»ç ”ç©¶ï¼Œæ˜¾ç¤ºæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ ‡å‡†æ–‡æœ¬åˆ°éŸ³ä¹åŸºå‡†ä¸Šä¼˜äºè¯„ä¼°çš„åŸºçº¿ã€‚é€šè¿‡æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬é˜æ˜äº†æ„æˆMusicGençš„æ¯ä¸ªç»„ä»¶çš„é‡è¦æ€§ã€‚*

è¯¥æ¨¡å‹ç”±[sanchit-gandhi](https://huggingface.co/sanchit-gandhi)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/facebookresearch/audiocraft)æ‰¾åˆ°ã€‚é¢„è®­ç»ƒæ£€æŸ¥ç‚¹å¯ä»¥åœ¨[Hugging Face Hub](https://huggingface.co/models?sort=downloads&search=facebook%2Fmusicgen-)ä¸Šæ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   åœ¨ä»[è¿™é‡Œ](https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md#importing--exporting-models)ä¸‹è½½åŸå§‹æ£€æŸ¥ç‚¹åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä½äº`src/transformers/models/musicgen/convert_musicgen_transformers.py`çš„**è½¬æ¢è„šæœ¬**è¿›è¡Œè½¬æ¢ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š

```py
python src/transformers/models/musicgen/convert_musicgen_transformers.py \
    --checkpoint small --pytorch_dump_folder /output/path --safe_serialization 
```

## ç”Ÿæˆ

MusicGenå…¼å®¹ä¸¤ç§ç”Ÿæˆæ¨¡å¼ï¼šè´ªå©ªå’ŒæŠ½æ ·ã€‚å®é™…ä¸Šï¼ŒæŠ½æ ·æ¯”è´ªå©ªäº§ç”Ÿçš„ç»“æœæ˜¾è‘—æ›´å¥½ï¼Œå› æ­¤æˆ‘ä»¬é¼“åŠ±å°½å¯èƒ½ä½¿ç”¨æŠ½æ ·æ¨¡å¼ã€‚æŠ½æ ·é»˜è®¤å¯ç”¨ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡åœ¨è°ƒç”¨`MusicgenForConditionalGeneration.generate()`æ—¶è®¾ç½®`do_sample=True`æ¥æ˜ç¡®æŒ‡å®šï¼Œæˆ–é€šè¿‡è¦†ç›–æ¨¡å‹çš„ç”Ÿæˆé…ç½®ï¼ˆè§ä¸‹æ–‡ï¼‰æ¥æŒ‡å®šã€‚

ç”Ÿæˆå—æ­£å¼¦ä½ç½®åµŒå…¥çš„é™åˆ¶ï¼Œè¾“å…¥é™åˆ¶ä¸º30ç§’ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒMusicGenä¸èƒ½ç”Ÿæˆè¶…è¿‡30ç§’çš„éŸ³é¢‘ï¼ˆ1503ä¸ªæ ‡è®°ï¼‰ï¼Œè¾“å…¥éŸ³é¢‘é€šè¿‡éŸ³é¢‘æç¤ºç”Ÿæˆä¹Ÿä¼šå¯¹æ­¤é™åˆ¶æœ‰æ‰€è´¡çŒ®ï¼Œå› æ­¤ï¼Œç»™å®š20ç§’çš„éŸ³é¢‘è¾“å…¥ï¼ŒMusicGenä¸èƒ½ç”Ÿæˆè¶…è¿‡é¢å¤–10ç§’çš„éŸ³é¢‘ã€‚

Transformersæ”¯æŒMusicGençš„å•å£°é“ï¼ˆ1é€šé“ï¼‰å’Œç«‹ä½“å£°ï¼ˆ2é€šé“ï¼‰å˜ä½“ã€‚å•å£°é“ç‰ˆæœ¬ç”Ÿæˆä¸€ç»„ä»£ç ä¹¦ã€‚ç«‹ä½“å£°ç‰ˆæœ¬ç”Ÿæˆ2ç»„ä»£ç ä¹¦ï¼Œæ¯ä¸ªé€šé“ï¼ˆå·¦/å³ï¼‰å„ä¸€ä¸ªï¼Œå¹¶ä¸”æ¯ç»„ä»£ç ä¹¦é€šè¿‡éŸ³é¢‘å‹ç¼©æ¨¡å‹ç‹¬ç«‹è§£ç ã€‚æ¯ä¸ªé€šé“çš„éŸ³é¢‘æµåˆå¹¶ä»¥äº§ç”Ÿæœ€ç»ˆçš„ç«‹ä½“å£°è¾“å‡ºã€‚

### æ— æ¡ä»¶ç”Ÿæˆ

æ— æ¡ä»¶ï¼ˆæˆ–'null'ï¼‰ç”Ÿæˆçš„è¾“å…¥å¯ä»¥é€šè¿‡æ–¹æ³•`MusicgenForConditionalGeneration.get_unconditional_inputs()`è·å¾—ï¼š

```py
>>> from transformers import MusicgenForConditionalGeneration

>>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")
>>> unconditional_inputs = model.get_unconditional_inputs(num_samples=1)

>>> audio_values = model.generate(**unconditional_inputs, do_sample=True, max_new_tokens=256)
```

éŸ³é¢‘è¾“å‡ºæ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_channels, sequence_length)`çš„ä¸‰ç»´Torchå¼ é‡ã€‚è¦å¬ç”Ÿæˆçš„éŸ³é¢‘æ ·æœ¬ï¼Œå¯ä»¥åœ¨ipynbç¬”è®°æœ¬ä¸­æ’­æ”¾å®ƒä»¬ï¼š

```py
from IPython.display import Audio

sampling_rate = model.config.audio_encoder.sampling_rate
Audio(audio_values[0].numpy(), rate=sampling_rate)
```

æˆ–è€…ä½¿ç”¨ç¬¬ä¸‰æ–¹åº“ï¼ˆä¾‹å¦‚`scipy`ï¼‰å°†å®ƒä»¬ä¿å­˜ä¸º`.wav`æ–‡ä»¶ï¼š

```py
>>> import scipy

>>> sampling_rate = model.config.audio_encoder.sampling_rate
>>> scipy.io.wavfile.write("musicgen_out.wav", rate=sampling_rate, data=audio_values[0, 0].numpy())
```

### æ–‡æœ¬æ¡ä»¶ç”Ÿæˆ

æ¨¡å‹å¯ä»¥é€šè¿‡ä½¿ç”¨[MusicgenProcessor](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor)é¢„å¤„ç†è¾“å…¥æ¥ç”Ÿæˆå—æ–‡æœ¬æç¤ºæ¡ä»¶çš„éŸ³é¢‘æ ·æœ¬ï¼š

```py
>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration

>>> processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
>>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

>>> inputs = processor(
...     text=["80s pop track with bassy drums and synth", "90s rock song with loud guitars and heavy drums"],
...     padding=True,
...     return_tensors="pt",
... )
>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)
```

`guidance_scale`ç”¨äºåˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆCFGï¼‰ï¼Œè®¾ç½®æ¡ä»¶å¯¹æ•°ï¼ˆä»æ–‡æœ¬æç¤ºé¢„æµ‹ï¼‰å’Œæ— æ¡ä»¶å¯¹æ•°ï¼ˆä»æ— æ¡ä»¶æˆ–'null'æç¤ºé¢„æµ‹ï¼‰ä¹‹é—´çš„æƒé‡ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´ä¸è¾“å…¥æç¤ºå¯†åˆ‡ç›¸å…³çš„æ ·æœ¬ï¼Œé€šå¸¸ä»¥éŸ³é¢‘è´¨é‡è¾ƒå·®ä¸ºä»£ä»·ã€‚é€šè¿‡è®¾ç½®`guidance_scale > 1`å¯ç”¨CFGã€‚ä¸ºè·å¾—æœ€ä½³ç»“æœï¼Œè¯·ä½¿ç”¨`guidance_scale=3`ï¼ˆé»˜è®¤å€¼ï¼‰ã€‚

### éŸ³é¢‘æç¤ºç”Ÿæˆ

ç›¸åŒçš„[MusicgenProcessor](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor)å¯ç”¨äºé¢„å¤„ç†ç”¨äºéŸ³é¢‘å»¶ç»­çš„éŸ³é¢‘æç¤ºã€‚åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ğŸ¤— Datasetsåº“åŠ è½½éŸ³é¢‘æ–‡ä»¶ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¿›è¡Œpipå®‰è£…ï¼š

```py
pip install --upgrade pip
pip install datasets[audio]
```

```py
>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
>>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

>>> dataset = load_dataset("sanchit-gandhi/gtzan", split="train", streaming=True)
>>> sample = next(iter(dataset))["audio"]

>>> # take the first half of the audio sample
>>> sample["array"] = sample["array"][: len(sample["array"]) // 2]

>>> inputs = processor(
...     audio=sample["array"],
...     sampling_rate=sample["sampling_rate"],
...     text=["80s blues track with groovy saxophone"],
...     padding=True,
...     return_tensors="pt",
... )
>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)
```

å¯¹äºæ‰¹é‡éŸ³é¢‘æç¤ºç”Ÿæˆï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨[MusicgenProcessor](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor)ç±»å¯¹ç”Ÿæˆçš„`audio_values`è¿›è¡Œåå¤„ç†ï¼Œä»¥å»é™¤å¡«å……ï¼š

```py
>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
>>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

>>> dataset = load_dataset("sanchit-gandhi/gtzan", split="train", streaming=True)
>>> sample = next(iter(dataset))["audio"]

>>> # take the first quarter of the audio sample
>>> sample_1 = sample["array"][: len(sample["array"]) // 4]

>>> # take the first half of the audio sample
>>> sample_2 = sample["array"][: len(sample["array"]) // 2]

>>> inputs = processor(
...     audio=[sample_1, sample_2],
...     sampling_rate=sample["sampling_rate"],
...     text=["80s blues track with groovy saxophone", "90s rock song with loud guitars and heavy drums"],
...     padding=True,
...     return_tensors="pt",
... )
>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)

>>> # post-process to remove padding from the batched audio
>>> audio_values = processor.batch_decode(audio_values, padding_mask=inputs.padding_mask)
```

### ç”Ÿæˆé…ç½®

æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹çš„é»˜è®¤å‚æ•°ï¼Œä¾‹å¦‚é‡‡æ ·ã€å¼•å¯¼æ¯”ä¾‹å’Œç”Ÿæˆçš„æ ‡è®°æ•°é‡ï¼Œå¯ä»¥åœ¨æ¨¡å‹çš„ç”Ÿæˆé…ç½®ä¸­æ‰¾åˆ°ï¼Œå¹¶æ ¹æ®éœ€è¦è¿›è¡Œæ›´æ–°ï¼š

```py
>>> from transformers import MusicgenForConditionalGeneration

>>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

>>> # inspect the default generation config
>>> model.generation_config

>>> # increase the guidance scale to 4.0
>>> model.generation_config.guidance_scale = 4.0

>>> # decrease the max length to 256 tokens
>>> model.generation_config.max_length = 256
```

è¯·æ³¨æ„ï¼Œä¼ é€’ç»™ç”Ÿæˆæ–¹æ³•çš„ä»»ä½•å‚æ•°éƒ½å°†**è¦†ç›–**ç”Ÿæˆé…ç½®ä¸­çš„å‚æ•°ï¼Œå› æ­¤åœ¨è°ƒç”¨ç”Ÿæˆæ—¶è®¾ç½®`do_sample=False`å°†è¦†ç›–ç”Ÿæˆé…ç½®ä¸­`model.generation_config.do_sample`çš„è®¾ç½®ã€‚

## æ¨¡å‹ç»“æ„

MusicGenæ¨¡å‹å¯ä»¥åˆ†è§£ä¸ºä¸‰ä¸ªä¸åŒçš„é˜¶æ®µï¼š

1.  æ–‡æœ¬ç¼–ç å™¨ï¼šå°†æ–‡æœ¬è¾“å…¥æ˜ å°„åˆ°ä¸€ç³»åˆ—éšè—çŠ¶æ€è¡¨ç¤ºã€‚é¢„è®­ç»ƒçš„MusicGenæ¨¡å‹ä½¿ç”¨æ¥è‡ªT5æˆ–Flan-T5çš„å†»ç»“æ–‡æœ¬ç¼–ç å™¨

1.  MusicGenè§£ç å™¨ï¼šä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ï¼Œæ ¹æ®ç¼–ç å™¨éšè—çŠ¶æ€è¡¨ç¤ºè‡ªå›å½’ç”ŸæˆéŸ³é¢‘æ ‡è®°ï¼ˆæˆ–ä»£ç ï¼‰

1.  éŸ³é¢‘ç¼–ç å™¨/è§£ç å™¨ï¼šç”¨äºå°†éŸ³é¢‘æç¤ºç¼–ç ä¸ºæç¤ºæ ‡è®°ï¼Œå¹¶é€šè¿‡è§£ç å™¨é¢„æµ‹çš„éŸ³é¢‘æ ‡è®°æ¢å¤éŸ³é¢‘æ³¢å½¢

å› æ­¤ï¼ŒMusicGenæ¨¡å‹å¯ä»¥ä½œä¸ºç‹¬ç«‹çš„è§£ç å™¨æ¨¡å‹ä½¿ç”¨ï¼Œå¯¹åº”äºç±»[MusicgenForCausalLM](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForCausalLM)ï¼Œæˆ–ä½œä¸ºåŒ…å«æ–‡æœ¬ç¼–ç å™¨å’ŒéŸ³é¢‘ç¼–ç å™¨/è§£ç å™¨çš„å¤åˆæ¨¡å‹ä½¿ç”¨ï¼Œå¯¹åº”äºç±»[MusicgenForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForConditionalGeneration)ã€‚å¦‚æœåªéœ€ä»é¢„è®­ç»ƒæ£€æŸ¥ç‚¹åŠ è½½è§£ç å™¨ï¼Œåˆ™å¯ä»¥é¦–å…ˆæŒ‡å®šæ­£ç¡®çš„é…ç½®ï¼Œæˆ–é€šè¿‡å¤åˆæ¨¡å‹çš„`.decoder`å±æ€§è®¿é—®ï¼š

```py
>>> from transformers import AutoConfig, MusicgenForCausalLM, MusicgenForConditionalGeneration

>>> # Option 1: get decoder config and pass to `.from_pretrained`
>>> decoder_config = AutoConfig.from_pretrained("facebook/musicgen-small").decoder
>>> decoder = MusicgenForCausalLM.from_pretrained("facebook/musicgen-small", **decoder_config)

>>> # Option 2: load the entire composite model, but only return the decoder
>>> decoder = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small").decoder
```

ç”±äºæ–‡æœ¬ç¼–ç å™¨å’ŒéŸ³é¢‘ç¼–ç å™¨/è§£ç å™¨æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´è¢«å†»ç»“ï¼ŒMusicGenè§£ç å™¨[MusicgenForCausalLM](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForCausalLM)å¯ä»¥åœ¨ç¼–ç å™¨éšè—çŠ¶æ€å’ŒéŸ³é¢‘ä»£ç çš„æ•°æ®é›†ä¸Šç‹¬ç«‹è®­ç»ƒã€‚å¯¹äºæ¨æ–­ï¼Œè®­ç»ƒå¥½çš„è§£ç å™¨å¯ä»¥ä¸å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨å’ŒéŸ³é¢‘ç¼–ç å™¨/è§£ç å™¨ç»“åˆï¼Œä»¥æ¢å¤å¤åˆ[MusicgenForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForConditionalGeneration)æ¨¡å‹ã€‚

æç¤ºï¼š

+   MusicGenæ˜¯åœ¨Encodecçš„32kHzæ£€æŸ¥ç‚¹ä¸Šè®­ç»ƒçš„ã€‚æ‚¨åº”ç¡®ä¿ä½¿ç”¨Encodecæ¨¡å‹çš„å…¼å®¹ç‰ˆæœ¬ã€‚

+   é‡‡æ ·æ¨¡å¼å¾€å¾€æ¯”è´ªå©ªæ¨¡å¼æä¾›æ›´å¥½çš„ç»“æœ - æ‚¨å¯ä»¥åœ¨è°ƒç”¨`MusicgenForConditionalGeneration.generate()`æ—¶ä½¿ç”¨å˜é‡`do_sample`åˆ‡æ¢é‡‡æ ·ã€‚

## MusicgenDecoderConfig

### `class transformers.MusicgenDecoderConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/configuration_musicgen.py#L30)

```py
( vocab_size = 2048 max_position_embeddings = 2048 num_hidden_layers = 24 ffn_dim = 4096 num_attention_heads = 16 layerdrop = 0.0 use_cache = True activation_function = 'gelu' hidden_size = 1024 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 initializer_factor = 0.02 scale_embedding = False num_codebooks = 4 audio_channels = 1 pad_token_id = 2048 bos_token_id = 2048 eos_token_id = None tie_word_embeddings = False **kwargs )
```

å‚æ•°

+   `vocab_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º2048ï¼‰â€” MusicgenDecoderæ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨`MusicgenDecoder`æ—¶ä¼ é€’çš„`inputs_ids`å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚

+   `hidden_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1024ï¼‰â€” å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º24ï¼‰â€” è§£ç å™¨å±‚æ•°ã€‚

+   `num_attention_heads`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º16ï¼‰â€” Transformerå—ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `ffn_dim`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º4096ï¼‰â€” Transformerå—ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `activation_function`ï¼ˆ`str`æˆ–`function`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"gelu"`ï¼‰â€” è§£ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"silu"`å’Œ`"gelu_new"`ã€‚

+   `dropout`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.1ï¼‰â€” åµŒå…¥ã€æ–‡æœ¬ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„dropoutæ¦‚ç‡ã€‚

+   `attention_dropout`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.0ï¼‰â€” æ³¨æ„åŠ›æ¦‚ç‡çš„dropoutæ¯”ç‡ã€‚

+   `activation_dropout`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.0ï¼‰â€” å…¨è¿æ¥å±‚å†…éƒ¨æ¿€æ´»çš„dropoutæ¯”ç‡ã€‚

+   `max_position_embeddings`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º2048ï¼‰â€” è¯¥æ¨¡å‹å¯èƒ½è¢«ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œå°†å…¶è®¾ç½®ä¸ºä¸€ä¸ªè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚512ã€1024æˆ–2048ï¼‰ã€‚

+   `initializer_factor`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.02ï¼‰â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layerdrop`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.0ï¼‰â€” è§£ç å™¨çš„LayerDropæ¦‚ç‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[LayerDropè®ºæ–‡](è§[https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))ã€‚

+   `scale_embedding`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” é€šè¿‡å°†å…¶é™¤ä»¥sqrt(hidden_size)æ¥ç¼©æ”¾åµŒå…¥ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚

+   `num_codebooks`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º4ï¼‰â€” è½¬å‘åˆ°æ¨¡å‹çš„å¹¶è¡Œç ä¹¦çš„æ•°é‡ã€‚

+   `tie_word_embeddings(bool,` *å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” è¾“å…¥å’Œè¾“å‡ºè¯åµŒå…¥æ˜¯å¦åº”è¯¥ç»‘å®šã€‚

+   `audio_channels`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1ï¼‰â€” éŸ³é¢‘æ•°æ®ä¸­çš„é€šé“æ•°ã€‚å•å£°é“ä¸º1ï¼Œç«‹ä½“å£°ä¸º2ã€‚ç«‹ä½“å£°æ¨¡å‹ä¸ºå·¦/å³è¾“å‡ºé€šé“ç”Ÿæˆå•ç‹¬çš„éŸ³é¢‘æµã€‚å•å£°é“æ¨¡å‹ç”Ÿæˆå•ä¸ªéŸ³é¢‘æµè¾“å‡ºã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ `MusicgenDecoder` é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª MusicGen è§£ç å™¨ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº MusicGen [facebook/musicgen-small](https://huggingface.co/facebook/musicgen-small) æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) çš„æ–‡æ¡£è·å–æ›´å¤šä¿¡æ¯ã€‚

## MusicgenConfig

### `class transformers.MusicgenConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/configuration_musicgen.py#L139)

```py
( **kwargs )
```

å‚æ•°

+   `kwargs` (*optional*) â€” å…³é”®å­—å‚æ•°çš„å­—å…¸ã€‚ç‰¹åˆ«æ˜¯ï¼š

    +   `text_encoder` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig), *optional*) â€” å®šä¹‰æ–‡æœ¬ç¼–ç å™¨é…ç½®çš„é…ç½®å¯¹è±¡å®ä¾‹ã€‚

    +   `audio_encoder` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig), *optional*) â€” å®šä¹‰éŸ³é¢‘ç¼–ç å™¨é…ç½®çš„é…ç½®å¯¹è±¡å®ä¾‹ã€‚

    +   `decoder` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig), *optional*) â€” å®šä¹‰è§£ç å™¨é…ç½®çš„é…ç½®å¯¹è±¡å®ä¾‹ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ [MusicgenModel](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenModel) é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª MusicGen æ¨¡å‹ï¼Œå®šä¹‰æ–‡æœ¬ç¼–ç å™¨ã€éŸ³é¢‘ç¼–ç å™¨å’Œ MusicGen è§£ç å™¨é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) çš„æ–‡æ¡£è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import (
...     MusicgenConfig,
...     MusicgenDecoderConfig,
...     T5Config,
...     EncodecConfig,
...     MusicgenForConditionalGeneration,
... )

>>> # Initializing text encoder, audio encoder, and decoder model configurations
>>> text_encoder_config = T5Config()
>>> audio_encoder_config = EncodecConfig()
>>> decoder_config = MusicgenDecoderConfig()

>>> configuration = MusicgenConfig.from_sub_models_config(
...     text_encoder_config, audio_encoder_config, decoder_config
... )

>>> # Initializing a MusicgenForConditionalGeneration (with random weights) from the facebook/musicgen-small style configuration
>>> model = MusicgenForConditionalGeneration(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
>>> config_text_encoder = model.config.text_encoder
>>> config_audio_encoder = model.config.audio_encoder
>>> config_decoder = model.config.decoder

>>> # Saving the model, including its configuration
>>> model.save_pretrained("musicgen-model")

>>> # loading model and config from pretrained folder
>>> musicgen_config = MusicgenConfig.from_pretrained("musicgen-model")
>>> model = MusicgenForConditionalGeneration.from_pretrained("musicgen-model", config=musicgen_config)
```

#### `from_sub_models_config`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/configuration_musicgen.py#L217)

```py
( text_encoder_config: PretrainedConfig audio_encoder_config: PretrainedConfig decoder_config: MusicgenDecoderConfig **kwargs ) â†’ export const metadata = 'undefined';MusicgenConfig
```

è¿”å›

[MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)

é…ç½®å¯¹è±¡å®ä¾‹

ä»æ–‡æœ¬ç¼–ç å™¨ã€éŸ³é¢‘ç¼–ç å™¨å’Œè§£ç å™¨é…ç½®å®ä¾‹åŒ–ä¸€ä¸ª [MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)ï¼ˆæˆ–æ´¾ç”Ÿç±»ï¼‰ã€‚

## MusicgenProcessor

### `class transformers.MusicgenProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/processing_musicgen.py#L26)

```py
( feature_extractor tokenizer )
```

å‚æ•°

+   `feature_extractor` (`EncodecFeatureExtractor`) â€” ä¸€ä¸ª [EncodecFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecFeatureExtractor) çš„å®ä¾‹ã€‚ç‰¹å¾æå–å™¨æ˜¯ä¸€ä¸ªå¿…éœ€çš„è¾“å…¥ã€‚

+   `tokenizer` (`T5Tokenizer`) â€” ä¸€ä¸ª [T5Tokenizer](/docs/transformers/v4.37.2/en/model_doc/mt5#transformers.T5Tokenizer) çš„å®ä¾‹ã€‚è¿™æ˜¯ä¸€ä¸ªå¿…éœ€çš„è¾“å…¥ã€‚

æ„å»ºä¸€ä¸ª MusicGen å¤„ç†å™¨ï¼Œå°† EnCodec ç‰¹å¾æå–å™¨å’Œ T5 åˆ†è¯å™¨å°è£…æˆä¸€ä¸ªå•ä¸€çš„å¤„ç†å™¨ç±»ã€‚

[MusicgenProcessor](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor) æä¾›äº† [EncodecFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecFeatureExtractor) å’Œ `TTokenizer` çš„æ‰€æœ‰åŠŸèƒ½ã€‚æŸ¥çœ‹ `__call__()` å’Œ [decode()](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenProcessor.decode) è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `batch_decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/processing_musicgen.py#L90)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•ç”¨äºè§£ç æ¥è‡ªMusicGenæ¨¡å‹çš„éŸ³é¢‘è¾“å‡ºæ‰¹æ¬¡æˆ–æ¥è‡ªæ ‡è®°å™¨çš„æ ‡è®°idæ‰¹æ¬¡ã€‚åœ¨è§£ç æ ‡è®°idçš„æƒ…å†µä¸‹ï¼Œæ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°T5Tokenizerçš„[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)ã€‚è¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/processing_musicgen.py#L108)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°T5Tokenizerçš„[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)ã€‚è¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

## MusicgenModel

### `class transformers.MusicgenModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L835)

```py
( config: MusicgenDecoderConfig )
```

å‚æ•°

+   `config`ï¼ˆ[MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)ï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„Musicgenè§£ç å™¨æ¨¡å‹ï¼Œè¾“å‡ºæ²¡æœ‰ç‰¹å®šå¤´éƒ¨çš„åŸå§‹éšè—çŠ¶æ€ã€‚

Musicgenæ¨¡å‹æ˜¯ç”±Jade Copetã€Felix Kreukã€Itai Gatã€Tal Remezã€David Kantã€Gabriel Synnaeveã€Yossi Adiã€Alexandre DÃ©fossezåœ¨[Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)ä¸­æå‡ºçš„ã€‚å®ƒæ˜¯ä¸€ä¸ªåœ¨æ¡ä»¶éŸ³ä¹ç”Ÿæˆä»»åŠ¡ä¸Šè®­ç»ƒçš„ç¼–ç å™¨è§£ç å™¨å˜æ¢å™¨ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¯¥æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ªPyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L855)

```py
( input_ids: LongTensor = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size * num_codebooks, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ï¼Œå¯¹åº”äºéŸ³é¢‘ä»£ç åºåˆ—ã€‚

    é€šè¿‡ä½¿ç”¨éŸ³é¢‘ç¼–ç å™¨æ¨¡å‹å¯¹éŸ³é¢‘æç¤ºè¿›è¡Œç¼–ç ä»¥é¢„æµ‹éŸ³é¢‘ä»£ç ï¼Œå¯ä»¥è·å¾—ç´¢å¼•ï¼Œä¾‹å¦‚ä½¿ç”¨[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[EncodecModel.encode()](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel.encode)ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

    `input_ids`å°†åœ¨å‰å‘ä¼ é€’ä¸­è‡ªåŠ¨ä»å½¢çŠ¶`(batch_size * num_codebooks, target_sequence_length)`è½¬æ¢ä¸º`(batch_size, num_codebooks, target_sequence_length)`ã€‚å¦‚æœæ‚¨ä»éŸ³é¢‘ç¼–ç æ¨¡å‹ï¼ˆå¦‚[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)ï¼‰è·å–éŸ³é¢‘ä»£ç ï¼Œè¯·ç¡®ä¿å¸§æ•°ç­‰äº1ï¼Œå¹¶ä¸”åœ¨å°†å…¶ä½œä¸º`input_ids`ä¼ é€’ä¹‹å‰ï¼Œå°†éŸ³é¢‘ä»£ç ä»`(frames, batch_size, num_codebooks, target_sequence_length)`é‡å¡‘ä¸º`(batch_size * num_codebooks, target_sequence_length)`ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¸­ï¼š

    +   å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º1ï¼Œ

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º0ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `encoder_hidden_states`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, encoder_sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—çš„è¾“å‡ºã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `encoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, encoder_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨ç¼–ç å™¨è¾“å…¥æ ‡è®°çš„å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œäº¤å‰æ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¸­ï¼š

    +   å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º1ï¼Œ

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º0ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¸­ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `cross_attn_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è§£ç å™¨ä¸­äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ï¼Œä»¥é¿å…åœ¨éšè—å¤´éƒ¨ä¸Šæ‰§è¡Œäº¤å‰æ³¨æ„åŠ›ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¸­ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, 1)`çš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰è€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

[MusicgenModel](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

## MusicgenForCausalLM

### `class transformers.MusicgenForCausalLM`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L906)

```py
( config: MusicgenDecoderConfig )
```

å‚æ•°

+   `config`ï¼ˆ[MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)ï¼‰â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

MusicGenè§£ç å™¨æ¨¡å‹åœ¨é¡¶éƒ¨å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´ã€‚

Musicgenæ¨¡å‹ç”±Jade Copetã€Felix Kreukã€Itai Gatã€Tal Remezã€David Kantã€Gabriel Synnaeveã€Yossi Adiã€Alexandre DÃ©fossezåœ¨[Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)ä¸­æå‡ºã€‚è¿™æ˜¯ä¸€ä¸ªåœ¨æ¡ä»¶éŸ³ä¹ç”Ÿæˆä»»åŠ¡ä¸Šè®­ç»ƒçš„ç¼–ç å™¨è§£ç å™¨å˜æ¢å™¨ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“å®ç°çš„é€šç”¨æ–¹æ³•ï¼Œä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼Œè°ƒæ•´è¾“å…¥åµŒå…¥å¤§å°ï¼Œä¿®å‰ªå¤´ç­‰ã€‚

è¯¥æ¨¡å‹ä¹Ÿæ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L942)

```py
( input_ids: LongTensor = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size * num_codebooks, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ï¼Œå¯¹åº”äºéŸ³é¢‘ä»£ç åºåˆ—ã€‚

    å¯ä»¥é€šè¿‡ä½¿ç”¨éŸ³é¢‘ç¼–ç å™¨æ¨¡å‹å¯¹éŸ³é¢‘æç¤ºè¿›è¡Œç¼–ç æ¥è·å–ç´¢å¼•ï¼Œä»¥é¢„æµ‹éŸ³é¢‘ä»£ç ï¼Œä¾‹å¦‚ä½¿ç”¨[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)ã€‚æŸ¥çœ‹[EncodecModel.encode()](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel.encode)è·å–è¯¦ç»†ä¿¡æ¯ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

    `input_ids`å°†åœ¨å‰å‘ä¼ é€’ä¸­è‡ªåŠ¨ä»å½¢çŠ¶`(batch_size * num_codebooks, target_sequence_length)`è½¬æ¢ä¸º`(batch_size, num_codebooks, target_sequence_length)`ã€‚å¦‚æœæ‚¨ä»éŸ³é¢‘ç¼–ç æ¨¡å‹ï¼ˆä¾‹å¦‚[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)ï¼‰è·å–éŸ³é¢‘ä»£ç ï¼Œè¯·ç¡®ä¿å¸§æ•°ç­‰äº1ï¼Œå¹¶ä¸”åœ¨å°†éŸ³é¢‘ä»£ç ä»`(frames, batch_size, num_codebooks, target_sequence_length)`é‡å¡‘ä¸º`(batch_size * num_codebooks, target_sequence_length)`ä¹‹å‰ï¼Œå°†å…¶ä½œä¸º`input_ids`ä¼ é€’ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¸­ï¼š

    +   1è¡¨ç¤º`æœªè¢«æ©ç›–`çš„æ ‡è®°ï¼Œ

    +   0è¡¨ç¤º`è¢«æ©ç›–`çš„æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `encoder_hidden_states`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, encoder_sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `encoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, encoder_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨ç¼–ç å™¨è¾“å…¥IDçš„å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œäº¤å‰æ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¸­ï¼š

    +   1è¡¨ç¤º`æœªè¢«æ©ç›–`çš„æ ‡è®°ï¼Œ

    +   0è¡¨ç¤º`è¢«æ©ç›–`çš„æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨æ— æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¸­ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç›–`ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`æ©ç›–`ã€‚

+   `cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*) â€” ç”¨äºå°†è§£ç å™¨ä¸­äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ï¼Œä»¥é¿å…åœ¨éšè—å¤´éƒ¨ä¸Šæ‰§è¡Œäº¤å‰æ³¨æ„åŠ›ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ã€‚

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›) â€” é•¿åº¦ä¸º`config.n_layers`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆè¯·å‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥å½¢çŠ¶ä¸º`(batch_size, 1)`çš„æœ€å`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰è€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

+   `labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” ç”¨äºè¯­è¨€å»ºæ¨¡çš„æ ‡ç­¾ã€‚è¯·æ³¨æ„ï¼Œæ¨¡å‹å†…éƒ¨**ç§»åŠ¨**æ ‡ç­¾ï¼Œå³æ‚¨å¯ä»¥è®¾ç½®`labels = input_ids`ã€‚ç´¢å¼•åœ¨`[-100, 0, ..., config.vocab_size]`ä¸­é€‰æ‹©ã€‚æ‰€æœ‰è®¾ç½®ä¸º`-100`çš„æ ‡ç­¾éƒ½è¢«å¿½ç•¥ï¼ˆè¢«`masked`ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—æ ‡ç­¾åœ¨`[0, ..., config.vocab_size]`ä¸­çš„æ ‡ç­¾ã€‚

    è¿”å›ï¼š[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)æˆ–`tuple(torch.FloatTensor)`ï¼šä¸€ä¸ª[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`ï¼Œæˆ–è€…å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚ 

    +   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾›`labels`æ—¶è¿”å›) â€” è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚

    +   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´éƒ¨çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

    +   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›) â€” é•¿åº¦ä¸º`config.n_layers`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

        åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

    +   `decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œ+ ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

        è§£ç å™¨åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºéšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

    +   `decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚

        è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

    +   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚

        è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

    +   `encoder_last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—è¾“å‡ºã€‚

    +   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œ+ ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

        ç¼–ç å™¨åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºéšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

    +   `encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚

        ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[MusicgenForCausalLM](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForCausalLM)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

## MusicgenForConditionalGeneration

### `class transformers.MusicgenForConditionalGeneration`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L1404)

```py
( config: Optional = None text_encoder: Optional = None audio_encoder: Optional = None decoder: Optional = None )
```

å‚æ•°

+   `config` ([MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

å…·æœ‰æ–‡æœ¬ç¼–ç å™¨ã€éŸ³é¢‘ç¼–ç å™¨å’ŒMusicgenè§£ç å™¨çš„å¤åˆMusicGenæ¨¡å‹ï¼Œç”¨äºå…·æœ‰æ–‡æœ¬å’Œ/æˆ–éŸ³é¢‘æç¤ºçš„éŸ³ä¹ç”Ÿæˆä»»åŠ¡ã€‚

Musicgenæ¨¡å‹ç”±Jade Copetã€Felix Kreukã€Itai Gatã€Tal Remezã€David Kantã€Gabriel Synnaeveã€Yossi Adiã€Alexandre DÃ©fossezåœ¨[Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)ä¸­æå‡ºã€‚å®ƒæ˜¯ä¸€ä¸ªåœ¨æ¡ä»¶éŸ³ä¹ç”Ÿæˆä»»åŠ¡ä¸Šè®­ç»ƒçš„ç¼–ç å™¨è§£ç å™¨transformerã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚è¯·æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¯¥æ¨¡å‹è¿˜æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/musicgen/modeling_musicgen.py#L1760)

```py
( input_ids: Optional = None attention_mask: Optional = None input_values: Optional = None padding_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None past_key_values: Tuple = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚å¦‚æœæä¾›å¡«å……ï¼Œåˆ™é»˜è®¤å°†å¿½ç•¥ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0, 1]`ï¼š

    +   1è¡¨ç¤ºæœªè¢«`masked`çš„æ ‡è®°ã€‚

    +   0è¡¨ç¤ºè¢«`masked`çš„æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size * num_codebooks, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ï¼Œå¯¹åº”äºéŸ³é¢‘ä»£ç åºåˆ—ã€‚

    å¯ä»¥é€šè¿‡ä½¿ç”¨éŸ³é¢‘ç¼–ç å™¨æ¨¡å‹å¯¹éŸ³é¢‘æç¤ºè¿›è¡Œç¼–ç ä»¥é¢„æµ‹éŸ³é¢‘ä»£ç æ¥è·å–ç´¢å¼•ï¼Œä¾‹å¦‚ä½¿ç”¨[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[EncodecModel.encode()](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel.encode)ã€‚

    [ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥IDï¼Ÿ](../glossary#decoder-input-ids)

    `decoder_input_ids`å°†åœ¨å‰å‘ä¼ é€’ä¸­è‡ªåŠ¨ä»å½¢çŠ¶`(batch_size * num_codebooks, target_sequence_length)`è½¬æ¢ä¸º`(batch_size, num_codebooks, target_sequence_length)`ã€‚å¦‚æœæ‚¨ä»éŸ³é¢‘ç¼–ç æ¨¡å‹ï¼ˆå¦‚[EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)ï¼‰è·å–éŸ³é¢‘ä»£ç ï¼Œè¯·ç¡®ä¿å¸§æ•°ç­‰äº1ï¼Œå¹¶ä¸”åœ¨å°†éŸ³é¢‘ä»£ç ä»`(frames, batch_size, num_codebooks, target_sequence_length)`é‡å¡‘ä¸º`(batch_size * num_codebooks, target_sequence_length)`ä¹‹å‰ï¼Œå°†å…¶ä½œä¸º`decoder_input_ids`ä¼ é€’ã€‚

+   `decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥`decoder_input_ids`ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(encoder_layers, encoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿ç¼–ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0, 1]`ã€‚

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ã€‚

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `decoder_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è§£ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0, 1]`ã€‚

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ã€‚

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*) â€” åœ¨è§£ç å™¨ä¸­å°†äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¸­ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«æ©ç›–ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«æ©ç›–ã€‚

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” å…ƒç»„åŒ…æ‹¬(`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*)æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›) â€” é•¿åº¦ä¸º`config.n_layers`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«å¯ä»¥ç”¨äºåŠ é€Ÿé¡ºåºè§£ç çš„é¢„è®¡ç®—éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼ˆè¯·å‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥å½¢çŠ¶ä¸º`(batch_size, 1)`çš„æœ€åä¸€ä¸ª`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰è€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ª`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚è¿™å¾ˆæœ‰ç”¨ï¼Œå¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚

    å¦‚æœ`decoder_input_ids`å’Œ`decoder_inputs_embeds`éƒ½æœªè®¾ç½®ï¼Œåˆ™`decoder_inputs_embeds`å–`inputs_embeds`çš„å€¼ã€‚

+   `use_cache` (`bool`, *optional*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput) æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[MusicgenConfig](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾›`labels`æ—¶è¿”å›) â€” è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`torch.FloatTensor`ï¼‰- è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰- é•¿åº¦ä¸º`config.n_layers`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„ä¸€ä¸ªå’Œæ¯å±‚è¾“å‡ºçš„ä¸€ä¸ªï¼‰ã€‚

    æ¯å±‚è§£ç å™¨çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºçš„ä¸€ä¸ªå’Œæ¯å±‚è¾“å‡ºçš„ä¸€ä¸ªï¼‰ã€‚

    æ¯å±‚ç¼–ç å™¨çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[MusicgenForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/musicgen#transformers.MusicgenForConditionalGeneration)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration
>>> import torch

>>> processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
>>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

>>> inputs = processor(
...     text=["80s pop track with bassy drums and synth", "90s rock song with loud guitars and heavy drums"],
...     padding=True,
...     return_tensors="pt",
... )

>>> pad_token_id = model.generation_config.pad_token_id
>>> decoder_input_ids = (
...     torch.ones((inputs.input_ids.shape[0] * model.decoder.num_codebooks, 1), dtype=torch.long)
...     * pad_token_id
... )

>>> logits = model(**inputs, decoder_input_ids=decoder_input_ids).logits
>>> logits.shape  # (bsz * num_codebooks, tgt_len, vocab_size)
torch.Size([8, 1, 2048])
```
