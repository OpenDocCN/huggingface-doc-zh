- en: Speed Comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/safetensors/speed](https://huggingface.co/docs/safetensors/speed)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '`Safetensors` is really fast. Let’s compare it against `PyTorch` by loading
    [gpt2](https://huggingface.co/gpt2) weights. To run the [GPU benchmark](#gpu-benchmark),
    make sure your machine has GPU or you have selected `GPU runtime` if you are using
    Google Colab.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s start by importing all the packages that will be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Download safetensors & torch weights for gpt2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: CPU benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This speedup is due to the fact that this library avoids unnecessary copies
    by mapping the file directly. It is actually possible to do on [pure pytorch](https://gist.github.com/Narsil/3edeec2669a5e94e4707aa0f901d2282).
    The currently shown speedup was gotten on:'
  prefs: []
  type: TYPE_NORMAL
- en: 'OS: Ubuntu 18.04.6 LTS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CPU: Intel(R) Xeon(R) CPU @ 2.00GHz'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPU benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The speedup works because this library is able to skip unnecessary CPU allocations.
    It is unfortunately not replicable in pure pytorch as far as we know. The library
    works by memory mapping the file, creating the tensor empty with pytorch and calling
    `cudaMemcpy` directly to move the tensor directly on the GPU. The currently shown
    speedup was gotten on:'
  prefs: []
  type: TYPE_NORMAL
- en: 'OS: Ubuntu 18.04.6 LTS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPU: Tesla T4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Driver Version: 460.32.03'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CUDA Version: 11.2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
