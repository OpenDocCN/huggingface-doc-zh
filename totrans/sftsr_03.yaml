- en: Speed Comparison
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 速度比较
- en: 'Original text: [https://huggingface.co/docs/safetensors/speed](https://huggingface.co/docs/safetensors/speed)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/safetensors/speed](https://huggingface.co/docs/safetensors/speed)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb)'
- en: '`Safetensors` is really fast. Let’s compare it against `PyTorch` by loading
    [gpt2](https://huggingface.co/gpt2) weights. To run the [GPU benchmark](#gpu-benchmark),
    make sure your machine has GPU or you have selected `GPU runtime` if you are using
    Google Colab.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '`Safetensors`非常快。让我们将其与`PyTorch`进行比较，加载[gpt2](https://huggingface.co/gpt2)权重。要运行[GPU基准测试](#gpu-benchmark)，请确保您的机器有GPU，或者如果您在使用Google
    Colab，则已选择`GPU runtime`。'
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请确保您已安装所有必要的库：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s start by importing all the packages that will be used:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入将要使用的所有包开始：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Download safetensors & torch weights for gpt2:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 下载safetensors和gpt2的torch权重：
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: CPU benchmark
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CPU基准测试
- en: '[PRE3]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This speedup is due to the fact that this library avoids unnecessary copies
    by mapping the file directly. It is actually possible to do on [pure pytorch](https://gist.github.com/Narsil/3edeec2669a5e94e4707aa0f901d2282).
    The currently shown speedup was gotten on:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种加速是因为这个库通过直接映射文件来避免不必要的拷贝。实际上，可以在[pure pytorch](https://gist.github.com/Narsil/3edeec2669a5e94e4707aa0f901d2282)上实现。当前显示的加速是在以下环境中获得的：
- en: 'OS: Ubuntu 18.04.6 LTS'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统：Ubuntu 18.04.6 LTS
- en: 'CPU: Intel(R) Xeon(R) CPU @ 2.00GHz'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：Intel(R) Xeon(R) CPU @ 2.00GHz
- en: GPU benchmark
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPU基准测试
- en: '[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The speedup works because this library is able to skip unnecessary CPU allocations.
    It is unfortunately not replicable in pure pytorch as far as we know. The library
    works by memory mapping the file, creating the tensor empty with pytorch and calling
    `cudaMemcpy` directly to move the tensor directly on the GPU. The currently shown
    speedup was gotten on:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种加速的原因是这个库能够跳过不必要的CPU分配。不幸的是，据我们所知，纯pytorch中无法复制这种加速效果。该库通过内存映射文件、使用pytorch创建空张量，并直接调用`cudaMemcpy`来将张量直接移动到GPU上。当前显示的加速是在以下环境中获得的：
- en: 'OS: Ubuntu 18.04.6 LTS.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统：Ubuntu 18.04.6 LTS。
- en: 'GPU: Tesla T4'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU：Tesla T4
- en: 'Driver Version: 460.32.03'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动程序版本：460.32.03
- en: 'CUDA Version: 11.2'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA版本：11.2
