- en: Dilated Neighborhood Attention Transformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‰©å¼ é‚»åŸŸæ³¨æ„åŠ›å˜æ¢å™¨
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dinat](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dinat)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dinat](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dinat)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: DiNAT was proposed in [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001)
    by Ali Hassani and Humphrey Shi.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: DiNATæ˜¯ç”±Ali Hassaniå’ŒHumphrey Shiåœ¨[æ‰©å¼ é‚»åŸŸæ³¨æ„åŠ›å˜æ¢å™¨](https://arxiv.org/abs/2209.15001)ä¸­æå‡ºçš„ã€‚
- en: It extends [NAT](nat) by adding a Dilated Neighborhood Attention pattern to
    capture global context, and shows significant performance improvements over it.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒé€šè¿‡æ·»åŠ æ‰©å¼ é‚»åŸŸæ³¨æ„åŠ›æ¨¡å¼æ¥æ‰©å±•[NAT](nat)ï¼Œä»¥æ•è·å…¨å±€ä¸Šä¸‹æ–‡ï¼Œå¹¶æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„æ€§èƒ½æ”¹è¿›ã€‚
- en: 'The abstract from the paper is the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*Transformers are quickly becoming one of the most heavily applied deep learning
    architectures across modalities, domains, and tasks. In vision, on top of ongoing
    efforts into plain transformers, hierarchical transformers have also gained significant
    attention, thanks to their performance and easy integration into existing frameworks.
    These models typically employ localized attention mechanisms, such as the sliding-window
    Neighborhood Attention (NA) or Swin Transformerâ€™s Shifted Window Self Attention.
    While effective at reducing self attentionâ€™s quadratic complexity, local attention
    weakens two of the most desirable properties of self attention: long range inter-dependency
    modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood
    Attention (DiNA), a natural, flexible and efficient extension to NA that can capture
    more global context and expand receptive fields exponentially at no additional
    cost. NAâ€™s local attention and DiNAâ€™s sparse global attention complement each
    other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT),
    a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant
    improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large
    model is faster and ahead of its Swin counterpart by 1.5% box AP in COCO object
    detection, 1.3% mask AP in COCO instance segmentation, and 1.1% mIoU in ADE20K
    semantic segmentation. Paired with new frameworks, our large variant is the new
    state of the art panoptic segmentation model on COCO (58.2 PQ) and ADE20K (48.5
    PQ), and instance segmentation model on Cityscapes (44.5 AP) and ADE20K (35.4
    AP) (no extra data). It also matches the state of the art specialized semantic
    segmentation models on ADE20K (58.2 mIoU), and ranks second on Cityscapes (84.5
    mIoU) (no extra data).*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*å˜æ¢å™¨æ­£åœ¨è¿…é€Ÿæˆä¸ºè·¨æ¨¡æ€ã€é¢†åŸŸå’Œä»»åŠ¡ä¸­æœ€å¹¿æ³›åº”ç”¨çš„æ·±åº¦å­¦ä¹ æ¶æ„ä¹‹ä¸€ã€‚åœ¨è§†è§‰é¢†åŸŸï¼Œé™¤äº†å¯¹æ™®é€šå˜æ¢å™¨çš„æŒç»­åŠªåŠ›å¤–ï¼Œåˆ†å±‚å˜æ¢å™¨ä¹Ÿå¼•èµ·äº†æå¤§å…³æ³¨ï¼Œè¿™è¦å½’åŠŸäºå®ƒä»¬çš„æ€§èƒ½å’Œæ˜“äºé›†æˆåˆ°ç°æœ‰æ¡†æ¶ä¸­ã€‚è¿™äº›æ¨¡å‹é€šå¸¸é‡‡ç”¨å±€éƒ¨åŒ–æ³¨æ„æœºåˆ¶ï¼Œä¾‹å¦‚æ»‘åŠ¨çª—å£é‚»åŸŸæ³¨æ„åŠ›ï¼ˆNAï¼‰æˆ–Swin
    Transformerçš„ç§»ä½çª—å£è‡ªæ³¨æ„åŠ›ã€‚è™½ç„¶æœ‰æ•ˆåœ°å‡å°‘äº†è‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡å¤æ‚åº¦ï¼Œä½†å±€éƒ¨æ³¨æ„åŠ›å‰Šå¼±äº†è‡ªæ³¨æ„åŠ›çš„ä¸¤ä¸ªæœ€ç†æƒ³çš„ç‰¹æ€§ï¼šé•¿è·ç¦»ç›¸äº’ä¾èµ–å»ºæ¨¡å’Œå…¨å±€æ„Ÿå—é‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æ‰©å¼ é‚»åŸŸæ³¨æ„åŠ›ï¼ˆDiNAï¼‰ï¼Œè¿™æ˜¯å¯¹NAçš„ä¸€ç§è‡ªç„¶ã€çµæ´»å’Œé«˜æ•ˆçš„æ‰©å±•ï¼Œå¯ä»¥åœ¨ä¸å¢åŠ é¢å¤–æˆæœ¬çš„æƒ…å†µä¸‹æ•è·æ›´å¤šçš„å…¨å±€ä¸Šä¸‹æ–‡å¹¶æŒ‡æ•°çº§æ‰©å±•æ„Ÿå—é‡ã€‚NAçš„å±€éƒ¨æ³¨æ„åŠ›å’ŒDiNAçš„ç¨€ç–å…¨å±€æ³¨æ„åŠ›äº’è¡¥ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥äº†æ‰©å¼ é‚»åŸŸæ³¨æ„åŠ›å˜æ¢å™¨ï¼ˆDiNATï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä¸¤è€…æ„å»ºçš„æ–°çš„åˆ†å±‚è§†è§‰å˜æ¢å™¨ã€‚DiNATçš„å˜ä½“åœ¨å¼ºåŸºçº¿æ¨¡å‹ï¼ˆå¦‚NATã€Swinå’ŒConvNeXtï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„å¤§å‹æ¨¡å‹åœ¨COCOç›®æ ‡æ£€æµ‹ä¸­æ¯”å…¶Swinå¯¹åº”ç‰©å¿«1.5%çš„box
    APï¼Œåœ¨COCOå®ä¾‹åˆ†å‰²ä¸­æ¯”å…¶å¿«1.3%çš„mask APï¼Œåœ¨ADE20Kè¯­ä¹‰åˆ†å‰²ä¸­æ¯”å…¶å¿«1.1%çš„mIoUã€‚ä¸æ–°æ¡†æ¶é…å¯¹ï¼Œæˆ‘ä»¬çš„å¤§å‹å˜ä½“æ˜¯COCOï¼ˆ58.2
    PQï¼‰å’ŒADE20Kï¼ˆ48.5 PQï¼‰çš„æ–°ä¸€ä»£å…¨æ™¯åˆ†å‰²æ¨¡å‹ï¼Œä»¥åŠCityscapesï¼ˆ44.5 APï¼‰å’ŒADE20Kï¼ˆ35.4 APï¼‰çš„å®ä¾‹åˆ†å‰²æ¨¡å‹ï¼ˆæ— é¢å¤–æ•°æ®ï¼‰ã€‚å®ƒè¿˜ä¸ADE20Kï¼ˆ58.2
    mIoUï¼‰ä¸Šçš„æœ€å…ˆè¿›ä¸“é—¨çš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹ç›¸åŒ¹é…ï¼Œå¹¶åœ¨Cityscapesï¼ˆ84.5 mIoUï¼‰ä¸Šæ’åç¬¬äºŒï¼ˆæ— é¢å¤–æ•°æ®ï¼‰ã€‚*'
- en: '![drawing](../Images/01b57d1cfb697d4055cf9fec6e5ca8f1.png) Neighborhood Attention
    with different dilation values. Taken from the [original paper](https://arxiv.org/abs/2209.15001).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![drawing](../Images/01b57d1cfb697d4055cf9fec6e5ca8f1.png) å…·æœ‰ä¸åŒæ‰©å¼ å€¼çš„é‚»åŸŸæ³¨æ„åŠ›ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2209.15001)ã€‚'
- en: This model was contributed by [Ali Hassani](https://huggingface.co/alihassanijr).
    The original code can be found [here](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç”±[Ali Hassani](https://huggingface.co/alihassanijr)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer)æ‰¾åˆ°ã€‚
- en: Usage tips
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: DiNAT can be used as a *backbone*. When `output_hidden_states = True`, it will
    output both `hidden_states` and `reshaped_hidden_states`. The `reshaped_hidden_states`
    have a shape of `(batch, num_channels, height, width)` rather than `(batch_size,
    height, width, num_channels)`.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: DiNATå¯ä»¥ç”¨ä½œ*éª¨å¹²*ã€‚å½“`output_hidden_states = True`æ—¶ï¼Œå®ƒå°†è¾“å‡º`hidden_states`å’Œ`reshaped_hidden_states`ã€‚`reshaped_hidden_states`çš„å½¢çŠ¶ä¸º`(batch,
    num_channels, height, width)`ï¼Œè€Œä¸æ˜¯`(batch_size, height, width, num_channels)`ã€‚
- en: 'Notes:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼š
- en: DiNAT depends on [NATTEN](https://github.com/SHI-Labs/NATTEN/)â€™s implementation
    of Neighborhood Attention and Dilated Neighborhood Attention. You can install
    it with pre-built wheels for Linux by referring to [shi-labs.com/natten](https://shi-labs.com/natten),
    or build on your system by running `pip install natten`. Note that the latter
    will likely take time to compile. NATTEN does not support Windows devices yet.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DiNATä¾èµ–äº[NATTEN](https://github.com/SHI-Labs/NATTEN/)å¯¹é‚»åŸŸæ³¨æ„åŠ›å’Œæ‰©å¼ é‚»åŸŸæ³¨æ„åŠ›çš„å®ç°ã€‚æ‚¨å¯ä»¥é€šè¿‡å‚è€ƒ[shi-labs.com/natten](https://shi-labs.com/natten)æ¥å®‰è£…Linuxçš„é¢„æ„å»ºè½®å­ï¼Œæˆ–è€…é€šè¿‡è¿è¡Œ`pip
    install natten`åœ¨æ‚¨çš„ç³»ç»Ÿä¸Šæ„å»ºã€‚è¯·æ³¨æ„ï¼Œåè€…å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´æ¥ç¼–è¯‘ã€‚NATTENç›®å‰ä¸æ”¯æŒWindowsè®¾å¤‡ã€‚
- en: Patch size of 4 is only supported at the moment.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›®å‰ä»…æ”¯æŒ4çš„è¡¥ä¸å¤§å°ã€‚
- en: Resources
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with DiNAT.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºçš„åˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨DiNATã€‚
- en: Image Classification
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ†ç±»
- en: '[DinatForImageClassification](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DinatForImageClassification](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatForImageClassification)ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)æ”¯æŒã€‚'
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦è¯·å‚é˜…ï¼š[å›¾åƒåˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/image_classification)
- en: If youâ€™re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªPull Requestï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: DinatConfig
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DinatConfig
- en: '### `class transformers.DinatConfig`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DinatConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dinat/configuration_dinat.py#L30)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dinat/configuration_dinat.py#L30)'
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`patch_size` (`int`, *optional*, defaults to 4) â€” The size (resolution) of
    each patch. NOTE: Only patch size of 4 is supported at the moment.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º4) â€” æ¯ä¸ªè¡¥ä¸çš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚æ³¨æ„ï¼šç›®å‰ä»…æ”¯æŒè¡¥ä¸å¤§å°ä¸º4ã€‚'
- en: '`num_channels` (`int`, *optional*, defaults to 3) â€” The number of input channels.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *å¯é€‰*, é»˜è®¤ä¸º3) â€” è¾“å…¥é€šé“æ•°ã€‚'
- en: '`embed_dim` (`int`, *optional*, defaults to 64) â€” Dimensionality of patch embedding.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embed_dim` (`int`, *å¯é€‰*, é»˜è®¤ä¸º64) â€” è¡¥ä¸åµŒå…¥çš„ç»´åº¦ã€‚'
- en: '`depths` (`List[int]`, *optional*, defaults to `[3, 4, 6, 5]`) â€” Number of
    layers in each level of the encoder.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depths` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º`[3, 4, 6, 5]`) â€” ç¼–ç å™¨æ¯ä¸ªçº§åˆ«çš„å±‚æ•°ã€‚'
- en: '`num_heads` (`List[int]`, *optional*, defaults to `[2, 4, 8, 16]`) â€” Number
    of attention heads in each layer of the Transformer encoder.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_heads` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º`[2, 4, 8, 16]`) â€” Transformerç¼–ç å™¨æ¯å±‚ä¸­çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`kernel_size` (`int`, *optional*, defaults to 7) â€” Neighborhood Attention kernel
    size.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º7) â€” é‚»åŸŸæ³¨æ„åŠ›æ ¸å¤§å°ã€‚'
- en: '`dilations` (`List[List[int]]`, *optional*, defaults to `[[1, 8, 1], [1, 4,
    1, 4], [1, 2, 1, 2, 1, 2], [1, 1, 1, 1, 1]]`) â€” Dilation value of each NA layer
    in the Transformer encoder.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dilations` (`List[List[int]]`, *å¯é€‰*, é»˜è®¤ä¸º`[[1, 8, 1], [1, 4, 1, 4], [1, 2,
    1, 2, 1, 2], [1, 1, 1, 1, 1]]`) â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªNAå±‚çš„æ‰©å¼ å€¼ã€‚'
- en: '`mlp_ratio` (`float`, *optional*, defaults to 3.0) â€” Ratio of MLP hidden dimensionality
    to embedding dimensionality.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp_ratio` (`float`, *å¯é€‰*, é»˜è®¤ä¸º3.0) â€” MLPéšè—ç»´åº¦ä¸åµŒå…¥ç»´åº¦çš„æ¯”ç‡ã€‚'
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) â€” Whether or not a learnable
    bias should be added to the queries, keys and values.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_bias` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦åº”å‘æŸ¥è¯¢ã€é”®å’Œå€¼æ·»åŠ å¯å­¦ä¹ åç½®ã€‚'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” The dropout
    probability for all fully connected layers in the embeddings and encoder.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.0) â€” åµŒå…¥å’Œç¼–ç å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¤±æ¦‚ç‡ã€‚'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” The
    dropout ratio for the attention probabilities.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¤±æ¯”ç‡ã€‚'
- en: '`drop_path_rate` (`float`, *optional*, defaults to 0.1) â€” Stochastic depth
    rate.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_path_rate` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.1) â€” éšæœºæ·±åº¦ç‡ã€‚'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” The
    non-linear activation function (function or string) in the encoder. If string,
    `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`æˆ–`function`, *å¯é€‰*, é»˜è®¤ä¸º`"gelu"`) â€” ç¼–ç å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"selu"`å’Œ`"gelu_new"`ã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) â€” The epsilon used
    by the layer normalization layers.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *å¯é€‰*, é»˜è®¤ä¸º1e-05) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚'
- en: '`layer_scale_init_value` (`float`, *optional*, defaults to 0.0) â€” The initial
    value for the layer scale. Disabled if <=0.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_scale_init_value` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.0) â€” å±‚ç¼©æ”¾çš„åˆå§‹å€¼ã€‚å¦‚æœ<=0ï¼Œåˆ™ç¦ç”¨ã€‚'
- en: '`out_features` (`List[str]`, *optional*) â€” If used as backbone, list of features
    to output. Can be any of `"stem"`, `"stage1"`, `"stage2"`, etc. (depending on
    how many stages the model has). If unset and `out_indices` is set, will default
    to the corresponding stages. If unset and `out_indices` is unset, will default
    to the last stage. Must be in the same order as defined in the `stage_names` attribute.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_features` (`List[str]`, *å¯é€‰*) â€” å¦‚æœç”¨ä½œéª¨å¹²ï¼Œè¦è¾“å‡ºçš„ç‰¹å¾åˆ—è¡¨ã€‚å¯ä»¥æ˜¯`"stem"`ã€`"stage1"`ã€`"stage2"`ç­‰ï¼ˆå–å†³äºæ¨¡å‹æœ‰å¤šå°‘é˜¶æ®µï¼‰ã€‚å¦‚æœæœªè®¾ç½®ä¸”è®¾ç½®äº†`out_indices`ï¼Œå°†é»˜è®¤ä¸ºç›¸åº”çš„é˜¶æ®µã€‚å¦‚æœæœªè®¾ç½®ä¸”`out_indices`æœªè®¾ç½®ï¼Œå°†é»˜è®¤ä¸ºæœ€åä¸€ä¸ªé˜¶æ®µã€‚å¿…é¡»æŒ‰ç…§`stage_names`å±æ€§ä¸­å®šä¹‰çš„é¡ºåºã€‚'
- en: '`out_indices` (`List[int]`, *optional*) â€” If used as backbone, list of indices
    of features to output. Can be any of 0, 1, 2, etc. (depending on how many stages
    the model has). If unset and `out_features` is set, will default to the corresponding
    stages. If unset and `out_features` is unset, will default to the last stage.
    Must be in the same order as defined in the `stage_names` attribute.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_indices` (`List[int]`, *å¯é€‰*) â€” å¦‚æœç”¨ä½œéª¨å¹²ï¼Œè¦è¾“å‡ºçš„ç‰¹å¾çš„ç´¢å¼•åˆ—è¡¨ã€‚å¯ä»¥æ˜¯0ã€1ã€2ç­‰ï¼ˆå–å†³äºæ¨¡å‹æœ‰å¤šå°‘é˜¶æ®µï¼‰ã€‚å¦‚æœæœªè®¾ç½®ä¸”è®¾ç½®äº†`out_features`ï¼Œå°†é»˜è®¤ä¸ºç›¸åº”çš„é˜¶æ®µã€‚å¦‚æœæœªè®¾ç½®ä¸”`out_features`æœªè®¾ç½®ï¼Œå°†é»˜è®¤ä¸ºæœ€åä¸€ä¸ªé˜¶æ®µã€‚å¿…é¡»æŒ‰ç…§`stage_names`å±æ€§ä¸­å®šä¹‰çš„é¡ºåºã€‚'
- en: This is the configuration class to store the configuration of a [DinatModel](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatModel).
    It is used to instantiate a Dinat model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Dinat [shi-labs/dinat-mini-in1k-224](https://huggingface.co/shi-labs/dinat-mini-in1k-224)
    architecture.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨ [DinatModel](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatModel)
    é…ç½®çš„é…ç½®ç±»ã€‚ç”¨äºæ ¹æ®æŒ‡å®šå‚æ•°å®ä¾‹åŒ– Dinat æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº Dinat [shi-labs/dinat-mini-in1k-224](https://huggingface.co/shi-labs/dinat-mini-in1k-224)
    æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: DinatModel
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DinatModel
- en: '### `class transformers.DinatModel`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DinatModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dinat/modeling_dinat.py#L692)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dinat/modeling_dinat.py#L692)'
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([DinatConfig](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([DinatConfig](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatConfig))
    â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare Dinat Model transformer outputting raw hidden-states without any specific
    head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸ Dinat æ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚è¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dinat/modeling_dinat.py#L727)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dinat/modeling_dinat.py#L727)'
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨ [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    è·å–åƒç´ å€¼ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚é˜… [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„
    `attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„
    `hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.models.dinat.modeling_dinat.DinatModelOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.dinat.modeling_dinat.DinatModelOutput` æˆ– `tuple(torch.FloatTensor)`'
- en: A `transformers.models.dinat.modeling_dinat.DinatModelOutput` or a tuple of
    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DinatConfig](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatConfig))
    and inputs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª `transformers.models.dinat.modeling_dinat.DinatModelOutput` æˆ–ä¸€ä¸ª `torch.FloatTensor`
    å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰ï¼ŒåŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[DinatConfig](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€è¾“å‡ºåºåˆ—ã€‚'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`,
    *optional*, returned when `add_pooling_layer=True` is passed) â€” Average pooling
    of the last layer hidden-state.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`,
    *optional*, returned when `add_pooling_layer=True` is passed) â€” æœ€åä¸€å±‚éšè—çŠ¶æ€çš„å¹³å‡æ± åŒ–ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each stage) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each stage) of shape
    `(batch_size, sequence_length, hidden_size)`.'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each stage) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸ªé˜¶æ®µä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è‡ªæ³¨æ„åŠ›å¤´ä¸­ç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼çš„æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ã€‚
- en: '`reshaped_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, hidden_size, height, width)`.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reshaped_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, hidden_size, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º
    + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs reshaped to include the spatial dimensions.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠåŒ…æ‹¬ç©ºé—´ç»´åº¦çš„åˆå§‹åµŒå…¥è¾“å‡ºçš„é‡å¡‘ã€‚
- en: The [DinatModel](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatModel)
    forward method, overrides the `__call__` special method.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[DinatModel](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: DinatForImageClassification
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DinatForImageClassification
- en: '### `class transformers.DinatForImageClassification`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DinatForImageClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dinat/modeling_dinat.py#L782)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dinat/modeling_dinat.py#L782)'
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([DinatConfig](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[DinatConfig](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatConfig)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: Dinat Model transformer with an image classification head on top (a linear layer
    on top of the final hidden state of the [CLS] token) e.g. for ImageNet.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Dinatæ¨¡å‹å˜å‹å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰å›¾åƒåˆ†ç±»å¤´ï¼ˆåœ¨[CLS]æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€é¡¶éƒ¨çš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºImageNetã€‚
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚
- en: '#### `forward`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dinat/modeling_dinat.py#L806)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dinat/modeling_dinat.py#L806)'
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€”
    åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—å›¾åƒåˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0,
    ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels
    > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.models.dinat.modeling_dinat.DinatImageClassifierOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.dinat.modeling_dinat.DinatImageClassifierOutput`æˆ–`tuple(torch.FloatTensor)`'
- en: A `transformers.models.dinat.modeling_dinat.DinatImageClassifierOutput` or a
    tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DinatConfig](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatConfig))
    and inputs.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª`transformers.models.dinat.modeling_dinat.DinatImageClassifierOutput`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[DinatConfig](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€”
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`çš„`torch.FloatTensor`ï¼‰ â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each stage) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º
    + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each stage) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸ªé˜¶æ®µä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`reshaped_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, hidden_size, height, width)`.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reshaped_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, hidden_size, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º
    + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs reshaped to include the spatial dimensions.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºï¼Œé‡å¡‘ä»¥åŒ…æ‹¬ç©ºé—´ç»´åº¦ã€‚
- en: The [DinatForImageClassification](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[DinatForImageClassification](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatForImageClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
