- en: Dilated Neighborhood Attention Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dinat](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dinat)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/111.4aa1285b.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/PipelineTag.44585822.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DiNAT was proposed in [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001)
    by Ali Hassani and Humphrey Shi.
  prefs: []
  type: TYPE_NORMAL
- en: It extends [NAT](nat) by adding a Dilated Neighborhood Attention pattern to
    capture global context, and shows significant performance improvements over it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Transformers are quickly becoming one of the most heavily applied deep learning
    architectures across modalities, domains, and tasks. In vision, on top of ongoing
    efforts into plain transformers, hierarchical transformers have also gained significant
    attention, thanks to their performance and easy integration into existing frameworks.
    These models typically employ localized attention mechanisms, such as the sliding-window
    Neighborhood Attention (NA) or Swin Transformerâ€™s Shifted Window Self Attention.
    While effective at reducing self attentionâ€™s quadratic complexity, local attention
    weakens two of the most desirable properties of self attention: long range inter-dependency
    modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood
    Attention (DiNA), a natural, flexible and efficient extension to NA that can capture
    more global context and expand receptive fields exponentially at no additional
    cost. NAâ€™s local attention and DiNAâ€™s sparse global attention complement each
    other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT),
    a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant
    improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large
    model is faster and ahead of its Swin counterpart by 1.5% box AP in COCO object
    detection, 1.3% mask AP in COCO instance segmentation, and 1.1% mIoU in ADE20K
    semantic segmentation. Paired with new frameworks, our large variant is the new
    state of the art panoptic segmentation model on COCO (58.2 PQ) and ADE20K (48.5
    PQ), and instance segmentation model on Cityscapes (44.5 AP) and ADE20K (35.4
    AP) (no extra data). It also matches the state of the art specialized semantic
    segmentation models on ADE20K (58.2 mIoU), and ranks second on Cityscapes (84.5
    mIoU) (no extra data).*'
  prefs: []
  type: TYPE_NORMAL
- en: '![drawing](../Images/01b57d1cfb697d4055cf9fec6e5ca8f1.png) Neighborhood Attention
    with different dilation values. Taken from the [original paper](https://arxiv.org/abs/2209.15001).'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [Ali Hassani](https://huggingface.co/alihassanijr).
    The original code can be found [here](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DiNAT can be used as a *backbone*. When `output_hidden_states = True`, it will
    output both `hidden_states` and `reshaped_hidden_states`. The `reshaped_hidden_states`
    have a shape of `(batch, num_channels, height, width)` rather than `(batch_size,
    height, width, num_channels)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notes:'
  prefs: []
  type: TYPE_NORMAL
- en: DiNAT depends on [NATTEN](https://github.com/SHI-Labs/NATTEN/)â€™s implementation
    of Neighborhood Attention and Dilated Neighborhood Attention. You can install
    it with pre-built wheels for Linux by referring to [shi-labs.com/natten](https://shi-labs.com/natten),
    or build on your system by running `pip install natten`. Note that the latter
    will likely take time to compile. NATTEN does not support Windows devices yet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patch size of 4 is only supported at the moment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to
    help you get started with DiNAT.
  prefs: []
  type: TYPE_NORMAL
- en: Image Classification
  prefs: []
  type: TYPE_NORMAL
- en: '[DinatForImageClassification](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If youâ€™re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  prefs: []
  type: TYPE_NORMAL
- en: DinatConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.DinatConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dinat/configuration_dinat.py#L30)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`patch_size` (`int`, *optional*, defaults to 4) â€” The size (resolution) of
    each patch. NOTE: Only patch size of 4 is supported at the moment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_channels` (`int`, *optional*, defaults to 3) â€” The number of input channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embed_dim` (`int`, *optional*, defaults to 64) â€” Dimensionality of patch embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`depths` (`List[int]`, *optional*, defaults to `[3, 4, 6, 5]`) â€” Number of
    layers in each level of the encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_heads` (`List[int]`, *optional*, defaults to `[2, 4, 8, 16]`) â€” Number
    of attention heads in each layer of the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel_size` (`int`, *optional*, defaults to 7) â€” Neighborhood Attention kernel
    size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dilations` (`List[List[int]]`, *optional*, defaults to `[[1, 8, 1], [1, 4,
    1, 4], [1, 2, 1, 2, 1, 2], [1, 1, 1, 1, 1]]`) â€” Dilation value of each NA layer
    in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlp_ratio` (`float`, *optional*, defaults to 3.0) â€” Ratio of MLP hidden dimensionality
    to embedding dimensionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) â€” Whether or not a learnable
    bias should be added to the queries, keys and values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” The dropout
    probability for all fully connected layers in the embeddings and encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” The
    dropout ratio for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_path_rate` (`float`, *optional*, defaults to 0.1) â€” Stochastic depth
    rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” The
    non-linear activation function (function or string) in the encoder. If string,
    `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) â€” The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_scale_init_value` (`float`, *optional*, defaults to 0.0) â€” The initial
    value for the layer scale. Disabled if <=0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`out_features` (`List[str]`, *optional*) â€” If used as backbone, list of features
    to output. Can be any of `"stem"`, `"stage1"`, `"stage2"`, etc. (depending on
    how many stages the model has). If unset and `out_indices` is set, will default
    to the corresponding stages. If unset and `out_indices` is unset, will default
    to the last stage. Must be in the same order as defined in the `stage_names` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`out_indices` (`List[int]`, *optional*) â€” If used as backbone, list of indices
    of features to output. Can be any of 0, 1, 2, etc. (depending on how many stages
    the model has). If unset and `out_features` is set, will default to the corresponding
    stages. If unset and `out_features` is unset, will default to the last stage.
    Must be in the same order as defined in the `stage_names` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [DinatModel](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatModel).
    It is used to instantiate a Dinat model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Dinat [shi-labs/dinat-mini-in1k-224](https://huggingface.co/shi-labs/dinat-mini-in1k-224)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: DinatModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.DinatModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dinat/modeling_dinat.py#L692)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([DinatConfig](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare Dinat Model transformer outputting raw hidden-states without any specific
    head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dinat/modeling_dinat.py#L727)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.dinat.modeling_dinat.DinatModelOutput` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.dinat.modeling_dinat.DinatModelOutput` or a tuple of
    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DinatConfig](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`,
    *optional*, returned when `add_pooling_layer=True` is passed) â€” Average pooling
    of the last layer hidden-state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each stage) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each stage) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`reshaped_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, hidden_size, height, width)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs reshaped to include the spatial dimensions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [DinatModel](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: DinatForImageClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.DinatForImageClassification`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dinat/modeling_dinat.py#L782)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([DinatConfig](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dinat Model transformer with an image classification head on top (a linear layer
    on top of the final hidden state of the [CLS] token) e.g. for ImageNet.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dinat/modeling_dinat.py#L806)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.dinat.modeling_dinat.DinatImageClassifierOutput` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.dinat.modeling_dinat.DinatImageClassifierOutput` or a
    tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DinatConfig](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€”
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each stage) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each stage) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`reshaped_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each stage) of shape `(batch_size, hidden_size, height, width)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs reshaped to include the spatial dimensions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [DinatForImageClassification](/docs/transformers/v4.37.2/en/model_doc/dinat#transformers.DinatForImageClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
