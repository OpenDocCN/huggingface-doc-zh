- en: GPU inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPUæ¨ç†
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/perf_infer_gpu_one](https://huggingface.co/docs/transformers/v4.37.2/en/perf_infer_gpu_one)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/perf_infer_gpu_one](https://huggingface.co/docs/transformers/v4.37.2/en/perf_infer_gpu_one)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: GPUs are the standard choice of hardware for machine learning, unlike CPUs,
    because they are optimized for memory bandwidth and parallelism. To keep up with
    the larger sizes of modern models or to run these large models on existing and
    older hardware, there are several optimizations you can use to speed up GPU inference.
    In this guide, youâ€™ll learn how to use FlashAttention-2 (a more memory-efficient
    attention mechanism), BetterTransformer (a PyTorch native fastpath execution),
    and bitsandbytes to quantize your model to a lower precision. Finally, learn how
    to use ğŸ¤— Optimum to accelerate inference with ONNX Runtime on Nvidia and AMD GPUs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸CPUä¸åŒï¼ŒGPUæ˜¯æœºå™¨å­¦ä¹ çš„æ ‡å‡†ç¡¬ä»¶é€‰æ‹©ï¼Œå› ä¸ºå®ƒä»¬é’ˆå¯¹å†…å­˜å¸¦å®½å’Œå¹¶è¡Œæ€§è¿›è¡Œäº†ä¼˜åŒ–ã€‚ä¸ºäº†è·Ÿä¸Šç°ä»£æ¨¡å‹çš„æ›´å¤§å°ºå¯¸æˆ–åœ¨ç°æœ‰å’Œè¾ƒæ—§çš„ç¡¬ä»¶ä¸Šè¿è¡Œè¿™äº›å¤§å‹æ¨¡å‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å‡ ç§ä¼˜åŒ–æ–¹æ³•æ¥åŠ é€ŸGPUæ¨ç†ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨FlashAttention-2ï¼ˆä¸€ç§æ›´èŠ‚çœå†…å­˜çš„æ³¨æ„åŠ›æœºåˆ¶ï¼‰ã€BetterTransformerï¼ˆPyTorchæœ¬åœ°å¿«é€Ÿæ‰§è¡Œè·¯å¾„ï¼‰å’Œbitsandbyteså°†æ¨¡å‹é‡åŒ–ä¸ºè¾ƒä½ç²¾åº¦ã€‚æœ€åï¼Œå­¦ä¹ å¦‚ä½•ä½¿ç”¨ğŸ¤—
    Optimumåœ¨Nvidiaå’ŒAMD GPUä¸ŠåŠ é€Ÿæ¨ç†ã€‚
- en: The majority of the optimizations described here also apply to multi-GPU setups!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæè¿°çš„å¤§å¤šæ•°ä¼˜åŒ–ä¹Ÿé€‚ç”¨äºå¤šGPUè®¾ç½®ï¼
- en: FlashAttention-2
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlashAttention-2
- en: FlashAttention-2 is experimental and may change considerably in future versions.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: FlashAttention-2æ˜¯å®éªŒæ€§çš„ï¼Œæœªæ¥ç‰ˆæœ¬å¯èƒ½ä¼šå‘ç”Ÿè¾ƒå¤§å˜åŒ–ã€‚
- en: '[FlashAttention-2](https://huggingface.co/papers/2205.14135) is a faster and
    more efficient implementation of the standard attention mechanism that can significantly
    speedup inference by:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlashAttention-2](https://huggingface.co/papers/2205.14135)æ˜¯æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶çš„æ›´å¿«ã€æ›´é«˜æ•ˆçš„å®ç°ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ˜¾è‘—åŠ é€Ÿæ¨ç†ï¼š'
- en: additionally parallelizing the attention computation over sequence length
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå¯ä»¥é€šè¿‡åœ¨åºåˆ—é•¿åº¦ä¸Šå¹¶è¡ŒåŒ–æ³¨æ„åŠ›è®¡ç®—æ¥ä¼˜åŒ–
- en: partitioning the work between GPU threads to reduce communication and shared
    memory reads/writes between them
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†å·¥ä½œåˆ†åŒºåœ¨GPUçº¿ç¨‹ä¹‹é—´ï¼Œä»¥å‡å°‘å®ƒä»¬ä¹‹é—´çš„é€šä¿¡å’Œå…±äº«å†…å­˜è¯»/å†™
- en: 'FlashAttention-2 is currently supported for the following architectures:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰æ”¯æŒä»¥ä¸‹æ¶æ„çš„FlashAttention-2ï¼š
- en: '[Bark](https://huggingface.co/docs/transformers/model_doc/bark#transformers.BarkModel)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bark](https://huggingface.co/docs/transformers/model_doc/bark#transformers.BarkModel)'
- en: '[Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)'
- en: '[DistilBert](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DistilBert](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel)'
- en: '[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)'
- en: '[GPTNeo](https://huggingface.co/docs/transformers/model_doc/gpt_neo#transformers.GPTNeoModel)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPTNeo](https://huggingface.co/docs/transformers/model_doc/gpt_neo#transformers.GPTNeoModel)'
- en: '[GPTNeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox#transformers.GPTNeoXModel)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPTNeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox#transformers.GPTNeoXModel)'
- en: '[Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)'
- en: '[Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)'
- en: '[Llava](https://huggingface.co/docs/transformers/model_doc/llava)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Llava](https://huggingface.co/docs/transformers/model_doc/llava)'
- en: '[VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)'
- en: '[MBart](https://huggingface.co/docs/transformers/model_doc/mbart#transformers.MBartModel)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MBart](https://huggingface.co/docs/transformers/model_doc/mbart#transformers.MBartModel)'
- en: '[Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)'
- en: '[Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)'
- en: '[OPT](https://huggingface.co/docs/transformers/model_doc/opt#transformers.OPTModel)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OPT](https://huggingface.co/docs/transformers/model_doc/opt#transformers.OPTModel)'
- en: '[Phi](https://huggingface.co/docs/transformers/model_doc/phi#transformers.PhiModel)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Phi](https://huggingface.co/docs/transformers/model_doc/phi#transformers.PhiModel)'
- en: '[Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model)'
- en: '[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)'
- en: You can request to add FlashAttention-2 support for another model by opening
    a GitHub Issue or Pull Request.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡æ‰“å¼€GitHub Issueæˆ–Pull Requestæ¥è¯·æ±‚ä¸ºå¦ä¸€ä¸ªæ¨¡å‹æ·»åŠ FlashAttention-2æ”¯æŒã€‚
- en: Before you begin, make sure you have FlashAttention-2 installed.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…FlashAttention-2ã€‚
- en: NVIDIAAMD
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIAAMD
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We strongly suggest referring to the detailed [installation instructions](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features)
    to learn more about supported hardware and data types!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¼ºçƒˆå»ºè®®å‚è€ƒè¯¦ç»†çš„[å®‰è£…è¯´æ˜](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features)ä»¥äº†è§£æ›´å¤šæ”¯æŒçš„ç¡¬ä»¶å’Œæ•°æ®ç±»å‹ï¼
- en: 'To enable FlashAttention-2, pass the argument `attn_implementation="flash_attention_2"`
    to [from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¯ç”¨FlashAttention-2ï¼Œè¯·å°†å‚æ•°`attn_implementation="flash_attention_2"`ä¼ é€’ç»™[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)ï¼š
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: FlashAttention-2 can only be used when the modelâ€™s dtype is `fp16` or `bf16`.
    Make sure to cast your model to the appropriate dtype and load them on a supported
    device before using FlashAttention-2.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰å½“æ¨¡å‹çš„dtypeä¸º`fp16`æˆ–`bf16`æ—¶ï¼Œæ‰èƒ½ä½¿ç”¨FlashAttention-2ã€‚åœ¨ä½¿ç”¨FlashAttention-2ä¹‹å‰ï¼Œè¯·ç¡®ä¿å°†æ¨¡å‹è½¬æ¢ä¸ºé€‚å½“çš„dtypeå¹¶åŠ è½½åˆ°æ”¯æŒçš„è®¾å¤‡ä¸Šã€‚
- en: You can also set `use_flash_attention_2=True` to enable FlashAttention-2 but
    it is deprecated in favor of `attn_implementation="flash_attention_2"`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥è®¾ç½®`use_flash_attention_2=True`æ¥å¯ç”¨FlashAttention-2ï¼Œä½†å·²è¢«å¼ƒç”¨ï¼Œæ¨èä½¿ç”¨`attn_implementation="flash_attention_2"`ã€‚
- en: 'FlashAttention-2 can be combined with other optimization techniques like quantization
    to further speedup inference. For example, you can combine FlashAttention-2 with
    8-bit or 4-bit quantization:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: FlashAttention-2å¯ä»¥ä¸å…¶ä»–ä¼˜åŒ–æŠ€æœ¯ï¼ˆå¦‚é‡åŒ–ï¼‰ç»“åˆï¼Œä»¥è¿›ä¸€æ­¥åŠ é€Ÿæ¨ç†ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥å°†FlashAttention-2ä¸8ä½æˆ–4ä½é‡åŒ–ç»“åˆä½¿ç”¨ï¼š
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Expected speedups
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é¢„æœŸçš„åŠ é€Ÿ
- en: You can benefit from considerable speedups for inference, especially for inputs
    with long sequences. However, since FlashAttention-2 does not support computing
    attention scores with padding tokens, you must manually pad/unpad the attention
    scores for batched inference when the sequence contains padding tokens. This leads
    to a significant slowdown for batched generations with padding tokens.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä»æ¨ç†ä¸­è·å¾—ç›¸å½“å¤§çš„åŠ é€Ÿï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…·æœ‰é•¿åºåˆ—çš„è¾“å…¥ã€‚ä½†æ˜¯ï¼Œç”±äºFlashAttention-2ä¸æ”¯æŒä½¿ç”¨å¡«å……ä»¤ç‰Œè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œå› æ­¤åœ¨åºåˆ—åŒ…å«å¡«å……ä»¤ç‰Œæ—¶ï¼Œæ‚¨å¿…é¡»æ‰‹åŠ¨å¡«å……/å–æ¶ˆå¡«å……æ³¨æ„åŠ›åˆ†æ•°ä»¥è¿›è¡Œæ‰¹é‡æ¨ç†ã€‚è¿™ä¼šå¯¼è‡´ä½¿ç”¨å¡«å……ä»¤ç‰Œè¿›è¡Œæ‰¹é‡ç”Ÿæˆæ—¶å‡ºç°æ˜¾ç€å‡é€Ÿã€‚
- en: To overcome this, you should use FlashAttention-2 without padding tokens in
    the sequence during training (by packing a dataset or [concatenating sequences](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L516)
    until reaching the maximum sequence length).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œåœ¨è®­ç»ƒæœŸé—´åº”è¯¥ä½¿ç”¨ä¸å¸¦å¡«å……ä»¤ç‰Œçš„FlashAttention-2ï¼ˆé€šè¿‡æ‰“åŒ…æ•°æ®é›†æˆ–[è¿æ¥åºåˆ—](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L516)ç›´åˆ°è¾¾åˆ°æœ€å¤§åºåˆ—é•¿åº¦ï¼‰ã€‚
- en: 'For a single forward pass on [tiiuae/falcon-7b](https://hf.co/tiiuae/falcon-7b)
    with a sequence length of 4096 and various batch sizes without padding tokens,
    the expected speedup is:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåœ¨[tiiuae/falcon-7b](https://hf.co/tiiuae/falcon-7b)ä¸Šè¿›è¡Œå•æ¬¡å‰å‘ä¼ é€’ï¼Œåºåˆ—é•¿åº¦ä¸º4096ï¼Œå„ç§æ‰¹é‡å¤§å°ä¸”æ²¡æœ‰å¡«å……ä»¤ç‰Œï¼Œé¢„æœŸçš„åŠ é€Ÿæ˜¯ï¼š
- en: '![](../Images/463d3f3c66f2489865a258a5082f46f7.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/463d3f3c66f2489865a258a5082f46f7.png)'
- en: 'For a single forward pass on [meta-llama/Llama-7b-hf](https://hf.co/meta-llama/Llama-7b-hf)
    with a sequence length of 4096 and various batch sizes without padding tokens,
    the expected speedup is:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåœ¨[meta-llama/Llama-7b-hf](https://hf.co/meta-llama/Llama-7b-hf)ä¸Šè¿›è¡Œå•æ¬¡å‰å‘ä¼ é€’ï¼Œåºåˆ—é•¿åº¦ä¸º4096ï¼Œå„ç§æ‰¹é‡å¤§å°ä¸”æ²¡æœ‰å¡«å……ä»¤ç‰Œï¼Œé¢„æœŸçš„åŠ é€Ÿæ˜¯ï¼š
- en: '![](../Images/7ae0be2e7a0a9e0f4d275f8884f4d7d3.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ae0be2e7a0a9e0f4d275f8884f4d7d3.png)'
- en: 'For sequences with padding tokens (generating with padding tokens), you need
    to unpad/pad the input sequences to correctly compute the attention scores. With
    a relatively small sequence length, a single forward pass creates overhead leading
    to a small speedup (in the example below, 30% of the input is filled with padding
    tokens):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå…·æœ‰å¡«å……ä»¤ç‰Œçš„åºåˆ—ï¼ˆä½¿ç”¨å¡«å……ä»¤ç‰Œç”Ÿæˆï¼‰ï¼Œæ‚¨éœ€è¦å–æ¶ˆå¡«å……/å¡«å……è¾“å…¥åºåˆ—ä»¥æ­£ç¡®è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ã€‚å¯¹äºç›¸å¯¹è¾ƒå°çš„åºåˆ—é•¿åº¦ï¼Œå•æ¬¡å‰å‘ä¼ é€’ä¼šäº§ç”Ÿé¢å¤–å¼€é”€ï¼Œå¯¼è‡´è½»å¾®åŠ é€Ÿï¼ˆåœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œè¾“å…¥çš„30%å¡«å……æœ‰å¡«å……ä»¤ç‰Œï¼‰ï¼š
- en: '![](../Images/44a86fa9a8504c27decb2bf7133621cb.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44a86fa9a8504c27decb2bf7133621cb.png)'
- en: 'But for larger sequence lengths, you can expect even more speedup benefits:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¯¹äºæ›´å¤§çš„åºåˆ—é•¿åº¦ï¼Œæ‚¨å¯ä»¥æœŸæœ›è·å¾—æ›´å¤šçš„åŠ é€Ÿæ•ˆç›Šï¼š
- en: FlashAttention is more memory efficient, meaning you can train on much larger
    sequence lengths without running into out-of-memory issues. You can potentially
    reduce memory usage up to 20x for larger sequence lengths. Take a look at the
    [flash-attention](https://github.com/Dao-AILab/flash-attention) repository for
    more details.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: FlashAttentionæ›´å…·å†…å­˜æ•ˆç‡ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥åœ¨æ›´å¤§çš„åºåˆ—é•¿åº¦ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè€Œä¸ä¼šé‡åˆ°å†…å­˜ä¸è¶³çš„é—®é¢˜ã€‚å¯¹äºæ›´å¤§çš„åºåˆ—é•¿åº¦ï¼Œæ‚¨å¯ä»¥å°†å†…å­˜ä½¿ç”¨é‡é™ä½å¤šè¾¾20å€ã€‚æŸ¥çœ‹[flash-attention](https://github.com/Dao-AILab/flash-attention)å­˜å‚¨åº“ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
- en: '![](../Images/36c674b28857433397883375e3a3644d.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36c674b28857433397883375e3a3644d.png)'
- en: PyTorch scaled dot product attention
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorchç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
- en: PyTorchâ€™s [`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)
    (SDPA) can also call FlashAttention and memory-efficient attention kernels under
    the hood. SDPA support is currently being added natively in Transformers and is
    used by default for `torch>=2.1.1` when an implementation is available.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchçš„[`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)ï¼ˆSDPAï¼‰ä¹Ÿå¯ä»¥åœ¨åº•å±‚è°ƒç”¨FlashAttentionå’Œå†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›æ ¸ã€‚å½“å¯ç”¨å®ç°æ—¶ï¼ŒSDPAæ”¯æŒç›®å‰æ­£åœ¨Transformersä¸­æœ¬åœ°æ·»åŠ ï¼Œå¹¶ä¸”åœ¨`torch>=2.1.1`æ—¶é»˜è®¤ç”¨äº`torch`ã€‚
- en: 'For now, Transformers supports SDPA inference and training for the following
    architectures:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼ŒTransformersæ”¯æŒä»¥ä¸‹æ¶æ„çš„SDPAæ¨ç†å’Œè®­ç»ƒï¼š
- en: '[Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)'
- en: '[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)'
- en: '[Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)'
- en: '[Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)'
- en: '[Idefics](https://huggingface.co/docs/transformers/model_doc/idefics#transformers.IdeficsModel)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Idefics](https://huggingface.co/docs/transformers/model_doc/idefics#transformers.IdeficsModel)'
- en: '[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)'
- en: '[Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)'
- en: '[Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)'
- en: '[Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model)'
- en: FlashAttention can only be used for models with the `fp16` or `bf16` torch type,
    so make sure to cast your model to the appropriate type first.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: FlashAttentionåªèƒ½ç”¨äºå…·æœ‰`fp16`æˆ–`bf16` torchç±»å‹çš„æ¨¡å‹ï¼Œå› æ­¤è¯·ç¡®ä¿é¦–å…ˆå°†æ‚¨çš„æ¨¡å‹è½¬æ¢ä¸ºé€‚å½“çš„ç±»å‹ã€‚
- en: 'By default, SDPA selects the most performant kernel available but you can check
    whether a backend is available in a given setting (hardware, problem size) with
    [`torch.backends.cuda.sdp_kernel`](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel)
    as a context manager:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼ŒSDPAé€‰æ‹©æœ€é«˜æ•ˆçš„å¯ç”¨å†…æ ¸ï¼Œä½†æ‚¨å¯ä»¥ä½¿ç”¨[`torch.backends.cuda.sdp_kernel`](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel)ä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¥æ£€æŸ¥åœ¨ç»™å®šè®¾ç½®ï¼ˆç¡¬ä»¶ã€é—®é¢˜å¤§å°ï¼‰ä¸­æ˜¯å¦æœ‰å¯ç”¨çš„åç«¯ï¼š
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you see a bug with the traceback below, try using the nightly version of
    PyTorch which may have broader coverage for FlashAttention:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çœ‹åˆ°ä¸‹é¢çš„å›æº¯ä¸­æœ‰é”™è¯¯ï¼Œè¯·å°è¯•ä½¿ç”¨PyTorchçš„å¤œé—´ç‰ˆæœ¬ï¼Œè¿™å¯èƒ½å¯¹FlashAttentionæœ‰æ›´å¹¿æ³›çš„è¦†ç›–èŒƒå›´ï¼š
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: BetterTransformer
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BetterTransformer
- en: Some BetterTransformer features are being upstreamed to Transformers with default
    support for native `torch.nn.scaled_dot_product_attention`. BetterTransformer
    still has a wider coverage than the Transformers SDPA integration, but you can
    expect more and more architectures to natively support SDPA in Transformers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›BetterTransformeråŠŸèƒ½æ­£åœ¨è¢«ä¸Šæ¸¸åˆ°Transformersï¼Œæ”¯æŒæœ¬æœº`torch.nn.scaled_dot_product_attention`ã€‚BetterTransformerä»ç„¶æ¯”Transformers
    SDPAé›†æˆå…·æœ‰æ›´å¹¿æ³›çš„è¦†ç›–èŒƒå›´ï¼Œä½†æ‚¨å¯ä»¥æœŸæœ›è¶Šæ¥è¶Šå¤šçš„æ¶æ„åœ¨Transformersä¸­æœ¬åœ°æ”¯æŒSDPAã€‚
- en: Check out our benchmarks with BetterTransformer and scaled dot product attention
    in the [Out of the box acceleration and memory savings of ğŸ¤— decoder models with
    PyTorch 2.0](https://pytorch.org/blog/out-of-the-box-acceleration/) and learn
    more about the fastpath execution in the [BetterTransformer](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)
    blog post.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹æˆ‘ä»¬åœ¨[PyTorch 2.0ä¸­ä½¿ç”¨BetterTransformerå’Œç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„å¼€ç®±å³ç”¨åŠ é€Ÿå’Œå†…å­˜èŠ‚çœ](https://pytorch.org/blog/out-of-the-box-acceleration/)ä¸­çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨[BetterTransformer](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)åšå®¢æ–‡ç« ä¸­äº†è§£æ›´å¤šå…³äºå¿«é€Ÿæ‰§è¡Œçš„ä¿¡æ¯ã€‚
- en: 'BetterTransformer accelerates inference with its fastpath (native PyTorch specialized
    implementation of Transformer functions) execution. The two optimizations in the
    fastpath execution are:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: BetterTransformeré€šè¿‡å…¶å¿«é€Ÿè·¯å¾„ï¼ˆTransformerå‡½æ•°çš„æœ¬æœºPyTorchä¸“ç”¨å®ç°ï¼‰æ‰§è¡ŒåŠ é€Ÿæ¨æ–­ã€‚å¿«é€Ÿè·¯å¾„æ‰§è¡Œä¸­çš„ä¸¤ä¸ªä¼˜åŒ–æ˜¯ï¼š
- en: fusion, which combines multiple sequential operations into a single â€œkernelâ€
    to reduce the number of computation steps
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: èåˆï¼Œå°†å¤šä¸ªè¿ç»­æ“ä½œç»„åˆæˆä¸€ä¸ªå•ä¸€çš„â€œå†…æ ¸â€ï¼Œä»¥å‡å°‘è®¡ç®—æ­¥éª¤çš„æ•°é‡
- en: skipping the inherent sparsity of padding tokens to avoid unnecessary computation
    with nested tensors
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è·³è¿‡å¡«å……ä»¤ç‰Œçš„å›ºæœ‰ç¨€ç–æ€§ï¼Œä»¥é¿å…ä½¿ç”¨åµŒå¥—å¼ é‡è¿›è¡Œä¸å¿…è¦çš„è®¡ç®—
- en: BetterTransformer also converts all attention operations to use the more memory-efficient
    [scaled dot product attention (SDPA)](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention),
    and it calls optimized kernels like [FlashAttention](https://huggingface.co/papers/2205.14135)
    under the hood.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: BetterTransformerè¿˜å°†æ‰€æœ‰æ³¨æ„åŠ›æ“ä½œè½¬æ¢ä¸ºæ›´èŠ‚çœå†…å­˜çš„[scaled dot product attention (SDPA)](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)ï¼Œå¹¶åœ¨åº•å±‚è°ƒç”¨ä¼˜åŒ–çš„å†…æ ¸ï¼Œå¦‚[FlashAttention](https://huggingface.co/papers/2205.14135)ã€‚
- en: Before you start, make sure you have ğŸ¤— Optimum [installed](https://huggingface.co/docs/optimum/installation).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…ğŸ¤— Optimum [ï¼ˆå·²å®‰è£…ï¼‰](https://huggingface.co/docs/optimum/installation)ã€‚
- en: 'Then you can enable BetterTransformer with the [PreTrainedModel.to_bettertransformer()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.to_bettertransformer)
    method:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[PreTrainedModel.to_bettertransformer()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.to_bettertransformer)æ–¹æ³•å¯ç”¨BetterTransformerï¼š
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can return the original Transformers model with the [reverse_bettertransformer()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.reverse_bettertransformer)
    method. You should use this before saving your model to use the canonical Transformers
    modeling:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨[reverse_bettertransformer()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.reverse_bettertransformer)æ–¹æ³•è¿”å›åŸå§‹çš„Transformersæ¨¡å‹ã€‚åœ¨ä¿å­˜æ¨¡å‹ä¹‹å‰ï¼Œåº”è¯¥ä½¿ç”¨è¿™ä¸ªæ–¹æ³•æ¥ä½¿ç”¨è§„èŒƒçš„Transformerså»ºæ¨¡ï¼š
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: bitsandbytes
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: bitsandbytes
- en: bitsandbytes is a quantization library that includes support for 4-bit and 8-bit
    quantization. Quantization reduces your model size compared to its native full
    precision version, making it easier to fit large models onto GPUs with limited
    memory.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: bitsandbytesæ˜¯ä¸€ä¸ªåŒ…å«å¯¹4ä½å’Œ8ä½é‡åŒ–æ”¯æŒçš„é‡åŒ–åº“ã€‚ä¸å…¶åŸç”Ÿå…¨ç²¾åº¦ç‰ˆæœ¬ç›¸æ¯”ï¼Œé‡åŒ–å¯ä»¥å‡å°æ¨¡å‹å¤§å°ï¼Œä½¿å…¶æ›´å®¹æ˜“é€‚åº”å†…å­˜æœ‰é™çš„GPUã€‚
- en: 'Make sure you have bitsandbytes and ğŸ¤— Accelerate installed:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿æ‚¨å·²å®‰è£…bitsandbyteså’ŒğŸ¤— Accelerateï¼š
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 4-bit
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4ä½
- en: To load a model in 4-bit for inference, use the `load_in_4bit` parameter. The
    `device_map` parameter is optional, but we recommend setting it to `"auto"` to
    allow ğŸ¤— Accelerate to automatically and efficiently allocate the model given the
    available resources in the environment.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨4ä½æ¨¡å‹ä¸­è¿›è¡Œæ¨æ–­ï¼Œä½¿ç”¨`load_in_4bit`å‚æ•°ã€‚`device_map`å‚æ•°æ˜¯å¯é€‰çš„ï¼Œä½†æˆ‘ä»¬å»ºè®®å°†å…¶è®¾ç½®ä¸º`"auto"`ï¼Œä»¥ä¾¿ğŸ¤— Accelerateæ ¹æ®ç¯å¢ƒä¸­çš„å¯ç”¨èµ„æºè‡ªåŠ¨é«˜æ•ˆåœ°åˆ†é…æ¨¡å‹ã€‚
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To load a model in 4-bit for inference with multiple GPUs, you can control
    how much GPU RAM you want to allocate to each GPU. For example, to distribute
    600MB of memory to the first GPU and 1GB of memory to the second GPU:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨å¤šä¸ªGPUä¸ŠåŠ è½½4ä½æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼Œæ‚¨å¯ä»¥æ§åˆ¶è¦ä¸ºæ¯ä¸ªGPUåˆ†é…å¤šå°‘GPU RAMã€‚ä¾‹å¦‚ï¼Œå°†600MBçš„å†…å­˜åˆ†é…ç»™ç¬¬ä¸€ä¸ªGPUï¼Œå°†1GBçš„å†…å­˜åˆ†é…ç»™ç¬¬äºŒä¸ªGPUï¼š
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 8-bit
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8ä½
- en: If youâ€™re curious and interested in learning more about the concepts underlying
    8-bit quantization, read the [Gentle Introduction to 8-bit Matrix Multiplication
    for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration)
    blog post.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å¯¹8ä½é‡åŒ–çš„æ¦‚å¿µæ„Ÿå…´è¶£å¹¶æƒ³äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·é˜…è¯»[Hugging Face Transformersã€Accelerateå’Œbitsandbytesä½¿ç”¨è§„æ¨¡åŒ–å˜å‹å™¨è¿›è¡Œ8ä½çŸ©é˜µä¹˜æ³•çš„åˆæ­¥ä»‹ç»](https://huggingface.co/blog/hf-bitsandbytes-integration)åšå®¢æ–‡ç« ã€‚
- en: 'To load a model in 8-bit for inference, use the `load_in_8bit` parameter. The
    `device_map` parameter is optional, but we recommend setting it to `"auto"` to
    allow ğŸ¤— Accelerate to automatically and efficiently allocate the model given the
    available resources in the environment:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨8ä½æ¨¡å‹ä¸­è¿›è¡Œæ¨æ–­ï¼Œä½¿ç”¨`load_in_8bit`å‚æ•°ã€‚`device_map`å‚æ•°æ˜¯å¯é€‰çš„ï¼Œä½†æˆ‘ä»¬å»ºè®®å°†å…¶è®¾ç½®ä¸º`"auto"`ï¼Œä»¥ä¾¿ğŸ¤— Accelerateæ ¹æ®ç¯å¢ƒä¸­çš„å¯ç”¨èµ„æºè‡ªåŠ¨é«˜æ•ˆåœ°åˆ†é…æ¨¡å‹ï¼š
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If youâ€™re loading a model in 8-bit for text generation, you should use the
    [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    method instead of the [Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)
    function which is not optimized for 8-bit models and will be slower. Some sampling
    strategies, like nucleus sampling, are also not supported by the [Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)
    for 8-bit models. You should also place all inputs on the same device as the model:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨è¦åŠ è½½8ä½æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œåº”è¯¥ä½¿ç”¨[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)æ–¹æ³•ï¼Œè€Œä¸æ˜¯æœªç»ä¼˜åŒ–çš„[Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)å‡½æ•°ï¼Œåè€…å¯¹8ä½æ¨¡å‹ä¸é€‚ç”¨ä¸”é€Ÿåº¦è¾ƒæ…¢ã€‚ä¸€äº›é‡‡æ ·ç­–ç•¥ï¼Œå¦‚æ ¸é‡‡æ ·ï¼Œä¹Ÿä¸å—[Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)æ”¯æŒã€‚æ‚¨è¿˜åº”è¯¥å°†æ‰€æœ‰è¾“å…¥æ”¾åœ¨ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡ä¸Šï¼š
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To load a model in 4-bit for inference with multiple GPUs, you can control
    how much GPU RAM you want to allocate to each GPU. For example, to distribute
    1GB of memory to the first GPU and 2GB of memory to the second GPU:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨å¤šä¸ªGPUä¸ŠåŠ è½½4ä½æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼Œæ‚¨å¯ä»¥æ§åˆ¶è¦ä¸ºæ¯ä¸ªGPUåˆ†é…å¤šå°‘GPU RAMã€‚ä¾‹å¦‚ï¼Œè¦å°†1GBå†…å­˜åˆ†é…ç»™ç¬¬ä¸€ä¸ªGPUï¼Œå°†2GBå†…å­˜åˆ†é…ç»™ç¬¬äºŒä¸ªGPUï¼š
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Feel free to try running a 11 billion parameter [T5 model](https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing)
    or the 3 billion parameter [BLOOM model](https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing)
    for inference on Google Colabâ€™s free tier GPUs!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: éšæ„å°è¯•åœ¨Google Colabçš„å…è´¹GPUä¸Šè¿è¡Œä¸€ä¸ªæ‹¥æœ‰110äº¿å‚æ•°çš„[T5æ¨¡å‹](https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing)æˆ–30äº¿å‚æ•°çš„[BLOOMæ¨¡å‹](https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing)è¿›è¡Œæ¨æ–­ï¼
- en: ğŸ¤— Optimum
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸ¤— Optimum
- en: Learn more details about using ORT with ğŸ¤— Optimum in the [Accelerated inference
    on NVIDIA GPUs](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#accelerated-inference-on-nvidia-gpus)
    and [Accelerated inference on AMD GPUs](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/amdgpu#accelerated-inference-on-amd-gpus)
    guides. This section only provides a brief and simple example.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: äº†è§£æœ‰å…³åœ¨[NVIDIA GPUä¸Šè¿›è¡ŒåŠ é€Ÿæ¨æ–­](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#accelerated-inference-on-nvidia-gpus)å’Œ[AMD
    GPUä¸Šè¿›è¡ŒåŠ é€Ÿæ¨æ–­](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/amdgpu#accelerated-inference-on-amd-gpus)çš„æŒ‡å—ä¸­ä½¿ç”¨ORTçš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚æœ¬èŠ‚ä»…æä¾›ç®€è¦ä¸”ç®€å•çš„ç¤ºä¾‹ã€‚
- en: ONNX Runtime (ORT) is a model accelerator that supports accelerated inference
    on Nvidia GPUs, and AMD GPUs that use [ROCm](https://www.amd.com/en/products/software/rocm.html)
    stack. ORT uses optimization techniques like fusing common operations into a single
    node and constant folding to reduce the number of computations performed and speedup
    inference. ORT also places the most computationally intensive operations on the
    GPU and the rest on the CPU to intelligently distribute the workload between the
    two devices.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX Runtimeï¼ˆORTï¼‰æ˜¯ä¸€ä¸ªæ¨¡å‹åŠ é€Ÿå™¨ï¼Œæ”¯æŒåœ¨Nvidia GPUå’Œä½¿ç”¨[ROCm](https://www.amd.com/en/products/software/rocm.html)å †æ ˆçš„AMD
    GPUä¸Šè¿›è¡ŒåŠ é€Ÿæ¨æ–­ã€‚ORTä½¿ç”¨ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚å°†å¸¸è§æ“ä½œèåˆä¸ºå•ä¸ªèŠ‚ç‚¹å’Œå¸¸é‡æŠ˜å ï¼Œä»¥å‡å°‘æ‰§è¡Œçš„è®¡ç®—é‡å¹¶åŠ å¿«æ¨æ–­é€Ÿåº¦ã€‚ORTè¿˜å°†è®¡ç®—å¯†é›†å‹æ“ä½œæ”¾åœ¨GPUä¸Šï¼Œå…¶ä½™æ“ä½œæ”¾åœ¨CPUä¸Šï¼Œæ™ºèƒ½åœ°åœ¨ä¸¤ä¸ªè®¾å¤‡ä¹‹é—´åˆ†é…å·¥ä½œè´Ÿè½½ã€‚
- en: 'ORT is supported by ğŸ¤— Optimum which can be used in ğŸ¤— Transformers. Youâ€™ll need
    to use an [ORTModel](https://huggingface.co/docs/optimum/v1.16.2/en/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel)
    for the task youâ€™re solving, and specify the `provider` parameter which can be
    set to either [`CUDAExecutionProvider`](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#cudaexecutionprovider),
    [`ROCMExecutionProvider`](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/amdgpu)
    or [`TensorrtExecutionProvider`](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#tensorrtexecutionprovider).
    If you want to load a model that was not yet exported to ONNX, you can set `export=True`
    to convert your model on-the-fly to the ONNX format:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ORTå—ğŸ¤— Optimumæ”¯æŒï¼Œå¯ä»¥åœ¨ğŸ¤— Transformersä¸­ä½¿ç”¨ã€‚æ‚¨éœ€è¦ä½¿ç”¨ä¸€ä¸ª[ORTModel](https://huggingface.co/docs/optimum/v1.16.2/en/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel)æ¥è§£å†³æ‚¨çš„ä»»åŠ¡ï¼Œå¹¶æŒ‡å®š`provider`å‚æ•°ï¼Œå¯ä»¥è®¾ç½®ä¸º[`CUDAExecutionProvider`](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#cudaexecutionprovider)ã€[`ROCMExecutionProvider`](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/amdgpu)æˆ–[`TensorrtExecutionProvider`](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#tensorrtexecutionprovider)ã€‚å¦‚æœè¦åŠ è½½å°šæœªå¯¼å‡ºä¸ºONNXçš„æ¨¡å‹ï¼Œå¯ä»¥è®¾ç½®`export=True`å°†æ‚¨çš„æ¨¡å‹å³æ—¶è½¬æ¢ä¸ºONNXæ ¼å¼ï¼š
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now youâ€™re free to use the model for inference:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥è‡ªç”±åœ°ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼š
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Combine optimizations
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“åˆä¼˜åŒ–
- en: 'It is often possible to combine several of the optimization techniques described
    above to get the best inference performance possible for your model. For example,
    you can load a model in 4-bit, and then enable BetterTransformer with FlashAttention:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸å¯ä»¥ç»“åˆä¸Šè¿°æè¿°çš„å¤šç§ä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥è·å¾—æœ€ä½³çš„æ¨æ–­æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥åŠ è½½ä¸€ä¸ª4ä½æ¨¡å‹ï¼Œç„¶åå¯ç”¨å¸¦æœ‰FlashAttentionçš„BetterTransformerï¼š
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
