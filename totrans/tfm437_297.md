# ViTMAE

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vit_mae](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vit_mae)

## 概述

ViTMAE模型是由Kaiming He、Xinlei Chen、Saining Xie、Yanghao Li、Piotr Dollár、Ross Girshick在[Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377v2)中提出的。该论文表明，通过预训练视觉Transformer（ViT）以重建被屏蔽补丁的像素值，可以在微调后获得优于监督预训练的结果。

论文的摘要如下：

*本文表明，屏蔽自动编码器（MAE）是计算机视觉的可扩展自监督学习器。我们的MAE方法很简单：我们屏蔽输入图像的随机补丁并重建缺失的像素。它基于两个核心设计。首先，我们开发了一个不对称的编码器-解码器架构，其中编码器仅在可见的补丁子集上操作（没有掩码标记），以及一个轻量级的解码器，从潜在表示和掩码标记中重建原始图像。其次，我们发现屏蔽输入图像的高比例，例如75％，产生了一个非平凡且有意义的自监督任务。结合这两个设计，我们能够高效有效地训练大型模型：我们加速训练（3倍或更多）并提高准确性。我们的可扩展方法允许学习具有良好泛化性能的高容量模型：例如，一个普通的ViT-Huge模型在仅使用ImageNet-1K数据的方法中获得了最佳准确性（87.8％）。在下游任务中的转移性能优于监督预训练，并显示出有希望的扩展行为。*

![drawing](../Images/e071321e730064254f534bad073dc38d.png) MAE架构。摘自[原始论文。](https://arxiv.org/abs/2111.06377)

这个模型是由[nielsr](https://huggingface.co/nielsr)贡献的。TensorFlow版本的模型是由[sayakpaul](https://github.com/sayakpaul)和[ariG23498](https://github.com/ariG23498)贡献的（贡献相同）。原始代码可以在[这里](https://github.com/facebookresearch/mae)找到。

## 使用提示

+   MAE（屏蔽自动编码）是一种用于自监督预训练视觉Transformer（ViTs）的方法。预训练目标相对简单：通过屏蔽大部分（75％）的图像补丁，模型必须重建原始像素值。可以使用[ViTMAEForPreTraining](/docs/transformers/v4.37.2/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining)来实现这一目的。

+   在预训练之后，一个“丢弃”用于重建像素的解码器，使用编码器进行微调/线性探测。这意味着在微调之后，可以直接将权重插入到[ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)中。

+   可以使用[ViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor)来为模型准备图像。有关更多信息，请参阅代码示例。

+   请注意，MAE的编码器仅用于对视觉补丁进行编码。然后，编码的补丁与掩码标记连接在一起，解码器（也由Transformer块组成）将其作为输入。每个掩码标记是一个共享的、可学习的向量，指示要预测的缺失补丁的存在。固定的sin/cos位置嵌入被添加到编码器和解码器的输入中。

+   要了解MAE的工作原理，可以查看这篇[文章](https://keras.io/examples/vision/masked_image_modeling/)。

## 资源

以下是官方Hugging Face和社区（由🌎表示）资源列表，可帮助您开始使用ViTMAE。

+   [ViTMAEForPreTraining](/docs/transformers/v4.37.2/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining)由这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining)支持，允许您从头开始预训练模型/在自定义数据上进一步预训练模型。

+   可以在[这里](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/ViTMAE/ViT_MAE_visualization_demo.ipynb)找到一个演示如何使用[ViTMAEForPreTraining](/docs/transformers/v4.37.2/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining)可视化重建像素值的笔记本。

如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将进行审查！资源应该展示一些新内容，而不是重复现有资源。

## ViTMAEConfig

### `class transformers.ViTMAEConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_mae/configuration_vit_mae.py#L29)

```py
( hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-12 image_size = 224 patch_size = 16 num_channels = 3 qkv_bias = True decoder_num_attention_heads = 16 decoder_hidden_size = 512 decoder_num_hidden_layers = 8 decoder_intermediate_size = 2048 mask_ratio = 0.75 norm_pix_loss = False **kwargs )
```

参数

+   `hidden_size` (`int`, *可选*, 默认为768) — 编码器层和池化层的维度。

+   `num_hidden_layers` (`int`, *可选*, 默认为12) — Transformer编码器中的隐藏层数。

+   `num_attention_heads` (`int`, *可选*, 默认为12) — Transformer编码器中每个注意力层的注意力头数。

+   `intermediate_size` (`int`, *可选*, 默认为3072) — Transformer编码器中“中间”（即前馈）层的维度。

+   `hidden_act` (`str` 或 `function`, *可选*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *可选*, 默认为0.0) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。

+   `attention_probs_dropout_prob` (`float`, *可选*, 默认为0.0) — 注意力概率的dropout比率。

+   `initializer_range` (`float`, *可选*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *可选*, 默认为1e-12) — 层归一化层使用的epsilon。

+   `image_size` (`int`, *可选*, 默认为224) — 每个图像的大小（分辨率）。

+   `patch_size` (`int`, *可选*, 默认为16) — 每个补丁的大小（分辨率）。

+   `num_channels` (`int`, *可选*, 默认为3) — 输入通道数。

+   `qkv_bias` (`bool`, *可选*, 默认为`True`) — 是否为查询、键和值添加偏置。

+   `decoder_num_attention_heads` (`int`, *可选*, 默认为16) — 解码器中每个注意力层的注意力头数。

+   `decoder_hidden_size` (`int`, *可选*, 默认为512) — 解码器的维度。

+   `decoder_num_hidden_layers` (`int`, *可选*, 默认为8) — 解码器中的隐藏层数。

+   `decoder_intermediate_size` (`int`, *可选*, 默认为2048) — 解码器中“中间”（即前馈）层的维度。

+   `mask_ratio` (`float`, *可选*, 默认为0.75) — 输入序列中被屏蔽的令牌数量的比例。

+   `norm_pix_loss` (`bool`, *可选*, 默认为`False`) — 是否使用归一化像素进行训练（参见论文中的表3）。在作者的实验中，使用归一化像素提高了表示质量。

这是一个配置类，用于存储[ViTMAEModel](/docs/transformers/v4.37.2/en/model_doc/vit_mae#transformers.ViTMAEModel)的配置。根据指定的参数实例化一个ViT MAE模型，定义模型架构。使用默认值实例化配置将产生类似于ViT [facebook/vit-mae-base](https://huggingface.co/facebook/vit-mae-base)架构的配置。

配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 并可用于控制模型输出。阅读 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 中的文档以获取更多信息。

示例：

```py
>>> from transformers import ViTMAEConfig, ViTMAEModel

>>> # Initializing a ViT MAE vit-mae-base style configuration
>>> configuration = ViTMAEConfig()

>>> # Initializing a model (with random weights) from the vit-mae-base style configuration
>>> model = ViTMAEModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

PytorchHide Pytorch content

## ViTMAEModel

### `class transformers.ViTMAEModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_mae/modeling_vit_mae.py#L623)

```py
( config )
```

参数

+   `config` ([ViTMAEConfig](/docs/transformers/v4.37.2/en/model_doc/vit_mae#transformers.ViTMAEConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

裸的 ViTMAE 模型变压器输出原始隐藏状态，没有特定的头部。这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_mae/modeling_vit_mae.py#L651)

```py
( pixel_values: Optional = None noise: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.vit_mae.modeling_vit_mae.ViTMAEModelOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) — 像素值。可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor) 获取像素值。有关详细信息，请参阅 [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值选择在 `[0, 1]` 之间：

    +   1 表示头部未被 `masked`，

    +   0 表示头部被 `masked`。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请查看返回的张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回的张量下的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

返回

`transformers.models.vit_mae.modeling_vit_mae.ViTMAEModelOutput` 或 `tuple(torch.FloatTensor)`

一个 `transformers.models.vit_mae.modeling_vit_mae.ViTMAEModelOutput` 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时），包含根据配置 ([ViTMAEConfig](/docs/transformers/v4.37.2/en/model_doc/vit_mae#transformers.ViTMAEConfig)) 和输入的各种元素。

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) — 指示哪些补丁被掩码（1）哪些没有被掩码（0）的张量。

+   `ids_restore` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 包含（打乱的）被掩码的补丁的原始索引的张量。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_hidden_states=True` 或当 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出 + 一个用于每个层的输出）。模型在每一层的输出的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选的*，当传递了 `output_attentions=True` 或当 `config.output_attentions=True` 时返回） — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。注意力 softmax 后的注意力权重，用于计算自注意力头部中的加权平均值。

[ViTMAEModel](/docs/transformers/v4.37.2/en/model_doc/vit_mae#transformers.ViTMAEModel) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, ViTMAEModel
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("facebook/vit-mae-base")
>>> model = ViTMAEModel.from_pretrained("facebook/vit-mae-base")

>>> inputs = image_processor(images=image, return_tensors="pt")
>>> outputs = model(**inputs)
>>> last_hidden_states = outputs.last_hidden_state
```

## ViTMAEForPreTraining

### `class transformers.ViTMAEForPreTraining`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_mae/modeling_vit_mae.py#L820)

```py
( config )
```

参数

+   `config` ([ViTMAEConfig](/docs/transformers/v4.37.2/en/model_doc/vit_mae#transformers.ViTMAEConfig)) — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

带有顶部解码器的 ViTMAE 模型变压器，用于自监督预训练。

请注意，我们在我们的[示例目录](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining)中提供了一个脚本，用于在自定义数据上预训练此模型。

此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般使用和行为相关的所有事项。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_mae/modeling_vit_mae.py#L946)

```py
( pixel_values: Optional = None noise: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.vit_mae.modeling_vit_mae.ViTMAEForPreTrainingOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height, width)`) — 像素值。可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor) 获取像素值。有关详细信息，请参阅 [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `head_mask` (`torch.FloatTensor`，形状为 `(num_heads,)` 或 `(num_layers, num_heads)`，*可选的*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选定在 `[0, 1]` 中：

    +   1 表示头部是 `not masked`。

    +   0 表示头部是 `masked`。

+   `output_attentions` (`bool`，*可选的*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请查看返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`，*可选的*) — 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回张量下的 `hidden_states`。

+   `return_dict` (`bool`，*可选的*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通的元组。

返回

`transformers.models.vit_mae.modeling_vit_mae.ViTMAEForPreTrainingOutput` 或 `tuple(torch.FloatTensor)`

一个 `transformers.models.vit_mae.modeling_vit_mae.ViTMAEForPreTrainingOutput` 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（[ViTMAEConfig](/docs/transformers/v4.37.2/en/model_doc/vit_mae#transformers.ViTMAEConfig)）和输入的各种元素。

+   `loss` (`torch.FloatTensor`，形状为 `(1,)`) — 像素重建损失。

+   `logits` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, patch_size ** 2 * num_channels)`) — 像素重建 logits。

+   `mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）—指示哪些补丁被屏蔽（1）哪些没有被屏蔽（0）的张量。

+   `ids_restore`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）—包含（打乱的）被屏蔽补丁的原始索引的张量。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[ViTMAEForPreTraining](/docs/transformers/v4.37.2/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, ViTMAEForPreTraining
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("facebook/vit-mae-base")
>>> model = ViTMAEForPreTraining.from_pretrained("facebook/vit-mae-base")

>>> inputs = image_processor(images=image, return_tensors="pt")
>>> outputs = model(**inputs)
>>> loss = outputs.loss
>>> mask = outputs.mask
>>> ids_restore = outputs.ids_restore
```

隐藏TensorFlow内容

## TFViTMAEModel

### `class transformers.TFViTMAEModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_mae/modeling_tf_vit_mae.py#L884)

```py
( config: ViTMAEConfig *inputs **kwargs )
```

参数

+   `config`（[ViTMAEConfig](/docs/transformers/v4.37.2/en/model_doc/vit_mae#transformers.ViTMAEConfig)）—具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

裸ViTMAE模型变压器输出原始隐藏状态，没有任何特定的头部。该模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型也是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。

`transformers`中的TensorFlow模型和层接受两种格式的输入：

+   所有输入都作为关键字参数（类似于PyTorch模型），或者

+   所有输入都作为列表、元组或字典的第一个位置参数。

支持第二种格式的原因是，Keras方法在将输入传递给模型和层时更喜欢这种格式。由于这种支持，当使用`model.fit()`等方法时，应该“只需工作” - 只需以`model.fit()`支持的任何格式传递输入和标签！但是，如果您想在Keras方法之外使用第二种格式，例如在使用Keras`Functional` API创建自己的层或模型时，有三种可能性可以用来收集所有输入张量在第一个位置参数中：

+   只有`pixel_values`的单个张量，没有其他内容：`model(pixel_values)`

+   具有给定顺序的一个或多个输入张量的长度可变列表：`model([pixel_values, attention_mask])`或`model([pixel_values, attention_mask, token_type_ids])`

+   一个字典，其中包含与文档字符串中给定的输入名称相关联的一个或多个输入张量：`model({"pixel_values": pixel_values, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些内容，因为您可以像对待任何其他Python函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_mae/modeling_tf_vit_mae.py#L897)

```py
( pixel_values: TFModelInputType | None = None noise: tf.Tensor = None head_mask: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';transformers.models.vit_mae.modeling_tf_vit_mae.TFViTMAEModelOutput or tuple(tf.Tensor)
```

参数

+   `pixel_values`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例的形状必须为`(batch_size, num_channels, height, width)`）— 像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`np.ndarray`或`tf.Tensor`，*可选*）— 用于使自注意力模块中的选定头部失效的掩码。掩码值选定在`[0, 1]`中：

    +   1表示头部未被屏蔽，

    +   0表示头部被屏蔽。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。此参数仅可在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。此参数仅可在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可以在急切模式下使用，在图模式下该值将始终设置为True。

+   `training`（`bool`，*可选*，默认为`False`）— 是否在训练模式下使用模型（一些模块，如dropout模块，在训练和评估之间具有不同的行为）。

返回

`transformers.models.vit_mae.modeling_tf_vit_mae.TFViTMAEModelOutput`或`tuple(tf.Tensor)`

一个`transformers.models.vit_mae.modeling_tf_vit_mae.TFViTMAEModelOutput`或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[ViTMAEConfig](/docs/transformers/v4.37.2/en/model_doc/vit_mae#transformers.ViTMAEConfig)）和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`）— 模型最后一层的隐藏状态序列的输出。

+   `mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`）— 指示哪些补丁被屏蔽（1）哪些未被屏蔽（0）的张量。

+   `ids_restore`（形状为`(batch_size, sequence_length)`的`tf.Tensor`）— 包含（打乱的）蒙版补丁的原始索引的张量。

+   `hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。模型在每层的输出隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[TFViTMAEModel](/docs/transformers/v4.37.2/en/model_doc/vit_mae#transformers.TFViTMAEModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, TFViTMAEModel
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("facebook/vit-mae-base")
>>> model = TFViTMAEModel.from_pretrained("facebook/vit-mae-base")

>>> inputs = image_processor(images=image, return_tensors="tf")
>>> outputs = model(**inputs)
>>> last_hidden_states = outputs.last_hidden_state
```

## TFViTMAEForPreTraining

### `class transformers.TFViTMAEForPreTraining`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_mae/modeling_tf_vit_mae.py#L1068)

```py
( config )
```

参数

+   `config`（[ViTMAEConfig](/docs/transformers/v4.37.2/en/model_doc/vit_mae#transformers.ViTMAEConfig)）—模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

ViTMAE模型变压器，顶部带有解码器，用于自监督预训练。此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存，调整输入嵌入，修剪头等）。

此模型还是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规TF 2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有相关信息。

`transformers`中的TensorFlow模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于PyTorch模型），或

+   将所有输入作为列表、元组或字典的第一个位置参数。

支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于这种支持，当使用`model.fit()`等方法时，应该“只需工作” - 只需传递`model.fit()`支持的任何格式的输入和标签！但是，如果您想在Keras方法之外使用第二种格式，例如在使用Keras`Functional`API创建自己的层或模型时，有三种可能性可用于收集第一个位置参数中的所有输入张量：

+   只有`pixel_values`的单个张量，没有其他内容：`model(pixel_values)`

+   一个长度可变的列表，其中包含一个或多个按照文档字符串中给定的顺序给出的输入张量：`model([pixel_values, attention_mask])`或`model([pixel_values, attention_mask, token_type_ids])`

+   一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"pixel_values": pixel_values, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像对待其他Python函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_mae/modeling_tf_vit_mae.py#L1196)

```py
( pixel_values: TFModelInputType | None = None noise: tf.Tensor = None head_mask: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';transformers.models.vit_mae.modeling_tf_vit_mae.TFViTMAEForPreTrainingOutput or tuple(tf.Tensor)
```

参数

+   `pixel_values`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例的形状必须为`(batch_size, num_channels, height, width)`）—像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`np.ndarray`或`tf.Tensor`，*可选*）—用于使自注意力模块中选择的头部失效的掩码。掩码值选择在`[0, 1]`中：

    +   1表示头部未被`屏蔽`，

    +   0表示头部被`屏蔽`。

+   `output_attentions`（`bool`，*可选*）—是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。此参数仅可在急切模式下使用，在图模式中将使用配置中的值。

+   `output_hidden_states`（`bool`，*可选*）—是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅可在急切模式下使用，在图模式中将使用配置中的值。

+   `return_dict`（`bool`，*可选*）—是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。此参数可在急切模式下使用，在图模式中该值将始终设置为True。

+   `training`（`bool`，*可选*，默认为`False`）—是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。

返回结果

`transformers.models.vit_mae.modeling_tf_vit_mae.TFViTMAEForPreTrainingOutput`或`tuple(tf.Tensor)`

一个`transformers.models.vit_mae.modeling_tf_vit_mae.TFViTMAEForPreTrainingOutput`或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[ViTMAEConfig](/docs/transformers/v4.37.2/en/model_doc/vit_mae#transformers.ViTMAEConfig)）和输入的各种元素。

+   `loss`（形状为`(1,)`的`tf.Tensor`）—像素重建损失。

+   `logits`（形状为`(batch_size, sequence_length, patch_size ** 2 * num_channels)`的`tf.Tensor`）—像素重建logits。

+   `mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`）—指示哪些补丁被屏蔽（1）哪些没有被屏蔽（0）的张量。

+   `ids_restore`（形状为`(batch_size, sequence_length)`的`tf.Tensor`）—包含（打乱的）被屏蔽补丁的原始索引的张量。

+   `hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组。模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[TFViTMAEForPreTraining](/docs/transformers/v4.37.2/en/model_doc/vit_mae#transformers.TFViTMAEForPreTraining)前向方法，覆盖`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, TFViTMAEForPreTraining
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("facebook/vit-mae-base")
>>> model = TFViTMAEForPreTraining.from_pretrained("facebook/vit-mae-base")

>>> inputs = image_processor(images=image, return_tensors="pt")
>>> outputs = model(**inputs)
>>> loss = outputs.loss
>>> mask = outputs.mask
>>> ids_restore = outputs.ids_restore
```
