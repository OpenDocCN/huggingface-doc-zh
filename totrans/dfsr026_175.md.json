["```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPFeatureExtractor requires_safety_checker: bool = True )\n```", "```py\n( prompt: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 50 guidance_scale: float = 7.5 gligen_scheduled_sampling_beta: float = 0.3 gligen_phrases: List = None gligen_boxes: List = None gligen_inpaint_image: Optional = None negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None clip_skip: Optional = None ) \u2192 export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple\n```", "```py\n>>> import torch\n>>> from diffusers import StableDiffusionGLIGENPipeline\n>>> from diffusers.utils import load_image\n\n>>> # Insert objects described by text at the region defined by bounding boxes\n>>> pipe = StableDiffusionGLIGENPipeline.from_pretrained(\n...     \"masterful/gligen-1-4-inpainting-text-box\", variant=\"fp16\", torch_dtype=torch.float16\n... )\n>>> pipe = pipe.to(\"cuda\")\n\n>>> input_image = load_image(\n...     \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/gligen/livingroom_modern.png\"\n... )\n>>> prompt = \"a birthday cake\"\n>>> boxes = [[0.2676, 0.6088, 0.4773, 0.7183]]\n>>> phrases = [\"a birthday cake\"]\n\n>>> images = pipe(\n...     prompt=prompt,\n...     gligen_phrases=phrases,\n...     gligen_inpaint_image=input_image,\n...     gligen_boxes=boxes,\n...     gligen_scheduled_sampling_beta=1,\n...     output_type=\"pil\",\n...     num_inference_steps=50,\n... ).images\n\n>>> images[0].save(\"./gligen-1-4-inpainting-text-box.jpg\")\n\n>>> # Generate an image described by the prompt and\n>>> # insert objects described by text at the region defined by bounding boxes\n>>> pipe = StableDiffusionGLIGENPipeline.from_pretrained(\n...     \"masterful/gligen-1-4-generation-text-box\", variant=\"fp16\", torch_dtype=torch.float16\n... )\n>>> pipe = pipe.to(\"cuda\")\n\n>>> prompt = \"a waterfall and a modern high speed train running through the tunnel in a beautiful forest with fall foliage\"\n>>> boxes = [[0.1387, 0.2051, 0.4277, 0.7090], [0.4980, 0.4355, 0.8516, 0.7266]]\n>>> phrases = [\"a waterfall\", \"a modern high speed train running through the tunnel\"]\n\n>>> images = pipe(\n...     prompt=prompt,\n...     gligen_phrases=phrases,\n...     gligen_boxes=boxes,\n...     gligen_scheduled_sampling_beta=1,\n...     output_type=\"pil\",\n...     num_inference_steps=50,\n... ).images\n\n>>> images[0].save(\"./gligen-1-4-generation-text-box.jpg\")\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( gpu_id: Optional = None device: Union = 'cuda' )\n```", "```py\n( batch_size num_channels_latents height width dtype device generator latents = None )\n```", "```py\n( enabled = True )\n```", "```py\n( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```", "```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer processor: CLIPProcessor image_encoder: CLIPVisionModelWithProjection image_project: CLIPImageProjection unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPFeatureExtractor requires_safety_checker: bool = True )\n```", "```py\n( prompt: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 50 guidance_scale: float = 7.5 gligen_scheduled_sampling_beta: float = 0.3 gligen_phrases: List = None gligen_images: List = None input_phrases_mask: Union = None input_images_mask: Union = None gligen_boxes: List = None gligen_inpaint_image: Optional = None negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None gligen_normalize_constant: float = 28.7 clip_skip: int = None ) \u2192 export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple\n```", "```py\n>>> import torch\n>>> from diffusers import StableDiffusionGLIGENTextImagePipeline\n>>> from diffusers.utils import load_image\n\n>>> # Insert objects described by image at the region defined by bounding boxes\n>>> pipe = StableDiffusionGLIGENTextImagePipeline.from_pretrained(\n...     \"anhnct/Gligen_Inpainting_Text_Image\", torch_dtype=torch.float16\n... )\n>>> pipe = pipe.to(\"cuda\")\n\n>>> input_image = load_image(\n...     \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/gligen/livingroom_modern.png\"\n... )\n>>> prompt = \"a backpack\"\n>>> boxes = [[0.2676, 0.4088, 0.4773, 0.7183]]\n>>> phrases = None\n>>> gligen_image = load_image(\n...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/gligen/backpack.jpeg\"\n... )\n\n>>> images = pipe(\n...     prompt=prompt,\n...     gligen_phrases=phrases,\n...     gligen_inpaint_image=input_image,\n...     gligen_boxes=boxes,\n...     gligen_images=[gligen_image],\n...     gligen_scheduled_sampling_beta=1,\n...     output_type=\"pil\",\n...     num_inference_steps=50,\n... ).images\n\n>>> images[0].save(\"./gligen-inpainting-text-image-box.jpg\")\n\n>>> # Generate an image described by the prompt and\n>>> # insert objects described by text and image at the region defined by bounding boxes\n>>> pipe = StableDiffusionGLIGENTextImagePipeline.from_pretrained(\n...     \"anhnct/Gligen_Text_Image\", torch_dtype=torch.float16\n... )\n>>> pipe = pipe.to(\"cuda\")\n\n>>> prompt = \"a flower sitting on the beach\"\n>>> boxes = [[0.0, 0.09, 0.53, 0.76]]\n>>> phrases = [\"flower\"]\n>>> gligen_image = load_image(\n...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/gligen/pexels-pixabay-60597.jpg\"\n... )\n\n>>> images = pipe(\n...     prompt=prompt,\n...     gligen_phrases=phrases,\n...     gligen_images=[gligen_image],\n...     gligen_boxes=boxes,\n...     gligen_scheduled_sampling_beta=1,\n...     output_type=\"pil\",\n...     num_inference_steps=50,\n... ).images\n\n>>> images[0].save(\"./gligen-generation-text-image-box.jpg\")\n\n>>> # Generate an image described by the prompt and\n>>> # transfer style described by image at the region defined by bounding boxes\n>>> pipe = StableDiffusionGLIGENTextImagePipeline.from_pretrained(\n...     \"anhnct/Gligen_Text_Image\", torch_dtype=torch.float16\n... )\n>>> pipe = pipe.to(\"cuda\")\n\n>>> prompt = \"a dragon flying on the sky\"\n>>> boxes = [[0.4, 0.2, 1.0, 0.8], [0.0, 1.0, 0.0, 1.0]]  # Set `[0.0, 1.0, 0.0, 1.0]` for the style\n\n>>> gligen_image = load_image(\n...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png\"\n... )\n\n>>> gligen_placeholder = load_image(\n...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png\"\n... )\n\n>>> images = pipe(\n...     prompt=prompt,\n...     gligen_phrases=[\n...         \"dragon\",\n...         \"placeholder\",\n...     ],  # Can use any text instead of `placeholder` token, because we will use mask here\n...     gligen_images=[\n...         gligen_placeholder,\n...         gligen_image,\n...     ],  # Can use any image in gligen_placeholder, because we will use mask here\n...     input_phrases_mask=[1, 0],  # Set 0 for the placeholder token\n...     input_images_mask=[0, 1],  # Set 0 for the placeholder image\n...     gligen_boxes=boxes,\n...     gligen_scheduled_sampling_beta=1,\n...     output_type=\"pil\",\n...     num_inference_steps=50,\n... ).images\n\n>>> images[0].save(\"./gligen-generation-text-image-box-style-transfer.jpg\")\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( gpu_id: Optional = None device: Union = 'cuda' )\n```", "```py\n( batch_size num_channels_latents height width dtype device generator latents = None )\n```", "```py\n( enabled = True )\n```", "```py\n( has_mask max_objs device )\n```", "```py\n( im new_width new_height )\n```", "```py\n( boxes size )\n```", "```py\n( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```", "```py\n( input normalize_constant device is_image = False )\n```", "```py\n( hidden_size gligen_phrases gligen_images gligen_boxes input_phrases_mask input_images_mask repeat_batch normalize_constant max_objs device )\n```", "```py\n( hidden_size repeat_batch max_objs device )\n```", "```py\n( im new_hw )\n```", "```py\n( images: Union nsfw_content_detected: Optional )\n```"]