["```py\n>>> import evaluate\n>>> accuracy = evaluate.load(\"accuracy\")\n```", "```py\n>>> word_length = evaluate.load(\"word_length\", module_type=\"measurement\")\n```", "```py\n>>> element_count = evaluate.load(\"lvwerra/element_count\", module_type=\"measurement\")\n```", "```py\n>>> evaluate.list_evaluation_modules(\n...   module_type=\"comparison\",\n...   include_community=False,\n...   with_details=True)\n\n[{'name': 'mcnemar', 'type': 'comparison', 'community': False, 'likes': 1},\n {'name': 'exact_match', 'type': 'comparison', 'community': False, 'likes': 0}]\n```", "```py\n>>> accuracy = evaluate.load(\"accuracy\")\n>>> accuracy.description\nAccuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\nAccuracy = (TP + TN) / (TP + TN + FP + FN)\n Where:\nTP: True positive\nTN: True negative\nFP: False positive\nFN: False negative\n```", "```py\n>>> accuracy.citation\n@article{scikit-learn,\n  title={Scikit-learn: Machine Learning in {P}ython},\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n  journal={Journal of Machine Learning Research},\n  volume={12},\n  pages={2825--2830},\n  year={2011}\n}\n```", "```py\n>>> accuracy.features\n{\n    'predictions': Value(dtype='int32', id=None),\n    'references': Value(dtype='int32', id=None)\n}\n```", "```py\n>>> accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1])\n{'accuracy': 0.5}\n```", "```py\n>>> for ref, pred in zip([0,1,0,1], [1,0,0,1]):\n>>>     accuracy.add(references=ref, predictions=pred)\n>>> accuracy.compute()\n{'accuracy': 0.5}\n```", "```py\n>>> for refs, preds in zip([[0,1],[0,1]], [[1,0],[0,1]]):\n>>>     accuracy.add_batch(references=refs, predictions=preds)\n>>> accuracy.compute()\n{'accuracy': 0.5}\n```", "```py\n>>> for model_inputs, gold_standards in evaluation_dataset:\n>>>     predictions = model(model_inputs)\n>>>     metric.add_batch(references=gold_standards, predictions=predictions)\n>>> metric.compute()\n```", "```py\n>>> clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n```", "```py\n>>> clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])\n\n{\n  'accuracy': 0.667,\n  'f1': 0.667,\n  'precision': 1.0,\n  'recall': 0.5\n}\n```", "```py\n>>> result = accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1])\n\n>>> hyperparams = {\"model\": \"bert-base-uncased\"}\n>>> evaluate.save(\"./results/\"experiment=\"run 42\", **result, **hyperparams)\nPosixPath('results/result-2022_05_30-22_09_11.json')\n```", "```py\n{\n    \"experiment\": \"run 42\",\n    \"accuracy\": 0.5,\n    \"model\": \"bert-base-uncased\",\n    \"_timestamp\": \"2022-05-30T22:09:11.959469\",\n    \"_git_commit_hash\": \"123456789abcdefghijkl\",\n    \"_evaluate_version\": \"0.1.0\",\n    \"_python_version\": \"3.9.12 (main, Mar 26 2022, 15:51:15) \\n[Clang 13.1.6 (clang-1316.0.21.2)]\",\n    \"_interpreter_path\": \"/Users/leandro/git/evaluate/env/bin/python\"\n}\n```", "```py\nevaluate.push_to_hub(\n  model_id=\"huggingface/gpt2-wikitext2\",  # model repository on hub\n  metric_value=0.5,                       # metric value\n  metric_type=\"bleu\",                     # metric name, e.g. accuracy.name\n  metric_name=\"BLEU\",                     # pretty name which is displayed\n  dataset_type=\"wikitext\",                # dataset name on the hub\n  dataset_name=\"WikiText\",                # pretty name\n  dataset_split=\"test\",                   # dataset split used\n  task_type=\"text-generation\",            # task id, see https://github.com/huggingface/datasets/blob/master/src/datasets/utils/resources/tasks.json\n  task_name=\"Text Generation\"             # pretty name for task\n)\n```", "```py\nfrom transformers import pipeline\nfrom datasets import load_dataset\nfrom evaluate import evaluator\nimport evaluate\n\npipe = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\", device=0)\ndata = load_dataset(\"imdb\", split=\"test\").shuffle().select(range(1000))\nmetric = evaluate.load(\"accuracy\")\n```", "```py\n>>> task_evaluator = evaluator(\"text-classification\")\n\n>>> results = task_evaluator.compute(model_or_pipeline=pipe, data=data, metric=metric,\n...                        label_mapping={\"NEGATIVE\": 0, \"POSITIVE\": 1},)\n\n>>> print(results)\n{'accuracy': 0.934}\n```", "```py\n>>> results = eval.compute(model_or_pipeline=pipe, data=data, metric=metric,\n...                        label_mapping={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n...                        strategy=\"bootstrap\", n_resamples=200)\n\n>>> print(results)\n{'accuracy':\n    {\n      'confidence_interval': (0.906, 0.9406749892841922),\n      'standard_error': 0.00865213251082787,\n      'score': 0.923\n    }\n}\n```", "```py\nimport evaluate\nfrom evaluate.visualization import radar_plot\n\n>>> data = [\n   {\"accuracy\": 0.99, \"precision\": 0.8, \"f1\": 0.95, \"latency_in_seconds\": 33.6},\n   {\"accuracy\": 0.98, \"precision\": 0.87, \"f1\": 0.91, \"latency_in_seconds\": 11.2},\n   {\"accuracy\": 0.98, \"precision\": 0.78, \"f1\": 0.88, \"latency_in_seconds\": 87.6}, \n   {\"accuracy\": 0.88, \"precision\": 0.78, \"f1\": 0.81, \"latency_in_seconds\": 101.6}\n   ]\n>>> model_names = [\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\"]\n>>> plot = radar_plot(data=data, model_names=model_names)\n>>> plot.show()\n```", "```py\nimport evaluate\nfrom evaluate.evaluation_suite import SubTask\n\nclass Suite(evaluate.EvaluationSuite):\n\n    def __init__(self, name):\n        super().__init__(name)\n\n        self.suite = [\n            SubTask(\n                task_type=\"text-classification\",\n                data=\"imdb\",\n                split=\"test[:1]\",\n                args_for_task={\n                    \"metric\": \"accuracy\",\n                    \"input_column\": \"text\",\n                    \"label_column\": \"label\",\n                    \"label_mapping\": {\n                        \"LABEL_0\": 0.0,\n                        \"LABEL_1\": 1.0\n                    }\n                }\n            ),\n            SubTask(\n                task_type=\"text-classification\",\n                data=\"sst2\",\n                split=\"test[:1]\",\n                args_for_task={\n                    \"metric\": \"accuracy\",\n                    \"input_column\": \"sentence\",\n                    \"label_column\": \"label\",\n                    \"label_mapping\": {\n                        \"LABEL_0\": 0.0,\n                        \"LABEL_1\": 1.0\n                    }\n                }\n            )\n        ]\n```", "```py\n>>> from evaluate import EvaluationSuite\n>>> suite = EvaluationSuite.load('mathemakitten/sentiment-evaluation-suite')\n>>> results = suite.run(\"huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli\")\n```"]