["```py\nimport torch\nfrom transformers import BitsAndBytesConfig\n\nconfig = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n```", "```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", quantization_config=config)\n```", "```py\nfrom peft import prepare_model_for_kbit_training\n\nmodel = prepare_model_for_kbit_training(model)\n```", "```py\nfrom peft import LoraConfig\n\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=8,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n```", "```py\nfrom peft import get_peft_model\n\nmodel = get_peft_model(model, config)\n```", "```py\nfrom transformers import AutoModelForCausalLM\nfrom peft import LoftQConfig, LoraConfig, get_peft_model\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", device_map=\"auto\")\nloftq_config = LoftQConfig(loftq_bits=4)\n```", "```py\nlora_config = LoraConfig(\n    init_lora_weights=\"loftq\",\n    loftq_config=loftq_config,\n    r=16,\n    lora_alpha=8,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n```", "```py\nconfig = LoraConfig(target_modules=\"all-linear\", ...)\n```"]