["```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> checkpoint = \"Salesforce/codegen-350M-mono\"\n>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n>>> text = \"def hello_world():\"\n\n>>> completion = model.generate(**tokenizer(text, return_tensors=\"pt\"))\n\n>>> print(tokenizer.decode(completion[0]))\ndef hello_world():\n    print(\"Hello World\")\n\nhello_world()\n```", "```py\n>>> from transformers import CodeGenConfig, CodeGenModel\n\n>>> # Initializing a CodeGen 6B configuration\n>>> configuration = CodeGenConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = CodeGenModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import CodeGenTokenizer\n\n>>> tokenizer = CodeGenTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[15496, 995]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[18435, 995]\n```", "```py\n>>> from transformers import CodeGenTokenizerFast\n\n>>> tokenizer = CodeGenTokenizerFast.from_pretrained(\"Salesforce/codegen-350M-mono\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[15496, 995]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[18435, 995]\n```", "```py\n>>> from transformers import AutoTokenizer, CodeGenModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-2B-mono\")\n>>> model = CodeGenModel.from_pretrained(\"Salesforce/codegen-2B-mono\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, CodeGenForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-2B-mono\")\n>>> model = CodeGenForCausalLM.from_pretrained(\"Salesforce/codegen-2B-mono\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```"]