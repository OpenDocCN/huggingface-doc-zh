["```py\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> checkpoint = \"Salesforce/codegen-350M-mono\"\n>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n>>> text = \"def hello_world():\"\n\n>>> completion = model.generate(**tokenizer(text, return_tensors=\"pt\"))\n\n>>> print(tokenizer.decode(completion[0]))\ndef hello_world():\n    print(\"Hello World\")\n\nhello_world()\n```", "```py\n( vocab_size = 50400 n_positions = 2048 n_ctx = 2048 n_embd = 4096 n_layer = 28 n_head = 16 rotary_dim = 64 n_inner = None activation_function = 'gelu_new' resid_pdrop = 0.0 embd_pdrop = 0.0 attn_pdrop = 0.0 layer_norm_epsilon = 1e-05 initializer_range = 0.02 use_cache = True bos_token_id = 50256 eos_token_id = 50256 tie_word_embeddings = False **kwargs )\n```", "```py\n>>> from transformers import CodeGenConfig, CodeGenModel\n\n>>> # Initializing a CodeGen 6B configuration\n>>> configuration = CodeGenConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = CodeGenModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file merges_file errors = 'replace' unk_token = '<|endoftext|>' bos_token = '<|endoftext|>' eos_token = '<|endoftext|>' pad_token = None add_prefix_space = False add_bos_token = False **kwargs )\n```", "```py\n>>> from transformers import CodeGenTokenizer\n\n>>> tokenizer = CodeGenTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[15496, 995]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[18435, 995]\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( vocab_file = None merges_file = None tokenizer_file = None unk_token = '<|endoftext|>' bos_token = '<|endoftext|>' eos_token = '<|endoftext|>' add_prefix_space = False **kwargs )\n```", "```py\n>>> from transformers import CodeGenTokenizerFast\n\n>>> tokenizer = CodeGenTokenizerFast.from_pretrained(\"Salesforce/codegen-350M-mono\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[15496, 995]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[18435, 995]\n```", "```py\n( token_ids: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None truncate_before_pattern: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';str\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, CodeGenModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-2B-mono\")\n>>> model = CodeGenModel.from_pretrained(\"Salesforce/codegen-2B-mono\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, CodeGenForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-2B-mono\")\n>>> model = CodeGenForCausalLM.from_pretrained(\"Salesforce/codegen-2B-mono\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```"]