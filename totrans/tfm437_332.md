# ALIGN

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/align`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/align)

## 概述

ALIGN 模型在[通过嘈杂文本监督扩展视觉和视觉语言表示学习](https://arxiv.org/abs/2102.05918)中由 Chao Jia、Yinfei Yang、Ye Xia、Yi-Ting Chen、Zarana Parekh、Hieu Pham、Quoc V. Le、Yunhsuan Sung、Zhen Li、Tom Duerig 提出。ALIGN 是一个多模态视觉和语言模型。它可用于图像文本相似度和零样本图像分类。ALIGN 具有双编码器架构，其中 EfficientNet 作为其视觉编码器，BERT 作为其文本编码器，并通过对比学习学习对齐视觉和文本表示。与以往的工作不同，ALIGN 利用了一个庞大的嘈杂数据集，并表明语料库的规模可以用来实现具有简单配方的 SOTA 表示。

论文的摘要如下：

*预训练表示对许多 NLP 和感知任务变得至关重要。虽然 NLP 中的表示学习已经过渡到在没有人类注释的原始文本上进行训练，但视觉和视觉语言表示仍然严重依赖于昂贵或需要专业知识的策划训练数据集。对于视觉应用，表示主要是使用具有显式类标签的数据集学习，如 ImageNet 或 OpenImages。对于视觉语言，像 Conceptual Captions、MSCOCO 或 CLIP 这样的流行数据集都涉及到一个不容易的数据收集（和清理）过程。这种昂贵的策划过程限制了数据集的规模，从而阻碍了训练模型的扩展。在本文中，我们利用了一个超过十亿个图像替代文本对的嘈杂数据集，该数据集是在 Conceptual Captions 数据集中没有昂贵的过滤或后处理步骤的情况下获得的。一个简单的双编码器架构学习使用对比损失对图像和文本对的视觉和语言表示进行对齐。我们表明，我们语料库的规模可以弥补其噪声，并导致即使使用如此简单的学习方案也能实现最先进的表示。我们的视觉表示在转移到 ImageNet 和 VTAB 等分类任务时表现出色。对齐的视觉和语言表示使零样本图像分类成为可能，并在 Flickr30K 和 MSCOCO 图像文本检索基准上取得了新的最先进结果，即使与更复杂的交叉注意力模型相比也是如此。这些表示还使得可以进行具有复杂文本和文本+图像查询的跨模态搜索。*

这个模型是由[Alara Dirik](https://huggingface.co/adirik)贡献的。原始代码未发布，这个实现是基于 Kakao Brain 根据原始论文实现的。

## 用法示例

ALIGN 使用 EfficientNet 获取视觉特征，使用 BERT 获取文本特征。然后，文本和视觉特征都投影到具有相同维度的潜在空间中。然后使用投影图像和文本特征之间的点积作为相似度分数。

AlignProcessor 将 EfficientNetImageProcessor 和 BertTokenizer 封装成一个单一实例，用于对文本进行编码和预处理图像。以下示例展示了如何使用 AlignProcessor 和 AlignModel 获取图像文本相似度分数。

```py
import requests
import torch
from PIL import Image
from transformers import AlignProcessor, AlignModel

processor = AlignProcessor.from_pretrained("kakaobrain/align-base")
model = AlignModel.from_pretrained("kakaobrain/align-base")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
candidate_labels = ["an image of a cat", "an image of a dog"]

inputs = processor(text=candidate_labels, images=image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

# this is the image-text similarity score
logits_per_image = outputs.logits_per_image

# we can take the softmax to get the label probabilities
probs = logits_per_image.softmax(dim=1)
print(probs)
```

## 资源

一系列官方 Hugging Face 和社区（由🌎表示）资源，可帮助您开始使用 ALIGN。

+   有关 [ALIGN 和 COYO-700M 数据集](https://huggingface.co/blog/vit-align) 的博客文章。

+   一个零样本图像分类 [演示](https://huggingface.co/spaces/adirik/ALIGN-zero-shot-image-classification)。

+   `kakaobrain/align-base` 模型的 [模型卡片](https://huggingface.co/kakaobrain/align-base)。

如果您有兴趣提交资源以包含在此处，请随时打开一个拉取请求，我们将进行审查。资源应该展示一些新内容，而不是重复现有资源。

## AlignConfig

### `class transformers.AlignConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/configuration_align.py#L298)

```py
( text_config = None vision_config = None projection_dim = 640 temperature_init_value = 1.0 initializer_range = 0.02 **kwargs )
```

参数

+   `text_config` (`dict`, *optional*) — 用于初始化 AlignTextConfig 的配置选项字典。

+   `vision_config` (`dict`, *optional*) — 用于初始化 AlignVisionConfig 的配置选项字典。

+   `projection_dim` (`int`, *optional*, 默认为 640) — 文本和视觉投影层的维度。

+   `temperature_init_value` (`float`, *optional*, 默认为 1.0) — *temperature* 参数的初始值。默认值与原始 ALIGN 实现相同。

+   `initializer_range` (`float`, *optional*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `kwargs` (*optional*) — 关键字参数字典。

AlignConfig 是用于存储 AlignModel 配置的配置类。它用于根据指定的参数实例化一个 ALIGN 模型，定义文本模型和视觉模型配置。使用默认值实例化配置将产生与 ALIGN [kakaobrain/align-base](https://huggingface.co/kakaobrain/align-base) 架构类似的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读来自 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import AlignConfig, AlignModel

>>> # Initializing a AlignConfig with kakaobrain/align-base style configuration
>>> configuration = AlignConfig()

>>> # Initializing a AlignModel (with random weights) from the kakaobrain/align-base style configuration
>>> model = AlignModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config

>>> # We can also initialize a AlignConfig from a AlignTextConfig and a AlignVisionConfig
>>> from transformers import AlignTextConfig, AlignVisionConfig

>>> # Initializing ALIGN Text and Vision configurations
>>> config_text = AlignTextConfig()
>>> config_vision = AlignVisionConfig()

>>> config = AlignConfig.from_text_vision_configs(config_text, config_vision)
```

#### `from_text_vision_configs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/configuration_align.py#L374)

```py
( text_config: AlignTextConfig vision_config: AlignVisionConfig **kwargs ) → export const metadata = 'undefined';AlignConfig
```

返回

AlignConfig

配置对象的一个实例

从对齐文本模型配置和对齐视觉模型配置实例化一个 AlignConfig（或派生类）。

## AlignTextConfig

### `class transformers.AlignTextConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/configuration_align.py#L35)

```py
( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 position_embedding_type = 'absolute' use_cache = True **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, 默认为 30522) — Align 文本模型的词汇量。定义了在调用 AlignTextModel 时可以表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, 默认为 768) — 编码器层和池化层的维度。

+   `num_hidden_layers` (`int`, *optional*, 默认为 12) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, 默认为 12) — Transformer 编码器中每个注意力层的注意力头数。

+   `intermediate_size` (`int`, *optional*, 默认为 3072) — Transformer 编码器中“中间”（通常称为前馈）层的维度。

+   `hidden_act` (`str`或`Callable`, *optional*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *optional*, 默认为 0.1) — 嵌入层、编码器和池化器中所有全连接层的 dropout 概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, 默认为 0.1) — 注意力概率的 dropout 比率。

+   `max_position_embeddings` (`int`, *optional*, 默认为 512) — 此模型可能使用的最大序列长度。通常设置为较大的值以防万一（例如 512、1024 或 2048）。

+   `type_vocab_size` (`int`, *optional*, 默认为 2) — 在调用 AlignTextModel 时传递的`token_type_ids`的词汇表大小。

+   `initializer_range` (`float`, *optional*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, 默认为 1e-12) — 层归一化层使用的 epsilon。

+   `pad_token_id` (`int`, *optional*, 默认为 0) — 填充标记 id。

+   `position_embedding_type` (`str`, *optional*, 默认为`"absolute"`) — 位置嵌入的类型。选择`"absolute"`、`"relative_key"`、`"relative_key_query"`中的一个。对于位置嵌入，请使用`"absolute"`。有关`"relative_key"`的更多信息，请参考[Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)。有关`"relative_key_query"`的更多信息，请参考[Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)]中的*Method 4* (https://arxiv.org/abs/2009.13658)。

+   `use_cache` (`bool`, *optional*, 默认为`True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。仅在`config.is_decoder=True`时相关。

这是用于存储 AlignTextModel 配置的配置类。根据指定的参数实例化一个 ALIGN 文本编码器，定义模型架构。使用默认值实例化配置将产生与 ALIGN [kakaobrain/align-base](https://huggingface.co/kakaobrain/align-base)架构的文本编码器类似的配置。这里的默认值是从 BERT 复制的。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import AlignTextConfig, AlignTextModel

>>> # Initializing a AlignTextConfig with kakaobrain/align-base style configuration
>>> configuration = AlignTextConfig()

>>> # Initializing a AlignTextModel (with random weights) from the kakaobrain/align-base style configuration
>>> model = AlignTextModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## AlignVisionConfig

### `class transformers.AlignVisionConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/configuration_align.py#L159)

```py
( num_channels: int = 3 image_size: int = 600 width_coefficient: float = 2.0 depth_coefficient: float = 3.1 depth_divisor: int = 8 kernel_sizes: List = [3, 3, 5, 3, 5, 5, 3] in_channels: List = [32, 16, 24, 40, 80, 112, 192] out_channels: List = [16, 24, 40, 80, 112, 192, 320] depthwise_padding: List = [] strides: List = [1, 2, 2, 2, 1, 2, 1] num_block_repeats: List = [1, 2, 2, 3, 3, 4, 1] expand_ratios: List = [1, 6, 6, 6, 6, 6, 6] squeeze_expansion_ratio: float = 0.25 hidden_act: str = 'swish' hidden_dim: int = 2560 pooling_type: str = 'mean' initializer_range: float = 0.02 batch_norm_eps: float = 0.001 batch_norm_momentum: float = 0.99 drop_connect_rate: float = 0.2 **kwargs )
```

参数

+   `num_channels` (`int`, *optional*, 默认为 3) — 输入通道数。

+   `image_size` (`int`, *optional*, 默认为 600) — 输入图像大小。

+   `width_coefficient` (`float`, *optional*, 默认为 2.0) — 每个阶段网络宽度的缩放系数。

+   `depth_coefficient` (`float`, *optional*, 默认为 3.1) — 每个阶段网络深度的缩放系数。

+   `depth_divisor` `int`, *optional*, 默认为 8) — 网络宽度的单位。

+   `kernel_sizes` (`List[int]`, *optional*, 默认为`[3, 3, 5, 3, 5, 5, 3]`) — 每个块中要使用的卷积核大小列表。

+   `in_channels` (`List[int]`, *optional*, 默认为`[32, 16, 24, 40, 80, 112, 192]`) — 用于卷积层中每个块的输入通道大小列表。

+   `out_channels` (`List[int]`, *可选*, 默认为`[16, 24, 40, 80, 112, 192, 320]`) — 用于卷积层中每个块中使用的输出通道大小列表。

+   `depthwise_padding` (`List[int]`, *可选*, 默认为`[]`) — 具有方形填充的块索引列表。

+   `strides` (`List[int]`, *可选*, 默认为`[1, 2, 2, 2, 1, 2, 1]`) — 用于卷积层中每个块中使用的步幅大小列表。

+   `num_block_repeats` (`List[int]`, *可选*, 默认为`[1, 2, 2, 3, 3, 4, 1]`) — 每个块重复的次数列表。

+   `expand_ratios` (`List[int]`, *可选*, 默认为`[1, 6, 6, 6, 6, 6, 6]`) — 每个块的缩放系数列表。

+   `squeeze_expansion_ratio` (`float`, *可选*, 默认为 0.25) — 挤压扩展比率。

+   `hidden_act` (`str`或`function`, *可选*, 默认为`"silu"`) — 每个块中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`, `"relu"`, `"selu"`, "gelu_new", "silu"和"mish"。

+   `hiddem_dim` (`int`, *可选*, 默认为 1280) — 分类头之前层的隐藏维度。

+   `pooling_type` (`str`或`function`, *可选*, 默认为`"mean"`) — 应用于密集分类头之前的最终池化类型。可用选项为[`"mean"`, `"max"`]

+   `initializer_range` (`float`, *可选*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `batch_norm_eps` (`float`, *可选*, 默认为 1e-3) — 批量归一化层使用的 epsilon。

+   `batch_norm_momentum` (`float`, *可选*, 默认为 0.99) — 批量归一化层使用的动量。

+   `drop_connect_rate` (`float`, *可选*, 默认为 0.2) — 跳过连接的丢弃率。

这是一个配置类，用于存储 AlignVisionModel 的配置。根据指定的参数实例化一个 ALIGN 视觉编码器，定义模型架构。使用默认值实例化配置将产生与 ALIGN [kakaobrain/align-base](https://huggingface.co/kakaobrain/align-base)架构的视觉编码器类似的配置。默认值来自 EfficientNet (efficientnet-b7)

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读来自 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import AlignVisionConfig, AlignVisionModel

>>> # Initializing a AlignVisionConfig with kakaobrain/align-base style configuration
>>> configuration = AlignVisionConfig()

>>> # Initializing a AlignVisionModel (with random weights) from the kakaobrain/align-base style configuration
>>> model = AlignVisionModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## AlignProcessor

### `class transformers.AlignProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/processing_align.py#L24)

```py
( image_processor tokenizer )
```

参数

+   `image_processor` (EfficientNetImageProcessor) — 图像处理器是必需的输入。

+   `tokenizer` ([`BertTokenizer`, `BertTokenizerFast`]) — 分词器是必需的输入。

构建一个 ALIGN 处理器，将 EfficientNetImageProcessor 和 BertTokenizer/BertTokenizerFast 包装成一个同时继承图像处理器和分词器功能的单个处理器。查看`__call__()`和 decode()以获取更多信息。

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/processing_align.py#L104)

```py
( *args **kwargs )
```

此方法将所有参数转发给 BertTokenizerFast 的 batch_decode()。请参考此方法的文档字符串获取更多信息。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/processing_align.py#L111)

```py
( *args **kwargs )
```

此方法将所有参数转发给 BertTokenizerFast 的 decode()。请参考此方法的文档字符串获取更多信息。

## AlignModel

### `class transformers.AlignModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/modeling_align.py#L1411)

```py
( config: AlignConfig )
```

参数

+   `config` (AlignConfig) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

这个模型继承自 PreTrainedModel。查看超类文档以获取库实现的所有模型的通用方法（例如下载或保存、调整输入嵌入大小、修剪头等）。

这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/modeling_align.py#L1543)

```py
( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    索引可以使用 AutoTokenizer 获得。查看 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()获取详细信息。

    什么是输入 ID？ attention_mask (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*): 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1 对应于`未被掩码`的标记，

    +   0 对应于`被掩码`的标记。

    什么是注意力掩码？ position_ids (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*): 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    什么是位置 ID？ token_type_ids (`torch.LongTensor`，形状为`({0})`，*可选*): 段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`之间：

    +   0 对应于*句子 A*标记，

    +   1 对应于*句子 B*标记。

    什么是标记类型 ID？ head_mask (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*): 用于使自注意力模块中选择的头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1 表示头部`未被掩码`，

    +   0 表示头部被`掩码`。

    inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*): 可选地，您可以选择直接传递嵌入表示而不是传递 `input_ids`。如果您想要更多控制权来将 `input_ids` 索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`): 像素值。默认情况下将忽略填充。可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 EfficientNetImageProcessor.`call`()。return_loss (`bool`, *optional*): 是否返回对比损失。output_attentions (`bool`, *optional*): 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的 `attentions`。output_hidden_states (`bool`, *optional*): 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的 `hidden_states`。return_dict (`bool`, *optional*): 是否返回 ModelOutput 而不是普通元组。

AlignModel 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

#### `get_text_features`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/modeling_align.py#L1445)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用 AutoTokenizer 获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode() 和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示未被屏蔽的标记，

    +   0 表示被屏蔽的标记。

    什么是注意力掩码？

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]` 中选择。

    什么是位置 ID？

+   `token_type_ids` (`torch.LongTensor` of shape `({0})`, *optional*) — 段标记索引，指示输入的第一部分和第二部分。索引在 `[0, 1]` 中选择：

    +   0 对应于 *句子 A* 标记，

    +   1 对应于 *句子 B* 标记。

    什么是标记类型 ID？

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示头部未被屏蔽，

    +   0 表示头部被屏蔽。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`({0}, hidden_size)`，*可选*) — 可选地，您可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。

返回

text_features (`torch.FloatTensor`，形状为`(batch_size, output_dim`)

通过将投影层应用于 AlignTextModel 的汇聚输出获得的文本嵌入。

AlignModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, AlignModel

>>> model = AlignModel.from_pretrained("kakaobrain/align-base")
>>> tokenizer = AutoTokenizer.from_pretrained("kakaobrain/align-base")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
>>> text_features = model.get_text_features(**inputs)
```

#### `get_image_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/modeling_align.py#L1498)

```py
( pixel_values: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。如果提供，可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 EfficientNetImageProcessor.`call`()。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 ModelOutput 而不是普通元组。

返回

image_features (`torch.FloatTensor`，形状为`(batch_size, output_dim)`)

通过将投影层应用于 AlignVisionModel 的汇聚输出获得的图像嵌入。

AlignModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, AlignModel

>>> model = AlignModel.from_pretrained("kakaobrain/align-base")
>>> processor = AutoProcessor.from_pretrained("kakaobrain/align-base")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> image_features = model.get_image_features(**inputs)
```

## AlignTextModel

### `class transformers.AlignTextModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/modeling_align.py#L1196)

```py
( config: AlignTextConfig add_pooling_layer: bool = True )
```

参数

+   `config` (AlignConfig) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

来自 ALIGN 的文本模型，没有顶部的头部或投影。此模型继承自 PreTrainedModel。检查超类文档以获取库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/modeling_align.py#L1221)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下，如果提供，将忽略填充。

    可以使用 AutoTokenizer 来获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   1 表示未被掩盖的标记，

    +   0 表示被掩盖的标记。

    什么是注意力掩码？

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置 ID？

+   `token_type_ids` (`torch.LongTensor`，形状为`({0})`，*可选*) — 指示输入的第一部分和第二部分的段标记索引。索引选择在`[0, 1]`中：

    +   0 对应于*句子 A*标记，

    +   1 对应于*句子 B*标记。

    什么是标记类型 ID？

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`中：

    +   1 表示头部未被掩盖，

    +   0 表示头部被掩盖。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`({0}, hidden_size)`，*可选*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回 ModelOutput 而不是普通元组。

返回

transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions 或`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions 或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（`<class 'transformers.models.align.configuration_align.AlignTextConfig'>`）和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）- 模型最后一层的隐藏状态序列。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）- 经过用于辅助预训练任务的层进一步处理后，序列的第一个标记（分类标记）的最后一层隐藏状态。例如，对于 BERT 系列模型，这将返回经过线性层和 tanh 激活函数处理后的分类标记。线性层的权重是从预训练期间的下一个句子预测（分类）目标中训练的。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型在每一层输出处的隐藏状态加上可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力 softmax 之后的注意权重，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`和`config.add_cross_attention=True`或当`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的交叉注意力层的注意权重，在注意力 softmax 之后，用于计算交叉注意力头中的加权平均值。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或当`config.use_cache=True`时返回）- 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`还有 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块中的键和值以及可选地在交叉注意力块中使用`config.is_encoder_decoder=True`）可用（参见`past_key_values`输入）以加速顺序解码。

AlignTextModel 的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, AlignTextModel

>>> model = AlignTextModel.from_pretrained("kakaobrain/align-base")
>>> tokenizer = AutoTokenizer.from_pretrained("kakaobrain/align-base")

>>> inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states
```

## AlignVisionModel

### `class transformers.AlignVisionModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/modeling_align.py#L1322)

```py
( config: AlignVisionConfig )
```

参数

+   `config`（AlignConfig）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

ALIGN 中的视觉模型，没有任何头部或顶部的投影。此模型继承自 PreTrainedModel。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/align/modeling_align.py#L1351)

```py
( pixel_values: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`）— 像素值。默认情况下将忽略填充，如果您提供填充。可以使用 AutoImageProcessor 获取像素值。有关详细信息，请参阅 EfficientNetImageProcessor.`call`()。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回 ModelOutput 而不是普通元组。

返回

`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`或`tuple(torch.FloatTensor)`

一个`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（`<class 'transformers.models.align.configuration_align.AlignVisionConfig'>`）和输入。

+   `last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`）— 模型最后一层的隐藏状态序列。

+   `pooler_output`（`torch.FloatTensor`，形状为`(batch_size, hidden_size)`）— 在空间维度上进行池化操作后的最后一层隐藏状态。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（如果模型具有嵌入层，则为嵌入的输出的一个+每一层的输出的一个）。

    模型在每一层输出处的隐藏状态以及可选的初始嵌入输出。

AlignVisionModel 的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, AlignVisionModel

>>> model = AlignVisionModel.from_pretrained("kakaobrain/align-base")
>>> processor = AutoProcessor.from_pretrained("kakaobrain/align-base")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled CLS states
```
