- en: Post-processors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/tokenizers/api/post-processors](https://huggingface.co/docs/tokenizers/api/post-processors)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link rel="modulepreload" href="/docs/tokenizers/v0.13.4.rc2/en/_app/assets/pages/__layout.svelte-hf-doc-builder.css">
    <link rel="modulepreload" href="/docs/tokenizers/v0.13.4.rc2/en/_app/start-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/tokenizers/v0.13.4.rc2/en/_app/chunks/vendor-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/tokenizers/v0.13.4.rc2/en/_app/chunks/paths-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/tokenizers/v0.13.4.rc2/en/_app/pages/__layout.svelte-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/tokenizers/v0.13.4.rc2/en/_app/pages/api/post-processors.mdx-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/tokenizers/v0.13.4.rc2/en/_app/chunks/Docstring-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/tokenizers/v0.13.4.rc2/en/_app/chunks/IconCopyLink-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/tokenizers/v0.13.4.rc2/en/_app/chunks/CodeBlock-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/tokenizers/v0.13.4.rc2/en/_app/chunks/TokenizersLanguageContent-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/tokenizers/v0.13.4.rc2/en/_app/chunks/ExampleCodeBlock-hf-doc-builder.js">PythonRustNode
  prefs: []
  type: TYPE_NORMAL
- en: BertProcessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.processors.BertProcessing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sep` (`Tuple[str, int]`) — A tuple with the string representation of the SEP
    token, and its id'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls` (`Tuple[str, int]`) — A tuple with the string representation of the CLS
    token, and its id'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This post-processor takes care of adding the special tokens needed by a Bert
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: a SEP token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a CLS token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ByteLevel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.processors.ByteLevel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`trim_offsets` (`bool`) — Whether to trim the whitespaces from the produced
    offsets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This post-processor takes care of trimming the offsets.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the ByteLevel BPE might include whitespaces in the produced tokens.
    If you don’t want the offsets to include these whitespaces, then this PostProcessor
    must be used.
  prefs: []
  type: TYPE_NORMAL
- en: RobertaProcessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.processors.RobertaProcessing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sep` (`Tuple[str, int]`) — A tuple with the string representation of the SEP
    token, and its id'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls` (`Tuple[str, int]`) — A tuple with the string representation of the CLS
    token, and its id'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trim_offsets` (`bool`, *optional*, defaults to `True`) — Whether to trim the
    whitespaces from the produced offsets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `True`) — Whether the add_prefix_space
    option was enabled during pre-tokenization. This is relevant because it defines
    the way the offsets are trimmed out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This post-processor takes care of adding the special tokens needed by a Roberta
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: a SEP token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a CLS token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It also takes care of trimming the offsets. By default, the ByteLevel BPE might
    include whitespaces in the produced tokens. If you don’t want the offsets to include
    these whitespaces, then this PostProcessor should be initialized with `trim_offsets=True`
  prefs: []
  type: TYPE_NORMAL
- en: TemplateProcessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.processors.TemplateProcessing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`single` (`Template`) — The template used for single sequences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pair` (`Template`) — The template used when both sequences are specified'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`special_tokens` (`Tokens`) — The list of special tokens used in each sequences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides a way to specify templates in order to add the special tokens to each
    input sequence as relevant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take `BERT` tokenizer as an example. It uses two special tokens, used
    to delimitate each sequence. `[CLS]` is always used at the beginning of the first
    sequence, and `[SEP]` is added at the end of both the first, and the pair sequences.
    The final result looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Single sequence: `[CLS] Hello there [SEP]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pair sequences: `[CLS] My name is Anthony [SEP] What is my name? [SEP]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With the type ids as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can achieve such behavior using a TemplateProcessing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, each input sequence is identified using a `$` construct. This
    identifier lets us specify each input sequence, and the type_id to use. When nothing
    is specified, it uses the default values. Here are the different ways to specify
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifying the sequence, with default `type_id == 0`: `$A` or `$B`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Specifying the *type_id* with default `sequence == A`: `$0`, `$1`, `$2`, …'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Specifying both: `$A:0`, `$B:1`, …'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The same construct is used for special tokens: `<identifier>(:<type_id>)?`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Warning**: You must ensure that you are giving the correct tokens/ids as
    these will be added to the Encoding without any further check. If the given ids
    correspond to something totally different in a *Tokenizer* using this *PostProcessor*,
    it might lead to unexpected results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Types:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Template (`str` or `List`):'
  prefs: []
  type: TYPE_NORMAL
- en: If a `str` is provided, the whitespace is used as delimiter between tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a `List[str]` is provided, a list of tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tokens (`List[Union[Tuple[int, str], Tuple[str, int], dict]]`):'
  prefs: []
  type: TYPE_NORMAL
- en: A `Tuple` with both a token and its associated ID, in any order
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `dict` with the following keys:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“id”: `str` => The special token id, as specified in the Template'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '“ids”: `List[int]` => The associated IDs'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '“tokens”: `List[str]` => The associated tokens'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The given dict expects the provided `ids` and `tokens` lists to have the same
    length.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
