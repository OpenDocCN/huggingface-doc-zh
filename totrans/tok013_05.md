# 标记化管道

> 原始文本：[https://huggingface.co/docs/tokenizers/pipeline](https://huggingface.co/docs/tokenizers/pipeline)

在调用`Tokenizer.encode`或`Tokenizer.encode_batch`时，输入文本将通过以下管道进行处理：

+   `归一化`

+   `预分词`

+   `模型`

+   `后处理`

我们将详细了解每个步骤的详细情况，以及当您想要`解码<解码>`一些标记ID时，以及🤗 Tokenizers库如何允许您根据需要自定义每个步骤。如果您已经熟悉这些步骤，并希望通过查看一些代码来学习，请跳转到`我们的BERT从头开始示例<示例>`。

对于需要`Tokenizer`的示例，我们将使用在`快速浏览`中训练的分词器，您可以通过以下方式加载：

PythonRustNode

```py
from tokenizers import Tokenizer
tokenizer = Tokenizer.from_file("data/tokenizer-wiki.json")
```

## 归一化

归一化，简而言之，是对原始字符串应用的一组操作，使其更少随机或“更干净”。常见操作包括去除空格、去除重音字符或将所有文本转换为小写。如果您熟悉[Unicode规范化](https://unicode.org/reports/tr15)，那么大多数分词器中也会应用非常常见的规范化操作。

每个规范化操作在🤗 Tokenizers库中由一个`Normalizer`表示，您可以通过使用`normalizers.Sequence`组合其中的几个。这是一个应用NFD Unicode规范化并去除重音的规范化器示例：

PythonRustNode

```py
from tokenizers import normalizers
from tokenizers.normalizers import NFD, StripAccents
normalizer = normalizers.Sequence([NFD(), StripAccents()])
```

您可以通过将其应用于任何字符串来手动测试该规范化器：

PythonRustNode

```py
normalizer.normalize_str("Héllò hôw are ü?")
# "Hello how are u?"
```

构建`Tokenizer`时，您可以通过更改相应属性来自定义其规范化：

PythonRustNode

```py
tokenizer.normalizer = normalizer
```

当然，如果更改了分词器应用归一化的方式，那么之后可能需要从头开始重新训练它。

## 预分词

预分词化是将文本分割为较小对象的行为，这些对象给出了在训练结束时您的标记将是什么的上限。一个好的思考方式是，预分词器将把您的文本分割成“单词”，然后，您的最终标记将是这些单词的一部分。

一个简单的预分词输入的方法是在空格和标点符号上进行分割，这是由`pre_tokenizers.Whitespace`预分词器完成的：

PythonRustNode

```py
from tokenizers.pre_tokenizers import Whitespace
pre_tokenizer = Whitespace()
pre_tokenizer.pre_tokenize_str("Hello! How are you? I'm fine, thank you.")
# [("Hello", (0, 5)), ("!", (5, 6)), ("How", (7, 10)), ("are", (11, 14)), ("you", (15, 18)),
#  ("?", (18, 19)), ("I", (20, 21)), ("'", (21, 22)), ('m', (22, 23)), ("fine", (24, 28)),
#  (",", (28, 29)), ("thank", (30, 35)), ("you", (36, 39)), (".", (39, 40))]
```

输出是一个元组列表，每个元组包含一个单词及其在原始句子中的跨度（用于确定我们的`Encoding`的最终`offsets`）。请注意，根据标点符号分割将分割像`"I'm"`这个例子中的缩写。

您可以将任何`PreTokenizer`组合在一起。例如，这是一个预分词器，将在空格、标点符号和数字上进行分割，将数字分隔为各自的数字：

PythonRustNode

```py
from tokenizers import pre_tokenizers
from tokenizers.pre_tokenizers import Digits
pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])
pre_tokenizer.pre_tokenize_str("Call 911!")
# [("Call", (0, 4)), ("9", (5, 6)), ("1", (6, 7)), ("1", (7, 8)), ("!", (8, 9))]
```

正如我们在`快速浏览`中看到的，您可以通过更改相应属性来自定义`Tokenizer`的预分词器：

PythonRustNode

```py
tokenizer.pre_tokenizer = pre_tokenizer
```

当然，如果更改了预分词器的方式，那么之后可能需要从头开始重新训练您的分词器。

## 模型

一旦输入文本被归一化和预分词，`Tokenizer`将在预分词上应用模型。这是管道中需要在您的语料库上进行训练的部分（或者如果您使用预训练的分词器，则已经训练过）。

模型的作用是根据其学习的规则将您的“单词”分割为标记。它还负责将这些标记映射到模型词汇表中的相应ID。

这个模型在初始化`Tokenizer`时传递，所以您已经知道如何自定义这部分。目前，🤗 Tokenizers库支持：

+   `models.BPE`

+   `models.Unigram`

+   `models.WordLevel`

+   `models.WordPiece`

有关每个模型及其行为的更多详细信息，您可以在[此处](components#models)查看

## 后处理

后处理是分词管道的最后一步，用于在返回之前对`Encoding`执行任何额外的转换，例如添加潜在的特殊标记。

正如我们在快速浏览中看到的，我们可以通过设置相应的属性来自定义`Tokenizer`的后处理器。例如，这是我们如何进行后处理以使输入适合BERT模型：

PythonRustNode

```py
from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[("[CLS]", 1), ("[SEP]", 2)],
)
```

请注意，与预分词器或规范化器相反，更改后处理器后不需要重新训练分词器。

## 所有在一起：从头开始的BERT分词器

让我们把所有这些部分放在一起来构建一个BERT分词器。首先，BERT依赖于WordPiece，因此我们用这个模型实例化一个新的`Tokenizer`：

PythonRustNode

```py
from tokenizers import Tokenizer
from tokenizers.models import WordPiece
bert_tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))
```

然后我们知道BERT通过去除重音符号和小写来预处理文本。我们还使用了一个Unicode规范化器：

PythonRustNode

```py
from tokenizers import normalizers
from tokenizers.normalizers import NFD, Lowercase, StripAccents
bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])
```

预分词器只是在空格和标点符号上进行分割：

PythonRustNode

```py
from tokenizers.pre_tokenizers import Whitespace
bert_tokenizer.pre_tokenizer = Whitespace()
```

后处理使用了我们在上一节中看到的模板：

PythonRustNode

```py
from tokenizers.processors import TemplateProcessing
bert_tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", 1),
        ("[SEP]", 2),
    ],
)
```

我们可以使用这个分词器，并在wikitext上进行训练，就像在`quicktour`中一样：

PythonRustNode

```py
from tokenizers.trainers import WordPieceTrainer
trainer = WordPieceTrainer(vocab_size=30522, special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
files = [f"data/wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]
bert_tokenizer.train(files, trainer)
bert_tokenizer.save("data/bert-wiki.json")
```

## 解码

除了对输入文本进行编码外，`Tokenizer`还具有用于解码的API，即将模型生成的ID转换回文本。这是通过方法`Tokenizer.decode`（用于一个预测文本）和`Tokenizer.decode_batch`（用于一批预测）来完成的。

`decoder`首先将ID转换回标记（使用分词器的词汇表），然后删除所有特殊标记，然后用空格连接这些标记：

PythonRustNode

```py
output = tokenizer.encode("Hello, y'all! How are you 😁 ?")
print(output.ids)
# [1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2]
tokenizer.decode([1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2])
# "Hello , y ' all ! How are you ?"
```

如果您使用了一个模型，该模型添加了特殊字符来表示给定“单词”的子标记（例如WordPiece中的`"##"`），则需要自定义`decoder`以正确处理它们。例如，如果我们以前的`bert_tokenizer`，默认解码将会给出：

PythonRustNode

```py
output = bert_tokenizer.encode("Welcome to the 🤗 Tokenizers library.")
print(output.tokens)
# ["[CLS]", "welcome", "to", "the", "[UNK]", "tok", "##eni", "##zer", "##s", "library", ".", "[SEP]"]
bert_tokenizer.decode(output.ids)
# "welcome to the tok ##eni ##zer ##s library ."
```

但通过将其更改为适当的解码器，我们得到：

PythonRustNode

```py
from tokenizers import decoders
bert_tokenizer.decoder = decoders.WordPiece()
bert_tokenizer.decode(output.ids)
# "welcome to the tokenizers library."
```
