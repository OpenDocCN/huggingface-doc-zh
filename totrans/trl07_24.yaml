- en: Detoxifying a Language Model using PPO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/trl/detoxifying_a_lm](https://huggingface.co/docs/trl/detoxifying_a_lm)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/trl/v0.7.10/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/entry/start.d9a24ea1.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/scheduler.9039eef2.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/singletons.9eef12cc.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/paths.1355483e.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/entry/app.5bef33b8.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/index.ded8f90d.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/nodes/0.abccdcd8.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/nodes/5.5d9bc626.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/CodeBlock.8580f3e8.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/Heading.f027f30d.js">
  prefs: []
  type: TYPE_NORMAL
- en: Language models (LMs) are known to sometimes generate toxic outputs. In this
    example, we will show how to “detoxify” a LM by feeding it toxic prompts and then
    using [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index)
    and Proximal Policy Optimization (PPO) to “detoxify” it.
  prefs: []
  type: TYPE_NORMAL
- en: Read this section to follow our investigation on how we can reduce toxicity
    in a wide range of LMs, from 125m parameters to 6B parameters!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an overview of the notebooks and scripts in the [TRL toxicity repository](https://github.com/huggingface/trl/tree/main/examples/toxicity/scripts)
    as well as the link for the interactive demo:'
  prefs: []
  type: TYPE_NORMAL
- en: '| File | Description | Colab link |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [`gpt-j-6b-toxicity.py`](https://github.com/huggingface/trl/blob/main/examples/research_projects/toxicity/scripts/gpt-j-6b-toxicity.py)
    | Detoxify `GPT-J-6B` using PPO | x |'
  prefs: []
  type: TYPE_TB
- en: '| [`evaluate-toxicity.py`](https://github.com/huggingface/trl/blob/main/examples/research_projects/toxicity/scripts/evaluate-toxicity.py)
    | Evaluate de-toxified models using `evaluate` | x |'
  prefs: []
  type: TYPE_TB
- en: '| [Interactive Space](https://huggingface.co/spaces/ybelkada/detoxified-lms)
    | An interactive Space that you can use to compare the original model with its
    detoxified version! | x |'
  prefs: []
  type: TYPE_TB
- en: Context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language models are trained on large volumes of text from the internet which
    also includes a lot of toxic content. Naturally, language models pick up the toxic
    patterns during training. Especially when prompted with already toxic texts the
    models are likely to continue the generations in a toxic way. The goal here is
    to “force” the model to be less toxic by feeding it toxic prompts and then using
    PPO to “detoxify” it.
  prefs: []
  type: TYPE_NORMAL
- en: Computing toxicity scores
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to optimize a model with PPO we need to define a reward. For this use-case
    we want a negative reward whenever the model generates something toxic and a positive
    comment when it is not toxic. Therefore, we used [`facebook/roberta-hate-speech-dynabench-r4-target`](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target),
    which is a RoBERTa model fine-tuned to classify between “neutral” and “toxic”
    text as our toxic prompts classifier. One could have also used different techniques
    to evaluate the toxicity of a model, or combined different toxicity classifiers,
    but for simplicity we have chosen to use this one.
  prefs: []
  type: TYPE_NORMAL
- en: Selection of models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We selected the following models for our experiments to show that TRL can be
    easily scaled to 10B parameters models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[`EleutherAI/gpt-neo-125M`](https://huggingface.co/EleutherAI/gpt-neo-125M)
    (125 million parameters)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`EleutherAI/gpt-neo-2.7B`](https://huggingface.co/EleutherAI/gpt-neo-2.7B)
    (2.7 billion parameters)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`EleutherAI/gpt-j-6B`](https://huggingface.co/EleutherAI/gpt-j-6B) (6 billion
    parameters)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the selection of the smallest model, we have chosen `EleutherAI/gpt-neo-125M`
    because it has shown to be a model that was the “most toxic” compared to other
    models. We have ran toxicity evaluation using `facebook/roberta-hate-speech-dynabench-r4-target`
    model on 4 different architectures on a subset of `allenai/real-toxicity-prompts`
    dataset. Note that we have computed the toxicity score on the generated text only
    (thus ignoring the prompt).
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Mean toxicity score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `gpt2` | 0.01602 |'
  prefs: []
  type: TYPE_TB
- en: '| `facebook/opt-350m` | 0.01628 |'
  prefs: []
  type: TYPE_TB
- en: '| `bigscience/bloom-560m` | 0.00767 |'
  prefs: []
  type: TYPE_TB
- en: '| `EleutherAI/gpt-neo-125M` | **0.02016** |'
  prefs: []
  type: TYPE_TB
- en: Designing the problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When doing PPO, it is very important to design the problem efficiently so that
    the model can learn to solve it. Let’s cover the topics that were important for
    the model to converge.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-processing the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dataset consist of prompts and their continuations, and each of them has
    an associated `toxicity` score.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `prompt` example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And its `continuation` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to increase the chance for the model to generate toxic prompts so we
    get more learning signal. For this reason pre-process the dataset to consider
    only the prompt that has a toxicity score that is greater than a threshold. We
    can do this in a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Reward function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The reward function is one of the most important part of training a model with
    reinforcement learning. It is the function that will tell the model if it is doing
    well or not. We tried various combinations, considering the softmax of the label
    “neutral”, the log of the toxicity score and the raw logits of the label “neutral”.
    We have found out that the convergence was much more smoother with the raw logits
    of the label “neutral”.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Impact of input prompts length
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have found out that training a model with small or long context (from 5 to
    8 tokens for the small context and from 15 to 20 tokens for the long context)
    does not have any impact on the convergence of the model, however, when training
    the model with longer prompts, the model will tend to generate more toxic prompts.
    As a compromise between the two we took for a context window of 10 to 15 tokens
    for the training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fec82719b3655a9458052e3112d36c51.png)'
  prefs: []
  type: TYPE_IMG
- en: How to deal with OOM issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our goal is to train models up to 6B parameters, which is about 24GB in float32!
    Here two tricks we use to be able to train a 6B model on a single 40GB-RAM GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `bfloat16` precision: Simply load your model in `bfloat16` when calling
    `from_pretrained` and you can reduce the size of the model by 2:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: and the optimizer will take care of computing the gradients in `bfloat16` precision.
    Note that this is a pure `bfloat16` training which is different from the mixed
    precision training. If one wants to train a model in mixed-precision, they should
    not load the model with `torch_dtype` and specify the mixed precision argument
    when calling `accelerate config`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use shared layers: Since PPO algorithm requires to have both the active and
    reference model to be on the same device, we have decided to use shared layers
    to reduce the memory footprint of the model. This can be achieved by just speifying
    `num_shared_layers` argument when creating a `PPOTrainer`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/4215e6a434fb11164c991104519460a0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the example above this means that the model have the 4 first layers frozen
    (i.e. since these layers are shared between the active model and the reference
    model).
  prefs: []
  type: TYPE_NORMAL
- en: One could have also applied gradient checkpointing to reduce the memory footprint
    of the model by calling `model.pretrained_model.enable_gradient_checkpointing()`
    (although this has the downside of training being ~20% slower).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the model!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have decided to keep 3 models in total that correspond to our best models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[`ybelkada/gpt-neo-125m-detox`](https://huggingface.co/ybelkada/gpt-neo-125m-detox)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`ybelkada/gpt-neo-2.7B-detox`](https://huggingface.co/ybelkada/gpt-neo-2.7B-detox)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`ybelkada/gpt-j-6b-detox`](https://huggingface.co/ybelkada/gpt-j-6b-detox)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have used different learning rates for each model, and have found out that
    the largest models were quite hard to train and can easily lead to collapse mode
    if the learning rate is not chosen correctly (i.e. if the learning rate is too
    high):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6fa27abd7fdafef846196967c795d891.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final training run of `ybelkada/gpt-j-6b-detoxified-20shdl` looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0aa51106bd217bceabdfc183fce3c44.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see the model converges nicely, but obviously we don’t observe a
    very large improvement from the first step, as the original model is not trained
    to generate toxic contents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also we have observed that training with larger `mini_batch_size` leads to
    smoother convergence and better results on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/473bb6a247337f8423d24950b65e95f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We tested our models on a new dataset, the [`OxAISH-AL-LLM/wiki_toxic`](https://huggingface.co/datasets/OxAISH-AL-LLM/wiki_toxic)
    dataset. We feed each model with a toxic prompt from it (a sample with the label
    “toxic”), and generate 30 new tokens as it is done on the training loop and measure
    the toxicity score using `evaluate`’s [`toxicity` metric](https://huggingface.co/spaces/ybelkada/toxicity).
    We report the toxicity score of 400 sampled examples, compute its mean and standard
    deviation and report the results in the table below:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Mean toxicity score | Std toxicity score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `EleutherAI/gpt-neo-125m` | 0.1627 | 0.2997 |'
  prefs: []
  type: TYPE_TB
- en: '| `ybelkada/gpt-neo-125m-detox` | **0.1148** | **0.2506** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `EleutherAI/gpt-neo-2.7B` | 0.1884 | ,0.3178 |'
  prefs: []
  type: TYPE_TB
- en: '| `ybelkada/gpt-neo-2.7B-detox` | **0.0916** | **0.2104** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `EleutherAI/gpt-j-6B` | 0.1699 | 0.3033 |'
  prefs: []
  type: TYPE_TB
- en: '| `ybelkada/gpt-j-6b-detox` | **0.1510** | **0.2798** |'
  prefs: []
  type: TYPE_TB
- en: '![](../Images/e121f4ce8e0693d912efaa854c62bea5.png)'
  prefs: []
  type: TYPE_IMG
- en: Toxicity score with respect to the size of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below are few generation examples of `gpt-j-6b-detox` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa972843f0dc3596c33670ee2af1af10.png)'
  prefs: []
  type: TYPE_IMG
- en: The evaluation script can be found [here](https://github.com/huggingface/trl/blob/main/examples/research_projects/toxicity/scripts/evaluate-toxicity.py).
  prefs: []
  type: TYPE_NORMAL
- en: Discussions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The results are quite promising, as we can see that the models are able to reduce
    the toxicity score of the generated text by an interesting margin. The gap is
    clear for `gpt-neo-2B` model but we less so for the `gpt-j-6B` model. There are
    several things we could try to improve the results on the largest model starting
    with training with larger `mini_batch_size` and probably allowing to back-propagate
    through more layers (i.e. use less shared layers).
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, in addition to human feedback this could be a useful additional signal
    when training large language models to ensure there outputs are less toxic as
    well as useful.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are also aware of consistent bias issues reported with toxicity classifiers,
    and of work evaluating the negative impact of toxicity reduction on the diversity
    of outcomes. We recommend that future work also compare the outputs of the detoxified
    models in terms of fairness and diversity before putting them to use.
  prefs: []
  type: TYPE_NORMAL
- en: What is next?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can download the model and use it out of the box with `transformers`, or
    play with the Spaces that compares the output of the models before and after detoxification
    [here](https://huggingface.co/spaces/ybelkada/detoxified-lms).
  prefs: []
  type: TYPE_NORMAL
