- en: Stable unCLIP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稳定的unCLIP
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/stable_unclip](https://huggingface.co/docs/diffusers/api/pipelines/stable_unclip)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/stable_unclip](https://huggingface.co/docs/diffusers/api/pipelines/stable_unclip)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Stable unCLIP checkpoints are finetuned from [Stable Diffusion 2.1](./stable_diffusion/stable_diffusion_2)
    checkpoints to condition on CLIP image embeddings. Stable unCLIP still conditions
    on text embeddings. Given the two separate conditionings, stable unCLIP can be
    used for text guided image variation. When combined with an unCLIP prior, it can
    also be used for full text to image generation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定的unCLIP检查点是从[稳定扩散2.1](./stable_diffusion/stable_diffusion_2)检查点微调而来，以对CLIP图像嵌入进行条件化。稳定的unCLIP仍然以文本嵌入为条件。鉴于这两种单独的条件，稳定的unCLIP可用于文本引导的图像变化。与unCLIP先验结合使用时，还可用于完整的文本到图像生成。
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要是：
- en: '*Contrastive models like CLIP have been shown to learn robust representations
    of images that capture both semantics and style. To leverage these representations
    for image generation, we propose a two-stage model: a prior that generates a CLIP
    image embedding given a text caption, and a decoder that generates an image conditioned
    on the image embedding. We show that explicitly generating image representations
    improves image diversity with minimal loss in photorealism and caption similarity.
    Our decoders conditioned on image representations can also produce variations
    of an image that preserve both its semantics and style, while varying the non-essential
    details absent from the image representation. Moreover, the joint embedding space
    of CLIP enables language-guided image manipulations in a zero-shot fashion. We
    use diffusion models for the decoder and experiment with both autoregressive and
    diffusion models for the prior, finding that the latter are computationally more
    efficient and produce higher-quality samples.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*像CLIP这样的对比模型已被证明能够学习到捕捉语义和风格的图像的稳健表示。为了利用这些表示进行图像生成，我们提出了一个两阶段模型：一个先验模型，根据文本标题生成一个CLIP图像嵌入，以及一个解码器，根据图像嵌入生成一个图像。我们展示了明确生成图像表示可以提高图像的多样性，同时最小化逼真度和标题相似度的损失。我们的解码器以图像表示为条件还可以产生保留其语义和风格的图像变化，同时变化那些不包含在图像表示中的非必要细节。此外，CLIP的联合嵌入空间使得可以通过语言引导图像操作。我们使用解码模型进行解码，并尝试使用自回归模型和扩散模型进行先验，发现后者在计算上更有效，并产生更高质量的样本。*'
- en: Tips
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示
- en: Stable unCLIP takes `noise_level` as input during inference which determines
    how much noise is added to the image embeddings. A higher `noise_level` increases
    variation in the final un-noised images. By default, we do not add any additional
    noise to the image embeddings (`noise_level = 0`).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定的unCLIP在推断过程中接受`noise_level`作为输入，该参数确定向图像嵌入中添加多少噪音。较高的`noise_level`会增加最终去噪图像的变化。默认情况下，我们不向图像嵌入中添加任何额外的噪音（`noise_level
    = 0`）。
- en: Text-to-Image Generation
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本到图像生成
- en: 'Stable unCLIP can be leveraged for text-to-image generation by pipelining it
    with the prior model of KakaoBrain’s open source DALL-E 2 replication [Karlo](https://huggingface.co/kakaobrain/karlo-v1-alpha):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定的unCLIP可以通过与KakaoBrain开源的DALL-E 2复制模型[Karlo](https://huggingface.co/kakaobrain/karlo-v1-alpha)的先前模型进行流水线处理，用于文本到图像的生成：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For text-to-image we use `stabilityai/stable-diffusion-2-1-unclip-small` as
    it was trained on CLIP ViT-L/14 embedding, the same as the Karlo model prior.
    [stabilityai/stable-diffusion-2-1-unclip](https://hf.co/stabilityai/stable-diffusion-2-1-unclip)
    was trained on OpenCLIP ViT-H, so we don’t recommend its use.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本到图像，我们使用`stabilityai/stable-diffusion-2-1-unclip-small`，因为它是在CLIP ViT-L/14嵌入上训练的，与Karlo模型先验相同。[stabilityai/stable-diffusion-2-1-unclip](https://hf.co/stabilityai/stable-diffusion-2-1-unclip)是在OpenCLIP
    ViT-H上训练的，因此我们不建议使用它。
- en: Text guided Image-to-Image Variation
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本引导的图像到图像变化
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Optionally, you can also pass a prompt to `pipe` such as:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，您还可以向`pipe`传递提示，例如：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 请务必查看调度器[指南](../../using-diffusers/schedulers)，以了解如何探索调度器速度和质量之间的权衡，并查看[跨流水线重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分，以了解如何有效地将相同组件加载到多个流水线中。
- en: StableUnCLIPPipeline
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StableUnCLIPPipeline
- en: '### `class diffusers.StableUnCLIPPipeline`'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.StableUnCLIPPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L61)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L61)'
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prior_tokenizer` (`CLIPTokenizer`) — A `CLIPTokenizer`.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_tokenizer`（`CLIPTokenizer`）—一个`CLIPTokenizer`。'
- en: '`prior_text_encoder` (`CLIPTextModelWithProjection`) — Frozen `CLIPTextModelWithProjection`
    text-encoder.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_text_encoder`（`CLIPTextModelWithProjection`）—冻结的`CLIPTextModelWithProjection`文本编码器。'
- en: '`prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    — The canonincal unCLIP prior to approximate the image embedding from the text
    embedding.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior`（[PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer））—用于从文本嵌入中近似图像嵌入的经典unCLIP先验。'
- en: '`prior_scheduler` (`KarrasDiffusionSchedulers`) — Scheduler used in the prior
    denoising process.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_scheduler`（`KarrasDiffusionSchedulers`）—先验去噪过程中使用的调度器。'
- en: '`image_normalizer` (`StableUnCLIPImageNormalizer`) — Used to normalize the
    predicted image embeddings before the noise is applied and un-normalize the image
    embeddings after the noise has been applied.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_normalizer`（`StableUnCLIPImageNormalizer`）—用于在应用噪音之前对预测的图像嵌入进行归一化，并在应用噪音后对图像嵌入进行反归一化。'
- en: '`image_noising_scheduler` (`KarrasDiffusionSchedulers`) — Noise schedule for
    adding noise to the predicted image embeddings. The amount of noise to add is
    determined by the `noise_level`.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_noising_scheduler`（`KarrasDiffusionSchedulers`）—用于向预测的图像嵌入添加噪音的噪音计划。要添加的噪音量由`noise_level`确定。'
- en: '`tokenizer` (`CLIPTokenizer`) — A `CLIPTokenizer`.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（`CLIPTokenizer`）— 一个`CLIPTokenizer`。'
- en: '`text_encoder` (`CLIPTextModel`) — Frozen `CLIPTextModel` text-encoder.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder`（`CLIPTextModel`）— 冻结的`CLIPTextModel`文本编码器。'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — A [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)
    to denoise the encoded image latents.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)）—
    用于去噪编码图像潜在特征的[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)。'
- en: '`scheduler` (`KarrasDiffusionSchedulers`) — A scheduler to be used in combination
    with `unet` to denoise the encoded image latents.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler`（`KarrasDiffusionSchedulers`）— 与`unet`结合使用的调度器，用于去噪编码图像潜在特征。'
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) Model to encode and decode images to and from
    latent representations.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）—
    变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。'
- en: Pipeline for text-to-image generation using stable unCLIP.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用稳定的unCLIP进行文本到图像生成的管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取为所有管道实现的通用方法（下载、保存、在特定设备上运行等）。
- en: 'The pipeline also inherits the following loading methods:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 该管道还继承了以下加载方法：
- en: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    for loading textual inversion embeddings'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    用于加载文本反演嵌入'
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    用于加载LoRA权重'
- en: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    for saving LoRA weights'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    用于保存LoRA权重'
- en: '#### `__call__`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L649)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L649)'
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt`（`str`或`List[str]`，*可选*）— 用于指导图像生成的提示。如果未定义，则需要传递`prompt_embeds`。'
- en: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The height in pixels of the generated image.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height`（`int`，*可选*，默认为`self.unet.config.sample_size * self.vae_scale_factor`）—
    生成图像的像素高度。'
- en: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The width in pixels of the generated image.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width`（`int`，*可选*，默认为`self.unet.config.sample_size * self.vae_scale_factor`）—
    生成图像的像素宽度。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 20) — The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps`（`int`，*可选*，默认为20）— 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 10.0) — A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale`（`float`，*可选*，默认为10.0）— 更高的指导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale
    > 1`时启用指导比例。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt`（`str`或`List[str]`，*可选*）— 用于指导图像生成中不包括什么的提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用指导（`guidance_scale
    < 1`）时被忽略。'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — The number of
    images to generate per prompt.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt`（`int`，*可选*，默认为1）— 每个提示生成的图像数量。'
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta`（`float`，*可选*，默认为0.0）— 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数eta（η）。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度器中被忽略。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator`（`torch.Generator`或`List[torch.Generator]`，*可选*）— 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for image generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents`（`torch.FloatTensor`，*可选*）— 从高斯分布中采样的预生成嘈杂潜在特征，用作图像生成的输入。可以用来使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜在特征张量。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，文本嵌入将从`prompt`输入参数生成。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds`将从`negative_prompt`输入参数生成。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的一个。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回一个[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)而不是一个普通的元组。'
- en: '`callback` (`Callable`, *optional*) — A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *可选*) — 在推理过程中每`callback_steps`步调用的函数。该函数将使用以下参数调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *可选*, 默认为1) — 调用`callback`函数的频率。如果未指定，则在每一步时调用回调函数。'
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *可选*) — 如果指定，则将其作为kwargs字典传递给`AttentionProcessor`，如[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的那样。'
- en: '`noise_level` (`int`, *optional*, defaults to `0`) — The amount of noise to
    add to the image embeddings. A higher `noise_level` increases the variance in
    the final un-noised images. See [StableUnCLIPPipeline.noise_image_embeddings()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.StableUnCLIPPipeline.noise_image_embeddings)
    for more details.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`noise_level` (`int`, *可选*, 默认为`0`) — 添加到图像嵌入中的噪声量。较高的`noise_level`会增加最终无噪声图像的方差。有关更多详细信息，请参阅[StableUnCLIPPipeline.noise_image_embeddings()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.StableUnCLIPPipeline.noise_image_embeddings)。'
- en: '`prior_num_inference_steps` (`int`, *optional*, defaults to 25) — The number
    of denoising steps in the prior denoising process. More denoising steps usually
    lead to a higher quality image at the expense of slower inference.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_num_inference_steps` (`int`, *可选*, 默认为`25`) — 先验去噪过程中的去噪步数。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。'
- en: '`prior_guidance_scale` (`float`, *optional*, defaults to 4.0) — A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_guidance_scale` (`float`, *可选*, 默认为`4.0`) — 更高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。当`guidance_scale
    > 1`时启用引导比例。'
- en: '`prior_latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents
    sampled from a Gaussian distribution, to be used as inputs for image embedding
    generation in the prior denoising process. Can be used to tweak the same generation
    with different prompts. If not provided, a latents tensor is generated by sampling
    using the supplied random `generator`.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成噪声潜变量，用作先验去噪过程中图像嵌入生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机`generator`进行采样生成潜变量张量。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *可选*) — 在计算提示嵌入时要从CLIP中跳过的层数。值为1意味着将使用前一层的输出来计算提示嵌入。'
- en: Returns
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    或 `tuple`'
- en: '`~ pipeline_utils.ImagePipelineOutput` if `return_dict` is True, otherwise
    a `tuple`. When returning a tuple, the first element is a list with the generated
    images.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`return_dict`为True，则为`~ pipeline_utils.ImagePipelineOutput`，否则为一个`tuple`。返回元组时，第一个元素是包含生成图像的列表。
- en: The call function to the pipeline for generation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: 'Examples:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#### `enable_attention_slicing`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_attention_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)'
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`slice_size` (`str` or `int`, *optional*, defaults to `"auto"`) — When `"auto"`,
    halves the input to the attention heads, so attention will be computed in two
    steps. If `"max"`, maximum amount of memory will be saved by running only one
    slice at a time. If a number is provided, uses as many slices as `attention_head_dim
    // slice_size`. In this case, `attention_head_dim` must be a multiple of `slice_size`.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`slice_size` (`str` 或 `int`, *可选*, 默认为`"auto"`) — 当为`"auto"`时，将输入减半到注意力头，因此注意力将分两步计算。如果为`"max"`，则通过一次只运行一个切片来节省最大内存量。如果提供了一个数字，则使用`attention_head_dim
    // slice_size`个切片。在这种情况下，`attention_head_dim`必须是`slice_size`的倍数。'
- en: Enable sliced attention computation. When this option is enabled, the attention
    module splits the input tensor in slices to compute attention in several steps.
    For more than one attention head, the computation is performed sequentially over
    each head. This is useful to save some memory in exchange for a small speed decrease.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片注意力计算。启用此选项时，注意力模块将输入张量分割为切片以在几个步骤中计算注意力。对于多个注意力头，计算将按顺序在每个头上执行。这对于节省一些内存以换取一点速度降低很有用。
- en: ⚠️ Don’t enable attention slicing if you’re already using `scaled_dot_product_attention`
    (SDPA) from PyTorch 2.0 or xFormers. These attention computations are already
    very memory efficient so you won’t need to enable this function. If you enable
    attention slicing with SDPA or xFormers, it can lead to serious slow downs!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ⚠️ 如果您已经在使用PyTorch 2.0或xFormers的`scaled_dot_product_attention`（SDPA），请不要启用注意力切片。这些注意力计算已经非常高效，因此您不需要启用此功能。如果您在SDPA或xFormers上启用了注意力切片，可能会导致严重减速！
- en: 'Examples:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#### `disable_attention_slicing`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_attention_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)'
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Disable sliced attention computation. If `enable_attention_slicing` was previously
    called, attention is computed in one step.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片注意力计算。如果之前调用了`enable_attention_slicing`，则注意力在一步中计算。
- en: '#### `enable_vae_slicing`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L159)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L159)'
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片VAE解码。启用此选项时，VAE将在几个步骤中将输入张量分割为切片以计算解码。这对于节省一些内存并允许更大的批量大小很有用。
- en: '#### `disable_vae_slicing`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L167)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L167)'
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片VAE解码。如果之前启用了`enable_vae_slicing`，则此方法将回到在一步中计算解码。
- en: '#### `enable_xformers_memory_efficient_attention`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_xformers_memory_efficient_attention`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)'
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`attention_op` (`Callable`, *optional*) — Override the default `None` operator
    for use as `op` argument to the [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)
    function of xFormers.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_op` (`Callable`, *可选*) — 用作`op`参数传递给xFormers的[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)函数的默认`None`操作符的覆盖。'
- en: Enable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).
    When this option is enabled, you should observe lower GPU memory usage and a potential
    speed up during inference. Speed up during training is not guaranteed.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 启用[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。启用此选项时，您应该观察到较低的GPU内存使用量和潜在的推理加速。训练期间的加速不能保证。
- en: ⚠️ When memory efficient attention and sliced attention are both enabled, memory
    efficient attention takes precedent.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ⚠️ 当内存高效注意力和切片注意力都启用时，内存高效注意力优先。
- en: 'Examples:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#### `disable_xformers_memory_efficient_attention`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_xformers_memory_efficient_attention`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)'
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Disable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。
- en: '#### `encode_prompt`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_prompt`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L302)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L302)'
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded device —
    (`torch.device`): torch device'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示设备 — (`torch.device`): torch设备'
- en: '`num_images_per_prompt` (`int`) — number of images that should be generated
    per prompt'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`) — 应该为每个提示生成的图像数量'
- en: '`do_classifier_free_guidance` (`bool`) — whether to use classifier free guidance
    or not'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_classifier_free_guidance` (`bool`) — 是否使用无分类器指导'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` 或 `List[str]`, *可选*) — 不用来指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即，如果`guidance_scale`小于`1`，则忽略）。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数生成negative_prompt_embeds。'
- en: '`lora_scale` (`float`, *optional*) — A LoRA scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale`（`float`，*可选*）- 如果加载了LoRA层，则将应用于文本编码器的所有LoRA层的LoRA比例。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip`（`int`，*可选*）- 在计算提示嵌入时要从CLIP中跳过的层数。值为1意味着将使用前一层的输出来计算提示嵌入。'
- en: Encodes the prompt into text encoder hidden states.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示编码为文本编码器的隐藏状态。
- en: '#### `noise_image_embeddings`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `noise_image_embeddings`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L603)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py#L603)'
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Add noise to the image embeddings. The amount of noise is controlled by a `noise_level`
    input. A higher `noise_level` increases the variance in the final un-noised images.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 向图像嵌入添加噪音。噪音的量由`noise_level`输入控制。较高的`noise_level`会增加最终未去噪图像的方差。
- en: 'The noise is applied in two ways:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 噪音以两种方式应用：
- en: A noise schedule is applied directly to the embeddings.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 噪音计划直接应用于嵌入。
- en: A vector of sinusoidal time embeddings are appended to the output.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将正弦时间嵌入向量附加到输出。
- en: In both cases, the amount of noise is controlled by the same `noise_level`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在两种情况下，噪音的量由相同的`noise_level`控制。
- en: The embeddings are normalized before the noise is applied and un-normalized
    after the noise is applied.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用噪音之前对嵌入进行归一化，应用噪音后取消归一化。
- en: StableUnCLIPImg2ImgPipeline
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StableUnCLIPImg2ImgPipeline
- en: '### `class diffusers.StableUnCLIPImg2ImgPipeline`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.StableUnCLIPImg2ImgPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L72)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L72)'
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`feature_extractor` (`CLIPImageProcessor`) — Feature extractor for image pre-processing
    before being encoded.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor`（`CLIPImageProcessor`）- 用于在编码之前对图像进行特征提取的特征提取器。'
- en: '`image_encoder` (`CLIPVisionModelWithProjection`) — CLIP vision model for encoding
    images.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_encoder`（`CLIPVisionModelWithProjection`）- 用于编码图像的CLIP视觉模型。'
- en: '`image_normalizer` (`StableUnCLIPImageNormalizer`) — Used to normalize the
    predicted image embeddings before the noise is applied and un-normalize the image
    embeddings after the noise has been applied.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_normalizer`（`StableUnCLIPImageNormalizer`）- 用于在应用噪音之前对预测的图像嵌入进行归一化，并在应用噪音后取消归一化图像嵌入。'
- en: '`image_noising_scheduler` (`KarrasDiffusionSchedulers`) — Noise schedule for
    adding noise to the predicted image embeddings. The amount of noise to add is
    determined by the `noise_level`.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_noising_scheduler`（`KarrasDiffusionSchedulers`）- 用于向预测的图像嵌入添加噪音的噪音计划。要添加的噪音量由`noise_level`确定。'
- en: '`tokenizer` (`~transformers.CLIPTokenizer`) — A [`~transformers.CLIPTokenizer`)].'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（`~transformers.CLIPTokenizer`）- 一个[`~transformers.CLIPTokenizer`]。'
- en: '`text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel))
    — Frozen [CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)
    text-encoder.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder`（[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)）-
    冻结的[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)文本编码器。'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — A [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)
    to denoise the encoded image latents.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)）-
    用于去噪编码图像潜在的[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)。'
- en: '`scheduler` (`KarrasDiffusionSchedulers`) — A scheduler to be used in combination
    with `unet` to denoise the encoded image latents.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler`（`KarrasDiffusionSchedulers`）- 用于与`unet`结合使用以去噪编码图像潜在的调度器。'
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) Model to encode and decode images to and from
    latent representations.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）-
    变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示形式。'
- en: Pipeline for text-guided image-to-image generation using stable unCLIP.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用稳定的unCLIP进行文本引导的图像生成管道。
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。
- en: 'The pipeline also inherits the following loading methods:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 该管道还继承了以下加载方法：
- en: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    for loading textual inversion embeddings'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    用于加载文本反演嵌入'
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    用于加载LoRA权重'
- en: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    for saving LoRA weights'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    用于保存LoRA权重'
- en: '#### `__call__`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L623)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L623)'
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    the image generation. If not defined, either `prompt_embeds` will be used or prompt
    is initialized to `""`.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` or `List[str]`, *optional*) — 指导图像生成的提示或提示。如果未定义，则将使用`prompt_embeds`或将prompt初始化为`""`。'
- en: '`image` (`torch.FloatTensor` or `PIL.Image.Image`) — `Image` or tensor representing
    an image batch. The image is encoded to its CLIP embedding which the `unet` is
    conditioned on. The image is *not* encoded by the `vae` and then used as the latents
    in the denoising process like it is in the standard Stable Diffusion text-guided
    image variation process.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`torch.FloatTensor` or `PIL.Image.Image`) — 代表图像批次的`Image`或张量。图像被编码为其CLIP嵌入，`unet`以此为条件。图像*不*由`vae`编码，然后像标准稳定扩散文本引导图像变化过程中那样作为去噪过程中的潜变量使用。'
- en: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The height in pixels of the generated image.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — 生成图像的像素高度。'
- en: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — The width in pixels of the generated image.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    — 生成图像的像素宽度。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 20) — The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *optional*, defaults to 20) — 去噪步骤的数量。更多的去噪步骤通常会导致图像质量更高，但推断速度较慢。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 10.0) — A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *optional*, defaults to 10.0) — 较高的引导比例值鼓励模型生成与文本`prompt`密切相关的图像，但会降低图像质量。当`guidance_scale
    > 1`时启用引导比例。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` or `List[str]`, *optional*) — 指导图像生成中不包含的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用引导时（`guidance_scale
    < 1`）忽略。'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — The number of
    images to generate per prompt.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的图像数量。'
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, *optional*, defaults to 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数eta（η）。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中被忽略。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for image generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中采样的预生成嘈杂潜变量，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机`generator`进行采样生成潜变量张量。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（prompt加权）。如果未提供，则从`prompt`输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入（prompt加权）。如果未提供，`negative_prompt_embeds`将从`negative_prompt`输入参数生成。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, defaults to `"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的一个。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)而不是普通元组。'
- en: '`callback` (`Callable`, *optional*) — A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *optional*) — 在推断过程中每隔`callback_steps`步调用的函数。该函数使用以下参数调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *optional*, defaults to 1) — 调用`callback`函数的频率。如果未指定，则在每一步调用回调函数。'
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给`AttentionProcessor`的kwargs字典，如[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的那样。'
- en: '`noise_level` (`int`, *optional*, defaults to `0`) — The amount of noise to
    add to the image embeddings. A higher `noise_level` increases the variance in
    the final un-noised images. See [StableUnCLIPPipeline.noise_image_embeddings()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.StableUnCLIPPipeline.noise_image_embeddings)
    for more details.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`noise_level`（`int`，*可选*，默认为`0`）-要添加到图像嵌入中的噪声量。较高的`noise_level`会增加最终无噪声图像中的方差。有关更多详细信息，请参阅[StableUnCLIPPipeline.noise_image_embeddings()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.StableUnCLIPPipeline.noise_image_embeddings)。'
- en: '`image_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated CLIP embeddings
    to condition the `unet` on. These latents are not used in the denoising process.
    If you want to provide pre-generated latents, pass them to `__call__` as `latents`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeds`（`torch.FloatTensor`，*可选*）-预生成的CLIP嵌入，用于对`unet`进行条件处理。这些潜在变量在去噪过程中不被使用。如果要提供预生成的潜在变量，请将它们作为`latents`传递给`__call__`。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip`（`int`，*可选*）-在计算提示嵌入时要从CLIP中跳过的层数。值为1意味着将使用预终层的输出来计算提示嵌入。'
- en: Returns
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)或`tuple`'
- en: '`~ pipeline_utils.ImagePipelineOutput` if `return_dict` is True, otherwise
    a `tuple`. When returning a tuple, the first element is a list with the generated
    images.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`return_dict`为True，则为`~ pipeline_utils.ImagePipelineOutput`，否则为`tuple`。返回元组时，第一个元素是包含生成图像的列表。
- en: The call function to the pipeline for generation.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成的管道的调用函数。
- en: 'Examples:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE18]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '#### `enable_attention_slicing`'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 启用注意力切片
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)'
- en: '[PRE19]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`slice_size` (`str` or `int`, *optional*, defaults to `"auto"`) — When `"auto"`,
    halves the input to the attention heads, so attention will be computed in two
    steps. If `"max"`, maximum amount of memory will be saved by running only one
    slice at a time. If a number is provided, uses as many slices as `attention_head_dim
    // slice_size`. In this case, `attention_head_dim` must be a multiple of `slice_size`.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`slice_size`（`str`或`int`，*可选*，默认为`"auto"`）-当为`"auto"`时，将输入减半到注意力头部，因此注意力将在两个步骤中计算。如果为`"max"`，将通过一次只运行一个切片来保存最大内存量。如果提供一个数字，则使用`attention_head_dim
    // slice_size`个切片。在这种情况下，`attention_head_dim`必须是`slice_size`的倍数。'
- en: Enable sliced attention computation. When this option is enabled, the attention
    module splits the input tensor in slices to compute attention in several steps.
    For more than one attention head, the computation is performed sequentially over
    each head. This is useful to save some memory in exchange for a small speed decrease.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片注意力计算。启用此选项时，注意力模块将输入张量分割成片段，以便在多个步骤中计算注意力。对于多个注意力头，计算将按顺序在每个头上执行。这对于节省一些内存以换取一点速度降低很有用。
- en: ⚠️ Don’t enable attention slicing if you’re already using `scaled_dot_product_attention`
    (SDPA) from PyTorch 2.0 or xFormers. These attention computations are already
    very memory efficient so you won’t need to enable this function. If you enable
    attention slicing with SDPA or xFormers, it can lead to serious slow downs!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ⚠️ 如果您已经在使用PyTorch 2.0或xFormers的`scaled_dot_product_attention`（SDPA），请不要启用注意力切片。这些注意力计算已经非常高效，因此您不需要启用此功能。如果您在SDPA或xFormers中启用了注意力切片，可能会导致严重减速！
- en: 'Examples:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE20]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#### `disable_attention_slicing`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用注意力切片
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)'
- en: '[PRE21]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Disable sliced attention computation. If `enable_attention_slicing` was previously
    called, attention is computed in one step.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片注意力计算。如果之前调用了`enable_attention_slicing`，则注意力将在一步中计算。
- en: '#### `enable_vae_slicing`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 启用VAE切片
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L160)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L160)'
- en: '[PRE22]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片VAE解码。启用此选项时，VAE将将输入张量分割成片段，以便在多个步骤中计算解码。这对于节省一些内存并允许更大的批量大小很有用。
- en: '#### `disable_vae_slicing`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用VAE切片
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L168)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L168)'
- en: '[PRE23]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片VAE解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到在一步中计算解码。
- en: '#### `enable_xformers_memory_efficient_attention`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 启用XFormers内存高效注意力
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)'
- en: '[PRE24]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`attention_op` (`Callable`, *optional*) — Override the default `None` operator
    for use as `op` argument to the [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)
    function of xFormers.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_op`（`Callable`，*可选*）-覆盖默认的`None`运算符，用作xFormers的[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)函数的`op`参数。'
- en: Enable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).
    When this option is enabled, you should observe lower GPU memory usage and a potential
    speed up during inference. Speed up during training is not guaranteed.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 启用来自 [xFormers](https://facebookresearch.github.io/xformers/) 的内存高效注意力。启用此选项时，您应该观察到较低的
    GPU 内存使用量和潜在的推理加速。训练期间的加速不被保证。
- en: ⚠️ When memory efficient attention and sliced attention are both enabled, memory
    efficient attention takes precedent.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ⚠️ 当启用内存高效注意力和切片注意力时，内存高效注意力优先。
- en: 'Examples:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#### `disable_xformers_memory_efficient_attention`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_xformers_memory_efficient_attention`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)'
- en: '[PRE26]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Disable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用来自 [xFormers](https://facebookresearch.github.io/xformers/) 的内存高效注意力。
- en: '#### `encode_prompt`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_prompt`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L264)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L264)'
- en: '[PRE27]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded device —
    (`torch.device`): torch device'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` 或 `List[str]`, *可选*) — 要编码的提示 — (`torch.device`): torch 设备'
- en: '`num_images_per_prompt` (`int`) — number of images that should be generated
    per prompt'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`) — 应该为每个提示生成的图像数量'
- en: '`do_classifier_free_guidance` (`bool`) — whether to use classifier free guidance
    or not'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_classifier_free_guidance` (`bool`) — 是否使用无分类器指导'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` 或 `List[str]`, *可选*) — 不指导图像生成的提示或提示。如果未定义，则必须传递 `negative_prompt_embeds`。在不使用指导时被忽略（即，如果
    `guidance_scale` 小于 `1`，则被忽略）。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，*例如* 提示加权。如果未提供，文本嵌入将从
    `prompt` 输入参数生成。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入，*例如*
    提示加权。如果未提供，将从 `negative_prompt` 输入参数生成 negative_prompt_embeds。'
- en: '`lora_scale` (`float`, *optional*) — A LoRA scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale` (`float`, *可选*) — 将应用于文本编码器的所有 LoRA 层的 LoRA 比例。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *可选*) — 在计算提示嵌入时要从 CLIP 跳过的层数。值为 1 表示将使用预最终层的输出来计算提示嵌入。'
- en: Encodes the prompt into text encoder hidden states.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示编码为文本编码器隐藏状态。
- en: '#### `noise_image_embeddings`'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `noise_image_embeddings`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L577)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py#L577)'
- en: '[PRE28]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Add noise to the image embeddings. The amount of noise is controlled by a `noise_level`
    input. A higher `noise_level` increases the variance in the final un-noised images.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 向图像嵌入添加噪声。噪声的量由 `noise_level` 输入控制。较高的 `noise_level` 增加了最终未去噪图像中的方差。
- en: 'The noise is applied in two ways:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声以两种方式应用：
- en: A noise schedule is applied directly to the embeddings.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入直接应用噪声计划。
- en: A vector of sinusoidal time embeddings are appended to the output.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一组正弦时间嵌入被附加到输出中。
- en: In both cases, the amount of noise is controlled by the same `noise_level`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，噪声的量由相同的 `noise_level` 控制。
- en: The embeddings are normalized before the noise is applied and un-normalized
    after the noise is applied.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用噪声之前对嵌入进行归一化，应用噪声后取消归一化。
- en: ImagePipelineOutput
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImagePipelineOutput
- en: '### `class diffusers.ImagePipelineOutput`'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.ImagePipelineOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L116)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L116)'
- en: '[PRE29]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`List[PIL.Image.Image]` or `np.ndarray`) — List of denoised PIL images
    of length `batch_size` or NumPy array of shape `(batch_size, height, width, num_channels)`.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`List[PIL.Image.Image]` 或 `np.ndarray`) — 长度为 `batch_size` 的去噪 PIL
    图像列表或形状为 `(batch_size, height, width, num_channels)` 的 NumPy 数组。'
- en: Output class for image pipelines.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图像管道的输出类。
