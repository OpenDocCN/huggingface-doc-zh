- en: NLLB
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLLB
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/nllb](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/nllb)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/nllb](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/nllb)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Updated tokenizer behavior
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新的分词器行为
- en: '**DISCLAIMER:** The default behaviour for the tokenizer was fixed and thus
    changed in April 2023. The previous version adds `[self.eos_token_id, self.cur_lang_code]`
    at the end of the token sequence for both target and source tokenization. This
    is wrong as the NLLB paper mentions (page 48, 6.1.1\. Model Architecture) :'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明：** 分词器的默认行为已在 2023 年 4 月修复并更改。之前的版本在目标和源分词序列的末尾都添加了 `[self.eos_token_id,
    self.cur_lang_code]`。这是错误的，因为 NLLB 论文提到了 (第 48 页，6.1.1\. 模型架构)：'
- en: '*Note that we prefix the source sequence with the source language, as opposed
    to the target language as previously done in several works (Arivazhagan et al.,
    2019; Johnson et al., 2017). This is primarily because we prioritize optimizing
    zero-shot performance of our model on any pair of 200 languages at a minor cost
    to supervised performance.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*请注意，我们将源序列前缀与源语言一起使用，而不是像以前的一些作品那样使用目标语言 (Arivazhagan 等人，2019；Johnson 等人，2017)。这主要是因为我们优先考虑在任何一对
    200 种语言上优化我们模型的零翻译性能，对监督性能的损失很小。*'
- en: 'Previous behaviour:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的行为：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: New behaviour
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 新行为
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Enabling the old behaviour can be done as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下方式启用旧行为：
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For more details, feel free to check the linked [PR](https://github.com/huggingface/transformers/pull/22313)
    and [Issue](https://github.com/huggingface/transformers/issues/19943).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 更多细节，请查看链接的 [PR](https://github.com/huggingface/transformers/pull/22313) 和 [Issue](https://github.com/huggingface/transformers/issues/19943)。
- en: Overview
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The NLLB model was presented in [No Language Left Behind: Scaling Human-Centered
    Machine Translation](https://arxiv.org/abs/2207.04672) by Marta R. Costa-jussà,
    James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe
    Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume
    Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip
    Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon
    Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov,
    Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre
    Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 'NLLB 模型由 Marta R. Costa-jussà、James Cross、Onur Çelebi、Maha Elbayad、Kenneth
    Heafield、Kevin Heffernan、Elahe Kalbassi、Janice Lam、Daniel Licht、Jean Maillard、Anna
    Sun、Skyler Wang、Guillaume Wenzek、Al Youngblood、Bapi Akula、Loic Barrault、Gabriel
    Mejia Gonzalez、Prangthip Hansanti、John Hoffman、Semarley Jarrett、Kaushik Ram Sadagopan、Dirk
    Rowe、Shannon Spruit、Chau Tran、Pierre Andrews、Necip Fazil Ayan、Shruti Bhosale、Sergey
    Edunov、Angela Fan、Cynthia Gao、Vedanuj Goswami、Francisco Guzmán、Philipp Koehn、Alexandre
    Mourachko、Christophe Ropers、Safiyyah Saleem、Holger Schwenk 和 Jeff Wang 在 [No Language
    Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672)
    中提出。'
- en: 'The abstract of the paper is the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*Driven by the goal of eradicating language barriers on a global scale, machine
    translation has solidified itself as a key focus of artificial intelligence research
    today. However, such efforts have coalesced around a small subset of languages,
    leaving behind the vast majority of mostly low-resource languages. What does it
    take to break the 200 language barrier while ensuring safe, high quality results,
    all while keeping ethical considerations in mind? In No Language Left Behind,
    we took on this challenge by first contextualizing the need for low-resource language
    translation support through exploratory interviews with native speakers. Then,
    we created datasets and models aimed at narrowing the performance gap between
    low and high-resource languages. More specifically, we developed a conditional
    compute model based on Sparsely Gated Mixture of Experts that is trained on data
    obtained with novel and effective data mining techniques tailored for low-resource
    languages. We propose multiple architectural and training improvements to counteract
    overfitting while training on thousands of tasks. Critically, we evaluated the
    performance of over 40,000 different translation directions using a human-translated
    benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark
    covering all languages in Flores-200 to assess translation safety. Our model achieves
    an improvement of 44% BLEU relative to the previous state-of-the-art, laying important
    groundwork towards realizing a universal translation system.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*受到在全球范围内消除语言障碍的目标驱动，机器翻译已经巩固自己作为当今人工智能研究的重点。然而，这些努力已经围绕一小部分语言展开，抛弃了绝大多数主要是低资源语言。要突破
    200 种语言障碍并确保安全、高质量的结果，同时考虑伦理因素，需要什么？在《No Language Left Behind》中，我们首先通过与母语者的探索性访谈来定位对低资源语言翻译支持的需求。然后，我们创建了旨在缩小低资源语言和高资源语言之间性能差距的数据集和模型。更具体地说，我们开发了一个基于
    Sparsely Gated Mixture of Experts 的条件计算模型，该模型是通过针对低资源语言量身定制的新颖和有效的数据挖掘技术获得的数据进行训练的。我们提出了多种架构和训练改进来对抗在数千个任务上训练时的过拟合。至关重要的是，我们使用人工翻译的基准
    Flores-200 评估了超过 40,000 种不同的翻译方向的性能，并结合了一个涵盖 Flores-200 中所有语言的新颖毒性基准来评估翻译的安全性。我们的模型相对于先前的最新技术实现提高了
    44% 的 BLEU，为实现通用翻译系统奠定了重要基础。*'
- en: This implementation contains the dense models available on release.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现包含发布的稠密模型。
- en: '**The sparse model NLLB-MoE (Mixture of Expert) is now available! More details
    [here](nllb-moe)**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**稀疏模型 NLLB-MoE (Mixture of Expert) 现已推出！更多细节请查看 [这里](nllb-moe)**'
- en: This model was contributed by [Lysandre](https://huggingface.co/lysandre). The
    authors’ code can be found [here](https://github.com/facebookresearch/fairseq/tree/nllb).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由 [Lysandre](https://huggingface.co/lysandre) 贡献。作者的代码可以在 [这里](https://github.com/facebookresearch/fairseq/tree/nllb)
    找到。
- en: Generating with NLLB
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 NLLB 生成
- en: While generating the target text set the `forced_bos_token_id` to the target
    language id. The following example shows how to translate English to French using
    the *facebook/nllb-200-distilled-600M* model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成目标文本时，将`forced_bos_token_id`设置为目标语言ID。以下示例展示了如何使用*facebook/nllb-200-distilled-600M*模型将英语翻译成法语。
- en: Note that we’re using the BCP-47 code for French `fra_Latn`. See [here](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)
    for the list of all BCP-47 in the Flores 200 dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用法语`fra_Latn`的BCP-47代码。在[Flores 200数据集](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)中查看所有BCP-47的列表。
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Generating from any other language than English
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从除英语以外的任何其他语言生成
- en: English (`eng_Latn`) is set as the default language from which to translate.
    In order to specify that you’d like to translate from a different language, you
    should specify the BCP-47 code in the `src_lang` keyword argument of the tokenizer
    initialization.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 英语（`eng_Latn`）被设置为默认的翻译源语言。为了指定您想要从其他语言翻译，您应该在分词器初始化的`src_lang`关键字参数中指定BCP-47代码。
- en: 'See example below for a translation from romanian to german:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下示例，将罗马尼亚语翻译成德语：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Resources
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Translation task guide](../tasks/translation)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[翻译任务指南](../tasks/translation)'
- en: '[Summarization task guide](../tasks/summarization)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[摘要任务指南](../tasks/summarization)'
- en: NllbTokenizer
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NllbTokenizer
- en: '### `class transformers.NllbTokenizer`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.NllbTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb.py#L47)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb.py#L47)'
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇文件的路径。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`，*可选*，默认为`"<s>"`) — 在预训练期间使用的序列开始标记。可以用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列开始的标记。使用的标记是`cls_token`。
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`，*可选*，默认为`"</s>"`) — 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列结束的标记。使用的标记是`sep_token`。
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`，*可选*，默认为`"</s>"`) — 分隔符标记，在从多个序列构建序列时使用，例如，用于序列分类的两个序列或用于文本和问题的问题回答。它还用作使用特殊标记构建的序列的最后一个标记。'
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`，*可选*，默认为`"<s>"`) — 用于进行序列分类（对整个序列进行分类而不是每个标记进行分类）时使用的分类器标记。在使用特殊标记构建时，它是序列的第一个标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`，*可选*，默认为`"<unk>"`) — 未知标记。词汇表中没有的标记无法转换为ID，而是设置为此标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`，*可选*，默认为`"<pad>"`) — 用于填充的标记，例如，当批处理不同长度的序列时。'
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str`，*可选*，默认为`"<mask>"`) — 用于屏蔽值的标记。在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`tokenizer_file` (`str`, *optional*) — The path to a tokenizer file to use
    instead of the vocab file.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_file` (`str`，*可选*) — 要使用的分词器文件的路径，而不是词汇文件。'
- en: '`src_lang` (`str`, *optional*) — The language to use as source language for
    translation.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`src_lang` (`str`，*可选*) — 用作翻译源语言的语言。'
- en: '`tgt_lang` (`str`, *optional*) — The language to use as target language for
    translation.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tgt_lang` (`str`，*可选*) — 用作翻译目标语言的语言。'
- en: '`sp_model_kwargs` (`Dict[str, str]`) — Additional keyword arguments to pass
    to the model initialization.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_model_kwargs` (`Dict[str, str]`) — 传递给模型初始化的额外关键字参数。'
- en: Construct an NLLB tokenizer.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个NLLB分词器。
- en: Adapted from [RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)
    and [XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer).
    Based on [SentencePiece](https://github.com/google/sentencepiece).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 改编自[RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)和[XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer)。基于[SentencePiece](https://github.com/google/sentencepiece)。
- en: The tokenization method is `<tokens> <eos> <language code>` for source language
    documents, and `<language code>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 源语言文档的分词方法是`<tokens> <eos> <语言代码>`，目标语言文档的分词方法是`<语言代码>`
- en: <tokens> <eos>` for target language documents.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <tokens> <eos>` 用于目标语言文档。
- en: 'Examples:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb.py#L269)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb.py#L269)'
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 将添加特殊标记的ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 第二个序列对的ID列表。'
- en: Returns
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 带有适当特殊标记的[输入ID](../glossary#input-ids)列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. An NLLB sequence has the following
    format, where `X` represents the sequence:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '通过连接和添加特殊标记从序列或序列对构建用于序列分类任务的模型输入。NLLB序列具有以下格式，其中`X`表示序列:'
- en: '`input_ids` (for encoder) `X [eos, src_lang_code]`'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (用于编码器) `X [eos, src_lang_code]`'
- en: '`decoder_input_ids`: (for decoder) `X [eos, tgt_lang_code]`'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`: (用于解码器) `X [eos, tgt_lang_code]`'
- en: BOS is never used. Pairs of sequences are not the expected use case, but they
    will be handled without a separator.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: BOS 从不使用。序列对不是预期的用例，但它们将被处理而无需分隔符。
- en: NllbTokenizerFast
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NllbTokenizerFast
- en: '### `class transformers.NllbTokenizerFast`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.NllbTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L59)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L59)'
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇表文件的路径。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`, *可选*, 默认为 `"<s>"`) — 在预训练期间使用的序列开始标记。可以用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列开始的标记。使用的标记是`cls_token`。
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *可选*, 默认为 `"</s>"`) — 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列结束的标记。使用的标记是`sep_token`。
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *可选*, 默认为 `"</s>"`) — 分隔符标记，用于从多个序列构建序列时使用，例如用于序列分类的两个序列或用于文本和问题的问题回答。它还用作使用特殊标记构建的序列的最后一个标记。'
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *可选*, 默认为 `"<s>"`) — 在进行序列分类（对整个序列而不是每个标记进行分类）时使用的分类器标记。当使用特殊标记构建序列时，它是序列的第一个标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *可选*, 默认为 `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为ID，而是被设置为此标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *可选*, 默认为 `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。'
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str`, *可选*, 默认为 `"<mask>"`) — 用于掩码值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`tokenizer_file` (`str`, *optional*) — The path to a tokenizer file to use
    instead of the vocab file.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_file` (`str`, *可选*) — 要使用的分词器文件的路径，而不是词汇表文件。'
- en: '`src_lang` (`str`, *optional*) — The language to use as source language for
    translation.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`src_lang` (`str`, *可选*) — 用作翻译源语言的语言。'
- en: '`tgt_lang` (`str`, *optional*) — The language to use as target language for
    translation.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tgt_lang` (`str`, *可选*) — 用作翻译目标语言的语言。'
- en: Construct a “fast” NLLB tokenizer (backed by HuggingFace’s *tokenizers* library).
    Based on [BPE](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个“快速”NLLB分词器（由HuggingFace的*tokenizers*库支持）。基于[BPE](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models)。
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大部分主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: The tokenization method is `<tokens> <eos> <language code>` for source language
    documents, and `<language code>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 源语言文档的分词方法是`<tokens> <eos> <language code>`，目标语言文档的分词方法是`<tokens> <eos>`。
- en: <tokens> <eos>` for target language documents.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`<tokens> <eos>` 用于目标语言文档。'
- en: 'Examples:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L212)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L212)'
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 将添加特殊标记的ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 第二个序列对的ID列表。'
- en: Returns
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: list of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 带有适当特殊标记的[输入ID](../glossary#input-ids)列表。
- en: Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. The special tokens depend on
    calling set_lang.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记从序列或序列对构建用于序列分类任务的模型输入。特殊标记取决于调用set_lang。
- en: 'An NLLB sequence has the following format, where `X` represents the sequence:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: NLLB序列具有以下格式，其中`X`表示序列：
- en: '`input_ids` (for encoder) `X [eos, src_lang_code]`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（用于编码器）`X [eos, src_lang_code]`'
- en: '`decoder_input_ids`: (for decoder) `X [eos, tgt_lang_code]`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`：（用于解码器）`X [eos, tgt_lang_code]`'
- en: BOS is never used. Pairs of sequences are not the expected use case, but they
    will be handled without a separator.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: BOS从不使用。序列对不是预期的用例，但它们将在没有分隔符的情况下处理。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L241)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L241)'
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）— ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of zeros.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 零的列表。
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. nllb does not make use of token type ids, therefore a list of zeros is returned.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建一个用于序列对分类任务的掩码。nllb不使用标记类型id，因此返回一个零的列表。
- en: '#### `set_src_lang_special_tokens`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_src_lang_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L296)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L296)'
- en: '[PRE12]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Reset the special tokens to the source lang setting.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 将特殊标记重置为源语言设置。
- en: 'In legacy mode: No prefix and suffix=[eos, src_lang_code].'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在传统模式下：无前缀，后缀=[eos, src_lang_code]。
- en: 'In default mode: Prefix=[src_lang_code], suffix = [eos]'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在默认模式下：前缀=[src_lang_code]，后缀=[eos]
- en: '#### `set_tgt_lang_special_tokens`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_tgt_lang_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L319)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/nllb/tokenization_nllb_fast.py#L319)'
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Reset the special tokens to the target lang setting.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 将特殊标记重置为目标语言设置。
- en: 'In legacy mode: No prefix and suffix=[eos, tgt_lang_code].'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在传统模式下：无前缀，后缀=[eos, tgt_lang_code]。
- en: 'In default mode: Prefix=[tgt_lang_code], suffix = [eos]'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在默认模式下：前缀=[tgt_lang_code]，后缀=[eos]
