- en: T5v1.1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: T5v1.1
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/t5v1.1](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/t5v1.1)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/t5v1.1](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/t5v1.1)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: T5v1.1 was released in the [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)
    repository by Colin Raffel et al. It’s an improved version of the original T5
    model. This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).
    The original code can be found [here](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: T5v1.1是由Colin Raffel等人在[google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)存储库中发布的。这是原始T5模型的改进版本。这个模型是由[patrickvonplaten](https://huggingface.co/patrickvonplaten)贡献的。原始代码可以在[这里](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)找到。
- en: Usage tips
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: 'One can directly plug in the weights of T5v1.1 into a T5 model, like so:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 可以直接将T5v1.1的权重插入到T5模型中，如下所示：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'T5 Version 1.1 includes the following improvements compared to the original
    T5 model:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: T5版本1.1相对于原始T5模型包括以下改进：
- en: GEGLU activation in the feed-forward hidden layer, rather than ReLU. See [this
    paper](https://arxiv.org/abs/2002.05202).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前馈隐藏层中使用GEGLU激活，而不是ReLU。参见[这篇论文](https://arxiv.org/abs/2002.05202)。
- en: Dropout was turned off in pre-training (quality win). Dropout should be re-enabled
    during fine-tuning.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预训练中关闭了Dropout（质量优胜）。在微调期间应重新启用Dropout。
- en: Pre-trained on C4 only without mixing in the downstream tasks.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅在C4上进行预训练，没有混合下游任务。
- en: No parameter sharing between the embedding and classifier layer.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入层和分类器层之间没有参数共享。
- en: “xl” and “xxl” replace “3B” and “11B”. The model shapes are a bit different
    - larger `d_model` and smaller `num_heads` and `d_ff`.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “xl”和“xxl”取代了“3B”和“11B”。模型形状有些不同 - 更大的`d_model`和较小的`num_heads`和`d_ff`。
- en: 'Note: T5 Version 1.1 was only pre-trained on [C4](https://huggingface.co/datasets/c4)
    excluding any supervised training. Therefore, this model has to be fine-tuned
    before it is usable on a downstream task, unlike the original T5 model. Since
    t5v1.1 was pre-trained unsupervisedly, there’s no real advantage to using a task
    prefix during single-task fine-tuning. If you are doing multi-task fine-tuning,
    you should use a prefix.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：T5版本1.1仅在[C4](https://huggingface.co/datasets/c4)上进行了预训练，不包括任何监督训练。因此，这个模型在用于下游任务之前必须进行微调，与原始T5模型不同。由于t5v1.1是无监督预训练的，单任务微调时使用任务前缀并没有真正的优势。如果进行多任务微调，应该使用前缀。
- en: 'Google has released the following variants:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Google发布了以下变体：
- en: '[google/t5-v1_1-small](https://huggingface.co/google/t5-v1_1-small)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[google/t5-v1_1-small](https://huggingface.co/google/t5-v1_1-small)'
- en: '[google/t5-v1_1-base](https://huggingface.co/google/t5-v1_1-base)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[google/t5-v1_1-base](https://huggingface.co/google/t5-v1_1-base)'
- en: '[google/t5-v1_1-large](https://huggingface.co/google/t5-v1_1-large)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[google/t5-v1_1-large](https://huggingface.co/google/t5-v1_1-large)'
- en: '[google/t5-v1_1-xl](https://huggingface.co/google/t5-v1_1-xl)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[google/t5-v1_1-xl](https://huggingface.co/google/t5-v1_1-xl)'
- en: '[google/t5-v1_1-xxl](https://huggingface.co/google/t5-v1_1-xxl).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[google/t5-v1_1-xxl](https://huggingface.co/google/t5-v1_1-xxl)。'
- en: Refer to [T5’s documentation page](t5) for all API reference, tips, code examples
    and notebooks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[T5的文档页面](t5)获取所有API参考、提示、代码示例和笔记本。
