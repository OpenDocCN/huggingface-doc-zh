- en: T5v1.1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/t5v1.1](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/t5v1.1)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: T5v1.1 was released in the [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)
    repository by Colin Raffel et al. It’s an improved version of the original T5
    model. This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).
    The original code can be found [here](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One can directly plug in the weights of T5v1.1 into a T5 model, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'T5 Version 1.1 includes the following improvements compared to the original
    T5 model:'
  prefs: []
  type: TYPE_NORMAL
- en: GEGLU activation in the feed-forward hidden layer, rather than ReLU. See [this
    paper](https://arxiv.org/abs/2002.05202).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout was turned off in pre-training (quality win). Dropout should be re-enabled
    during fine-tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-trained on C4 only without mixing in the downstream tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No parameter sharing between the embedding and classifier layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “xl” and “xxl” replace “3B” and “11B”. The model shapes are a bit different
    - larger `d_model` and smaller `num_heads` and `d_ff`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: T5 Version 1.1 was only pre-trained on [C4](https://huggingface.co/datasets/c4)
    excluding any supervised training. Therefore, this model has to be fine-tuned
    before it is usable on a downstream task, unlike the original T5 model. Since
    t5v1.1 was pre-trained unsupervisedly, there’s no real advantage to using a task
    prefix during single-task fine-tuning. If you are doing multi-task fine-tuning,
    you should use a prefix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Google has released the following variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[google/t5-v1_1-small](https://huggingface.co/google/t5-v1_1-small)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[google/t5-v1_1-base](https://huggingface.co/google/t5-v1_1-base)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[google/t5-v1_1-large](https://huggingface.co/google/t5-v1_1-large)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[google/t5-v1_1-xl](https://huggingface.co/google/t5-v1_1-xl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[google/t5-v1_1-xxl](https://huggingface.co/google/t5-v1_1-xxl).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to [T5’s documentation page](t5) for all API reference, tips, code examples
    and notebooks.
  prefs: []
  type: TYPE_NORMAL
