["```py\npip install -U albumentations opencv-python\n```", "```py\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"scene_parse_150\", split=\"train\")\n>>> index = 10\n>>> dataset[index]\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=683x512 at 0x7FB37B0EC810>,\n 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=683x512 at 0x7FB37B0EC9D0>,\n 'scene_category': 927}\n```", "```py\n>>> dataset[index][\"image\"]\n```", "```py\n>>> dataset[index][\"annotation\"]\n```", "```py\n>>> import matplotlib.pyplot as plt\n\n>>> def visualize_seg_mask(image: np.ndarray, mask: np.ndarray):\n...    color_seg = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n...    palette = np.array(create_ade20k_label_colormap())\n...    for label, color in enumerate(palette):\n...        color_seg[mask == label, :] = color\n...    color_seg = color_seg[..., ::-1]  # convert to BGR\n\n...    img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map\n...    img = img.astype(np.uint8)\n\n...    plt.figure(figsize=(15, 10))\n...    plt.imshow(img)\n...    plt.axis(\"off\")\n...    plt.show()\n\n>>> visualize_seg_mask(\n...     np.array(dataset[index][\"image\"]),\n...     np.array(dataset[index][\"annotation\"])\n... )\n```", "```py\n>>> import albumentations\n\n>>> transform = albumentations.Compose(\n...     [\n...         albumentations.Resize(256, 256),\n...         albumentations.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n...     ]\n... )\n```", "```py\n>>> def transforms(examples):\n...     transformed_images, transformed_masks = [], []\n...\n...     for image, seg_mask in zip(examples[\"image\"], examples[\"annotation\"]):\n...         image, seg_mask = np.array(image), np.array(seg_mask)\n...         transformed = transform(image=image, mask=seg_mask)\n...         transformed_images.append(transformed[\"image\"])\n...         transformed_masks.append(transformed[\"mask\"])\n...\n...     examples[\"pixel_values\"] = transformed_images\n...     examples[\"label\"] = transformed_masks\n...     return examples\n```", "```py\n>>> dataset.set_transform(transforms)\n```", "```py\n>>> image = np.array(dataset[index][\"pixel_values\"])\n>>> mask = np.array(dataset[index][\"label\"])\n\n>>> visualize_seg_mask(image, mask)\n```", "```py\n>>> from torchvision.transforms import Resize, ColorJitter, Compose\n\n>>> transformation_chain = Compose([\n...     Resize((256, 256)),\n...     ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)\n... ])\n>>> resize = Resize((256, 256))\n\n>>> def train_transforms(example_batch):\n...     example_batch[\"pixel_values\"] = [transformation_chain(x) for x in example_batch[\"image\"]]\n...     example_batch[\"label\"] = [resize(x) for x in example_batch[\"annotation\"]]\n...     return example_batch\n\n>>> dataset.set_transform(train_transforms)\n\n>>> image = np.array(dataset[index][\"pixel_values\"])\n>>> mask = np.array(dataset[index][\"label\"])\n\n>>> visualize_seg_mask(image, mask)\n```"]