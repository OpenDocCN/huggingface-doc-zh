- en: Generation with LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs的生成
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial](https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial](https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: LLMs, or Large Language Models, are the key component behind text generation.
    In a nutshell, they consist of large pretrained transformer models trained to
    predict the next word (or, more precisely, token) given some input text. Since
    they predict one token at a time, you need to do something more elaborate to generate
    new sentences other than just calling the model — you need to do autoregressive
    generation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs，或大型语言模型，是文本生成背后的关键组件。简而言之，它们由大型预训练的变压器模型组成，训练用于预测给定一些输入文本的下一个单词（或更准确地说，令牌）。由于它们一次预测一个令牌，因此您需要做一些更复杂的事情来生成新的句子，而不仅仅是调用模型
    - 您需要进行自回归生成。
- en: Autoregressive generation is the inference-time procedure of iteratively calling
    a model with its own generated outputs, given a few initial inputs. In 🤗 Transformers,
    this is handled by the [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    method, which is available to all models with generative capabilities.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归生成是在推理时迭代调用模型以生成输出的过程，给定一些初始输入。在🤗 Transformers中，这由[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)方法处理，适用于所有具有生成能力的模型。
- en: 'This tutorial will show you how to:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将向您展示如何：
- en: Generate text with an LLM
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLM生成文本
- en: Avoid common pitfalls
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免常见陷阱
- en: Next steps to help you get the most out of your LLM
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帮助您充分利用LLM的下一步
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请确保您已安装所有必要的库：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Generate text
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成文本
- en: A language model trained for [causal language modeling](tasks/language_modeling)
    takes a sequence of text tokens as input and returns the probability distribution
    for the next token.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 进行[因果语言建模](tasks/language_modeling)训练的语言模型将文本令牌序列作为输入，并返回下一个令牌的概率分布。
- en: <https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: <https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov>
- en: '"Forward pass of an LLM"'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '"LLM的前向传递"'
- en: A critical aspect of autoregressive generation with LLMs is how to select the
    next token from this probability distribution. Anything goes in this step as long
    as you end up with a token for the next iteration. This means it can be as simple
    as selecting the most likely token from the probability distribution or as complex
    as applying a dozen transformations before sampling from the resulting distribution.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs进行自回归生成的一个关键方面是如何从这个概率分布中选择下一个令牌。在这一步中可以采取任何方法，只要最终得到下一次迭代的令牌即可。这意味着它可以简单地从概率分布中选择最可能的令牌，也可以在从结果分布中抽样之前应用十几种转换。
- en: <https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov>
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: <https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov>
- en: '"Autoregressive generation iteratively selects the next token from a probability
    distribution to generate text"'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '"自回归生成通过从概率分布中迭代选择下一个令牌来生成文本"'
- en: The process depicted above is repeated iteratively until some stopping condition
    is reached. Ideally, the stopping condition is dictated by the model, which should
    learn when to output an end-of-sequence (`EOS`) token. If this is not the case,
    generation stops when some predefined maximum length is reached.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 上述过程会重复迭代，直到达到某个停止条件。理想情况下，停止条件由模型决定，该模型应该学会何时输出一个终止序列（`EOS`）令牌。如果不是这种情况，当达到某个预定义的最大长度时，生成会停止。
- en: Properly setting up the token selection step and the stopping condition is essential
    to make your model behave as you’d expect on your task. That is why we have a
    [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    file associated with each model, which contains a good default generative parameterization
    and is loaded alongside your model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正确设置令牌选择步骤和停止条件对于使您的模型在任务上表现如您期望的方式至关重要。这就是为什么我们为每个模型关联一个[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)文件，其中包含一个良好的默认生成参数设置，并且与您的模型一起加载。
- en: Let’s talk code!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈代码！
- en: If you’re interested in basic LLM usage, our high-level [`Pipeline`](pipeline_tutorial)
    interface is a great starting point. However, LLMs often require advanced features
    like quantization and fine control of the token selection step, which is best
    done through [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate).
    Autoregressive generation with LLMs is also resource-intensive and should be executed
    on a GPU for adequate throughput.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对基本LLM用法感兴趣，我们的高级[`Pipeline`](pipeline_tutorial)接口是一个很好的起点。然而，LLMs通常需要高级功能，如量化和对令牌选择步骤的精细控制，最好通过[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)来实现。LLMs的自回归生成也需要大量资源，并且应该在GPU上执行以获得足够的吞吐量。
- en: First, you need to load the model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要加载模型。
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You’ll notice two flags in the `from_pretrained` call:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到`from_pretrained`调用中有两个标志：
- en: '`device_map` ensures the model is moved to your GPU(s)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_map`确保模型被移动到您的GPU上'
- en: '`load_in_4bit` applies [4-bit dynamic quantization](main_classes/quantization)
    to massively reduce the resource requirements'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_in_4bit`应用[4位动态量化](main_classes/quantization)以大幅减少资源需求'
- en: There are other ways to initialize a model, but this is a good baseline to begin
    with an LLM.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他初始化模型的方法，但这是一个很好的基准，可以开始使用LLM。
- en: Next, you need to preprocess your text input with a [tokenizer](tokenizer_summary).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要使用[tokenizer](tokenizer_summary)对文本输入进行预处理。
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `model_inputs` variable holds the tokenized text input, as well as the attention
    mask. While [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    does its best effort to infer the attention mask when it is not passed, we recommend
    passing it whenever possible for optimal results.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`model_inputs`变量保存了标记化的文本输入，以及注意力掩码。虽然[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)会尽力推断注意力掩码，但我们建议尽可能在生成时传递它以获得最佳结果。'
- en: After tokenizing the inputs, you can call the [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    method to returns the generated tokens. The generated tokens then should be converted
    to text before printing.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在对输入进行标记化后，您可以调用[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)方法返回生成的标记。然后应将生成的标记转换为文本后打印。
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, you don’t need to do it one sequence at a time! You can batch your
    inputs, which will greatly improve the throughput at a small latency and memory
    cost. All you need to do is to make sure you pad your inputs properly (more on
    that below).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您不需要一次处理一个序列！您可以对输入进行批处理，这将大大提高吞吐量，同时延迟和内存成本很小。您只需要确保正确填充输入即可（下文有更多信息）。
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And that’s it! In a few lines of code, you can harness the power of an LLM.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！在几行代码中，您就可以利用LLM的强大功能。
- en: Common pitfalls
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见陷阱
- en: There are many [generation strategies](generation_strategies), and sometimes
    the default values may not be appropriate for your use case. If your outputs aren’t
    aligned with what you’re expecting, we’ve created a list of the most common pitfalls
    and how to avoid them.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多[生成策略](generation_strategies)，有时默认值可能不适合您的用例。如果您的输出与您的预期不符，我们已经创建了一个关于最常见陷阱以及如何避免它们的列表。
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Generated output is too short/long
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成的输出过短/过长
- en: If not specified in the [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    file, `generate` returns up to 20 tokens by default. We highly recommend manually
    setting `max_new_tokens` in your `generate` call to control the maximum number
    of new tokens it can return. Keep in mind LLMs (more precisely, [decoder-only
    models](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)) also return
    the input prompt as part of the output.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)文件中未指定，`generate`默认返回最多20个标记。我们强烈建议在`generate`调用中手动设置`max_new_tokens`以控制它可以返回的最大新标记数量。请记住，LLMs（更准确地说，仅解码器模型）还会将输入提示作为输出的一部分返回。
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Incorrect generation mode
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成模式不正确
- en: By default, and unless specified in the [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    file, `generate` selects the most likely token at each iteration (greedy decoding).
    Depending on your task, this may be undesirable; creative tasks like chatbots
    or writing an essay benefit from sampling. On the other hand, input-grounded tasks
    like audio transcription or translation benefit from greedy decoding. Enable sampling
    with `do_sample=True`, and you can learn more about this topic in this [blog post](https://huggingface.co/blog/how-to-generate).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，除非在[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)文件中指定，`generate`在每次迭代中选择最可能的标记（贪婪解码）。根据您的任务，这可能是不希望的；像聊天机器人或写作文章这样的创造性任务受益于抽样。另一方面，像音频转录或翻译这样的输入驱动任务受益于贪婪解码。通过`do_sample=True`启用抽样，您可以在此[博客文章](https://huggingface.co/blog/how-to-generate)中了解更多关于这个主题的信息。
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Wrong padding side
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 填充方向错误
- en: LLMs are [decoder-only](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)
    architectures, meaning they continue to iterate on your input prompt. If your
    inputs do not have the same length, they need to be padded. Since LLMs are not
    trained to continue from pad tokens, your input needs to be left-padded. Make
    sure you also don’t forget to pass the attention mask to generate!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs是仅解码器架构，意味着它们会继续迭代您的输入提示。如果您的输入长度不同，就需要进行填充。由于LLMs没有经过训练以从填充标记继续，因此您的输入需要进行左填充。确保不要忘记传递注意力掩码以生成！
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Wrong prompt
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示错误
- en: 'Some models and tasks expect a certain input prompt format to work properly.
    When this format is not applied, you will get a silent performance degradation:
    the model kinda works, but not as well as if you were following the expected prompt.
    More information about prompting, including which models and tasks need to be
    careful, is available in this [guide](tasks/prompting). Let’s see an example with
    a chat LLM, which makes use of [chat templating](chat_templating):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型和任务期望特定的输入提示格式才能正常工作。如果未应用此格式，您将获得沉默的性能下降：模型可能会运行，但不如按照预期提示那样好。有关提示的更多信息，包括哪些模型和任务需要小心，可在此[指南](tasks/prompting)中找到。让我们看一个聊天LLM的示例，它使用聊天模板：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Further resources
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多资源
- en: 'While the autoregressive generation process is relatively straightforward,
    making the most out of your LLM can be a challenging endeavor because there are
    many moving parts. For your next steps to help you dive deeper into LLM usage
    and understanding:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然自回归生成过程相对简单，但充分利用您的LLM可能是一项具有挑战性的努力，因为其中有许多要素。为了帮助您深入了解LLM的使用和理解，请继续以下步骤：
- en: Advanced generate usage
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级生成用法
- en: '[Guide](generation_strategies) on how to control different generation methods,
    how to set up the generation configuration file, and how to stream the output;'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[指南](generation_strategies)关于如何控制不同的生成方法，如何设置生成配置文件，以及如何流式传输输出；'
- en: '[Guide](chat_templating) on the prompt template for chat LLMs;'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[指南](chat_templating)关于聊天LLM的提示模板；'
- en: '[Guide](tasks/prompting) on to get the most of prompt design;'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[指南](tasks/prompting)如何充分利用提示设计；'
- en: API reference on [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig),
    [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate),
    and [generate-related classes](internal/generation_utils). Most of the classes,
    including the logits processors, have usage examples!
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    上的 API 参考，[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)，以及
    [generate-related classes](internal/generation_utils)。大多数类，包括 logits 处理器，都有使用示例！'
- en: LLM leaderboards
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM 排行榜
- en: '[Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    which focuses on the quality of the open-source models;'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)，专注于开源模型的质量；'
- en: '[Open LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard),
    which focuses on LLM throughput.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Open LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)，专注于
    LLM 吞吐量。'
- en: Latency, throughput and memory utilization
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 延迟、吞吐量和内存利用率
- en: '[Guide](llm_tutorial_optimization) on how to optimize LLMs for speed and memory;'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Guide](llm_tutorial_optimization) 如何优化 LLMs 的速度和内存；'
- en: '[Guide](main_classes/quantization) on quantization such as bitsandbytes and
    autogptq, which shows you how to drastically reduce your memory requirements.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Guide](main_classes/quantization) 关于量化，如 bitsandbytes 和 autogptq，展示了如何大幅减少内存需求。'
- en: Related libraries
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关库
- en: '[`text-generation-inference`](https://github.com/huggingface/text-generation-inference),
    a production-ready server for LLMs;'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[`text-generation-inference`](https://github.com/huggingface/text-generation-inference)，一个为
    LLMs 准备的生产就绪服务器；'
- en: '[`optimum`](https://github.com/huggingface/optimum), an extension of 🤗 Transformers
    that optimizes for specific hardware devices.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[`optimum`](https://github.com/huggingface/optimum)，一个 🤗 Transformers 的扩展，针对特定硬件设备进行优化。'
