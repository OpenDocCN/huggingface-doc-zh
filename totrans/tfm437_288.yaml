- en: Swin2SR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/swin2sr](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/swin2sr)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/255.ebfa76f4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Swin2SR model was proposed in [Swin2SR: SwinV2 Transformer for Compressed
    Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos
    V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte. Swin2R improves the [SwinIR](https://github.com/JingyunLiang/SwinIR/)
    model by incorporating [Swin Transformer v2](swinv2) layers which mitigates issues
    such as training instability, resolution gaps between pre-training and fine-tuning,
    and hunger on data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Compression plays an important role on the efficient transmission and storage
    of images and videos through band-limited systems such as streaming services,
    virtual reality or videogames. However, compression unavoidably leads to artifacts
    and the loss of the original information, which may severely degrade the visual
    quality. For these reasons, quality enhancement of compressed images has become
    a popular research topic. While most state-of-the-art image restoration methods
    are based on convolutional neural networks, other transformers-based methods such
    as SwinIR, show impressive performance on these tasks. In this paper, we explore
    the novel Swin Transformer V2, to improve SwinIR for image super-resolution, and
    in particular, the compressed input scenario. Using this method we can tackle
    the major issues in training transformer vision models, such as training instability,
    resolution gaps between pre-training and fine-tuning, and hunger on data. We conduct
    experiments on three representative tasks: JPEG compression artifacts removal,
    image super-resolution (classical and lightweight), and compressed image super-resolution.
    Experimental results demonstrate that our method, Swin2SR, can improve the training
    convergence and performance of SwinIR, and is a top-5 solution at the “AIM 2022
    Challenge on Super-Resolution of Compressed Image and Video”.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![drawing](../Images/f1fba26a0f54f4e6e010a9479238494a.png) Swin2SR architecture.
    Taken from the [original paper.](https://arxiv.org/abs/2209.11345)'
  prefs: []
  type: TYPE_IMG
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code can be found [here](https://github.com/mv-lab/swin2sr).
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Demo notebooks for Swin2SR can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Swin2SR).
  prefs: []
  type: TYPE_NORMAL
- en: A demo Space for image super-resolution with SwinSR can be found [here](https://huggingface.co/spaces/jjourney1125/swin2sr).
  prefs: []
  type: TYPE_NORMAL
- en: Swin2SRImageProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.Swin2SRImageProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/swin2sr/image_processing_swin2sr.py#L38)'
  prefs: []
  type: TYPE_NORMAL
- en: '( do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_pad:
    bool = True pad_size: int = 8 **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**do_rescale** (`bool`, *optional*, defaults to `True`) — Whether to rescale
    the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`
    parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rescale_factor** (`int` or `float`, *optional*, defaults to `1/255`) — Scale
    factor to use if rescaling the image. Can be overridden by the `rescale_factor`
    parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a Swin2SR image processor.
  prefs: []
  type: TYPE_NORMAL
- en: '#### preprocess'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/swin2sr/image_processing_swin2sr.py#L109)'
  prefs: []
  type: TYPE_NORMAL
- en: '( images: Union do_rescale: Optional = None rescale_factor: Optional = None
    do_pad: Optional = None pad_size: Optional = None return_tensors: Union = None
    data_format: Union = <ChannelDimension.FIRST: ''channels_first''> input_data_format:
    Union = None **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**images** (`ImageInput`) — Image to preprocess. Expects a single or batch
    of images with pixel values ranging from 0 to 255\. If passing in images with
    pixel values between 0 and 1, set `do_rescale=False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_rescale** (`bool`, *optional*, defaults to `self.do_rescale`) — Whether
    to rescale the image values between [0 - 1].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rescale_factor** (`float`, *optional*, defaults to `self.rescale_factor`)
    — Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_pad** (`bool`, *optional*, defaults to `True`) — Whether to pad the image
    to make the height and width divisible by `window_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_size** (`int`, *optional*, defaults to 32) — The size of the sliding
    window for the local attention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_tensors** (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unset: Return a list of `np.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of typ, input_data_format=input_data_formate
    `tf.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**data_format** (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unset: Use the channel dimension format of the input image.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_data_format** (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess an image or batch of images.
  prefs: []
  type: TYPE_NORMAL
- en: Swin2SRConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.Swin2SRConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/swin2sr/configuration_swin2sr.py#L30)'
  prefs: []
  type: TYPE_NORMAL
- en: ( image_size = 64 patch_size = 1 num_channels = 3 num_channels_out = None embed_dim
    = 180 depths = [6, 6, 6, 6, 6, 6] num_heads = [6, 6, 6, 6, 6, 6] window_size =
    8 mlp_ratio = 2.0 qkv_bias = True hidden_dropout_prob = 0.0 attention_probs_dropout_prob
    = 0.0 drop_path_rate = 0.1 hidden_act = 'gelu' use_absolute_embeddings = False
    initializer_range = 0.02 layer_norm_eps = 1e-05 upscale = 2 img_range = 1.0 resi_connection
    = '1conv' upsampler = 'pixelshuffle' **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image_size** (`int`, *optional*, defaults to 64) — The size (resolution)
    of each image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**patch_size** (`int`, *optional*, defaults to 1) — The size (resolution) of
    each patch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_channels** (`int`, *optional*, defaults to 3) — The number of input channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_channels_out** (`int`, *optional*, defaults to `num_channels`) — The
    number of output channels. If not set, it will be set to `num_channels`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**embed_dim** (`int`, *optional*, defaults to 180) — Dimensionality of patch
    embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**depths** (`list(int)`, *optional*, defaults to `[6, 6, 6, 6, 6, 6]`) — Depth
    of each layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_heads** (`list(int)`, *optional*, defaults to `[6, 6, 6, 6, 6, 6]`) —
    Number of attention heads in each layer of the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**window_size** (`int`, *optional*, defaults to 8) — Size of windows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mlp_ratio** (`float`, *optional*, defaults to 2.0) — Ratio of MLP hidden
    dimensionality to embedding dimensionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**qkv_bias** (`bool`, *optional*, defaults to `True`) — Whether or not a learnable
    bias should be added to the queries, keys and values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_dropout_prob** (`float`, *optional*, defaults to 0.0) — The dropout
    probability for all fully connected layers in the embeddings and encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_probs_dropout_prob** (`float`, *optional*, defaults to 0.0) — The
    dropout ratio for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**drop_path_rate** (`float`, *optional*, defaults to 0.1) — Stochastic depth
    rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_act** (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder. If string,
    `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_absolute_embeddings** (`bool`, *optional*, defaults to `False`) — Whether
    or not to add absolute position embeddings to the patch embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initializer_range** (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**layer_norm_eps** (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**upscale** (`int`, *optional*, defaults to 2) — The upscale factor for the
    image. 2/3/4/8 for image super resolution, 1 for denoising and compress artifact
    reduction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**img_range** (`float`, *optional*, defaults to 1.0) — The range of the values
    of the input image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**resi_connection** (`str`, *optional*, defaults to `"1conv"`) — The convolutional
    block to use before the residual connection in each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**upsampler** (`str`, *optional*, defaults to `"pixelshuffle"`) — The reconstruction
    reconstruction module. Can be ‘pixelshuffle’/‘pixelshuffledirect’/‘nearest+conv’/None.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [Swin2SRModel](/docs/transformers/v4.37.2/en/model_doc/swin2sr#transformers.Swin2SRModel).
    It is used to instantiate a Swin Transformer v2 model according to the specified
    arguments, defining the model architecture. Instantiating a configuration with
    the defaults will yield a similar configuration to that of the Swin Transformer
    v2 [caidas/swin2sr-classicalsr-x2-64](https://huggingface.co/caidas/swin2sr-classicalsr-x2-64)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Swin2SRModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.Swin2SRModel'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/swin2sr/modeling_swin2sr.py#L809)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([Swin2SRConfig](/docs/transformers/v4.37.2/en/model_doc/swin2sr#transformers.Swin2SRConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare Swin2SR Model transformer outputting raw hidden-states without any
    specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/swin2sr/modeling_swin2sr.py#L862)'
  prefs: []
  type: TYPE_NORMAL
- en: '( pixel_values: FloatTensor head_mask: Optional = None output_attentions: Optional
    = None output_hidden_states: Optional = None return_dict: Optional = None ) →
    [transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**pixel_values** (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [Swin2SRImageProcessor.**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Swin2SRConfig](/docs/transformers/v4.37.2/en/model_doc/swin2sr#transformers.Swin2SRConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [Swin2SRModel](/docs/transformers/v4.37.2/en/model_doc/swin2sr#transformers.Swin2SRModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Swin2SRForImageSuperResolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.Swin2SRForImageSuperResolution'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/swin2sr/modeling_swin2sr.py#L1063)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([Swin2SRConfig](/docs/transformers/v4.37.2/en/model_doc/swin2sr#transformers.Swin2SRConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swin2SR Model transformer with an upsampler head on top for image super resolution
    and restoration.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/swin2sr/modeling_swin2sr.py#L1096)'
  prefs: []
  type: TYPE_NORMAL
- en: '( pixel_values: Optional = None head_mask: Optional = None labels: Optional
    = None output_attentions: Optional = None output_hidden_states: Optional = None
    return_dict: Optional = None ) → `transformers.modeling_outputs.ImageSuperResolutionOutput`
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**pixel_values** (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [Swin2SRImageProcessor.**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.modeling_outputs.ImageSuperResolutionOutput` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.modeling_outputs.ImageSuperResolutionOutput` or a tuple of `torch.FloatTensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration ([Swin2SRConfig](/docs/transformers/v4.37.2/en/model_doc/swin2sr#transformers.Swin2SRConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Reconstruction loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**reconstruction** (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Reconstructed images, possibly upscaled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the model at the output of each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [Swin2SRForImageSuperResolution](/docs/transformers/v4.37.2/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
