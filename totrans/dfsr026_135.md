# 一致性模型

> 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/consistency_models](https://huggingface.co/docs/diffusers/api/pipelines/consistency_models)

一致性模型是由杨松、Prafulla Dhariwal、Mark Chen和Ilya Sutskever在[一致性模型](https://huggingface.co/papers/2303.01469)中提出的。

论文摘要如下：

*扩散模型显著推动了图像、音频和视频生成领域的发展，但它们依赖于引起生成速度缓慢的迭代采样过程。为了克服这一限制，我们提出了一致性模型，这是一类通过直接将噪声映射到数据来生成高质量样本的新模型。它们通过设计支持快速一步生成，同时仍允许多步采样以在计算和样本质量之间进行交换。它们还支持零样本数据编辑，如图像修补、着色和超分辨率，而无需在这些任务上进行明确训练。一致性模型可以通过蒸馏预训练的扩散模型或作为独立的生成模型进行训练。通过大量实验，我们证明它们在一步和少步采样方面优于现有的扩散模型蒸馏技术，实现了CIFAR-10的新的FID最佳值为3.55，ImageNet 64x64为6.20。当单独训练时，一致性模型成为一类新的生成模型，可以在标准基准测试中优于现有的一步、非对抗性生成模型，如CIFAR-10、ImageNet 64x64和LSUN 256x256。*

原始代码库可以在[openai/consistency_models](https://github.com/openai/consistency_models)找到，还可以在[openai](https://huggingface.co/openai)获得额外的检查点。

这个管道是由[dg845](https://github.com/dg845)和[ayushtues](https://huggingface.co/ayushtues)贡献的。❤️

## 提示

为了进一步加速，使用`torch.compile`在<1秒内生成多个图像：

```py
  import torch
  from diffusers import ConsistencyModelPipeline

  device = "cuda"
  # Load the cd_bedroom256_lpips checkpoint.
  model_id_or_path = "openai/diffusers-cd_bedroom256_lpips"
  pipe = ConsistencyModelPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)
  pipe.to(device)

+ pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead", fullgraph=True)

  # Multistep sampling
  # Timesteps can be explicitly specified; the particular timesteps below are from the original GitHub repo:
  # https://github.com/openai/consistency_models/blob/main/scripts/launch.sh#L83
  for _ in range(10):
      image = pipe(timesteps=[17, 0]).images[0]
      image.show()
```

## ConsistencyModelPipeline

### `class diffusers.ConsistencyModelPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/consistency_models/pipeline_consistency_models.py#L63)

```py
( unet: UNet2DModel scheduler: CMStochasticIterativeScheduler )
```

参数

+   `unet`（[UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel））— 用于去噪编码图像潜在特征的`UNet2DModel`。

+   `scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin））— 与`unet`结合使用以去噪编码图像潜在特征的调度器。目前仅与[CMStochasticIterativeScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/cm_stochastic_iterative#diffusers.CMStochasticIterativeScheduler)兼容。

用于无条件或类别条件图像生成的管道。

该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/consistency_models/pipeline_consistency_models.py#L167)

```py
( batch_size: int = 1 class_labels: Union = None num_inference_steps: int = 1 timesteps: List = None generator: Union = None latents: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `batch_size`（`int`，*可选*，默认为1）— 要生成的图像数量。

+   `class_labels`（`torch.Tensor`或`List[int]`或`int`，*可选*）— 用于条件类别一致性模型的可选类别标签。如果模型不是类别条件的，则不使用。

+   `num_inference_steps`（`int`，*可选*，默认为1）— 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `timesteps`（`List[int]`，*可选*）— 用于去噪过程的自定义时间步长。如果未定义，则使用等间距的`num_inference_steps`时间步长。必须按降序排列。

+   `generator`（`torch.Generator`，*可选*）— 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents`（`torch.FloatTensor`，*可选*）— 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可以用来使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜变量张量。

+   `output_type`（`str`，*可选*，默认为`"pil"`）— 生成图像的输出格式。选择`PIL.Image`或`np.array`之间。

+   `return_dict`（`bool`，*可选*，默认为`True`）— 是否返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)而不是普通的元组。

+   `callback`（`Callable`，*可选*）— 在推断期间每`callback_steps`步调用的函数。该函数使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps`（`int`，*可选*，默认为1）— 调用`callback`函数的频率。如果未指定，则在每一步调用回调。

返回

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)或`tuple`

如果`return_dict`为`True`，则返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)，否则返回一个`tuple`，其中第一个元素是包含生成图像的列表。

示例：

```py
>>> import torch

>>> from diffusers import ConsistencyModelPipeline

>>> device = "cuda"
>>> # Load the cd_imagenet64_l2 checkpoint.
>>> model_id_or_path = "openai/diffusers-cd_imagenet64_l2"
>>> pipe = ConsistencyModelPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)
>>> pipe.to(device)

>>> # Onestep Sampling
>>> image = pipe(num_inference_steps=1).images[0]
>>> image.save("cd_imagenet64_l2_onestep_sample.png")

>>> # Onestep sampling, class-conditional image generation
>>> # ImageNet-64 class label 145 corresponds to king penguins
>>> image = pipe(num_inference_steps=1, class_labels=145).images[0]
>>> image.save("cd_imagenet64_l2_onestep_sample_penguin.png")

>>> # Multistep sampling, class-conditional image generation
>>> # Timesteps can be explicitly specified; the particular timesteps below are from the original Github repo:
>>> # https://github.com/openai/consistency_models/blob/main/scripts/launch.sh#L77
>>> image = pipe(num_inference_steps=None, timesteps=[22, 0], class_labels=145).images[0]
>>> image.save("cd_imagenet64_l2_multistep_sample_penguin.png")
```

## ImagePipelineOutput

### `class diffusers.ImagePipelineOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L116)

```py
( images: Union )
```

参数

+   `images`（`List[PIL.Image.Image]`或`np.ndarray`）— 长度为`batch_size`的去噪PIL图像列表或形状为`(batch_size, height, width, num_channels)`的NumPy数组。

图像管道的输出类。
