["```py\nawait inference.translation({\n  model: 't5-base',\n  inputs: 'My name is Wolfgang and I live in Berlin'\n})\n\nawait hf.translation({\n  model: \"facebook/nllb-200-distilled-600M\",\n  inputs: \"how is the weather like in Gaborone\",\n  parameters : {\n    src_lang: \"eng_Latn\",\n    tgt_lang: \"sot_Latn\"\n  }\n})\n\nawait inference.textToImage({\n  model: 'stabilityai/stable-diffusion-2',\n  inputs: 'award winning high resolution photo of a giant tortoise/((ladybird)) hybrid, [trending on artstation]',\n  parameters: {\n    negative_prompt: 'blurry',\n  }\n})\n```", "```py\nnpm install @huggingface/inference\nnpm install @huggingface/hub\nnpm install @huggingface/agents\n```", "```py\nimport { HfInference } from \"@huggingface/inference\";\nimport { HfAgent } from \"@huggingface/agents\";\nimport { createRepo, commit, deleteRepo, listFiles } from \"@huggingface/hub\";\nimport type { RepoId, Credentials } from \"@huggingface/hub\";\n```", "```py\n<script type=\"module\"> import { HfInference } from 'https://cdn.jsdelivr.net/npm/@huggingface/inference@2.6.4/+esm';\n    import { createRepo, commit, deleteRepo, listFiles } from \"https://cdn.jsdelivr.net/npm/@huggingface/hub@0.13.0/+esm\"; </script>\n```", "```py\n// esm.sh\nimport { HfInference } from \"https://esm.sh/@huggingface/inference\"\nimport { HfAgent } from \"https://esm.sh/@huggingface/agents\";\n\nimport { createRepo, commit, deleteRepo, listFiles } from \"https://esm.sh/@huggingface/hub\"\n// or npm:\nimport { HfInference } from \"npm:@huggingface/inference\"\nimport { HfAgent } from \"npm:@huggingface/agents\";\n\nimport { createRepo, commit, deleteRepo, listFiles } from \"npm:@huggingface/hub\"\n```", "```py\nimport { HfInference } from \"@huggingface/inference\";\n\nconst HF_TOKEN = \"hf_...\";\n\nconst inference = new HfInference(HF_TOKEN);\n\n// You can also omit \"model\" to use the recommended model for the task\nawait inference.translation({\n  model: 't5-base',\n  inputs: 'My name is Wolfgang and I live in Amsterdam'\n})\n\nawait inference.textToImage({\n  model: 'stabilityai/stable-diffusion-2',\n  inputs: 'award winning high resolution photo of a giant tortoise/((ladybird)) hybrid, [trending on artstation]',\n  parameters: {\n    negative_prompt: 'blurry',\n  }\n})\n\nawait inference.imageToText({\n  data: await (await fetch('https://picsum.photos/300/300')).blob(),\n  model: 'nlpconnect/vit-gpt2-image-captioning',  \n})\n\n// Using your own dedicated inference endpoint: https://hf.co/docs/inference-endpoints/\nconst gpt2 = inference.endpoint('https://xyz.eu-west-1.aws.endpoints.huggingface.cloud/gpt2');\nconst { generated_text } = await gpt2.textGeneration({inputs: 'The answer to the universe is'});\n```", "```py\nimport {HfAgent, LLMFromHub, defaultTools} from '@huggingface/agents';\n\nconst HF_TOKEN = \"hf_...\";\n\nconst agent = new HfAgent(\n  HF_TOKEN,\n  LLMFromHub(HF_TOKEN),\n  [...defaultTools]\n);\n\n// you can generate the code, inspect it and then run it\nconst code = await agent.generateCode(\"Draw a picture of a cat wearing a top hat. Then caption the picture and read it out loud.\");\nconsole.log(code);\nconst messages = await agent.evaluateCode(code)\nconsole.log(messages); // contains the data\n\n// or you can run the code directly, however you can't check that the code is safe to execute this way, use at your own risk.\nconst messages = await agent.run(\"Draw a picture of a cat wearing a top hat. Then caption the picture and read it out loud.\")\nconsole.log(messages); \n```", "```py\nimport { createRepo, uploadFile, deleteFiles } from \"@huggingface/hub\";\n\nconst HF_TOKEN = \"hf_...\";\n\nawait createRepo({\n  repo: \"my-user/nlp-model\", // or {type: \"model\", name: \"my-user/nlp-test\"},\n  credentials: {accessToken: HF_TOKEN}\n});\n\nawait uploadFile({\n  repo: \"my-user/nlp-model\",\n  credentials: {accessToken: HF_TOKEN},\n  // Can work with native File in browsers\n  file: {\n    path: \"pytorch_model.bin\",\n    content: new Blob(...) \n  }\n});\n\nawait deleteFiles({\n  repo: {type: \"space\", name: \"my-user/my-space\"}, // or \"spaces/my-user/my-space\"\n  credentials: {accessToken: HF_TOKEN},\n  paths: [\"README.md\", \".gitattributes\"]\n});\n```", "```py\nsudo corepack enable\npnpm install\n\npnpm -r format:check\npnpm -r lint:check\npnpm -r test\n```", "```py\npnpm -r build\n```"]