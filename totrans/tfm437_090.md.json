["```py\nRUN_SLOW=1 pytest tests/\nRUN_SLOW=1 pytest examples/\n```", "```py\npytest\n```", "```py\nmake test\n```", "```py\npython -m pytest -n auto --dist=loadfile -s -v ./tests/\n```", "```py\npytest --collect-only -q\n```", "```py\npytest tests/test_optimization.py --collect-only -q\n```", "```py\npytest tests/utils/test_logging.py\n```", "```py\npytest tests/test_optimization.py::OptimizationTest::test_adam_w\n```", "```py\npytest tests/test_optimization.py::OptimizationTest\n```", "```py\npytest tests/test_optimization.py::OptimizationTest --collect-only -q\n```", "```py\npytest -k adam tests/test_optimization.py\n```", "```py\npytest -k \"not adam\" tests/test_optimization.py\n```", "```py\npytest -k \"ada and not adam\" tests/test_optimization.py\n```", "```py\npytest -k \"test_adam_w or test_adam_w\" tests/test_optimization.py\n```", "```py\npytest -k \"test and ada\" tests/test_optimization.py\n```", "```py\nRUN_SLOW=1 pytest -m accelerate_tests tests/models/opt/test_modeling_opt.py \n```", "```py\nr\"\"\"\nReturns:\n\nExample:\n    ```", "```py\"\"\"\n\n```", "```py\npytest --doctest-modules <path_to_file_or_dir>\n```", "```py\npip install pytest-picked\n```", "```py\npytest --picked\n```", "```py\npip install pytest-xdist\n```", "```py\n[tool:pytest]\nlooponfailroots = transformers tests\n```", "```py\n[pytest]\nlooponfailroots = transformers tests\n```", "```py\npytest *ls -1 tests/*py | grep -v test_modeling*\n```", "```py\npytest --cache-clear tests\n```", "```py\npip install pytest-flakefinder\n```", "```py\npytest --flake-finder --flake-runs=5 tests/test_failing_test.py\n```", "```py\npip install pytest-random-order\n```", "```py\npytest tests\n[...]\nUsing --random-order-bucket=module\nUsing --random-order-seed=573663\n```", "```py\npytest --random-order-seed=573663\n[...]\nUsing --random-order-bucket=module\nUsing --random-order-seed=573663\n```", "```py\npytest --random-order-bucket=none tests/test_a.py tests/test_c.py tests/test_b.py\n```", "```py\npytest --random-order-bucket=none\n```", "```py\npip install pytest-sugar\n```", "```py\npytest -p no:sugar\n```", "```py\npytest --pspec tests/test_optimization.py\n```", "```py\npip install pytest-instafail\n```", "```py\npytest --instafail\n```", "```py\nCUDA_VISIBLE_DEVICES=\"\" pytest tests/utils/test_logging.py\n```", "```py\nCUDA_VISIBLE_DEVICES=\"1\" pytest tests/utils/test_logging.py\n```", "```py\n@require_torch_multi_gpu\ndef test_example_with_multi_gpu():\n```", "```py\n@require_tf\ndef test_tf_thing_with_tensorflow():\n```", "```py\n@require_torch_gpu\n@slow\ndef test_example_slow_on_gpu():\n```", "```py\n@parameterized.expand(...)\n@require_torch_multi_gpu\ndef test_integration_foo():\n```", "```py\nfrom transformers.testing_utils import get_gpu_count\n\nn_gpu = get_gpu_count()  # works with torch and tf\n```", "```py\nTRANSFORMERS_TEST_DEVICE=\"cpu\" pytest tests/utils/test_logging.py\n```", "```py\nTRANSFORMERS_TEST_BACKEND=\"torch_npu\" pytest tests/utils/test_logging.py\n```", "```py\nimport torch\nimport torch_npu\n# !! Further additional imports can be added here !!\n\n# Specify the device name (eg. 'cuda', 'cpu', 'npu')\nDEVICE_NAME = 'npu'\n\n# Specify device-specific backends to dispatch to.\n# If not specified, will fallback to 'default' in 'testing_utils.py`\nMANUAL_SEED_FN = torch.npu.manual_seed\nEMPTY_CACHE_FN = torch.npu.empty_cache\nDEVICE_COUNT_FN = torch.npu.device_count\n```", "```py\nCUDA_VISIBLE_DEVICES=0,1 RUN_SLOW=1 pytest -sv tests/test_trainer_distributed.py\n```", "```py\npytest -s tests/utils/test_logging.py\n```", "```py\npy.test tests --junitxml=result.xml\n```", "```py\npytest --color=no tests/utils/test_logging.py\n```", "```py\npytest --pastebin=failed tests/utils/test_logging.py\n```", "```py\npytest --pastebin=all tests/utils/test_logging.py\n```", "```py\n# test_this1.py\nimport unittest\nfrom parameterized import parameterized\n\nclass TestMathUnitTest(unittest.TestCase):\n @parameterized.expand( [\n            (\"negative\", -1.5, -2.0),\n            (\"integer\", 1, 1.0),\n            (\"large fraction\", 1.6, 1),\n        ] )\n    def test_floor(self, name, input, expected):\n        assert_equal(math.floor(input), expected)\n```", "```py\npytest -k \"negative and integer\" tests/test_mytest.py\n```", "```py\npytest -k \"not negative\" tests/test_mytest.py\n```", "```py\npytest test_this1.py --collect-only -q\n```", "```py\ntest_this1.py::TestMathUnitTest::test_floor_0_negative\ntest_this1.py::TestMathUnitTest::test_floor_1_integer\ntest_this1.py::TestMathUnitTest::test_floor_2_large_fraction\n```", "```py\npytest test_this1.py::TestMathUnitTest::test_floor_0_negative  test_this1.py::TestMathUnitTest::test_floor_1_integer\n```", "```py\n# test_this2.py\nimport pytest\n\n@pytest.mark.parametrize( \"name, input, expected\",\n    [\n        (\"negative\", -1.5, -2.0),\n        (\"integer\", 1, 1.0),\n        (\"large fraction\", 1.6, 1),\n    ], )\ndef test_floor(name, input, expected):\n    assert_equal(math.floor(input), expected)\n```", "```py\npytest test_this2.py --collect-only -q\n```", "```py\ntest_this2.py::test_floor[integer-1-1.0]\ntest_this2.py::test_floor[negative--1.5--2.0]\ntest_this2.py::test_floor[large fraction-1.6-1]\n```", "```py\npytest test_this2.py::test_floor[negative--1.5--2.0] test_this2.py::test_floor[integer-1-1.0]\n```", "```py\nfrom transformers.testing_utils import TestCasePlus\n\nclass PathExampleTest(TestCasePlus):\n    def test_something_involving_local_locations(self):\n        data_dir = self.tests_dir / \"fixtures/tests_samples/wmt_en_ro\"\n```", "```py\nfrom transformers.testing_utils import TestCasePlus\n\nclass PathExampleTest(TestCasePlus):\n    def test_something_involving_stringified_locations(self):\n        examples_dir = self.examples_dir_str\n```", "```py\nfrom transformers.testing_utils import TestCasePlus\n\nclass ExamplesTests(TestCasePlus):\n    def test_whatever(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n```", "```py\ndef test_whatever(self):\n    tmp_dir = self.get_auto_remove_tmp_dir()\n```", "```py\ndef test_whatever(self):\n    tmp_dir = self.get_auto_remove_tmp_dir(\"./xxx\")\n```", "```py\nimport os\nfrom transformers.testing_utils import ExtendSysPath\n\nbindir = os.path.abspath(os.path.dirname(__file__))\nwith ExtendSysPath(f\"{bindir}/..\"):\n    from test_trainer import TrainerIntegrationCommon  # noqa\n```", "```py\n@unittest.skip(\"this bug needs to be fixed\")\ndef test_feature_x():\n```", "```py\n@pytest.mark.skip(reason=\"this bug needs to be fixed\")\n```", "```py\n@pytest.mark.xfail\ndef test_feature_x():\n```", "```py\ndef test_feature_x():\n    if not has_something():\n        pytest.skip(\"unsupported configuration\")\n```", "```py\nimport pytest\n\nif not pytest.config.getoption(\"--custom-flag\"):\n    pytest.skip(\"--custom-flag is missing, skipping tests\", allow_module_level=True)\n```", "```py\ndef test_feature_x():\n    pytest.xfail(\"expected to fail until bug XYZ is fixed\")\n```", "```py\ndocutils = pytest.importorskip(\"docutils\", minversion=\"0.3\")\n```", "```py\n@pytest.mark.skipif(sys.version_info < (3,6), reason=\"requires python3.6 or higher\")\ndef test_feature_x():\n```", "```py\n@unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\ndef test_feature_x():\n```", "```py\n@pytest.mark.skipif(sys.platform == 'win32', reason=\"does not run on windows\")\nclass TestClass():\n    def test_feature_x(self):\n```", "```py\nfrom transformers.testing_utils import slow\n@slow\ndef test_integration_foo():\n```", "```py\nRUN_SLOW=1 pytest tests\n```", "```py\n@parameteriz ed.expand(...)\n@slow\ndef test_integration_foo():\n```", "```py\ngrep tiny tests examples\n```", "```py\nimport sys\n\ndef print_to_stdout(s):\n    print(s)\n\ndef print_to_stderr(s):\n    sys.stderr.write(s)\n\ndef test_result_and_stdout(capsys):\n    msg = \"Hello\"\n    print_to_stdout(msg)\n    print_to_stderr(msg)\n    out, err = capsys.readouterr()  # consume the captured output streams\n    # optional: if you want to replay the consumed streams:\n    sys.stdout.write(out)\n    sys.stderr.write(err)\n    # test:\n    assert msg in out\n    assert msg in err\n```", "```py\ndef raise_exception(msg):\n    raise ValueError(msg)\n\ndef test_something_exception():\n    msg = \"Not a good value\"\n    error = \"\"\n    try:\n        raise_exception(msg)\n    except Exception as e:\n        error = str(e)\n        assert msg in error, f\"{msg} is in the exception:\\n{error}\"\n```", "```py\nfrom io import StringIO\nfrom contextlib import redirect_stdout\n\ndef print_to_stdout(s):\n    print(s)\n\ndef test_result_and_stdout():\n    msg = \"Hello\"\n    buffer = StringIO()\n    with redirect_stdout(buffer):\n        print_to_stdout(msg)\n    out = buffer.getvalue()\n    # optional: if you want to replay the consumed streams:\n    sys.stdout.write(out)\n    # test:\n    assert msg in out\n```", "```py\nfrom transformers.testing_utils import CaptureStdout\n\nwith CaptureStdout() as cs:\n    function_that_writes_to_stdout()\nprint(cs.out)\n```", "```py\nfrom transformers.testing_utils import CaptureStdout\n\nmsg = \"Secret message\\r\"\nfinal = \"Hello World\"\nwith CaptureStdout() as cs:\n    print(msg + final)\nassert cs.out == final + \"\\n\", f\"captured: {cs.out}, expecting {final}\"\n```", "```py\nfrom transformers.testing_utils import CaptureStderr\n\nwith CaptureStderr() as cs:\n    function_that_writes_to_stderr()\nprint(cs.err)\n```", "```py\nfrom transformers.testing_utils import CaptureStd\n\nwith CaptureStd() as cs:\n    function_that_writes_to_stdout_and_stderr()\nprint(cs.err, cs.out)\n```", "```py\nfrom transformers import logging\nfrom transformers.testing_utils import CaptureLogger\n\nmsg = \"Testing 1, 2, 3\"\nlogging.set_verbosity_info()\nlogger = logging.get_logger(\"transformers.models.bart.tokenization_bart\")\nwith CaptureLogger(logger) as cl:\n    logger.info(msg)\nassert cl.out, msg + \"\\n\"\n```", "```py\nfrom transformers.testing_utils import mockenv\n\nclass HfArgumentParserTest(unittest.TestCase):\n @mockenv(TRANSFORMERS_VERBOSITY=\"error\")\n    def test_env_override(self):\n        env_level_str = os.getenv(\"TRANSFORMERS_VERBOSITY\", None)\n```", "```py\nfrom transformers.testing_utils import TestCasePlus\n\nclass EnvExampleTest(TestCasePlus):\n    def test_external_prog(self):\n        env = self.get_env()\n        # now call the external program, passing `env` to it\n```", "```py\nseed = 42\n\n# python RNG\nimport random\n\nrandom.seed(seed)\n\n# pytorch RNGs\nimport torch\n\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# numpy RNG\nimport numpy as np\n\nnp.random.seed(seed)\n\n# tf RNG\ntf.random.set_seed(seed)\n```", "```py\npytest tests/utils/test_logging.py -W error::UserWarning --pdb\n```", "```py\n- run:\n    name: run CI experiment\n    command: |\n        set +euo pipefail\n        echo \"setting run-all-despite-any-errors-mode\"\n        this_command_will_fail\n        echo \"but bash continues to run\"\n        # emulate another failure\n        false\n        # but the last command must be a success\n        echo \"during experiment do not remove: reporting success to CI, even if there were failures\"\n```", "```py\ncmd_that_may_fail || true\n```"]