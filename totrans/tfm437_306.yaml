- en: EnCodec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/encodec](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/encodec)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The EnCodec neural codec model was proposed in [High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438)
    by Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging
    neural networks. It consists in a streaming encoder-decoder architecture with
    quantized latent space trained in an end-to-end fashion. We simplify and speed-up
    the training by using a single multiscale spectrogram adversary that efficiently
    reduces artifacts and produce high-quality samples. We introduce a novel loss
    balancer mechanism to stabilize training: the weight of a loss now defines the
    fraction of the overall gradient it should represent, thus decoupling the choice
    of this hyper-parameter from the typical scale of the loss. Finally, we study
    how lightweight Transformer models can be used to further compress the obtained
    representation by up to 40%, while staying faster than real time. We provide a
    detailed description of the key design choices of the proposed model including:
    training objective, architectural changes and a study of various perceptual loss
    functions. We present an extensive subjective evaluation (MUSHRA tests) together
    with an ablation study for a range of bandwidths and audio domains, including
    speech, noisy-reverberant speech, and music. Our approach is superior to the baselines
    methods across all evaluated settings, considering both 24 kHz monophonic and
    48 kHz stereophonic audio.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [Matthijs](https://huggingface.co/Matthijs), [Patrick
    Von Platen](https://huggingface.co/patrickvonplaten) and [Arthur Zucker](https://huggingface.co/ArthurZ).
    The original code can be found [here](https://github.com/facebookresearch/encodec).
  prefs: []
  type: TYPE_NORMAL
- en: Usage example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is a quick example of how to encode and decode an audio using this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: EncodecConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.EncodecConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encodec/configuration_encodec.py#L35)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`target_bandwidths` (`List[float]`, *optional*, defaults to `[1.5, 3.0, 6.0,
    12.0, 24.0]`) — The range of diffent bandwiths the model can encode audio with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*, defaults to 24000) — The sampling rate
    at which the audio waveform should be digitalized expressed in hertz (Hz).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audio_channels` (`int`, *optional*, defaults to 1) — Number of channels in
    the audio data. Either 1 for mono or 2 for stereo.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalize` (`bool`, *optional*, defaults to `False`) — Whether the audio shall
    be normalized when passed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunk_length_s` (`float`, *optional*) — If defined the audio is pre-processed
    into chunks of lengths `chunk_length_s` and then encoded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`overlap` (`float`, *optional*) — Defines the overlap between each chunk. It
    is used to compute the `chunk_stride` using the following formulae : `int((1.0
    - self.overlap) * self.chunk_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 128) — Intermediate representation
    dimension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_filters` (`int`, *optional*, defaults to 32) — Number of convolution kernels
    of first `EncodecConv1d` down sampling layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_residual_layers` (`int`, *optional*, defaults to 1) — Number of residual
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`upsampling_ratios` (`Sequence[int]` , *optional*, defaults to `[8, 5, 4, 2]`)
    — Kernel size and stride ratios. The encoder uses downsampling ratios instead
    of upsampling ratios, hence it will use the ratios in the reverse order to the
    ones specified here that must match the decoder order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`norm_type` (`str`, *optional*, defaults to `"weight_norm"`) — Normalization
    method. Should be in `["weight_norm", "time_group_norm"]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel_size` (`int`, *optional*, defaults to 7) — Kernel size for the initial
    convolution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`last_kernel_size` (`int`, *optional*, defaults to 7) — Kernel size for the
    last convolution layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`residual_kernel_size` (`int`, *optional*, defaults to 3) — Kernel size for
    the residual layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dilation_growth_rate` (`int`, *optional*, defaults to 2) — How much to increase
    the dilation with each layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_causal_conv` (`bool`, *optional*, defaults to `True`) — Whether to use
    fully causal convolution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_mode` (`str`, *optional*, defaults to `"reflect"`) — Padding mode for
    the convolutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`compress` (`int`, *optional*, defaults to 2) — Reduced dimensionality in residual
    branches (from Demucs v3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_lstm_layers` (`int`, *optional*, defaults to 2) — Number of LSTM layers
    at the end of the encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trim_right_ratio` (`float`, *optional*, defaults to 1.0) — Ratio for trimming
    at the right of the transposed convolution under the `use_causal_conv = True`
    setup. If equal to 1.0, it means that all the trimming is done at the right.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_size` (`int`, *optional*, defaults to 1024) — Number of discret codes
    that make up VQVAE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_dim` (`int`, *optional*) — Dimension of the codebook vectors. If
    not defined, uses `hidden_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_conv_shortcut` (`bool`, *optional*, defaults to `True`) — Whether to use
    a convolutional layer as the ‘skip’ connection in the `EncodecResnetBlock` block.
    If False, an identity function will be used, giving a generic residual connection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of an [EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel).
    It is used to instantiate a Encodec model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the [facebook/encodec_24khz](https://huggingface.co/facebook/encodec_24khz)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: EncodecFeatureExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.EncodecFeatureExtractor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encodec/feature_extraction_encodec.py#L29)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_size` (`int`, *optional*, defaults to 1) — The feature dimension of
    the extracted features. Use 1 for mono, 2 for stereo.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*, defaults to 24000) — The sampling rate
    at which the audio waveform should be digitalized expressed in hertz (Hz).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding_value` (`float`, *optional*, defaults to 0.0) — The value that is
    used to fill the padding values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunk_length_s` (`float`, *optional*) — If defined the audio is pre-processed
    into chunks of lengths `chunk_length_s` and then encoded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`overlap` (`float`, *optional*) — Defines the overlap between each chunk. It
    is used to compute the `chunk_stride` using the following formulae : `int((1.0
    - self.overlap) * self.chunk_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs an EnCodec feature extractor.
  prefs: []
  type: TYPE_NORMAL
- en: This feature extractor inherits from [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: Instantiating a feature extractor with the defaults will yield a similar configuration
    to that of the [facebook/encodec_24khz](https://huggingface.co/facebook/encodec_24khz)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encodec/feature_extraction_encodec.py#L84)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`raw_audio` (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`)
    — The sequence or batch of sequences to be processed. Each sequence can be a numpy
    array, a list of float values, a list of numpy arrays or a list of list of float
    values. The numpy array must be of shape `(num_samples,)` for mono audio (`feature_size
    = 1`), or `(2, num_samples)` for stereo audio (`feature_size = 2`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `True`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`bool`, *optional*, defaults to `False`) — Activates truncation
    to cut input sequences longer than `max_length` to `max_length`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — Maximum length of the returned list and
    optionally padding length (see above).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*) — The sampling rate at which the `audio`
    input was sampled. It is strongly recommended to pass `sampling_rate` at the forward
    call to prevent silent errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to featurize and prepare for the model one or several sequence(s).
  prefs: []
  type: TYPE_NORMAL
- en: EncodecModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.EncodecModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encodec/modeling_encodec.py#L526)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([EncodecConfig](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The EnCodec neural audio codec model. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encodec/modeling_encodec.py#L707)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`audio_codes` (`torch.FloatTensor` of shape `(batch_size, nb_chunks, chunk_length)`,
    *optional*) — Discret code embeddings computed using `model.encode`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audio_scales` (`torch.Tensor` of shape `(batch_size, nb_chunks)`, *optional*)
    — Scaling factor for each `audio_codes` input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding_mask` (`torch.Tensor` of shape `(batch_size, channels, sequence_length)`)
    — Padding mask used to pad the `input_values`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decodes the given frames into an output audio waveform.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the output might be a bit bigger than the input. In that case, any
    extra steps at the end can be trimmed.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encodec/modeling_encodec.py#L579)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_values` (`torch.Tensor` of shape `(batch_size, channels, sequence_length)`)
    — Float values of the input audio waveform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding_mask` (`torch.Tensor` of shape `(batch_size, channels, sequence_length)`)
    — Padding mask used to pad the `input_values`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bandwidth` (`float`, *optional*) — The target bandwidth. Must be one of `config.target_bandwidths`.
    If `None`, uses the smallest possible bandwidth. bandwidth is represented as a
    thousandth of what it is, e.g. 6kbps bandwidth is represented as bandwidth ==
    6.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encodes the input audio waveform into discrete codes.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/encodec/modeling_encodec.py#L755)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, channels, sequence_length)`,
    *optional*) — Raw audio input converted to Float and padded to the approriate
    length in order to be encoded using chunks of length self.chunk_length and a stride
    of `config.chunk_stride`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding_mask` (`torch.BoolTensor` of shape `(batch_size, channels, sequence_length)`,
    *optional*) — Mask to avoid computing scaling factors on padding token indices
    (can we avoid computing conv on these+). Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding_mask` should always be passed, unless the input was truncated or not
    padded. This is because in order to process tensors effectively, the input audio
    should be padded so that `input_length % stride = step` with `step = chunk_length-stride`.
    This ensures that all chunks are of the same shape'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`bandwidth` (`float`, *optional*) — The target bandwidth. Must be one of `config.target_bandwidths`.
    If `None`, uses the smallest possible bandwidth. bandwidth is represented as a
    thousandth of what it is, e.g. 6kbps bandwidth is represented as `bandwidth ==
    6.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audio_codes` (`torch.FloatTensor` of shape `(batch_size, nb_chunks, chunk_length)`,
    *optional*) — Discret code embeddings computed using `model.encode`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audio_scales` (`torch.Tensor` of shape `(batch_size, nb_chunks)`, *optional*)
    — Scaling factor for each `audio_codes` input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.encodec.modeling_encodec.EncodecOutput` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.encodec.modeling_encodec.EncodecOutput` or a tuple of
    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([EncodecConfig](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`audio_codes` (`torch.FloatTensor` of shape `(batch_size, nb_chunks, chunk_length)`,
    *optional*) — Discret code embeddings computed using `model.encode`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audio_values` (`torch.FlaotTensor` of shape `(batch_size, sequence_length)`,
    *optional*) Decoded audio values, obtained using the decoder part of Encodec.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [EncodecModel](/docs/transformers/v4.37.2/en/model_doc/encodec#transformers.EncodecModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
