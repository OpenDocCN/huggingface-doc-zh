- en: TAPEX
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TAPEX
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/tapex](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/tapex)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/tapex](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/tapex)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This model is in maintenance mode only, we don’t accept any new PRs changing
    its code.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型仅处于维护模式，我们不接受任何更改其代码的新PR。
- en: 'If you run into any issues running this model, please reinstall the last version
    that supported this model: v4.30.0. You can do so by running the following command:
    `pip install -U transformers==4.30.0`.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在运行此模型时遇到任何问题，请重新安装支持此模型的最后一个版本：v4.30.0。您可以通过运行以下命令来执行：`pip install -U transformers==4.30.0`。
- en: Overview
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The TAPEX model was proposed in [TAPEX: Table Pre-training via Learning a Neural
    SQL Executor](https://arxiv.org/abs/2107.07653) by Qian Liu, Bei Chen, Jiaqi Guo,
    Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. TAPEX pre-trains a BART
    model to solve synthetic SQL queries, after which it can be fine-tuned to answer
    natural language questions related to tabular data, as well as performing table
    fact checking.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'TAPEX模型是由Qian Liu、Bei Chen、Jiaqi Guo、Morteza Ziyadi、Zeqi Lin、Weizhu Chen、Jian-Guang
    Lou在[TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653)中提出的。TAPEX对BART模型进行预训练，以解决合成SQL查询，然后可以微调以回答与表格数据相关的自然语言问题，以及执行表格事实检查。'
- en: 'TAPEX has been fine-tuned on several datasets:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: TAPEX已在几个数据集上进行了微调：
- en: '[SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) (Sequential
    Question Answering by Microsoft)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) (Microsoft的顺序问答)'
- en: '[WTQ](https://github.com/ppasupat/WikiTableQuestions) (Wiki Table Questions
    by Stanford University)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WTQ](https://github.com/ppasupat/WikiTableQuestions)（由斯坦福大学提供的维基表问题）'
- en: '[WikiSQL](https://github.com/salesforce/WikiSQL) (by Salesforce)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WikiSQL](https://github.com/salesforce/WikiSQL)（由Salesforce提供）'
- en: '[TabFact](https://tabfact.github.io/) (by USCB NLP Lab).'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TabFact](https://tabfact.github.io/)（由USCB NLP实验室提供）。'
- en: 'The abstract from the paper is the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Recent progress in language model pre-training has achieved a great success
    via leveraging large-scale unstructured textual data. However, it is still a challenge
    to apply pre-training on structured tabular data due to the absence of large-scale
    high-quality tabular data. In this paper, we propose TAPEX to show that table
    pre-training can be achieved by learning a neural SQL executor over a synthetic
    corpus, which is obtained by automatically synthesizing executable SQL queries
    and their execution outputs. TAPEX addresses the data scarcity challenge via guiding
    the language model to mimic a SQL executor on the diverse, large-scale and high-quality
    synthetic corpus. We evaluate TAPEX on four benchmark datasets. Experimental results
    demonstrate that TAPEX outperforms previous table pre-training approaches by a
    large margin and achieves new state-of-the-art results on all of them. This includes
    improvements on the weakly-supervised WikiSQL denotation accuracy to 89.5% (+2.3%),
    the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA denotation
    accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.2% (+3.2%). To our knowledge,
    this is the first work to exploit table pre-training via synthetic executable
    programs and to achieve new state-of-the-art results on various downstream tasks.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*最近在语言模型预训练方面取得了巨大成功，通过利用大规模的非结构化文本数据。然而，由于缺乏大规模高质量的表格数据，将预训练应用于结构化表格数据仍然是一个挑战。在本文中，我们提出了TAPEX，以表明通过学习一个神经SQL执行器可以实现表格预训练，这是通过自动合成可执行的SQL查询及其执行输出获得的合成语料库。TAPEX通过引导语言模型在多样化、大规模和高质量的合成语料库上模仿SQL执行器来解决数据稀缺性挑战。我们在四个基准数据集上评估了TAPEX。实验结果表明，TAPEX在所有这些数据集上均大幅优于以前的表格预训练方法，并在所有数据集上取得了新的最先进结果。这包括将弱监督的WikiSQL指称准确率提高到89.5%（+2.3%），将WikiTableQuestions指称准确率提高到57.5%（+4.8%），将SQA指称准确率提高到74.5%（+3.5%），以及将TabFact准确率提高到84.2%（+3.2%）。据我们所知，这是第一项利用合成可执行程序进行表格预训练并在各种下游任务上取得新的最先进结果的工作。*'
- en: Usage tips
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: TAPEX is a generative (seq2seq) model. One can directly plug in the weights
    of TAPEX into a BART model.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TAPEX是一个生成（seq2seq）模型。可以直接将TAPEX的权重插入到BART模型中。
- en: TAPEX has checkpoints on the hub that are either pre-trained only, or fine-tuned
    on WTQ, SQA, WikiSQL and TabFact.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TAPEX在中心点上有检查点，这些检查点要么只是预训练的，要么在WTQ、SQA、WikiSQL和TabFact上进行了微调。
- en: 'Sentences + tables are presented to the model as `sentence + " " + linearized
    table`. The linearized table has the following format: `col: col1 | col2 | col
    3 row 1 : val1 | val2 | val3 row 2 : ...`.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '句子+表格以`句子 + " " + 线性化表格`的形式呈现给模型。线性化表格的格式如下：`col: col1 | col2 | col 3 row 1
    : val1 | val2 | val3 row 2 : ...`。'
- en: TAPEX has its own tokenizer, that allows to prepare all data for the model easily.
    One can pass Pandas DataFrames and strings to the tokenizer, and it will automatically
    create the `input_ids` and `attention_mask` (as shown in the usage examples below).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TAPEX有自己的分词器，可以轻松为模型准备所有数据。可以将Pandas DataFrames和字符串传递给分词器，它将自动创建`input_ids`和`attention_mask`（如下面的使用示例所示）。
- en: 'Usage: inference'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用途：推理
- en: Below, we illustrate how to use TAPEX for table question answering. As one can
    see, one can directly plug in the weights of TAPEX into a BART model. We use the
    [Auto API](auto), which will automatically instantiate the appropriate tokenizer
    ([TapexTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapex#transformers.TapexTokenizer))
    and model ([BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration))
    for us, based on the configuration file of the checkpoint on the hub.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，我们说明如何使用TAPEX进行表格问答。正如大家所看到的，可以直接将TAPEX的权重插入到BART模型中。我们使用[Auto API](auto)，它将根据中心点上的配置文件自动实例化适当的分词器（[TapexTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapex#transformers.TapexTokenizer)）和模型（[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)）。
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note that [TapexTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapex#transformers.TapexTokenizer)
    also supports batched inference. Hence, one can provide a batch of different tables/questions,
    or a batch of a single table and multiple questions, or a batch of a single query
    and multiple tables. Let’s illustrate this:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[TapexTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapex#transformers.TapexTokenizer)还支持批量推断。因此，可以提供不同表/问题的批处理，或单个表和多个问题的批处理，或单个查询和多个表的批处理。让我们举个例子：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In case one wants to do table verification (i.e. the task of determining whether
    a given sentence is supported or refuted by the contents of a table), one can
    instantiate a [BartForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForSequenceClassification)
    model. TAPEX has checkpoints on the hub fine-tuned on TabFact, an important benchmark
    for table fact checking (it achieves 84% accuracy). The code example below again
    leverages the [Auto API](auto).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要进行表验证（即确定给定句子是否由表的内容支持或反驳的任务），可以实例化一个[BartForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForSequenceClassification)模型。
    TAPEX在hub上有针对TabFact进行微调的检查点，这是表事实检查的重要基准（它达到84%的准确率）。下面的代码示例再次利用了[Auto API](auto)。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: TAPEX architecture is the same as BART, except for tokenization. Refer to [BART
    documentation](bart) for information on configuration classes and their parameters.
    TAPEX-specific tokenizer is documented below.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: TAPEX架构与BART相同，除了标记化。有关配置类及其参数的信息，请参阅[BART文档](bart)。下面记录了TAPEX特定的标记器。
- en: TapexTokenizer
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TapexTokenizer
- en: '### `class transformers.TapexTokenizer`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TapexTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py#L194)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py#L194)'
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file`（`str`）—词汇表文件的路径。'
- en: '`merges_file` (`str`) — Path to the merges file.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file`（`str`）—合并文件的路径。'
- en: '`do_lower_case` (`bool`, *optional*, defaults to `True`) — Whether or not to
    lowercase the input when tokenizing.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_lower_case`（`bool`，*可选*，默认为`True`）—在标记化时是否将输入转换为小写。'
- en: '`errors` (`str`, *optional*, defaults to `"replace"`) — Paradigm to follow
    when decoding bytes to UTF-8\. See [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)
    for more information.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`errors`（`str`，*可选*，默认为`"replace"`）—解码字节为UTF-8时要遵循的范例。有关更多信息，请参阅[bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`，*可选*，默认为`"<s>"`）—在预训练期间使用的序列开始标记。可以用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用特殊标记构建序列时，这不是用于序列开头的标记。使用的标记是`cls_token`。
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`（`str`，*可选*，默认为`"</s>"`）—序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 构建序列时使用特殊标记时，这不是用于序列结尾的标记。使用的标记是`sep_token`。
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token`（`str`，*可选*，默认为`"</s>"`）—分隔符标记，在从多个序列构建序列时使用，例如，用于序列分类的两个序列或用于问题回答的文本和问题。它也用作使用特殊标记构建的序列的最后一个标记。'
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token`（`str`，*可选*，默认为`"<s>"`）—分类器标记，用于进行序列分类（对整个序列进行分类，而不是对每个标记进行分类）。在使用特殊标记构建时，它是序列的第一个标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*可选*，默认为`"<unk>"`）—未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token`（`str`，*可选*，默认为`"<pad>"`）—用于填充的标记，例如在批处理不同长度的序列时。'
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token`（`str`，*可选*，默认为`"<mask>"`）—用于屏蔽值的标记。在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) — Whether or not
    to add an initial space to the input. This allows to treat the leading word just
    as any other word. (BART tokenizer detect beginning of words by the preceding
    space).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_prefix_space`（`bool`，*可选*，默认为`False`）—是否在输入中添加初始空格。这允许将前导单词视为任何其他单词。（BART标记器通过前导空格检测单词的开头）。'
- en: '`max_cell_length` (`int`, *optional*, defaults to 15) — Maximum number of characters
    per cell when linearizing a table. If this number is exceeded, truncation takes
    place.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_cell_length`（`int`，*可选*，默认为15）—线性化表时每个单元格的最大字符数。如果超过此数字，将进行截断。'
- en: Construct a TAPEX tokenizer. Based on byte-level Byte-Pair-Encoding (BPE).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个TAPEX标记器。基于字节级字节对编码（BPE）。
- en: 'This tokenizer can be used to flatten one or more table(s) and concatenate
    them with one or more related sentences to be used by TAPEX models. The format
    that the TAPEX tokenizer creates is the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器可用于展平一个或多个表格，并将它们与一个或多个相关句子连接起来，以供TAPEX模型使用。TAPEX分词器创建的格式如下：
- en: 'sentence col: col1 | col2 | col 3 row 1 : val1 | val2 | val3 row 2 : …'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 句子列：col1 | col2 | col 3 行1：val1 | val2 | val3 行2：…
- en: The tokenizer supports a single table + single query, a single table and multiple
    queries (in which case the table will be duplicated for every query), a single
    query and multiple tables (in which case the query will be duplicated for every
    table), and multiple tables and queries. In other words, you can provide a batch
    of tables + questions to the tokenizer for instance to prepare them for the model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器支持单个表格+单个查询，单个表格和多个查询（在这种情况下，每个查询将被复制以匹配每个表格），单个查询和多个表格（在这种情况下，每个表格将被复制以匹配每个查询），以及多个表格和查询。换句话说，您可以为分词器提供一批表格+问题，以便为模型准备它们。
- en: Tokenization itself is based on the BPE algorithm. It is identical to the one
    used by BART, RoBERTa and GPT-2.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 分词本身基于BPE算法。它与BART、RoBERTa和GPT-2使用的算法相同。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `__call__`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py#L515)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py#L515)'
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`table` (`pd.DataFrame`, `List[pd.DataFrame]`) — Table(s) containing tabular
    data.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`table`（`pd.DataFrame`，`List[pd.DataFrame]`）— 包含表格数据的表格。'
- en: '`query` (`str` or `List[str]`, *optional*) — Sentence or batch of sentences
    related to one or more table(s) to be encoded. Note that the number of sentences
    must match the number of tables.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query`（`str`或`List[str]`，*可选*）— 与一个或多个表格相关的句子或句子批次。请注意，句子的数量必须与表格的数量相匹配。'
- en: '`answer` (`str` or `List[str]`, *optional*) — Optionally, the corresponding
    answer to the questions as supervision.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`answer`（`str`或`List[str]`，*可选*）— 可选地，与问题相关的答案作为监督。'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens`（`bool`，*可选*，默认为`True`）— 在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了自动添加到输入id的标记。如果要自动添加`bos`或`eos`标记，则这很有用。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`（`bool`，`str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`False`）—
    激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True`或`''longest''`：填充到批次中最长的序列（如果只提供单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: 使用参数`max_length`指定的最大长度进行填充，或者如果未提供该参数，则填充到模型的最大可接受输入长度。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`或`''do_not_pad''`（默认）：不填充（即可以输出具有不同长度的序列的批次）。'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`（`bool`，`str`或[TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)，*可选*，默认为`False`）—
    激活和控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True`或`''longest_first''`：使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则将逐个标记截断，从最长序列中删除一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`: 使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则只会截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: 使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则截断到模型的最大可接受输入长度。如果提供了一对序列（或一批对序列），则只会截断第二个序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`或`''do_not_truncate''`（默认）：不截断（即可以输出长度大于模型最大可接受输入大小的批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *可选*) — 控制截断/填充参数之一使用的最大长度。'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置或设置为 `None`，则将使用预定义的模型最大长度（如果截断/填充参数需要最大长度）。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *可选*, 默认为 0) — 如果与 `max_length` 一起设置为一个数字，则当 `return_overflowing_tokens=True`
    时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义重叠标记的数量。'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words` (`bool`, *可选*, 默认为 `False`) — 输入是否已经预分词（例如，已分词）。如果设置为
    `True`，分词器会假定输入已经分词（例如，通过在空格上分割），然后对其进行标记化。这对于 NER 或标记分类很有用。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *可选*) — 如果设置，将序列填充到提供的值的倍数。需要激活 `padding`。这对于启用
    NVIDIA 硬件上的 Tensor Cores（计算能力 `>= 7.5`，Volta）特别有用。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回 TensorFlow `tf.constant` 对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回 PyTorch `torch.Tensor` 对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回 Numpy `np.ndarray` 对象。'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to encode the sequences with the special tokens relative to their model.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (`bool`, *可选*, 默认为 `True`) — 是否对序列进行编码，使用相对于其模型的特殊标记。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *可选*, 默认为 `False`) — 激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`: 填充到批次中最长的序列（如果只提供了单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: 填充到由参数 `max_length` 指定的最大长度，或者填充到模型可接受的最大输入长度（如果未提供该参数）。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）: 无填充（即，可以输出长度不同的序列批次）。'
- en: '`truncation` (`bool`, `str`, `TapexTruncationStrategy` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    — *optional*, defaults to `False`):'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str`, `TapexTruncationStrategy` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    — *可选*, 默认为 `False`):'
- en: 'Activates and controls truncation. Accepts the following values:'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 激活和控制截断。接受以下值：
- en: '`''drop_rows_to_fit''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will truncate row by row, removing rows from the table.'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''drop_rows_to_fit''`: 截断到由参数 `max_length` 指定的最大长度，或者截断到模型可接受的最大输入长度（如果未提供该参数）。这将逐行截断，从表中删除行。'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest_first''`: 截断到由参数 `max_length` 指定的最大长度，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列），则将逐个标记截断，从一对序列中最长的序列中删除一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`: 截断到由参数 `max_length` 指定的最大长度，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列），则仅截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: 截断到由参数 `max_length` 指定的最大长度，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列），则仅截断第二个序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`或`''do_not_truncate''`（默认）：不截断（即，可以输出具有大于模型最大可接受输入大小的序列长度的批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters. If left unset or set to `None`, this will
    use the predefined model maximum length if a maximum length is required by one
    of the truncation/padding parameters. If the model has no specific maximum input
    length (like XLNet) truncation/padding to a maximum length will be deactivated.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *optional*) — 控制截断/填充参数之一使用的最大长度。如果未设置或设置为`None`，则将使用预定义的模型最大长度（如果截断/填充参数之一需要最大长度）。如果模型没有特定的最大输入长度（如XLNet），则截断/填充到最大长度将被停用。'
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *optional*, 默认为0) — 如果与`max_length`一起设置为一个数字，则当`return_overflowing_tokens=True`时返回的溢出标记将包含从截断序列末尾返回的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义了重叠标记的数量。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *optional*) — 如果设置，将填充序列到提供的值的倍数。这对于在具有计算能力`>=
    7.5`（Volta）的NVIDIA硬件上启用Tensor Cores特别有用。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — 如果设置，将返回张量而不是Python整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回TensorFlow `tf.constant`对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回PyTorch `torch.Tensor`对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回Numpy `np.ndarray`对象。'
- en: Main method to tokenize and prepare for the model one or several table-sequence
    pair(s).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对一个或多个表序列对进行标记化和为模型准备的主要方法。
- en: '#### `save_vocabulary`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py#L486)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py#L486)'
- en: '[PRE5]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
