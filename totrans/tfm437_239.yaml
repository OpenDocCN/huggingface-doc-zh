- en: TAPEX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/tapex](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/tapex)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: This model is in maintenance mode only, we don’t accept any new PRs changing
    its code.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run into any issues running this model, please reinstall the last version
    that supported this model: v4.30.0. You can do so by running the following command:
    `pip install -U transformers==4.30.0`.'
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The TAPEX model was proposed in [TAPEX: Table Pre-training via Learning a Neural
    SQL Executor](https://arxiv.org/abs/2107.07653) by Qian Liu, Bei Chen, Jiaqi Guo,
    Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. TAPEX pre-trains a BART
    model to solve synthetic SQL queries, after which it can be fine-tuned to answer
    natural language questions related to tabular data, as well as performing table
    fact checking.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TAPEX has been fine-tuned on several datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) (Sequential
    Question Answering by Microsoft)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[WTQ](https://github.com/ppasupat/WikiTableQuestions) (Wiki Table Questions
    by Stanford University)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[WikiSQL](https://github.com/salesforce/WikiSQL) (by Salesforce)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TabFact](https://tabfact.github.io/) (by USCB NLP Lab).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Recent progress in language model pre-training has achieved a great success
    via leveraging large-scale unstructured textual data. However, it is still a challenge
    to apply pre-training on structured tabular data due to the absence of large-scale
    high-quality tabular data. In this paper, we propose TAPEX to show that table
    pre-training can be achieved by learning a neural SQL executor over a synthetic
    corpus, which is obtained by automatically synthesizing executable SQL queries
    and their execution outputs. TAPEX addresses the data scarcity challenge via guiding
    the language model to mimic a SQL executor on the diverse, large-scale and high-quality
    synthetic corpus. We evaluate TAPEX on four benchmark datasets. Experimental results
    demonstrate that TAPEX outperforms previous table pre-training approaches by a
    large margin and achieves new state-of-the-art results on all of them. This includes
    improvements on the weakly-supervised WikiSQL denotation accuracy to 89.5% (+2.3%),
    the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA denotation
    accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.2% (+3.2%). To our knowledge,
    this is the first work to exploit table pre-training via synthetic executable
    programs and to achieve new state-of-the-art results on various downstream tasks.*'
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TAPEX is a generative (seq2seq) model. One can directly plug in the weights
    of TAPEX into a BART model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TAPEX has checkpoints on the hub that are either pre-trained only, or fine-tuned
    on WTQ, SQA, WikiSQL and TabFact.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sentences + tables are presented to the model as `sentence + " " + linearized
    table`. The linearized table has the following format: `col: col1 | col2 | col
    3 row 1 : val1 | val2 | val3 row 2 : ...`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TAPEX has its own tokenizer, that allows to prepare all data for the model easily.
    One can pass Pandas DataFrames and strings to the tokenizer, and it will automatically
    create the `input_ids` and `attention_mask` (as shown in the usage examples below).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage: inference'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Below, we illustrate how to use TAPEX for table question answering. As one can
    see, one can directly plug in the weights of TAPEX into a BART model. We use the
    [Auto API](auto), which will automatically instantiate the appropriate tokenizer
    ([TapexTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapex#transformers.TapexTokenizer))
    and model ([BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration))
    for us, based on the configuration file of the checkpoint on the hub.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that [TapexTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapex#transformers.TapexTokenizer)
    also supports batched inference. Hence, one can provide a batch of different tables/questions,
    or a batch of a single table and multiple questions, or a batch of a single query
    and multiple tables. Let’s illustrate this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In case one wants to do table verification (i.e. the task of determining whether
    a given sentence is supported or refuted by the contents of a table), one can
    instantiate a [BartForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForSequenceClassification)
    model. TAPEX has checkpoints on the hub fine-tuned on TabFact, an important benchmark
    for table fact checking (it achieves 84% accuracy). The code example below again
    leverages the [Auto API](auto).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: TAPEX architecture is the same as BART, except for tokenization. Refer to [BART
    documentation](bart) for information on configuration classes and their parameters.
    TAPEX-specific tokenizer is documented below.
  prefs: []
  type: TYPE_NORMAL
- en: TapexTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TapexTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py#L194)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`merges_file` (`str`) — Path to the merges file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_lower_case` (`bool`, *optional*, defaults to `True`) — Whether or not to
    lowercase the input when tokenizing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`errors` (`str`, *optional*, defaults to `"replace"`) — Paradigm to follow
    when decoding bytes to UTF-8\. See [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)
    for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) — Whether or not
    to add an initial space to the input. This allows to treat the leading word just
    as any other word. (BART tokenizer detect beginning of words by the preceding
    space).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_cell_length` (`int`, *optional*, defaults to 15) — Maximum number of characters
    per cell when linearizing a table. If this number is exceeded, truncation takes
    place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a TAPEX tokenizer. Based on byte-level Byte-Pair-Encoding (BPE).
  prefs: []
  type: TYPE_NORMAL
- en: 'This tokenizer can be used to flatten one or more table(s) and concatenate
    them with one or more related sentences to be used by TAPEX models. The format
    that the TAPEX tokenizer creates is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'sentence col: col1 | col2 | col 3 row 1 : val1 | val2 | val3 row 2 : …'
  prefs: []
  type: TYPE_NORMAL
- en: The tokenizer supports a single table + single query, a single table and multiple
    queries (in which case the table will be duplicated for every query), a single
    query and multiple tables (in which case the query will be duplicated for every
    table), and multiple tables and queries. In other words, you can provide a batch
    of tables + questions to the tokenizer for instance to prepare them for the model.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization itself is based on the BPE algorithm. It is identical to the one
    used by BART, RoBERTa and GPT-2.
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py#L515)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`table` (`pd.DataFrame`, `List[pd.DataFrame]`) — Table(s) containing tabular
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query` (`str` or `List[str]`, *optional*) — Sentence or batch of sentences
    related to one or more table(s) to be encoded. Note that the number of sentences
    must match the number of tables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`answer` (`str` or `List[str]`, *optional*) — Optionally, the corresponding
    answer to the questions as supervision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to encode the sequences with the special tokens relative to their model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`bool`, `str`, `TapexTruncationStrategy` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    — *optional*, defaults to `False`):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Activates and controls truncation. Accepts the following values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`''drop_rows_to_fit''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will truncate row by row, removing rows from the table.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters. If left unset or set to `None`, this will
    use the predefined model maximum length if a maximum length is required by one
    of the truncation/padding parameters. If the model has no specific maximum input
    length (like XLNet) truncation/padding to a maximum length will be deactivated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to tokenize and prepare for the model one or several table-sequence
    pair(s).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_vocabulary`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/tapex/tokenization_tapex.py#L486)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
